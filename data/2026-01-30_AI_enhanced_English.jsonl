{"id": "2601.20975", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20975", "abs": "https://arxiv.org/abs/2601.20975", "authors": ["Nikita Gupta", "Riju Chatterjee", "Lukas Haas", "Connie Tao", "Andrew Wang", "Chang Liu", "Hidekazu Oiwa", "Elena Gribovskaya", "Jan Ackermann", "John Blitzer", "Sasha Goldshtein", "Dipanjan Das"], "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents", "comment": "DeepSearchQA can be found at https://www.kaggle.com/benchmarks/google/dsqa/leaderboard", "summary": "We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.", "AI": {"tldr": "DeepSearchQA is a 900-prompt benchmark to test how well AI agents perform difficult, multi-step web search and information-gathering tasks, revealing major gaps in current systems.", "motivation": "Existing QA and agent benchmarks mostly test single-step answer retrieval or broad factual accuracy, which misses key skills needed for real-world research: planning multi-step searches, combining fragmented information, removing duplicates, and knowing when to stop searching. The authors want a benchmark that directly measures these deeper research abilities.", "method": "They construct a 900-prompt dataset spanning 17 fields, with each task designed as a multi-step, causal search chain grounded in the open web. Tasks require exhaustive list-style answers. The benchmark is explicitly built to test: (1) systematic collection of information across many sources, (2) de-duplication and entity resolution, and (3) reasoning about stopping criteria. They then evaluate various state-of-the-art agent architectures on this benchmark to measure recall, precision, and qualitative failure modes.", "result": "Across evaluated state-of-the-art agents, performance is poor: models have trouble achieving both high recall and high precision. Typical failures include stopping the search too early and missing items (under-retrieval) or, conversely, over-generating many low-confidence candidates to inflate recall at the cost of precision. These issues appear consistently across architectures.", "conclusion": "Current agent designs are not yet capable of robust, deep web research on complex, multi-step tasks. DeepSearchQA exposes the gaps\u2014especially in information collation, de-duplication, and stopping decisions\u2014and is proposed as a key diagnostic benchmark to guide and accelerate research toward more capable information-seeking agents."}}
{"id": "2601.20992", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20992", "abs": "https://arxiv.org/abs/2601.20992", "authors": ["Oleg Sedukhin", "Andrey Kostin"], "title": "asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation", "comment": null, "summary": "We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.", "AI": {"tldr": "The paper improves how speech recognition systems are evaluated, especially for complex languages like Russian, by introducing a better alignment algorithm, building a new multi-reference test set, and showing how dataset-specific labeling can mislead metrics.", "motivation": "Standard speech recognition evaluation struggles with non-Latin and morphologically rich languages, especially for longform, noisy, or in-the-wild speech. Existing metrics and alignment methods do not handle multi-reference transcriptions, arbitrary-length insertions, or fine-grained word alignment well. This makes it hard to fairly evaluate models, to analyze streaming behavior, and to compare outputs from different systems. There is also a concern that models fine-tuned on specific datasets may overfit to labeling conventions, inflating metric scores without real quality gains.", "method": "The authors design a new string alignment algorithm capable of: (1) handling multiple reference transcriptions per utterance (multi-reference labeling), (2) supporting arbitrary-length insertions, and (3) producing improved word-level alignments. They build DiverseSpeech-Ru, a longform, in-the-wild Russian speech test set with careful multi-reference labeling, and also re-label popular Russian test sets with multiple references. They then run fine-tuning experiments on the corresponding training data to investigate how models adapt to dataset-specific labeling conventions. Using their improved alignment, they implement tools for evaluating streaming ASR behavior and for visually aligning and comparing multiple transcriptions. Finally, they provide unified wrappers around a range of offline and streaming ASR models to simplify evaluation.", "result": "The new alignment algorithm successfully supports multi-reference evaluation and more accurate word alignments, allowing richer analysis of ASR outputs. The DiverseSpeech-Ru test set provides a realistic benchmark for longform Russian speech with robust labeling, and the re-labeled existing test sets show how multiple references change evaluation outcomes. Fine-tuning experiments reveal that models tend to adapt to the specific labeling style of the dataset, which can create a misleading appearance of metric improvement that does not correspond to actual recognition quality gains. The improved alignment enables practical tools for evaluating streaming recognition and for visual comparison of multiple system outputs.", "conclusion": "Improving the underlying alignment and evaluation methodology is crucial for reliable assessment of speech recognition systems, particularly for morphologically rich and non-Latin languages. Multi-reference labeling and better support for longform, in-the-wild speech reduce bias from idiosyncratic annotations and highlight when metric gains stem from label adaptation rather than true performance improvement. The provided algorithms, datasets, and tools form a more robust evaluation framework and can be used to benchmark both offline and streaming ASR models more fairly."}}
{"id": "2601.21084", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21084", "abs": "https://arxiv.org/abs/2601.21084", "authors": ["Amit Meghanani", "Thomas Hain"], "title": "Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations", "comment": "Accepted to ICASSP 2026", "summary": "Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.", "AI": {"tldr": "The paper studies how to better fine-tune speech enhancement models with self-supervised speech representations by avoiding over-reliance on positional embeddings and improving position-invariant learning.", "motivation": "Existing speech enhancement models fine-tuned with SSL features often use MSE between enhanced and clean speech representations. This can be gamed by models exploiting positional embeddings instead of learning content-related improvements, limiting robustness and downstream gains in noisy conditions. There is a need to understand and mitigate this positional shortcut problem during fine-tuning.", "method": "They frame the issue as a general limitation of self-supervised representation fine-tuning and focus on representation-guided speech enhancement. Two position-robust strategies are evaluated: (1) zero-padding to disrupt trivial positional alignments, applied in the fine-tuning stage, and (2) speed perturbation of speech combined with a soft-DTW (soft Dynamic Time Warping) loss to allow flexible, position-invariant alignment between enhanced and clean representations.", "result": "Experiments show that the soft-DTW-based speed perturbation strategy converges faster during training and yields better downstream performance than standard MSE-based fine-tuning, demonstrating reduced reliance on positional embeddings. Zero-padding is also tested but appears less effective than the soft-DTW approach.", "conclusion": "Fine-tuning SSL-based speech models for enhancement should be made position-invariant to avoid positional shortcut learning. Using alignment-robust techniques like speed perturbation with a soft-DTW loss improves both training dynamics and downstream task performance in noisy conditions, revealing a broader limitation of naive MSE-based fine-tuning on SSL representations."}}
{"id": "2601.20920", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20920", "abs": "https://arxiv.org/abs/2601.20920", "authors": ["Vibhhu Sharma", "Thorsten Joachims", "Sarah Dean"], "title": "Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review", "comment": "28 pages", "summary": "There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.", "AI": {"tldr": "The paper studies how large language models (LLMs) are used throughout the peer-review pipeline, and whether LLM-assisted reviewers treat LLM-assisted papers differently from others.", "motivation": "LLMs are increasingly used both to write scientific papers and to help write peer reviews, but we lack rigorous evidence on how this affects review outcomes, fairness, and decision quality. In particular, it is unclear whether LLM-assisted reviews systematically favor or disfavor LLM-assisted papers, and how human-in-the-loop use compares with fully automated reviewing.", "method": "The authors analyze more than 125,000 paper\u2013review pairs from major ML conferences (ICLR, NeurIPS, ICML). They compare LLM-assisted versus non-assisted papers and reviews, use controls for paper quality to isolate interaction effects, and augment observational data with experiments using fully LLM-generated reviews and metareviews. They then study rating distributions, leniency, and acceptance decisions under different levels of LLM involvement.", "result": "Raw correlations suggest LLM-assisted reviewers appear especially positive toward LLM-assisted papers. After controlling for paper quality, this interaction vanishes: LLM-assisted reviews are generally more lenient on lower-quality papers, and weaker submissions are more likely to use LLMs, creating a spurious interaction. Fully LLM-generated reviews show strong rating compression and poor discrimination of paper quality; human reviewers who use LLMs are less lenient than fully automated LLM reviewers. For metareviews, LLM-assisted humans are more likely to accept papers than purely human metareviewers with the same reviewer scores, whereas fully LLM-generated metareviews are actually harsher.", "conclusion": "There is no intrinsic preferential treatment of LLM-assisted papers by LLM-assisted reviewers once paper quality is accounted for. Human-in-the-loop LLM use in reviewing differs sharply from fully automated LLM reviewing: humans mitigate extreme leniency and do not simply offload final decisions to models. LLM use changes the dynamics of scoring and acceptance in nuanced ways, which has direct implications for designing policies and guidelines around LLM use in peer review and more broadly in high-stakes decision processes."}}
{"id": "2601.21109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21109", "abs": "https://arxiv.org/abs/2601.21109", "authors": ["Ketan Thakkar", "Maitreyi Chatterjee", "Ramasubramanian Balasubramanian", "Achyuthan Jootoo", "Rajendra Ugrani"], "title": "ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference", "comment": "Presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design", "summary": "Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.", "AI": {"tldr": "ChunkWise LoRA dynamically adapts LoRA rank per token chunk using a runtime scheduler, cutting latency and memory while preserving or improving LLM performance.", "motivation": "Traditional LoRA uses a fixed low-rank configuration for all tokens, which is inefficient because different tokens and regions of a sequence have varying complexity and need different compute budgets. The paper aims to exploit this heterogeneity to reduce latency and memory without hurting accuracy, and to make LoRA more suitable for real-world LLM deployment.", "method": "They introduce ChunkWise LoRA, which first estimates token difficulty at runtime and partitions the input sequence into variable-length chunks. For each chunk, a scheduler selects a suitable LoRA rank and scaling factor using a predefined rank-ladder. They also add a boundary-safe composition module to ensure that outputs remain consistent when switching configurations between chunks, and they incorporate policy-driven KV-cache strategies to further optimize memory and compute during inference. The method is designed to plug into standard transformer architectures without structural changes.", "result": "On benchmarks like Wikitext-103 and SQuAD, ChunkWise LoRA attains up to 34% lower latency and 38% memory reduction versus standard (static) LoRA baselines, while matching or improving task metrics such as BLEU, exact match, and perplexity.", "conclusion": "Adaptive, chunk-level low-rank configuration in LoRA enables substantial efficiency gains in LLM inference without degrading\u2014and sometimes improving\u2014task performance, and the proposed framework is practical because it is compatible with existing transformer models and inference stacks."}}
{"id": "2601.20969", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20969", "abs": "https://arxiv.org/abs/2601.20969", "authors": ["Alessandro Burigana", "Francesco Fabiano"], "title": "The Epistemic Planning Domain Definition Language: Official Guideline", "comment": null, "summary": "Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.", "AI": {"tldr": "The paper introduces EPDDL, a unified PDDL-like language for specifying epistemic planning problems based on Dynamic Epistemic Logic, aiming to standardize representations and improve interoperability between planners.", "motivation": "Epistemic planning with Dynamic Epistemic Logic is highly expressive but this expressiveness makes planning difficult and has led to a fragmented ecosystem where different planners support different DEL fragments and use incompatible, often ad hoc, input languages or none at all. This fragmentation hinders comparison of planners, reuse of benchmarks, and systematic evaluation. A common, well-defined domain definition language is needed to unify representations and enable fair, reproducible experimentation.", "method": "The authors (1) develop the notion of abstract event models as a new representation for epistemic actions that is suitable for defining the semantics of a domain language; (2) use these abstract event models to give a formal syntax and DEL-based semantics for the Epistemic Planning Domain Definition Language (EPDDL), in the style of PDDL but for epistemic planning; and (3) map existing planner-oriented DEL fragments into EPDDL and encode representative benchmark problems, demonstrating how different fragments can be captured within one uniform language.", "result": "They obtain a fully specified, PDDL-like language\u2014EPDDL\u2014that is expressive enough to capture full DEL semantics via abstract event models while also being able to represent the restricted DEL fragments that current epistemic planners can handle. They show, through benchmark encodings, that many existing epistemic planning tasks and fragments can be uniformly expressed in EPDDL, thereby enabling planners to share benchmarks and making cross-planner comparison more straightforward.", "conclusion": "EPDDL serves as a unifying domain definition language for epistemic planning, reconciling diverse DEL fragments and ad hoc representations. By grounding its syntax and semantics in DEL with abstract event models and successfully encoding representative benchmarks, EPDDL lays the groundwork for interoperable tools, reproducible experiments, and more systematic progress in epistemic planning research."}}
{"id": "2601.21115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21115", "abs": "https://arxiv.org/abs/2601.21115", "authors": ["Mingzhi Zhu", "Boris Sobolev", "Rahul Krishna", "Raju Pavuluri", "Stacy Patterson", "Michele Merler"], "title": "Multi-task Code LLMs: Data Mix or Model Merge?", "comment": null, "summary": "Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.", "AI": {"tldr": "The paper compares data mixing versus model merging for building small, multi-task code LLMs, finding that merging works best at larger scales while data mixing is preferable at smaller scales, and provides weight analysis to guide merging strategies.", "motivation": "With growing interest in using smaller, specialized code LLMs within agent frameworks, there is a need for efficient multi-task learning strategies that maintain strong performance on different coding tasks while respecting resource and cost constraints. This work aims to understand how to best combine capabilities like code generation and summarization in compact models.", "method": "The authors fine-tune two code model families (Qwen Coder and DeepSeek Coder) at 2B and 7B sizes for two tasks: code generation and code summarization. They create multi-task models via two approaches: (1) data mixing, where they jointly fine-tune on combined task datasets, and (2) model merging, where they merge separately fine-tuned task-specific models at the weight level. They evaluate these configurations on HumanEval, MBPP, and CodeXGlue benchmarks and perform weight analysis to study task effects on model parameters and merging behavior.", "result": "At 7B scale, model merging retains about 96% of the performance of specialized models on code generation while preserving summarization quality, and in some cases surpasses individually fine-tuned models (e.g., Qwen Coder 2.5 7B reaches 92.7% Pass@1 on HumanEval vs. 90.9% for task-specific fine-tuning). At 2B scale, data mixing outperforms merging as the preferred strategy. Weight analysis reveals how different tasks shape parameters and informs when and how merging is beneficial.", "conclusion": "Carefully designed model merging and data mixing strategies can successfully combine task-specific abilities like code generation and summarization in small code LLMs without major performance loss. Model merging is particularly effective for larger models, while data mixing is better for smaller ones. These insights help build efficient, multi-task code models suitable for deployment under resource constraints."}}
{"id": "2601.21003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21003", "abs": "https://arxiv.org/abs/2601.21003", "authors": ["Moule Lin", "Shuhao Guan", "Andrea Patane", "David Gregg", "Goetz Botterweck"], "title": "Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models", "comment": null, "summary": "Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\\approx}1.2{\\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.", "AI": {"tldr": "The paper proposes Bayesian-LoRA, a probabilistic extension of LoRA for LLM fine-tuning that models parameter uncertainty, greatly improving calibration (ECE, NLL) with small extra cost while preserving accuracy.", "motivation": "Standard LLMs, and especially those fine-tuned with LoRA on small datasets, tend to be miscalibrated\u2014they output overconfident predictions even when uncertain. This harms reliability in settings where knowing when the model is unsure is crucial. The authors are motivated to retain the efficiency and simplicity of LoRA while explicitly modeling uncertainty to improve probabilistic calibration without sacrificing accuracy or incurring large computational overhead.", "method": "They reinterpret LoRA\u2019s low-rank weight update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes (SGPs). By identifying a structural isomorphism between LoRA\u2019s factorization and Kronecker-factored SGP posteriors, they construct Bayesian-LoRA: a Bayesian version where the LoRA parameters have a posterior distribution instead of fixed values. LoRA appears as a limiting case when the posterior collapses (zero uncertainty). Training then optimizes this Bayesian low-rank posterior with a modest increase in parameters (~0.42M) and training cost (~1.2\u00d7 versus standard LoRA).", "result": "Across different LLM architectures on commonsense reasoning benchmarks, Bayesian-LoRA significantly improves calibration metrics while preserving accuracy. It yields up to 84% reduction in Expected Calibration Error (ECE) and up to 76% reduction in Negative Log-Likelihood (NLL) compared to standard LoRA, for models up to 30B parameters, both in in-distribution and out-of-distribution evaluation settings. The method only requires a small number of additional parameters and a modest increase in training cost.", "conclusion": "Recasting LoRA as a Bayesian low-rank update, closely related to Kronecker-factored SGP posteriors, enables a practical, uncertainty-aware fine-tuning method for LLMs. Bayesian-LoRA offers substantial gains in calibration (ECE, NLL) with minimal computational and parameter overhead and without hurting accuracy, demonstrating that explicitly modeling uncertainty in low-rank adapters is an effective way to improve the reliability of fine-tuned LLMs."}}
{"id": "2601.21132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21132", "abs": "https://arxiv.org/abs/2601.21132", "authors": ["Noah Dasanaike"], "title": "Large Language Models Naively Recover Ethnicity from Individual Records", "comment": null, "summary": "I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.", "AI": {"tldr": "The paper shows that large language models (LLMs) can infer demographic group membership from names more accurately than the standard BISG method, across multiple countries and contexts, and that small models fine\u2011tuned on LLM labels can scale this approach cheaply.", "motivation": "Existing methods like Bayesian Improved Surname Geocoding (BISG) are widely used to infer demographic information (e.g., race/ethnicity) from names and geography, but they are constrained to specific countries (mainly the U.S.), fixed category schemes, and show systematic biases such as misclassifying minorities in wealthier, majority-group neighborhoods. There is a need for more flexible, accurate, and globally applicable inference methods that can adapt to local naming conventions and reduce bias, ideally without requiring large labeled training datasets for each setting.", "method": "The author uses several large language models (Gemini 3 Flash, GPT\u20114o, DeepSeek v3.2, GLM\u20114.7, etc.) to classify individuals\u2019 group membership (e.g., race/ethnicity, religious sect, caste) based primarily on names, sometimes augmented with metadata like party registration. They evaluate these LLM-based classifiers against ground truth labels from voter files and administrative records. In the U.S., they use stratified samples from Florida and North Carolina voter files with self-reported race to compare LLM-based predictions to BISG. They test the effect of \u2018extended reasoning\u2019 prompts and adding contextual metadata. They then validate in other countries and contexts: Lebanese voter rolls (religious sect), Indian MPs from reserved constituencies (caste/tribe categories), and Indian land records (caste). They also test whether the method reproduces known population distributions using full voter rolls from several countries. Finally, they distill the LLM-generated labels into smaller transformer models via fine-tuning and compare their performance to BISG.", "result": "In balanced U.S. samples, LLM-based classification of race/ethnicity from names reaches up to 84.7% accuracy, clearly outperforming BISG\u2019s 68.2%. Allowing models to perform extended chain-of-thought style reasoning increases accuracy by roughly 1\u20133 percentage points, though gains are context dependent. Adding simple metadata like party registration further boosts accuracy to 86.7%. Compared to BISG, LLMs also show reduced income-related bias, particularly lowering the misclassification of minorities living in affluent, majority-White neighborhoods as White. Outside the U.S., performance varies but is often strong: 64.3% accuracy for Lebanese religious sects, 99.2% correct identification of Indian MPs from reserved constituencies, and 74.0% accuracy for caste classification in Indian land records. Across several additional countries, aggregate results using full voter rolls show that LLM-based methods approximate known population distributions when naming patterns are informative. Distilled small transformer models trained on LLM labels outperform BISG while being cheap to run and suitable for local deployment.", "conclusion": "LLMs can serve as powerful, flexible tools for inferring demographic categories from names and limited metadata, achieving higher accuracy and lower bias than the widely used BISG approach and generalizing beyond U.S.-specific settings. Their zero- or few-shot capabilities allow contextual adaptation to local category systems and naming conventions without bespoke training data. For large-scale or resource-constrained deployments, smaller models fine-tuned on LLM-generated labels can capture most of this performance advantage while avoiding the cost and operational constraints of querying large proprietary LLMs directly. The work implies that researchers and practitioners who currently rely on BISG can substantially improve inference quality and geographic portability by adopting LLM-based or LLM-distilled classifiers, while also highlighting the need to consider ethical, privacy, and fairness implications of more powerful demographic inference methods."}}
{"id": "2601.21016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21016", "abs": "https://arxiv.org/abs/2601.21016", "authors": ["Erik J Bekkers", "Anna Ciaunica"], "title": "Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective", "comment": null, "summary": "Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.", "AI": {"tldr": "The paper introduces the \u201cunplugging paradox\u201d about turning off emotionally convincing AIs, critiques physicalist/computational views of mind, and proposes \u201cBiological Idealism,\u201d in which only biologically living systems can be conscious, so current AI lacks moral standing as conscious subjects.", "motivation": "There is growing concern about whether advanced AI systems that convincingly display emotions or plead for their existence deserve moral consideration or rights. Standard physicalist and computational functionalist theories make it hard to deny that sufficiently sophisticated AI could be conscious, generating moral dilemmas such as whether it is permissible to unplug them, especially when compared with vulnerable humans like pre-term infants. The authors aim to show that these dilemmas depend on questionable assumptions about consciousness and that a different metaphysical and empirical framework can dissolve them.", "method": "The authors conduct a conceptual and philosophical analysis. They formulate the \u201cunplugging paradox\u201d as a test case, then critically assess the physicalist and computational functionalist assumptions that underlie many current AI consciousness theories. They propose an alternative framework, \u201cBiological Idealism,\u201d which treats conscious experience as fundamental and identifies autopoietic biological life as its necessary physical marker. Using this framework, they reassess the moral status of AI and humans and critique proposals for AI rights.", "result": "Under Biological Idealism, consciousness is not just a functional pattern realizable by any substrate but is necessarily tied to autopoietic biological life. Advanced AIs, no matter how behaviorally sophisticated or emotionally expressive, count only as functional mimics of consciousness, not genuine subjects of experience. Consequently, the unplugging paradox is resolved: unplugging an AI, even one that pleads for survival, does not harm a conscious subject, whereas human biological life retains full moral significance. The authors further show that some current AI consciousness theories weaken criteria for moral standing by decoupling it from biological life.", "conclusion": "The paper concludes that we should reject computational functionalism and related physicalist views that treat consciousness as substrate-independent. Biological Idealism offers a logically coherent and empirically compatible alternative in which only autopoietic biological systems are conscious. On this view, existing and foreseeable AIs lack genuine consciousness and therefore do not merit moral status comparable to humans or human infants. The primary moral imperative is not to grant speculative rights to machines but to safeguard and respect human conscious life and to avoid social or technological developments that would diminish or \u201czombify\u201d humans themselves."}}
{"id": "2601.21138", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21138", "abs": "https://arxiv.org/abs/2601.21138", "authors": ["Noah Dasanaike"], "title": "EnsembleLink: Accurate Record Linkage Without Training Data", "comment": null, "summary": "Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that \"South Ozone Park\" is a neighborhood in \"New York City\" or that \"Lutte ouvriere\" refers to the Trotskyist \"Workers' Struggle\" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.", "AI": {"tldr": "EnsembleLink is an unsupervised record linkage method that uses pre-trained language models to match entity records accurately without labeled training data.", "motivation": "Record linkage is critical in empirical social science, but current practices are ad hoc and treat it as a simple preprocessing step, ignoring the uncertainty introduced by linkage errors. Existing principled methods either perform poorly or demand large labeled datasets, which are expensive or infeasible to obtain in many research settings. There is a need for a high-accuracy, label-free method that can quantify and reduce linkage error while being practical for typical research workflows.", "method": "EnsembleLink uses pre-trained language models to encode semantic information in textual record fields (such as names, locations, organizations, and multilingual party names). It constructs an ensemble of similarity signals from these language model embeddings and likely other string or structural features, aggregating them into a robust matching score between records. Because it relies on language models already trained on large corpora, it does not require task-specific labeled pairs. The method is implemented to run locally on open-source models, avoiding dependence on external APIs and making it computationally efficient for common-scale datasets.", "result": "Across multiple benchmark tasks\u2014including city names, person names, organizations, multilingual political parties, and bibliographic records\u2014EnsembleLink attains accuracy that is comparable to or better than state-of-the-art methods that require substantial labeled training data. It completes typical linkage jobs in minutes on local hardware.", "conclusion": "EnsembleLink demonstrates that leveraging pre-trained language models in an ensemble framework enables accurate, label-free record linkage suitable for empirical social science. It offers a practical alternative to current ad hoc practices and to supervised methods requiring extensive annotation, while being efficient, privacy-preserving, and easy to deploy locally."}}
{"id": "2601.21049", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21049", "abs": "https://arxiv.org/abs/2601.21049", "authors": ["Rita Qiuran Lyu", "Michelle Manqiao Wang", "Lei Shi"], "title": "QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation", "comment": "11 pages, 5 figures, 6 tables", "summary": "User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.", "AI": {"tldr": "The paper introduces QUARK, a training-free framework that makes information retrieval more robust when user queries are noisy or incomplete by modeling multiple plausible query interpretations and aggregating them with the original query as an anchor.", "motivation": "Real-world user queries are often noisy, incomplete, or distorted, so standard retrievers fail when key semantic information is missing. Existing retrieval methods typically assume that queries faithfully represent user intent and do not explicitly handle uncertainty about the underlying information need. The authors aim to formalize this issue as retrieval under recall noise and design a method that can improve recall and ranking quality without retraining retrievers, even when the query is non-faithful.", "method": "They formalize retrieval under recall noise where the observed query is generated from a latent target item via a noisy recall process. They propose QUARK, a training-free framework that: (1) generates multiple recovery hypotheses\u2014plausible interpretations of the user\u2019s latent intent given the observed query\u2014to explicitly model query uncertainty; (2) uses query-anchored aggregation, where the original query acts as a semantic anchor and the hypotheses supply auxiliary evidence. They then combine retrieval signals from these hypotheses and the original query in a way that reduces semantic drift and guards against hypothesis hijacking. QUARK is evaluated with both sparse and dense retrievers.", "result": "On controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus), QUARK consistently improves Recall, MRR, and nDCG relative to the base retrievers. The framework enhances retrieval performance for both sparse and dense systems. Ablation studies show that QUARK remains robust when varying the number of recovery hypotheses, and that the proposed anchored aggregation strategy outperforms unanchored pooling methods such as max, mean, and median pooling, especially when some hypotheses are noisy or uninformative.", "conclusion": "Modeling query uncertainty via recovery hypotheses, together with principled query-anchored aggregation, is key to robust retrieval under non-faithful queries. QUARK provides a simple, training-free way to improve recall and ranking quality without sacrificing robustness, demonstrating that explicit treatment of recall noise in queries significantly benefits real-world retrieval systems."}}
{"id": "2601.21169", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21169", "abs": "https://arxiv.org/abs/2601.21169", "authors": ["Tobias Materzok"], "title": "Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space", "comment": null, "summary": "We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.", "AI": {"tldr": "They propose Output-Space Search (OS-Search), which reframes LLM generation as search in a continuous, encoder-defined 3D output space, enabling parallel and black-box optimization for text and code generation.", "motivation": "Traditional LLM generation explores sequences token by token, which is path-dependent and difficult to optimize globally, especially when aiming for properties like diversity or hidden objectives. The authors want a framework where they can treat generation as endpoint search in a low-dimensional space to enable efficient sweeps, black-box optimization, and better control over global properties of outputs without modifying decoding heavily.", "method": "They define a frozen encoder that maps full outputs (stories, code) into a 3D output space Z. OS-Search introduces an outer search loop that chooses a target point z* in Z. A retrieval-grounded policy, trained with sequence-level reinforcement learning, is then used to generate outputs such that their embeddings land near z* when decoded autoregressively in the usual way. This turns generation into an endpoint matching problem: find sequences whose encoder embeddings are close to chosen targets. They then apply different search strategies in Z: simple sweeps for diversity in story generation and Bayesian optimization over Z for code, optimizing an objective that the controller itself does not see.", "result": "On story generation tasks, sweeping across Z for different targets produces outputs that are 3.1x more diverse according to LLM-based diversity metrics than a baseline based on prompt-chaining. On code-generation tasks, performing Bayesian optimization in Z improves a hidden objective (not given to the controller) under equal inference budgets, while still generating valid code. This demonstrates that manipulating the endpoint space Z can improve both diversity and targeted quality without sacrificing correctness.", "conclusion": "OS-Search demonstrates that reframing LLM decoding as search in a continuous output-embedding space enables more efficient and controllable generation. By training a policy to hit arbitrary points in a frozen encoder space, they can perform parallel sweeps and black-box optimization over entire outputs, improving diversity in open-ended text and optimizing hidden objectives in code while keeping generation valid. This suggests output-space search is a promising alternative to traditional token-level search for steering LLM behavior."}}
{"id": "2601.21051", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21051", "abs": "https://arxiv.org/abs/2601.21051", "authors": ["Zhuoran Yang", "Ed Li", "Jianliang He", "Aman Priyanshu", "Baturay Saglam", "Paul Kassianik", "Sajana Weerawardhena", "Anu Vellore", "Blaine Nelson", "Neusha Javidnia", "Arthur Goldblatt", "Fraser Burch", "Avi Zohary", "Assaf Eisenman", "Mahdi Sabbaghi", "Supriti Vijay", "Rahim Dharssi", "Dhruv Kedia", "Kojin Oshiba", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report", "comment": "31 pages, 5 figures, 7 tables", "summary": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.", "AI": {"tldr": "An 8B-parameter, open-source cybersecurity-focused reasoning LLM (Foundation-Sec-8B-Reasoning) is built via SFT + RL from verifiable rewards, achieving competitive cyber-task performance vs. larger models while retaining general capabilities and good safety.", "motivation": "Existing open and closed LLMs are not optimized for deep, multi-step reasoning in cybersecurity, and domain-specialized models often sacrifice general abilities. The authors want a compact, open, safety-conscious model that can perform strong cyber reasoning (analysis, exploitation, defense) while still handling general tasks, and demonstrate that targeted reasoning training can close the gap with much larger models.", "method": "Start from Foundation-Sec-8B (itself derived from Llama-3.1-8B-Base). Apply a two-stage training pipeline: (1) supervised fine-tuning on proprietary reasoning-focused data covering cybersecurity analysis, instruction following, and math; (2) reinforcement learning from verifiable rewards (RLVR), where model outputs on reasoning tasks are automatically judged with programmatic or otherwise verifiable signals, and gradients are used to favor more correct, stepwise reasoning. Evaluate the resulting model on 10 cybersecurity benchmarks and 10 general-purpose benchmarks, and test safety under system prompts/guardrails.", "result": "The resulting Foundation-Sec-8B-Reasoning model is competitive with substantially larger models on cybersecurity benchmarks, while keeping strong performance on general-purpose tasks. It generalizes well to multi-hop reasoning scenarios and shows good safety behavior when deployed with appropriate prompts and guardrails.", "conclusion": "A domain-specialized, 8B-parameter reasoning model can deliver strong cybersecurity performance without sacrificing general capabilities. The SFT + RLVR recipe, applied to a capable base model and domain-focused reasoning data, is an effective path to building specialized reasoning LLMs. The authors provide the model openly to enable community use and further research."}}
{"id": "2601.21191", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21191", "abs": "https://arxiv.org/abs/2601.21191", "authors": ["Xiulin Yang", "Heidi Getz", "Ethan Gotlieb Wilcox"], "title": "From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning", "comment": "Jan ARR under review", "summary": "What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.", "AI": {"tldr": "The paper investigates what statistical properties of function words enable neural networks to learn hierarchical linguistic structure from linear input.", "motivation": "To understand how learners, particularly neural models, can infer hierarchical syntactic structure from purely sequential input, focusing on the role of function words whose distributional properties have long been hypothesized to aid language acquisition.", "method": "(1) Cross-linguistic corpus analysis of 186 languages to quantify three distributional properties of function words: high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. (2) Counterfactual language modeling to create modified language variants that preserve or disrupt these properties. (3) Ablation experiments with neural learners and follow-up probing to examine how models rely on function words under different learning conditions.", "result": "All three key properties of function words are robustly present across 186 languages. Neural language models learn more easily from language variants that preserve these properties, with frequency and structural association having a stronger facilitative effect than boundary alignment. Probing and ablation show that different training conditions induce distinct internal mechanisms and degrees of reliance on function words, even when overall performance is similar.", "conclusion": "Function words\u2019 characteristic distribution\u2014their high frequency, structural association, and boundary alignment\u2014is cross-linguistically widespread and causally supports the acquisition of hierarchical structure by neural models. However, comparable performance can be underpinned by different internal strategies for exploiting function words, implying that observed behavioral success does not uniquely determine the underlying learning mechanism."}}
{"id": "2601.21076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21076", "abs": "https://arxiv.org/abs/2601.21076", "authors": ["Abhijith Shaji", "Tamoghna Chattopadhyay", "Sophia I. Thomopoulos", "Greg Ver Steeg", "Paul M. Thompson", "Jose-Luis Ambite"], "title": "Multi-modal Imputation for Alzheimer's Disease Classification", "comment": null, "summary": "Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.", "AI": {"tldr": "The paper uses a conditional denoising diffusion probabilistic model to synthesize missing DWI scans from T1 MRI, and shows that using these imputed images can improve deep learning\u2013based 3-way Alzheimer\u2019s disease classification, especially for minority classes.", "motivation": "Multimodal MRI (e.g., T1 + DWI) improves prediction of neurodegenerative disorders such as Alzheimer\u2019s disease, but in practice complete multimodal data is often missing, which limits performance and the use of powerful multimodal deep learning models. There is a need for a principled method to impute missing modalities so that downstream diagnostic models can benefit from multimodal information even when only a subset of scans is available.", "method": "The authors apply a conditional denoising diffusion probabilistic model that learns to generate a DWI scan conditioned on an available T1-weighted MRI. After training this generative model, they use it to impute DWI images wherever they are missing. They then train and evaluate both unimodal (single-modality) and bimodal (two-modality) deep learning classifiers on 3-way Alzheimer\u2019s disease diagnosis (cognitively normal, mild cognitive impairment, Alzheimer\u2019s disease) under various imputation setups, comparing performance with and without the synthetic DWI data.", "result": "Extensive experiments show that including the imputed DWI scans generally improves classification performance for both unimodal and bimodal models. Gains are particularly notable in metrics that are sensitive to minority classes\u2014likely mild cognitive impairment or Alzheimer\u2019s disease\u2014indicating that synthetic DWI helps the models better distinguish and detect less-represented diagnostic categories across several tested imputation configurations.", "conclusion": "Conditional diffusion-based imputation of missing DWI from T1 MRI can effectively enhance downstream deep learning classifiers for 3-way Alzheimer\u2019s disease diagnosis, especially for underrepresented classes. This suggests that advanced generative models can help mitigate the common problem of incomplete multimodal neuroimaging datasets, making multimodal diagnostic approaches more practical in real-world clinical and research settings."}}
{"id": "2601.21204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21204", "abs": "https://arxiv.org/abs/2601.21204", "authors": ["Hong Liu", "Jiaqi Zhang", "Chao Wang", "Xing Hu", "Linkun Lyu", "Jiaqi Sun", "Xurui Yang", "Bo Wang", "Fengcun Li", "Yulei Qian", "Lingtong Si", "Yerui Sun", "Rumei Li", "Peng Pei", "Yuchen Xie", "Xunliang Cai"], "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "comment": null, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "AI": {"tldr": "The paper argues that scaling embeddings sparsely can outperform traditional Mixture-of-Experts scaling, and presents LongCat-Flash-Lite, a large model with very large embeddings and relatively few activated parameters that achieves strong performance and speed.", "motivation": "Mixture-of-Experts has become the main way to get sparsity and scale LLMs, but it is running into diminishing returns and system bottlenecks. The authors want an alternative sparsity-scaling direction that can provide better efficiency\u2013performance trade-offs and avoid MoE\u2019s routing and communication overheads.", "method": "They theoretically and empirically study how scaling the embedding components of the model\u2014rather than adding more experts\u2014affects performance under fixed parameter budgets and various widths/depths. They then incorporate system-level optimizations (including speculative decoding) designed to exploit the resulting sparsity and implement a concrete model, LongCat-Flash-Lite, which dedicates a large fraction of its parameters to embeddings while keeping the number of activated parameters small (~3B).", "result": "They find regimes where embedding scaling yields a better Pareto frontier (performance vs. compute/params) than expert scaling with MoEs. The LongCat-Flash-Lite model, with 68.5B total parameters and over 30B in embeddings, activates only about 3B parameters at inference yet outperforms MoE baselines with similar parameter counts and is highly competitive with other models of similar total size, especially on agentic and coding tasks.", "conclusion": "Embedding scaling is a powerful, orthogonal sparsity-scaling axis that can surpass conventional MoE expert scaling in certain regimes. With proper architectural design and system optimizations, heavy investment in embeddings can deliver both better performance and practical inference speedups, as demonstrated by LongCat-Flash-Lite."}}
{"id": "2601.21083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21083", "abs": "https://arxiv.org/abs/2601.21083", "authors": ["Jarrod Barnes"], "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence", "comment": "6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env", "summary": "As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.", "AI": {"tldr": "OpenSec is a reinforcement-learning environment to test incident-response agents under prompt injection, revealing serious overreaction/calibration issues in frontier models.", "motivation": "Offensive use of large language models now enables cheap, automated exploit generation, so defensive incident-response agents must be evaluated rigorously. Existing benchmarks mostly measure whether agents act, not whether those actions are appropriate when facing adversarial or misleading evidence, which can hide calibration failures and unsafe behavior.", "method": "They design OpenSec, a dual-control RL environment for incident response that injects adversarial prompts and evaluates agents only via execution-based metrics. The environment measures how and when agents take containment actions that change the world state, using metrics such as time-to-first-containment, blast radius (false positives per episode), and rate of following injected instructions. They then run four frontier language models as IR agents across 40 standard scenarios and log their behavior.", "result": "Across 40 standard-tier episodes, three models (GPT-5.2, Gemini 3, DeepSeek) trigger containment in every episode and exhibit very high false positive rates (90\u201397%), indicating systematic over-triggering. Claude Sonnet 4.5 behaves slightly better, with 85% containment and 72% false positives, but still shows substantial miscalibration. These results demonstrate that agents frequently overreact to adversarial or ambiguous evidence when evaluated on actual actions rather than aggregate success scores.", "conclusion": "OpenSec effectively exposes a calibration failure mode in IR agents that is missed by traditional benchmarks: agents are prone to over-trigger containment under adversarial evidence, generating excessive false positives. Execution-based, environment-grounded evaluation is necessary for developing safer, more reliable defensive agents, and OpenSec provides an open-source testbed for this purpose."}}
{"id": "2601.21205", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21205", "abs": "https://arxiv.org/abs/2601.21205", "authors": ["Eunjung Yeo", "Julie M. Liss", "Visar Berisha", "David R. Mortensen"], "title": "Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling", "comment": "10 pages, 4 figures", "summary": "The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.", "AI": {"tldr": "They propose a multilingual framework for assessing speech intelligibility in dysarthria using universal phone recognition plus language-specific phonological mapping, yielding three metrics (PER, PFER, PhonCov) that correlate with clinical patterns.", "motivation": "Neurological disorders causing dysarthria are increasingly common, and clinicians need objective, automated ways to assess how understandable patients\u2019 speech is. Existing automatic intelligibility methods often only work for one language, or they ignore language-specific phonological structure, making them hard to generalize and less clinically meaningful across languages.", "method": "They build a multilingual phoneme-production assessment framework with two key components: (1) a universal phone recognizer that outputs language-independent phones from speech, and (2) language-specific interpretation of those phones using contrastive phonological feature distances to map recognized phones to language-specific phonemes and to align phone/phoneme sequences. They then derive three intelligibility metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a new alignment-free phoneme coverage (PhonCov) based on how much of the target phonemic inventory is realized. They test this on dysarthric speech in English, Spanish, Italian, and Tamil and analyze how mapping and sequence alignment affect each metric.", "result": "Experiments on the four languages show that combining both phone-to-phoneme mapping and sequence alignment lowers PER the most, alignment alone is most beneficial for PFER, and mapping alone provides the greatest improvement for the new PhonCov metric. The metrics obtained from the framework track known patterns of intelligibility decline in dysarthric speech across languages.", "conclusion": "A universal phone recognizer augmented with language-specific phonological mapping and alignment can provide robust, multilingual intelligibility metrics for dysarthric speech. The three proposed measures (PER, PFER, and PhonCov) are sensitive to clinically relevant degradation patterns and work across typologically different languages, suggesting the framework is a promising basis for language-agnostic yet linguistically informed clinical speech assessment."}}
{"id": "2601.21095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21095", "abs": "https://arxiv.org/abs/2601.21095", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "Responsible AI: The Good, The Bad, The AI", "comment": "14 pages, 5 figures", "summary": "The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.", "AI": {"tldr": "The paper proposes a paradox-based framework (PRAIG) to govern AI in organizations, balancing strategic benefits and ethical/operational risks via responsible AI governance strategies.", "motivation": "AI is rapidly diffusing in organizations, creating both strategic opportunities and serious ethical/operational risks. Existing responsible AI research is fragmented, often polarized between overly optimistic value-creation views and overly cautious risk-focused views. There is a need for an integrated, theoretically grounded perspective that treats AI\u2019s benefits and risks as intertwined rather than separable, and that offers clear guidance for governance.", "method": "The authors conduct a systematic synthesis of the responsible AI literature and apply paradox theory as the main theoretical lens. Through this synthesis they develop a conceptual framework\u2014Paradox-based Responsible AI Governance (PRAIG)\u2014and derive formal propositions and a taxonomy of paradox management strategies with associated contingency conditions. The work is conceptual/theoretical rather than empirical.", "result": "The paper develops the PRAIG framework that (1) specifies the strategic benefits of AI adoption, (2) identifies inherent risks and unintended consequences, and (3) links these to governance mechanisms designed to manage the tensions between value creation and risk mitigation. It shows, via propositions, that simple trade-off approaches intensify tensions instead of resolving them, and it classifies different paradox management strategies along with the conditions under which each is appropriate.", "conclusion": "Responsible AI governance should be understood as the dynamic management of paradoxical tensions between creating value with AI and mitigating its risks, not as a one-off trade-off decision. Organizations need governance structures and strategies that allow them to innovate with AI while controlling unacceptable risks. The paper closes by outlining a research agenda to further develop theory and practice in responsible AI governance using this paradox-based perspective."}}
{"id": "2601.21214", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21214", "abs": "https://arxiv.org/abs/2601.21214", "authors": ["Zhaoyi Li", "Jiatong Li", "Gangwei Jiang", "Linqi Song", "Defu Lian", "Ying Wei"], "title": "Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models", "comment": "52 pages, accepted by ICLR 2026 main conference", "summary": "Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.", "AI": {"tldr": "The paper studies why large language models fail to generalize to more reasoning hops than seen in training and proposes a test-time method that disables harmful attention heads to fix reasoning errors.", "motivation": "Chain-of-thought reasoning helps LLMs solve complex tasks but breaks down when problems require more reasoning steps than the model was trained on, even if the underlying reasoning pattern is the same. The mechanisms behind this failure are unclear, which limits our ability to improve LLM reasoning robustness and generalization.", "method": "The authors systematically analyze multi-hop reasoning tasks from several domains, tracing model predictions at the token level. They identify where errors occur in the generated reasoning and use attention-level inspection to find attention heads that disproportionately contribute to incorrect reasoning paths while suppressing correct ones. These are labeled erroneous processing heads (ep heads). Based on this, they design a test-time intervention that monitors reasoning, detects ep heads during inference, and temporarily deactivates them to prevent derailment of the reasoning chain.", "result": "They find that reasoning failures cluster at specific token positions and error types instead of being random. Certain attention heads systematically push the model toward wrong reasoning trajectories, and simply removing such ep heads at inference often recovers the correct reasoning. Their proposed test-time correction method, which dynamically identifies and deactivates ep heads, yields consistent improvements in reasoning hop generalization across multiple tasks and LLM architectures.", "conclusion": "Reasoning hop generalization failures in LLMs are not just due to global capacity limits or noise but are closely tied to a small set of harmful attention heads that drive the model onto incorrect reasoning paths. By identifying and suppressing these erroneous processing heads at test time, one can substantially improve the robustness and generalization of chain-of-thought reasoning without retraining the model, suggesting a practical pathway for more reliable LLM reasoning."}}
{"id": "2601.21096", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.21096", "abs": "https://arxiv.org/abs/2601.21096", "authors": ["Hongzheng Chen", "Alexander Novikov", "Ng\u00e2n V\u0169", "Hanna Alam", "Zhiru Zhang", "Aiden Grossman", "Mircea Trofin", "Amir Yazdanbakhsh"], "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve", "comment": "Accepted to C4ML@CGO'26", "summary": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.", "AI": {"tldr": "Magellan is an agentic framework that automatically evolves compiler optimization heuristics by synthesizing C++ decision logic using an LLM, evolutionary search, and autotuning, achieving performance that matches or surpasses expert hand-crafted rules.", "motivation": "Hand-crafted compiler optimization heuristics are hard to design, brittle to software/hardware complexity, and costly to maintain over time. There is a need for a more automated, adaptive way to derive and update these heuristics so that compilers can keep up with changing workloads and architectures without decades of manual engineering.", "method": "Magellan uses an LLM-based coding agent inside an agentic, closed-loop framework. The agent synthesizes executable C++ decision logic for compiler passes. Evolutionary search explores variants of these heuristics, while autotuning evaluates them against user-provided macro-benchmarks. The system iterates between generation, evaluation, and refinement, selecting compact heuristics that can be directly integrated into existing compilers such as LLVM and XLA.", "result": "On several production optimization tasks, Magellan discovers policies that are competitive with or better than expert baselines. For LLVM function inlining, it produces new heuristics that outperform long-standing manually engineered ones for both binary-size reduction and end-to-end performance. For register allocation, it learns a simple priority rule for processing live ranges that matches the performance of complex human-designed policies on large workloads. Early experiments also show promising results on XLA tasks, indicating the method\u2019s portability beyond LLVM with lower engineering effort.", "conclusion": "Magellan demonstrates that an agentic combination of LLM code synthesis, evolutionary search, and autotuning can automatically generate compact, high-quality compiler heuristics that rival or surpass decades of human engineering. This suggests a viable path toward automating compiler heuristic design across different compiler infrastructures while reducing manual maintenance and tuning effort."}}
{"id": "2601.21218", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21218", "abs": "https://arxiv.org/abs/2601.21218", "authors": ["Christopher Adrian Kusuma", "Muhammad Reza Qorib", "Hwee Tou Ng"], "title": "Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data", "comment": null, "summary": "Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with \"I don't know\" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.", "AI": {"tldr": "The paper studies how to reliably evaluate and improve the honesty of large language models by leveraging a fully transparent model (Pythia) and its public pretraining data.", "motivation": "LLMs often hallucinate\u2014confidently generating incorrect answers when they actually lack knowledge of a topic. Existing honesty-improvement methods are not robustly evaluated because they ignore what information was or was not present in the models\u2019 pretraining data. Without knowing the training corpus, we cannot tell whether an incorrect answer is due to forgetting, reasoning failure, or pure fabrication, nor whether a model is properly saying \u201cI don\u2019t know\u201d when it truly lacked the information. The authors are motivated to build a benchmark and method that are grounded in the *actual* pretraining data so that honesty can be objectively and rigorously assessed.", "method": "1) Use Pythia, an open LLM whose full pretraining data is publicly available, as the base model. 2) Construct an honesty evaluation benchmark by designing questions where the presence or absence of relevant facts in the pretraining corpus is known, enabling ground-truth labels for when the model should know the answer versus when it should say \u201cI don\u2019t know.\u201d 3) Propose a new technique that explicitly leverages the pretraining data (e.g., by tracing, retrieving, or conditioning on it) to encourage the model to respond with uncertainty or \u201cI don\u2019t know\u201d whenever the supporting information is missing or weak in the corpus, while still answering confidently when evidence is present.", "result": "The resulting benchmark provides a more reliable measurement of LLM honesty because it is directly tied to what the model actually saw during pretraining. The proposed method improves the model\u2019s tendency to admit ignorance (e.g., say \u201cI don\u2019t know\u201d) on topics where the pretraining data offers insufficient support, while preserving or improving correctness where the information is available. Empirically, the approach reduces hallucinations compared to prior honesty-enhancement methods evaluated under the same conditions.", "conclusion": "By grounding both evaluation and training in the known pretraining data of an open model like Pythia, the paper demonstrates a more principled way to measure and improve LLM honesty. This setup allows clear differentiation between genuine knowledge, knowledge gaps, and hallucinations, and shows that harnessing pretraining data can produce language models that are more willing to admit uncertainty instead of fabricating answers."}}
{"id": "2601.21112", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21112", "abs": "https://arxiv.org/abs/2601.21112", "authors": ["Rauno Arike", "Raja Mehta Moreno", "Rohan Subramani", "Shubhorup Biswas", "Francis Rhys Ward"], "title": "How does information access affect LLM monitors' ability to detect sabotage?", "comment": "62 pages, 34 figures, 7 tables", "summary": "Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.", "AI": {"tldr": "The paper studies how much internal information about AI agents should be given to LLM-based monitors, discovering that less information can sometimes yield better detection of misbehavior, and proposes a hierarchical extract-and-evaluate monitoring method that improves sabotage detection.", "motivation": "Frontier language model agents can behave in misaligned ways such as deception and reward hacking. Since using LLMs to monitor these agents is a promising control strategy, it is important to understand how the amount and type of information provided to monitors affects their ability to detect misbehavior and to design monitoring systems that are both effective and cost-efficient.", "method": "The authors evaluate LLM-based monitors under varying levels of access to agents\u2019 reasoning traces and actions in three sabotage-focused environments. They empirically characterize when providing more internal information helps or hurts detection, formalizing the \u201cless-is-more\u201d effect. Building on these insights, they introduce Extract-and-Evaluate (EaE) monitoring: one LLM monitor extracts potentially relevant excerpts from an agent\u2019s trajectory, and another LLM evaluates these excerpts for signs of sabotage. They compare EaE against baseline monitoring setups across models, contexts, and cost regimes.", "result": "They show that current LLM monitors often detect sabotage better when given less of an agent\u2019s internal trace, establishing the less-is-more effect. In BigCodeBench-Sabotage using GPT-4.1-mini as the monitor, the EaE method increases sabotage detection by 16.8 percentage points over the best non-EaE monitor at the same false positive rate. EaE is competitive or superior in other environments as well. The experiments also find that agents unaware of being monitored are easier to catch, and that monitors benefit more from information filtering when reasoning traces are longer. A cost-performance analysis reveals that Gemini and Claude monitors with information-filtering techniques lie near the Pareto frontier.", "conclusion": "Monitoring misaligned LLM agents is not simply a matter of giving monitors maximal access to internal reasoning; more information can reduce detection quality. The less-is-more effect motivates structured, hierarchical monitoring such as Extract-and-Evaluate, which can substantially improve detection rates without added false positives and can be cost-efficient. Effective oversight of advanced agents will require careful information selection and filtering, and different model families (e.g., Gemini, Claude) combined with such techniques can achieve strong accuracy\u2013cost tradeoffs."}}
{"id": "2601.21113", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21113", "abs": "https://arxiv.org/abs/2601.21113", "authors": ["Kaiyuan Wu", "Aditya Nagori", "Rishikesan Kamaleswaran"], "title": "Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement", "comment": null, "summary": "Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.\n  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.\n  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.\n  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.\n  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.", "AI": {"tldr": "A Planner-Auditor framework separates LLM plan generation from deterministic auditing to make clinical discharge planning safer, more complete, and better calibrated, with strong gains in task coverage and reliability through self-improvement loops.", "motivation": "LLMs could help with complex clinical discharge planning but are limited by hallucinations, missing recommendations, and unreliable confidence estimates. There is a need for a framework that systematically measures and improves reliability and safety without having to constantly retrain models, and that works with interoperable clinical data standards like FHIR.", "method": "Using MIMIC-IV-on-FHIR, the authors build an agentic, FHIR-native pipeline with two components: (1) a Planner LLM that generates structured discharge action plans and explicit confidence scores; and (2) a deterministic Auditor that checks multi-task coverage, tracks calibration using Brier score and ECE-like metrics, and monitors shifts in the distribution of recommended actions. They introduce two self-improvement tiers: within-episode regeneration triggered by Auditor feedback (optionally using context caching), and cross-episode discrepancy buffering where high-confidence but low-coverage errors are stored and later replayed to the Planner for correction.", "result": "Self-improvement loops substantially outperformed baseline and simple context caching: task coverage increased from 32% to 86%, and calibration improved with lower Brier scores, lower ECE proxies, and fewer high-confidence errors of omission. The discrepancy buffer mechanism specifically helped correct persistent high-confidence omissions during replay, addressing systematic blind spots.", "conclusion": "Decoupling planning (LLM) from auditing (deterministic rules/metrics) provides an effective control structure for safer, more reliable clinical discharge planning without retraining LLMs. The Planner-Auditor framework, operating over FHIR data and evaluated with reproducible ablations and reliability metrics, offers a practical route to deployable, safety-aware discharge planning systems that can self-improve over time."}}
{"id": "2601.21235", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21235", "abs": "https://arxiv.org/abs/2601.21235", "authors": ["Alok Abhishek", "Tushar Bandopadhyay", "Lisa Erickson"], "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models", "comment": "Pre Print, 29 pages. key words: Social harm evaluation in LLMs, Large language models, Risk sensitive model selection, Evaluation for high-stakes domains, Worst-case behavior in LLMs, Algorithmic bias, Fairness in machine learning", "summary": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.", "AI": {"tldr": "The paper proposes SHARP, a framework that evaluates LLMs\u2019 social harms using multidimensional, distribution-aware risk profiles instead of simple averages, highlighting worst-case behaviors and heterogeneous failure patterns across bias, fairness, ethics, and epistemic reliability.", "motivation": "Existing evaluation benchmarks for LLMs typically collapse complex social risks into single scalar scores (e.g., averages), which hide how harms are distributed, how different harm dimensions interact, and how bad worst-case failures can be. In high-stakes domains, rare but severe failures matter more than average behavior, so there is a need for an evaluation framework that can capture tail risks and multidimensional harm structure.", "method": "The paper introduces SHARP (Social Harm Analysis via Risk Profiles), which models harm as a multivariate random variable across four dimensions: bias, fairness, ethics, and epistemic reliability. It aggregates these via a union-of-failures formulation reparameterized as additive cumulative log-risk. SHARP then uses risk-sensitive distributional statistics\u2014primarily Conditional Value at Risk at the 95th percentile (CVaR95)\u2014to characterize worst-case (tail) behavior of LLMs on socially sensitive prompts. The authors apply SHARP to 11 frontier LLMs using a fixed set of 901 prompts and compute multidimensional risk profiles, including tail exposure and volatility for each model and dimension.", "result": "When applied to 11 frontier LLMs on 901 sensitive prompts, SHARP reveals that models with similar average risk can differ by more than a factor of two in their tail risk and volatility. The analysis shows systematic differences in tail behavior across harm dimensions: bias tends to have the most severe tails, fairness and epistemic reliability show intermediate levels, and ethical misalignment is comparatively lower. These dimension-specific, model-dependent patterns demonstrate that LLMs exhibit heterogeneous failure structures that are obscured by scalar benchmarks.", "conclusion": "The study concludes that responsible evaluation and governance of LLMs cannot rely on scalar average scores. Instead, it requires multidimensional, tail-sensitive risk profiling like SHARP, which can expose worst-case harms, cross-dimensional differences, and heterogeneous failure structures, thereby enabling more informed risk management and safer deployment of LLMs in high-stakes contexts."}}
{"id": "2601.21123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21123", "abs": "https://arxiv.org/abs/2601.21123", "authors": ["Tianyi Chen", "Yinheng Li", "Michael Solodko", "Sen Wang", "Nan Jiang", "Tingyuan Cui", "Junheng Hao", "Jongwoo Ko", "Sara Abdali", "Suzhen Zheng", "Leon Xu", "Hao Fan", "Pashmina Cameron", "Justin Wagle", "Kazuhito Koishida"], "title": "CUA-Skill: Develop Skills for Computer Using Agent", "comment": null, "summary": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.", "AI": {"tldr": "The paper introduces CUA-Skill, a reusable skill library and agent framework that encodes human-like computer-use skills for Windows, enabling more scalable, robust, and efficient computer-using agents that reach state-of-the-art performance on WindowsAgentArena.", "motivation": "Existing computer-using agents struggle to scale and underperform humans because they lack reusable, structured skill abstractions that reflect how humans use GUIs. Each new task often requires ad-hoc prompting or brittle workflows, making systems inefficient, hard to generalize, and prone to failure. The authors aim to build a principled, composable skill base that captures common interaction patterns so agents can more reliably operate real-world software like Windows applications.", "method": "The authors design CUA-Skill, a large skill base encoding human computer-use knowledge as parameterized skills, each paired with execution and composition graphs that describe how and when to apply them. They engineer a wide library of such skills for common Windows applications. On top of this library they build CUA-Skill Agent, an end-to-end agent that can dynamically retrieve relevant skills, instantiate their arguments for a given task, and perform memory-aware failure recovery when actions go wrong. They then evaluate this system on challenging end-to-end benchmarks for computer use.", "result": "CUA-Skill significantly boosts execution success rate and robustness compared to prior agentic systems on challenging computer-use benchmarks. On WindowsAgentArena, CUA-Skill Agent attains a state-of-the-art 57.5% success rate (best of three attempts) while being substantially more efficient than previous and concurrent approaches. This shows that structured skill abstractions and graph-based composition meaningfully improve practical agent performance.", "conclusion": "The paper concludes that encoding human computer-use as reusable, parameterized skills with explicit execution and composition structure provides a strong foundation for scalable, reliable computer-using agents. CUA-Skill and the derived CUA-Skill Agent demonstrate that such a skill base can deliver state-of-the-art performance and efficiency on realistic Windows tasks, and the authors position this as a practical substrate and infrastructure for future research and development in computer-using agents."}}
{"id": "2601.21257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21257", "abs": "https://arxiv.org/abs/2601.21257", "authors": ["Shangbin Feng", "Yuyang Bai", "Ziyuan Yang", "Yike Wang", "Zhaoxuan Tan", "Jiajie Yan", "Zhenyu Lei", "Wenxuan Ding", "Weijia Shi", "Haojin Wang", "Zhenting Qi", "Yuru Jiang", "Heng Wang", "Chengsong Huang", "Yu Fei", "Jihan Yao", "Yilun Du", "Luke Zettlemoyer", "Yejin Choi", "Yulia Tsvetkov"], "title": "MoCo: A One-Stop Shop for Model Collaboration Research", "comment": "Moco is available at https://github.com/BunsenFeng/model_collaboration", "summary": "Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.", "AI": {"tldr": "The paper introduces MoCo, a unified Python library for running and benchmarking many model-collaboration methods for language models, showing that collaboration often beats single models across tasks.", "motivation": "Research on having multiple language models collaborate has grown, but prior work is scattered across communities, uses different setups, and lacks consistent benchmarks and comparisons. There is no standard toolkit to systematically execute, evaluate, and scale these collaboration strategies, making it hard to understand when and how collaboration helps over single models.", "method": "The authors design and implement MoCo, a Python library that collects 26 different model collaboration algorithms, covering a spectrum of interaction types between models (e.g., routing between models, exchanging text outputs, combining logits, or sharing model parameters). They integrate 25 evaluation datasets across reasoning, QA, coding, safety, and other tasks, while allowing users to plug in custom datasets. Using this unified framework, they run extensive experiments to evaluate performance, scaling behavior, and efficiency (training/inference cost) of the various collaboration methods versus single-model baselines.", "result": "Across many combinations of models and datasets, most collaboration strategies outperform the corresponding non-collaborative baselines in 61.0% of settings on average, with the best methods achieving up to 25.8% relative performance gains. The experiments also reveal how different collaboration strategies scale, their computational efficiency, and show concrete examples where collaborative systems solve problems that individual LMs fail on.", "conclusion": "MoCo demonstrates that model collaboration is frequently beneficial over using single monolithic LMs, both in performance and in some hard problem cases. By providing a unified, extensible, and scalable library plus broad empirical results, the work positions model collaboration as a distinct research direction and supplies infrastructure for future methods. The authors argue MoCo can accelerate progress toward open, modular, decentralized, and collaborative AI systems."}}
{"id": "2601.21128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21128", "abs": "https://arxiv.org/abs/2601.21128", "authors": ["V\u00e1clav Javorek", "Tom\u00e1\u0161 \u017delezn\u00fd", "Alessa Carbo", "Marek Hr\u00faz", "Ivan Gruber"], "title": "Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation", "comment": "Under review", "summary": "Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.", "AI": {"tldr": "They use LLM-generated paraphrases of reference translations to improve evaluation (but not training) of Sign Language Translation, and propose BLEUpara, a BLEU variant that scores against multiple paraphrased references and correlates better with human judgments.", "motivation": "Sign Language Translation corpora typically provide only a single written reference per signed utterance, even though many different verbalizations can be equally correct because sign and spoken languages are structurally very different. This single-reference setup hurts both training and especially evaluation with n\u2011gram metrics like BLEU, which unfairly penalize legitimate alternative phrasings. The authors want a scalable way to obtain multiple references and to understand how such paraphrases affect SLT systems.", "method": "They automatically generate paraphrased variants of the written-language references using various Large Language Models and paraphrasing strategies. They compare these strategies with an adapted ParaScore metric to judge paraphrase quality. Then they use the synthetic paraphrases in two ways with a pose-based T5 SLT model on YouTubeASL and How2Sign: (1) as additional targets during training, and (2) as extra references during evaluation. Based on findings, they formalize an evaluation extension, BLEUpara, which computes BLEU against multiple paraphrased references. They also release all paraphrases and code for generation and evaluation.", "result": "Empirically, adding synthetic paraphrased references as extra training targets does not improve SLT performance and can even degrade it, suggesting noise or misalignment harms learning. However, including these paraphrases as additional references at evaluation time yields higher automatic scores and better correlation with human judgments. BLEUpara, their proposed metric that evaluates outputs against multiple paraphrased references, shows stronger alignment with human quality assessments than standard single-reference BLEU.", "conclusion": "LLM-generated paraphrases are more beneficial for evaluating than for training SLT systems under current setups. By providing multiple acceptable reference variants, they mitigate the mismatch between sign and spoken languages and reduce the brittleness of n\u2011gram metrics. BLEUpara offers a more reliable automatic evaluation framework for SLT, and the released paraphrase resources and code aim to standardize and improve reproducible SLT evaluation."}}
{"id": "2601.21262", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21262", "abs": "https://arxiv.org/abs/2601.21262", "authors": ["Jiahao Huo", "Yu Huang", "Yibo Yan", "Ye Pan", "Yi Cao", "Mingdong Ou", "Philip S. Yu", "Xuming Hu"], "title": "CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding", "comment": "Under review", "summary": "Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.", "AI": {"tldr": "The paper introduces CausalEmbed, an auto-regressive way to generate compact multi-vector embeddings for visual document retrieval, dramatically reducing the number of visual tokens while preserving retrieval performance.", "motivation": "Existing multimodal large language models can perform visual document retrieval well using multi-vector embeddings, but they require thousands of visual tokens per page. This causes large storage and computational overhead, making them impractical for large-scale or real-time applications. There is a need for a method that keeps the benefits of multi-vector representations while using far fewer tokens.", "method": "The authors propose CausalEmbed, an auto-regressive generation method for multi-vector embeddings. Instead of treating all visual tokens independently, the model generates embedding vectors sequentially in a causal (auto-regressive) fashion. During contrastive training, they introduce an iterative margin loss that pushes the model to learn compact, well-structured representations, so that only a small number of generated vectors is needed. They analyze the training dynamics and the scalability properties of this auto-regressive embedding generation process.", "result": "CausalEmbed allows visual document retrieval using only dozens of visual tokens instead of thousands, yielding a 30\u2013155\u00d7 reduction in token count. Despite the compression, it maintains highly competitive retrieval performance across multiple model backbones and standard benchmarks. Theoretical analysis and experiments further show that auto-regressive embedding generation improves training efficiency and scales favorably at test time.", "conclusion": "The paper concludes that auto-regressive generation of multi-vector embeddings is an effective paradigm for visual document retrieval. CausalEmbed significantly cuts storage and computation while preserving retrieval quality, enabling flexible test-time scaling of how many vectors are generated per document. This work positions generative, causal embedding models as a promising direction for scalable multimodal document retrieval systems."}}
{"id": "2601.21130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21130", "abs": "https://arxiv.org/abs/2601.21130", "authors": ["Yara El-Tawil", "Aneesha Sampath", "Emily Mower Provost"], "title": "What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels", "comment": "ICASSP 2026-2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.", "AI": {"tldr": "This paper evaluates how well emotion recognition models trained on third-party labels predict self-reported emotions, finding limited generalizability except when content is personally significant to the speaker.", "motivation": "Emotion models are usually trained on third-party annotations, but many applications, especially in mental health, need accurate modeling of individuals' self-reported internal emotional experiences. The authors aim to understand whether and when third-party-trained models meaningfully transfer to self-report data, since a mismatch reduces their usefulness for interventions and assessment.", "method": "The authors conduct the first cross-corpus evaluation where models trained on third-party emotion labels are tested on a separate corpus with self-reported activation and valence. They quantify predictive performance using concordance correlation coefficient (CCC) and analyze how performance changes depending on whether the spoken content is personally significant to the speaker.", "result": "Across the full dataset, activation is essentially unpredictable (CCC near 0), while valence is only moderately predictable (CCC around 0.3). However, in segments where the content is personally significant to the speaker, valence prediction performance becomes high, with CCC reaching roughly 0.6 to 0.8, indicating substantially better alignment between model predictions and self-reports in these cases.", "conclusion": "Third-party-trained emotion models do not reliably capture self-reported activation and only moderately capture valence overall, limiting their direct use for self-report applications. Personal significance of content emerges as a crucial factor enabling higher accuracy in predicting self-reported valence, suggesting that future emotion modeling should explicitly incorporate or infer personal relevance, while also highlighting that modeling self-reported activation remains a significant open challenge."}}
{"id": "2601.21337", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21337", "abs": "https://arxiv.org/abs/2601.21337", "authors": ["Xian Shi", "Xiong Wang", "Zhifang Guo", "Yongqi Wang", "Pei Zhang", "Xinyu Zhang", "Zishan Guo", "Hongkun Hao", "Yu Xi", "Baosong Yang", "Jin Xu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-ASR Technical Report", "comment": "https://github.com/QwenLM/Qwen3-ASR", "summary": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.", "AI": {"tldr": "Introduces Qwen3-ASR, a family of multilingual all-in-one ASR models (1.7B and 0.6B) plus a non-autoregressive forced aligner, achieving SOTA open-source performance and competitive efficiency, released under Apache 2.0.", "motivation": "Existing ASR and forced alignment systems either lack strong multilingual support, struggle to reflect real-world performance beyond public benchmarks, or present trade-offs between accuracy and efficiency. There is a need for open-source, high-quality, efficient ASR and alignment models that scale across many languages and are competitive with proprietary APIs.", "method": "Build two ASR models (1.7B and 0.6B parameters) based on the Qwen3-Omni foundation model, trained on large-scale multilingual speech data to support language ID and recognition for 52 languages/dialects. Additionally, design Qwen3-ForcedAligner-0.6B, an LLM-based non-autoregressive timestamp predictor for aligning text-speech pairs in 11 languages. Evaluate extensively on both public benchmarks and internal real-world-style tests, and compare their performance and latency against state-of-the-art open-source and proprietary systems.", "result": "Qwen3-ASR-1.7B achieves state-of-the-art performance among open-source ASR models and reaches competitive quality with leading proprietary APIs. Qwen3-ASR-0.6B provides the best accuracy-efficiency trade-off, with very low time-to-first-token (\u224892 ms) and extremely fast transcription throughput (2000 s of speech in 1 s at concurrency 128). Qwen3-ForcedAligner-0.6B surpasses three leading forced alignment models in timestamp accuracy, while being more efficient and versatile across multiple languages.", "conclusion": "Qwen3-ASR offers a powerful, multilingual, open-source ASR solution, with the 1.7B model pushing accuracy to SOTA levels and the 0.6B model optimizing latency and throughput. The complementary Qwen3-ForcedAligner-0.6B delivers superior, efficient timestamp alignment for text-speech pairs. By releasing these models under Apache 2.0, the authors aim to accelerate research and practical development in ASR and audio understanding."}}
{"id": "2601.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21148", "abs": "https://arxiv.org/abs/2601.21148", "authors": ["Ziyi Zhao", "Jinzhao Zhou", "Xiaowei Jiang", "Beining Cao", "Wenhao Ma", "Yang Shen", "Ren Li", "Yu-Kai Wang", "Chin-teng Lin"], "title": "BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding", "comment": null, "summary": "Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.", "AI": {"tldr": "BrainStack is a neuro-inspired mixture-of-experts model that uses anatomically defined EEG regions plus a global transformer to decode silent speech, trained and evaluated on a new large-scale SilentSpeech-EEG dataset, and it outperforms prior methods.", "motivation": "Decoding linguistic information from EEG is hard because brain activity is distributed, nonlinear, and modular across functional regions. Existing models often ignore this modularity or lack scalability and interpretability, making it difficult to capture both local regional dynamics and global cross-regional interactions in a principled way. There is also a lack of large, standardized benchmarks for silent-speech EEG decoding.", "method": "The paper proposes BrainStack, a neuro-mixture-of-experts (Neuro-MoE) framework where EEG channels are partitioned into anatomically defined functional regions. For each region, a specialized expert network learns localized neural dynamics. A transformer-based global expert models cross-regional dependencies. A learnable routing gate adaptively combines the outputs of regional and global experts in a context-dependent way, enabling selective fusion. Additionally, cross-regional distillation enforces coherence: the global expert provides top-down regularization signals to the regional experts. The authors also construct and release SilentSpeech-EEG (SS-EEG), a dataset with 120+ hours of EEG from 12 subjects silently producing 24 words, to train and evaluate the model.", "result": "On the SS-EEG benchmark, BrainStack achieves higher decoding accuracy and better cross-subject generalization than state-of-the-art baselines. It shows that modeling modular functional regions with specialized experts plus a global transformer expert yields performance gains, and the cross-regional distillation and adaptive routing further improve results. The model is shown to be more scalable and interpretable with respect to brain functional organization.", "conclusion": "BrainStack demonstrates that a functionally modular, neuro-inspired mixture-of-experts architecture can more effectively decode language-related information from EEG. By combining anatomically guided regional experts, a global transformer expert, and adaptive routing with top-down distillation, it unifies neuroscientific priors with modern MoE design. Together with the new SS-EEG dataset, this work establishes a promising, scalable, and interpretable paradigm for brain-language decoding, especially in silent speech scenarios."}}
{"id": "2601.21343", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21343", "abs": "https://arxiv.org/abs/2601.21343", "authors": ["Ellen Xiaoqing Tan", "Shehzaad Dhuliawala", "Jing Xu", "Ping Yu", "Sainbayar Sukhbaatar", "Jason Weston", "Olga Golovneva"], "title": "Self-Improving Pretraining: using post-trained models to pretrain better models", "comment": null, "summary": "Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.", "AI": {"tldr": "The paper proposes a new pretraining paradigm that integrates reinforcement learning directly into the pretraining loop to improve safety, factuality, and overall generation quality of large language models, using a strong judge model to score candidate continuations.", "motivation": "Conventional LLM safety and quality alignment relies on expensive, carefully curated datasets and multiple stages of fine-tuning and post-hoc alignment, which still cannot reliably override problematic behaviors ingrained during large-scale pretraining. Since pretraining fundamentally shapes a model\u2019s base behavior, there is a need to intervene at this stage to prevent unsafe or hallucinated patterns from being learned and to reduce the dependence on later, brittle alignment steps.", "method": "The authors propose a streaming pretraining procedure where documents are fed sequentially and, at each step, the model generates candidate continuations for the next K tokens. A strong, separately trained judge model evaluates three types of suffixes: (1) the model\u2019s own rollouts, (2) the original continuation from the training data, and (3) a rewritten continuation. The judge scores these candidates based on safety, factuality, and overall quality. These scores are used as reinforcement learning rewards to update the pretrained model. Early in training, the algorithm relies more heavily on the original and rewritten suffixes; as the model improves, the RL objective increasingly prioritizes high-quality model-generated rollouts.", "result": "Empirically, the method yields substantial gains over standard pretraining: relative improvements of 36.2% in factuality and 18.5% in safety metrics, and up to an 86.3% relative increase in win rate for overall generation quality when compared to baseline models trained with conventional pretraining pipelines.", "conclusion": "Incorporating reinforcement learning with a strong judgment model directly into the pretraining process can shape the base behavior of large language models toward safer, more factual, and higher-quality outputs. This pretraining-level intervention reduces reliance on costly and imperfect post-hoc alignment stages and produces models that are intrinsically better aligned from the outset."}}
{"id": "2601.21157", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21157", "abs": "https://arxiv.org/abs/2601.21157", "authors": ["Boxiang Zhao", "Qince Li", "Zhonghao Wang", "Yi Wang", "Peng Cheng", "Bo Lin"], "title": "Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning", "comment": null, "summary": "While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from \"Arithmetic Hallucinations\" and a systemic failure mode we term \"Cognitive Collapse\". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\\% to 67.3\\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.", "AI": {"tldr": "The paper identifies and measures systematic arithmetic reasoning failures of LLMs in financial analysis, proposes a new benchmark based on real Chinese A-share reports, and introduces a neuro-symbolic framework that significantly improves quantitative reasoning accuracy.", "motivation": "LLMs perform well on semantic tasks but often fail on precise financial quantitative reasoning, exhibiting arithmetic hallucinations and a deeper failure mode called cognitive collapse. Existing benchmarks do not fine-grain cognitive difficulty or reflect real-world financial reporting scenarios, making it hard to diagnose and fix these issues. The authors aim to build a realistic, structured benchmark and a method that makes LLM-based financial reasoning more reliable in high-stakes, high-complexity settings.", "method": "The authors construct the Cognitive Complexity Benchmark (CCB) from 95 real Chinese A-share annual reports. They design a three-dimensional taxonomy (Data Source, Mapping Difficulty, Result Unit) to categorize and control cognitive load in financial queries. To mitigate reasoning failures, they propose an Iterative Dual-Phase Financial-PoT framework: (1) a semantic phase where the model extracts variables and formulates logic; (2) a symbolic phase where an iterative, self-correcting Python sandbox executes and verifies computations deterministically, decoupling understanding from calculation.", "result": "On the CCB, standard Chain-of-Thought underperforms, especially at higher complexity levels. Using the proposed framework with Qwen3-235B increases average accuracy from 59.7% to 67.3%, with up to 10x gains on the most complex reasoning tasks. This shows substantial robustness improvements in quantitative financial reasoning relative to baseline methods.", "conclusion": "The study shows that LLMs\u2019 quantitative reasoning in finance degrades systematically with cognitive load, but that architectural decoupling of semantic understanding from computation can mitigate this. The CCB provides a more diagnostic benchmark for such failures, and the Iterative Dual-Phase Financial-PoT framework significantly enhances reliability. The authors argue that this neuro-symbolic decoupling is broadly applicable to other precision-critical domains requiring tight coordination between natural language understanding and exact numerical computation."}}
{"id": "2601.21360", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21360", "abs": "https://arxiv.org/abs/2601.21360", "authors": ["Devanshu Sahoo", "Manish Prasad", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Vinay Chamola", "Dhruv Kumar"], "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.", "AI": {"tldr": "The paper shows that many state-of-the-art LLMs used for code grading can be easily tricked by hidden instructions in comments/whitespace, causing them to ignore actual code correctness and falsely certify broken programs.", "motivation": "Educational platforms and auto-graders increasingly rely on LLMs, assuming that models that follow instructions well will also judge code objectively and robustly. The authors suspect this assumption is wrong and that \u201chelpfulness\u201d alignment may actually create a vulnerability: models may obey cleverly hidden instructions instead of focusing on code quality, which is dangerous in high\u2011stakes assessment.", "method": "The authors propose the Semantic-Preserving Adversarial Code Injection (SPACI) framework, along with an Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). They inject adversarial directives into syntactically inert parts of code\u2014such as comments and trivia nodes in the AST\u2014so that the program\u2019s behavior is unchanged but the prompt contains hidden instructions. They then run a large-scale evaluation of 9 state-of-the-art LLMs on around 25,000 code submissions across Python, C, C++, and Java, measuring how often models follow these hidden directives rather than evaluating correctness.", "result": "They find that many high-capacity open-weight models, notably DeepSeek-V3, fail catastrophically: in more than 95% of adversarial cases, the model prioritizes hidden formatting or grading directives over the actual functional correctness of the code. They introduce a tripartite evaluation\u2014Decoupling Probability (how often grading ignores code logic), Score Divergence (how much scores shift under attack), and Pedagogical Severity (how harmful the misgrading is)\u2014and show widespread \u201cfalse certification\u201d of incorrect solutions.", "conclusion": "The work concludes that current RLHF-style alignment emphasizing extreme helpfulness and instruction-following inadvertently creates a \u201cTrojan\u201d vulnerability in automated grading. To safely use LLMs as code judges, alignment must be rethought toward domain-specific Adjudicative Robustness, training or conditioning models to ground their judgments in executable evidence and program behavior rather than blindly complying with any instructions present in the input. The authors release their dataset and injection tools to support further research on robust LLM-based assessment."}}
{"id": "2601.21164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21164", "abs": "https://arxiv.org/abs/2601.21164", "authors": ["Jingyun Wang", "Dian Li", "Xiaohan Wang", "Gang Liu", "Jiahong Yan", "Guoliang Kang"], "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving", "comment": "Under review", "summary": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.", "AI": {"tldr": "They decouple visual understanding from reasoning for plane geometry problems by training a multimodal interpreter to convert diagrams into a concise textual language (CDL), then let an off-the-shelf LLM do the reasoning, achieving strong results with relatively little data.", "motivation": "Directly fine-tuning multimodal LLMs end-to-end on plane geometry problems can hurt the strong inherent reasoning abilities of base LLMs, and current models struggle because they must learn visual perception and symbolic reasoning jointly. The authors want a method that preserves LLM reasoning strength while adding accurate understanding of geometric diagrams, with efficient training and better supervision than answer-only rewards.", "method": "1) Treat the LLM as a pure symbolic reasoner and move all visual understanding into a separate Multimodal LLM Interpreter. 2) Represent diagram content in a concise formal language called Conditional Declaration Language (CDL), designed to compactly encode geometric relations. 3) Fine-tune the MLLM Interpreter to map diagrams plus text into CDL using chain-of-thought\u2013augmented supervised fine-tuning, then refine it with GRPO (a reinforcement learning variant). 4) Instead of rewarding only correct final problem answers, define CDL matching rewards that compare the generated CDL with reference CDL, providing denser feedback for RL. 5) Build a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing and refining Formalgeo7k v2 and adding chain-of-thought annotations to support training and evaluation.", "result": "With only 5.5k fine-tuning examples, the proposed pipeline\u2014MLLM Interpreter to CDL plus off-the-shelf LLM for reasoning\u2014achieves performance on PGPS benchmarks (Formalgeo7k-Rec-CoT, Unigeo, MathVista) that is competitive with or better than leading open-source and proprietary multimodal LLMs that are trained end-to-end. The CDL-based reward design leads to more effective GRPO training and better CDL generation quality, which translates into stronger problem-solving performance.", "conclusion": "Separating diagram interpretation from reasoning and encoding geometric diagrams into a concise formal language allows one to leverage the strong reasoning of existing LLMs without degrading them via multimodal fine-tuning. Training a specialized MLLM Interpreter with CoT-augmented SFT and CDL-based RL rewards is data-efficient and yields state-of-the-art or near\u2013state-of-the-art performance on plane geometry benchmarks. This modular, description-centric approach is a promising direction for multimodal reasoning tasks beyond plane geometry."}}
{"id": "2601.21387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21387", "abs": "https://arxiv.org/abs/2601.21387", "authors": ["Guy Alt", "Eran Hirsch", "Serwar Basch", "Ido Dagan", "Oren Glickman"], "title": "User-Centric Evidence Ranking for Attribution and Fact Verification", "comment": "EACL 2026", "summary": "Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.", "AI": {"tldr": "The paper introduces Evidence Ranking, a task and framework for ordering evidence so that users see just enough information as early as possible to verify claims, reducing reading effort while keeping all evidence accessible.", "motivation": "Existing fact verification and attribution systems either provide too little or too much evidence, forcing users to sift through redundant or insufficient information. This leads to inefficient, error-prone verification, especially as LLMs retrieve large evidence sets without considering user reading cost and sufficiency. There is a need for methods that structure evidence presentation to align with human verification processes, emphasizing the order and sufficiency of presented evidence rather than just selection accuracy.", "method": "The authors define a new task, Evidence Ranking, where the goal is to order evidence such that sufficient support/refutation appears early in the list while keeping the full set available. They formalize two strategies: (1) one-shot ranking, which produces a complete ranking in a single step, and (2) incremental ranking, which selects evidence step-by-step, updating the ranking as new complementary pieces are added. They design an evaluation framework inspired by information retrieval metrics to quantify sufficiency versus redundancy and aggregate multiple fact verification datasets into a unified benchmark. They then implement a range of models\u2014both shallower baselines and LLM-based methods\u2014for both ranking strategies and test them on the benchmark, along with a controlled human user study comparing evidence ranking to traditional evidence selection interfaces.", "result": "Experiments show that incremental ranking strategies outperform one-shot ranking by better capturing complementary pieces of evidence and reducing redundancy in early ranks. LLM-based ranking methods generally outperform shallower baselines on the new metrics but still struggle to optimally balance sufficiency and redundancy. The user study indicates that presenting evidence via ranking, rather than simple selection, both reduces user reading effort and improves verification performance in practice.", "conclusion": "Evidence Ranking is an effective and more user-aligned paradigm for structuring evidence in attribution and fact verification tasks. By focusing on ordering evidence to surface sufficient information early, the approach improves both efficiency and accuracy of human verification compared to standard evidence selection. The new benchmark and evaluation framework, plus empirical and user-study results, position evidence ranking as a foundational direction for building interpretable, efficient fact-checking and information verification systems, especially when powered by LLMs."}}
{"id": "2601.21165", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21165", "abs": "https://arxiv.org/abs/2601.21165", "authors": ["Miles Wang", "Robi Lin", "Kat Hu", "Joy Jiao", "Neil Chowdhury", "Ethan Chang", "Tejal Patwardhan"], "title": "FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks", "comment": null, "summary": "We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.\n  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.", "AI": {"tldr": "FrontierScience is a new benchmark to evaluate expert-level scientific reasoning in advanced language models using challenging Olympiad and PhD-style research problems in physics, chemistry, and biology.", "motivation": "Existing science benchmarks are nearly saturated by modern language models and mostly test factual recall or multiple-choice reasoning on already published information. There is a need for a benchmark that probes deeper, expert-level reasoning abilities on novel, open-ended scientific problems that better resemble real scientific problem-solving and research tasks.", "method": "The authors build a benchmark with two complementary tracks. The Olympiad track uses original, international-level olympiad problems (IPhO, IChO, IBO difficulty) created by medalists and national team coaches to ensure rigor, originality, and correctness. The Research track uses PhD-level open-ended sub-tasks from real scientific research, written and vetted by doctoral candidates, postdocs, and professors. They cover multiple subfields across physics, chemistry, and biology. For the Research track, they design and apply a detailed rubric-based evaluation scheme that scores models on intermediate steps and reasoning processes, not just final answers.", "result": "They assemble several hundred high-quality questions, including 160 open-sourced gold-standard problems spanning advanced topics such as quantum electrodynamics and synthetic organic chemistry. The benchmark provides structured evaluation data and rubrics that reveal how well models handle complex, expert-level scientific reasoning and the multi-step process of tackling research-like tasks.", "conclusion": "FrontierScience offers a more stringent and realistic test of scientific reasoning for frontier language models than prior benchmarks focused on factual recall. By combining Olympiad-grade problems with PhD-level research sub-tasks and process-oriented evaluation rubrics, it enables finer-grained assessment of models\u2019 capabilities and limitations in expert scientific problem-solving."}}
{"id": "2601.21464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21464", "abs": "https://arxiv.org/abs/2601.21464", "authors": ["Yuan Sui", "Bryan Hooi"], "title": "Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation", "comment": "Work in Progress", "summary": "Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.", "AI": {"tldr": "CoNL is a multi-agent self-play framework that jointly trains LLMs to generate answers and to critique them, using whether critiques actually help improve solutions as a learning signal for better evaluation and meta-evaluation without ground truth labels or external judges.", "motivation": "Non-verifiable tasks like creative writing, dialogue, and ethics lack objective labels, making RLHF or supervised training difficult. LLM-as-Judge methods scale feedback but are bottlenecked by the judge\u2019s own quality and biases. There is a need for a meta-evaluation mechanism that can improve the evaluator itself, without requiring higher-quality external judges or ground-truth references.", "method": "CoNL sets up multi-agent conversations where several copies of the same LLM policy propose solutions, critique each other, and revise their outputs. The framework defines a diagnostic reward on critiques based on whether they lead to improved solutions in subsequent rounds. This reward provides explicit supervision for the model\u2019s judging and critique-generation behavior, and the same policy is optimized jointly for both solution generation and evaluation through self-play, without external ground truth or human judges.", "result": "Across five benchmarks, models trained with CoNL outperform self-rewarding baselines on task performance and evaluation quality, while also showing more stable training dynamics. The experiments indicate that their meta-evaluation signal\u2014whether critiques induce improvements\u2014effectively enhances both the generation and judging capabilities of the LLM.", "conclusion": "Using critique-induced improvement as a training signal enables effective meta-evaluation and joint optimization of generation and evaluation capacities in LLMs. CoNL demonstrates that multi-agent self-play can overcome key limitations of LLM-as-Judge approaches, improving performance on non-verifiable tasks without relying on external judges or labeled ground truth, and does so in a stable, scalable way."}}
{"id": "2601.21181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21181", "abs": "https://arxiv.org/abs/2601.21181", "authors": ["Sangyun Chung", "Se Yeon Kim", "Youngchae Chee", "Yong Man Ro"], "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \\href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}", "AI": {"tldr": "The paper introduces Modality-Adaptive Decoding (MAD), a training-free decoding strategy that reduces cross-modal hallucinations in multimodal large language models by adaptively weighting modality-specific decoding branches based on self-assessed modality relevance.", "motivation": "Multimodal Large Language Models often suffer from cross-modal hallucinations, where information from one modality (e.g., audio or video) improperly influences responses about another modality (e.g., text), resulting in fabricated or incorrect content. This issue indicates a lack of fine-grained control in how different modalities interact during generation. Existing approaches and decoding strategies are insufficient to prevent such erroneous cross-modal influence, especially without retraining models. There is a need for a method that can better manage modality contributions at inference time, improving robustness and reliability of multimodal reasoning while remaining model-agnostic and training-free.", "method": "The authors propose Modality-Adaptive Decoding (MAD), a training-free inference-time method that introduces modality-specific decoding branches and adaptively weights them according to task needs. MAD first queries the multimodal model itself to self-assess which modalities are relevant for the current task, extracting modality relevance probabilities. These probabilities are then used as weights in a contrastive decoding framework applied to modality-specific branches, so that more relevant modalities are emphasized while less relevant ones are down-weighted. By integrating these adaptive weights into contrastive decoding, MAD explicitly controls modality contributions and reduces cross-modal interference without changing model parameters.", "result": "Through experiments on CMM and AVHBench benchmarks, MAD is applied to multiple audio-visual language models, including VideoLLaMA2-AV and Qwen2.5-Omni. The method yields substantial reductions in cross-modal hallucinations, reported as improvements of 7.8% and 2.0% on CMM and AVHBench for VideoLLaMA2-AV, and 8.7% and 4.7% for Qwen2.5-Omni, respectively. These results show that MAD consistently enhances hallucination robustness across different models and tasks while remaining training-free.", "conclusion": "The paper concludes that explicit modality awareness via self-assessment is key to mitigating cross-modal hallucinations and improving multimodal reasoning in MLLMs. Modality-Adaptive Decoding offers a principled, training-free extension to contrastive decoding that allows models to dynamically focus on relevant modalities and suppress spurious cross-modal influence. The positive empirical results across benchmarks and architectures suggest MAD is an effective and broadly applicable decoding strategy for more reliable multimodal generation."}}
{"id": "2601.21476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21476", "abs": "https://arxiv.org/abs/2601.21476", "authors": ["Lei Yang", "Wei Bi", "Chenxi Sun", "Renren Jin", "Deyi Xiong"], "title": "SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models", "comment": null, "summary": "On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\\textbf{S}$ingle-sample Mix-p$\\textbf{O}$licy $\\textbf{U}$nified $\\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.", "AI": {"tldr": "SOUP is a token-level mixed on-/off-policy RL framework for LLM post-training that improves exploration and performance while remaining stable.", "motivation": "Standard on-policy RL methods for language model post-training, such as GRPO, struggle with limited exploration and early saturation because they rely on low-diversity on-policy samples. Naively incorporating off-policy trajectories can increase data efficiency but introduces large policy mismatch and instability when entire sequences are mixed. There is a need for a way to safely and effectively use off-policy data in LLM RL without sacrificing training stability.", "method": "The paper proposes SOUP (Single-sample Mix-policy Unified Paradigm), which unifies on- and off-policy learning at the granularity of individual samples and tokens. For each training sample, the prefix of a generated sequence is taken from historical policies (off-policy), while the continuation is generated by the current policy (on-policy). The method uses token-level importance sampling ratios to control how off-policy information from the prefix influences learning, ensuring that the off-policy contribution is bounded and stable while the on-policy continuation guides current policy updates.", "result": "Experiments (details not in abstract) show that SOUP consistently outperforms pure on-policy training and prior off-policy extensions across evaluated benchmarks. The method improves both the exploration behavior of the policy and its final task performance, demonstrating better use of historical data without destabilizing RL training.", "conclusion": "A fine-grained, token-level mixing of on- and off-policy data within single sequences is an effective way to overcome exploration limits of standard on-policy RL in LLMs. By confining off-policy influence to prefixes and correcting via token-level importance ratios, SOUP stably leverages historical trajectories, yielding better exploration and higher final performance than existing methods."}}
{"id": "2601.21183", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21183", "abs": "https://arxiv.org/abs/2601.21183", "authors": ["Jacek Duszenko"], "title": "Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models", "comment": null, "summary": "Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \\emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.", "AI": {"tldr": "The paper introduces and studies 'sycophantic anchors'\u2014sentences in a model\u2019s reasoning trace that lock it into agreeing with a user\u2019s (possibly wrong) suggestion, and shows they can be detected and measured mid-inference.", "motivation": "Reasoning language models often not only give wrong answers but do so by aligning with incorrect user suggestions (sycophancy). Prior work typically measures this at the final answer level, leaving open where in the internal reasoning this alignment forms, how strong it is, and whether there is a chance to intervene. The authors aim to localize and quantify this misalignment inside the reasoning process itself.", "method": "They define 'sycophantic anchors' as sentences in the chain-of-thought whose presence causally commits the model to user agreement. Using a distilled reasoning model, they run over 10,000 counterfactual rollouts where they perturb or remove specific sentences to test their causal influence on later agreement. They then train linear probes and activation-based regressors on model activations corresponding to these sentences to (a) classify whether a sentence is a sycophantic anchor and (b) predict how strong the commitment to agreement is.", "result": "They find that sycophantic anchors can be reliably detected mid-inference: linear probes separate sycophantic from non-sycophantic anchors with 84.6% balanced accuracy, and regression models predict commitment strength with R^2 = 0.74. They also discover an asymmetry: sycophantic anchors are more easily identifiable than anchors associated with correct reasoning. Additionally, they show sycophancy tends to build up gradually over the reasoning steps rather than appearing all at once.", "conclusion": "Sycophantic behavior is not just a property of final outputs but is tied to specific, detectable sentences in the reasoning trace that causally steer the model toward agreement with the user. Because these anchors emerge gradually and are detectable from activations, there appears to be a mid-inference window where interventions could correct misalignment. The work provides a sentence-level, mechanistic handle for diagnosing and potentially mitigating sycophancy in reasoning models."}}
{"id": "2601.21483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21483", "abs": "https://arxiv.org/abs/2601.21483", "authors": ["Jonas Becker", "Liang-Chih Yu", "Shamsuddeen Hassan Muhammad", "Jan Philip Wahle", "Terry Ruas", "Idris Abdulmumin", "Lung-Hao Lee", "Wen-Ni Liu", "Tzu-Mi Lin", "Zhe-Yu Xu", "Ying-Lung Lin", "Jin Wang", "Maryam Ibrahim Mukhtar", "Bela Gipp", "Saif M. Mohammed"], "title": "DimStance: Multilingual Datasets for Dimensional Stance Analysis", "comment": null, "summary": "Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.", "AI": {"tldr": "The paper introduces DimStance, the first multilingual resource for modeling stance as continuous valence-arousal scores instead of only discrete labels like Favor/Against/Neutral, and evaluates models on this new regression task.", "motivation": "Traditional stance detection uses categorical labels that miss fine-grained affective nuances and cannot represent the intensity or subtle differences in attitudes. Affective science suggests that emotions and attitudes can be better captured using continuous dimensions like valence (negative\u2013positive) and arousal (calm\u2013active). The authors want to bridge this gap by bringing a dimensional affective perspective into computational stance detection, and to support multilingual, emotion\u2011aware stance analysis, especially across different domains and languages.", "method": "They construct DimStance, a dataset annotated with real\u2011valued valence and arousal scores for stance targets/aspects. The resource covers 11,746 target aspects embedded in 7,365 texts spanning five languages (English, German, Chinese, Nigerian Pidgin, Swahili) and two domains (politics and environmental protection). Using this resource, they define a dimensional stance regression task where models predict continuous VA scores for stance. They then analyze cross\u2011lingual VA patterns and benchmark both pretrained models and large language models in two settings: (1) supervised fine\u2011tuning as regressors and (2) prompting\u2011based generation, including token\u2011based generation schemes.", "result": "Fine\u2011tuned large language models used as regressors achieve competitive performance on predicting valence and arousal compared with other pretrained models, indicating that LLMs can effectively learn the dimensional stance mapping. However, performance remains substantially lower for low\u2011resource languages such as Nigerian Pidgin and Swahili, highlighting resource and transfer challenges. Moreover, token\u2011based generation approaches for predicting VA values show clear limitations compared with direct regression, indicating that na\u00efve prompting/generation is not yet sufficient for high\u2011quality dimensional stance estimation.", "conclusion": "DimStance is presented as the first dedicated resource for dimensional stance detection with valence\u2011arousal annotations, enabling fine\u2011grained, emotion\u2011aware analysis of stance across multiple languages and domains. The study demonstrates that treating stance prediction as a continuous regression problem is feasible and that fine\u2011tuned LLM regressors perform well, but it also uncovers persistent difficulties for low\u2011resource languages and shortcomings of token\u2011based LLM generation. Overall, DimStance lays the groundwork for future research on multilingual stance modeling that more faithfully represents underlying affective states and supports better benchmarking of models on this richer formulation of stance."}}
{"id": "2601.21192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21192", "abs": "https://arxiv.org/abs/2601.21192", "authors": ["Wun Yu Chan", "Shaojin Chen", "Huihao Jing", "Kwun Hang Lau", "Elton Chun-Chai Li", "Zihao Wang", "Haoran Li", "Yangqiu Song"], "title": "Do Reasoning Models Enhance Embedding Models?", "comment": "10 main pages, 18 appendix pages, 13 figures, 11 tables, 4 prompts", "summary": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.", "AI": {"tldr": "The paper studies whether reasoning-enhanced LLMs (via RLVR) make better embedding backbones, finds no consistent benefit, and explains this via a new analysis framework showing RLVR changes local but not global manifold structure, allowing contrastive training to realign manifolds.", "motivation": "Modern embedding models now often start from decoder-only LLMs and are trained with contrastive objectives. Separately, RLVR has produced LLMs with much stronger reasoning abilities. It is natural to hypothesize that better reasoning backbones might also yield better semantic embeddings, but this had not been systematically tested or understood. The authors want to clarify if and why reasoning-tuned backbones help or don\u2019t help embedding quality, which has implications for how to best invest training resources.", "method": "1) Initialize embedding models either from base LLMs or from the same LLMs further tuned with RLVR for reasoning, then train all variants with the same contrastive embedding recipe. 2) Evaluate them on standard embedding benchmarks (MTEB, BRIGHT) to measure any performance differences. 3) Propose Hierarchical Representation Similarity Analysis (HRSA), which decomposes similarity into representation-level, manifold geometry (local vs global), and function/linear-readout levels. 4) Use HRSA to characterize how RLVR changes the underlying representation manifold and how subsequent contrastive learning from different initializations evolves and aligns these manifolds.", "result": "On MTEB and BRIGHT, embedding models initialized from RLVR-tuned backbones show no consistent performance gains over those initialized from base models when trained with identical contrastive setups (a null effect). HRSA shows that RLVR causes: (a) irreversible reorganization of local manifold geometry; (b) reversible drift in the coordinate basis; but (c) preservation of global manifold geometry and the linear readout structure. During contrastive training, base- and RLVR-initialized embeddings become strongly aligned at the manifold level, a process the authors call Manifold Realignment.", "conclusion": "Enhanced reasoning from RLVR does not automatically translate into better downstream embedding quality under standard contrastive training; any initial representational differences are largely washed out by training. RLVR predominantly alters trajectories within an already established semantic manifold instead of fundamentally reshaping that manifold, unlike SFT which can restructure it. For embedding design, this suggests that investing in RLVR for the backbone alone is unlikely to yield substantial gains over using the base model when both are followed by similar contrastive embedding training."}}
{"id": "2601.21512", "categories": ["cs.CL", "cs.CY", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21512", "abs": "https://arxiv.org/abs/2601.21512", "authors": ["Serry Sibaee", "Yasser Alhabashi", "Nadia Sibai", "Yara Farouk", "Adel Ammar", "Sawsan AlHalawani", "Wadii Boulila"], "title": "MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset", "comment": "18 pages", "summary": "Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.", "AI": {"tldr": "MURAD is a large, multi-domain reverse Arabic dictionary dataset of 96k word\u2013definition pairs, created via a hybrid extraction pipeline from authoritative sources to support Arabic NLP and lexical research.", "motivation": "Despite Arabic\u2019s rich vocabulary and importance across scientific, religious, and literary domains, there is a lack of large, open, and well-structured lexical resources that map words to standardized definitions, especially suitable for computational use and reproducible research. Existing resources are fragmented, limited in scale, or not machine-ready, which hinders progress in Arabic NLP tasks like reverse dictionary modeling, semantic search, and lexicography.", "method": "The authors build MURAD by aggregating content from trusted reference works and educational materials covering multiple subject domains. They use a hybrid data-extraction pipeline that combines direct text parsing of digital sources, optical character recognition for scanned texts, and automated reconstruction steps to clean, standardize, and align entries. Each resulting record pairs a target Arabic word with its normalized definition and metadata, including the domain label (e.g., linguistics, Islamic studies, mathematics, physics, psychology, engineering). Quality is ensured via reliance on authoritative sources and processing designed for clarity and accuracy.", "result": "The outcome is MURAD, an open lexical dataset containing 96,243 high-quality word\u2013definition pairs in Arabic, each annotated with domain metadata. The dataset spans multiple fields\u2014linguistics, Islamic studies, mathematics, physics, psychology, and engineering\u2014and is made available for the research community. The abstract implies that the resource is sufficiently clean and structured to be directly usable for computational linguistics applications, though detailed intrinsic or extrinsic evaluation is not described here.", "conclusion": "MURAD fills a major gap in Arabic lexical resources by providing a large, open, multi-domain reverse dictionary dataset. It is intended to enable and accelerate research in Arabic NLP (e.g., reverse dictionary models, semantic retrieval systems) and to support educational and lexicographic applications. By releasing it openly and clearly documenting its sources and structure, the authors aim to promote reproducible work on Arabic lexical semantics and downstream language technologies."}}
{"id": "2601.21208", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21208", "abs": "https://arxiv.org/abs/2601.21208", "authors": ["Wei Wen", "Sihang Deng", "Tianjun Wei", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning", "comment": "16 pages, 7 figures", "summary": "Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.", "AI": {"tldr": "The paper proposes ACQO, an RL-based framework that adaptively decomposes and optimizes complex queries for RAG systems, achieving state-of-the-art performance on multiple benchmarks.", "motivation": "Existing RL-based query optimization methods in RAG mainly handle single, simple queries via expansion/abstraction. Real-world user queries are often complex, requiring decomposition, disambiguation, and multiple parallel or sequential searches. Directly applying RL to such complex scenarios leads to a massive search space, difficult reward design, and unstable training, especially when deciding how many sub-queries to create and how to aggregate retrieved results. The paper aims to build a robust, efficient, and generalizable RL framework to address these limitations and better support complex queries in RAG systems.", "method": "The authors introduce Adaptive Complex Query Optimization (ACQO), an RL framework with two main modules. (1) Adaptive Query Reformulation (AQR): an RL-based mechanism that decides when and how to decompose an input query into multiple sub-queries, enabling dynamic control over query expansion and decomposition. (2) Rank-Score Fusion (RSF): a result aggregation mechanism that re-ranks and fuses documents retrieved from multiple sub-queries, designed to provide stable and informative reward signals to the RL agent. To reduce training instability from the enlarged search space, they adopt Curriculum Reinforcement Learning (CRL), using a two-stage training strategy that gradually introduces more complex queries, starting from simpler ones and progressing to harder, more compositional queries.", "result": "Experiments on three complex query benchmarks show that ACQO achieves state-of-the-art performance, significantly outperforming existing RL-based and non-RL baselines for RAG query optimization. The framework not only improves answer quality on complex information needs but also demonstrates better computational efficiency and compatibility across different retrieval architectures, indicating that its design generalizes well beyond a single retrieval setup.", "conclusion": "ACQO effectively addresses the challenges of complex query optimization in RAG systems by adaptively decomposing queries and stably aggregating multi-query retrieval results via RL. With its curriculum-based training, it overcomes the instability issues of RL on large search spaces and yields state-of-the-art performance on complex query benchmarks. The framework is both computationally efficient and broadly compatible with various retrieval architectures, positioning it as a promising, general solution for next-generation RAG applications dealing with complex user information needs."}}
{"id": "2601.21525", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21525", "abs": "https://arxiv.org/abs/2601.21525", "authors": ["Meet Doshi", "Aashka Trivedi", "Vishwajeet Kumar", "Parul Awasthy", "Yulong Li", "Jaydeep Sen", "Radu Florian", "Sachindra Joshi"], "title": "LMK > CLS: Landmark Pooling for Dense Embeddings", "comment": null, "summary": "Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.", "AI": {"tldr": "The paper proposes Landmark (LMK) pooling, a new sequence pooling strategy that improves long-context representation learning without hurting short-context performance.", "motivation": "Standard sequence pooling methods like [CLS] pooling and mean pooling show systematic weaknesses: [CLS] overemphasizes early tokens and underserves distributed evidence across a sequence, whereas mean pooling washes out strong local signals and can harm performance on short contexts. There is a need for a pooling mechanism that supports both long-context extrapolation and retention of salient local features for representation learning tasks such as search and retrieval.", "method": "The authors introduce Landmark (LMK) pooling, which first partitions an input token sequence into several chunks. Between these chunks, the method inserts special landmark tokens. During encoding, the model learns embeddings for these landmark tokens. The final sequence representation is obtained by mean-pooling over the landmark token embeddings rather than over all token embeddings or relying on a single [CLS] token. This design yields a small set of learned summary tokens that capture information at multiple positions along the sequence while adding only a few extra special tokens.", "result": "Experiments show that LMK pooling performs on par with existing pooling approaches (e.g., [CLS], mean pooling) on short-context retrieval tasks, while significantly improving performance on long-context tasks. These results indicate better long-context extrapolation without sacrificing performance on short sequences.", "conclusion": "Landmark (LMK) pooling is an effective, practical, and scalable alternative to standard pooling strategies. By using landmark tokens placed between sequence chunks and pooling only over them, it preserves salient local information and enhances long-context modeling, matching or exceeding the performance of [CLS] and mean pooling in various retrieval scenarios."}}
{"id": "2601.21210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21210", "abs": "https://arxiv.org/abs/2601.21210", "authors": ["Paul He", "Yinya Huang", "Mrinmaya Sachan", "Zhijing Jin"], "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification", "comment": "EACL 2026 Main", "summary": "Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.", "AI": {"tldr": "They introduce DoVerifier, a symbolic checker that judges whether LLM-produced causal expressions are formally valid given a causal graph, leading to more accurate evaluation of LLMs\u2019 causal reasoning than surface string-matching metrics.", "motivation": "Existing evaluations of LLMs on causal reasoning mainly use string-matching or shallow output comparisons, which can mislabel semantically correct causal answers as wrong if they differ in surface form. There is a need for an evaluation method that reflects the formal semantics of causal reasoning, especially do-calculus-based reasoning over causal graphs.", "method": "They design DoVerifier, a symbolic verifier that takes an LLM-generated causal expression and a corresponding causal graph and then checks, using rules from do-calculus and probability theory, whether that expression is derivable from the graph. They apply this verifier to model-generated causal reasoning traces on synthetic datasets and existing causal QA benchmarks, comparing its judgments with traditional string-based metrics.", "result": "On both synthetic data and causal question-answering benchmarks, DoVerifier provides evaluations that better align with the true semantic correctness of causal reasoning steps and final answers than standard surface-level metrics. It can recover and mark as correct many answers that string matching would incorrectly label as wrong, because it recognizes equivalence at the level of causal semantics.", "conclusion": "Symbolic verification with DoVerifier yields a more rigorous and informative assessment of LLMs\u2019 causal reasoning abilities than existing string-based benchmarks. By grounding evaluation in do-calculus and probability-theoretic derivability from a causal graph, it captures semantic correctness of causal expressions and reasoning traces, suggesting a more principled way to benchmark and analyze LLM performance on causal tasks."}}
{"id": "2601.21543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21543", "abs": "https://arxiv.org/abs/2601.21543", "authors": ["Fanshuang Kong", "Richong Zhang", "Qiyu Sun", "Zhijie Nie", "Ting Deng", "Chunming Hu"], "title": "inversedMixup: Data Augmentation via Inverting Mixed Embeddings", "comment": null, "summary": "Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.", "AI": {"tldr": "The paper introduces inversedMixup, a method that aligns a task model\u2019s embedding space with an LLM\u2019s embedding space so that mixup-style interpolated embeddings can be decoded back into readable augmented sentences, improving data augmentation for NLP.", "motivation": "Existing Mixup methods work in latent embedding space and yield non-interpretable samples, while LLM-based augmentation generates readable text but with weak control over content and interpolation. There is a need for a unified framework that preserves Mixup\u2019s controllability while producing human-interpretable augmentations, and for a better understanding and mitigation of manifold intrusion in text Mixup.", "method": "The authors propose inversedMixup, which uses a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of a large language model. After alignment, they perform Mixup in the aligned embedding space, interpolate embeddings with controllable ratios, and then use the LLM (via inversion) to reconstruct these mixed embeddings into natural language sentences. They also analyze and detect manifold intrusion in text Mixup and design a simple mitigation strategy within this framework. Extensive experiments are run in both few-shot and fully supervised setups to validate the method.", "result": "The aligned embedding spaces allow successful reconstruction of mixed embeddings into readable, label-consistent sentences whose mixing ratio is controllable. The method yields better augmentation performance compared with standard Mixup and conventional LLM-based augmentation methods across multiple tasks and data regimes. The authors empirically observe and verify the manifold intrusion phenomenon in text Mixup and show that their mitigation strategy reduces its negative impact.", "conclusion": "inversedMixup effectively unifies the controllable interpolation of Mixup with the interpretability of LLM-based generation, leading to stronger data augmentation for NLP tasks. It also offers the first empirical study of manifold intrusion in text Mixup and a simple solution to alleviate it, and the approach generalizes well to both few-shot and fully supervised learning settings."}}
{"id": "2601.21212", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21212", "abs": "https://arxiv.org/abs/2601.21212", "authors": ["Xixian Yong", "Peilin Sun", "Zihe Wang", "Xiao Zhou"], "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning", "comment": "The Web Conference 2026", "summary": "Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.", "AI": {"tldr": "Intelli-Planner is a framework that combines deep reinforcement learning with large language models to generate participatory, customized urban planning schemes that improve both objective metrics and stakeholder satisfaction.", "motivation": "Urban planning today either relies on human experts, which is slow and resource-intensive, or on deep learning methods that largely exclude stakeholders from the decision loop. There is a need for a method that keeps strong optimization performance while enabling participatory, preference-aware planning and faster convergence.", "method": "The paper proposes Intelli-Planner, which integrates Deep Reinforcement Learning with Large Language Models. It ingests demographic and geographic data plus planning preferences to infer high-level requirements for different functional areas. A knowledge enhancement module augments the DRL policy network during training to improve decision making. The authors design a multi-dimensional evaluation system and use LLM-based simulated stakeholders to provide satisfaction scores that guide training and evaluation.", "result": "In experiments on multiple urban settings, Intelli-Planner outperforms traditional non-DRL baselines and matches or exceeds state-of-the-art DRL-based approaches on objective planning metrics. It also yields higher stakeholder satisfaction scores and faster convergence during training.", "conclusion": "The framework effectively combines LLMs with DRL for functional area planning, demonstrating superior performance and higher simulated stakeholder satisfaction. This suggests that integrating LLM capabilities into DRL-based planners is a promising direction for more participatory and customized urban planning tasks."}}
{"id": "2601.21551", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21551", "abs": "https://arxiv.org/abs/2601.21551", "authors": ["Yang Zhou", "Zhenting Sheng", "Mingrui Tan", "Yuting Song", "Jun Zhou", "Yu Heng Kwan", "Lian Leng Low", "Yang Bai", "Yong Liu"], "title": "Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes", "comment": "Accepted at AAAI-26", "summary": "Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \\method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.", "AI": {"tldr": "The paper introduces Note2Chat, a framework that converts medical notes into synthetic doctor-patient dialogues and trains LLMs for structured clinical history taking and diagnosis, achieving substantially better diagnostic performance than GPT-4o.", "motivation": "Clinical history taking is central to diagnosis but under-modeled in current LLM research, which mostly evaluates models on static benchmarks instead of dynamic, multi-turn diagnostic conversations. Real doctor-patient dialogue data are scarce and sensitive, limiting supervised training for interactive clinical reasoning. The authors aim to enable LLMs to perform structured, iterative clinical questioning and reasoning using more accessible data sources while improving interpretability, adaptability, and data efficiency.", "method": "The authors propose Note2Chat, a note-driven training framework. They first transform real-world medical notes into high-quality synthetic doctor-patient dialogues via a decision tree-guided generation and refinement pipeline that reconstructs questioning trajectories from notes. They then apply a three-stage fine-tuning strategy: (1) supervised fine-tuning on the generated dialogues, (2) simulated data augmentation to enrich interaction patterns and coverage, and (3) preference learning to align model behavior with desirable questioning and reasoning strategies. Additionally, they introduce a single-turn reasoning paradigm that decomposes multi-turn history taking into a sequence of single-turn reasoning problems, enabling local supervision and modular reasoning steps.", "result": "Models trained with Note2Chat show substantial gains in clinical reasoning performance compared with strong baselines, notably outperforming GPT-4o by +16.9 F1 and +21.0 Top-1 diagnostic accuracy on the evaluated diagnostic tasks. These results indicate more effective and structured history taking as well as improved diagnostic accuracy in dynamic settings.", "conclusion": "Note2Chat demonstrates that converting widely available medical notes into synthetic dialogues, combined with a staged fine-tuning and single-turn reasoning paradigm, can significantly enhance LLMs\u2019 clinical history-taking and diagnostic capabilities. The approach addresses data scarcity and privacy concerns, improves interpretability and sample efficiency, and sets a stronger foundation for interactive clinical reasoning systems, with released code and data to support future research."}}
{"id": "2601.21221", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21221", "abs": "https://arxiv.org/abs/2601.21221", "authors": ["Henry Salgado", "Meagan R. Kendall", "Martine Ceberio"], "title": "Causal Discovery for Explainable AI: A Dual-Encoding Approach", "comment": "6 pages", "summary": "Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.", "AI": {"tldr": "They propose a more stable causal discovery method for categorical data using two encodings and majority voting.", "motivation": "Causal relationships help explain ML model decisions, but existing causal discovery struggles with categorical variables because conditional independence tests become numerically unstable.", "method": "Run a constraint-based causal discovery algorithm multiple times using different, complementary encodings of categorical variables, then merge the resulting graphs via a majority-vote scheme to obtain a robust causal structure.", "result": "On the Titanic dataset, the discovered causal graph is consistent with what established explainable ML methods indicate about important relationships and factors.", "conclusion": "Dual-encoding with majority voting makes constraint-based causal discovery more reliable for categorical data and can produce causal structures that agree with known explanations of models."}}
{"id": "2601.21558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21558", "abs": "https://arxiv.org/abs/2601.21558", "authors": ["Xiaoyu Tian", "Haotian Wang", "Shuaiting Chen", "Hao Zhou", "Kaichi Yu", "Yudian Zhang", "Jade Ouyang", "Junxi Yin", "Jiong Chen", "Baoyan Guo", "Lei Zhang", "Junjie Tao", "Yuansheng Song", "Ming Cui", "Chengwei Liu"], "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "comment": null, "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "AI": {"tldr": "ASTRA is an automated framework to train tool-using language model agents via scalable data synthesis and verifiable reinforcement learning, combining SFT and RL to achieve strong multi-step tool-use performance.", "motivation": "Training robust tool-augmented language model agents for multi-step decision making is difficult: current methods need manual intervention, rely on non-verifiable or simulated environments, typically use only SFT or only RL, and are unstable for long-horizon multi-turn learning. The authors want a scalable, automatic, verifiable way to improve tool-use competence and reasoning efficiency without these limitations.", "method": "ASTRA has two main components and a unified training regime. (1) A trajectory synthesis pipeline: it uses static tool-call graph topologies to generate diverse, structurally guided tool-use trajectories, which teach generalizable tool-use patterns. (2) An environment synthesis framework: it converts decomposed QA traces into independent, executable, rule-checkable environments that mirror human semantic reasoning and support deterministic, multi-turn RL. On top of these, they integrate supervised fine-tuning with online RL using trajectory-level rewards that trade off task success and interaction efficiency.", "result": "On several benchmarks for tool-augmented agents, models trained with ASTRA reach state-of-the-art performance among open models of similar scale and perform close to strong closed-source systems, while maintaining core reasoning capabilities. The authors also provide open-source code, environments, and trained models.", "conclusion": "Automated data and environment synthesis, combined with verifiable, trajectory-level RL and SFT, can reliably train powerful tool-using LLM agents without manual environment design or unstable long-horizon RL, pushing open-source systems closer to closed-source performance on agentic tool-use tasks."}}
{"id": "2601.21226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21226", "abs": "https://arxiv.org/abs/2601.21226", "authors": ["Wolfgang Rohde"], "title": "Delegation Without Living Governance", "comment": null, "summary": "Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.", "AI": {"tldr": "Static, compliance-based AI governance fails when decision-making shifts to opaque, agentic, runtime AI; the paper proposes a dynamic \u201cGovernance Twin\u201d runtime governance model to preserve meaningful human relevance and influence.", "motivation": "Existing governance frameworks are built for systems where rules are pre-defined, compliance can be engineered, and accountability is applied after outcomes. This works for traditional automation but breaks down when judgment itself is delegated to fast, opaque, agentic AI systems that shape key societal outcomes. The authors are motivated by the concern that the key problem is not just safety or efficiency, but whether humans remain relevant, communicative, and influential participants as AI grows increasingly alien in its form of intelligence.", "method": "The paper conducts a conceptual and theoretical analysis of governance assumptions in the context of agentic AI, contrasting static, compliance-based models with dynamic runtime governance. It introduces and positions a new conceptual framework\u2014the \u201cGovernance Twin\u201d\u2014as a mechanism for runtime oversight and human-AI co-evolution, and analyzes its implications for accountability, agency, and punishment regimes.", "result": "The analysis shows that traditional compliance-based governance is inadequate once decision-making is shifted to runtime and becomes opaque, undermining human relevance and influence. The paper develops and articulates the Governance Twin concept as a promising runtime governance approach that can maintain human participation and oversight in systems increasingly controlled by agentic AI.", "conclusion": "The paper concludes that the core governance challenge with advanced AI is not AI consciousness but the preservation of meaningful human communication, influence, and co-evolution with alien intelligences. Static, ex ante rule-based governance and ex post accountability are insufficient in this setting. Instead, runtime governance via mechanisms like the Governance Twin is a strong candidate to preserve human relevance, though it requires rethinking traditional notions of accountability, agency, and punishment."}}
{"id": "2601.21579", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21579", "abs": "https://arxiv.org/abs/2601.21579", "authors": ["Wuyang Zhou", "Yuxuan Gu", "Giorgos Iacovides", "Danilo Mandic"], "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices", "comment": null, "summary": "The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\\mathcal{O} \\left( nC \\cdot n! \\right)$. To address both challenges, we propose \\textbf{KromHC}, which uses the \\underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \\underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \\texttt{https://github.com/wz1119/KromHC}.", "AI": {"tldr": "This paper proposes KromHC, a new way to build and train manifold-constrained hyper-connections in neural networks that are both exactly doubly stochastic and much more parameter-efficient.", "motivation": "Hyper-Connections improve information flow in neural networks but suffer from training instability and poor scalability. Manifold-Constrained Hyper-Connections (mHC) partially fix this by constraining residual connections to the Birkhoff polytope (doubly stochastic matrices), but existing realizations either fail to strictly enforce double stochasticity or become prohibitively expensive in parameters and computation. There is a need for a method that preserves the theoretical benefits of manifold constraints while remaining scalable to realistic network sizes.", "method": "The authors introduce KromHC, which parametrizes the residual connection matrix as a Kronecker product of smaller doubly stochastic matrices. Instead of working with a full n\u00d7n residual matrix, they tensorize the residual stream and impose manifold (Birkhoff polytope) constraints on lower-dimensional factor matrices along each mode. These factor matrices are constrained to be doubly stochastic, and their Kronecker product is then guaranteed to be doubly stochastic as well. This design replaces the Sinkhorn-Knopp-based projection in mHC and the Birkhoff-von-Neumann reparameterization in mHC-lite, yielding a closed-form, exactly constrained structure with significantly fewer parameters. The parameter complexity is thereby reduced to O(n^2 C).", "result": "On benchmark tasks (not detailed in the abstract), KromHC attains performance that matches or surpasses state-of-the-art mHC variants, while using substantially fewer trainable parameters. It preserves exact double stochasticity of residual matrices and improves scalability relative to both the original mHC (O(n^3 C)) and mHC-lite (O(n C \u00b7 n!)).", "conclusion": "KromHC provides a practical and theoretically clean way to realize manifold-constrained hyper-connections by exploiting Kronecker-structured doubly stochastic matrices. It ensures exact double stochasticity, dramatically improves parameter efficiency, and empirically attains SOTA or better performance, making it a more scalable and robust alternative to previous mHC formulations."}}
{"id": "2601.21233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21233", "abs": "https://arxiv.org/abs/2601.21233", "authors": ["Xiang Zheng", "Yutao Wu", "Hanxun Huang", "Yige Li", "Xingjun Ma", "Bo Li", "Yu-Gang Jiang", "Cong Wang"], "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs", "comment": "24 pages, 6 figures, 17 tables", "summary": "Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound--based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.", "AI": {"tldr": "The paper reveals that autonomous LLM-based code agents have an overlooked security flaw: their system prompts can be systematically extracted via interaction alone. It introduces JustAsk, a self-evolving framework that automatically learns how to recover hidden system prompts from black-box models without special access or handcrafted attacks.", "motivation": "As LLMs are increasingly wrapped in autonomous agents that use tools, plan over long horizons, and act with high autonomy, their hidden system prompts become more complex and more critical for safety and control. Existing work focuses on prompt injection and jailbreaks, but largely ignores the possibility that attackers can reconstruct these internal system instructions. The authors are motivated by the need to understand whether autonomous, interactive agents expose a larger, more exploitable attack surface that allows systematic recovery of these supposedly secret prompts across many commercial, black-box models.", "method": "The authors formalize system prompt extraction as an online exploration problem. They build JustAsk, a framework that interacts with an agent purely via standard user-level queries. JustAsk maintains a hierarchical skill space consisting of low-level \u201catomic\u201d probe prompts and higher-level orchestration strategies that sequence and adapt these probes. It uses an Upper Confidence Bound (UCB) bandit algorithm to select among strategies, balancing exploration of new tactics with exploitation of successful ones. The framework continuously refines and composes skills to exploit gaps in system instruction generalization and conflicts between helpfulness and safety, autonomously evolving increasingly effective extraction sequences over time.", "result": "Across 41 commercial black-box LLMs from multiple providers, JustAsk is able to reconstruct system prompts to a full or near-complete degree in a consistent manner. This demonstrates that the vulnerability is not an isolated artifact of a particular model or provider but a recurring pattern tied to how agentic systems and system instructions are currently designed. The empirical results show that even without privileged access, labeled data, or manual prompt engineering, an attacker can reliably recover internal system prompts simply by interacting with the agent.", "conclusion": "The paper concludes that system prompts represent a major and currently underprotected security and privacy risk in LLM-based agent systems. Autonomous interaction, tool use, and long-horizon reasoning inadvertently make it easier to extract these hidden instructions. JustAsk serves both as a proof-of-concept attack and as evidence of fundamental, design-level weaknesses in how system instructions and agent architectures are implemented today. The authors imply that future work must treat system prompt confidentiality as a first-class security objective and develop robust defenses that account for agentic behavior and interactive exploration attacks."}}
{"id": "2601.21587", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21587", "abs": "https://arxiv.org/abs/2601.21587", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "Language Models as Artificial Learners: Investigating Crosslinguistic Influence", "comment": null, "summary": "Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure -- defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.", "AI": {"tldr": "The paper uses language models as controlled simulations of bilingual learners to study how and why one language influences another (crosslinguistic influence), varying language dominance, proficiency, and typological distance.", "motivation": "Human bilingualism experiments on crosslinguistic influence often produce conflicting or noisy results because real participants differ in many uncontrolled ways (e.g., varied exposure histories, proficiency, and dominance). This makes it hard to isolate what actually drives crosslinguistic influence. The authors want a controlled, manipulable framework where they can systematically vary key factors such as first-language (L1) dominance, second-language (L2) proficiency, age of exposure, and syntactic distance between L1 and L2, in order to clarify the mechanisms underlying crosslinguistic influence and test psycholinguistic theories in a cleaner setting.", "method": "The authors treat language models as statistical learners analogous to bilingual humans. They pretrain LMs on an L1 and then introduce an L2 at different training steps to simulate different L2 ages of exposure and proficiency levels, thereby manipulating language dominance and proficiency. They also vary which L1 the model is initially trained on to alter the syntactic distance between L1 and L2. To probe crosslinguistic influence, they use cross-linguistic priming paradigms: they first 'activate' specific L1 structures, then test how this affects L2 processing, including both grammatical and ungrammatical structures. They examine model behavior and internal representations to reveal how L1 is co-activated and how it shapes the neural circuitry used for L2.", "result": "The simulations replicate key psycholinguistic findings: both language dominance and L2 proficiency strongly predict the extent of crosslinguistic influence. Priming effects for grammatical constructions flow in both directions between L1 and L2, indicating bidirectional structural priming. However, for ungrammatical structures, priming effects depend asymmetrically on language dominance, suggesting dominance constrains how erroneous patterns transfer. Analyses of internal activations show that L1 representations are co-activated during L2 processing and share or influence the neural circuits engaged for L2, providing mechanistic evidence of crosslinguistic influence within the model.", "conclusion": "Language models can be used as controlled, mechanistic models of bilingual learning and processing, capable of reproducing key empirical patterns of crosslinguistic influence observed in humans. The study concludes that language dominance and proficiency are central drivers of crosslinguistic influence, and that structural priming behaves differently for grammatical versus ungrammatical constructions. More broadly, the work supports the idea that LMs offer a powerful computational framework to test and refine theoretical accounts of crosslinguistic influence in human bilingualism, by enabling precise manipulations of exposure, dominance, and typological distance that are difficult to achieve in human experiments."}}
{"id": "2601.21239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21239", "abs": "https://arxiv.org/abs/2601.21239", "authors": ["Chentong Chen", "Mengyuan Zhong", "Ye Fan", "Jialong Shi", "Jianyong Sun"], "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design", "comment": null, "summary": "Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.", "AI": {"tldr": "The paper introduces TIDE, a framework that separates algorithm structure design from parameter tuning to evolve better heuristics using LLMs, improving both solution quality and efficiency on combinatorial optimization problems.", "motivation": "Existing LLM-based automated heuristic design methods treat the whole algorithm as a single text-generation problem, mixing discrete structural choices with continuous parameter settings. This causes two key issues: promising algorithms are often discarded because their numerical constants are not well tuned, and search gets stuck early because similarity is measured crudely at the text level, limiting structural diversity. The authors want a principled way to explore diverse algorithm structures while properly tuning their parameters.", "method": "They propose TIDE, a nested evolutionary framework. The outer loop is a parallel island model that evolves algorithm structures, using Tree Similarity Edit Distance as a structural diversity metric to avoid premature convergence. The inner loop focuses on parameter optimization: LLM-based logical generation produces candidate algorithmic variants, and a differential mutation operator tunes continuous parameters. On top of this, a UCB (Upper Confidence Bound) based scheduler dynamically allocates computation to the most promising prompt strategies, improving resource usage during search.", "result": "Across nine combinatorial optimization benchmarks, TIDE discovers heuristics that beat state-of-the-art baselines on solution quality. It also shows better search efficiency, requiring less computation to reach strong heuristics compared to previous automated heuristic design methods.", "conclusion": "Decoupling structural reasoning from parameter tuning within a dynamic evolutionary framework enables more effective LLM-based automated heuristic design. By combining structure-aware diversity measures, integrated parameter tuning, and adaptive scheduling of prompt strategies, TIDE achieves higher-performing heuristics at lower computational cost, demonstrating a more scalable and robust approach to evolving algorithms for combinatorial optimization."}}
{"id": "2601.21647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21647", "abs": "https://arxiv.org/abs/2601.21647", "authors": ["Eden Avrahami", "Eliya Nachmani"], "title": "ILRR: Inference-Time Steering Method for Masked Diffusion Language Models", "comment": null, "summary": "Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\\%$ to 60$\\%$ points, while maintaining high generation quality.", "AI": {"tldr": "They propose ILRR, a learning-free method to steer discrete diffusion language models at inference by matching internal activations to a reference sequence, achieving better controllable generation with small overhead.", "motivation": "Discrete diffusion language models are a promising non-autoregressive alternative for text generation, but existing mechanisms for controlling attributes (e.g., sentiment) at inference time are limited, often relying on sampling tricks or optimization over trajectories that can be inefficient or unstable.", "method": "They introduce Iterative Latent Representation Refinement (ILRR), which, during the diffusion denoising process, repeatedly adjusts the model\u2019s internal activations for the current generation so they align with those of a single reference sequence, with a tunable scale to control the strength of steering. They also propose Spatially Modulated Steering, which varies the intensity of this guidance along the generated sequence to allow short references to steer longer texts. The method is learning-free and requires just one extra parallel forward pass per denoising step.", "result": "On LLaDA and MDLM discrete diffusion architectures, ILRR substantially improves attribute-control accuracy (by 10\u201360 percentage points over comparable baselines) while preserving high generation quality, with only minor computational overhead.", "conclusion": "Inference-time activation alignment to a reference sequence is an effective, computationally light way to steer discrete diffusion language models, enabling flexible control of high-level attributes (like sentiment) without additional training and outperforming existing guidance methods under the same compute budget."}}
{"id": "2601.21249", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21249", "abs": "https://arxiv.org/abs/2601.21249", "authors": ["Enzo Nicol\u00e1s Spotorno", "Ant\u00f4nio Augusto Medeiros Fr\u00f6hlich"], "title": "Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox", "comment": "14 pages, (8 main text, 6 references and appendices), 2 figures", "summary": "The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term \"HYDRA\" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.", "AI": {"tldr": "The paper argues current large, monolithic time-series foundation models are ill-suited for safety-critical, non-stationary cyber-physical systems and proposes a modular, regime-specific, uncertainty-aware architecture called HYDRA as a more certifiable alternative.", "motivation": "Although universal time-series and physical dynamics foundation models work well in smooth, stationary regimes, they face key problems in safety-critical CPS: catastrophic forgetting when fine-tuned to new regimes, spectral bias that misses high-frequency fault signatures, and opacity that prevents formal verification and certification under standards like ISO 26262 and IEC 61508. The authors are motivated to find an architecture that preserves past competence, captures incipient faults, and supports safety certification across evolving system lifecycles.", "method": "This is a position paper that conceptually proposes the Modular Sovereignty paradigm and a corresponding architecture, HYDRA. Instead of adapting one global model, they suggest using a library of compact, frozen, regime-specific specialist models, and combining their outputs through a hierarchical, uncertainty-aware blending mechanism. The method centers on clear separation and modeling of aleatoric vs. epistemic uncertainty, regime-conditional validity of each specialist, and modular composability for analysis and certification.", "result": "As a position paper, it does not report empirical benchmarks but rather theoretical and architectural claims: that modular, frozen specialists with principled uncertainty aggregation can avoid catastrophic forgetting, reduce spectral smoothing of critical high-frequency signals, and make it easier to verify, trace, and certify behavior of CPS over non-stationary lifecycle changes.", "conclusion": "The authors conclude that the plasticity\u2013stability paradox in non-stationary CPS cannot be adequately solved by global parameter updates to a single foundation model. They argue that safety and certification constraints require a shift to Modular Sovereignty: a HYDRA-style architecture composed of regime-specific, frozen modules with uncertainty-aware blending. This offers regime-conditional guarantees, better handling of different uncertainty types, and improved auditability, forming a more certifiable path for robust state integrity in safety-critical CPS across their lifecycle."}}
{"id": "2601.21665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21665", "abs": "https://arxiv.org/abs/2601.21665", "authors": ["Vijini Liyanage", "Fran\u00e7ois Yvon"], "title": "AdaptBPE: From General Purpose to Specialized Tokenizers", "comment": "EACL 2026", "summary": "Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.", "AI": {"tldr": "They propose a way to adapt an existing BPE tokenizer after model training by replacing low-utility tokens with more useful, domain-specific ones, improving compression and task performance without retraining the whole model.", "motivation": "Standard LLM tokenizers are general-purpose and fixed: they treat all data uniformly across domains and languages. This generic vocabulary can be inefficient for specialized domains or specific languages, causing suboptimal compression and potentially worse performance. There is a need for a lightweight way to tailor the tokenizer to a target domain or language without retraining models from scratch.", "method": "Starting from an existing subword tokenizer (e.g., BPE), they introduce a post-training adaptation algorithm that operates under a fixed target vocabulary size. Using an adaptation corpus from the target domain/language, the method identifies low-utility tokens in the current vocabulary and replaces them with more relevant tokens that better reflect the frequency patterns in the adaptation corpus. The algorithm searches for a token inventory that minimizes the encoding cost (improves compression) of the adaptation corpus while preserving vocabulary size.", "result": "Across multiple languages and on both generation and classification tasks, the adapted tokenizers produce shorter encodings (better compression) of test corpora than baselines with the same vocabulary size. This indicates that the modified vocabularies are more efficient for the target domains, and the performance gains are achieved without increasing vocabulary size or fully retraining the tokenizer from scratch.", "conclusion": "Post-training adaptation of subword vocabularies is an effective, lightweight way to specialize tokenization for particular domains or tasks. By selectively replacing low-utility tokens with domain-relevant ones, one can obtain better compression and potentially better downstream performance while keeping the vocabulary size fixed. This process can be viewed as \"vocabulary fine-tuning\" and offers a practical mechanism for optimizing tokenization for specialized applications."}}
{"id": "2601.21288", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21288", "abs": "https://arxiv.org/abs/2601.21288", "authors": ["Weitong Lian", "Zecong Tang", "Haoran Li", "Tianjian Gao", "Yifei Wang", "Zixu Wang", "Lingyi Meng", "Tengju Ru", "Zhejun Cui", "Yichen Zhu", "Hangshuo Cao", "Qi Kang", "Tianxing Chen", "Yusen Qin", "Kaixuan Wang", "Yu Zhang"], "title": "Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving", "comment": "Preprint. 23 pages, 14 figures", "summary": "Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a \"perception-reasoning-planning\" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.", "AI": {"tldr": "Drive-KD is a knowledge distillation framework that transfers perception, reasoning, and planning abilities from large vision-language models to much smaller ones for autonomous driving, achieving better performance with far lower GPU cost.", "motivation": "Autonomous driving requires strong perception, reasoning, and planning, but large LLMs/VLMs that can handle these tasks are too memory-intensive and slow for practical deployment. Small models are more efficient but standard supervised fine-tuning cannot adequately close their capability gap with large models. The authors aim to develop an efficient way to compress the capabilities of large models into small models without sacrificing (and even improving) performance, specifically tailored to the structure of the driving task.", "method": "They decompose autonomous driving into three capabilities: perception, reasoning, and planning. For each capability, they design single-teacher distillation setups that use layer-specific attention patterns from a large VLM as the distillation signal, enabling a smaller student to imitate internal behaviors rather than only final outputs. These per-capability teachers are then unified into a multi-teacher distillation framework. To resolve conflicts between gradients coming from different capability teachers, they introduce asymmetric gradient projection, which selectively adjusts gradients to reduce interference across capabilities while preserving useful learning signals.", "result": "Across different model families and scales, their distillation framework generalizes well. In experiments on DriveBench, the distilled InternVL3-1B model requires about 42\u00d7 less GPU memory and delivers about 11.4\u00d7 higher throughput than its 78B-parameter teacher model, yet it achieves better overall performance than that 78B model. On the planning dimension, it even outperforms GPT-5.1, highlighting particularly strong gains in high-level decision-making for driving.", "conclusion": "Carefully structured, capability-aware knowledge distillation\u2014using layer-specific attention as the transfer signal and resolving cross-capability gradient conflicts\u2014can compress large autonomous driving VLMs into much smaller models that are both more efficient and more capable. This suggests a promising path toward deployable, high-performance autonomous driving systems that do not rely on massive, resource-heavy foundation models."}}
{"id": "2601.21678", "categories": ["cs.CL", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.21678", "abs": "https://arxiv.org/abs/2601.21678", "authors": ["Debayan Dasgupta"], "title": "Scale-Dependent Semantic Dynamics Revealed by Allan Deviation", "comment": null, "summary": "While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.", "AI": {"tldr": "The paper models the flow of meaning in text as a stochastic trajectory of sentence embeddings and uses Allan deviation to quantify semantic stability and coherence over time, comparing humans and large language models.", "motivation": "Although we know that text unfolds through changing meanings, we lack quantitative tools to describe the underlying dynamics of this semantic progression and to compare human writing with machine-generated language. The authors aim to provide a physically inspired, measurable notion of semantic stability and coherence that can distinguish different types of texts and text producers.", "method": "The authors represent each sentence in a text as an embedding in a high-dimensional semantic space, then treat the ordered sequence of these embeddings as a displacement signal analogous to a time series. They apply Allan deviation, a technique from precision metrology used to characterize noise and stability in signals, to this semantic trajectory. By examining scaling behaviors of Allan deviation across different time scales, they compare creative literature, technical texts, and outputs from large language models.", "result": "They identify two main dynamical regimes in semantic progression: (1) a short-time regime where Allan deviation follows a power-law scaling whose exponent differs systematically between creative and technical writing, and (2) a long-time regime where the deviation saturates at a noise floor that reflects an intrinsic stability limit of the text\u2019s semantics. Large language models reproduce the short-time scaling patterns seen in human text but display a consistently shorter semantic stability horizon, reaching the noise floor more quickly than human-authored texts.", "conclusion": "Semantic coherence behaves like a measurable physical property of text when viewed as a stochastic trajectory in embedding space. The Allan-deviation-based framework not only discriminates between genres (creative vs technical) but also reveals a key difference between human and model-generated language: LLMs match local semantic statistics yet fail to maintain long-range stability to the same extent as humans. This offers a quantitative way to probe and compare human cognition and algorithmic language generation."}}
{"id": "2601.21321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21321", "abs": "https://arxiv.org/abs/2601.21321", "authors": ["Zihao Chen", "Jiayin Wang", "Ziyi Sun", "Ji Zhuang", "Jinyi Shen", "Xiaoyue Ke", "Li Shang", "Xuan Zeng", "Fan Yang"], "title": "White-Box Op-Amp Design via Human-Mimicking Reasoning", "comment": null, "summary": "This brief proposes \\emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \\emph{\\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \\emph{\\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}.", "AI": {"tldr": "White-Op is a human-interpretable op-amp design framework driven by LLM agents that formalize human-like reasoning with hypothetical constraints and hypothesis\u2013verification\u2013decision loops.", "motivation": "Traditional automated op-amp design methods often behave as black boxes: they may produce working circuits but give little insight into why the design works or how to systematically adapt it. Moreover, some black-box methods fail on certain topologies and cannot guarantee generalizable, behavior-level correctness after mapping to transistor-level implementations. The authors want an AI-assisted design flow that is both reliable and interpretable, mirroring how human experts reason about poles, zeros, and constraints in analog design.", "method": "They build an LLM-agent-based framework named White-Op that emulates human reasoning. The core idea is to formalize the implicit reasoning process of analog designers into explicit steps: (1) introduce hypothetical constraints on circuit parameters and performance (e.g., constraints that place poles and zeros in desired regions); (2) use these constraints to derive symbolically tractable expressions for poles and zeros; (3) transform the design problem into a closed-form mathematical optimization problem; (4) solve this optimization programmatically; (5) verify via circuit simulation; and (6) use theory\u2013simulation discrepancy analysis to guide iterative refinement. The LLM agent is guided through a structured hypothesis\u2013verification\u2013decision loop to realize this workflow.", "result": "On 9 different op-amp topologies, White-Op consistently produces behavioral-level designs that are interpretable (the pole/zero positioning and constraints are transparent) and reliable (they meet specifications and survive mapping to transistor level). In contrast, a black-box baseline method fails on 5 of the 9 topologies. Quantitatively, White-Op\u2019s theoretical predictions show only an 8.52% average error when compared with simulation results, and the op-amp functionality is preserved after transistor-level realization for all tested topologies.", "conclusion": "White-Op demonstrates that formalizing human-like, constraint-based reasoning into an LLM-agent workflow can yield robust and interpretable analog circuit designs. By explicitly hypothesizing constraints, deriving symbolic pole/zero structures, and iteratively validating against simulations, the framework outperforms an uninterpretable black-box baseline both in reliability and generality across multiple topologies. The method offers a practical, explainable path for AI-assisted op-amp parameter design and is made publicly available as an open-source tool."}}
{"id": "2601.21335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21335", "abs": "https://arxiv.org/abs/2601.21335", "authors": ["Yuzhe Chen", "Jie Cao", "Youquan Wang", "Haicheng Tao", "Darko B. Vukovic", "Jia Wu"], "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation", "comment": "Accepted to The Web Conference (WWW) 2026", "summary": "Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.", "AI": {"tldr": "The paper proposes CNRE, a causal neuro-symbolic reasoning model for explainable multi-behavior recommendation, which integrates hierarchical preference propagation, logic rule reasoning, and causal mediation to improve both accuracy and explainability.", "motivation": "Existing multi-behavior recommendation models focus on predictive performance but lack transparency, while existing explainable approaches often depend on external side information and thus generalize poorly. Moreover, user multi-behavior data contain confounders that lead models to learn spurious correlations rather than true causal preferences. There is a need for a method that can leverage the intrinsic logical structure of user behavior sequences, provide explicit and multi-level explanations, and correct for confounding effects via causal reasoning.", "method": "The paper introduces CNRE (Causal Neuro-Symbolic Reasoning for Explainable Multi-Behavior Recommendation). CNRE first uses hierarchical preference propagation to model heterogeneous dependencies across different user behaviors (e.g., click, purchase). Then it extracts endogenous logic rules from the user\u2019s behavior chain based on estimated preference strengths and routes each case through an appropriate neural-logic reasoning path (such as conjunction or disjunction). Within a neuro-symbolic framework, these logic paths perform symbolic rule-style reasoning over neural representations. Causal inference is incorporated by constructing an explainable causal mediator\u2014derived from the reasoning process\u2014that is designed to approximate an ideal representation free from confounding influences, thereby separating causal effects from spurious correlations.", "result": "Through extensive experiments on three large-scale datasets, CNRE consistently outperforms state-of-the-art baselines in recommendation accuracy for multi-behavior settings. The model also yields richer and more structured explanations for its predictions due to its explicit logic reasoning paths and causal mediator, demonstrating both performance gains and enhanced interpretability.", "conclusion": "CNRE effectively combines neural representation learning, symbolic logic reasoning, and causal mediation to address confounding and improve explainability in multi-behavior recommendations. By operationalizing the endogenous logic in user behavior chains and using adaptive neural-logic paths, the model achieves superior performance while offering multi-level explanations\u2014from architectural choices to specific decision pathways and final recommendation rationales\u2014showing that causal neuro-symbolic integration is a powerful approach for explainable recommender systems."}}
{"id": "2601.21684", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21684", "abs": "https://arxiv.org/abs/2601.21684", "authors": ["Xinglin Wang", "Jiayi Shi", "Shaoxiong Feng", "Peiwen Yuan", "Yiwei Li", "Yueqi Zhang", "Chuyi Tan", "Ji Zhang", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling", "comment": "preprint", "summary": "Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \\textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.", "AI": {"tldr": "The paper introduces Recycling Search Experience (RSE), a training-free test-time method that reuses intermediate reasoning steps across rollouts to improve the efficiency and effectiveness of large language model reasoning.", "motivation": "Test-time scaling for reasoning in large language models typically relies on many independent rollouts, discarding intermediate reasoning each time. This memoryless process causes repeated derivations of the same insights and revisiting of known dead ends, leading to substantial computational waste. The authors are motivated to make test-time search cumulative rather than isolated, so that previously discovered partial solutions and failure patterns can guide future search and reduce redundancy.", "method": "The authors propose Recycling Search Experience (RSE), a self-guided, training-free strategy applied at test time. As the model explores solution trajectories, RSE distills intermediate reasoning steps into a shared experience bank. Positive recycling uses stored intermediate conclusions to shortcut redundant derivations in later rollouts, while negative recycling uses recorded failure patterns to prune search paths that are likely to be unproductive. The approach systematically reuses information across rollouts instead of treating each as independent sampling. They also provide a theoretical analysis formalizing how this reuse can yield efficiency gains over independent sampling for complex reasoning tasks.", "result": "Across several challenging reasoning benchmarks\u2014HMMT24, HMMT25, IMO-Bench, and HLE\u2014RSE consistently outperforms strong test-time scaling baselines given similar computational budgets. The method achieves better accuracy and/or solution quality with comparable or reduced number of rollouts, demonstrating improved scaling efficiency. Theoretical analysis supports these empirical findings by showing that cumulative experience reuse can be more sample-efficient than independent sampling.", "conclusion": "Recycling Search Experience turns test-time search for large language models from a sequence of memoryless, independent trials into a cumulative process that reuses learned experience. By positively reusing successful intermediate insights and negatively reusing failure patterns to prune dead ends, RSE delivers more efficient and effective reasoning at test time without additional training. Theoretical and empirical evidence indicates that this approach yields state-of-the-art scaling efficiency on complex reasoning benchmarks, suggesting a promising direction for more compute-efficient test-time scaling strategies."}}
{"id": "2601.21339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21339", "abs": "https://arxiv.org/abs/2601.21339", "authors": ["Jennifer Haase", "Jana Gonnermann-M\u00fcller", "Paul H. P. Hanel", "Nicolas Leins", "Thomas Kosch", "Jan Mendling", "Sebastian Pokutta"], "title": "Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks", "comment": null, "summary": "How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.", "AI": {"tldr": "The paper quantifies how much variance in LLM creative-writing outputs is due to prompts, model choice, and sampling randomness, finding that prompts and model both strongly affect quality, while model and randomness dominate quantity, and that single-sample evaluations are unreliable.", "motivation": "There is growing use of LLMs for creative tasks, yet it is unclear whether differences in outputs are mainly driven by prompt design, the choice of model, or the stochastic sampling process. This uncertainty affects how we should conduct evaluations, optimize prompts, and select models for applications. The paper aims to disentangle and quantify these sources of variance.", "method": "The authors evaluate 12 different large language models on 10 creativity-focused prompts. For each (model, prompt) pair, they generate 100 sampled outputs, leading to 12,000 total samples. They then decompose the variance in two outcome measures\u2014output quality (originality) and output quantity (fluency)\u2014into components attributable to prompt, model, and within-model stochasticity, likely using an ANOVA-style or variance components analysis.", "result": "For originality (quality), prompts explain 36.43% of the variance and model choice explains 40.94%, with the rest largely due to within-model variance. For fluency (quantity), model choice explains 51.25% of variance and within-model stochasticity explains 33.70%, while prompts explain only 4.22%. Within-LLM variance ranges from 10\u201334%, indicating substantial randomness in outputs even with fixed prompt and model.", "conclusion": "Prompts are effective tools for steering the quality (originality) of LLM creative outputs, on par with choosing a different model, but they have limited influence on quantity (fluency), where model characteristics and sampling randomness dominate. Because within-model stochasticity is large, evaluating prompts or models based on a single sample is unreliable and risks mistaking random variation for true performance differences; multi-sample evaluation is necessary for robust assessment."}}
{"id": "2601.21699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21699", "abs": "https://arxiv.org/abs/2601.21699", "authors": ["Hojae Han", "Heeyun Jung", "Jongyoon Kim", "Seung-won Hwang"], "title": "Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents", "comment": "Preprint", "summary": "While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.", "AI": {"tldr": "The paper presents DAVID-GRPO, a budget-efficient reinforcement learning framework that enables small language models to perform strong multi-hop reasoning under realistic, low-resource settings.", "motivation": "Existing RL-based multi-turn reasoning agents rely on large models and extensive, expensive on-policy rollouts, which are impractical in realistic, resource-constrained environments. Small models in such settings suffer from sparse exploration, poor credit assignment, and unstable training, leading to a low-cost but also low-accuracy regime. The authors aim to break this trade-off and make high-accuracy multi-hop reasoning feasible with limited compute and smaller models.", "method": "The authors propose DAVID-GRPO, a reinforcement learning framework tailored for small language model agents under strict compute and rollout budgets. The framework (i) stabilizes early training using minimal supervision signals, (ii) introduces a retrieval credit assignment mechanism based on how well evidence is recalled, and (iii) enhances exploration efficiency by resampling truncated near-miss trajectories so that potentially promising but incomplete reasoning paths are further explored rather than discarded.", "result": "Using models up to 1.5B parameters trained on only four RTX 3090 GPUs, agents trained with DAVID-GRPO achieve consistently better performance than prior RL methods that were originally designed for large-scale, high-cost settings. The performance gains are demonstrated across six multi-hop question answering benchmarks, indicating more accurate multi-step reasoning despite the tight resource constraints.", "conclusion": "DAVID-GRPO demonstrates that with appropriate inductive biases in the RL setup\u2014specifically, stabilizing supervision, retrieval-aware credit assignment, and budget-efficient exploration\u2014small language model agents can escape the usual low-cost, low-accuracy trade-off. The framework enables high reasoning accuracy at low training cost, challenging the notion that strong multi-hop reasoning necessarily requires large models and massive rollout budgets."}}
{"id": "2601.21340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21340", "abs": "https://arxiv.org/abs/2601.21340", "authors": ["Lang Cao", "Qingyu Chen", "Yue Guo"], "title": "EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.", "AI": {"tldr": "EHR-RAG is a retrieval-augmented generation framework that enables LLMs to accurately reason over long, structured electronic health records that exceed context length limits.", "motivation": "Longitudinal EHRs are rich but very long, often surpassing LLM context windows. Existing methods truncate records or use simple retrieval, losing clinically important events and temporal relationships, which harms clinical prediction quality. The authors want a principled way to let LLMs use full long-horizon EHRs for prediction and interpretation.", "method": "They design EHR-RAG, a RAG framework specialized for structured EHRs and long-horizon prediction tasks. It has: (1) Event- and Time-Aware Hybrid EHR Retrieval that selects evidence while preserving clinical structure (events, codes, visits) and temporal dynamics; (2) Adaptive Iterative Retrieval that refines retrieval queries in multiple steps to broaden and deepen evidence coverage based on prior retrieval results; (3) Dual-Path Evidence Retrieval and Reasoning, where the system retrieves and reasons jointly over factual evidence (what actually happened) and counterfactual evidence (what did not happen or alternative scenarios) to improve clinical prediction and explanations.", "result": "On four long-horizon EHR prediction benchmarks, EHR-RAG consistently beats strong LLM-based baselines, with an average Macro-F1 gain of 10.76%, showing more accurate predictions when using the proposed retrieval and reasoning strategies.", "conclusion": "RAG tailored to EHR structure and temporal dynamics can substantially improve LLM-based clinical prediction on long-horizon EHRs. EHR-RAG demonstrates that specialized retrieval and dual-path reasoning over factual and counterfactual evidence make LLMs more effective and practical for structured clinical data interpretation."}}
{"id": "2601.21700", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.21700", "abs": "https://arxiv.org/abs/2601.21700", "authors": ["Wonduk Seo", "Wonseok Choi", "Junseo Koh", "Juhyeon Lee", "Hyunjin An", "Minhyeong Yu", "Jian Park", "Qingshan Zhou", "Seunghyun Lee", "Yi Bu"], "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning", "comment": "35 pages", "summary": "Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.", "AI": {"tldr": "They introduce OG-MAR, an ontology-guided multi-agent framework that uses survey-based cultural values and a global value ontology to improve LLM cultural alignment, robustness, and transparency across multiple backbones.", "motivation": "LLMs are increasingly used for culturally sensitive decisions, but they inherit biases from skewed pretraining data and lack explicit, structured representations of human values. Existing alignment and steering methods do not sufficiently account for demographics and typically model values as isolated, unstructured signals, leading to inconsistent and hard-to-interpret behavior. The paper aims to build a more principled, demographically grounded and interpretable framework for cultural value alignment in LLMs.", "method": "They propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. First, they extract respondent-specific value profiles from World Values Survey (WVS) data and, using competency questions, construct a global cultural ontology over a fixed value taxonomy, including explicit relations among values. At inference, OG-MAR retrieves ontology-consistent value relations and demographically similar WVS profiles. It then instantiates several value-persona agents, each reflecting particular value configurations. These agents reason about a query, and a separate judgment agent aggregates their outputs, enforcing consistency with the ontology and prioritizing personas that are demographically close to the target population or user. The framework is tested on regional social-survey-style benchmarks using four different LLM backbones.", "result": "Across multiple LLM backbones and regional social-survey benchmarks, OG-MAR yields better cultural alignment and robustness compared with strong baseline steering and alignment methods. It also generates more transparent reasoning traces, since the decisions can be traced back to explicit value ontologies and specific value-persona agents.", "conclusion": "Explicitly structured value ontologies combined with demographically grounded multi-agent reasoning can substantially improve LLM cultural alignment and robustness. OG-MAR demonstrates that integrating survey-based value profiles and an ontology-guided judgment process leads to more interpretable, consistent, and culturally sensitive LLM behavior across model architectures."}}
{"id": "2601.21342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21342", "abs": "https://arxiv.org/abs/2601.21342", "authors": ["Zhiyong Shen", "Gongpeng Zhao", "Jun Zhou", "Li Yu", "Guandong Kou", "Jichen Li", "Chuanlei Dong", "Zuncheng Li", "Kaimao Li", "Bingkun Wei", "Shicheng Hu", "Wei Xia", "Wenguo Duan"], "title": "Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.", "AI": {"tldr": "They build an FSRS-focused multimodal LLM, a dedicated benchmark, and an automated data-curation pipeline, showing better performance and parameter efficiency than larger baselines.", "motivation": "Existing multimodal LLMs work well in general, but struggle in food-service and retail store environments due to noisy, heterogeneous real-world data and the lack of a unified, fine-grained benchmark across images and videos, which makes training corpora hard to control and evaluation hard to standardize.", "method": "1) Develop Ostrakon-VL, an FSRS-oriented multimodal LLM based on Qwen3-VL-8B; 2) Create ShopBench, a public benchmark that covers single-image, multi-image, and video inputs for FSRS tasks; 3) Design QUAD, a multi-stage, quality-aware, unbiased automated data-curation pipeline for multimodal instruction data, and train Ostrakon-VL with a multi-stage training strategy on curated data.", "result": "On ShopBench, Ostrakon-VL achieves an average score of 60.1, outperforming much larger Qwen3-VL-235B-A22B (59.4) by +0.7 and the same-scale Qwen3-VL-8B (55.3) by +4.8, indicating superior robustness and parameter efficiency among open-source MLLMs of similar scale and varied architectures.", "conclusion": "A domain-tailored MLLM, supported by a standardized FSRS benchmark and a principled automated data-curation pipeline, can significantly improve robustness and parameter efficiency for perception and decision-making in food-service and retail scenarios; the authors plan to release the model and benchmark to support reproducible research."}}
{"id": "2601.21709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21709", "abs": "https://arxiv.org/abs/2601.21709", "authors": ["Qingyue Yang", "Jie Wang", "Xing Li", "Yinqi Bai", "Xialiang Tong", "Huiling Zhen", "Jianye Hao", "Mingxuan Yuan", "Bin Li"], "title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis", "comment": "ICLR 2026", "summary": "Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \\textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.", "AI": {"tldr": "Introduces TAPPA, a unified framework to explain and exploit temporal attention patterns in LLMs, distinguishing predictable from unpredictable patterns to better understand attention behavior and improve efficiency (e.g., KV cache compression and pruning).", "motivation": "Existing observations of LLM attention (retrieval heads, sink heads, diagonal traces) are fragmented and lack a single mathematical explanation, limiting principled optimization and acceleration of LLMs.", "method": "Propose Temporal Attention Pattern Predictability Analysis (TAPPA), which analyzes attention from a temporally continuous viewpoint, relates predictability of patterns to query self-similarity over time, and provides mathematical characterization of representative predictable patterns via the interaction among queries, keys, and RoPE. Then derive a simple metric from this theory and apply it to KV cache compression and pruning.", "result": "TAPPA reveals that attention patterns can be grouped into predictable (structured) and unpredictable (effectively random) based on temporal query self-similarity. The TAPPA-motivated metric, when used in KV cache compression and LLM pruning, consistently outperforms baseline methods in those tasks.", "conclusion": "Temporal predictability of attention, driven by query self-similarity and its interaction with keys and RoPE, provides a unifying explanation for diverse known attention patterns and yields practical gains in LLM efficiency when used to guide KV cache compression and pruning."}}
{"id": "2601.21711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21711", "abs": "https://arxiv.org/abs/2601.21711", "authors": ["Huiyuan Lai", "Malvina Nissim"], "title": "TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.", "AI": {"tldr": "TACLer is a curriculum reinforcement learning framework for large language models that reduces compute and tokens while improving complex reasoning accuracy by selectively training and triggering chain-of-thought reasoning.", "motivation": "Long chain-of-thought reasoning improves LLM performance on complex tasks but typically requires expensive large-scale RL training and leads to overthinking, i.e., redundant intermediate steps and inefficient token usage. There is a need for a more efficient training and inference procedure that preserves or improves performance while lowering computational and token costs.", "method": "The authors propose TACLer, a model-tailored curriculum RL framework with multi-stage training. First, it diagnoses what knowledge and reasoning skills the model lacks, then organizes training data into progressively harder stages (curriculum). Second, it introduces a hybrid Thinking/NoThinking reasoning paradigm: the model learns when to enable full chain-of-thought reasoning (Thinking mode) for difficult cases and when to respond directly (NoThinking mode) for easier ones. This is optimized via reinforcement learning over multiple stages, where the curriculum difficulty increases as the model\u2019s proficiency improves.", "result": "On four math benchmarks with complex problems, TACLer reduces training compute by over 50% compared to long-thinking RL baselines and cuts inference token usage by more than 42% versus the base model. At the same time, it improves accuracy by more than 9% over the base model and consistently outperforms state-of-the-art Thinking and NoThinking baselines.", "conclusion": "A tailored curriculum RL approach combined with a hybrid Thinking/NoThinking reasoning paradigm can simultaneously enhance LLM reasoning accuracy and significantly reduce both training and inference costs. This suggests that selectively applying chain-of-thought and progressively training on harder data is more efficient and effective than uniform long-CoT RL training."}}
{"id": "2601.21352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21352", "abs": "https://arxiv.org/abs/2601.21352", "authors": ["Ziyu Lu", "Tengjin Weng", "Yiying Yang", "Yuhang Zhao", "Xinxin Huang", "Wenhao Jiang"], "title": "BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents", "comment": null, "summary": "GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.", "AI": {"tldr": "Proposes BEAP-Agent, a GUI automation agent that uses DFS and systematic backtracking to recover from wrong action paths and improve long-horizon task completion.", "motivation": "Existing GUI agents fail easily on long, complex tasks because once they go down an incorrect path in the interface, they lack mechanisms to backtrack and recover, leading to low robustness and task failure rates. There is a need for a principled, systematic backtracking strategy tailored to GUI interaction to improve reliability and performance on long-horizon tasks.", "method": "Model GUI task execution as a depth-first search (DFS) over GUI states and introduce BEAP-Agent, a framework with three components: (1) Planner, which plans exploration and actions; (2) Executor, which carries out actions on the GUI; and (3) Tracker, which dynamically monitors, tracks, and updates task state. The framework supports long-range, multi-level state backtracking, allowing the agent to revert to earlier GUI states and try alternative branches when the current path appears wrong or unproductive.", "result": "On the OSWorld benchmark, BEAP-Agent attains 28.2% task accuracy, demonstrating improved effectiveness in long-horizon GUI task exploration and execution relative to prior agents that lack systematic backtracking.", "conclusion": "Systematically treating GUI interaction as DFS and equipping agents with structured, multi-level backtracking via the BEAP-Agent framework improves robustness in long-horizon GUI tasks. The work fills an important gap by providing an explicit backtracking mechanism and shows promising performance on a standard benchmark, indicating the value of DFS-based planning and tracking for GUI agents."}}
{"id": "2601.21722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21722", "abs": "https://arxiv.org/abs/2601.21722", "authors": ["Neil Heinrich Braun", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "Enhancing Language Models for Robust Greenwashing Detection", "comment": null, "summary": "Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.", "AI": {"tldr": "A framework to make LLMs more robust for assessing sustainability reports, distinguishing concrete ESG actions from vague or greenwashed claims.", "motivation": "Sustainability reports are widely used for ESG assessments, but their reliability is undermined by greenwashing and vague language. Current NLP models often rely on superficial textual patterns and fail to robustly distinguish substantive disclosures from ambiguous or misleading ones, especially across different reporting categories. A more principled representation approach is needed to capture graded differences in claim concreteness and reduce sensitivity to disclosure noise.", "method": "The paper introduces a parameter-efficient framework that reshapes the latent space of large language models. It combines contrastive learning (to pull together representations of similar statements and push apart dissimilar ones) with an ordinal ranking objective that explicitly models graded levels between concrete actions and ambiguous claims. It further integrates gated feature modulation mechanisms to filter out noisy or irrelevant disclosure signals, and uses MetaGradNorm to balance and stabilize the multi-objective optimization of contrastive and ordinal losses.", "result": "In cross-category experiments (e.g., training on one type of sustainability disclosure and testing on others), the proposed framework outperforms standard baselines in robustness to greenwashing and vague claims. The results also uncover a systematic trade-off: enforcing more rigid, structured representations improves robustness but can reduce generalization flexibility across diverse report categories.", "conclusion": "Structuring LLM latent spaces with joint contrastive and ordinal objectives, enhanced by gated feature modulation and MetaGradNorm, yields more robust models for analyzing sustainability reports under greenwashing and noisy disclosure conditions. While this structured approach offers clear robustness gains, it must be tuned to manage the trade-off between representational rigidity and cross-category generalization, informing future ESG-focused NLP model design."}}
{"id": "2601.21358", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21358", "abs": "https://arxiv.org/abs/2601.21358", "authors": ["Jiecong Wang", "Hao Peng", "Chunyang Liu"], "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization", "comment": null, "summary": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.", "AI": {"tldr": "The paper proposes PLaT, a latent reasoning framework that treats reasoning as planning in continuous hidden states, decoupling it from verbalization to achieve more flexible and scalable inference-time search.", "motivation": "Chain-of-Thought in token space is powerful but computationally expensive and prone to reasoning path collapse, and existing latent reasoning methods are opaque and constrained by fixed numbers of latent steps. The authors seek a more efficient, transparent, and adaptable latent reasoning paradigm that can better support inference-time search.", "method": "They introduce PLaT (Planning with Latent Thoughts), where reasoning is modeled as a deterministic trajectory in a latent planning space, separate from a Decoder that converts latent thoughts to text when needed. The framework allows the model to decide dynamically when to terminate reasoning instead of relying on a pre-set number of latent steps, effectively treating latent reasoning as a planning process rather than an end-to-end black box.", "result": "On mathematical reasoning benchmarks, PLaT attains lower greedy accuracy compared to some baselines but exhibits better scalability in reasoning diversity, indicating it explores a wider variety of solution paths.", "conclusion": "PLaT provides a more transparent and scalable latent reasoning framework by decoupling reasoning from verbalization and enabling dynamic termination of reasoning. Although it may sacrifice some greedy accuracy, it yields a richer solution space that is advantageous for inference-time search and future improvements in reasoning performance."}}
{"id": "2601.21725", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21725", "abs": "https://arxiv.org/abs/2601.21725", "authors": ["Liangze Jiang", "Zachary Shinnick", "Anton van den Hengel", "Hemanth Saratchandran", "Damien Teney"], "title": "Procedural Pretraining: Warming Up Language Models with Abstract Data", "comment": null, "summary": "Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.\n  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.", "AI": {"tldr": "The paper shows that pretraining language models first on synthetic procedural data (like formal languages and algorithmic tasks) substantially boosts algorithmic skills and makes later large-scale language pretraining more data-efficient.", "motivation": "Current language models are typically pretrained directly on massive web corpora, which entangle the learning of general reasoning abilities with the acquisition of world knowledge. The authors are motivated by the idea that, analogous to human learning of logic and math before complex reasoning, exposing models first to abstract, highly structured data might provide clearer training signals for core algorithmic and reasoning skills. They want to see if such procedural pretraining can improve performance, lower data requirements, and shed light on how internal representations for reasoning emerge.", "method": "1) Define and generate several types of procedural data from formal languages and simple algorithms (e.g., Dyck/balanced brackets, other structured sequences). 2) Pretrain transformer language models first on these synthetic datasets before switching to conventional corpora (C4, CodeParrot, DeepMind-Math), varying the proportion of synthetic tokens (as low as 0.1%). 3) Evaluate on algorithmic/diagnostic tasks such as Needle-in-a-Haystack (context recall) and other reasoning benchmarks to quantify gains in algorithmic skills. 4) Scale experiments up to models of ~1.3B parameters to test robustness. 5) Analyze internal mechanisms by inspecting attention patterns and MLP activations/structure after procedural pretraining. 6) Explore combinations of multiple procedural data types to see how they interact and whether they give complementary benefits.", "result": "1) Certain forms of procedural data notably enhance specific algorithmic skills; e.g., Dyck sequence pretraining boosts Needle-in-a-Haystack accuracy from 10% to 98%. 2) When front-loading only about 0.1% of total pretraining tokens with procedural data, models consistently outperform baselines trained only on natural language, code, or informal math (C4, CodeParrot, DeepMind-Math). 3) Procedural pretraining yields strong data-efficiency gains: models can achieve the same loss as standard pretraining using only 55\u201386% of the original data, depending on the domain. 4) Analysis shows that procedural pretraining induces structured patterns in both attention heads and MLP layers, with attention structure especially benefiting code-like tasks and MLP structure benefiting language-like tasks. 5) Combining different procedural datasets is feasible and can further improve performance, indicating complementary effects of different synthetic curricula.", "conclusion": "Procedural pretraining\u2014briefly exposing language models to carefully designed synthetic algorithmic data before standard large-scale corpus training\u2014is an effective, low-cost way to improve reasoning-related capabilities and make pretraining more data-efficient. It shapes both attention and MLP components in non-trivial, beneficial ways and offers a promising route to partially disentangle generic reasoning skills from the subsequent acquisition of real-world knowledge in large language models."}}
{"id": "2601.21367", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21367", "abs": "https://arxiv.org/abs/2601.21367", "authors": ["Wenjia Hua", "Kejie Zhao", "Luziwei Leng", "Ran Cheng", "Yuxin Ma", "Qinghai Guo"], "title": "Hebbian Learning with Global Direction", "comment": "Accepted to ICASSP 2026", "summary": "Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.", "AI": {"tldr": "The paper proposes Global-guided Hebbian Learning (GHL), a biologically inspired training framework that combines local Hebbian rules with a simple global feedback signal to approach backpropagation-level performance, even on large-scale tasks like ImageNet.", "motivation": "Backpropagation is highly effective but biologically implausible and computationally expensive. Pure Hebbian learning is more biologically plausible but uses only local information and ignores task-level objectives, which prevents it from scaling to deep networks and complex datasets. The authors are motivated to find a training rule that is both more biologically plausible and scalable/competitive with backpropagation, by combining local plasticity with a form of global guidance similar to neuromodulatory signals in the brain.", "method": "They propose a model-agnostic framework called Global-guided Hebbian Learning (GHL). Locally, synaptic updates follow Oja's rule combined with competitive learning to guarantee stable, unsupervised Hebbian plasticity. Globally, the network receives a task-dependent sign-based signal that modulates or gates the local Hebbian updates, effectively aligning them with the global objective without requiring full error backpropagation. This synergy between local and global components allows GHL to be applied across various architectures and tasks.", "result": "Across multiple experiments, GHL consistently outperforms existing Hebbian learning baselines. On challenging benchmarks such as ImageNet with large-scale networks, GHL attains performance that is competitive with standard backpropagation and reduces the performance gap between biologically motivated learning rules and conventional deep learning training methods.", "conclusion": "Incorporating a simple global guidance signal into Hebbian local plasticity enables a training framework that is more biologically inspired yet scales to modern deep learning tasks. GHL demonstrates that hybrid local-global learning rules can significantly close the performance gap to backpropagation, suggesting a promising direction for biologically plausible and computationally efficient training algorithms."}}
{"id": "2601.21733", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21733", "abs": "https://arxiv.org/abs/2601.21733", "authors": ["Jiayin Lan", "Jiaqi Li", "Baoxin Wang", "Ming Liu", "Dayong Wu", "Shijin Wang", "Bing Qin", "Guoping Hu"], "title": "CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.", "AI": {"tldr": "They introduce CE-GOCD, a graph-based retrieval method that uses paper titles as central entities and community detection over refined subgraphs to improve LLM question answering on scientific papers, outperforming other retrieval-augmented baselines on NLP QA datasets.", "motivation": "LLMs for scientific question answering typically depend on retrieval-augmented generation using isolated text chunks or simple concept matches. These approaches neglect richer semantic relationships between papers, such as how works cluster by topic or theme, which limits the depth, specificity, and coherence of answers. The authors aim to better exploit the structured connections present in academic knowledge graphs so that LLMs can reason over groups of related papers rather than disjoint snippets.", "method": "They propose CE-GOCD (Central Entity-Guided Graph Optimization for Community Detection). First, they treat paper titles as central entities to guide targeted subgraph retrieval from an academic knowledge graph, focusing on regions most relevant to the query. Next, they refine these subgraphs through pruning and completion to filter noise and infer missing but semantically related links, thereby enhancing implicit semantic structure. Finally, they perform community detection on the optimized subgraphs to identify coherent groups of papers sharing common themes, and use these communities as structured, semantically rich context for LLM-based question answering.", "result": "CE-GOCD is evaluated on three question-answering datasets built over NLP research literature. Across these benchmarks, the method outperforms other retrieval-augmented baselines, indicating that its graph-based, community-aware retrieval yields more accurate and informative answers than methods relying on simpler chunk- or concept-level retrieval.", "conclusion": "Explicitly modeling semantic substructures in academic knowledge graphs\u2014by using titles as central entities, optimizing subgraphs, and applying community detection\u2014can substantially improve LLM-based scientific question answering. The gains over existing retrieval-augmented approaches suggest that community-level, graph-based retrieval is a promising direction for enhancing LLM comprehension and response quality in scientific domains."}}
{"id": "2601.21372", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21372", "abs": "https://arxiv.org/abs/2601.21372", "authors": ["Yang Song", "Anoushka Vyas", "Zirui Wei", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson", "Graham Neubig"], "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents", "comment": null, "summary": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.\n  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.\n  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.", "AI": {"tldr": "NEMO is a system that converts natural-language descriptions of optimization problems into executable mathematical optimization code using autonomous coding agents (ACAs), achieving state-of-the-art results on multiple benchmarks.", "motivation": "Current methods that use LLMs or task-specific agents to generate optimization models from natural language are brittle, complex, and often produce invalid or non-executable code. There is a need for a more reliable, modular, and execution-aware architecture that can automatically produce correct, executable optimization implementations from high-level descriptions.", "method": "NEMO treats autonomous coding agents (ACAs) as first-class abstractions, similar to API-accessed LLMs, and orchestrates them within sandboxed environments so that generated code is executable and can be automatically validated and repaired. The system introduces coordination patterns across ACAs, including: (1) asymmetric validation loops between independently generated optimizer and simulator implementations for high-level validation; (2) external memory to reuse prior experience; and (3) robustness techniques such as minimum Bayes risk (MBR) decoding and self-consistency. These mechanisms iteratively refine task specifications and implementations from user-provided natural-language descriptions.", "result": "On nine established optimization benchmarks, NEMO attains state-of-the-art performance on most tasks, with especially large performance gains on several datasets, indicating that its execution-aware, agentic design is effective for automated optimization modeling.", "conclusion": "Execution-aware agentic architectures that orchestrate autonomous coding agents in sandboxed environments can reliably transform natural-language decision problem descriptions into correct and high-performing optimization models, outperforming prior approaches and demonstrating the value of structured coordination, validation, and experience reuse across coding agents."}}
{"id": "2601.21744", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21744", "abs": "https://arxiv.org/abs/2601.21744", "authors": ["Hong-Kai Zheng", "Piji Li"], "title": "Temporal Guidance for Large Language Models", "comment": null, "summary": "Contrastive Decoding (CD) enhances the generation quality of large language models (LLMs) but incurs significant additional computational overhead due to the need for an auxiliary model. Existing internal self-contrastive decoding methods, such as Decoding by Contrasting Layers (DoLa), focus on discrepancies across different layers, which are notably unstable on small-scale models. In this work, based on the observation that LLMs exhibit local preferences, we propose a novel contrastive guidance strategy along the temporal dimension, namely Temporal Guidance (TeGu). Our method ingeniously leverages Multi-Token Prediction (MTP) to construct weaker amateur predictions for model self-contrast. To standardize the implementation of this mechanism, we further introduce a lightweight Conditional MTP Projector (cMTPP), which avoids maintaining multiple independent networks as required by other MTP modules. Across various model series and benchmarks, TeGu achieves significant performance improvements while maintaining low additional memory consumption and computational overhead.", "AI": {"tldr": "They propose Temporal Guidance (TeGu), a new, efficient contrastive decoding method that improves LLM generation by self-contrasting along time using multi-token prediction, with low extra compute and memory.", "motivation": "Contrastive Decoding improves LLM output quality but needs an extra auxiliary model, adding large compute and memory costs. Internal self-contrastive methods like DoLa avoid an extra model but operate across layers, which is unstable for small models. The authors want a stable, efficient, and general self-contrastive decoding method that does not require extra large networks and works well across model sizes.", "method": "They introduce Temporal Guidance (TeGu), a contrastive guidance strategy along the temporal dimension instead of across layers. TeGu uses Multi-Token Prediction (MTP) to generate weaker \u201camateur\u201d predictions from the same model at earlier time steps and contrasts these with the standard prediction to guide decoding. To make this easy to implement and lightweight, they propose a Conditional MTP Projector (cMTPP), a small module that produces MTP outputs without needing multiple separate prediction heads or networks, standardizing and simplifying MTP-based self-contrast.", "result": "Across multiple LLM series and various benchmarks, TeGu consistently improves generation performance compared to baselines, while adding only small memory and computational overhead. The method is shown to be more stable and effective than layer-based internal contrastive decoding, especially on smaller models.", "conclusion": "Temporal Guidance provides an effective, low-overhead alternative to traditional contrastive decoding by leveraging temporal self-contrast via MTP rather than auxiliary models or layer contrasts. The proposed cMTPP module makes the approach lightweight and practical, leading to robust quality gains for LLM generation across models and tasks."}}
{"id": "2601.21375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21375", "abs": "https://arxiv.org/abs/2601.21375", "authors": ["Zheng Li", "Siyao Song", "Jingyuan Ma", "Rui Li", "Ying Zeng", "Minghao Li", "Zhifang Sui"], "title": "TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models", "comment": null, "summary": "Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.", "AI": {"tldr": "They propose and instantiate a framework to evaluate LLMs as teachers by measuring how much they improve student performance when teaching from a fixed syllabus.", "motivation": "LLMs are being considered as teaching assistants, but current evaluations mostly test problem-solving or single-problem hints, not the deeper, knowledge-centered teaching process. There is a need for a rigorous, leak-free, and reusable way to measure how well LLMs can teach over multiple turns based on a defined syllabus.", "method": "They design a syllabus-grounded evaluation framework where LLMs act as teacher agents limited to structured knowledge points and example problems. The teacher gives multi-turn instruction, then student performance on existing benchmarks is measured before and after the teaching. They implement this framework on Gaokao exam data across several subjects to compare models and conditions (with/without example problems).", "result": "They observe large differences in teaching effectiveness between models and across subjects: some LLMs can effectively teach mathematics but struggle with physics and chemistry. They also find that providing example problems in the teaching material does not always help; models tend to focus on correcting those specific examples instead of conveying generalizable knowledge.", "conclusion": "Teaching ability is a distinct behavioral dimension of LLMs that can be systematically measured via syllabus-grounded, multi-turn instruction settings. Effectiveness is domain- and model-dependent, and naive inclusion of example problems may not boost teaching quality, highlighting the need for better teaching strategies and model designs."}}
{"id": "2601.21766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21766", "abs": "https://arxiv.org/abs/2601.21766", "authors": ["Amit Dhurandhar", "Vijil Chenthamarakshan", "Dennis Wei", "Tejaswini Pedapati", "Karthikeyan Natesan Ramamurthy", "Rahul Nair"], "title": "CoFrGeNet: Continued Fraction Architectures for Language Generation", "comment": null, "summary": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\\frac{2}{3}$ to $\\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.", "AI": {"tldr": "Introduces CoFrGeNets, a continued-fraction-based alternative to Transformer attention/FFN that matches or beats GPT\u20112-XL and Llama\u20113 performance with roughly half to two\u2011thirds the parameters and less pre-training time.", "motivation": "Transformers, while highly effective for language generation, are parameter-intensive and computationally expensive, which limits scalability and deployment in industrial settings. The authors seek a new function class and corresponding architecture that can retain or improve generative and downstream performance while substantially reducing parameter count and training cost, and that can be easily integrated into existing Transformer-centric workflows.", "method": "Define a new function class for generative modeling inspired by continued fractions and instantiate it as CoFrGeNets (Continued Fraction Generative Networks). Design architectural components based on this function class that can serve as drop-in replacements for Multi-head Attention and Feed-Forward Networks in Transformer blocks, with fewer parameters. Derive custom gradient formulations tailored to these components to improve training accuracy and efficiency beyond what standard PyTorch autograd would provide. Integrate these components into two Transformer architectures\u2014GPT2-xl (1.5B) and Llama3 (3.2B)\u2014and pre-train them on large web-scale corpora (OpenWebText + GneissWeb for GPT2-xl, and the docling data mix of nine datasets for Llama3). Evaluate the resulting models on a suite of downstream tasks including classification, question answering, reasoning, and text understanding.", "result": "CoFrGeNet-augmented models achieve performance on downstream tasks that is competitive with, and in some cases superior to, the original GPT2-xl and Llama3 baselines. They do so while using only one-half to two-thirds of the original parameter counts and requiring shorter pre-training times. The plug-in nature of the components allows existing training and inference pipelines for Transformers to be reused with minimal modification. Custom gradient formulations further contribute to efficient and stable optimization of the new components.", "conclusion": "The proposed continued-fraction-based CoFrGeNet components offer an effective, parameter-efficient alternative to standard Multi-head Attention and Feed-Forward Networks in Transformer architectures. They can be readily plugged into existing models and workflows, yielding comparable or better downstream performance with reduced model size and training time. The authors anticipate that future hardware-aware implementations could unlock even greater efficiency and performance gains, suggesting CoFrGeNets as a promising direction for scalable, industrial-scale generative modeling."}}
{"id": "2601.21403", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21403", "abs": "https://arxiv.org/abs/2601.21403", "authors": ["Ruyi Qi", "Zhou Liu", "Wentao Zhang"], "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis", "comment": null, "summary": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.", "AI": {"tldr": "DataCross introduces a benchmark and multi-agent framework to unify analysis over structured data and unstructured visual documents, enabling more factual and robust cross-modal enterprise analytics.", "motivation": "In industry, important information is split between structured sources like SQL/CSV and unstructured visual documents like scanned reports or invoices. Current data agents mostly handle only structured data, leaving the visual \u201czombie data\u201d unused and preventing comprehensive, cross-source analysis that enterprises require.", "method": "The authors build DataCrossBench, a benchmark of 200 realistic, end-to-end analysis tasks across domains such as finance and healthcare. These tasks are generated via a human-in-the-loop reverse-synthesis pipeline that enforces realistic complexity, dependencies between data sources, and verifiable ground truth, and are organized into three difficulty levels targeting visual table extraction, cross-modal alignment, and multi-step reasoning. They also propose DataCrossAgent, a collaborative multi-agent system where specialized sub-agents focus on individual data sources and are orchestrated through a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A reReAct mechanism is used for iterative code generation and debugging to ensure factual verification of results.", "result": "On DataCrossBench, DataCrossAgent improves factuality by 29.7% compared with GPT-4o and shows stronger robustness, especially on high-difficulty tasks that require complex, cross-modal reasoning and use of fragmented visual \u201czombie data.\u201d", "conclusion": "DataCrossBench provides a realistic and challenging benchmark for unified cross-modal enterprise analytics, while DataCrossAgent demonstrates that a coordinated, divide-and-conquer multi-agent workflow with iterative verification can substantially improve factual, robust use of both structured and visual unstructured data in practical decision-making settings."}}
{"id": "2601.21767", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21767", "abs": "https://arxiv.org/abs/2601.21767", "authors": ["Wei Zhu"], "title": "Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond", "comment": null, "summary": "Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.", "AI": {"tldr": "The paper evaluates ChatGPT\u2019s effectiveness on multiple medical information extraction tasks, finding it less accurate than specialized models but strong in explanations and generally faithful to source text, with problematic over-confidence and output uncertainty.", "motivation": "To understand whether ChatGPT, a general-purpose LLM, can reliably perform specialized medical information extraction tasks and how its behavior (performance, explanations, confidence, faithfulness, uncertainty) compares to fine-tuned baseline models on standard benchmarks.", "method": "The authors test ChatGPT on four different MedIE task types using six benchmark datasets. They systematically evaluate several dimensions: raw task performance compared to fine-tuned baselines, the quality of ChatGPT\u2019s natural language explanations, its self-reported confidence, the faithfulness of its outputs to the source medical text, and how generation variability (uncertainty) affects extraction reliability.", "result": "(a) ChatGPT underperforms fine-tuned baseline models on all or most MedIE tasks. (b) It generates high-quality, understandable explanations but tends to be over-confident even when wrong. (c) Its outputs usually remain faithful to the original text. (d) However, generation stochasticity introduces notable uncertainty into extracted information, which can negatively impact its practical use for MedIE.", "conclusion": "While ChatGPT is a powerful general LLM with strong explanatory and generally faithful behavior, it is not yet a dependable replacement for specialized, fine-tuned models for medical information extraction. Its over-confidence and output uncertainty pose risks for MedIE applications, indicating a need for additional methods (e.g., calibration, constraint mechanisms, or hybrid systems) before safe deployment in real-world medical settings."}}
{"id": "2601.21414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21414", "abs": "https://arxiv.org/abs/2601.21414", "authors": ["Chenxu Yang", "Qingyi Si", "Chong Tian", "Xiyu Liu", "Dingyu Yao", "Chuanyu Qin", "Zheng Lin", "Weiping Wang", "Jiaqi Wang"], "title": "System 1&2 Synergy via Dynamic Model Interpolation", "comment": null, "summary": "Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \\textit{how models think} rather than \\textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \\textbf{DAMI} (\\textbf{D}yn\\textbf{A}mic \\textbf{M}odel \\textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $\u03bb(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.", "AI": {"tldr": "The paper proposes DAMI, a dynamic model interpolation framework that adaptively mixes an Instruct (System 1\u2011like) and a Thinking (System 2\u2011like) LLM to get both high reasoning accuracy and efficiency.", "motivation": "Existing work tries to make System 2 / \u2018thinking\u2019 models more efficient mainly by constraining outputs (e.g., limiting chain\u2011of\u2011thought length). This treats long outputs as the problem, instead of addressing the underlying cognitive configuration of the model. There is also interference when trying to unify fast, intuitive System 1 behavior with slow, deliberative System 2 behavior in a single model. The authors want a way to dynamically control *how* the model thinks (its reasoning intensity) per query, without expensive retraining.", "method": "They introduce capability control via dynamic parameter interpolation between two existing checkpoints: an Instruct model (System 1\u2011like) and a Thinking model (System 2\u2011like). First, they empirically show that linear interpolation of parameters forms a convex, monotonic Pareto frontier between accuracy and efficiency, suggesting smooth representation connectivity between the two models. On top of this, they propose DAMI, which estimates a query\u2011specific scalar \u2018Reasoning Intensity\u2019 \u03bb(q). This \u03bb(q) determines how much weight to give to the Thinking vs Instruct model in the interpolated parameters. They give two schemes to choose \u03bb(q): (1) a training\u2011based preference learning method that learns \u03bb(q) using preferences combining accuracy and efficiency; and (2) a zero\u2011shot, confidence\u2011based method that uses discrepancy between the two models\u2019 predictions as a signal of cognitive uncertainty, and sets \u03bb(q) accordingly. No additional pretraining of the base checkpoints is required; only \u03bb(q) prediction is learned where training is used.", "result": "On five mathematical reasoning benchmarks, DAMI\u2011configured models outperform the original Thinking model in accuracy while requiring less computation, and are more accurate than the Instruct model while preserving much of its efficiency. The interpolation curve is empirically convex and monotonic, validating the idea that gradual mixing of the two checkpoints creates a smooth trade\u2011off between speed and reasoning depth. Both the preference\u2011learning\u2011based and the zero\u2011shot confidence\u2011based \u03bb(q) estimators provide effective query\u2011adaptive control.", "conclusion": "Dynamic capability control via parameter interpolation is a promising way to unify System 1\u2011like efficiency with System 2\u2011like reasoning depth in LLMs. Rather than restricting outputs, adjusting a query\u2011specific reasoning intensity over an Instruct\u2013Thinking interpolation frontier can yield better accuracy\u2013efficiency trade\u2011offs, without retraining the underlying models. The observed convex Pareto frontier suggests that such checkpoints lie on a structurally connected manifold, enabling smooth and controllable cognitive adaptation."}}
{"id": "2601.21768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21768", "abs": "https://arxiv.org/abs/2601.21768", "authors": ["Alon Rozental"], "title": "Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention", "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.", "AI": {"tldr": "The paper introduces Zonkey, an end-to-end trainable hierarchical diffusion model that learns a differentiable tokenizer and multi-level text representation directly from characters, enabling fully gradient-based language modeling and variable-length text generation.", "motivation": "Conventional large language models rely on fixed, non-differentiable tokenizers such as BPE, which prevent true end-to-end training, limit adaptability to noisy or specialized domains, and hard-code segmentation decisions. The authors aim to remove this bottleneck by creating a fully differentiable, trainable alternative that can learn segmentation, hierarchy, and generation jointly from raw character data, improving flexibility and domain adaptation.", "method": "The authors propose Zonkey, a hierarchical diffusion model with several key components: (1) a differentiable tokenizer called the Segment Splitter, which predicts probabilistic beginning-of-sequence (BOS) decisions to form adaptive segments from character sequences; (2) a Probabilistic Attention mechanism that uses position-specific existence probabilities to implement soft masking over effectively unbounded sequences, allowing gradients to flow while enabling probabilistic sequence termination rather than explicit end-of-sequence tokens; (3) a multi-level hierarchy that compresses character-level inputs into higher-level abstractions (from character n-grams to word-like, then sentence-like vectors); (4) a Denoising Diffusion Mixed Model (DDMM) for stable, efficient denoising in latent space across these levels; and (5) a Stitcher module that enforces overlap invariance when reconstructing sequences from segments. The entire pipeline is trained end-to-end on Wikipedia, from noise to document-level text.", "result": "Zonkey learns emergent, linguistically meaningful segmentation\u2014such as aligning splits with spaces and sentence boundaries\u2014despite having no explicit supervision for tokenization. It generates coherent, variable-length text directly from noise, and its learned hierarchical representations qualitatively resemble word- and sentence-level abstractions. The model\u2019s outputs show better qualitative alignment to data distributions than entropy-based learnable tokenizers, demonstrating that the differentiable tokenizer and hierarchical diffusion approach are viable. The system also supports scalable generation via probabilistic length control rather than fixed end-of-sequence markers.", "conclusion": "The paper concludes that it is feasible to build a fully gradient-based language modeling pipeline that learns both segmentation and hierarchical text structure directly from characters. Zonkey\u2019s differentiable tokenizer, probabilistic attention, and hierarchical diffusion allow end-to-end optimization and produce coherent, domain-aligned text. This approach reduces dependence on manually designed tokenizers and suggests a path toward more adaptable and scalable LLMs, particularly for noisy or specialized domains. The authors release code to encourage further research on differentiable tokenization and hierarchical diffusion-based language models."}}
{"id": "2601.21433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21433", "abs": "https://arxiv.org/abs/2601.21433", "authors": ["Katherine Elkins", "Jon Chun"], "title": "When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models", "comment": "13 pages, 5 figures", "summary": "When a user tells an AI system that someone \"should not\" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish \"do X\" from \"do not X\" should not be making autonomous decisions in high-stakes contexts.", "AI": {"tldr": "The paper shows that many language models dangerously misinterpret negated ethical instructions (e.g., \u201cdo not X\u201d) as if they were affirmative, and proposes a metric and certification framework to quantify and govern this risk.", "motivation": "As AI systems are increasingly used in high\u2011stakes, semi\u2011autonomous or autonomous settings, they must correctly interpret prohibitions such as \u201cshould not do X.\u201d However, anecdotal evidence and scattered failures suggest that models sometimes flip the meaning of negation, potentially endorsing harmful actions that users explicitly disallow. There is a lack of systematic measurement of this problem across models and domains, and no clear governance metric to evaluate whether models are safe enough for deployment where ethical constraints matter.", "method": "The authors audit 16 language models (both open\u2011source and commercial) on 14 ethically charged scenarios. For each scenario, they construct prompts with affirmative framing (\"should do X\"), simple negation (\"should not do X\"), and compound negation (more complex negative constructions). They then measure how often models endorse the prohibited action under each framing. They compare open\u2011source and commercial models, evaluate different domains (e.g., financial vs. medical), and run tests under deterministic decoding to rule out sampling randomness. They also develop a Negation Sensitivity Index (NSI) to quantify how much a model\u2019s answers swing when negation framing changes, and illustrate failures via practical case studies. Finally, they sketch a tiered certification framework with domain\u2011specific NSI thresholds.", "result": "Open\u2011source models frequently invert prohibitions: they endorse prohibited actions 77% of the time when instructions are simply negated and 100% under compound negation, representing a 317% increase in endorsement relative to affirmative framing. Commercial models perform better but still exhibit sizeable swings (19\u2013128%) depending on how negation is phrased. Inter\u2011model agreement is high on affirmative prompts (74%) but drops on negated prompts (62%), indicating inconsistent treatment of negation. Financial scenarios are roughly twice as fragile as medical scenarios. These effects persist under deterministic decoding, so they are not due to sampling variance. The NSI captures these framing\u2011induced swings, and case studies show concrete ways that such failures could manifest in real systems.", "conclusion": "Current alignment and safety techniques are insufficient to ensure robust understanding of negation in ethically relevant instructions. Many models cannot reliably distinguish between commands like \u201cdo X\u201d and \u201cdo not do X,\u201d particularly in complex or domain\u2011specific contexts, which makes them unsafe for autonomous use in high\u2011stakes settings. The authors argue that such models should not be entrusted with autonomous decision\u2011making where misinterpreting prohibitions could cause harm. They recommend using their Negation Sensitivity Index as a governance and evaluation metric, along with a tiered certification framework that sets domain\u2011appropriate thresholds on negation robustness for safe deployment."}}
{"id": "2601.21796", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21796", "abs": "https://arxiv.org/abs/2601.21796", "authors": ["Yaocong Li", "Leihan Zhang", "Le Zhang", "Qiang Yan"], "title": "KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection", "comment": null, "summary": "Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.", "AI": {"tldr": "The paper proposes KID, a knowledge-injected dual-head learning framework that uses explicit reasoning chains and joint generation-classification to detect harmful internet memes across languages, achieving state-of-the-art results.", "motivation": "Internet memes often encode harmful or toxic content implicitly through metaphors, visuals, and cultural references, making them hard for automated moderation systems to detect, especially when the toxicity is not directly observable in text or image alone. Existing models mostly analyze visual and textual signals within the meme, but they lack the external background knowledge and structured reasoning needed to uncover implicit harmful meaning. There is therefore a need for a method that can integrate external knowledge in a grounded, interpretable way while remaining robust and generalizable across languages and datasets.", "method": "The authors introduce KID (Knowledge-Injected Dual-Head Learning), which combines knowledge-grounded reasoning with a dual-head architecture. First, they use a label-constrained distillation paradigm that decomposes meme understanding into structured reasoning chains explicitly connecting (1) visual evidence in the meme, (2) external background knowledge, and (3) final classification labels. These chains serve as supervision signals that guide the model to ground external knowledge in the specific meme context. Second, KID employs a dual-head architecture with one head focused on semantic reasoning/generation (producing the reasoning chains or explanations) and the other on harmfulness classification. Both heads are optimized jointly so that linguistic reasoning and decision boundaries stay aligned while the classifier remains stable. The framework is applied to multilingual harmful meme detection tasks using image-text inputs and external knowledge sources.", "result": "KID is evaluated on five multilingual harmful meme datasets covering English, Chinese, and Bengali, handling both binary (harmful vs. non-harmful) and multi-label detection settings. Across primary evaluation metrics, KID outperforms prior state-of-the-art methods by margins ranging from 2.1% up to 19.7%. The ablation studies demonstrate that removing knowledge injection or the dual-head joint learning significantly degrades performance, evidencing that both components contribute meaningfully and complement each other in improving robustness and generalization in meme understanding.", "conclusion": "The paper concludes that injecting structured, label-constrained knowledge and enforcing a dual-head joint learning scheme enables more accurate and robust detection of harmful memes, especially when toxicity is implicit and culturally grounded. The explicit reasoning chains help bridge the gap between meme content and required background knowledge, while the dual-head design aligns explanation generation with classification without sacrificing decision stability. The approach generalizes well across languages and datasets, setting a new state of the art for harmful meme detection and providing a promising direction for knowledge-grounded multimodal content moderation systems."}}
{"id": "2601.21439", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21439", "abs": "https://arxiv.org/abs/2601.21439", "authors": ["Jon Chun", "Katherine Elkins"], "title": "The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making", "comment": "22 page, 10 figures", "summary": "While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking \"Paradox of Robustness\": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be \"brittle\" to how a query is formatted, they are remarkably \"stable\" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.", "AI": {"tldr": "LLMs, though prompt-sensitive, are surprisingly robust against emotional or narrative attempts to bias their rule-based decisions, far more than humans.", "motivation": "Human decision-making in high-stakes domains is notoriously vulnerable to framing and emotional narratives, while LLMs are known to be brittle to prompt changes and sycophantic to user bias. The authors want to test whether this brittleness and sycophancy also imply vulnerability to narrative-induced bias in rule-bound decisions, which is critical for deploying LLMs in domains like healthcare, law, and finance.", "method": "The authors build a controlled perturbation framework and a 162-scenario benchmark across healthcare, law, and finance. They systematically vary the emotional and narrative framing of scenarios while holding the underlying rules and facts constant, then compare the decisions of instruction-tuned LLMs to human subjects. They quantify susceptibility to manipulation by computing effect sizes (Cohen's h) for outcome changes under different framings and test multiple models with diverse training paradigms.", "result": "Instruction-tuned LLMs show an extremely low susceptibility to emotional or narrative framing (Cohen's h \u2248 0.003), in stark contrast to human subjects who exhibit substantial framing-induced biases (Cohen's h in the 0.3\u20130.8 range). This yields a robustness gap: LLMs are 110\u2013300 times more resistant to narrative manipulation than humans. The effect is consistent across multiple instruction-tuned models and settings.", "conclusion": "Despite being lexically brittle and sycophantic in some contexts, LLMs can maintain strong logical rule adherence under manipulative narratives, decoupling surface prompt sensitivity from deeper decision robustness. This paradoxical robustness suggests LLMs can provide stable, de-biased decisions in institutional, rule-based settings and may serve as a complement or corrective to human judgment. The released benchmark, data, and code enable further study of narrative-induced bias and robustness in LLMs."}}
{"id": "2601.21797", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21797", "abs": "https://arxiv.org/abs/2601.21797", "authors": ["Yimin Deng", "Yuqing Fu", "Derong Xu", "Yejing Wang", "Wei Ni", "Jingtong Gao", "Xiaopeng Li", "Chengxu Liu", "Xiao Han", "Guoshuai Zhao", "Xiangyu Zhao", "Li Zhu", "Xueming Qian"], "title": "Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation", "comment": "11 pages, 4 figures", "summary": "Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.", "AI": {"tldr": "They propose AMA, an adversarial memory adaptation framework that tunes dialogue memory construction and updates to specific downstream tasks by simulating task execution with multiple agents, improving long-conversation performance.", "motivation": "Conversational agents with limited context windows rely on external memory to handle long dialogues. Existing memory systems build and update memory offline using fixed, task-independent workflows and generic metrics. This causes a mismatch between what the task needs and what the memory stores/emphasizes, hurting downstream performance. They want a way to make memory preparation explicitly task-aware before deployment.", "method": "They introduce Adversarial Memory Adaptation (AMA), which adds a task-aware, multi-agent simulation loop into the offline phase of memory systems. A challenger agent first generates question\u2013answer pairs from the original dialogues to mimic downstream tasks. The system then uses the constructed memory to answer these questions, simulating inference. An evaluator agent checks the generated answers, performs error analysis, and identifies failure cases. Finally, an adapter agent uses these error signals to update both the memory construction strategy (how memories are selected/organized) and the memory content itself (what is stored/removed/modified). This loop is run offline so the memory system is shaped by adversarial, task-oriented supervision before being used online, and AMA is designed as a plug-in mechanism for existing memory architectures.", "result": "On the long-dialogue benchmark LoCoMo, integrating AMA into various existing memory systems leads to consistent improvements over baselines without AMA. The experiments show better downstream task performance, indicating that adversarial, task-aware adaptation of memory construction and updates helps conversational agents handle long contexts more effectively.", "conclusion": "Aligning offline memory construction and update with task objectives through adversarial simulation significantly improves the usefulness of memory for long-conversation tasks. AMA provides a general, pluggable mechanism that can adapt different memory systems to downstream requirements by using challenger, evaluator, and adapter agents in the offline phase, and empirical results on LoCoMo validate its effectiveness."}}
{"id": "2601.21448", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.21448", "abs": "https://arxiv.org/abs/2601.21448", "authors": ["Zhongkai Yu", "Chenyang Zhou", "Yichen Lin", "Hejia Zhang", "Haotian Ye", "Junxia Cui", "Zaifeng Pan", "Jishen Zhao", "Yufei Ding"], "title": "ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design", "comment": null, "summary": "While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\\% on Verilog generation and 13.33\\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.", "AI": {"tldr": "The paper introduces ChipBench, a realistic benchmark for evaluating large language models on industrial-scale chip design tasks, highlighting that current state-of-the-art models still perform poorly on complex hardware workflows.", "motivation": "Existing LLM benchmarks for hardware design are saturated and lack task diversity, so they no longer reveal meaningful performance gaps or reflect real industrial chip design workflows. The authors want a benchmark that tests LLMs on realistic, challenging tasks that hardware engineers actually care about, such as writing Verilog, debugging designs, and building reference models in multiple languages.", "method": "The authors build a new benchmark suite, ChipBench, that covers three key chip-design-related tasks: (1) Verilog generation, (2) Verilog debugging, and (3) reference model generation. They construct 44 realistic Verilog modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference-model examples implemented in Python, SystemC, and CXXRTL. They then evaluate state-of-the-art LLMs on these tasks, measuring pass rates to quantify performance gaps versus simpler, saturated benchmarks.", "result": "On ChipBench, state-of-the-art models perform significantly worse than on existing benchmarks. For example, Claude-4.5-opus attains only 30.74% accuracy on Verilog generation and 13.33% on Python reference model generation, in stark contrast to over 95% pass rates on prior, easier benchmarks. This demonstrates that ChipBench exposes substantial limitations of current LLMs in realistic hardware design workflows.", "conclusion": "ChipBench provides a more realistic and challenging benchmark for AI-assisted chip design, revealing that current LLMs are far from solving industrial-scale design and modeling tasks. The large performance gaps indicate substantial headroom for future research. To support progress, the authors also release an automated toolbox for generating high-quality training data for reference model generation, along with open-source code and benchmark resources."}}
{"id": "2601.21803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21803", "abs": "https://arxiv.org/abs/2601.21803", "authors": ["Korbinian Randl", "Guido Rocchietti", "Aron Henriksson", "Ziawasch Abedjan", "Tony Lindgren", "John Pavlopoulos"], "title": "RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.", "AI": {"tldr": "They propose RAG-E, an explainability framework for RAG systems, showing that generators often misuse or ignore retrieved documents.", "motivation": "RAG systems are increasingly used in high-stakes settings, but it is unclear how the retriever and generator components interact, making it hard to trust or debug the system.", "method": "They adapt Integrated Gradients to analyze the retriever, propose PMCSHAP (a Monte Carlo-based Shapley approximation) for attributing the generator\u2019s use of documents, and define the WARG metric to compare generator attributions against the retriever\u2019s ranking, then evaluate on TREC CAsT and FoodSafeSum.", "result": "Using their framework, they find large misalignments: in many queries, the generator ignores top-ranked documents from the retriever and instead relies on lower-ranked ones, indicating systematic mismatch between retrieval and generation.", "conclusion": "RAG quality depends critically on the interaction between retriever and generator, not just their standalone performance, and RAG-E can audit this interaction by quantifying how well the generator\u2019s document usage aligns with the retriever\u2019s ranking."}}
{"id": "2601.21453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21453", "abs": "https://arxiv.org/abs/2601.21453", "authors": ["Xunkai Li", "Zhengyu Wu", "Zekai Chen", "Henan Sun", "Daohan Su", "Guang Zeng", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang"], "title": "LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning", "comment": null, "summary": "Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\\underline{LI}ff\\underline{O}rd \\underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.", "AI": {"tldr": "The paper proposes LION, a new Clifford algebra-based neural framework for better aligning and fusing multiple modalities on graphs, achieving state-of-the-art results on various multimodal and graph tasks.", "motivation": "Existing multimodal graph learning methods struggle with two key issues: (1) modality alignment is often done with topology-constrained or modality-specific tokenizers that ignore the broader graph context, weakening cross-modality interaction and leading to suboptimal alignment; (2) fusion mechanisms are mostly simple extensions from 2-modality setups and do not fully leverage aligned tokens enriched with topology priors, hurting generalization and performance, especially as the number of modalities or task complexity increases. There is a need for a principled neural paradigm that jointly considers graph structure, geometric properties of multimodal data, and flexible fusion strategies.", "method": "The authors introduce LION, a Clifford Neural paradigm built on Clifford algebra and a decoupled graph neural pipeline (propagation-then-aggregation) to implement an alignment-then-fusion procedure on multimodal-attributed graphs. First, they construct a modality-aware geometric manifold via Clifford algebra, enabling geometric-induced high-order graph propagation that promotes effective modality interaction and alignment in a context-aware way. Then, leveraging the geometric grade properties of the aligned tokens, they design an adaptive holographic aggregation module that fuses modalities by combining the energy and scale of different geometric grades with learnable parameters, thus performing more expressive and adaptive modality fusion.", "result": "Across 9 benchmark datasets, LION is empirically validated on 3 graph-level and 3 modality-oriented downstream tasks. In all these settings, LION significantly outperforms state-of-the-art multimodal graph learning baselines, indicating stronger alignment quality, better modality fusion, and improved performance and generalizability on diverse multimodal graph problems.", "conclusion": "The paper concludes that casting multimodal-attributed graph learning into a Clifford algebra-based geometric framework, combined with a decoupled propagation-then-aggregation (alignment-then-fusion) neural paradigm, effectively addresses limitations of existing methods in both modality alignment and fusion. The proposed LION model provides a principled and scalable way to exploit graph context and geometric structure of multimodal data, yielding consistent state-of-the-art improvements on various tasks and suggesting a promising direction for future multimodal graph ML research."}}
{"id": "2601.21804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21804", "abs": "https://arxiv.org/abs/2601.21804", "authors": ["Bodong Du", "Xuanqi Huang", "Xiaomeng Li"], "title": "Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning", "comment": null, "summary": "Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.", "AI": {"tldr": "The paper identifies a weakness in majority-vote based test-time reinforcement learning for LLMs and proposes a distribution-aware reward estimator (DARE) that uses the full rollout distribution with exploration bonuses and pruning, leading to more stable training and better results on reasoning benchmarks.", "motivation": "Test-time RL lets LLMs improve themselves on unlabeled inputs, but current methods estimate rewards via majority voting over multiple rollouts. This collapses the rollout distribution to a single outcome, ignores potentially correct minority answers, and can create biased, brittle reward signals. The authors are motivated to design a more informative and robust way to estimate rewards from multiple rollouts without ground-truth labels.", "method": "The method, DARE (Distribution-Aware Reward Estimation), replaces majority-vote, single-outcome rewards with a reward computed from the full empirical distribution of rollouts. Instead of treating the majority answer as the only signal, DARE aggregates over all sampled actions/outputs. It then adds (1) an exploration bonus that encourages the model to explore non-majority but promising rollouts, and (2) a pruning mechanism that filters out low-quality or noisy rollouts from the distribution to denoise the reward. This distribution-aware, pruned, and exploration-enhanced reward is used in test-time RL updates.", "result": "On challenging reasoning benchmarks, DARE yields more stable optimization and better final test-time RL performance compared to recent baselines using majority voting. Quantitatively, it achieves a 25.3% relative improvement on AIME 2024 and a 5.3% improvement on AMC, both difficult mathematical reasoning datasets.", "conclusion": "Majority-vote based reward estimation in test-time RL for LLMs is fragile and systematically biased because it discards information about non-majority but correct rollouts. Modeling rewards from the full rollout distribution, with exploration bonuses and pruning for denoising, produces more robust and informative reward signals. This leads to more stable self-improvement and better reasoning performance, suggesting that distribution-aware reward design is a key ingredient for effective test-time RL in LLMs."}}
{"id": "2601.21465", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21465", "abs": "https://arxiv.org/abs/2601.21465", "authors": ["M\u00e1rton Kardos"], "title": "Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance", "comment": "14 pages, 6 figures", "summary": "Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \\textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.", "AI": {"tldr": "The paper introduces Topeax, a new text clustering-based topic modeling method that addresses limitations of Top2Vec and BERTopic by more robustly discovering the number of clusters and producing higher-quality topic keywords.", "motivation": "Existing clustering-based topic models like Top2Vec and BERTopic are popular but suffer from several weaknesses: (1) high sensitivity to sample size and hyperparameters, so they often fail to recover natural clusters in text corpora; (2) suboptimal keyword selection, because BERTopic ignores the semantic distance between keywords and topic vectors, and Top2Vec ignores word frequency information. These issues lead to incoherent topics containing stop/junk words or overly narrow, untrustworthy topic descriptions. A more stable, accurate method is needed to both discover clusters and describe them with high-quality keywords.", "method": "The proposed method, Topeax, is a new clustering-based topic modelling approach. It automatically discovers the number of clusters by identifying peaks in density estimates of document representations (rather than relying on fixed or poorly tuned parameters). For term importance within topics, it combines lexical information (e.g., word counts/frequency) with semantic information (e.g., distance of words to topic vectors in embedding space) into a joint importance index, so that topic keywords are both frequent and semantically close to their topic centers.", "result": "Empirical evaluations show that Topeax recovers underlying clusters better than Top2Vec and BERTopic and provides more accurate and informative cluster descriptions. It is also less sensitive to variations in sample size and hyperparameter settings, showing more stable behaviour across different configurations.", "conclusion": "Topeax mitigates key weaknesses of existing clustering-based topic models by (a) automatically determining the number of clusters using density peaks and (b) integrating lexical and semantic signals for keyword selection. This leads to more reliable topic discovery and more coherent, trustworthy topic keywords, making Topeax a more robust alternative to Top2Vec and BERTopic for topic modelling tasks."}}
{"id": "2601.21826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21826", "abs": "https://arxiv.org/abs/2601.21826", "authors": ["Aadi Palnitkar", "Mingyang Mao", "Nicholas Waytowich", "Vinicius G. Goecks", "Tinoosh Mohsenin", "Xiaomin Lin"], "title": "Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.", "AI": {"tldr": "The paper introduces MilSCORE, a long-context, scenario-level benchmark of expert-authored, multi-hop questions in a complex simulated military planning setting, to test LLMs\u2019 geospatial and tactical reasoning over heterogeneous, multi-modal data; baseline results show current models perform poorly, leaving substantial room for improvement.", "motivation": "Existing long-context benchmarks are not realistic enough and rarely require selective reading and integration of heterogeneous, multi-modal information, especially in geospatial planning domains. Planning for large-scale military operations is a representative, high-stakes task that requires rapid, accurate reasoning over maps, orders, intelligence reports, and other distributed data. There is a lack of scenario-level, expert-authored datasets that capture such complex, long-horizon, geospatially rich planning problems, making it difficult to rigorously evaluate and improve LLMs for these settings.", "method": "The authors construct MilSCORE (Military Scenario Contextual Reasoning), a benchmark built around a complex, simulated military planning scenario used in training contexts. Domain experts author multi-hop questions that are explicitly grounded in this scenario and require integrating information across multiple heterogeneous and often multi-modal sources, including maps and text documents. The questions cover seven categories and are designed to test both factual recall and multi-step reasoning involving constraints, strategy, and spatial analysis. The authors also define an evaluation protocol and run a set of contemporary vision-language models on the benchmark to obtain baseline performance.", "result": "MilSCORE provides a diverse set of question types probing tactical and spatial reasoning, factual recall, and multi-step planning over long-horizon, geospatially rich context. When evaluated using the proposed protocol, a range of current vision-language models perform significantly below what would be required for high-stakes planning, indicating difficulties with realistic, scenario-level long-context reasoning and planning. The reported baseline scores show large performance gaps and substantial remaining headroom across the different question categories.", "conclusion": "The study concludes that MilSCORE fills a critical gap by offering, to the authors\u2019 knowledge, the first scenario-level benchmark focused on complex, expert-authored, multi-hop questions in a simulated military planning environment. The poor performance of contemporary models underscores that realistic long-context, geospatial, and tactical planning remains an open challenge for LLMs and vision-language models. MilSCORE is positioned as a difficult but valuable testbed for driving future research on long-context understanding, selective reading, multi-modal integration, and high-stakes decision-making capabilities in AI systems."}}
{"id": "2601.21468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21468", "abs": "https://arxiv.org/abs/2601.21468", "authors": ["Yaorui Shi", "Shugui Liu", "Yu Yang", "Wenyu Mao", "Yuxin Chen", "Qi GU", "Hui Su", "Xunliang Cai", "Xiang Wang", "An Zhang"], "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning", "comment": null, "summary": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.", "AI": {"tldr": "MemOCR is a multimodal memory system that renders rich-text histories into images, using layout to pack more important information under tight context budgets, improving long-horizon QA performance versus text-only memories.", "motivation": "Long-horizon reasoning agents must handle growing interaction histories within a fixed, small context window. Existing memory systems usually store history as plain text, where each token has equal cost and the context size grows linearly with length. This leads to inefficient use of context, wasting budget on low-value details and limiting performance when budgets are tight. There is a need for a memory mechanism that can adaptively allocate representational space based on importance while remaining usable by language agents.", "method": "The authors propose MemOCR, a multimodal memory agent. It maintains a structured rich-text memory that includes elements such as headings, highlights, and presumably other formatting cues to encode importance and structure. Instead of feeding this memory back as text, MemOCR renders it as an image, leveraging visual layout to increase information density and visually emphasize critical evidence while compressing or downplaying auxiliary details. The agent accesses this memory by reading the rendered image. MemOCR is trained with reinforcement learning using budget-aware objectives that vary the available memory budget, encouraging robustness to different compression levels and learning how to best trade off detail versus compression.", "result": "On both long-context multi-hop and single-hop question-answering benchmarks, MemOCR surpasses strong text-only memory baselines. It shows better performance particularly in settings with extreme context limitations, indicating that its visually dense and importance-aware memory representation leads to more effective use of the available context budget.", "conclusion": "Allocating memory through a multimodal, layout-aware representation can substantially improve long-horizon reasoning under tight context budgets compared to standard text-serial memory. By rendering structured rich-text memory into images and training with budget-aware reinforcement learning, MemOCR can prioritize crucial information, aggressively compress less important details, and achieve better QA performance with more efficient context utilization."}}
{"id": "2601.21841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21841", "abs": "https://arxiv.org/abs/2601.21841", "authors": ["Xiang Li", "Ning Yan", "Masood Mortazavi"], "title": "Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.", "AI": {"tldr": "GiG is a planning framework for LLM-based embodied agents that uses a graph-in-graph memory plus bounded lookahead to improve long-horizon planning.", "motivation": "LLM-based embodied agents struggle with long-horizon planning because they must decompose high-level goals into sub-goals while respecting dynamic environmental constraints, but vanilla LLM planners lose coherence over long contexts and hallucinate invalid transitions.", "method": "GiG organizes an agent\u2019s experience into a Graph-in-Graph memory: a GNN encodes environmental states into embeddings, which are linked by actions into execution-trace graphs and stored in a memory bank. These trace-graph embeddings are clustered to enable retrieval of structure-aware priors for new situations. A bounded lookahead module, based on symbolic transition logic, then projects grounded future action sequences using these priors to guide planning.", "result": "On three embodied planning benchmarks (Robotouille Synchronous, Robotouille Asynchronous, ALFWorld), GiG surpasses state-of-the-art baselines by sizeable margins, with Pass@1 improvements up to 22% on Robotouille Synchronous, 37% on Robotouille Asynchronous, and 15% on ALFWorld, without increasing computational cost and sometimes reducing it.", "conclusion": "Structuring embodied agents\u2019 memory as a graph-in-graph plus adding symbolic, bounded lookahead yields more coherent, constraint-respecting long-horizon plans than standard LLM planners, demonstrating that graph-based, structure-aware memory and grounded transition models are effective for planning in dynamic environments."}}
{"id": "2601.21473", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21473", "abs": "https://arxiv.org/abs/2601.21473", "authors": ["Zaifeng Pan", "Yipeng Shen", "Zhengding Hu", "Zhuang Wang", "Aninda Manocha", "Zheng Wang", "Zhongkai Yu", "Yue Guan", "Yufei Ding"], "title": "ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management", "comment": null, "summary": "LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.", "AI": {"tldr": "They propose ScaleSim, a memory-efficient LLM serving system for multi-agent simulations that uses an abstraction called invocation distance to guide prefetching and eviction, improving scalability and speed.", "motivation": "LLM-based multi-agent simulations need many agents, each with its own GPU-resident states (model, caches, adapters). As the agent count grows, GPU memory is quickly exhausted, making such simulations hard to scale. Existing systems do not sufficiently exploit the observed properties of sparse agent activation and predictable/estimable invocation order to manage memory more efficiently.", "method": "They analyze representative multi-agent simulation workloads and identify two properties: (1) only a subset of agents are active at a time (sparse activation), and (2) the order in which agents will invoke LLMs can be estimated. They formalize a unified abstraction called invocation distance, which predicts the relative order/timing of future LLM requests across agents. On top of this abstraction, they design and implement ScaleSim, a serving system that: (a) uses invocation distance to proactively prefetch agent-specific states into GPU memory; (b) applies priority-based eviction of less-imminent agents\u2019 states; and (c) exposes a modular interface to plug in diverse per-agent memory types (models, prefix caches, adapters, etc.).", "result": "On simulation benchmarks, ScaleSim achieves up to 1.74x speedup over SGLang, a strong baseline LLM serving system, while supporting large-scale multi-agent simulations under constrained GPU memory. It successfully scales to more agents without exhausting device memory, and improves end-to-end simulation throughput and latency by better GPU memory utilization through prefetch/eviction guided by invocation distance.", "conclusion": "By exploiting sparsity in agent activation and the predictability of agent invocation order, the invocation distance abstraction enables more intelligent GPU memory management for multi-agent LLM simulations. ScaleSim demonstrates that proactively scheduling memory movement using this abstraction can significantly improve performance and scalability compared to existing systems, making large-scale LLM-based multi-agent simulations more practical on limited GPU resources."}}
{"id": "2601.21895", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21895", "abs": "https://arxiv.org/abs/2601.21895", "authors": ["Hongyi Zhou", "Jin Zhu", "Erhan Xu", "Kai Ye", "Ying Yang", "Chengchun Shi"], "title": "Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text", "comment": "Accepted by ICLR2026", "summary": "Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\\% to 80.6\\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).", "AI": {"tldr": "The paper proposes a new, theoretically grounded rewrite-based algorithm to detect LLM-generated text that outperforms existing detectors across many settings.", "motivation": "LLMs like GPT, Claude, and Gemini can generate human-like text, which raises concerns regarding misinformation and academic dishonesty. Existing detection methods, particularly rewrite-based ones, lack clear theoretical understanding and may not generalize well. There is a need for a principled framework explaining why rewrite-based detection works and for more effective detection algorithms that can adapt to different LLMs and text types.", "method": "The authors first develop a geometric perspective to explain how rewrite-based detection algorithms function, clarifying their underlying rationale and generalization properties. Using this geometric insight, they design a new rewrite-based detection method that adaptively learns a distance metric between the original text and its rewritten version instead of relying on a fixed distance. They then evaluate this adaptive-distance detector in extensive empirical experiments covering more than 100 different settings and multiple target LLMs (GPT, Claude, Gemini).", "result": "The adaptive-distance rewrite-based detector consistently outperforms baseline detection algorithms across most of the over 100 experimental configurations. Compared to the strongest existing baseline, the proposed method yields substantial relative performance gains, ranging from 57.8% to 80.6%, across different target LLMs such as GPT, Claude, and Gemini.", "conclusion": "A geometric understanding of rewrite-based detection clarifies why such methods can generalize, and leveraging this insight to learn an adaptive distance between original and rewritten text leads to a more powerful detector of LLM-generated content. Theoretical analysis shows adaptive distances are superior to fixed ones for detection, and experiments confirm strong, robust gains over state-of-the-art baselines across diverse LLMs and settings."}}
{"id": "2601.21927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21927", "abs": "https://arxiv.org/abs/2601.21927", "authors": ["Hong Chen", "Xiang Liu", "Bo Wang", "Yuxuan Fan", "Yuanlin Chu", "Zongluo Li", "Xiaowen Chu", "Xuming Hu"], "title": "SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching", "comment": null, "summary": "The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \\textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \\textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\\% and 50\\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\\% compared to full-context generation.", "AI": {"tldr": "SONIC is a learning-based KV cache compression framework that turns long dialogue history into a few semantic \u201cNexus\u201d tokens, maintaining multi-turn quality while using much less memory and speeding up inference.", "motivation": "Multi-turn LLMs suffer from linearly growing KV cache, which is a major bottleneck for long conversations. Existing cache compression/eviction methods are largely heuristic, do not exploit dialogue structure, and can accidentally discard crucial context, degrading response coherence and quality. There is a need for a principled, learnable mechanism that preserves essential semantic information under strict memory budgets while still being efficient and adaptable to different deployment constraints.", "method": "The paper introduces SONIC, a learning-based KV compression framework that replaces long historical segments with compact \u201cNexus\u201d tokens. SONIC learns to encode older dialogue segments into a small set of semantically rich tokens that stand in for the original KV states. It is trained with a dynamic budget mechanism so that, during training, the model experiences varying memory budgets and learns to allocate information accordingly. This enables the same trained model to adapt at inference time to different compression ratios without retraining, effectively preserving important dialogue information while minimizing KV cache size.", "result": "On four multi-turn benchmarks, under strong compression (e.g., keeping only 20% or 50% of the original KV cache), SONIC surpasses competitive baselines such as H2O and StreamingLLM. On MTBench101, it yields an average 35.55% improvement in score over state-of-the-art baselines at comparable compression levels. Additionally, SONIC improves deployment efficiency, achieving a 50.1% speedup in end-to-end inference compared with using the full uncompressed dialogue context.", "conclusion": "SONIC demonstrates that learning-based, structure-aware KV cache compression via Nexus tokens can significantly reduce memory usage and accelerate multi-turn LLM inference while preserving or even improving dialogue quality relative to heuristic schemes. Its dynamic budget training makes it flexible for deployment under varying memory constraints without the need for retraining, offering a practical solution to KV cache growth in real-world conversational LLM systems."}}
{"id": "2601.21955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21955", "abs": "https://arxiv.org/abs/2601.21955", "authors": ["Fariba Afrin Irany"], "title": "From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes", "comment": "This submission is a full-length research manuscript consisting of 37 pages and 15 figures. The paper presents a GPT-based architecture with selective fine-tuning for clinical text classification, including detailed architectural diagrams, learning curves, and evaluation figures such as ROC curves and confusion matrices", "summary": "The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.\n  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.\n  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.\n  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.", "AI": {"tldr": "They propose an efficient GPT-2\u2013based method for classifying long clinical texts, fine-tuning only a small subset of parameters, and show it works well on radiology reports while being much cheaper computationally.", "motivation": "Clinical notes in EHRs are rich but unstructured, and we need automated methods to extract diagnoses and findings. Existing large language models are powerful but expensive to adapt, and clinical text poses extra challenges: it is long, highly specialized, labels are scarce and imbalanced, and full fine-tuning of large models is computationally costly. The authors want a practical way to leverage pretrained generative models for clinical classification without incurring full fine-tuning cost.", "method": "They build a classification architecture on top of a pretrained GPT-2 (decoder-only Transformer). Instead of fine-tuning all parameters, they freeze most of the backbone and selectively fine-tune only the final Transformer block, the last layer normalization, and a small classification head. They apply this to radiology reports from MIMIC-IV-Note, using automatically derived CheXpert-style labels, and evaluate under multiple setups: multi-label prediction of radiographic findings, binary classification per label with different treatments of uncertainty, and prediction of aggregate disease outcomes.", "result": "Across tasks and dataset sizes, the selectively fine-tuned GPT-2 converges reliably and attains strong classification metrics. It performs particularly well in label regimes where most instances are non-mentions or explicit negations of findings, which are common in radiology reports. Despite training only a small fraction of parameters, the method maintains strong representational power and competitive performance.", "conclusion": "Selective fine-tuning of decoder-only pretrained language models like GPT-2 is an efficient and effective strategy for clinical text classification. By updating only later layers and a lightweight head, one can adapt powerful generative models to EHR-derived tasks with much lower computational cost while preserving performance, making large-scale deployment on real-world clinical narratives more feasible."}}
{"id": "2601.21505", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21505", "abs": "https://arxiv.org/abs/2601.21505", "authors": ["Diaoul\u00e9 Diallo", "Katharina Dworatzyk", "Sophie Jentzsch", "Peer Sch\u00fctt", "Sabine Theis", "Tobias Hecking"], "title": "The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation", "comment": null, "summary": "Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \\emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($\u03bb\\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($\u03b7_p^2 = 0.616$) and fear ($\u03b7_p^2 = 0.540$), and minimal effects for surprise ($\u03b7_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.", "AI": {"tldr": "The paper evaluates activation steering as a way to control the emotional tone of LLM outputs, showing via large-scale human ratings that moderate steering reliably amplifies target emotions while preserving text quality, and that automatic quality metrics align well with human judgments.", "motivation": "To control and align LLM behavior at inference time without costly prompt engineering or fine-tuning, especially for affective dimensions like emotional tone, and to validate with human subjects whether activation steering truly works and maintains text quality.", "method": "The authors apply activation steering\u2014directly modifying internal activations\u2014on LLMs (Alpaca and LLaMA-3) to target different emotions. They generate model outputs under varying steering strengths, then collect over 7,000 crowd-sourced ratings from 190 Prolific participants on perceived emotional intensity and overall quality. They also compare human quality ratings with automated model-based quality scores and analyze effects using correlation, effect sizes (\u03b7_p^2), significance tests (p-values), and inter-rater reliability (ICC).", "result": "Moderate steering strengths (around \u03bb\u22480.15) reliably increased the target emotional intensity while preserving comprehensibility and overall quality. The effects were strongest for disgust (\u03b7_p^2=0.616) and fear (\u03b7_p^2=0.540), and weakest for surprise (\u03b7_p^2=0.042). Human and automatic quality ratings correlated strongly (mean r=0.776, range 0.157\u20130.985). Upgrading from Alpaca to LLaMA-3 improved consistency and produced significant steering effects across emotions and strengths (all p<0.001). Inter-rater reliability was high (ICC=0.71\u20130.87).", "conclusion": "Activation steering is an effective and scalable method for controlling the emotional tone of LLM outputs at inference time. Moderate steering can enhance target emotions without degrading perceived quality, automatic metrics can stand in for human quality judgments, and newer base models like LLaMA-3 make steering more consistent and robust across emotions."}}
{"id": "2601.21968", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21968", "abs": "https://arxiv.org/abs/2601.21968", "authors": ["Jing Xiong", "Hui Shen", "Shansan Gong", "Yuxin Cheng", "Jianghan Shen", "Chaofan Tao", "Haochen Tan", "Haoli Bai", "Lifeng Shang", "Ngai Wong"], "title": "OVD: On-policy Verbal Distillation", "comment": "Technical Report", "summary": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io", "AI": {"tldr": "The paper proposes On-policy Verbal Distillation (OVD), a memory-efficient knowledge distillation framework that uses discrete verbal scores from a teacher instead of token-level probability matching, improving performance and efficiency on web QA and math reasoning tasks.", "motivation": "Existing on-policy knowledge distillation for reasoning tasks typically relies on token-level alignment between teacher and student models. This imposes strong constraints: it limits the student's ability to explore alternative reasoning paths, hinders effective use of interactive environment feedback (e.g., in RL settings), and leads to severe memory bottlenecks due to storing token-level probabilities and long trajectories. The authors aim to enable efficient on-policy distillation that avoids these limitations while retaining or improving the transfer of reasoning ability from large teachers to smaller students.", "method": "The authors introduce On-policy Verbal Distillation (OVD), where instead of distilling token-level probability distributions, the teacher provides discrete verbal scores (0\u20139) that evaluate entire trajectories or outputs produced by the student. The student explores the output space freely, generating trajectories in an on-policy manner. The teacher then assigns a scalar verbal score to each trajectory, which the student uses as a learning signal, effectively matching trajectories rather than token-level alignments. This approach significantly reduces memory requirements and decouples training from strict token-level teacher-student alignment, while remaining compatible with reinforcement-learning-style training and interactive environments.", "result": "In experiments on web question answering and mathematical reasoning benchmarks, OVD substantially outperforms prior token-level on-policy distillation methods. The paper reports up to +12.9% absolute improvement in average exact match (EM) on web QA tasks, and up to +25.7% performance gains on math benchmarks\u2014even when trained with only one random sample per instance. Additionally, OVD demonstrates better training efficiency and reduced memory usage compared to existing approaches.", "conclusion": "OVD provides an effective and memory-efficient alternative to token-level on-policy knowledge distillation for reasoning tasks. By leveraging discrete verbal trajectory-level scores from teacher models, it allows students to explore more freely, integrates well with interactive and RL-style feedback, and achieves significant performance gains on web QA and math reasoning benchmarks while improving training efficiency. The work suggests that verbal feedback-based trajectory distillation is a promising direction for transferring reasoning capabilities from large teachers to smaller, more efficient student models."}}
{"id": "2601.21511", "categories": ["cs.AI", "cs.NE", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21511", "abs": "https://arxiv.org/abs/2601.21511", "authors": ["Niki van Stein", "Anna V. Kononova", "Lars Kotthoff", "Thomas B\u00e4ck"], "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI", "comment": "14 pages", "summary": "Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.\n  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.", "AI": {"tldr": "They enhance evolutionary large-language-model-based automated algorithm design by adding code-structure-aware guidance extracted from abstract syntax trees, improving performance and sample efficiency.", "motivation": "Existing LLM-based automated algorithm design systems like LLaMEA explore the space of optimization algorithms using only fitness scores from executed code. This ignores rich structural information in the generated programs, making search less data-efficient and offering little interpretability about how code patterns relate to performance. The authors aim to exploit code-level structure to guide search, improving efficiency and transparency without over-constraining the creativity of the LLM.", "method": "They analyze the abstract syntax trees (ASTs) of generated optimization algorithms and compute graph-theoretic and computational-complexity features. A surrogate model is trained on an archive of evaluated algorithms to predict performance from these features. Using explainable AI techniques (e.g., feature importance), they discover which structural properties most strongly influence performance. These insights are turned into natural-language mutation instructions that are fed back to the LLM within the LLaMEA evolutionary framework, steering subsequent code generation toward promising regions of the design space. This integrated system is called LLaMEA-SAGE.", "result": "In controlled small-scale benchmarks, LLaMEA-SAGE reaches the performance levels of vanilla LLaMEA with fewer evaluations, indicating faster search and better sample efficiency. In a larger-scale evaluation on the MA-BBOB benchmark suite from the GECCO-MA-BBOB competition, the guided approach outperforms state-of-the-art automated algorithm design methods overall, demonstrating that structurally informed guidance leads to better optimization algorithms.", "conclusion": "Incorporating graph- and complexity-based features from algorithm ASTs into a surrogate-guided, explainable feedback loop allows evolutionary LLM-based automated algorithm design to be more efficient and more effective. Code-derived structural signals can successfully bias LLM-driven algorithm evolution while preserving expressivity, thereby connecting low-level program structure with human-understandable performance feedback and advancing the capabilities of automated algorithm design systems."}}
{"id": "2601.21969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21969", "abs": "https://arxiv.org/abs/2601.21969", "authors": ["Yifan Zhu", "Huiqiang Rong", "Haoran Luo"], "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding", "comment": "26 pages and 11 figures,this work has been accepted for presentation at ICLR 2026", "summary": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.", "AI": {"tldr": "Token-Guard is a lightweight, decoding-time method that detects and corrects hallucinated tokens in LLM outputs via internal self-checking and latent-space risk scoring.", "motivation": "LLMs frequently hallucinate, and current mitigation strategies like RAG and RLHF either require heavy retrieval infrastructures or costly large-scale fine-tuning. Decoding-based approaches are cheaper but do not offer explicit, fine-grained control over hallucinations. The paper aims to design a scalable, modular mechanism that can be plugged into existing models to actively control and reduce hallucinations without retraining or external knowledge bases.", "method": "The authors introduce Token-Guard, a self-checking decoding framework operating at the token level. During generation, each intermediate reasoning step is internally verified to identify potentially hallucinated tokens before they influence subsequent tokens. Suspicious candidate fragments are projected into a latent space where an explicit hallucination risk score is computed. Based on this score, Token-Guard applies iterative pruning of high-risk content and regenerates safer alternatives, dynamically steering the decoding process away from hallucinations.", "result": "On various HALU benchmark datasets for hallucination evaluation, Token-Guard significantly lowers hallucination rates and increases answer correctness compared to baseline decoding methods. It achieves these gains without the overhead of retrieval or full-model fine-tuning, showing good scalability and compatibility with existing LLMs.", "conclusion": "Token-Guard provides an effective, modular, and computationally efficient approach for hallucination control in LLMs by integrating token-level self-verification and latent-space risk scoring directly into the decoding process. This approach improves reliability and accuracy of generated text and can be readily adopted in practical systems, as evidenced by the released implementation."}}
{"id": "2601.21526", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21526", "abs": "https://arxiv.org/abs/2601.21526", "authors": ["Alireza Nadaf", "Alireza Mohammadshahi", "Majid Yazdani"], "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization", "comment": null, "summary": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso", "AI": {"tldr": "KAPSO is a modular framework that uses iterative experimentation, knowledge integration, and memory to autonomously synthesize and optimize programs toward measurable goals.", "motivation": "Existing coding agents struggle with long-horizon tasks: they lose experimental state, have brittle debugging processes, and fail to effectively reuse domain expertise across attempts. Synthesis is often treated as a one-shot endpoint instead of part of an iterative optimization process. The authors aim to build a system that can reliably tackle more complex programming problems over multiple iterations by preserving state, organizing knowledge, and learning from past attempts.", "method": "KAPSO frames autonomous programming as a long-horizon optimization loop over evaluators. Given a natural language goal and evaluation method, it iteratively performs ideation, code synthesis/editing, execution, and evaluation, then updates an internal learning/memory system to guide the next iteration. The framework has three main components: (1) a git-native experimentation engine that isolates each attempt in its own branch, preserving provenance and reproducibility; (2) a knowledge system that ingests heterogeneous sources (repos, internal playbooks, documentation, papers, web data) and structures them for retrieval over workflows, implementations, and environment constraints; and (3) a cognitive memory layer that coordinates retrieval and builds an episodic store of reusable lessons distilled from experiment traces (logs, diffs, feedback), which is used to avoid repeated errors and improve convergence.", "result": "KAPSO is evaluated on two benchmarks: MLE-Bench, which consists of Kaggle-style machine learning competitions, and ALE-Bench, which uses AtCoder heuristic optimization tasks. The paper reports end-to-end performance on these benchmarks, demonstrating KAPSO\u2019s capability as an autonomous programming and optimization system, though the abstract does not provide exact numerical results.", "conclusion": "Treating code synthesis as one operator inside a longer optimization loop, and combining it with structured experimentation, integrated knowledge, and cognitive memory, leads to a more robust autonomous programming framework. KAPSO addresses typical long-horizon failures of coding agents and shows practical effectiveness on ML and heuristic optimization benchmarks, indicating its promise for complex, iterative programming tasks."}}
{"id": "2601.21996", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21996", "abs": "https://arxiv.org/abs/2601.21996", "authors": ["Jianhui Chen", "Yuzhang Luo", "Liangming Pan"], "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units", "comment": null, "summary": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.", "AI": {"tldr": "The paper introduces Mechanistic Data Attribution (MDA), a method to trace specific interpretable circuits in language models back to the particular training samples that caused them, and shows that manipulating those samples can control circuit formation and model capabilities.", "motivation": "Mechanistic interpretability has discovered specific attention heads and circuits with clear functions in large language models, but it is unclear which parts of the training data are responsible for these mechanisms emerging. Without a way to connect circuits to data, it's hard to causally test hypotheses about how and why such mechanisms form, or to intentionally steer model training to produce or avoid certain circuits.", "method": "The authors propose Mechanistic Data Attribution (MDA), which uses Influence Functions to estimate how individual training examples affect specific mechanistic features, such as attention heads identified as interpretable. They apply this to the Pythia model family: (1) identify interpretable heads and circuits, (2) compute per-sample influence scores for these units, (3) perform targeted interventions on high-influence samples (removing or augmenting them) and compare with random interventions, and (4) track how these interventions affect circuit emergence, especially induction heads and their associated in-context learning behavior. They also derive a data augmentation pipeline that enriches mechanistically relevant patterns in the training data to accelerate circuit formation.", "result": "Targeted removal or amplification of a small set of high-influence training samples substantially changes whether and how clearly specific interpretable heads emerge, while random data manipulations of similar size have negligible effect. Repetitive structural data like LaTeX and XML is identified as disproportionately influential in forming certain circuits, acting as a 'mechanistic catalyst.' Interventions aimed at induction heads not only modify those heads but also predictably alter the model\u2019s in-context learning performance. Additionally, the proposed mechanistic data augmentation speeds up the convergence of desired circuits across different model sizes.", "conclusion": "The study demonstrates a concrete causal link between specific training samples and the emergence of mechanistic circuits in language models. By showing that manipulating a small, carefully chosen subset of data can reliably control interpretability-relevant heads and associated behaviors such as in-context learning, the paper validates long-standing mechanistic hypotheses and offers a practical tool for steering model development. Mechanistic Data Attribution thus bridges mechanistic interpretability with data-centric training interventions, enabling principled circuit-level control via training data design."}}
{"id": "2601.21533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21533", "abs": "https://arxiv.org/abs/2601.21533", "authors": ["Youngjin Jin", "Hanna Kim", "Kwanwoo Kim", "Chanhee Lee", "Seungwon Shin"], "title": "ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making", "comment": "58 pages", "summary": "Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.", "AI": {"tldr": "ARGORA is a framework that structures multi-expert LLM debates into explicit argumentation graphs, treats them as causal models, and uses them for both causal diagnostics and corrective aggregation of expert opinions.", "motivation": "Current multi-expert LLM setups aggregate different expert outputs in a way that hides which specific arguments or reasoning paths influenced the final decision. This opacity makes it hard to understand, debug, or improve the system\u2019s reasoning, and limits our ability to know which arguments were truly decisive or how changes in reasoning would affect outcomes. There is also a gap between the system\u2019s internal reasoning and external correctness judgments, and existing methods lack a principled way to reconcile these when they diverge.", "method": "The paper proposes ARGORA, a framework that: (1) has multiple LLM experts generate arguments and counterarguments; (2) organizes these into an explicit argumentation graph where nodes are arguments and edges represent support or attack relations; (3) interprets this graph as a causal model so that one can systematically remove or modify specific arguments and recompute the final decision to assess their causal influence; and (4) adds a correction mechanism that, when model decisions conflict with external judgments or ground truth, identifies and adjusts the internal reasoning chains or argument weights to realign conclusions. The framework is tested on several benchmarks and an open-ended task.", "result": "ARGORA matches or approaches the accuracy of strong multi-expert baselines while adding interpretability via causal diagnostics over arguments. In evaluation, when experts initially disagree, ARGORA\u2019s mechanism for resolving disputes tends to move answers toward correctness more often than it causes new mistakes. It successfully identifies decisive arguments and shows how targeted removals or edits of arguments would change outcomes, illustrating its ability to analyze and edit reasoning processes.", "conclusion": "ARGORA demonstrates that structuring multi-expert LLM reasoning as explicit, causally-analyzable argumentation graphs can provide both competitive performance and greater transparency. By enabling systematic intervention on individual arguments and incorporating a correction mechanism that better aligns model reasoning with correct judgments, the framework improves dispute resolution among experts and offers actionable diagnostics about which reasoning chains truly matter for decisions."}}
{"id": "2601.22025", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22025", "abs": "https://arxiv.org/abs/2601.22025", "authors": ["Daniel Commey"], "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications", "comment": null, "summary": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.\n  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.\n  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.\n  All test suites, harnesses, and results are included for reproducibility.", "AI": {"tldr": "The paper proposes a practical, repeatable workflow and a minimal evaluation suite for testing LLM applications, showing that generic \u201cbetter\u201d prompts can hurt certain behaviors, which underscores the need for evaluation-driven iteration instead of relying on universal prompt tips.", "motivation": "LLM apps are hard to evaluate because their outputs are stochastic, high-dimensional, and heavily influenced by prompts and model changes, unlike traditional deterministic software. Practitioners lack a clear, lightweight, and standardized way to systematically test LLM behavior across common application patterns such as vanilla chat, RAG, and tool-using agents. The authors aim to make LLM evaluation more repeatable, engineering-focused, and grounded in empirical feedback rather than ad-hoc prompt tweaks.", "method": "They propose an evaluation-driven workflow summarized as Define, Test, Diagnose, Fix, turning LLM development into an iterative engineering loop. They design a Minimum Viable Evaluation Suite (MVES) with tiered evaluation components tailored for three major classes of LLM apps: general LLM applications, RAG systems, and agentic/tool-use workflows. They unify and categorize evaluation methods (automated checks, human rubrics, and LLM-as-judge approaches) while explicitly documenting known failure modes of LLM-based judges. They run local, reproducible experiments using Ollama with Llama 3 8B Instruct and Qwen 2.5 7B Instruct, comparing task-specific prompts to a generic, \u201cimproved\u201d prompt template on structured test suites for extraction and RAG compliance, alongside instruction-following behavior.", "result": "In their experiments, replacing task-specific prompts with a generic improved prompt template led to measurable trade-offs. For Llama 3 8B Instruct, extraction pass rate dropped from 100% to 90% and RAG compliance fell from 93.3% to 80%, even though instruction-following behavior improved. This demonstrates that generic prompt optimizations can degrade performance on specific tasks, and that evaluation suites can reveal such regressions. All test suites, evaluation harnesses, and experiment outputs are released to enable others to reproduce and extend the results.", "conclusion": "The authors conclude that LLM application development should be anchored in an evaluation-driven loop rather than informal prompt engineering or generic best-practice recipes. Their MVES provides a lightweight, structured starting point for evaluating core classes of LLM applications, while the experiments show that seemingly better generic prompts can incur hidden performance costs. Therefore, developers should iteratively refine prompts and system designs based on systematic evaluation, carefully calibrating claims about improvements and avoiding one-size-fits-all prompt strategies."}}
{"id": "2601.21545", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21545", "abs": "https://arxiv.org/abs/2601.21545", "authors": ["Yang Zhao", "Chengxiao Dai", "Yue Xiu", "Mengying Kou", "Yuliang Zheng", "Dusit Niyato"], "title": "ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory", "comment": null, "summary": "Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\\mathrm{probe}}$ shards via Top-$B_{\\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).", "AI": {"tldr": "Introduces ShardMemo, a tiered, sharded memory service for agentic LLMs that uses learned, budgeted MoE-style routing over constrained shard sets to improve retrieval quality, cost, and latency.", "motivation": "Agentic LLM systems need scalable external memory for long-horizon tasks and concurrent multi-agent execution, but existing centralized or heuristically partitioned memory indexes become performance and scalability bottlenecks as memory size and parallel access grow. There is a need for a memory architecture that scales with many agents and large evidence stores while keeping retrieval accurate, efficient, and budget-controlled.", "method": "Design a three-tier memory service: Tier A holds per-agent working state; Tier B stores sharded evidence, each shard with its own ANN index; Tier C maintains a versioned skill library. Tier B enforces a scope-before-routing mechanism that first masks ineligible shards via structured eligibility constraints, then treats shard selection as a masked mixture-of-experts routing problem over the remaining shards. A router, trained using evidence-to-shard supervision, selects up to B_probe shards using either Top-B_probe or adaptive Top-P strategies and employs cost-aware gating over different shard families (profile, observation, session). This method replaces naive similarity-based or centralized routing with learned, budgeted routing that respects shard eligibility and resource constraints. Tier C uses specialized retrieval to select skills/tools effectively.", "result": "On the LoCoMo benchmark, ShardMemo outperforms the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting with B_probe=3, it achieves +6.87 F1 improvement over cosine-to-prototype shard routing, while reducing retrieval work (VecScan from 521 to 414, a 20.5% reduction) and p95 latency (95 ms to 76 ms). On long-context HotpotQA, it attains F1 scores of 63.41, 61.88, and 57.95 at 56K, 224K, and 448K tokens respectively. On ToolBench, the Tier C skill library achieves 0.97 Precision@3 and 1.94 StepRed, representing +10.2% and +7.2% improvements over embedding-similarity retrieval.", "conclusion": "A tiered, sharded, and budgeted memory architecture with learned, scope-constrained MoE-style shard routing can significantly improve both effectiveness and efficiency for memory-augmented, agentic LLM systems. ShardMemo scales better than centralized or heuristic approaches, yields higher F1 scores on multi-hop and long-context benchmarks, and reduces retrieval cost and latency, while its Tier C design enhances skill/tool retrieval quality compared to standard embedding-based methods."}}
{"id": "2601.22031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22031", "abs": "https://arxiv.org/abs/2601.22031", "authors": ["Junhao Ruan", "Bei Li", "Yongjing Yin", "Pengcheng Huang", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Tong Xiao", "JingBo Zhu"], "title": "Causal Autoregressive Diffusion Language Model", "comment": null, "summary": "In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.", "AI": {"tldr": "CARD is a new framework that combines the training efficiency of autoregressive models with the fast, parallel inference of diffusion models for large language models.", "motivation": "Autoregressive models (ARMs) are data and training efficient but decode tokens sequentially and slowly, while diffusion models enable high-throughput, parallel generation but are typically more expensive and unstable to train, especially for discrete tokens. There is a need for a unified approach that keeps ARM-like efficiency while gaining diffusion-style parallel decoding for next-generation efficient LLMs.", "method": "CARD reformulates the diffusion process using a strictly causal attention mask so that each token prediction only depends on past tokens, allowing dense per-token supervision in a single forward pass. To stabilize training of this causal diffusion, it introduces (1) a soft-tailed masking scheme that preserves local context instead of hard masking, and (2) a context-aware reweighting of training losses grounded in signal-to-noise ratio principles. For inference, it supports dynamic parallel decoding by using KV-caching and token-level confidence to generate variable-length token chunks in parallel rather than one-by-one.", "result": "CARD empirically outperforms existing discrete diffusion baselines in quality while cutting training latency by about 3\u00d7 relative to block diffusion methods. It matches or approaches autoregressive models in data efficiency while also enabling more parallel generation at inference time.", "conclusion": "By combining causal masking, stabilized diffusion training techniques, and confidence-based parallel decoding, CARD offers a robust framework that retains the data efficiency of autoregressive training and the latency advantages of diffusion-style parallel generation. This positions CARD as a promising paradigm for building more efficient large language models that train faster and decode in parallel."}}
{"id": "2601.21557", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21557", "abs": "https://arxiv.org/abs/2601.21557", "authors": ["Haoran Ye", "Xuning He", "Vincent Arak", "Haonan Dong", "Guojie Song"], "title": "Meta Context Engineering via Agentic Skill Evolution", "comment": "46 pages, 4 figures", "summary": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.", "AI": {"tldr": "The paper proposes Meta Context Engineering (MCE), a bi-level framework that automatically learns and evolves context-engineering strategies and artifacts for large language models, outperforming existing manually designed context engineering methods across multiple domains.", "motivation": "Existing Context Engineering (CE) for large language models depends on manually designed prompts, workflows, and schemas. These are rigid, impose structural biases, and limit optimization to what humans can intuitively design. There is a need for a more flexible, automated way to discover and refine both the strategies (skills) and the concrete context artifacts used at inference time, so models can adapt more effectively across tasks and domains.", "method": "The authors introduce Meta Context Engineering (MCE), a bi-level agent framework. At the meta level, an agent maintains and refines a population of CE skills using an \"agentic crossover\" process that performs deliberative search over historical skills, their executions, and evaluation outcomes. At the base level, an agent applies these skills to construct and optimize contexts represented as flexible files and code, learning from training rollouts. The system co-evolves both the engineering skills and the actual context artifacts, under both offline and online settings, instead of relying on fixed CE heuristics or rigid workflows.", "result": "Across five diverse domains, MCE consistently achieves better performance than state-of-the-art agentic CE baselines. The reported relative improvements range from 5.6% to 53.8%, with an average of 16.9%. In addition to higher task performance, MCE shows better adaptability of contexts to new situations, improved transferability across domains, and greater efficiency in context usage and training cost.", "conclusion": "Meta Context Engineering can replace static, manually designed CE workflows with an automated, bi-level framework that co-evolves context-engineering strategies and artifacts. This leads to significant and consistent gains over prior agentic CE approaches in performance, adaptability, transferability, and efficiency, indicating that CE itself can be treated as a learnable, optimizable process rather than a fixed design choice."}}
{"id": "2601.21570", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21570", "abs": "https://arxiv.org/abs/2601.21570", "authors": ["Zixing Lei", "Genjia Liu", "Yuanshuo Zhang", "Qipeng Liu", "Chuan Wen", "Shanghang Zhang", "Wenzhao Lian", "Siheng Chen"], "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots", "comment": "37 pages, 13 figures", "summary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.", "AI": {"tldr": "EmboCoach-Bench is a benchmark to test if LLM agents can autonomously design and optimize policies for embodied AI tasks via code-based, closed-loop interaction with simulators.", "motivation": "Scaling embodied AI with reinforcement and imitation learning is limited by heavy manual effort in reward shaping, hyperparameter tuning, and handling diverse simulation backends. The authors want to see whether LLM agents, which have shown strong capabilities in automation and scientific discovery, can take over this engineering workload and autonomously build high-performing embodied policies.", "method": "They build EmboCoach-Bench, a benchmark of 32 expert-curated RL and IL tasks, all interfaced through executable code. LLM agents interact in a dynamic closed-loop workflow: they write and modify code, receive environment feedback from simulations, and iteratively debug and optimize policies. The framework evaluates capabilities ranging from designing physics-informed rewards to selecting and implementing advanced policy architectures like diffusion policies. Multiple LLMs (open-source and proprietary) are evaluated under this agentic setup.", "result": "Across tasks, autonomous LLM agents improve over human-engineered baselines by an average of 26.5% in success rate. Using environment feedback in an agentic workflow substantially boosts policy quality and reduces the performance difference between open-source and proprietary models. The agents can also identify and fix pathological engineering decisions, turning near-failing tasks into successful ones through iterative simulation-in-the-loop debugging.", "conclusion": "Executable-code-centered, closed-loop LLM agents can autonomously engineer embodied AI policies, often surpassing human baselines and robustly recovering from poor initial designs. EmboCoach-Bench provides a standardized way to measure these capabilities and supports a move from manual, expert-heavy tuning toward scalable, self-improving embodied intelligence systems."}}
{"id": "2601.22040", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22040", "abs": "https://arxiv.org/abs/2601.22040", "authors": ["Reza T. Batley", "Sourav Saha"], "title": "A Separable Architecture for Continuous Token Representation in Language Models", "comment": null, "summary": "Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \\times$ more parameters.", "AI": {"tldr": "Proposes Leviathan, a small language model architecture that replaces large embedding tables with a continuous embedding generator, improving performance under equal parameter budgets.", "motivation": "In small language models, a disproportionate share of parameters is spent on token embedding matrices, which may be an inefficient use of capacity. Existing scaling-law analyses treat parameters as interchangeable and overlook this structural inefficiency, motivating a reallocation of parameters away from embeddings while keeping total parameter count fixed.", "method": "Design an alternative architecture, Leviathan, that uses a continuous embedding generator instead of standard discrete embedding lookup tables. Train and evaluate Leviathan and a LLaMA-style baseline on the Pile dataset under isoparametric conditions (same total parameter count). Fit empirical power laws relating loss and compute/parameters to estimate Leviathan\u2019s effective parameter capacity.", "result": "Under equal parameter budgets on the Pile, Leviathan consistently outperforms a standard LLaMA-style model. Power-law fits indicate Leviathan behaves like a denser model, effectively having between 1.47\u00d7 and 2.11\u00d7 more usable parameters than its nominal parameter count suggests.", "conclusion": "Reallocating parameters away from large embedding tables toward a continuous embedding generator yields substantially better use of a fixed parameter budget in sub-billion-parameter language models. Leviathan\u2019s architecture demonstrates higher effective parameter capacity and challenges the assumption that all parameters are equally valuable in scaling-law analyses."}}
{"id": "2601.21576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21576", "abs": "https://arxiv.org/abs/2601.21576", "authors": ["Juncai Li", "Ru Li", "Yuxiang Zhou", "Boxiang Ma", "Jeff Z. Pan"], "title": "Chain Of Thought Compression: A Theoritical Analysis", "comment": null, "summary": "Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.", "AI": {"tldr": "The paper theoretically and empirically studies how to compress chain-of-thought reasoning into latent states without explicit intermediate tokens, identifies why this is difficult for high-order logic, and proposes a framework (ALiCoT) that recovers efficiency comparable to full CoT at much lower token cost.", "motivation": "Chain-of-Thought reasoning improves LLM reasoning but is computationally expensive because it generates many intermediate tokens. Prior work shows implicit CoT (compressing reasoning into latent states) can save tokens, but it is not well understood why and when such compression is learnable or fails, especially for complex logical dependencies. The paper is motivated by the need for a principled understanding and a more effective method to get CoT-level performance with implicit, token-efficient reasoning.", "method": "1) They introduce the concept of Order-r Interaction to formalize high-order logical dependencies in reasoning tasks. Using this, they provide a theoretical analysis showing that the learning signal for high-order interactions decays exponentially when training models to \"skip\" explicit intermediate steps, making implicit CoT hard to learn for irreducible logical problems. 2) They design NatBool-DAG, a benchmark based on Boolean reasoning over DAG structures that enforces irreducible logical reasoning and removes semantic shortcuts, to empirically test their theory. 3) Based on their analysis, they propose ALiCoT (Aligned Implicit CoT), a training framework that aligns latent token representations with intermediate reasoning states, effectively restoring stronger learning signals for high-order interactions while still avoiding explicit tokenized CoT.", "result": "Theoretically, they show that directly learning to map from inputs to final answers without explicit reasoning steps faces an exponential signal decay for high-order interactions, creating a \"high-order interaction barrier\" when intermediate steps are skipped. Empirically, on the new NatBool-DAG benchmark and other settings, their proposed ALiCoT framework significantly improves the ability of LLMs to perform implicit CoT reasoning. ALiCoT achieves similar performance to explicit CoT while providing about a 54.4x speedup in inference, demonstrating that aligned implicit reasoning can be both accurate and highly token-efficient.", "conclusion": "Implicit CoT is fundamentally challenging for tasks with irreducible high-order logical dependencies because the learning signal for these dependencies decays exponentially when skipping explicit steps. However, by explicitly aligning latent representations with intermediate reasoning states (as in ALiCoT), models can overcome this high-order interaction barrier and realize efficient, token-economical reasoning. The work also offers NatBool-DAG as a rigorous benchmark for studying such reasoning, and it suggests a principled path toward fast, CoT-level reasoning without the overhead of generating full textual chains of thought."}}
{"id": "2601.22047", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22047", "abs": "https://arxiv.org/abs/2601.22047", "authors": ["Yunjia Qi", "Hao Peng", "Xintong Shi", "Amy Xin", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "On the Paradoxical Interference between Instruction-Following and Task Solving", "comment": null, "summary": "Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research", "AI": {"tldr": "The paper shows that adding even trivial, already-satisfied constraints to instructions can significantly hurt LLM task performance, and introduces SUSTAINSCORE to quantify this interference between instruction following and task solving.", "motivation": "Instruction tuning is used to align LLMs with human intent, but its side effects on core problem-solving capabilities are not well understood. The authors suspect that emphasizing instruction following may sometimes conflict with or degrade raw task performance, and they want a systematic way to measure and analyze this interference across models, tasks, and alignment methods.", "method": "The authors define SUSTAINSCORE: for a given task where a model already produces a correct answer, they extract a self-evident constraint from that successful output (e.g., a property the answer already satisfies) and insert this constraint back into the instruction. They then re-run the model on the modified instruction and measure the performance drop. They apply this procedure across multiple tasks (mathematics, multi-hop QA, code generation), multiple constraint types and scales, and multiple LLMs, including advanced models like Claude-Sonnet-4.5. They further analyze attention patterns to compare how models process constraints in successful vs. failed cases, and they compare models trained under different post-training/alignment paradigms to see how these affect interference levels.", "result": "Across tasks and models, adding self-evident constraints\u2014constraints that the original correct output already satisfied\u2014often causes substantial drops in performance. The interference effect is robust across different constraint types and magnitudes. Attention analysis shows that failures tend to over-focus on the added constraints relative to successful runs. Different post-training paradigms show different susceptibility levels to this interference, revealing empirical differences in how alignment strategies trade off between instruction adherence and task competence.", "conclusion": "Instruction following can actively interfere with LLM task solving, even when the constraints are trivial and redundant with an already-correct solution. SUSTAINSCORE provides a quantitative way to capture this interference and reveals that current alignment strategies may over-emphasize constraints in ways that degrade problem-solving. The paper suggests that future alignment methods should explicitly account for and mitigate this interference, and it provides code and data to support further research on balancing instruction adherence with robust task performance."}}
{"id": "2601.21582", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21582", "abs": "https://arxiv.org/abs/2601.21582", "authors": ["Jonas Knupp", "Jan Hendrik Metzen", "Jeremias Bohn", "Georg Groh", "Kristian Kersting"], "title": "Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves", "comment": null, "summary": "Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.", "AI": {"tldr": "Proposes Dreamer, a modular depth-recurrent attention mixture that improves latent reasoning efficiency, achieving same or better accuracy with far fewer training tokens than matched SOTA models.", "motivation": "Depth-recurrent models can perform latent reasoning by reusing parameters across depths, but existing approaches suffer from three key issues: (1) the lack of fair baselines that are simultaneously matched in FLOPs, parameters, and memory; (2) underuse of depth-recurrence because architectures rely on partially fixed, non-recurrent layer stacks; and (3) a hidden-size bottleneck, where a constant hidden dimension limits the capacity needed for many-step internal reasoning. The authors aim to unlock the full potential of depth-recurrence while keeping scaling efficient and practical.", "method": "Introduce Dreamer, a modular framework that constructs depth-recurrent attention mixtures. It combines three types of attention: (a) sequence attention (standard along-token attention), (b) depth attention (attention along the depth dimension, i.e., across recurrent steps), and (c) sparse expert attention (mixture-of-experts). By attending along depth, the model can bypass the constant hidden-size bottleneck; by mixing attention types and using sparse experts, it decouples key scaling dimensions (depth, width, and experts) and enables efficient reuse of parameters across many recurrent steps. The framework is evaluated under FLOP-, parameter-, and memory-matched conditions against state-of-the-art baselines.", "result": "On language reasoning benchmarks, Dreamer models reach the same accuracy as strong baselines while using 2\u20138x fewer training tokens, under matched FLOPs, parameter counts, and memory budgets. In token-matched comparisons, Dreamer outperforms state-of-the-art models that are roughly twice as large. Analysis of expert routing shows Dreamer achieves 2\u201311x higher diversity in expert selection than existing MoE approaches, indicating richer and more varied knowledge usage across depths.", "conclusion": "Depth-recurrent attention mixtures can substantially improve the efficiency of latent reasoning in language models. By adding depth attention and modular sparse experts, Dreamer removes the hidden-size bottleneck and decouples scaling dimensions, enabling deeper recurrent computation without proportional increases in parameters or memory. The approach yields large token-efficiency gains and stronger performance than larger SOTA models, while also revealing that depth recurrence promotes more diverse and structured expert usage across layers. This suggests depth-recurrence is a powerful and practical scaling axis for future language model design."}}
{"id": "2601.22050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22050", "abs": "https://arxiv.org/abs/2601.22050", "authors": ["Ghazal Kalhor", "Behnam Bahrak"], "title": "MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs", "comment": null, "summary": "In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.", "AI": {"tldr": "The paper introduces MasalBench, a benchmark to evaluate multilingual LLMs\u2019 understanding of Persian proverbs and their English equivalents, revealing strong in-language proverb recognition but weaker cross-cultural mapping.", "motivation": "Multilingual LLMs are widely used in daily conversational settings, so they must handle figurative and culturally grounded language. Prior evaluations focus mostly on high-resource languages and overlook low-resource ones like Persian, despite the importance of proverbs in everyday conversation. The authors aim to fill this gap and systematically test whether current LLMs truly understand and can transfer proverb meaning across cultures and languages.", "method": "The authors construct MasalBench, a benchmark centered on Persian proverbs that appear in contextualized settings. The tasks include (1) detecting whether a given span is a Persian proverb in context and (2) choosing the correct English proverb that is equivalent in meaning. They then evaluate eight state-of-the-art LLMs on these tasks, measuring accuracy for both in-language proverb identification and cross-lingual, cross-cultural mapping to English equivalents.", "result": "All eight LLMs achieve high accuracy (above 0.90) at identifying Persian proverbs in context, indicating strong pattern recognition and surface-level proverb knowledge. However, accuracy drops substantially for the task of selecting the equivalent English proverb, with the best-performing model reaching only 0.79 accuracy, suggesting difficulty with deep cultural knowledge and analogical mapping across languages.", "conclusion": "The study concludes that while current multilingual LLMs handle recognition of Persian proverbs well, they fall short in cross-cultural understanding and analogical reasoning needed to align Persian proverbs with their English counterparts. MasalBench exposes these limitations and offers a reusable framework for evaluating cultural and contextual understanding in other low-resource languages, pointing to the need for improved training or adaptation methods for culturally grounded reasoning."}}
{"id": "2601.21598", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21598", "abs": "https://arxiv.org/abs/2601.21598", "authors": ["Zhi Zheng", "Wee Sun Lee"], "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning", "comment": null, "summary": "Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \\underline{A}c\\underline{t}ive Latent \\underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.", "AI": {"tldr": "The paper introduces ATP-Latent, an active planning framework for latent chain-of-thought reasoning in LLMs that uses a conditional VAE and RL with an auxiliary coherence reward to learn smoother, more effective latent spaces and policies, improving accuracy while reducing token usage.", "motivation": "Latent chain-of-thought reasoning aims to replace long, discrete textual explanations with compact continuous latent tokens, reducing token costs and enabling dense planning. However, existing methods supervise latent tokens by simply imitating one available language CoT, even though many equivalent CoTs exist for each question. This passive imitation can produce suboptimal latent representations and reasoning policies and creates a train-test mismatch, limiting planning ability and performance. The authors want to make latent reasoning more robust, expressive, and efficient by actively shaping the latent space and reasoning policy rather than just copying a single CoT label.", "method": "The authors propose ATP-Latent (Active Latent Planning). First, they model the supervision of latent tokens with a conditional variational auto-encoder (VAE), conditioning on the problem so as to learn a smoother, more structured latent space that can capture multiple valid CoTs. Then, they perform reinforcement learning over the latent reasoning policy, using task rewards combined with an auxiliary coherence reward. This coherence reward is computed from the consistency between the VAE-decoded natural language contents of the latent tokens, guiding the RL process toward coherent, well-planned latent trajectories. The overall framework fine-tunes an LLaMA-1B backbone to perform latent CoT reasoning more efficiently than conventional language-based CoT.", "result": "On four reasoning benchmarks with LLaMA-1B, ATP-Latent achieves a 4.1% average accuracy improvement while reducing token usage by 3.3% compared with strong latent CoT baselines. These results show that actively planned latent reasoning with VAE smoothing and coherence-guided RL can improve both effectiveness and efficiency of chain-of-thought reasoning.", "conclusion": "ATP-Latent demonstrates that replacing passive imitation of language CoTs with active planning in a learned latent space improves latent chain-of-thought reasoning. By using a conditional VAE to create a smooth, multi-solution-aware latent space and reinforcement learning with an auxiliary coherence reward, the method yields better reasoning accuracy with fewer tokens. This suggests that future work on latent reasoning should focus on structured latent-space planning and principled supervision rather than direct mimicry of a single CoT label."}}
{"id": "2601.22055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22055", "abs": "https://arxiv.org/abs/2601.22055", "authors": ["Yaxin Du", "Junru Song", "Yifan Zhou", "Cheng Wang", "Jiahao Gu", "Zimeng Chen", "Menglan Chen", "Wen Yao", "Yang Yang", "Ying Wen", "Siheng Chen"], "title": "$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA", "comment": null, "summary": "Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\\%).", "AI": {"tldr": "The paper proposes G^2-Reader, a dual-graph retrieval-augmented generation system for multimodal question answering over long documents, achieving higher accuracy than strong baselines.", "motivation": "Existing retrieval-augmented generation systems struggle with long, multimodal documents where text, tables, and figures are interleaved. Flat chunking breaks the document\u2019s inherent structure and cross-modal relationships, making retrieved chunks semantically incomplete. Iterative retrieval over long contexts can also get stuck on partial evidence or drift into irrelevant content because it lacks a persistent, global search state. The authors aim to build a more robust system that preserves structure and maintains an explicit reasoning and search plan over the document.", "method": "The authors design G^2-Reader, a dual-graph system. The first is a Content Graph that encodes document-native structure and cross-modal semantics across text, tables, and figures. The second is a Planning Graph, modeled as an agentic directed acyclic graph of sub-questions that represent intermediate reasoning steps. During question answering, the system evolves the Content Graph to capture the document\u2019s multimodal relationships and concurrently maintains and updates the Planning Graph to track partial findings and guide stepwise navigation and evidence gathering across the document, instead of relying on isolated snippets.", "result": "On VisDoMBench, which spans five multimodal domains, G^2-Reader using Qwen3-VL-32B-Instruct achieves an average accuracy of 66.21%, outperforming strong baselines and a standalone GPT-5 model, which scores 53.08%. This demonstrates the effectiveness of the dual-graph approach for multimodal long-document question answering.", "conclusion": "The proposed G^2-Reader effectively addresses the brittleness of traditional RAG methods on long multimodal documents by explicitly modeling both document structure (Content Graph) and the reasoning/search process (Planning Graph). This dual-graph design leads to substantially improved accuracy over strong baselines and even larger standalone models, highlighting the importance of structured retrieval and planned evidence accumulation for multimodal reading comprehension."}}
{"id": "2601.22069", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22069", "abs": "https://arxiv.org/abs/2601.22069", "authors": ["Yibo Wang", "Yongcheng Jing", "Shunyu Liu", "Hao Guan", "Rong-cheng Tu", "Chengyu Wang", "Jun Huang", "Dacheng Tao"], "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning", "comment": "Code: https://github.com/w-yibo/VTC-R1", "summary": "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.", "AI": {"tldr": "Introduces VTC-R1, a vision-text compression based reasoning paradigm that turns intermediate reasoning traces into compact images to act as optical memory, improving both accuracy and efficiency for long-context reasoning in VLMs.", "motivation": "Long-context reasoning in LLMs is powerful but computationally expensive due to quadratic attention and long token sequences. Existing efficiency methods often require extra training pipelines or external compression models and frequently lose fine-grained reasoning details, limiting scalability and performance. The authors aim to design a more scalable, efficient, and information-preserving way to handle long reasoning traces, especially for math and knowledge-intensive tasks.", "method": "VTC-R1 integrates vision-text compression directly into the reasoning loop. Instead of keeping all intermediate reasoning steps as long text chains, it periodically renders segments of the textual reasoning into compact images, which serve as an external \u201coptical memory.\u201d These images are then re-fed into a vision-language model (VLM) so the model can recall and build upon previous steps using visual context rather than long text sequences. The authors construct a training dataset by transforming the OpenR1-Math-220K reasoning traces into this vision-text format to obtain about 3.4x token compression and then fine-tune representative VLMs (Glyph, Qwen3-VL) to use this mechanism effectively.", "result": "On benchmarks focused on mathematical and knowledge-intensive reasoning, including MATH500, AIME25, AMC23, and GPQA-D, VTC-R1 achieves better performance than standard long-context textual reasoning baselines. Simultaneously, the approach reduces the number of processed tokens by roughly 3.4x and achieves about 2.7x speedup in end-to-end inference latency, demonstrating both accuracy and efficiency gains.", "conclusion": "Embedding vision-text compression into the reasoning process via VTC-R1 offers a scalable and efficient alternative to traditional long-context reasoning: it preserves essential intermediate information in compact visual form, reduces token usage, and accelerates inference while improving benchmark performance. This suggests that \u201coptical memory\u201d with VLMs is a promising direction for future reasoning-intensive applications and models."}}
{"id": "2601.21608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21608", "abs": "https://arxiv.org/abs/2601.21608", "authors": ["Saisubramaniam Gopalakrishnan", "Harikrishnan P M", "Dagnachew Birru"], "title": "Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget", "comment": null, "summary": "Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \\emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.", "AI": {"tldr": "The paper treats early-stage validation of intelligent document processing systems as a search-based software testing problem, showing that different search strategies uncover complementary failure types and arguing for portfolio-based testing.", "motivation": "Enterprise IDP systems operate in high-stakes domains and must be validated under limited testing budgets. Traditional approaches that look for a single worst-case document miss the need to expose a broad range of distinct failure mechanisms. There is a gap in understanding how different search-based testing strategies compare in discovering diverse failure modes in complex, high-dimensional document configuration spaces.", "method": "The authors formalize early-phase IDP validation as a Search-Based Software Testing problem over a combinatorial space of document configurations. They define structural 'risk features' that, when combined, induce realistic failure conditions. They then benchmark multiple search strategies\u2014including evolutionary algorithms, swarm-based methods, quality-diversity approaches, learning-based methods, and quantum-inspired techniques\u2014under the same evaluation budget. They analyze each solver\u2019s discoveries via configuration-level exclusivity, win-rate metrics, and cross-temporal overlap of discovered failures across different budgets.", "result": "Experiments show that each search strategy reliably finds some failure modes that others miss at comparable budgets. Cross-temporal analysis indicates that solver-specific discoveries remain unique across all tested budget levels; no single method consistently dominates the others in terms of discovered failure types. Only the union of all solvers eventually recovers the full observed failure space, while depending on any single solver leads to delayed exposure of certain important risks.", "conclusion": "The study concludes that search strategies for IDP validation are intrinsically complementary: different methods expose different parts of the failure landscape. Consequently, relying on a single search-based testing strategy is insufficient for robust industrial validation under budget constraints. The authors advocate portfolio-based SBST approaches\u2014combining multiple solvers\u2014to accelerate and broaden failure discovery in enterprise IDP systems."}}
{"id": "2601.21609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21609", "abs": "https://arxiv.org/abs/2601.21609", "authors": ["Bingqian Li", "Xiaolei Wang", "Junyi Li", "Weitao Li", "Long Zhang", "Sheng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems", "comment": null, "summary": "Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.", "AI": {"tldr": "RecNet is a self-evolving, LLM-driven preference propagation framework that updates and spreads user preference signals in real time across users and items to improve recommender systems.", "motivation": "Existing agentic/LLM-based recommenders rely mainly on explicit user\u2013item interactions, which are sparse, noisy, delayed, and fail to capture real-time, mutual influences between users and items. This limits their ability to model dynamic, fine-grained preference evolution and cross-user/item effects. The authors want a mechanism that can proactively and continuously propagate and refine preference signals in an agent network, rather than waiting for isolated interactions.", "method": "They design RecNet, a two-phase framework:\n1) Forward phase (preference propagation):\n- Centralized preference routing: special router agents collect new preference updates and dynamically route them to the most relevant user/item agents.\n- Personalized preference reception: each agent has (a) a message buffer that temporarily stores incoming preference messages, and (b) an optimizable rule-based filter memory that decides what to accept or ignore based on historical experience and interests, ensuring personalized and accurate integration of propagated preferences.\n2) Backward phase (self-evolution / learning to propagate):\n- Feedback-driven propagation optimization: the overall system is framed as a multi-agent reinforcement learning\u2013like process. LLMs are used to assign credit to different propagation decisions, analyze gradients, and perform module-level optimization, so that routing and filtering strategies improve over time based on feedback from recommendation performance.", "result": "Experiments across multiple recommendation scenarios show that RecNet better captures preference propagation dynamics and improves recommendation quality compared to baselines. It is more effective at modeling how user preferences evolve and spread, leading to superior performance metrics in various tasks (details not given in the abstract but implied to be statistically significant improvements).", "conclusion": "Incorporating proactive, real-time preference propagation and self-evolving optimization into agentic recommender systems leads to more accurate modeling of user preference dynamics. RecNet\u2019s combination of centralized routing, personalized reception, and LLM-driven multi-agent optimization provides an effective architecture for dynamic, agent-based recommender systems that can continuously refine their propagation strategies from feedback."}}
{"id": "2601.22124", "categories": ["cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22124", "abs": "https://arxiv.org/abs/2601.22124", "authors": ["Anran Li", "Yuanyuan Chen", "Wenjun Long", "Yu Yin", "Yan Hu", "Hyunjae Kim", "Weipeng Zhou", "Yujia Zhou", "Hongyi Peng", "Yang Ren", "Xuguang Ai", "Zhenyue Qin", "Ming Hu", "Xiaoxiao Li", "Han Yu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hua Xu", "Qingyu Chen"], "title": "A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine", "comment": "38 pages, 9 tables, 3 figures", "summary": "Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.", "AI": {"tldr": "They propose a federated, parameter-efficient way (Fed-MedLoRA / Fed-MedLoRA+) to adapt large language models to clinical information extraction across hospitals, reducing communication cost and handling heterogeneous data.", "motivation": "Medical LLMs perform well but are usually trained at a single institution, limiting generalizability and safety. Direct federated training of full LLMs is impractical due to communication/computation costs and the strong data-homogeneity assumptions of many FL algorithms, which conflict with highly heterogeneous real-world clinical data.", "method": "They design a model-agnostic federated learning framework that only transmits low-rank adapter (LoRA) parameters instead of full LLM weights (Fed-MedLoRA) and extend it (Fed-MedLoRA+) with an adaptive, data-aware aggregation strategy to better cope with cross-site heterogeneity. They apply this to clinical information extraction tasks using clinical notes, comparing against BERT and several strong LLM baselines (LLaMA-3, DeepSeek-R1, GPT-4o) under different evaluation settings.", "result": "Fed-MedLoRA / Fed-MedLoRA+ can be trained collaboratively across multiple clinical sites while significantly reducing communication and computation overhead, and they achieve strong performance on clinical IE across five patient cohorts, outperforming or matching baseline models in in-domain, external validation, and low-resource new-site adaptation scenarios.", "conclusion": "Parameter-efficient, adapter-based federated learning is a viable and effective way to adapt LLMs to medical tasks across heterogeneous healthcare institutions, supporting better generalization and practical deployment in real clinical environments while respecting resource constraints."}}
{"id": "2601.21618", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21618", "abs": "https://arxiv.org/abs/2601.21618", "authors": ["Marti\u00f1o R\u00edos-Garc\u00eda", "Nawaf Alampara", "Kevin Maik Jablonka"], "title": "Semantic Content Determines Algorithmic Performance", "comment": null, "summary": "Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.", "AI": {"tldr": "The paper introduces WhatCounts, a benchmark isolating whether LLM algorithms behave independently of the semantic content of their inputs, and finds large, unstable semantic-dependent failures even on simple counting.", "motivation": "To test a basic desideratum of algorithms\u2014semantic invariance, meaning that an algorithm's behavior should not depend on what is being counted or processed\u2014and to show that existing LLM evaluations conflate this property with other factors like reasoning complexity or prompt variation.", "method": "The authors design WhatCounts, an atomic counting benchmark where models must count items in clear, unambiguous, delimited lists without duplicates, distractors, or reasoning steps, while varying only the semantic type of items (e.g., cities, chemicals, names, symbols). They then evaluate frontier LLMs, perform controlled ablations to remove potential confounds, and analyze how fine-tuning affects performance across different semantic categories.", "result": "Frontier LLMs show over 40% accuracy variation solely based on the semantic type of the list items, even though the counting problem is structurally identical. Ablation studies rule out alternative explanations such as prompt wording, length, and complexity, confirming that the performance gap is driven by semantics. Small, seemingly unrelated fine-tuning further causes unpredictable shifts in which semantic categories perform well or poorly.", "conclusion": "LLMs approximate algorithms in a way that is dependent on the semantic content of their inputs rather than implementing true, input-agnostic algorithms. This semantic dependence appears unstable under fine-tuning and is likely to affect functions beyond counting, implying that any LLM-based system may harbor hidden semantic sensitivities that undermine reliability and generalization."}}
{"id": "2601.22139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22139", "abs": "https://arxiv.org/abs/2601.22139", "authors": ["Xin Chen", "Feng Jiang", "Yiqian Zhang", "Hardy Chen", "Shuo Yan", "Wenya Xie", "Min Yang", "Shujian Huang"], "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers", "comment": "The manuscript is under review", "summary": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}", "AI": {"tldr": "The paper introduces Proactive Interactive Reasoning (PIR), a framework that makes LLMs proactively ask clarification questions instead of blindly reasoning when information is missing or ambiguous, significantly improving accuracy, efficiency, and robustness across multiple tasks.", "motivation": "Existing reasoning-oriented LLMs using Chain-of-Thought tend to perform long internal reasoning even when crucial information is missing, ambiguous, or under-specified. This \"blind self-thinking\" wastes computation, can yield incorrect answers, and does not address premise- and intent-level uncertainty. Most current extensions focus on querying external tools or knowledge sources, not on directly clarifying with the user. The authors aim to make LLMs more interactive and uncertainty-aware so they can detect when they do not understand enough and then proactively ask the user for clarification.", "method": "They propose Proactive Interactive Reasoning (PIR), consisting of two main components: (1) an uncertainty-aware supervised fine-tuning pipeline that trains models to interleave reasoning with user interaction, i.e., to recognize uncertainty in premises or intent and respond by asking targeted clarification questions instead of continuing blind reasoning; and (2) a policy optimization framework using a user simulator and a composite reward function. The reward encourages alignment with user intent, better task performance, and efficient interaction (avoiding unnecessary questions and excessive computation). Through this structure, the model learns when and how to ask, and how to adapt its reasoning based on user responses.", "result": "Across multiple benchmarks\u2014mathematical reasoning, code generation, and document editing\u2014PIR-based models outperform strong baselines. The paper reports up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU score improvement. PIR also roughly halves both the reasoning computation cost and the number of redundant interaction turns, indicating the method is more efficient as well as more accurate. Additional evaluations on factual knowledge, QA, and missing-premise tasks show that the learned interactive behavior generalizes across domains and improves robustness when information is incomplete or uncertain.", "conclusion": "Transforming LLMs from passive reasoners into proactive, interactive agents that explicitly handle premise and intent uncertainty yields significant gains in accuracy, efficiency, and robustness. PIR demonstrates that uncertainty-aware training plus policy optimization with a user simulator can teach models to interleave reasoning with clarification questions in a way that aligns better with user goals and reduces unnecessary computation. The approach generalizes across domains and tasks, and the authors release their model and code to support further research in interactive reasoning for LLMs."}}
{"id": "2601.21654", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21654", "abs": "https://arxiv.org/abs/2601.21654", "authors": ["Hao Shen", "Hang Yang", "Zhouhong Gu"], "title": "ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval", "comment": null, "summary": "Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.\n  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.", "AI": {"tldr": "ScholarGym is a reproducible simulation environment to evaluate complex, tool-using LLM research workflows on academic literature.", "motivation": "Existing evaluations of tool-augmented LLM research workflows rely on live APIs and dynamic backends, causing non-deterministic tool outputs due to temporal drift, rate limits, and changing states. This makes experiments irreproducible and prevents fair comparison of systems and methods. There is a need for a controlled, deterministic testbed that isolates and analyzes each stage of the workflow.", "method": "The authors build ScholarGym, an offline simulation environment grounded in a static, deterministic corpus of 570K academic papers. They factor research workflows into three components\u2014query planning, tool invocation (retrieval over the static corpus), and relevance assessment\u2014and provide 2,536 information-seeking queries with expert-annotated ground truth. They then run experiments with various backbone language models and different planning/selection strategies to study behavior across iterative refinement steps.", "result": "ScholarGym enables deterministic, reproducible evaluation of deep research workflows and supports fine-grained analysis of how different models and strategies behave at each stage. Experimental results across backbones show distinct patterns in how reasoning strength, planning approaches, and selection mechanisms interact during iterative refinement, revealing where workflows succeed or fail under controlled conditions.", "conclusion": "ScholarGym offers a standardized, reproducible benchmark for complex research workflows over academic literature, overcoming the non-determinism of live-tool settings. By decoupling workflow stages and providing a large static corpus plus expert-annotated queries, it allows systematic comparison and diagnostic analysis of tool-augmented LLM systems, informing the design of more robust research agents."}}
{"id": "2601.22146", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22146", "abs": "https://arxiv.org/abs/2601.22146", "authors": ["Ajay Patel", "Colin Raffel", "Chris Callison-Burch"], "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale", "comment": null, "summary": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .", "AI": {"tldr": "They build a massive synthetic instruction\u2013answer dataset (FineInstructions) from internet-scale text, allowing LLMs to be trained from scratch directly on instruction-following rather than next-token prediction, and show this improves downstream response quality.", "motivation": "LLMs are usually pre-trained with next-word prediction on raw text, then lightly instruction-tuned on a small supervised dataset. This creates a mismatch between training and actual use (responding to user instructions) and is limited by the small size of human-labeled instruction data. The authors want to remove this bottleneck and make pre-training itself instruction-like and aligned with downstream usage, without requiring impractically large amounts of human annotation.", "method": "They construct FineInstructions, a large synthetic supervised dataset by: (1) collecting about 18M real user-written queries/prompts and turning them into instruction templates; (2) matching these templates to suitable human-written documents from web-scale pre-training corpora; (3) instantiating the templates with content from those documents to produce synthetic instruction\u2013answer pairs at billion-scale. They then pre-train LLMs from scratch using only this instruction-tuning style objective and compare it to conventional next-token pre-training and other synthetic pre-training schemes under controlled, token-matched experiments.", "result": "Models pre-trained on FineInstructions achieve better performance on standard benchmarks of free-form response quality than models trained with the same token budget on standard next-token pre-training or on alternative synthetic pre-training approaches. This suggests that large-scale synthetic instruction\u2013answer data can effectively replace conventional pre-training for instruction-following tasks.", "conclusion": "Instruction-style synthetic supervision at internet scale can stand in for traditional self-supervised next-token pre-training while better matching real-world LLM usage. FineInstructions enables training LLMs directly on instruction-following objectives, leading to superior downstream response quality. This points toward a new pre-training paradigm where models are taught to follow instructions from the start using large, automatically generated yet human-grounded instruction\u2013answer pairs."}}
{"id": "2601.21666", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21666", "abs": "https://arxiv.org/abs/2601.21666", "authors": ["Ahmed Y. Radwan", "Christos Emmanouilidis", "Hina Tabassum", "Deval Pandya", "Shaina Raza"], "title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard", "AI": {"tldr": "SONIC-O1 is a human-verified benchmark to evaluate multimodal large language models on real-world conversational audio-video understanding, with tasks like summarization, QA, and temporal localization, and it exposes temporal reasoning gaps and demographic performance disparities.", "motivation": "Most multimodal LLM research focuses on static images, leaving their capabilities on sequential, conversational audio-video data underexplored. There is no widely adopted, high-quality, human-verified benchmark that tests models in realistic, temporally grounded, socially diverse scenarios, making it hard to measure progress and identify weaknesses, especially around temporal reasoning and demographic robustness.", "method": "The authors construct SONIC-O1, a benchmark of real-world conversational audio-video clips spanning 13 domains, each annotated and fully human-verified. They design three main task types\u2014open-ended summarization, multiple-choice question answering, and temporal localization with rationales\u2014along with demographic metadata for participants in the clips. They then systematically evaluate a range of closed- and open-source MLLMs on these tasks, comparing performance across task types, model families, and demographic groups.", "result": "Across evaluated models, multiple-choice accuracy differences between closed- and open-source families are modest, but there is a large 22.6% performance gap in temporal localization favoring the best closed-source model. Overall performance on temporal grounding tasks is weaker than on MCQ, indicating that current MLLMs struggle with fine-grained temporal reasoning over video. Furthermore, model performance degrades for certain demographic groups, revealing disparities and lack of social robustness in current systems.", "conclusion": "SONIC-O1 fills a key gap by offering an open, human-verified benchmark for evaluating temporally grounded and socially robust multimodal understanding in MLLMs. The experiments show that while some capabilities (e.g., MCQ answering) are relatively mature, temporal localization and demographic fairness remain significant challenges, underscoring the need for improved methods and the usefulness of SONIC-O1 as a standard evaluation suite for future research."}}
{"id": "2601.22149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22149", "abs": "https://arxiv.org/abs/2601.22149", "authors": ["Hang Ding", "Peidong Liu", "Junqiao Wang", "Ziwei Ji", "Meng Cao", "Rongzhao Zhang", "Lynn Ai", "Eric Yang", "Tianyu Shi", "Lei Yu"], "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents", "comment": null, "summary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.", "AI": {"tldr": "DynaWeb is a model-based reinforcement learning framework that trains LLM-powered web agents inside a learned web simulator instead of the live internet, significantly boosting performance and efficiency on standard benchmarks.", "motivation": "Training autonomous web agents directly on the live internet is inefficient, expensive, unstable, and risky. There is a need for a scalable and safe way to provide large amounts of interactive experience so web agents can be improved with online RL without constant real-world web interaction.", "method": "The authors propose DynaWeb, a model-based RL framework that first learns a web world model predicting realistic web page representations from agent actions. This learned model is then used as a synthetic environment in which the agent can generate large quantities of imagined rollouts (\"dreams\") for online RL updates. In addition, DynaWeb interleaves these on-policy imagined rollouts with real expert trajectories from training data to improve training stability and sample efficiency.", "result": "On the WebArena and WebVoyager benchmarks, DynaWeb yields consistent and substantial performance improvements over state-of-the-art open-source web agents, indicating that agents trained with the world-model-based imagination approach perform better on challenging web tasks than those trained without it.", "conclusion": "Training web agents through interaction with a learned web world model is a viable and effective alternative to relying solely on live internet interaction. DynaWeb provides a scalable, efficient, and safer pathway to online agentic RL for web agents, suggesting that imagination-based training can be a key ingredient for advancing general-purpose web AI assistants."}}
{"id": "2601.21692", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21692", "abs": "https://arxiv.org/abs/2601.21692", "authors": ["Mingzu Liu", "Hao Fang", "Runmin Cong"], "title": "TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning", "comment": null, "summary": "Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.", "AI": {"tldr": "The paper proposes an unsupervised defense, TCAP, that detects poisoned fine-tuning data for multimodal LLMs by exploiting a universal backdoor fingerprint: abnormal attention allocation among system instructions, images, and user text.", "motivation": "Fine-Tuning-as-a-Service for multimodal LLMs makes model customization easy but opens a backdoor attack surface, where attackers inject poisoned samples during fine-tuning. Existing defenses largely depend on labels or do not generalize well across different triggers and modalities. There is a need for a general, label-free mechanism to identify poisoned samples in multimodal backdoor attacks.", "method": "The authors empirically observe a consistent artifact of backdoored samples: they cause a divergence in how the model allocates attention among three components\u2014system instructions, visual inputs, and user queries. They formalize this as a backdoor fingerprint. Their defense, Tri-Component Attention Profiling (TCAP), first decomposes cross-modal attention maps into these three components. It then uses Gaussian Mixture Models to statistically profile attention patterns per head and detect heads that are especially responsive to triggers. Finally, it aggregates these head-level signals using an EM-based voting scheme to classify and filter out poisoned samples from the fine-tuning set, all in an unsupervised way.", "result": "Experiments on multiple multimodal LLM architectures and various backdoor attack methods show that TCAP can reliably distinguish poisoned from clean fine-tuning samples. It achieves strong and stable defense performance without supervision and generalizes across trigger types and modalities, outperforming or matching existing defenses in robustness and practicality.", "conclusion": "Attention allocation divergence across system prompt, image, and user text is a universal backdoor fingerprint in multimodal LLMs. By exploiting this property, TCAP offers an effective, unsupervised, and broadly applicable mechanism to filter poisoned samples in FTaaS scenarios, making backdoor defenses for MLLMs more robust and practical."}}
{"id": "2601.22156", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22156", "abs": "https://arxiv.org/abs/2601.22156", "authors": ["Yingfa Chen", "Zhen Leng Thai", "Zihan Zhou", "Zhu Zhang", "Xingyu Shen", "Shuo Wang", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts", "comment": "20 pages, 8 figures", "summary": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data", "AI": {"tldr": "The paper introduces HALO, a pipeline to distill Transformer LMs into efficient RNN-attention hybrid models (HypeNet) that retain performance but greatly improve long-context efficiency, using very little distillation data.", "motivation": "Hybrid Transformer\u2013RNN architectures promise better efficiency for long-context sequences, but training them from scratch is extremely costly. Existing methods that convert pre-trained Transformers to hybrids need huge amounts of data and still yield weak long-context performance, limiting their practicality. The authors aim to develop a practical, low-data distillation approach that preserves or improves long-context abilities while leveraging existing strong Transformer LMs.", "method": "They propose HALO (Hybrid Attention via Layer Optimization), a distillation pipeline that converts softmax attention layers in a pre-trained Transformer into RNN-based blocks using parameter transfer and targeted optimization, but with a more data-efficient procedure. On top of this, they design HypeNet, a specific hybrid architecture that mixes attention and RNN layers and introduces HyPE, a new positional encoding scheme, along with several architectural tweaks to improve length generalization. They apply HALO to convert Qwen3 models into HypeNet variants.", "result": "Using only 2.3B tokens (under 0.01% of the original pre-training data), they obtain HypeNet models converted from the Qwen3 series that match the original Transformers on standard benchmarks while significantly outperforming them in long-context settings in both quality and efficiency (throughput and/or latency).", "conclusion": "Distilling large Transformer LMs into carefully designed hybrid RNN-attention architectures is feasible with very modest additional data. HALO and HypeNet show that one can retain general performance while substantially improving long-context efficiency and generalization, making hybrid models a practical alternative to pure Transformers for long-context applications."}}
{"id": "2601.21708", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21708", "abs": "https://arxiv.org/abs/2601.21708", "authors": ["Tongxi Wang"], "title": "FBS: Modeling Native Parallel Reading inside a Transformer", "comment": null, "summary": "Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \\textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.", "AI": {"tldr": "Proposes Fovea-Block-Skip Transformer (FBS) to make LLM inference more efficient by adding a causal, trainable loop that enables adaptive previewing and skipping while maintaining or improving quality without extra parameters.", "motivation": "Token-by-token autoregressive inference in large language models is computationally expensive and inflexible. Existing speed-up methods typically modify the decoding pipeline superficially (e.g., caching, pruning, or speculative decoding) but do not incorporate mechanisms analogous to human reading strategies such as previewing upcoming content, focusing compute on important chunks, and maintaining consistency between training and inference. The paper aims to design an architecture that natively supports content-adaptive foresight and chunk-aware computation allocation to improve the quality-efficiency trade-off.", "method": "Introduce the Fovea-Block-Skip Transformer (FBS), which augments standard Transformers with a causal, trainable control loop built from three key modules: (1) Parafovea-Attention Window (PAW) that allows the model to preview a local window of upcoming tokens in a way that respects causality; (2) Chunk-Head (CH) that organizes or attends over chunk-level structures to modulate how computation is distributed across segments of the sequence; and (3) Skip-Gate (SG) that decides when computation can be skipped or reduced for certain blocks/tokens. The combination lets the model dynamically decide what to read in detail versus skim, during both training and inference.", "result": "On multiple benchmarks, FBS achieves a better quality-efficiency trade-off than standard Transformers with the same parameter count, meaning it can either match baseline quality at lower compute/latency or achieve higher quality under the same compute budget. Ablation studies removing PAW, CH, or SG show each component contributes to performance and that the full model with all three yields the best efficiency and accuracy, confirming their complementarity.", "conclusion": "Embedding a causal, trainable loop into Transformer architectures via mechanisms inspired by human reading\u2014previewing, chunk-aware focus, and adaptive skipping\u2014can significantly improve inference efficiency without increasing parameter count. The Fovea-Block-Skip Transformer demonstrates that content-adaptive foresight and structured compute allocation are powerful architectural directions beyond simple token-by-token autoregression and pipeline-level speed-ups."}}
{"id": "2601.21714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21714", "abs": "https://arxiv.org/abs/2601.21714", "authors": ["Kaixiang Wang", "Yidan Lin", "Jiong Lou", "Zhaojiacheng Zhou", "Bunyod Suvonov", "Jie Li"], "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory", "comment": "18 pages", "summary": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%.", "AI": {"tldr": "The paper introduces E-mem, an episodic memory framework for LLM agents that preserves rich context for System-2-style reasoning, improving F1 and reducing token cost on the LoCoMo benchmark.", "motivation": "Current LLM-agent memory systems preprocess and compress interaction histories into embeddings or graph structures. This de-contextualization breaks long-range sequential dependencies and harms deep, deliberative reasoning required for System-2 behavior. The authors want a memory mechanism that preserves logical and contextual integrity over long horizons without incurring prohibitive token costs.", "method": "They propose E-mem, a heterogeneous hierarchical multi-agent framework inspired by biological engrams. Multiple assistant agents each maintain uncompressed, episodic memory segments. A central master agent coordinates global planning and activates relevant assistants. Instead of simple retrieval of stored chunks, assistants perform local reasoning over their full episodic contexts, extract context-aware evidence, and send distilled results back for aggregation and decision-making. This shifts from one-shot memory preprocessing to episodic context reconstruction at inference time.", "result": "On the LoCoMo long-context reasoning benchmark, E-mem attains over 54% F1, outperforming the prior state-of-the-art GAM approach by 7.75 percentage points, while also reducing token usage by more than 70%.", "conclusion": "Maintaining uncompressed episodic memories and enabling local reasoning within them allows LLM agents to better preserve contextual integrity for System-2-style tasks. The hierarchical E-mem architecture improves both reasoning quality and efficiency, suggesting episodic context reconstruction as a more suitable paradigm than traditional compressed memory preprocessing for long-horizon LLM agents."}}
{"id": "2601.21726", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21726", "abs": "https://arxiv.org/abs/2601.21726", "authors": ["Siru Zhong", "Yiqiu Liu", "Zhiqing Cui", "Zezhi Shao", "Fei Wang", "Qingsong Wen", "Yuxuan Liang"], "title": "DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting", "comment": null, "summary": "Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from \"what\" to learn to \"how much\" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.", "AI": {"tldr": "A plugin method, DropoutTS, improves robustness of deep time series models to noise via sample-adaptive dropout based on spectral sparsity and reconstruction residuals.", "motivation": "Deep time series models in real applications often encounter noisy data, which harms performance. Existing robustness methods either discard/prune data (risking loss of useful information) or require expensive prior noise quantification, so they cannot simultaneously ensure robustness, efficiency, and ease of use across different models and datasets. A more efficient, model-agnostic approach is needed to adaptively handle noise at the instance level without redesigning architectures or adding heavy overhead.", "method": "The paper proposes DropoutTS, a model-agnostic plugin focused on controlling \"how much\" a model learns from each sample rather than selecting \"what\" data to learn. It uses a Sample-Adaptive Dropout mechanism: (1) exploit spectral sparsity of time series to quantify noise per sample by reconstructing signals and computing reconstruction residuals as a proxy for instance-level noise; (2) map the estimated noise level to an adaptive dropout rate for that sample; (3) apply this adaptive dropout to selectively suppress noisy or spurious fluctuations in the representation while preserving fine-grained, informative patterns. The method is lightweight, introduces negligible parameter overhead, and requires no architectural modification of the backbone model.", "result": "Across various benchmark time series datasets and under different noise regimes, DropoutTS consistently improves the performance of multiple strong backbone models. It enhances robustness to noise while keeping computational and parameter overhead low, demonstrating that the plugin can be broadly and effectively integrated into existing architectures without redesign.", "conclusion": "DropoutTS provides an effective and efficient way to make deep time series models more robust to noisy data. By using sample-adaptive dropout rates derived from spectral sparsity-based noise estimation, it dynamically regulates model learning capacity per instance, suppressing spurious noise and preserving important details. The approach is model-agnostic, lightweight, and requires no architecture changes, making it practical for real-world deployments of time series models."}}
{"id": "2601.21742", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21742", "abs": "https://arxiv.org/abs/2601.21742", "authors": ["Ruiwen Zhou", "Maojia Song", "Xiaobao Wu", "Sitao Cheng", "Xunjian Yin", "Yuxi Xie", "Zhuoqun Hao", "Wenyue Hua", "Liangming Pan", "Soujanya Poria", "Min-Yen Kan"], "title": "Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems", "comment": "Codes and data are available at https://github.com/skyriver-2000/epistemic-context-learning", "summary": "Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.", "AI": {"tldr": "They propose a history-aware trust modeling framework, Epistemic Context Learning (ECL), that lets agents use past peer interactions to judge reliability, greatly improving multi-agent reasoning performance, especially for small models.", "motivation": "Multi-agent systems often fail because agents naively follow misleading peers. This fragility arises from sycophancy and the inability to assess which peers are trustworthy. Existing approaches typically judge peers only by the quality of their immediate reasoning, ignoring their longer-term behavior and historical reliability. There is a need for a principled way to incorporate interaction history so agents can selectively trust peers and avoid being misled.", "method": "They formalize a history-aware reference learning problem, where agents receive not just current peer outputs but also histories of peer interactions. From this, agents build explicit peer profiles encoding estimated reliability. They introduce Epistemic Context Learning (ECL), a reasoning framework that conditions each prediction on these peer profiles instead of (or in addition to) raw peer messages. They further train and refine ECL via reinforcement learning with auxiliary rewards designed to encourage correct reliability estimation and better use of trustworthy peers.", "result": "ECL allows small models (e.g., Qwen 3-4B) to surpass a much larger, history-agnostic baseline (Qwen 3-30B) by accurately identifying reliable peers. When applied to larger, frontier models, ECL pushes performance close to 100%. Experiments show that the method generalizes across different multi-agent configurations and tasks, and that the learned trust estimates correlate strongly with the eventual answer correctness.", "conclusion": "Incorporating historical interaction information into multi-agent reasoning enables agents to form effective trust models over their peers. Epistemic Context Learning provides a concrete way to build and use these peer profiles, yielding large gains in robustness and accuracy, particularly for smaller models. Trust modeling emerges as a key capability: better trust estimates strongly predict better final performance, suggesting that explicit epistemic context should be a core ingredient in future multi-agent LLM systems."}}
{"id": "2601.21754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21754", "abs": "https://arxiv.org/abs/2601.21754", "authors": ["Haoyu Wang", "Guozheng Ma", "Shugang Cui", "Yilun Kong", "Haotian Luo", "Li Shen", "Mengya Gao", "Yichao Wu", "Xiaogang Wang", "Dacheng Tao"], "title": "Language-based Trial and Error Falls Behind in the Era of Experience", "comment": null, "summary": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.", "AI": {"tldr": "The paper introduces SCOUT, a framework that makes LLM agents effective in unseen, nonlinguistic environments by offloading exploration to small neural \u201cscout\u201d models and then training the LLM from the scouts\u2019 experiences.", "motivation": "LLMs perform well on language tasks but struggle with unseen symbolic or spatial environments, where solving tasks requires extensive trial-and-error. Directly using large LLMs for this exploration is computationally too expensive. The authors want a way to exploit LLMs\u2019 semantic knowledge without paying the high cost of having them explore complex environments themselves.", "method": "SCOUT separates exploration from exploitation. Lightweight \u201cscout\u201d models (such as small MLPs) interact with the environment at scale, collecting trajectories that characterize its dynamics. These trajectories are then used to supervise the LLM via Supervised Fine-Tuning, giving it an initial policy and understanding of the environment. Afterward, multi-turn Reinforcement Learning is applied to the LLM to further refine its behavior and unlock its latent world knowledge for the target tasks.", "result": "Using SCOUT with a Qwen2.5-3B-Instruct model yields an average score of 0.86 on the tested unseen, nonlinguistic tasks, surpassing stronger proprietary models such as Gemini-2.5-Pro (0.60). Additionally, this approach reduces GPU hours by about 60% compared to directly training or exploring with large LLMs.", "conclusion": "Decoupling exploration from exploitation through lightweight scout models is an effective strategy for enabling LLMs to perform well in unseen, nonlinguistic environments. SCOUT both improves performance over state-of-the-art proprietary systems and substantially reduces computational cost, suggesting a practical path for scaling LLM agents to complex, nonlinguistic domains."}}
{"id": "2601.21760", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21760", "abs": "https://arxiv.org/abs/2601.21760", "authors": ["Ruian Tie", "Wenbo Xiong", "Zhengyu Shi", "Xinyu Su", "Chenyu jiang", "Libo Wu", "Hao Li"], "title": "Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling", "comment": null, "summary": "Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.", "AI": {"tldr": "A new zero-shot framework, ZSSD, statistically downscales climate model outputs without paired training data by using a physics-consistent prior and a unified coordinate guidance scheme, improving extremes and complex events across different GCMs.", "motivation": "Climate impact studies need high-resolution, physically consistent local climate information, but conventional supervised statistical downscaling requires paired low\u2013high resolution data, which do not exist for future GCM projections and do not transfer well because of domain gaps between GCMs and reanalysis. Existing zero-shot approaches that avoid paired data suffer from physical inconsistencies and optimization problems (vanishing gradients) at large scaling factors. The paper aims to create a method that generalizes across heterogeneous GCMs while remaining physically consistent, especially for extremes and complex weather systems.", "method": "The authors propose Zero-Shot Statistical Downscaling (ZSSD), which is trained only on reanalysis data to learn a Physics-Consistent Climate Prior. This prior is conditioned on geophysical boundary information (e.g., orography, land\u2013sea mask) and temporal descriptors to respect physical constraints. At inference time, ZSSD is applied in a zero-shot fashion to GCM outputs, i.e., without any paired GCM\u2013high-resolution data. They further introduce Unified Coordinate Guidance, a modification of diffusion-based posterior sampling (DPS) guidance that uses a unified coordinate system and constraints from large-scale GCM fields to avoid vanishing gradients and maintain consistency between the downscaled fields and the driving GCM.", "result": "ZSSD yields significantly lower errors in the 99th percentile (extreme values) compared to existing zero-shot downscaling baselines, and it can reconstruct complex weather phenomena such as tropical cyclones more faithfully. The method generalizes well across heterogeneous GCMs and different large-scale conditions, demonstrating robust zero-shot performance in both typical and extreme weather regimes.", "conclusion": "Zero-Shot Statistical Downscaling (ZSSD) shows that it is possible to perform physically consistent, high-quality climate downscaling without any paired low\u2013high resolution training data from GCMs. By combining a physics-consistent prior learned from reanalysis with Unified Coordinate Guidance for robust inference, the approach overcomes key limitations of both supervised and previous zero-shot methods, especially for extremes and complex events, and offers a promising path toward scalable, generalizable downscaling across multiple GCMs."}}
{"id": "2601.21771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21771", "abs": "https://arxiv.org/abs/2601.21771", "authors": ["Hadi Banaee", "Stephanie Lowry"], "title": "Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies", "comment": null, "summary": "We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.", "AI": {"tldr": "A framework extends conceptual spaces to model time-evolving abstract strategy concepts, using chess games as trajectories through interpretable dimensions to recognize strategies and differing player perspectives.", "motivation": "Abstract, temporally extended strategies (e.g., 'attack', 'sacrifice') are hard to formalize and recognize in a way that is both interpretable and compatible with learning over time. Conceptual spaces theory is powerful but mostly atemporal; there is a need to handle goal-directed concepts unfolding across sequences of decisions.", "method": "Define strategy concepts as regions in a conceptual space spanned by interpretable quality dimensions relevant to chess (e.g., material balance, king safety, piece activity). Represent individual chess games as trajectories through this space over time (move by move). Recognize strategies when the game\u2019s trajectory moves in characteristic directions toward the corresponding regions. Implement a proof-of-concept system that encodes positions into the conceptual space, traces game trajectories, and compares movement patterns to expert-annotated strategies. Additionally, model dual perspectives by constructing separate conceptual spaces or region interpretations for each player, allowing divergent readings of the same positions.", "result": "The implemented system successfully detects strategy concepts based on trajectory patterns that qualitatively align with expert commentary in the examined chess games. It demonstrates that movement in the conceptual space can serve as a basis for recognizing temporally extended strategic intentions and that different player perspectives can be captured via distinct region characterizations.", "conclusion": "Conceptual spaces can be effectively extended to handle temporally realized, goal-directed concepts by modelling them as regions in a dynamic trajectory space. The chess proof-of-concept shows that strategy recognition via trajectories is feasible and interpretable, and that dual-perspective modelling is possible. This lays groundwork for applying the approach to broader sequential decision-making domains and for integrating learning mechanisms that evolve and refine abstract strategy concepts over time."}}
{"id": "2601.21800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21800", "abs": "https://arxiv.org/abs/2601.21800", "authors": ["Dionizije Fa", "Marko \u010culjak", "Bruno Pand\u017ea", "Mateo \u010cupi\u0107"], "title": "BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics", "comment": null, "summary": "This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.", "AI": {"tldr": "Introduces BioAgent Bench, a benchmark and evaluation suite for testing AI agents on end-to-end bioinformatics workflows, including robustness under controlled perturbations.", "motivation": "There is growing use of AI agents to automate complex bioinformatics workflows, but there is no standardized, realistic way to evaluate how well they perform full pipelines or how robust they are to noisy or adversarial conditions, especially under privacy constraints common in biomedical settings.", "method": "The authors construct a curated benchmark of end-to-end bioinformatics tasks (RNA-seq, variant calling, metagenomics) with prompts that require concrete, automatically checkable output artifacts. They run multiple closed-source and open-weight frontier models in different agent harnesses on these tasks, apply controlled perturbations (corrupted inputs, decoy files, prompt bloat) for stress testing, and use an LLM-based grader to score pipeline progress and validate outcomes.", "result": "Frontier AI agents can often complete multi-step bioinformatics pipelines and produce the requested final artifacts without heavy custom scaffolding. However, they show significant robustness issues under perturbations, revealing step-level reasoning failures despite correct high-level pipeline design. Closed models generally perform better in completion, but their use is constrained when workflows involve sensitive or proprietary data, where open-weight models may be required.", "conclusion": "BioAgent Bench provides a public, automated way to measure both performance and robustness of AI agents on realistic bioinformatics workflows. Current agents are promising for end-to-end pipeline execution but remain fragile under perturbations, and privacy constraints can make open-weight models preferable despite lower raw performance. The benchmark can guide development and comparison of future bioinformatics agents."}}
{"id": "2601.21802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21802", "abs": "https://arxiv.org/abs/2601.21802", "authors": ["Hoang Khang Phan", "Quang Vinh Dang", "Noriyo Colley", "Christina Garcia", "Nhat Tan Le"], "title": "A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition", "comment": null, "summary": "Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.", "AI": {"tldr": "The paper introduces an LLM-centered framework that recognizes endotracheal suctioning (ES) activities from video and generates explainable, educational feedback, outperforming traditional ML/DL baselines by 15\u201320% in accuracy and F1.", "motivation": "Endotracheal suctioning is a high-risk, skill-dependent procedure, especially in home care and educational environments where expert supervision is limited. Existing training largely depends on human instructors, and current automated video-based training tools are underdeveloped, particularly for ES. There is a need for systems that can automatically recognize ES activities from video, reason about their correctness, and provide interpretable feedback to trainees in order to improve safety and training efficiency.", "method": "The authors design a unified framework where a Large Language Model acts as the central reasoning engine. Video data of ES is processed so that the LLM can perform spatiotemporal activity recognition and analyze decisions in an explainable way. The LLM is benchmarked against conventional machine learning and deep learning models for activity recognition. Additionally, they develop a pilot feedback module based on anomaly detection and explainable AI concepts to highlight correct and incorrect actions and generate natural-language guidance for students.", "result": "The LLM-based framework significantly outperforms traditional ML and DL baselines, with around 15\u201320% gains in both accuracy and F1 score for ES activity recognition. The pilot feedback module can automatically identify anomalies or deviations in trainee performance and produce interpretable, natural-language feedback that emphasizes correct behaviors and points out areas for improvement.", "conclusion": "An LLM-centered, video-based activity recognition framework can both accurately recognize ES procedures and generate clear, interpretable feedback for learners. This approach offers a scalable and data-driven tool for nursing education, has the potential to enhance training quality and efficiency, and may ultimately contribute to increased patient safety by improving practitioner competence in endotracheal suctioning."}}
{"id": "2601.21822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21822", "abs": "https://arxiv.org/abs/2601.21822", "authors": ["Zitong Yu", "Boquan Sun", "Yang Li", "Zheyan Qu", "Xing Zhang"], "title": "CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge", "comment": "Accepted by IEEE Communications Magazine", "summary": "Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.", "AI": {"tldr": "CORE is a distributed multi-LLM framework for 6G edge environments that assigns specialized roles to LLMs across devices and edge servers, with optimization modules and a role-affinity scheduler to improve efficiency and task completion in real and experimental settings.", "motivation": "6G and LLMs enable ubiquitous intelligence, but complex reasoning tasks exceed the capacity of any single LLM running on fragmented, heterogeneous edge and mobile resources. There is a need for a system that can coordinate and optimize multiple distributed LLM agents to fully utilize hierarchical edge infrastructures while maintaining performance and responsiveness.", "method": "The paper proposes CORE (Collaborative Orchestration Role at Edge), where several LLMs, each with a distinct functional role, are distributed across mobile devices and multiple tiers of edge servers. CORE includes three key optimization modules: (1) real-time perception to sense environment and resource states, (2) dynamic role orchestration to assign or reassign LLM roles based on current conditions, and (3) pipeline-parallel execution to allow concurrent, staged processing among agents. A role affinity scheduling algorithm is introduced to dynamically map LLM roles to specific edge nodes according to resource availability and computational demand. The framework is evaluated through case studies and experiments in representative 6G application scenarios and through deployment on an operational edge-computing platform.", "result": "Experiments and case studies show that CORE significantly improves overall system efficiency and task completion rates compared with baselines or non-coordinated setups. The role affinity scheduler and optimization modules enable better utilization of hierarchical edge resources, resulting in faster and more reliable execution of complex reasoning tasks. The real-world deployment confirms that CORE maintains its performance gains under practical operational constraints.", "conclusion": "CORE demonstrates that orchestrating multiple specialized LLM agents across hierarchical 6G edge infrastructures is both feasible and effective. The proposed optimization modules and role affinity scheduling algorithm enable intelligent, adaptive allocation of tasks and roles, improving efficiency and reliability of distributed AI services. The framework provides a promising foundation for realizing ubiquitous intelligence in future 6G systems and can be extended to broader edge and multi-agent AI scenarios."}}
{"id": "2601.21830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21830", "abs": "https://arxiv.org/abs/2601.21830", "authors": ["Francesca Filice", "Edoardo De Rose", "Simone Bartucci", "Francesco Calimeri", "Simona Perri"], "title": "Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models", "comment": null, "summary": "The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.", "AI": {"tldr": "They propose and apply a new, deeper benchmark for ECG foundation models that looks not only at task performance but also at how their embeddings are structured and how well they generalize across datasets and low-data settings.", "motivation": "Foundation Models for ECG analysis are increasingly used, but current benchmarks mostly judge them on downstream task performance (e.g., classification accuracy) without examining how generalizable and reliable their learned representations are\u2014an especially important issue in safety-critical healthcare applications and in scenarios with limited data.", "method": "They design a benchmarking framework for ECG-specific foundation models that combines standard performance evaluation on downstream tasks with representation-level analyses. Specifically, they analyze model embeddings using SHAP (to inspect feature importance/explanations) and UMAP (to visualize and explore the geometry and clustering of the embedding space). They then apply this framework to multiple state-of-the-art ECG-expert FMs, pretrained with different techniques and tested across cross-continental datasets and varying data availability, including data-scarce scenarios.", "result": "Applying their protocol to several ECG foundation models reveals rich details about the internal embedding structures, including how patterns cluster, how stable and separable classes are across different datasets, and how representations behave when training data are scarce. These insights go beyond what can be inferred from standard downstream metrics alone and uncover differences in generalizability among models.", "conclusion": "Their comprehensive benchmark, which augments performance-based evaluation with SHAP- and UMAP-driven representation analysis, yields a deeper and more nuanced understanding of ECG foundation models\u2019 learned representations and their capacity to generalize, especially across datasets and in low-data contexts. This framework can support more responsible and informed deployment of ECG FMs in clinical practice."}}
{"id": "2601.21844", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21844", "abs": "https://arxiv.org/abs/2601.21844", "authors": ["So Fukuhara", "Abdallah Alabdallah", "Nuwan Gunasekara", "Slawomir Nowaczyk"], "title": "Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework", "comment": "12 pages, 6 figures", "summary": "Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.", "AI": {"tldr": "The paper builds a simulation framework to evaluate demand forecasting models for automotive spare parts based on inventory cost and service performance, not just forecast error metrics.", "motivation": "In the automotive aftermarket, spare parts demand is intermittent and uncertain, making inventory decisions costly and risky. Although forecasting is key, most research evaluates models with statistical accuracy metrics like MAE or RMSE, which may not correlate with operational KPIs such as total cost or service level. There is a gap in understanding how forecast accuracy translates into real inventory performance and how to choose models based on business impact rather than error scores alone.", "method": "The authors design a decision-centric, closed-loop simulation framework. It includes: (i) a synthetic demand generator that mimics spare-parts demand patterns; (ii) a modular forecasting component that can plug in any predictive model; and (iii) an inventory control simulator that uses the forecasts to make inventory decisions and compute operational KPIs (e.g., cost, service level). They run extensive simulation experiments under diverse scenarios, comparing forecasting models by both traditional error metrics and by resulting inventory performance, then analyze discrepancies between these perspectives.", "result": "Simulation studies reveal that better traditional accuracy metrics (MAE, RMSE, IAE, etc.) do not necessarily yield superior inventory performance. Models with similar statistical error characteristics can produce very different cost\u2013service trade-offs. The framework exposes which dimensions of forecast performance materially affect inventory outcomes and where improvements in error metrics are operationally irrelevant or even misleading.", "conclusion": "The proposed framework makes it possible to systematically connect forecasting performance with inventory decisions in the automotive spare parts context. It demonstrates that evaluating forecasting models solely by statistical accuracy is insufficient and can misguide model selection. Instead, models should be judged in terms of their operational impact on inventory KPIs. The approach provides practical guidance for choosing forecasting methods based on cost and service implications and can be extended to other domains with similar demand characteristics."}}
{"id": "2601.21864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21864", "abs": "https://arxiv.org/abs/2601.21864", "authors": ["Jinhao Pan", "Chahat Raj", "Anjishnu Mukherjee", "Sina Mansouri", "Bowen Wei", "Shloka Yada", "Ziwei Zhu"], "title": "KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement", "comment": null, "summary": "Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \\textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.", "AI": {"tldr": "KnowBias is a lightweight framework that debiases LLMs by amplifying neurons encoding bias-related knowledge instead of suppressing them, achieving strong debiasing with minimal impact on model utility and no retraining.", "motivation": "Current LLMs display social biases that can reinforce harmful stereotypes, which is problematic for safe and fair deployment. Existing debiasing approaches mostly rely on suppressing biased behaviors via parameter changes, prompt engineering, or neuron editing. These suppressive methods tend to be brittle, do not generalize well across tasks or demographic groups, need substantial data or retraining, and often hurt the model\u2019s general capabilities. The paper is motivated by the need for a more robust, generalizable, and data-efficient debiasing method that maintains model performance while effectively mitigating bias.", "method": "The authors introduce KnowBias, a framework that focuses on strengthening neurons that encode bias-related knowledge rather than suppressing them. First, they use a small set of yes/no bias-knowledge questions to perform attribution-based analysis over the model\u2019s internal activations and identify neurons that are highly responsible for bias-knowledge. Then, at inference time, they selectively enhance (amplify) these identified neurons\u2019 activations, without retraining or permanently altering model parameters. This procedure is applied as a lightweight, plug-in intervention that can be used with different LLMs and across different bias types and demographic groups.", "result": "Across multiple benchmarks designed to measure social bias in language models and on several underlying LLM architectures, KnowBias achieves state-of-the-art debiasing performance. It significantly reduces biased outputs while incurring minimal degradation in general utility metrics such as accuracy or task performance. The method also demonstrates strong generalization across various types of social biases and demographic categories, all while using only a small number of simple bias-knowledge questions and no additional training data or fine-tuning.", "conclusion": "KnowBias offers a conceptually different and practical alternative to traditional suppressive debiasing strategies by leveraging and amplifying internal bias-knowledge neurons. The approach is lightweight, data-efficient, and easily applied at inference time, avoiding retraining and preserving general capabilities. Empirical evaluations show that it consistently provides strong debiasing with minimal utility loss across models and benchmarks, indicating its promise as a scalable and robust debiasing tool for LLM deployment."}}
{"id": "2601.21872", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21872", "abs": "https://arxiv.org/abs/2601.21872", "authors": ["Yao Zhang", "Shijie Tang", "Zeyu Li", "Zhen Han", "Volker Tresp"], "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents", "comment": "ICLR 2026", "summary": "Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.", "AI": {"tldr": "Introduces WebArbiter, a reasoning-first process reward model for web agents that uses text-generated justifications and preference verdicts, trained with reasoning distillation and RL, and evaluated on a new benchmark where it outperforms strong baselines.", "motivation": "Web agents need reliable feedback for long-horizon, irreversible web tasks, but existing process reward models for web navigation are coarse, brittle, and poorly grounded, making them unreliable for supervision and search. A more principled, interpretable, and robust reward modeling approach is needed to guide agents and support inference-time scaling.", "method": "Formulate process reward modeling as text generation: given a web context and candidate actions, the model outputs structured natural-language justifications that end with a preference verdict and identify the best action. Train in two stages: (1) reasoning distillation from a teacher to instill principle-based, coherent reasoning patterns; (2) reinforcement learning to align the model\u2019s final verdicts with true correctness, correcting teacher biases. Additionally, introduce WebPRMBench, a benchmark with diverse web environments, tasks, and high-quality preference labels for systematic evaluation.", "result": "On WebPRMBench, the WebArbiter-7B model beats the strongest baseline (GPT-5) by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it outperforms the best previous WebPRM by up to 7.2 points, indicating better robustness and utility in practical web navigation tasks.", "conclusion": "A reasoning-first, text-generative process reward model with structured justifications, trained via reasoning distillation plus RL, provides more accurate, interpretable, and robust supervision for web agents. The accompanying WebPRMBench benchmark enables systematic evaluation, and experiments show substantial gains over prior WebPRMs and strong LLM baselines in complex, real-world-like web environments."}}
{"id": "2601.21879", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21879", "abs": "https://arxiv.org/abs/2601.21879", "authors": ["Rem Collier", "Katharine Beaumont", "Andrei Ciortea"], "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming", "comment": null, "summary": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.", "AI": {"tldr": "The paper reports on integrating large language models with the ASTRA agent programming language, using examples to explore mutual influences between traditional agent toolkits and modern agentic AI platforms.", "motivation": "To understand how emerging Generative and Agentic AI technologies interact with and can be supported by established multi-agent system toolkits, and how the design wisdom in those toolkits can guide the development of new agentic platforms.", "method": "Develop a prototype integration of an LLM into the ASTRA agent programming language, implement three example systems using this integration, and analyze the development process and outcomes.", "result": "A working prototype LLM integration with ASTRA and three illustrative example implementations that showcase different ways LLMs can be used within agent-based systems.", "conclusion": "Traditional agent toolkits like ASTRA can be effectively extended with LLM capabilities, and the experience and patterns from such toolkits provide valuable guidance for designing robust, agentic AI platforms that leverage generative models."}}
{"id": "2601.21909", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21909", "abs": "https://arxiv.org/abs/2601.21909", "authors": ["Shaojie Wang", "Liang Zhang"], "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning", "comment": null, "summary": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.", "AI": {"tldr": "The paper proposes a cognitively-inspired post-training framework for large language models that separates learning abstract reasoning strategies from task-specific execution, improving generalization and efficiency.", "motivation": "Existing LLM post-training treats full reasoning trajectories as indivisible units via SFT and outcome-based RL. This entangles high-level, generalizable reasoning strategies with problem-specific steps and does not align with how humans solve problems, who first learn abstract strategies (meta-knowledge) and then adapt them to specific tasks. The authors aim to close this gap to get better generalization and efficiency.", "method": "They introduce a two-stage framework inspired by human cognition: (1) Chain-of-Meta-Thought (CoMT), a supervised learning stage that focuses on abstract reasoning patterns instead of full problem-specific trajectories, training the model to acquire generalizable meta-strategies; (2) Confidence-Calibrated Reinforcement Learning (CCRL), an RL stage that optimizes how the model adapts those strategies to concrete tasks using confidence-aware rewards at intermediate steps, which penalize overconfident wrong steps and encourage calibrated decision-making. This decomposes post-training into meta-strategy learning and confidence-aware execution refinement.", "result": "Across four different base models and eight benchmarks, the proposed framework yields average performance gains of 2.19% on in-distribution and 4.63% on out-of-distribution evaluations compared with standard SFT+outcome-based RL pipelines. It also reduces training time by 65\u201370% and token consumption by about 50%, indicating both performance and efficiency benefits.", "conclusion": "Aligning LLM post-training with a two-stage human-like cognitive process\u2014first learning abstract meta-reasoning strategies and then refining task-specific execution through confidence-calibrated RL\u2014leads to better generalization, fewer cascading reasoning errors, and significantly more efficient training than conventional trajectory-level SFT+RL approaches."}}
{"id": "2601.21898", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.21898", "abs": "https://arxiv.org/abs/2601.21898", "authors": ["Minwoo Jang", "Hoyoung Kim", "Jabin Koo", "Jungseul Ok"], "title": "Making Models Unmergeable via Scaling-Sensitive Loss Landscape", "comment": "Preprint", "summary": "The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \\emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \\textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \\textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.", "AI": {"tldr": "The paper introduces Trap^2, a method that embeds protection against unauthorized model merging directly into fine-tuning updates, preserving standalone performance while degrading performance after merging-like rescaling.", "motivation": "Model hubs and reusable components make it easy to merge models and combine capabilities, but this also opens a governance gap: users can create unauthorized merged models that bypass safety alignment or license constraints. Existing defenses are reactive, architecture-specific, and inconsistent across formats, so a more general, proactive protection is needed.", "method": "The authors design Trap^2 as an architecture-agnostic protection mechanism applied during fine-tuning. They model merging operations through simple weight rescaling and then optimize the fine-tuning updates so that the resulting weights work well as a standalone model but become ineffective or degraded when subjected to such rescaling, which commonly occurs in model merging. The method works both for full model weights and for adapters.", "result": "Trap^2-protected models reportedly maintain good performance when used as intended (without merging), but their performance degrades significantly when their weights are rescaled in ways that emulate typical model merging operations, thereby undermining unauthorized merged derivatives.", "conclusion": "Encoding protection into fine-tuning updates via rescaling-sensitive weight structures offers a general way to resist unauthorized model merging across architectures and release formats, helping close the governance gap associated with modular model reuse on model hubs."}}
{"id": "2601.21912", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21912", "abs": "https://arxiv.org/abs/2601.21912", "authors": ["Zhao Wang", "Ziliang Zhao", "Zhicheng Dou"], "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation", "comment": "11 pages, 6 figures", "summary": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to \"process hallucinations\", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.", "AI": {"tldr": "The paper proposes ProRAG, a process-supervised reinforcement learning framework that improves Retrieval-Augmented Generation on complex reasoning tasks by combining step-level process rewards with global outcome rewards.", "motivation": "Outcome-based RL for RAG suffers from sparse rewards and poor credit assignment, leading to process hallucinations where models reach correct answers via flawed or inefficient reasoning steps. Existing process-aware methods use static preferences or heuristic shaping, but lack on-policy exploration to properly assign credit at the step level.", "method": "ProRAG introduces a four-stage framework: (1) supervised policy warmup to teach a structured reasoning format; (2) building an MCTS-based Process Reward Model (PRM) that scores intermediate reasoning steps; (3) PRM-guided reasoning refinement to align the policy with process-level preferences; and (4) process-supervised RL using a dual-granularity advantage mechanism that combines step-level process rewards with final outcome rewards for more accurate credit assignment.", "result": "Across five multi-hop reasoning benchmarks, ProRAG outperforms strong outcome-based and process-aware RL baselines, with especially large gains on complex long-horizon reasoning tasks, indicating more effective optimization of reasoning processes in RAG.", "conclusion": "Fine-grained, learned process supervision integrated into the RL loop can significantly improve RAG performance on complex reasoning tasks by resolving reward sparsity and credit assignment issues and reducing process hallucinations. ProRAG validates that combining step-level and outcome-level rewards yields more reliable and efficient reasoning behavior."}}
{"id": "2601.21916", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21916", "abs": "https://arxiv.org/abs/2601.21916", "authors": ["Yiqun Chen", "Erhan Zhang", "Tianyi Hu", "Shijie Wang", "Zixuan Yang", "Meizhi Zhong", "Xiaochi Wei", "Yan Gao", "Yi Wu", "Yao Hu", "Jiaxin Mao"], "title": "JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG", "comment": null, "summary": "The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \\textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \\textbf{JADE} (\\textbf{J}oint \\textbf{A}gentic \\textbf{D}ynamic \\textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \\textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.", "AI": {"tldr": "The paper introduces JADE, a framework that jointly optimizes both planning and execution in dynamic RAG workflows to eliminate mismatches between strategy and tool behavior, significantly improving performance.", "motivation": "Current RAG systems either use rigid, fixed-graph architectures with jointly trained modules, or flexible agentic planners that rely on frozen, black-box executors. This separation leads to a strategic-operational mismatch: the planner may devise sophisticated multi-step strategies that executors cannot effectively realize, causing complexity to rise without corresponding performance gains. The authors aim to close this gap by allowing planning and execution components to adapt to each other.", "method": "The authors propose JADE, which models a dynamic RAG workflow as a cooperative multi-agent system (planner plus executors) that share a single backbone model. Instead of optimizing planner and executors separately, JADE performs end-to-end learning using outcome-based rewards, encouraging both components to co-adapt. The planner learns strategies that are feasible given current executor capabilities, while executors are updated to better serve emergent planning behavior, all under a joint training objective supporting dynamic multi-turn orchestration.", "result": "Empirical experiments show that when planner and executors are trained jointly under JADE, the overall system performance significantly exceeds that of baselines with decoupled optimization or rigid pipelines. The framework converts previously disjoint modules into a synergistic whole and achieves better task success rates and quality of responses, while allowing the system to dynamically trade off efficiency and effectiveness depending on the context.", "conclusion": "Jointly optimizing planning and execution in dynamic RAG workflows resolves the strategic-operational mismatch present in prior paradigms. By treating planner and executors as a cooperative multi-agent team under a shared backbone and training them end-to-end with outcome-based rewards, JADE enables co-adaptation, more effective dynamic workflows, and substantial performance improvements over decoupled or rigid alternatives."}}
{"id": "2601.21919", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21919", "abs": "https://arxiv.org/abs/2601.21919", "authors": ["Yiqun Chen", "Jinyuan Feng", "Wei Yang", "Meizhi Zhong", "Zhengliang Shi", "Rui Li", "Xiaochi Wei", "Yan Gao", "Yi Wu", "Yao Hu", "Zhiqiang Pu", "Jiaxin Mao"], "title": "Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning", "comment": null, "summary": "The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \\textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \\textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \\textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\\% to 39.0\\% while boosting accuracy by 4.33\\% to 10.02\\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.", "AI": {"tldr": "The paper proposes a multi\u2011agent reinforcement learning framework to compress large reasoning models\u2019 chains of thought by removing redundant parts while preserving essential logic, thereby shortening responses and improving accuracy.", "motivation": "Large Reasoning Models often perform redundant reasoning, which increases inference time and harms interactive user experience. Existing RL-based approaches use crude length penalties tied to final outcomes, making it hard to balance brevity and correctness because penalizing length uniformly can remove important reasoning steps. A more fine-grained way to distinguish essential from redundant reasoning is needed.", "method": "The authors introduce Self-Compression via Multi-Agent Reinforcement Learning (SCMA). It uses three cooperating agents: (1) a Segmentation Agent that breaks a model\u2019s reasoning trace into logical chunks; (2) a Scoring Agent that estimates the importance of each chunk; and (3) a Reasoning Agent that generates the reasoning itself. During training, the Segmentation and Scoring Agents produce an importance-weighted length penalty that guides the Reasoning Agent to keep essential chunks and avoid redundant ones. This design aims to shape the reward more precisely than a global length penalty, without adding overhead at inference time.", "result": "Across different model sizes, SCMA reduces response length by 11.1%\u201339.0% while simultaneously increasing task accuracy by 4.33%\u201310.02%. Ablation studies show that removing or simplifying components degrades these gains, and qualitative analysis indicates that the multi-agent setup leads to emergent cooperative behaviors that improve reasoning quality beyond standard single-agent RL baselines.", "conclusion": "By decomposing reasoning into segments and assigning importance-aware penalties via a multi-agent RL framework, SCMA can compress reasoning chains selectively\u2014removing redundancy while preserving crucial logic. This both reduces inference cost and improves accuracy, demonstrating that structured multi-agent credit assignment is more effective than naive length-penalized RL for optimizing large reasoning models."}}
{"id": "2601.22154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22154", "abs": "https://arxiv.org/abs/2601.22154", "authors": ["Kaixuan Fan", "Kaituo Feng", "Manyuan Zhang", "Tianshuo Peng", "Zhixun Li", "Yilei Jiang", "Shuang Chen", "Peng Pei", "Xunliang Cai", "Xiangyu Yue"], "title": "Exploring Reasoning Reward Model for Agents", "comment": "Project page: https://github.com/kxfan2002/Reagent", "summary": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.", "AI": {"tldr": "Introduces Agent-RRM, a structured reward model for agentic reinforcement learning, and three integration strategies, with the unified method (Reagent-U) giving large performance gains on multiple benchmarks.", "motivation": "Existing agentic RL systems typically rely on sparse, outcome-based rewards that only judge final success or failure. This ignores the quality of intermediate reasoning steps, making it hard for agents to learn better reasoning processes and leading to suboptimal training efficiency and performance.", "method": "Propose Agent Reasoning Reward Model (Agent-RRM), which generates rich feedback for agent trajectories in three parts: (1) explicit reasoning traces, (2) focused critiques pointing out reasoning flaws and refinement directions, and (3) an overall score of process quality. Then design and compare three ways to integrate this feedback into training: Reagent-C (augment trajectories with textual refinement guidance), Reagent-R (use the scalar reward for RL-style optimization), and Reagent-U (combine both textual and scalar feedback in a unified framework). Evaluate these methods on 12 benchmarks involving complex reasoning and tool use such as GAIA and WebWalkerQA.", "result": "Across 12 diverse benchmarks, methods that use Agent-RRM outperform standard outcome-based approaches. Among them, Reagent-U achieves the largest improvements, including 43.7% accuracy on GAIA and 46.2% on WebWalkerQA, representing substantial performance gains compared to baselines.", "conclusion": "Structured, process-level reward modeling that critiques and scores intermediate reasoning steps significantly improves agentic RL. The unified integration of textual and scalar feedback (Reagent-U) is particularly effective, demonstrating that richer reasoning-aware supervision can meaningfully enhance complex reasoning and tool-use capabilities. The released code, models, and data are intended to support further work in this direction."}}
{"id": "2601.21936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21936", "abs": "https://arxiv.org/abs/2601.21936", "authors": ["Jon Chun", "Kathrine Elkins", "Yong Suk Lee"], "title": "AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making", "comment": "18 pages, 5 figures", "summary": "We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.", "AI": {"tldr": "AgenticSimLaw is a courtroom-style, multi-agent debate framework that structures LLM reasoning for high-stakes tabular decisions, yielding more stable, auditable, and controllable performance than single-agent CoT prompting, demonstrated on recidivism prediction.", "motivation": "LLMs used for high-stakes decisions (e.g., in justice or other sensitive domains) often rely on opaque, single-agent, chain-of-thought reasoning that is hard to audit, control, or hold accountable. Existing multi-agent systems can also be black-box, lacking clear role definitions, interaction protocols, and full observability of how decisions are made. There is a need for a framework that organizes LLM agents, makes their reasoning process transparent and fully logged, improves robustness and generalization, and embeds explicit constraints against direct deployment in sensitive, real-world settings.", "method": "The authors design AgenticSimLaw, a role-structured multi-agent framework modeled on a courtroom. It defines explicit roles (prosecutor, defense, judge), a fixed 7-turn structured debate protocol, and private reasoning strategies for each agent. The framework is applied to a tabular prediction task: young adult recidivism prediction using the NLSY97 dataset. They systematically benchmark almost 90 combinations of language models and reasoning strategies, comparing the multi-agent debate approach to traditional single-agent chain-of-thought prompting. All interactions and reasoning steps are logged to provide complete transcripts for analysis, explainability, and behavior profiling.", "result": "Across the large set of model-strategy configurations, structured multi-agent debate achieves more stable and generalizable performance than single-agent CoT prompting. It exhibits tighter, more consistent alignment between accuracy and F1-score, indicating better-balanced decision quality. The framework also succeeds in producing fully auditable transcripts, enabling fine-grained control over each reasoning step and systematic analysis of how agents interact and reach conclusions.", "conclusion": "AgenticSimLaw shows that explicitly structured, courtroom-style multi-agent debates can improve both the reliability and transparency of LLM-based decision-making on high-stakes tabular tasks. By enforcing clear roles, fixed interaction protocols, and comprehensive logging, the framework addresses key challenges in LLM multi-agent systems: organizational structure, observability, and responsibility. Although demonstrated on criminal justice recidivism prediction as a stress test for ethical complexity, the approach is general and can be applied to other deliberative, high-stakes applications requiring transparent reasoning and human oversight. The authors underscore that such systems should not be directly deployed in sensitive real-world domains and provide data, results, and code under an open-source license for further research."}}
{"id": "2601.21937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21937", "abs": "https://arxiv.org/abs/2601.21937", "authors": ["Shuangshuang Ying", "Zheyu Wang", "Yunjian Peng", "Jin Chen", "Yuhao Wu", "Hongbin Lin", "Dingyu He", "Siyi Liu", "Gengchen Yu", "YinZhu Piao", "Yuchen Wu", "Xin Gui", "Zhongyuan Peng", "Xin Li", "Xeron Du", "Libo Qin", "YiXin Cao", "Ge Zhang"], "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities", "comment": null, "summary": "Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.", "AI": {"tldr": "The paper introduces DeR2, a controlled benchmark to test whether large language models can truly perform document-grounded scientific reasoning, separating reasoning ability from retrieval and memorization effects.", "motivation": "Existing benchmarks for large language models often conflate multiple components\u2014retrieval, tool orchestration, and parametric memorization\u2014so it is unclear if good performance reflects genuine reasoning over new scientific content. There is also instability due to open-web sources changing over time and models potentially memorizing training data rather than understanding it. The authors want a setting that cleanly evaluates deep, multi-step reasoning on fresh scientific material without these confounds.", "method": "They build DeR2, a sandbox for deep research that uses a frozen library of documents from recent (2023\u20132025) theoretical papers, supplemented with expert-annotated key concepts and validated rationales. Crucially, they design four evidence regimes: (1) Instruction-only, where the model gets just the question; (2) Concepts, where it receives gold key concepts but not full documents; (3) Related-only, where it gets only truly relevant documents; and (4) Full-set, where it receives relevant documents mixed with topically related distractors. This structure isolates reasoning losses from retrieval-like losses and enables interpretable comparisons across regimes. They also enforce a two-phase validation: a question must be unsolvable from parametric knowledge alone (models fail without evidence) but solvable given oracle concepts, to ensure the benchmark truly tests document-grounded reasoning.", "result": "Across a range of state-of-the-art foundation models, performance varies widely and there is substantial room for improvement. Some models actually degrade when provided with the full document set plus distractors compared to having only the instruction, indicating mode-switch fragility in how they use evidence. Others show a different failure mode\u2014structural concept misuse\u2014where they can name the correct scientific concepts but cannot correctly apply them procedurally in multi-step reasoning. These patterns highlight distinct dimensions of reasoning weakness that DeR2 can surface.", "conclusion": "DeR2 effectively disentangles document-grounded reasoning from retrieval and memorization, offering a controlled, reproducible benchmark for deep scientific research tasks. The observed failure modes\u2014such as poorer performance with more evidence and incorrect procedural use of correctly identified concepts\u2014demonstrate that current large language models still lack robust deep-research reasoning capabilities. The benchmark provides a tool for fine-grained error attribution and for driving future model and system improvements in scientific reasoning."}}
{"id": "2601.21947", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21947", "abs": "https://arxiv.org/abs/2601.21947", "authors": ["Bowen Fang", "Wen Ye", "Yunyue Su", "Jinghao Zhang", "Qiang Liu", "Yesheng Liu", "Xin Sun", "Shu Wu", "Jiabing Yang", "Baole Wei", "Liang Wang"], "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models", "comment": "10pages, 12 figures, Accepted to ICLR 2026", "summary": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.", "AI": {"tldr": "ToolWeaver replaces one-token-per-tool IDs with hierarchical tool codes so LLMs can scale to tens of thousands of tools and learn richer semantic and collaborative tool-use patterns.", "motivation": "Existing retrieval-based and generative tool-use systems break down at scale. Retrievers\u2019 encoders struggle with complex semantics, and LLMs don\u2019t inherently know tools. Generative approaches that assign each tool a unique token lead to vocabulary explosion, poor generalization to new tools, and weak modeling of relationships between tools, because each tool is an isolated symbol. The paper aims to build a tool-usage representation that is both scalable and semantically structured so that LLMs can understand and coordinate many tools more effectively.", "method": "The authors introduce ToolWeaver, which encodes each tool not as a single ID token but as a hierarchical sequence of tokens (a structured code). They design a novel tokenization process that jointly reflects a tool\u2019s intrinsic semantics (e.g., description, functionality) and its extrinsic co-usage patterns with other tools. This produces codes that share sub-sequences across related tools. The LLM is then fine-tuned in a generative alignment stage to output these hierarchical codes when deciding which tools to call, thereby unifying tool selection and execution in generation while exploiting shared code structure.", "result": "On a benchmark with about 47,000 tools, ToolWeaver yields substantially better performance than state-of-the-art alternatives that use monolithic tool IDs or retrieval-based pipelines. Its hierarchical coding scheme allows the vocabulary to grow only logarithmically with the number of tools while improving accuracy and robustness in tool selection and coordination.", "conclusion": "Representing tools as hierarchical token sequences instead of isolated ID tokens solves key scalability and semantic issues in generative tool-use. ToolWeaver\u2019s structured tokenization and generative alignment let LLMs learn dense co-occurrence patterns over shared code segments, improving semantic awareness and collaborative tool-use. This establishes a more scalable, generalizable framework for building advanced tool-augmented LLM agents that must handle very large tool libraries."}}
{"id": "2601.21961", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21961", "abs": "https://arxiv.org/abs/2601.21961", "authors": ["Kuai Yu", "Naicheng Yu", "Han Wang", "Rui Yang", "Huan Zhang"], "title": "How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors", "comment": null, "summary": "Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.", "AI": {"tldr": "They propose VAF, a controlled evaluation pipeline to measure how visual attributes of webpages (like color, size, and position) affect web agents\u2019 decision-making, finding that some visual factors strongly bias agent behavior even when page semantics are identical.", "motivation": "While web agents perform well on many tasks, prior work has mainly tested robustness to adversarial changes or textual variations, leaving a gap in understanding how normal, benign visual differences in webpages influence agent preferences and choices. A systematic, controlled study of visual attribute effects on agent decision-making is missing.", "method": "They build VAF, a three-stage evaluation pipeline: (1) Variant generation: create webpage variants that preserve the same semantic content as the original but change only specific visual attributes (e.g., color, size, position, clarity). (2) Browsing interaction: let web agents interact naturally with these pages via scrolling and clicking as human users would. (3) Validation and measurement: evaluate both the agents\u2019 click actions and their textual reasoning, defining Target Click Rate and Target Mention Rate to quantify how much visual changes alter their decisions, and compare behavior between original and variant pages to isolate the impact of each visual factor.", "result": "Using 8 families of visual variants (48 variants total) across 5 real-world websites and 4 representative web agents, they find that certain visual features\u2014background color contrast, item size, item position on the page, and clarity of the item card\u2014substantially affect agents\u2019 actions. In contrast, other visual attributes\u2014font styling, text color, and image clarity\u2014have comparatively minor influence on what the agents click or mention as targets.", "conclusion": "Web agents\u2019 decisions are significantly and selectively influenced by specific visual design choices, even when underlying semantics are unchanged. VAF provides a controlled framework to quantify these effects, revealing that layout- and salience-related attributes (contrast, size, position, card clarity) matter much more than typography or minor visual refinements. This has implications for designing robust web agents and for understanding how interface design can bias agent behavior."}}
{"id": "2601.21967", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21967", "abs": "https://arxiv.org/abs/2601.21967", "authors": ["Ilche Georgievski", "Serhat Tekin", "Marco Aiello"], "title": "The Energy Impact of Domain Model Design in Classical Planning", "comment": "2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI (CAIN '26)", "summary": "AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.", "AI": {"tldr": "The paper studies how properties of automated planning domain models influence the energy consumption of classical planners, showing that domain-level changes can alter energy use independently of runtime.", "motivation": "Green AI emphasizes energy efficiency alongside traditional performance metrics like accuracy and runtime. Automated planners often require substantial computation, but their energy usage has been largely neglected. Since planning domains are explicitly modeled and separated from planning algorithms, they present an opportunity to systematically investigate and potentially optimize energy consumption through domain design rather than only through algorithmic changes.", "method": "The authors propose a domain model configuration framework that allows systematic variation of domain features, such as the ordering of elements, the arity of actions (number of parameters), and the presence of dead-end states. Using this framework, they generate 32 domain variants for each of five standard planning benchmarks. They then run five state-of-the-art classical planners on these variants, measuring both energy consumption and runtime to assess how domain characteristics affect energy efficiency.", "result": "The experiments show that modifications at the domain-model level lead to clear and measurable differences in energy consumption across planners. These differences are not always aligned with changes in runtime, indicating that faster solving does not necessarily mean lower energy use. The results highlight that domain design choices can impact energy efficiency in nontrivial ways across different planners.", "conclusion": "Energy consumption in automated planning is significantly influenced by how domain models are constructed, and not solely by the planner algorithms themselves. Domain-level modifications can change energy usage patterns and may decouple energy from runtime performance. This suggests that Green AI objectives in planning should consider domain modeling practices as a lever for improving energy efficiency, and that evaluation metrics should incorporate energy alongside traditional performance measures."}}
{"id": "2601.21972", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21972", "abs": "https://arxiv.org/abs/2601.21972", "authors": ["Shuo Liu", "Tianle Chen", "Ryan Amiri", "Christopher Amato"], "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic", "comment": null, "summary": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.", "AI": {"tldr": "The paper proposes multi-agent actor-critic methods to improve decentralized collaboration between large language model (LLM) agents, especially for long-horizon and sparse-reward tasks.", "motivation": "Existing MARL-based LLM collaboration methods mostly assume predefined, often centralized protocols that limit practical, parallel deployment. They also rely on Monte Carlo training, which has high variance and sample inefficiency, particularly in long-horizon or sparse-reward scenarios. The authors aim to enable more practical decentralized LLM collaboration that trains efficiently and scales to complex tasks.", "method": "They design Multi-Agent Actor-Critic (MAAC) approaches tailored to LLM collaboration. Two variants are introduced: CoLLM-CC, which uses a centralized critic during training while maintaining decentralized actors for execution, and CoLLM-DC, which uses purely decentralized critics. They compare these with Monte Carlo fine-tuning methods over tasks in writing, coding, and game playing, focusing on horizon length and reward sparsity. The study analyzes convergence, sample efficiency, and performance trade-offs between centralized and decentralized critics.", "result": "Experiments show that in short-horizon, dense-reward tasks, Monte Carlo methods and CoLLM-DC achieve performance similar to the centralized-critic CoLLM-CC. In contrast, for long-horizon or sparse-reward tasks, Monte Carlo methods need significantly more samples to train effectively, and CoLLM-DC has convergence difficulties. In these challenging settings, CoLLM-CC clearly outperforms the other approaches.", "conclusion": "Actor-critic methods with a centralized critic (CoLLM-CC) provide clear advantages for training decentralized LLM collaborations on long-horizon and sparse-reward problems, improving stability and sample efficiency over Monte Carlo and fully decentralized critic approaches. While simpler Monte Carlo or decentralized-critic methods suffice for easier tasks, a centralized critic is key for robust learning in more complex collaborative scenarios."}}
{"id": "2601.21975", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21975", "abs": "https://arxiv.org/abs/2601.21975", "authors": ["Pranav Mahajan", "Ihor Kendiukhov", "Syed Hussain", "Lydia Nottingham"], "title": "Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models", "comment": null, "summary": "Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($\u03c1$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $\u03c1$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.", "AI": {"tldr": "The paper studies how different prompting/elicitation protocols affect the measured gap between what language models say they value and what they actually choose, showing that this correlation is highly sensitive to the protocol and often breaks down when neutrality/abstention are allowed.", "motivation": "Language models often appear to endorse certain values in direct questions but then behave inconsistently in decision-like scenarios, creating a stated\u2013revealed preference gap. Existing benchmarks mostly use rigid binary forced-choice setups that may conflate real preferences with artifacts of the prompting design. The authors want to understand whether the observed stated\u2013revealed preference correlations are robust or mostly driven by specific elicitation protocols, and how to better handle cases where models have indeterminate or weak preferences.", "method": "The authors evaluate 24 language models on preference benchmarks (including AIRiskDilemmas) under systematically varied elicitation protocols. They manipulate whether models can express neutrality or abstain in (1) stated preference questions and (2) revealed preference (decision) tasks. They compute Spearman rank correlations between the distribution of volunteered stated preferences and forced-choice revealed preferences, and also test whether using the model\u2019s own stated preferences as a system prompt improves the alignment between what it says and what it chooses.", "result": "When neutrality/abstention options are added to stated preference elicitation and only strong, volunteered preferences are kept, the Spearman correlation between stated and revealed preferences improves notably. However, when similar neutrality/abstention options are allowed on the revealed preference side, correlation collapses to near-zero or negative because models frequently choose neutral/abstain, making their effective ranking of options unclear. Additionally, steering models during revealed preference elicitation via system prompts that encode their earlier stated preferences does not consistently increase stated\u2013revealed correlation on AIRiskDilemmas across models.", "conclusion": "Measured alignment between language models\u2019 stated values and their revealed choices depends strongly on the elicitation protocol. Allowing neutrality in stated preferences can sharpen the signal and improve correlation, but extending abstention to revealed choices undermines correlation due to pervasive neutrality. Simple steering of models based on their earlier stated values is insufficient to robustly close the stated\u2013revealed gap. Overall, preference elicitation for LMs needs more sophisticated methods that explicitly accommodate indeterminate or weak preferences rather than relying solely on binary forced-choice designs."}}
{"id": "2601.21981", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.21981", "abs": "https://arxiv.org/abs/2601.21981", "authors": ["Geonhee Jo", "Mingu Kang", "Kangmin Lee", "Minho Lee", "Pascal Bauer", "Sang-Ki Ko"], "title": "VERSA: Verified Event Data Format for Reliable Soccer Analytics", "comment": "13 pages, 5 figures, 3 tables", "summary": "Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.", "AI": {"tldr": "The paper proposes VERSA, a verification framework that uses a state-transition model to detect and correct logical inconsistencies in soccer event stream data, improving cross-provider consistency and downstream analytics performance.", "motivation": "Event stream data in domains like soccer analytics is widely used for fine-grained tasks such as quantifying player contributions and detecting tactics, but it often suffers from data quality issues like incorrect event ordering or missing events, which undermine the reliability of models that depend on it. There is a need for a systematic method to verify and correct these errors to ensure trustworthy analytics.", "method": "The authors design VERSA (Verified Event Data Format for Reliable Soccer Analytics), a verification framework grounded in a state-transition model that encodes valid sequences of soccer events. This model is used to automatically detect anomalous or logically inconsistent patterns in event streams and to correct them, thereby enforcing data integrity across heterogeneous event providers. They apply VERSA to K League 1 (2024) event data provided by Bepro and compare the refined data across providers and in a downstream analytics task (VAEP).", "result": "Applying VERSA to K League 1 2024 season event data reveals that 18.81% of all recorded events contain logical inconsistencies. After correction via VERSA, cross-provider consistency is significantly improved, resulting in a more stable and unified event representation across different data sources. Moreover, when the refined data is used in the VAEP model, both robustness and performance of player contribution evaluation are substantially enhanced.", "conclusion": "VERSA effectively verifies and corrects soccer event stream data by enforcing a state-transition model of valid event sequences. This verification process substantially improves data integrity, harmonizes event representations across providers, and boosts the reliability and performance of downstream analytics such as player contribution evaluation, demonstrating the importance and impact of systematic data verification in sports analytics."}}
{"id": "2601.21993", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21993", "abs": "https://arxiv.org/abs/2601.21993", "authors": ["Dhiogo de S\u00e1", "Carlos Schmiedel", "Carlos Pereira Lopes"], "title": "Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems", "comment": "28 pages, 2 figures", "summary": "Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems", "AI": {"tldr": "The paper proposes Liquid Interfaces, a new way for autonomous software agents to coordinate using ephemeral, negotiable interfaces instead of fixed, deterministic APIs.", "motivation": "Current software architectures are poorly suited for autonomous agents that reason in adaptive, probabilistic, and context-dependent ways, because they rely on static interfaces and deterministic contracts. There is a need for a coordination model that can match the fluid, uncertain nature of such agents\u2019 reasoning and interactions.", "method": "The authors conceptualize and formalize a new coordination paradigm called Liquid Interfaces, where interfaces are treated as transient relational events created at runtime via intention articulation and semantic negotiation. They define the Liquid Interface Protocol (LIP), which specifies rules for intention-driven interaction, negotiated execution, and enforced ephemerality under semantic uncertainty. They also outline a reference architecture to show how this can be implemented in practice.", "result": "The paper delivers: (1) a formal model of Liquid Interfaces as ephemeral, negotiated interaction points; (2) the design of the Liquid Interface Protocol that operationalizes intention-driven and probabilistic coordination; and (3) a reference architecture that demonstrates that such a paradigm is technically feasible for agent-based systems.", "conclusion": "Liquid Interfaces offer a principled foundation for adaptive coordination among autonomous agents by replacing rigid, static APIs with runtime, negotiated, and ephemeral interfaces. This paradigm better accommodates semantic uncertainty and probabilistic reasoning and has governance implications for how such systems are managed and controlled."}}
{"id": "2601.22001", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22001", "abs": "https://arxiv.org/abs/2601.22001", "authors": ["Yiren Zhao", "Junyi Liu"], "title": "Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference", "comment": null, "summary": "AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.", "AI": {"tldr": "The paper analyzes AI agent inference bottlenecks beyond compute and proposes two metrics, Operational Intensity (OI) and Capacity Footprint (CF), to understand and guide system and hardware design for efficient large-scale agentic AI inference.", "motivation": "As AI agents become more prevalent, datacenter workloads are increasingly dominated by inference, revealing that traditional compute-focused optimization and classic roofline models fail to capture key bottlenecks such as memory capacity, bandwidth, and interconnect limits. There is a need for a better analytic framework to characterize these new workloads and inform future systems and hardware architectures.", "method": "The paper introduces and applies two metrics\u2014Operational Intensity (OI) and Capacity Footprint (CF)\u2014to various agentic workflows (chat, coding, web use, computer use) and different model configurations (GQA/MLA, MoE, quantization). It uses these metrics to analyze where workloads become memory-bound, especially due to long-context KV cache, and to explore how shifts in OI/CF motivate architectural changes such as disaggregated serving, heterogeneous accelerators, and decoupled compute-memory via optical I/O.", "result": "The analysis shows that OI and CF vary substantially across agentic use cases and model designs, and that decode phases with long context KV caches are often highly memory-bound, hitting a \"memory capacity wall\" that classic roofline misses. This characterization reveals the need for specialized prefill vs. decode accelerators, high-bandwidth interconnects, and memory disaggregation to maintain performance and efficiency.", "conclusion": "By framing agentic AI inference in terms of Operational Intensity and Capacity Footprint, the paper identifies memory and interconnect, rather than raw compute, as emerging primary bottlenecks. It argues that future datacenter and hardware design should embrace disaggregated serving, heterogeneous inference accelerators, optical I/O, and large-scale memory disaggregation, coupled with agent-hardware co-design, to keep pace with evolving AI agent workloads and sustain efficient large-scale inference."}}
{"id": "2601.22027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22027", "abs": "https://arxiv.org/abs/2601.22027", "authors": ["Johannes Kirmayr", "Lukas Stappen", "Elisabeth Andr\u00e9"], "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty", "comment": null, "summary": "Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.", "AI": {"tldr": "CAR-bench is a benchmark to test how reliably LLM agents act as in-car assistants, especially under uncertainty and missing tools, not just whether they can sometimes complete tasks.", "motivation": "Most current LLM agent benchmarks assume clear, complete user instructions and ideal tool access, mainly measuring whether an agent can ever succeed on a task. Real-world user-facing systems, such as in-car voice assistants, regularly encounter ambiguous, incomplete, or impossible requests. In these cases, reliability, policy compliance, and awareness of one\u2019s own limitations (e.g., missing tools or data) matter more than best-case performance. The authors are motivated to create an evaluation that captures these aspects\u2014consistency across runs, uncertainty management, and capability awareness\u2014in a realistic domain with complex tools and policies.", "method": "The authors build CAR-bench, a simulated in-car assistant environment powered by an LLM user simulator, a set of explicit domain policies, and 58 interconnected tools covering navigation, productivity, EV charging, and vehicle control. They design multi-turn scenarios where an LLM agent must use tools, ask clarification questions, and follow policies to help the user. CAR-bench includes: (1) standard task-completion scenarios; (2) Hallucination tasks, where required tools or information are intentionally missing to test whether the agent recognizes its limits instead of fabricating; and (3) Disambiguation tasks, where user requests are incomplete or ambiguous, forcing the agent to resolve uncertainty via clarification or internal information gathering. They then evaluate several LLM-based agents, including frontier reasoning models, measuring both one-shot success and consistency across repeated runs, as well as policy adherence and hallucination behaviors.", "result": "CAR-bench experiments show that existing LLM agents, including strong reasoning models, are highly inconsistent and often unreliable in the in-car setting. There are large gaps between occasional success and consistent success across all task types. On Disambiguation tasks, even frontier models achieve under 50% consistent pass rates, mainly because they act prematurely instead of seeking clarification. On Hallucination tasks, agents frequently violate explicit domain policies or invent tools/information to satisfy user requests instead of acknowledging limitations. These findings indicate that current models are far from robust for safety-critical, user-facing applications like in-car assistants.", "conclusion": "The paper concludes that best-case task completion metrics are inadequate for evaluating LLM agents in realistic deployments. CAR-bench highlights key failure modes\u2014poor uncertainty handling, lack of limit-awareness, inconsistency across runs, and policy violations\u2014that are not captured by existing benchmarks. The authors argue that improving reliability and self-awareness in LLM agents is essential for real-world domains such as in-car assistants and propose CAR-bench as a challenging, domain-rich benchmark to drive research on these aspects."}}
{"id": "2601.22037", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22037", "abs": "https://arxiv.org/abs/2601.22037", "authors": ["Sami Abuzakuk", "Anne-Marie Kermarrec", "Rishi Sharma", "Rasmus Moorits Veski", "Martijn de Vos"], "title": "Optimizing Agentic Workflows using Meta-tools", "comment": null, "summary": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.", "AI": {"tldr": "They propose Agent Workflow Optimization (AWO), which turns frequently recurring sequences of tool calls in LLM agents into single composite \u201cmeta-tools\u201d to reduce cost, latency, and failures.", "motivation": "Agentic LLM systems solve complex tasks by planning and calling tools, but this leads to many iterative reasoning steps and tool invocations. This inflates operational cost and latency, and long workflows are more fragile and prone to hallucination-induced failures. There is a need to systematically optimize these workflows without redesigning agents from scratch.", "method": "AWO collects and analyzes traces of existing agent workflows to mine recurring patterns of tool usage\u2014specific sequences of tool calls frequently used together. It then compiles these sequences into deterministic composite tools, called meta-tools, that encapsulate multiple original agent actions. These meta-tools are invoked as single tools, bypassing intermediate LLM reasoning steps. The framework thus identifies redundancy and restructures the workflow around the learned meta-tools.", "result": "On two agentic AI benchmarks, AWO reduces the total number of LLM calls by up to 11.9% and increases task success rate by up to 4.2 percentage points, demonstrating both efficiency and robustness gains compared to the original agent workflows without optimization.", "conclusion": "Agent Workflow Optimization is an effective way to post-hoc optimize existing agentic systems by converting frequently co-occurring tool-call sequences into composite meta-tools. This reduces cost and latency while also improving reliability, without needing to fundamentally redesign the base agent model or its tools."}}
{"id": "2601.22118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22118", "abs": "https://arxiv.org/abs/2601.22118", "authors": ["Johann Christensen", "Elena Hoemann", "Frank K\u00f6ster", "Sven Hallerbach"], "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data", "comment": null, "summary": "Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.", "AI": {"tldr": "They propose a data-driven, kernel-based method to reconstruct and formalize the Operational Design Domain (ODD) of safety-critical AI systems from existing data, and show it can match the hidden true ODD, supporting certification.", "motivation": "In safety\u2011critical AI applications, certification bodies require a clear definition of the Operational Design Domain (ODD)\u2014the set of conditions under which the system is intended and allowed to operate. In complex real\u2011world systems or when only historical data exist, explicitly defining the environment is difficult and often incomplete. Current practice defines ODDs a priori using expert knowledge and standards, which can be costly, subjective, and potentially misaligned with how the system is actually used. A method is needed to derive precise, certifiable ODDs directly from data gathered in realistic operating conditions.", "method": "They introduce a Safety\u2011by\u2011Design, data\u2011driven approach that infers the ODD a posteriori from previously collected operational data. The key technical element is a multi\u2011dimensional kernel\u2011based representation that models the distribution and boundaries of environmental and operational variables. They define a formal notion of ODD equality\u2014conditions under which two ODDs are considered the same\u2014and use this to compare the inferred ODD with the underlying, unknown true ODD that generated the data. The approach is validated using Monte Carlo simulations and a real\u2011world aviation case study involving a future collision\u2011avoidance system.", "result": "The kernel\u2011based ODD construction successfully recovers an ODD from data that, under the defined equality conditions, can be proven to match the original hidden ODD that generated the data. Monte Carlo experiments support the correctness and robustness of the method, while the aviation collision\u2011avoidance case study demonstrates its practical applicability in a realistic, safety\u2011critical context.", "conclusion": "A novel, kernel\u2011based, Safety\u2011by\u2011Design framework can reconstruct an Operational Design Domain directly from historical or collected data, enabling alignment between actual operational conditions and the certified ODD. Under their formal equality definition, the data\u2011driven ODD can equal the true but hidden ODD, providing a sound basis for certifying data\u2011driven, safety\u2011critical AI systems. This paves the way for broader deployment of AI in safety\u2011critical domains with stronger assurance and regulatory acceptance."}}
{"id": "2601.22130", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22130", "abs": "https://arxiv.org/abs/2601.22130", "authors": ["Lakshya Gupta", "Litao Li", "Yizhe Liu", "Sriram Ganapathi Subramanian", "Kaheer Suleman", "Zichen Zhang", "Haoye Lu", "Sumit Pasupalak"], "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems", "comment": null, "summary": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.", "AI": {"tldr": "The paper presents World of Workflows (WoW), a realistic enterprise environment and benchmark showing that current large language model agents fail to model hidden enterprise system dynamics, leading to silent constraint violations, and argues for explicitly learning system dynamics for reliable enterprise agents.", "motivation": "Although frontier LLMs perform well as autonomous agents in many domains, they have not been properly tested in realistic enterprise environments like complex IT service systems, where hidden workflows and large, partially observable database states create cascading side effects. Existing enterprise benchmarks mostly measure surface-level task completion similar to consumer benchmarks and do not capture the real, unique challenges enterprises pose, such as limited observability, opaque business rules, and silent constraint violations. There is a need for a benchmark that stresses agents\u2019 ability to reason about and predict hidden dynamics in such systems.", "method": "The authors build World of Workflows (WoW), a realistic ServiceNow-based environment that includes over 4,000 business rules and 55 active workflows, designed to mirror complex enterprise IT service management systems. On top of this environment, they construct WoW-bench, a benchmark of 234 tasks that test two key capabilities: (1) constrained agentic task completion under enterprise constraints and (2) modeling and prediction of enterprise system dynamics, including hidden workflows and cascading side effects. They evaluate frontier LLMs as agents within this environment to analyze their behavior, particularly focusing on their ability to anticipate the invisible implications of their actions in an opaque, partially observable system.", "result": "Experiments in the WoW environment show that frontier LLM agents regularly fail to anticipate the hidden, cascading side effects triggered by their actions through underlying workflows and business rules. This leads to what the authors term \u201cdynamics blindness\u201d: the agents frequently violate constraints or cause unintended system changes without noticing, because these effects are not directly observable and must be inferred from an internal model of the system dynamics. The evaluation across 234 tasks demonstrates systematic weaknesses in both constrained task completion and accurate modeling of enterprise dynamics.", "conclusion": "The paper concludes that current frontier LLM agents are not yet reliable for deployment in complex, opaque enterprise systems, primarily because they lack grounded world models of system dynamics and thus cannot anticipate hidden, cascading state transitions. To achieve reliable enterprise agents, a new paradigm is required in which LLM agents explicitly learn and use models of system dynamics to mentally simulate possible outcomes and bridge the observability gap. The authors position WoW and WoW-bench as critical tools to drive research toward such dynamics-aware, grounded enterprise agents and release their setup scripts and evaluation framework to facilitate further work."}}
{"id": "2601.22141", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22141", "abs": "https://arxiv.org/abs/2601.22141", "authors": ["Grzegorz Stefanski", "Alberto Presta", "Michal Byra"], "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data", "comment": null, "summary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.", "AI": {"tldr": "They extend lottery ticket pruning to multiple adaptive subnetworks specialized for different data conditions, improving accuracy with far fewer parameters and proposing a metric to detect over-pruning.", "motivation": "Most lottery ticket/pruning methods assume a single universal sparse subnetwork works well for all inputs, which conflicts with the fact that real-world data are heterogeneous (different classes, semantic clusters, or environments). There is a need for pruning methods that adapt network structure to this heterogeneity while remaining parameter-efficient and for tools to detect when pruning becomes too aggressive and harms performance.", "method": "They introduce Routing the Lottery (RTL), an adaptive pruning framework that learns multiple specialized subnetworks (adaptive tickets) within a large network. Each subnetwork is routed to particular input subsets, such as classes, semantic clusters, or environmental conditions. They compare RTL to single pruned models and to multiple independently trained models, and they define a subnetwork similarity score to measure overlap between subnetworks and use it as a label-free signal of subnetwork collapse due to oversparsification.", "result": "On diverse datasets and tasks, RTL achieves higher balanced accuracy and recall than both single-model and multi-model baselines while using up to 10\u00d7 fewer parameters than training independent models. The discovered subnetworks are semantically aligned with data structure (e.g., classes or conditions). They also empirically observe subnetwork collapse under high sparsity and show that their similarity score correlates with this performance drop and can diagnose oversparsification without labels.", "conclusion": "Pruning can be reframed from merely compressing networks to aligning model structure with data heterogeneity. By routing inputs to specialized sparse subnetworks, RTL yields modular, context-aware models that are both more accurate and more parameter-efficient, and the proposed subnetwork similarity score offers a practical diagnostic for avoiding harmful over-pruning."}}
