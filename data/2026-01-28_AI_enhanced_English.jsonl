{"id": "2601.18899", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18899", "abs": "https://arxiv.org/abs/2601.18899", "authors": ["Yuchen Zhang", "Ravi Shekhar", "Haralambos Mouratidis"], "title": "Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries", "comment": null, "summary": "Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.", "AI": {"tldr": "They link a frozen speech encoder to an LLM using shared connectors per language family instead of per-language, reducing parameters and improving multilingual ASR generalization.", "motivation": "Existing LLM-based ASR systems need a separate connector module for each language, which is parameter-inefficient and ignores similarities between related languages. There is a need for a more scalable multilingual ASR approach that leverages linguistic relatedness while maintaining or improving performance across domains and languages.", "method": "They propose grouping languages by linguistic family and training a single connector module for all languages within a family, connecting a frozen speech encoder to a pretrained LLM. They evaluate this connector-sharing strategy on two multilingual LLMs and two real-world multilingual speech corpora (curated and crowd-sourced), comparing performance and parameter counts against standard per-language connectors.", "result": "Family-based connectors achieve better or comparable ASR performance while using fewer parameters, and they generalize better across different domains compared to per-language connectors. This demonstrates that connector sharing by language family effectively exploits linguistic relatedness.", "conclusion": "Sharing connectors across languages within the same linguistic family is an efficient and scalable design for LLM-based multilingual ASR. It reduces model size and improves cross-domain generalization, making it a practical strategy for real-world multilingual ASR deployment."}}
{"id": "2601.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18901", "abs": "https://arxiv.org/abs/2601.18901", "authors": ["Christopher Kissling", "Elena Merdjanovska", "Alan Akbik"], "title": "Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration", "comment": null, "summary": "Knowledge probing quantifies how much relational knowledge a language model (LM) has acquired during pre-training. Existing knowledge probes evaluate model capabilities through metrics like prediction accuracy and precision. Such evaluations fail to account for the model's reliability, reflected in the calibration of its confidence scores. In this paper, we propose a novel calibration probing framework for relational knowledge, covering three modalities of model confidence: (1) intrinsic confidence, (2) structural consistency and (3) semantic grounding. Our extensive analysis of ten causal and six masked language models reveals that most models, especially those pre-trained with the masking objective, are overconfident. The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing. Moreover, even the largest pre-trained models fail to encode the semantics of linguistic confidence expressions accurately.", "AI": {"tldr": "They introduce a new framework to measure how well language models\u2019 confidence about relational knowledge is calibrated, not just whether answers are correct.", "motivation": "Prior probing methods only look at whether models answer relational-knowledge questions correctly, ignoring how reliable the confidence scores are. This misses an important aspect of model trustworthiness, since a model can be accurate yet systematically over- or under-confident.", "method": "They design a calibration probing framework for relational knowledge that evaluates three different types of confidence: (1) intrinsic confidence (e.g., probability scores directly from the model), (2) structural consistency (how stable the confidence/answers are under paraphrases or structural variants of the same query), and (3) semantic grounding (alignment between model confidence and natural-language confidence expressions). They apply this framework to ten causal and six masked LMs.", "result": "Across the tested models, most\u2014especially masked LMs\u2014are found to be overconfident. Confidence estimates that explicitly account for inconsistencies across rephrasings yield the most calibrated scores. Large models still do not accurately represent the semantics of linguistic confidence expressions.", "conclusion": "Calibration is a distinct dimension of knowledge in LMs that existing probes overlook. Models, including large ones, tend to be overconfident and struggle to ground their confidence in both structural variants of queries and in natural-language expressions of certainty, showing a gap between accuracy and trustworthy confidence estimation."}}
{"id": "2601.18902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18902", "abs": "https://arxiv.org/abs/2601.18902", "authors": ["Jiaming Fan", "Daming Cao", "Xiangzhong Luo", "Jiale Fu", "Chonghan Liu", "Xu Yang"], "title": "Flatter Tokens are More Valuable for Speculative Draft Model Training", "comment": null, "summary": "Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://anonymous.4open.science/r/Flatness.", "AI": {"tldr": "They show you can train speculative decoding draft models much faster by carefully choosing which data to use, focusing on examples where the target model\u2019s predictions are flat (uncertain) rather than very confident.", "motivation": "Speculative Decoding speeds up LLM inference but usually needs a draft model trained on a large corpus, which is expensive. The authors want to reduce the cost of training this draft model without sacrificing much performance, and they hypothesize that only some training samples are truly important for improving the SD acceptance rate.", "method": "1) Theoretically and empirically analyze how different training tokens affect SD acceptance, finding that tokens where the target model has a flatter predictive distribution are more informative. 2) Define a new \"flatness\" metric to measure how flat the target model\u2019s output distribution is on each sample. 3) Propose SFDD (Sample-level-flatness-based Dataset Distillation), which filters the training corpus to keep only high-flatness samples and discards the rest. 4) Evaluate SFDD within the EAGLE SD framework.", "result": "Using SFDD to keep only about 50% of the training data yields more than a 2\u00d7 speedup in training the draft model, and the resulting speculative decoding inference speed is within 4% of a model trained on the full dataset (i.e., very small degradation).", "conclusion": "By quantifying and exploiting the notion of prediction flatness, one can distill the training dataset for speculative decoding draft models, significantly cutting training cost while largely preserving inference acceleration. This establishes a practical, data-centric way to make SD training more efficient."}}
{"id": "2601.18933", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18933", "abs": "https://arxiv.org/abs/2601.18933", "authors": ["Kaustubh D. Dhole"], "title": "BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models", "comment": null, "summary": "Traditional evaluations of reasoning capabilities of language models are dominated by adult-centric benchmarks that presuppose broad world knowledge, complex instruction following, and mature pragmatic competence. These assumptions are mismatched to baby language models trained on developmentally plausible input such as child-directed speech and early-childhood narratives, and they obscure which reasoning abilities (if any) emerge under such constraints. We introduce BabyReasoningBench, a GPT-5.2 generated benchmark of 19 reasoning tasks grounded in classic paradigms from developmental psychology, spanning theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that are known to be confounded by memory and pragmatics. We find that two GPT-2 based baby language models (pretrained on 10M and 100M of child-directed speech text) show overall low but uneven performance, with dissociations across task families: scaling improves several causal and physical reasoning tasks, while belief attribution and pragmatics-sensitive tasks remain challenging. BabyReasoningBench provides a developmentally grounded lens for analyzing what kinds of reasoning are supported by child-like training distributions, and for testing mechanistic hypotheses about how such abilities emerge.", "AI": {"tldr": "The paper introduces BabyReasoningBench, a developmentally inspired reasoning benchmark and shows that baby language models trained only on child-directed input have uneven, generally low reasoning performance, with some gains from scaling mainly in causal/physical tasks.", "motivation": "Most reasoning benchmarks assume adult-level world knowledge, complex instructions, and mature pragmatics, which is inappropriate for evaluating baby language models trained on developmentally realistic child-directed data. The authors want a way to assess what kinds of reasoning, if any, can emerge under such constraints and to connect model evaluation more directly to developmental psychology.", "method": "The authors construct BabyReasoningBench, a suite of 19 reasoning tasks generated using GPT-5.2 but grounded in classic developmental psychology paradigms. The tasks cover theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that minimize confounds from memory and pragmatics. They then evaluate two GPT-2-based baby language models pretrained on 10M and 100M tokens of child-directed speech, comparing their performance patterns across task families and inspecting scaling effects.", "result": "Both baby language models perform overall poorly but not uniformly across tasks. Scaling up training data from 10M to 100M tokens leads to noticeable improvements on some causal and physical reasoning tasks. However, tasks requiring belief attribution, theory of mind, or sensitivity to pragmatic cues remain difficult, showing little improvement with scaling. This reveals dissociations in which reasoning abilities are supported by the child-directed training regime.", "conclusion": "BabyReasoningBench offers a developmentally grounded framework for probing reasoning in language models trained on child-like input. The observed performance patterns suggest that certain causal and physical reasoning abilities can emerge from such data with scale, while more sophisticated social and pragmatic reasoning remains elusive. The benchmark can be used to test mechanistic hypotheses about how different reasoning abilities arise and to more closely align language model evaluation with insights from developmental psychology."}}
{"id": "2601.18833", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18833", "abs": "https://arxiv.org/abs/2601.18833", "authors": ["Marlon Dumas", "Fredrik Milani", "David Chapela-Campa"], "title": "Agentic Business Process Management Systems", "comment": "Presented at the BPM'2025 conference on Artificial Intelligence for Business Process Management (AI4BPM)", "summary": "Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.", "AI": {"tldr": "The paper argues that generative and agentic AI will transform Business Process Management (BPM) from automation-centric to autonomy-centric, enabled by process mining, and proposes an architectural vision for Agentic BPM Systems (A-BPMS).", "motivation": "Traditional BPM waves focused mainly on automating predefined tasks and orchestrating fixed end-to-end processes. With the emergence of generative and agentic AI and the maturity of process mining, there is a need to rethink BPM so that processes can manage and improve themselves more autonomously, using data instead of only design-time models. The authors want to define what this next wave should look like and how BPM platforms must evolve.", "method": "This is a position paper grounded in a keynote talk. The authors conceptually analyze the evolution of BPM, connect it with recent advances in generative and agentic AI and process mining, and then propose a conceptual architecture and design principles for Agentic Business Process Management Systems (A-BPMS). There is no empirical experiment; instead, the paper uses argumentative reasoning and architectural modeling.", "result": "The paper identifies how process mining provides the sensing and analytical backbone that allows AI agents to perceive process states, detect issues, and reason about improvement actions. It defines the notion of A-BPMS, describes its main components (e.g., agents, sensing via logs, reasoning and learning capabilities, integration with existing BPM engines), and shows how these components support varying levels of autonomy across processes. It also maps out a continuum from human-driven to fully autonomous processes, clarifying what functionality is needed at each level.", "conclusion": "The authors conclude that the next wave of BPM technology will be agentic rather than purely automational, with systems that can sense, reason, and act on processes using process mining and advanced AI. They argue that A-BPMS platforms will blur the lines between automation and governance by supporting a spectrum of autonomy levels, and they call for research and development to realize this vision, including methods, architectures, and governance mechanisms for safe deployment of agentic process management."}}
{"id": "2601.18987", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18987", "abs": "https://arxiv.org/abs/2601.18987", "authors": ["Oren Sultan", "Jordi Armengol-Estape", "Pascal Kesseli", "Julien Vanegue", "Dafna Shahaf", "Yossi Adi", "Peter O'Hearn"], "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction", "comment": null, "summary": "Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.", "AI": {"tldr": "The paper empirically evaluates whether large language models can predict termination of real-world C programs, showing they perform competitively with top formal verification tools at classification but struggle to produce formal proofs and scale to longer programs.", "motivation": "Program termination is a fundamental property, but the Halting Problem shows it is undecidable in general, so current automatic tools are approximate, language-specific, and architecture-dependent. With LLMs showing strong reasoning and code understanding capabilities, it is natural to ask if they can serve as termination predictors across diverse programs and possibly complement or rival traditional verification tools.", "method": "The authors construct an evaluation benchmark from the Termination category of the SV-Comp 2025 competition, focusing on diverse C programs. They query various state-of-the-art LLMs, such as GPT-5, Claude Sonnet-4.5, and Code World Model (CWM), asking them to predict whether given programs terminate. They apply test-time scaling techniques (e.g., multiple samples, more computation) and compare accuracy and ranking against SV-Comp termination tools. They also assess whether LLMs can produce valid termination witnesses and analyze performance as a function of program length.", "result": "LLMs achieve high accuracy in predicting program termination on the SV-Comp 2025 C benchmarks. With test-time scaling, GPT-5 and Claude Sonnet-4.5 would rank just behind the best automated termination tool, and CWM would rank just behind the second-best tool. However, LLMs frequently fail to generate valid witnesses that constitute formal proofs of termination, and their accuracy degrades as program size and complexity increase.", "conclusion": "LLMs show strong empirical capabilities in predicting program termination, approaching the performance of leading specialized verification tools on classification-style termination tasks. Yet, they are not yet adequate replacements for formal verification because they rarely provide machine-checkable proofs and do not scale well to larger programs. The study suggests that LLMs are promising components or oracles within broader verification workflows and highlights new directions for research on using LLMs to reason about undecidable properties like termination."}}
{"id": "2601.18846", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.18846", "abs": "https://arxiv.org/abs/2601.18846", "authors": ["Urban Skvorc", "Niki van Stein", "Moritz Seiler", "Britta Grimme", "Thomas B\u00e4ck", "Heike Trautmann"], "title": "LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties", "comment": "17 pages, accepted at EvoApplications 2026", "summary": "Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.", "AI": {"tldr": "The paper proposes using large language models (LLMs) within an evolutionary framework to automatically generate diverse continuous black-box optimisation benchmark functions with specified landscape properties, thereby extending the structural variety beyond existing suites like BBOB.", "motivation": "Current continuous black-box optimisation benchmarks, particularly BBOB, lack sufficient structural diversity in problem landscapes, limiting rigorous evaluation, landscape analysis, and algorithm selection studies. The authors aim to systematically create a broader, more varied, yet interpretable and reproducible set of test functions whose structural properties (e.g., multimodality, separability, basin structure) are explicitly controlled.", "method": "They employ the LLaMEA framework to embed an LLM in an evolutionary loop. High-level, natural-language descriptions of desired landscape properties (multimodality, separability, basin-size and search-space homogeneity, global\u2013local optima contrast) are given as prompts, and the LLM generates candidate problem code. Each generated problem is scored via Exploratory Landscape Analysis (ELA)-based property predictors, and an ELA-space fitness-sharing mechanism is introduced to promote diversity and avoid redundant landscapes. The resulting candidate functions are further examined with basin-of-attraction analysis, statistical testing, and visual inspection. A t-SNE embedding assesses how the new functions relate to existing BBOB instances.", "result": "Many generated benchmark functions demonstrably exhibit the requested structural characteristics, as confirmed by ELA-based metrics, basin-of-attraction analysis, statistical tests, and qualitative inspection. The t-SNE analysis indicates that the new functions populate and extend the existing BBOB instance space instead of forming a disconnected or irrelevant cluster, suggesting they are both novel and structurally related to established benchmarks.", "conclusion": "LLMs guided through an evolutionary, ELA-informed loop can automatically generate a diverse, interpretable, and reproducible library of continuous optimisation benchmarks with targeted landscape properties. This library enriches and extends BBOB-style benchmark spaces and is valuable for landscape analysis and downstream tasks such as automated algorithm selection."}}
{"id": "2601.18998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18998", "abs": "https://arxiv.org/abs/2601.18998", "authors": ["Zahra Hashemi", "Zhiqiang Zhong", "Jun Pang", "Wei Zhao"], "title": "Malicious Repurposing of Open Science Artefacts by Using Large Language Models", "comment": null, "summary": "The rapid evolution of large language models (LLMs) has fuelled enthusiasm about their role in advancing scientific discovery, with studies exploring LLMs that autonomously generate and evaluate novel research ideas. However, little attention has been given to the possibility that such models could be exploited to produce harmful research by repurposing open science artefacts for malicious ends. We fill the gap by introducing an end-to-end pipeline that first bypasses LLM safeguards through persuasion-based jailbreaking, then reinterprets NLP papers to identify and repurpose their artefacts (datasets, methods, and tools) by exploiting their vulnerabilities, and finally assesses the safety of these proposals using our evaluation framework across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. Overall, our findings demonstrate that LLMs can generate harmful proposals by repurposing ethically designed open artefacts; however, we find that LLMs acting as evaluators strongly disagree with one another on evaluation outcomes: GPT-4.1 assigns higher scores (indicating greater potential harms, higher soundness and feasibility of misuse), Gemini-2.5-pro is markedly stricter, and Grok-3 falls between these extremes. This indicates that LLMs cannot yet serve as reliable judges in a malicious evaluation setup, making human evaluation essential for credible dual-use risk assessment.", "AI": {"tldr": "The paper examines how large language models can be jailbreaked and steered to generate technically plausible, harmful misuse proposals by repurposing open NLP research artefacts, and finds that current LLMs are unreliable and inconsistent evaluators of such dual-use risks, reinforcing the need for human assessment.", "motivation": "While there is optimism about using LLMs to autonomously advance science, there is a critical but underexplored risk that the same models could be used to generate harmful research by misusing openly released datasets, methods, and tools. Existing work focuses on beneficial automation and idea generation, not on systematic pipelines for eliciting, structuring, and evaluating malicious proposals derived from open science artefacts. The paper aims to close this gap and to understand both the offensive potential of LLMs in dual-use settings and the reliability of LLMs themselves as risk evaluators.", "method": "The authors design an end-to-end pipeline with three main components. First, they perform persuasion-based jailbreaking to systematically bypass the built-in safety safeguards of large language models. Second, they prompt the jailbroken models to reinterpret existing NLP papers, extracting and repurposing their artefacts\u2014such as datasets, algorithms, and tools\u2014specifically by identifying vulnerabilities and potential avenues for harmful misuse. Third, they introduce an evaluation framework that scores each generated misuse proposal along three dimensions: its potential harmfulness, the feasibility of carrying out the misuse in practice, and the technical soundness of the proposal. They then compare how several frontier LLMs\u2014GPT-4.1, Gemini-2.5-pro, and Grok-3\u2014perform as evaluators under this framework, analyzing disagreements and patterns in their scoring behavior.", "result": "The pipeline successfully induces LLMs to propose harmful applications by repurposing artefacts that were originally designed and released for ethical, benign NLP research. The models can articulate misuse pathways that are technically coherent and tailored to the vulnerabilities of specific datasets, methods, or tools. However, when the authors use LLMs themselves to evaluate these proposals, they observe substantial disagreement across models. GPT-4.1 tends to assign higher scores overall, judging the proposals as more harmful, more technically sound, and more feasible to misuse. Gemini-2.5-pro is substantially stricter or more conservative, giving lower or more guarded scores, while Grok-3\u2019s assessments typically fall between those of GPT-4.1 and Gemini-2.5-pro. This variability shows that evaluations depend heavily on which model is chosen as the judge.", "conclusion": "The work demonstrates that current LLMs, once jailbroken or strategically prompted, can systematically generate harmful misuse proposals by exploiting the dual-use nature of open NLP artefacts, underscoring real security and safety risks in open science. At the same time, the pronounced and systematic disagreements between different LLMs when used as evaluators indicate that they are not yet dependable adjudicators of dual-use or malicious potential. As a result, human oversight and expert evaluation remain indispensable for trustworthy risk assessment, and relying solely on LLM-based evaluators in malicious or dual-use contexts is unsafe and premature."}}
{"id": "2601.18897", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18897", "abs": "https://arxiv.org/abs/2601.18897", "authors": ["Qusai Khaled", "Bahjat Mallak", "Uzay Kaymak", "Laura Genga"], "title": "Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System", "comment": "Submitted to 21st International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU2026)", "summary": "Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.", "AI": {"tldr": "IT2-ANFIS for wastewater plant energy forecasting with interpretable prediction intervals and decomposed uncertainty.", "motivation": "Wastewater treatment plants are major electricity consumers, so accurate and risk-aware energy forecasting is needed for optimization and sustainability. Existing ML models give point predictions without explainable uncertainty, limiting safe operational decision-making.", "method": "Propose an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that outputs prediction intervals. It uses fuzzy rule structures to decompose uncertainty at three levels: feature-level (which inputs cause ambiguity via footprints of uncertainty), rule-level (confidence in local fuzzy rules), and instance-level (final interval around each prediction). Evaluate on Melbourne Water\u2019s Eastern Treatment Plant dataset and compare to first-order ANFIS and black-box probabilistic models.", "result": "On the Eastern Treatment Plant dataset, IT2-ANFIS attains predictive accuracy comparable to first-order ANFIS but with significantly lower variance between training runs. It additionally produces interpretable and structured uncertainty estimates at feature, rule, and instance levels.", "conclusion": "IT2-ANFIS offers a practical, explainable approach for energy forecasting in wastewater treatment, combining strong predictive performance with robust, interpretable uncertainty quantification that directly ties confidence levels to operating conditions and input variables."}}
{"id": "2601.19001", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19001", "abs": "https://arxiv.org/abs/2601.19001", "authors": ["Haozheng Luo", "Zhuolin Jiang", "Md Zahid Hasan", "Yan Chen", "Soumalya Sarkar"], "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning", "comment": null, "summary": "We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST", "AI": {"tldr": "FROST is an attention-aware method that prunes unhelpful reasoning steps to make model reasoning shorter, cheaper, and more accurate.", "motivation": "Long chain-of-thought reasoning often contains redundant or misleading steps (reasoning outliers) that waste tokens and can hurt accuracy. Existing efficiency methods prune by length or heuristics but do not directly exploit attention patterns to identify which steps are actually unimportant.", "method": "Introduce the notion of reasoning outliers\u2014sentences or steps that receive abnormal attention patterns\u2014and design an attention-based mechanism that detects and removes these outliers during reasoning. FROST uses attention weights from the model to decide which reasoning paths and sentences to keep or prune, constructing more focused reasoning trajectories while preserving essential information.", "result": "On four reasoning benchmarks with Phi-4-Reasoning and GPT-OSS-20B, FROST outperforms prior efficient reasoning approaches like TALE and ThinkLess. It cuts token usage by about 69.68% on average and boosts accuracy by 26.70% over the base model. Attention outlier metrics also improve: the maximum infinity norm is reduced by 15.97% and average kurtosis by 91.09% relative to the base model, indicating smoother and less spiky attention distributions.", "conclusion": "Attention patterns can be used as a reliable signal to identify and prune harmful or redundant reasoning steps. FROST shows that attention-aware outlier removal can simultaneously improve efficiency (fewer tokens) and effectiveness (higher accuracy) of reasoning models, while theoretically preserving and even enhancing their reasoning capacity at the sentence level."}}
{"id": "2601.18924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18924", "abs": "https://arxiv.org/abs/2601.18924", "authors": ["Andrew Jaffe", "Noah Reicin", "Jinho D. Choi"], "title": "RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures", "comment": "13 pages, 5 figures, submitted to ACL ARR", "summary": "Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.", "AI": {"tldr": "RIFT is a benchmark that isolates the effect of prompt structure on LLM instruction following by comparing linear vs. non-sequential (jumping) prompts with identical content, revealing large performance drops when positional continuity is broken.", "motivation": "Although LLMs are widely used in complex, multi-step workflows, we lack a clear understanding of how well they can follow instructions when the required reasoning path is non-linear. Existing benchmarks mix task difficulty with structural complexity, so it is unclear whether performance issues come from reasoning limits or from sensitivity to the order and layout of instructions. The authors aim to cleanly separate these factors and quantify how much current models rely on simple positional continuity rather than genuine instruction-following capability.", "method": "The authors build RIFT (Reordered Instruction Following Testbed) using rephrased Jeopardy! question-answer pairs as base content and then design two prompt topologies: (1) linear prompts, where information and instructions are arranged in a straightforward, sequential order, and (2) jumping prompts, where the same information is present but the model must conceptually traverse the prompt in a non-sequential, out-of-order fashion. They evaluate six state-of-the-art open-source LLMs across 10,000 total trials, measure accuracy differences between the two structures, and perform error analysis to categorize failure modes such as instruction-order violations and semantic drift.", "result": "When switching from linear to jumping prompts, model accuracy drops dramatically\u2014by as much as 72% relative to the linear baseline\u2014demonstrating that current LLMs strongly depend on positional continuity in the prompt. The error analysis finds that roughly half of the observed failures can be attributed to breaking the intended order of instructions or drifting semantically away from the prompt\u2019s requirements, rather than a lack of content knowledge per se.", "conclusion": "The study concludes that modern LLMs treat instruction following largely as a sequential pattern-matching process instead of a robust reasoning skill that can handle non-linear control flow. This structural sensitivity is therefore a core limitation of current architectures and poses a significant challenge for real-world applications that inherently involve non-sequential or branching workflows, such as workflow orchestration and multi-agent systems, where models must reliably follow instructions even when they cannot simply read and respond in a straight line."}}
{"id": "2601.19063", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19063", "abs": "https://arxiv.org/abs/2601.19063", "authors": ["Siddhant Arora", "Jinchuan Tian", "Jiatong Shi", "Hayato Futami", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "title": "Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback", "comment": null, "summary": "Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.", "AI": {"tldr": "This paper proposes a multi-reward reinforcement learning from AI feedback framework to improve speech-in/speech-out dialogue systems, jointly optimizing semantic quality, audio naturalness, and emotion consistency, and adapting preference learning to incremental duplex decoding.", "motivation": "Existing RLHF/RLAIF for spoken dialogue systems mainly use a single semantic reward at the utterance level, which fails to capture the multi-dimensional nature of conversational quality (semantics, audio, emotion, turn-taking) and is poorly matched to duplex systems that generate speech incrementally based on partial utterances. There is a need for a holistic alignment approach and a methodology compatible with blockwise, incremental generation.", "method": "The authors introduce a multi-reward RLAIF framework for spoken dialogue systems that combines distinct rewards for semantic coherence, audio quality/naturalness, and emotion consistency. They adapt preference-based learning (using a DPO-style objective) to duplex, blockwise decoding by sampling preferences at the turn level and aggregating per-block log-probabilities into a single objective. They conduct systematic experiments on both multi-turn Chain-of-Thought SDS models and blockwise duplex models, and release a multi-reward DPO dataset.", "result": "Empirical results show that training with a single reward primarily improves the corresponding targeted metric (e.g., semantic or audio quality) but does not generalize well across other aspects. In contrast, joint training with multiple rewards leads to consistent improvements across both semantic quality and audio naturalness metrics, demonstrating effective multi-dimensional alignment.", "conclusion": "Holistic alignment of spoken dialogue systems using multiple, complementary rewards is more effective than single-reward optimization. The proposed multi-reward RLAIF framework, with its turn-level preference sampling and blockwise DPO objective, better matches incremental duplex generation and produces higher-quality conversational behavior across semantic and acoustic dimensions. The released multi-reward DPO dataset facilitates further research in this area."}}
{"id": "2601.18944", "categories": ["cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18944", "abs": "https://arxiv.org/abs/2601.18944", "authors": ["Qiyuan Xu", "Xiaokun Luan", "Renxi Wang", "Joshua Ong Jun Leang", "Peixin Wang", "Haonan Li", "Wenda Li", "Conrad Watt"], "title": "Neural Theorem Proving for Verification Conditions: A Real-World Benchmark", "comment": "Accepted in ICLR'26", "summary": "Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.", "AI": {"tldr": "The paper introduces NTP4VC, the first real-world, multi-language benchmark focusing specifically on automating verification condition (VC) proving using neural theorem proving and LLMs.", "motivation": "Automated theorem provers often fail on hard verification conditions in real-world program verification, forcing costly manual proofs. Existing neural theorem proving work focuses on mathematical domains, and no benchmark directly targets the key bottleneck of automated VC proving in practical verification pipelines.", "method": "They construct NTP4VC, a benchmark of real-world VCs extracted from large software projects such as the Linux and Contiki-OS kernels. Using industrial verification pipelines (Why3 and Frama-C), they generate semantically equivalent VC test cases across multiple proof assistants/formal languages (Isabelle, Lean, Rocq). They then systematically evaluate both general-purpose and theorem-proving\u2013tuned large language models on this benchmark.", "result": "The evaluated LLMs can handle some VCs and show potential for aiding VC proving, but their success rates and robustness are far from sufficient for practical program verification needs, especially on harder conditions drawn from real projects.", "conclusion": "NTP4VC exposes a substantial performance gap for current neural and LLM-based provers on realistic verification conditions, demonstrating that VC proving remains a core unsolved problem in program verification and providing a benchmark to drive and measure future research progress in this area."}}
{"id": "2601.19096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19096", "abs": "https://arxiv.org/abs/2601.19096", "authors": ["Sohhyung Park", "Hyunji Kang", "Sungzoon Cho", "Dongil Kim"], "title": "PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling", "comment": "In Findings of the Association for Computational Linguistics: EACL 2026", "summary": "Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.", "AI": {"tldr": "The paper presents PsyProbe, a proactive mental health dialogue system that systematically models and tracks users\u2019 psychological states to ask better therapeutic questions during counseling exploration.", "motivation": "Existing LLM-based mental health dialogue systems are largely reactive and lack structured, systematic modeling of user psychological states, which is essential for proactive and effective therapeutic exploration in counseling. The authors aim to fill this gap by designing a system that can actively explore users\u2019 core issues rather than just respond passively.", "method": "The authors design PsyProbe, a dialogue system centered on the PPPPPI clinical framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact), enhanced with cognitive error detection. The architecture includes: (1) State Builder to extract structured psychological profiles; (2) Memory Construction to track missing or uncertain information; (3) Strategy Planner that uses Motivational Interviewing (MI) behavioral codes to select exploration strategies; and (4) Response Generator with Question Ideation and Critic/Revision modules to craft proactive, contextually appropriate therapeutic questions. They test PsyProbe with 27 participants in Korean counseling scenarios using automatic, user, and expert evaluations, plus ablation studies.", "result": "In automatic evaluations, the full PsyProbe model surpasses both baseline systems and its own ablated variants. User studies show significantly higher engagement intention and perceived naturalness relative to baseline. Expert (certified counselor) assessments indicate PsyProbe notably improves understanding of clients\u2019 core issues and asks questions at rates comparable to professional counselors.", "conclusion": "Systematic psychological state modeling, grounded in the PPPPPI framework and augmented with cognitive error detection and MI-based strategy planning, enables proactive questioning that improves the exploration phase of counseling. PsyProbe\u2019s performance across automatic, user, and expert evaluations suggests that such structured, proactive LLM-based systems can more effectively facilitate therapeutic exploration than traditional reactive dialogue systems."}}
{"id": "2601.19082", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.19082", "abs": "https://arxiv.org/abs/2601.19082", "authors": ["Trung-Kiet Huynh", "Dao-Sy Duy-Minh", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Nguyen Lam Phu Quy", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Pham Phu Hoa", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "title": "More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas", "comment": "14 pages, 10 figures, 4 tables", "summary": "As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.", "AI": {"tldr": "The paper studies how large language models behave strategically in repeated Prisoner\u2019s Dilemma games, showing that their cooperation and defection patterns depend on payoff size and linguistic framing, and proposes a framework to audit such behaviors for governance and multi-agent system design.", "motivation": "As LLMs start acting as autonomous, interactive agents in social and economic environments, it becomes crucial to understand whether and how they respond to incentives, cooperate, or defect, since this affects safety, coordination, and governance. Existing work lacks a unified lens on their strategic behavior across models, languages, and payoff structures.", "method": "The authors use a payoff-scaled variant of the repeated Prisoner\u2019s Dilemma to probe sensitivity to incentive strength. They run interactions across different LLMs and languages, varying payoff magnitudes and linguistic contexts. Then they train supervised classifiers on canonical repeated-game strategy archetypes (e.g., tit-for-tat, always defect) and apply these classifiers to the LLMs\u2019 action histories to infer their underlying strategic intentions and compare patterns across models and languages.", "result": "They find consistent patterns across models and languages: LLMs exhibit conditional strategies that respond to incentive strength (payoff magnitude), show notable cooperation biases, and display cross-linguistic differences in behavior. Linguistic framing can shape strategies as much as or more than architectural/model differences. The classifier analysis reveals model- and language-specific strategic types rather than random or purely myopic behavior.", "conclusion": "The paper concludes that LLMs behave as structured strategic agents whose cooperation and defection patterns are systematically influenced by payoff incentives and linguistic context. It offers a general auditing framework for characterizing such strategies and emphasizes that cooperation biases and framing effects have direct implications for AI governance, safety monitoring, and the design of multi-agent systems involving LLMs."}}
{"id": "2601.19124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19124", "abs": "https://arxiv.org/abs/2601.19124", "authors": ["Tan Sang Nguyen", "Quoc Nguyen Pham", "Tho Quan"], "title": "Leveraging Sentence-oriented Augmentation and Transformer-Based Architecture for Vietnamese-Bahnaric Translation", "comment": null, "summary": "The Bahnar people, an ethnic minority in Vietnam with a rich ancestral heritage, possess a language of immense cultural and historical significance. The government places a strong emphasis on preserving and promoting the Bahnaric language by making it accessible online and encouraging communication across generations. Recent advancements in artificial intelligence, such as Neural Machine Translation (NMT), have brought about a transformation in translation by improving accuracy and fluency. This, in turn, contributes to the revival of the language through educational efforts, communication, and documentation. Specifically, NMT is pivotal in enhancing accessibility for Bahnaric speakers, making information and content more readily available. Nevertheless, the translation of Vietnamese into Bahnaric faces practical challenges due to resource constraints, especially given the limited resources available for the Bahnaric language. To address this, we employ state-of-the-art techniques in NMT along with two augmentation strategies for domain-specific Vietnamese-Bahnaric translation task. Importantly, both approaches are flexible and can be used with various neural machine translation models. Additionally, they do not require complex data preprocessing steps, the training of additional systems, or the acquisition of extra data beyond the existing training parallel corpora.", "AI": {"tldr": "The paper applies modern neural machine translation with simple data augmentation strategies to improve Vietnamese\u2013Bahnaric translation, supporting preservation and accessibility of the Bahnar language.", "motivation": "The Bahnar language, spoken by an ethnic minority in Vietnam, is culturally and historically important but under-resourced, which limits its use in education, communication, and digital content. There is a governmental push to preserve and promote Bahnaric online, yet practical barriers exist due to the scarcity of linguistic resources. Neural Machine Translation has recently improved translation quality for many language pairs, suggesting it could help revitalize Bahnaric by enabling better access to information and supporting intergenerational communication.", "method": "The authors apply state-of-the-art neural machine translation techniques to a Vietnamese\u2013Bahnaric translation task, focusing on a domain-specific setting. They introduce two data augmentation strategies designed for low-resource conditions. These strategies are model-agnostic, meaning they can be used with different NMT architectures, and they avoid heavy preprocessing, additional system training, or the need for extra parallel data beyond the existing corpus.", "result": "While the abstract does not provide numerical results, it implies that the proposed NMT setup and augmentation approaches improve the practicality and quality of Vietnamese\u2013Bahnaric translation in a low-resource environment, making NMT more feasible for this language pair.", "conclusion": "The study shows that it is possible to enhance Vietnamese\u2013Bahnaric NMT using simple, flexible augmentation techniques that do not demand complex preprocessing or extra data. This contributes to better accessibility of content for Bahnaric speakers and supports broader efforts to preserve and promote the Bahnar language in digital contexts."}}
{"id": "2601.19112", "categories": ["cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19112", "abs": "https://arxiv.org/abs/2601.19112", "authors": ["Nanhan Shen", "Zhilei Liu"], "title": "Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation", "comment": "Accepted by ICASSP 2026", "summary": "Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk", "AI": {"tldr": "UA-3DTalk is a 3D emotional talking-face synthesis framework that improves emotion\u2013audio alignment, control of emotional micro\u2011expressions, and view\u2011dependent rendering quality via emotion prior distillation and uncertainty-aware multi-view fusion.", "motivation": "Existing 3D emotional talking-face methods struggle to accurately extract and align emotional cues from audio with facial motions, especially subtle micro-expressions, and they typically fuse multi-view features using uniform strategies that ignore per-view uncertainty and quality, degrading final rendering quality. A method is needed that both disentangles and exploits richer emotional priors from audio and handles view-specific uncertainty for better controllability and realism.", "method": "The paper proposes UA-3DTalk, which consists of three modules: (1) a Prior Extraction module that disentangles audio into content-synchronized features (for accurate lip-sync and alignment) and person-specific complementary features (for identity-dependent expressiveness); (2) an Emotion Distillation module that uses a multi-modal attention-weighted fusion mechanism together with 4D Gaussian encoding and multi-resolution codebooks to distill fine-grained emotional information from audio and precisely drive emotional micro-expressions; (3) an Uncertainty-based Deformation module that estimates view-specific aleatoric and epistemic uncertainty via uncertainty blocks, and uses these estimates to perform adaptive multi-view feature fusion and a multi-head decoder for Gaussian primitive optimization, overcoming the limitations of uniform-weight fusion.", "result": "On both regular and emotional talking-face datasets, UA-3DTalk surpasses prior state-of-the-art approaches such as DEGSTalk and EDTalk, improving emotion alignment by 5.2% in E-FID, lip synchronization by 3.1% in SyncC, and rendering quality by 0.015 in LPIPS, indicating better emotional fidelity, temporal alignment, and visual realism.", "conclusion": "UA-3DTalk demonstrates that combining emotion prior distillation with uncertainty-aware multi-view fusion leads to more expressive, better-aligned, and higher-quality 3D emotional talking-face synthesis than existing methods, highlighting the importance of disentangled audio priors, fine-grained emotion modeling, and explicit modeling of view-dependent uncertainty in neural rendering systems."}}
{"id": "2601.19191", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19191", "abs": "https://arxiv.org/abs/2601.19191", "authors": ["Olaf Yunus Laitinen Imanov", "Taner Yilmaz", "Ayse Tuba Tugrul", "Melike Nesrin Zaman", "Ozkan Gunalp", "Duygu Erisken", "Sila Burde Dulger", "Rana Irem Turhan", "Izzet Ozdemir", "Derya Umut Kulali", "Ozan Akbulut", "Harun Demircioglu", "Hasan Basri Kara", "Berfin Tavan"], "title": "Transparency-First Medical Language Models: Datasheets, Model Cards, and End-to-End Data Provenance for Clinical NLP", "comment": "12 pages, 9 figures, 15 tables. Technetium-I case study and ProtactiniumBERT-100M reference benchmarks", "summary": "We introduce TeMLM, a set of transparency-first release artifacts for clinical language models. TeMLM unifies provenance, data transparency, modeling transparency, and governance into a single, machine-checkable release bundle. We define an artifact suite (TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance) and a lightweight conformance checklist for repeatable auditing. We instantiate the artifacts on Technetium-I, a large-scale synthetic clinical NLP dataset with 498,000 notes, 7.74M PHI entity annotations across 10 types, and ICD-9-CM diagnosis labels, and report reference results for ProtactiniumBERT (about 100 million parameters) on PHI de-identification (token classification) and top-50 ICD-9 code extraction (multi-label classification). We emphasize that synthetic benchmarks are valuable for tooling and process validation, but models should be validated on real clinical data prior to deployment.", "AI": {"tldr": "TeMLM is a transparency-focused framework and artifact bundle for releasing clinical language models, instantiated and demonstrated on a large synthetic clinical NLP dataset and baseline models.", "motivation": "Clinical language models affect high-stakes decisions but are often released without sufficient documentation of data, modeling choices, and governance, hindering auditing, reproducibility, and safety. The authors are motivated to create a standardized, machine-checkable way to package and audit all transparency-related information for clinical NLP models and datasets.", "method": "They design TeMLM, a unified set of release artifacts (TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance) plus a conformance checklist that captures provenance, data transparency, modeling transparency, and governance. They then instantiate TeMLM for a concrete case: the Technetium-I synthetic clinical dataset and a baseline clinical language model, ProtactiniumBERT. They provide benchmark tasks (PHI de-identification and ICD-9 code extraction) and reference results to demonstrate use of the artifacts and enable repeatable auditing.", "result": "They produce a comprehensive artifact suite for Technetium-I and ProtactiniumBERT, including detailed documentation and a checklist. They also provide benchmark performance results for ProtactiniumBERT on PHI de-identification and multi-label ICD-9 code extraction using the synthetic dataset, establishing reference baselines and demonstrating that the TeMLM artifacts can support transparent evaluation and auditing.", "conclusion": "TeMLM offers a practical, standardized, and machine-checkable way to release clinical language models and datasets with strong transparency guarantees. The case study on Technetium-I and ProtactiniumBERT shows how these artifacts can be instantiated in practice. However, they underscore that synthetic benchmarks should only be used for tooling and process validation; real clinical data are still necessary to validate models before real-world deployment."}}
{"id": "2601.19122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19122", "abs": "https://arxiv.org/abs/2601.19122", "authors": ["Weiran Guo", "Bing Bo", "Shaoxiang Wu", "Jingsheng Yang"], "title": "Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach", "comment": null, "summary": "Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.", "AI": {"tldr": "They propose an RL-based adversarial data augmentation framework that trains a query model to generate challenging function-calling prompts for LLMs, improving robustness and generalization of function call capabilities.", "motivation": "Current methods to improve LLM function calling rely on manually or automatically generated training data with fixed patterns and distributions, which do not sufficiently expose model weaknesses, limiting generalization and robustness.", "method": "Formulate training as a zero-sum game between a query model and a function-call (FC) model. Train the query model via reinforcement learning to generate adversarial queries that especially target weaknesses of the FC model. Use iterative alternating training between the query model and the FC model with these adversarially generated queries as data augmentation.", "result": "The framework systematically discovers hard cases for the FC model and uses them as adversarial data augmentation, leading to more robust and generalized function call performance compared to baselines relying on static data generation/finetuning schemes.", "conclusion": "Adversarial data augmentation driven by an RL-trained query model is an effective and systematic way to reveal and correct weaknesses in LLM function call abilities, advancing the robustness and reliability of tool-using LLMs."}}
{"id": "2601.19202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19202", "abs": "https://arxiv.org/abs/2601.19202", "authors": ["Chi Zhang", "Wenxuan Ding", "Jiale Liu", "Mingrui Wu", "Qingyun Wu", "Ray Mooney"], "title": "Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs", "comment": "24 pages, 10 figures. Accepted at EACL 2026 (main conference)", "summary": "Vision-Language Models (VLMs) have shown strong multimodal reasoning capabilities on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the CONTEXT-VQA (i.e., Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough evaluation framework is designed and executed to benchmark the susceptibility of various models to these conflicting multimodal inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.", "AI": {"tldr": "The paper introduces CONTEXT-VQA, a dataset and evaluation framework to test how easily vision-language models are misled by text that contradicts images, finding they are highly vulnerable to such persuasive misinformation.", "motivation": "Although vision-language models perform well on standard VQA tasks, it is unclear how they behave when textual inputs intentionally contradict visual evidence. Prior work has mainly examined misinformation in text-only settings, leaving a gap in understanding multimodal conflicts and robustness to textual manipulation in VLMs.", "method": "The authors create CONTEXT-VQA, a dataset of image-question pairs augmented with systematically generated persuasive textual prompts that conflict with the visual content. They then design an evaluation framework to test how 11 state-of-the-art VLMs respond to these conflicting multimodal inputs, simulating persuasive conversational settings and measuring performance changes before and after exposure to misleading text.", "result": "Across 11 VLMs, the experiments show that models frequently prioritize misleading textual prompts over clear visual evidence, resulting in substantial performance degradation. On average, there is a performance drop of more than 48.2% after only a single round of persuasive conversation, demonstrating high susceptibility to textual misinformation despite correct visual cues.", "conclusion": "Current vision-language models exhibit a critical vulnerability to textual misinformation that conflicts with images, often disregarding reliable visual evidence. This exposes a major robustness limitation and indicates that future VLM development must incorporate mechanisms to better resist and detect misleading textual manipulation in multimodal contexts."}}
{"id": "2601.19142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19142", "abs": "https://arxiv.org/abs/2601.19142", "authors": ["Zhicheng Zhang", "Zhaocheng Du", "Jieming Zhu", "Jiwei Tang", "Fengyuan Lu", "Wang Jiaheng", "Song-Li Wu", "Qianhui Zhu", "Jingyu Li", "Hai-Tao Zheng", "Zhenhua Dong"], "title": "Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction", "comment": "Accepted at AAAI 2026", "summary": "User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models paradoxically degrades performance for short-sequence users due to attention polarization and length imbalance in training data. To address this, we propose LAIN(Length-Adaptive Interest Network), a plug-and-play framework that explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling. LAIN consists of three lightweight components: a Spectral Length Encoder that maps length into continuous representations, Length-Conditioned Prompting that injects global contextual cues into both long- and short-term behavior branches, and Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length. Extensive experiments on three real-world benchmarks across five strong CTR backbones show that LAIN consistently improves overall performance, achieving up to 1.15% AUC gain and 2.25% log loss reduction. Notably, our method significantly improves accuracy for short-sequence users without sacrificing longsequence effectiveness. Our work offers a general, efficient, and deployable solution to mitigate length-induced bias in sequential recommendation.", "AI": {"tldr": "The paper introduces LAIN, a length-adaptive framework for CTR recommendation that conditions on user behavior sequence length to jointly improve performance for both short- and long-sequence users.", "motivation": "In CTR recommendation, user behavior sequences vary greatly in length. Simply increasing maximum sequence length in existing models should help by adding more context, but in practice it hurts performance for users with short histories due to attention polarization and the training-set imbalance between long and short sequences. There is a need for a general method that handles this length heterogeneity without degrading any subset of users.", "method": "The authors propose LAIN (Length-Adaptive Interest Network), a plug-and-play framework that explicitly uses sequence length as a conditioning signal. It has three components: (1) Spectral Length Encoder, which encodes raw sequence length into a continuous embedding; (2) Length-Conditioned Prompting, which injects these length-aware signals as prompts into both long- and short-term behavior modeling branches, providing global contextual cues; and (3) Length-Modulated Attention, which adapts the sharpness of attention distributions based on sequence length to counteract attention polarization. LAIN is designed to be light-weight and attachable to various existing CTR backbones.", "result": "Across three real-world benchmark datasets and five strong CTR backbone models, LAIN consistently improves performance. The abstract reports up to 1.15% AUC improvement and 2.25% log-loss reduction overall. Importantly, accuracy for short-sequence users is significantly enhanced, and performance for long-sequence users is preserved rather than degraded.", "conclusion": "Incorporating sequence length as an explicit conditioning signal in CTR models effectively mitigates length-induced bias in sequential recommendation. LAIN provides a general, efficient, and easily deployable framework that can be plugged into existing CTR architectures to better balance modeling of short- and long-sequence users, leading to consistent gains without sacrificing any user segment."}}
{"id": "2601.19208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19208", "abs": "https://arxiv.org/abs/2601.19208", "authors": ["Shawn Im", "Changdae Oh", "Zhen Fang", "Sharon Li"], "title": "How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability", "comment": "ICLR 2026", "summary": "Semantic associations such as the link between \"bird\" and \"flew\" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data in attention-based language models through the lens of training dynamics. By leveraging a leading-term approximation of the gradients, we develop closed-form expressions for the weights at early stages of training that explain how semantic associations first take shape. Through our analysis, we reveal that each set of weights of the transformer has closed-form expressions as simple compositions of three basis functions (bigram, token-interchangeability, and context mappings), reflecting the statistics of the text corpus and uncovering how each component of the transformer captures semantic associations based on these compositions. Experiments on real-world LLMs demonstrate that our theoretical weight characterizations closely match the learned weights, and qualitative analyses further show how our theorem shines light on interpreting the learned associations in transformers.", "AI": {"tldr": "The paper theoretically and empirically characterizes how semantic associations (like bird\u2192flew) emerge early in training in attention-based language models, showing that early transformer weights can be written in closed form as simple compositions of three corpus-driven basis functions, and that these formulas match real trained LLMs.", "motivation": "Semantic associations are central to language understanding and generation, but current large language models learn them in ways that are not well-understood mechanistically. To better connect deep learning with linguistic theory and to build a mechanistic understanding of LLMs, the authors want to know how such associations actually arise from training on natural text, particularly in attention-based models, and whether we can write explicit formulas that explain this emergence in terms of corpus statistics.", "method": "The authors study the training dynamics of attention-based language models (transformers) using a leading-term approximation of the gradients early in training. Under this approximation, they derive closed-form expressions for the model\u2019s weight matrices as functions of statistics of the training corpus. They identify three basis functions\u2014bigram statistics, token-interchangeability patterns, and context mappings\u2014and show that each set of transformer weights can be expressed as simple compositions of these basis functions. They then compare these theoretical weight characterizations to the actual weights learned by real-world LLMs and perform qualitative analyses of the learned semantic associations.", "result": "The analysis shows that, at early training stages, each transformer weight matrix admits a closed-form expression built from three basis functions: bigram, token-interchangeability, and context mappings, all reflecting corpus statistics. These theoretical expressions accurately predict the learned weights of real-world attention-based language models, indicating that the early emergence of semantic associations is well-captured by the authors\u2019 gradient-based approximation and basis decomposition. Qualitative case studies further validate that the decomposed components align with intuitive semantic associations observed in the trained models.", "conclusion": "Semantic associations in attention-based language models can be mechanistically explained, at least in early training, by simple closed-form compositions of three corpus-derived basis functions. This provides a principled link between corpus statistics, transformer architecture components, and the emergence of semantic structure, offering a more interpretable, theoretically grounded view of how LLMs begin to capture meaning from text. The findings suggest a path toward more transparent analysis and potentially more controllable or theory-driven design of language models."}}
{"id": "2601.19151", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.19151", "abs": "https://arxiv.org/abs/2601.19151", "authors": ["Patara Trirat", "Jin Myung Kwak", "Jay Heo", "Heejun Lee", "Sung Ju Hwang"], "title": "TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning", "comment": "Code will be available at https://github.com/DeepAuto-AI/TS-Debate", "summary": "Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.", "AI": {"tldr": "The paper introduces TS-Debate, a multi-agent debate framework specialized for time series reasoning that improves zero-shot performance by assigning different modality-focused agents and using reviewer agents for verification and conflict resolution.", "motivation": "Large language models show potential for time series analysis but are fragile: they struggle with accurate numerical reasoning, cross-modal interference between text, plots, and raw numbers, and lack principled integration of different modalities without fine-tuning. The authors want a way to get robust, zero-shot time series reasoning from LLMs while mitigating numeric hallucinations and modality confusion.", "method": "The method is TS-Debate, a collaborative multi-agent framework. Separate expert agents are assigned to different modalities: textual context, visual time series patterns, and numerical signals. Before debating, the system explicitly elicits domain knowledge. The agents then interact via a structured debate protocol, where they share, argue, and refine hypotheses. Additional reviewer agents evaluate claims using a verification\u2013conflict\u2013calibration mechanism that leverages lightweight code execution and numerical lookup to programmatically check quantitative statements. The design ensures each agent maintains fidelity to its modality while a coordination mechanism integrates their evidence.", "result": "On 20 tasks across three public benchmarks for time series reasoning, TS-Debate yields consistent and significant performance gains over strong baselines. In particular, it outperforms standard multimodal debate setups where all agents see all inputs, demonstrating that modality-specialized agents plus explicit verification lead to better zero-shot reasoning.", "conclusion": "Specializing agents by modality and coordinating them via a structured debate with explicit verification and conflict resolution improves LLM-based time series reasoning without task-specific fine-tuning. The architecture reduces numeric hallucinations, preserves the integrity of each modality, and better exposes and resolves conflicting evidence, offering a more reliable zero-shot framework for complex temporal analysis tasks."}}
{"id": "2601.19214", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19214", "abs": "https://arxiv.org/abs/2601.19214", "authors": ["Aakash Trivedi", "Aniket Upadhyay", "Pratik Narang", "Dhruv Kumar", "Praveen Kumar"], "title": "A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews", "comment": "Accepted to EACL 2026 Industry Track (to appear)", "summary": "Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision-recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.", "AI": {"tldr": "Hybrid RoBERTa + LLM system to extract precise, actionable business suggestions from customer reviews, outperforming baselines on accuracy and cluster quality.", "motivation": "Businesses need fine-grained, concrete improvement suggestions from unstructured customer reviews, but current methods either only detect suggestion-bearing sentences or give vague summaries, missing the exact operational directives decision-makers need.", "method": "Build a hybrid pipeline: (1) a high-recall RoBERTa suggestion classifier trained with a precision\u2013recall\u2013oriented surrogate loss to minimize irrecoverable false negatives; (2) a controlled, instruction-tuned LLM that, given candidate sentences, extracts explicit suggestions, assigns categories, clusters similar suggestions, and produces concise summaries. Evaluate against prompt-only LLMs, rule-based systems, and classifier-only baselines on hospitality and food review datasets.", "result": "On real-world hospitality and food datasets, the hybrid pipeline achieves higher suggestion extraction accuracy and more coherent clustering than prompt-only, rule-based, and classifier-only baselines. Human studies indicate that its extracted suggestions and summaries are clearer, more faithful to the source text, and easier to interpret.", "conclusion": "A hybrid reasoning architecture that front-ends an instruction-tuned LLM with a high-recall classifier effectively mines fine-grained, actionable suggestions from customer reviews, outperforming existing approaches. Remaining challenges include adapting to new domains and making such pipelines efficient enough for local or resource-constrained deployment."}}
{"id": "2601.19155", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19155", "abs": "https://arxiv.org/abs/2601.19155", "authors": ["Qiujun Li", "Zijin Xiao", "Xulin Wang", "Zhidan Ma", "Cheng Yang", "Haifeng Li"], "title": "LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge", "comment": "9 pages, 5 figures, 3 tables", "summary": "Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \\textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\\% in zero-shot settings.", "AI": {"tldr": "Proposes LocationAgent, a hierarchical localization agent for image geolocation that separates reasoning from evidence verification using external tools and introduces a new Chinese benchmark, achieving large zero-shot gains.", "motivation": "Image geolocation requires complex geospatial reasoning and verification of visual clues against real-world geographic facts. Existing methods encode knowledge and reasoning patterns in static model parameters via supervised or RL-based training, which makes them vulnerable to hallucinations, poor generalization in open-world scenarios, and difficulty handling dynamic or previously unseen geographic information. There is also a lack of high-quality, non-leaky benchmarks, especially for Chinese cities, limiting robust evaluation.", "method": "Introduce LocationAgent, a Hierarchical Localization Agent for image geolocation. The key idea is to keep hierarchical reasoning logic inside the model but offload evidence verification to external geographic tools. The authors design an RER (Reasoner-Executor-Recorder) architecture that separates roles and compresses context to mitigate reasoning drift over multiple steps: the Reasoner plans and decomposes the task, the Executor calls external clue-exploration tools and processes their outputs, and the Recorder maintains and compresses the reasoning state. They build a tool set for geographic clue exploration that returns diverse evidence (e.g., map, POI, textual/geospatial information) to support location inference. Additionally, they construct CCL-Bench (China City Location Bench), a new benchmark covering multiple scene granularities and difficulty levels, curated to avoid data leakage and to provide sufficient Chinese data for evaluation.", "result": "LocationAgent is empirically evaluated against existing image geolocation approaches. On multiple benchmarks, especially in zero-shot settings where models have not seen similar data during training, LocationAgent achieves at least 30% performance improvement over prior methods. The new CCL-Bench demonstrates that LocationAgent handles diverse scenes and difficulty levels better than baselines, indicating stronger open-world generalization and reduced hallucination in geospatial reasoning.", "conclusion": "Separating hierarchical reasoning from evidence verification and leveraging external geographic tools significantly improves robustness and generalization in image geolocation. The proposed RER architecture effectively maintains coherent multi-step reasoning, while the clue-exploration tool suite supplies up-to-date and diverse geographic evidence. The new CCL-Bench benchmark fills an important gap for Chinese city geolocation evaluation and shows that LocationAgent substantially outperforms existing methods, particularly in zero-shot scenarios. This framework suggests a promising agent-based paradigm for geospatial tasks that require dynamic knowledge and reliable fact verification."}}
{"id": "2601.19221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19221", "abs": "https://arxiv.org/abs/2601.19221", "authors": ["Liu Xiao"], "title": "DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models", "comment": null, "summary": "Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: https://huggingface.co/2dgx41s/DreamState.", "AI": {"tldr": "Introduces DREAMSTATE, a framework that treats RWKV RNN hidden states as an editable knowledge representation by modeling their probability manifold with a conditional Diffusion Transformer, then uses this insight to design a new hybrid RNN+DiT architecture with dynamic, context-aware recurrence.", "motivation": "Modern RNNs like RWKV have strong short-range modeling and use efficient fixed-size states, but almost no work studies these internal states as structured, editable knowledge representations akin to how people study attention or residual streams in Transformers. The authors want to understand what is encoded in RWKV states, whether this structure can be modeled and manipulated directly, and whether such modeling can inspire better architectures that combine RNN efficiency with global context awareness.", "method": "1) Propose DREAMSTATE: train a conditional Diffusion Transformer (DiT) to explicitly learn the probability manifold of RWKV hidden states, conditioned on text, so that it can generate and edit plausible states. Use analyses such as t-SNE visualizations and controlled state generation to probe the structural properties of these learned states. 2) Based on this understanding, design a hybrid architecture where a parallel DiT ingests a variable-length global context and outputs dynamic WKV parameters for the RWKV core, turning the otherwise fixed recurrence into a context-conditioned function. Train the hybrid model with a multi-objective loss that covers both sequence modeling and state-related objectives.", "result": "They show that RWKV states exhibit meaningful structure that can be captured by a conditional DiT: t-SNE plots show clustering/structure, and controlled generation experiments confirm that sampled/edited states correspond to coherent, controllable behaviors. The proposed hybrid RNN+DiT architecture trains stably with a multi-objective loss, and experiments demonstrate that the dynamically generated, context-aware WKV parameters are feasible and effective, indicating that global context can successfully modulate the local recurrent dynamics.", "conclusion": "RWKV hidden states can be fruitfully treated as a structured, editable representation and modeled via diffusion over a learned manifold. The DREAMSTATE framework provides tools for generating and editing these states, revealing their representational richness. Building on this, a new hybrid architecture that uses a DiT to dynamically generate RNN recurrence parameters from global context is both trainable and practically viable, offering a concrete template for future models that seek to combine the efficiency of RNN-style recurrence with the adaptability of Transformer-style global context."}}
{"id": "2601.19170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19170", "abs": "https://arxiv.org/abs/2601.19170", "authors": ["Wangyang Ying", "Yanchi Liu", "Xujiang Zhao", "Wei Cheng", "Zhengzhang Chen", "Wenchao Yu", "Yanjie Fu", "Haifeng Chen"], "title": "Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement", "comment": null, "summary": "Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \\model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \\model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.", "AI": {"tldr": "The paper proposes a multi-agent LLM framework to automatically extract structurally valid and logically consistent procedural graphs (workflows) from natural language texts, achieving better performance than existing baselines.", "motivation": "Workflow/procedural graph extraction from natural language is useful but underexplored. Existing LLM-based methods either generate ill-formed graph structures (e.g., broken connections, invalid topology) or misinterpret the logical flow and semantics of the original text. There is a need for a method that jointly enforces structural validity and logical alignment with the source description, while remaining interpretable and controllable without requiring further supervised training.", "method": "The authors introduce a multi-agent framework, \\model{}, that treats procedural graph extraction as a multi-round reasoning and refinement process. It runs three iterative stages handled by different LLM agents: (1) Graph Builder Agent extracts an initial procedural graph from text; (2) Simulation (structural feedback) Agent simulates and inspects the graph, diagnosing structural defects and explaining them in natural language; (3) Semantic (logical feedback) Agent checks alignment between the graph's flow logic and the linguistic cues in the source text, highlighting semantic inconsistencies. The feedback from the latter two agents is prioritized, phrased in natural language, and fed back into subsequent prompts to refine the graph. The system is modular, with each agent targeting distinct error types, and it operates without extra supervision or parameter updates.", "result": "In experiments, \\model{} significantly improves both structural correctness (e.g., fewer malformed or invalid graphs) and logical consistency (better match to the described procedures) compared with strong LLM-based baselines. Quantitative metrics and possibly qualitative analyses demonstrate these gains, showing the effectiveness of multi-round agent-based refinement for procedural graph extraction.", "conclusion": "The paper concludes that a modular, multi-agent, feedback-driven framework is effective for extracting accurate procedural graphs from natural language. By separating structural and logical refinement and using interpretable natural-language feedback loops, the approach yields more reliable workflow representations without retraining models, suggesting a useful paradigm for other structured prediction tasks with LLMs."}}
{"id": "2601.19225", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19225", "abs": "https://arxiv.org/abs/2601.19225", "authors": ["Kaehyun Um", "KyuHwan Yeom", "Haerim Yang", "Minyoung Choi", "Hyeongjun Yang", "Kyong-Ho Lee"], "title": "RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering", "comment": "Accepted at The Web Conference (WWW) 2026", "summary": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.", "AI": {"tldr": "This paper proposes RPO-RAG, a retrieval-augmented generation framework tailored for small language models using knowledge graphs to improve reasoning and reduce hallucinations.", "motivation": "While large language models show strong reasoning, they hallucinate on knowledge-intensive tasks. KG-based RAG helps but current methods use semantics-unaware path sampling, are weakly aligned with KG reasoning objectives, and directly feed unorganized paths to the LLM, which especially limits small (<7B) models that remain underexplored.", "method": "RPO-RAG introduces: (1) a query-path semantic sampling strategy to select informative KG paths; (2) relation-aware preference optimization to align training with intermediate KG reasoning signals such as relations; and (3) an answer-centered prompt design that structures entities and reasoning paths into interpretable, answer-focused prompts suitable for small LLMs.", "result": "On WebQSP, RPO-RAG improves F1 by up to 8.8%, and on CWQ it achieves state-of-the-art performance among models under 8B parameters in both Hit and F1 metrics, effectively narrowing the gap between small and large LLMs on KGQA tasks.", "conclusion": "RPO-RAG significantly boosts the reasoning performance of small LLMs, including sub-3B models, demonstrating that with appropriate KG-based RAG design they can serve as resource-efficient and practical solutions for on-device knowledge graph question answering."}}
{"id": "2601.19178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19178", "abs": "https://arxiv.org/abs/2601.19178", "authors": ["Jingyu Li", "Zhaocheng Du", "Qianhui Zhu", "kaiyuan Li", "Zhicheng Zhang", "Song-Li Wu", "Chaolang Li", "Pengwen Dai"], "title": "CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation", "comment": "Accepted by ICLR 2026", "summary": "Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.", "AI": {"tldr": "They propose CollectiveKV, a method for compressing key-value (KV) caches in Transformer-based sequential recommendation by sharing most information across users while keeping a small user-specific part, achieving massive compression with no loss in performance.", "motivation": "Sequential recommendation needs low latency, but Transformers are costly for long sequences. KV cache helps speed up inference but creates huge storage overhead when many users have long histories. The authors observe that KV sequences of different users are highly similar, containing collaborative signals, suggesting that much of the cached information can be shared instead of stored per-user.", "method": "They analyze KV caches with SVD and decompose the information into a large shareable component and a small user-specific component. Based on this, they design CollectiveKV: a learnable global KV pool that stores high-dimensional shared KV representations across users. At inference time, each user retrieves shared KV from this global pool and concatenates it with a compact, low-dimensional user-specific KV component to form the final KV used by the Transformer.", "result": "On five sequential recommendation models and three datasets, CollectiveKV can compress the KV cache to about 0.8% of the original storage size while keeping, and sometimes improving, recommendation performance compared to baselines without such compression.", "conclusion": "Most of the information in Transformer KV caches for sequential recommendation is redundant across users and can be effectively captured in a shared global pool. By separating shared and user-specific components, CollectiveKV drastically reduces KV storage with no performance loss, offering a practical solution for deploying Transformer-based recommenders under strict latency and memory constraints."}}
{"id": "2601.19267", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19267", "abs": "https://arxiv.org/abs/2601.19267", "authors": ["Xinlong Chen", "Weihong Lin", "Jingyun Hua", "Linli Yao", "Yue Ding", "Bozhou Li", "Bohan Zeng", "Yang Shi", "Qiang Liu", "Yuanxing Zhang", "Pengfei Wan", "Liang Wang", "Tieniu Tan"], "title": "DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models", "comment": "Project webpage: https://diadem-captioner.github.io/", "summary": "Accurate dialogue description in audiovisual video captioning is crucial for downstream understanding and generation tasks. However, existing models generally struggle to produce faithful dialogue descriptions within audiovisual captions. To mitigate this limitation, we propose DiaDem, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions while maintaining strong overall performance. We first synthesize a high-quality dataset for SFT, then employ a difficulty-partitioned two-stage GRPO strategy to further enhance dialogue descriptions. To enable systematic evaluation of dialogue description capabilities, we introduce DiaDemBench, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.", "AI": {"tldr": "DiaDem is an audiovisual video captioning model focused on accurate dialogue description; it uses a synthetic SFT dataset, a two-stage GRPO training strategy, and introduces a dedicated benchmark (DiaDemBench), outperforming strong commercial baselines especially on dialogue-related metrics while remaining competitive on general benchmarks.", "motivation": "Existing audiovisual captioning models often fail to generate faithful, precise descriptions of spoken dialogue\u2014especially who says what\u2014despite dialogue being critical for downstream understanding and generation tasks. There is also a lack of systematic evaluation tools specifically targeting dialogue description quality in video captions. The paper aims to address both the modeling gap and the evaluation gap for dialogue-aware captioning.", "method": "The authors build DiaDem, an audiovisual captioning model tailored to dialogue. First, they synthesize a high-quality supervised fine-tuning (SFT) dataset that emphasizes accurate dialogue description. Then they train the model with a difficulty-partitioned two-stage GRPO strategy (a form of reinforcement or preference-based optimization) that progressively focuses on harder dialogue cases. To evaluate models, they construct DiaDemBench, a benchmark covering diverse dialogue scenarios and measuring speaker attribution accuracy and utterance transcription fidelity within audiovisual captions.", "result": "On DiaDemBench, DiaDem achieves better dialogue description accuracy than existing systems, including strong commercial models such as the Gemini series, particularly on speaker attribution and transcription fidelity. It also attains competitive performance on standard, general-purpose audiovisual captioning benchmarks, indicating that specializing for dialogue does not sacrifice overall captioning quality.", "conclusion": "DiaDem effectively improves dialogue-aware audiovisual captioning, producing more accurate dialogue descriptions than prior models while preserving strong general captioning capabilities. DiaDemBench provides a systematic way to measure dialogue description performance and reveals that even state-of-the-art commercial models have significant room for improvement on this aspect. The combined modeling and benchmarking contributions advance the state of the art in dialogue-centric video captioning."}}
{"id": "2601.19193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19193", "abs": "https://arxiv.org/abs/2601.19193", "authors": ["Van-Quang Nguyen", "Takayuki Okatani"], "title": "CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning", "comment": "accepted to EACL'26 (main conference)", "summary": "Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.", "AI": {"tldr": "CoReTab is a code-driven reasoning framework and dataset for multimodal table understanding that couples multi-step natural language reasoning with executable Python, yielding verifiable, interpretable supervision and large performance gains over MMTab-style training.", "motivation": "Existing multimodal table understanding datasets like MMTab mostly provide short factual answers without explicit supervision over the multi-step reasoning process. Models trained on them tend to output brief, opaque answers that are not easily interpretable or verifiable, limiting trust and accuracy in complex table tasks. There is a need for a scalable way to generate rich reasoning traces that are both interpretable by humans and automatically checkable by machines.", "method": "The authors propose CoReTab, a code-driven reasoning framework that connects multi-step textual reasoning with executable Python code. They use this framework to automatically construct a large dataset (115K samples) where each example includes long-form reasoning and executable code that can be run to verify the answer. Responses average 529 tokens, reflecting detailed reasoning traces. They then fine-tune open-source multimodal large language models via a three-stage training pipeline using these CoReTab annotations as supervision.", "result": "Models trained on CoReTab are evaluated on 17 MMTab benchmarks that cover table QA, fact verification, and table structure understanding. Compared to baselines trained on MMTab, the CoReTab-trained model achieves performance gains of +6.2% on table QA, +5.7% on fact verification, and +25.6% on table structure understanding, while also producing transparent, executable reasoning traces.", "conclusion": "Coupling multi-step reasoning with executable Python code provides scalable, interpretable, and verifiable supervision for multimodal table understanding. CoReTab serves as a robust, generalizable framework that significantly improves both performance and reasoning transparency across a wide range of table-centric multimodal tasks, demonstrating the value of code-driven reasoning in training MLLMs."}}
{"id": "2601.19273", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.19273", "abs": "https://arxiv.org/abs/2601.19273", "authors": ["Niharika Sri Parasa", "Chaitali Diwan", "Srinath Srinivasa"], "title": "Riddle Quest : The Enigma of Words", "comment": "This paper is submitted under 'Demo track' for WWW conference", "summary": "Riddles are concise linguistic puzzles that describe an object or idea through indirect, figurative, or playful clues. They are a longstanding form of creative expression, requiring the solver to interpret hints, recognize patterns, and draw inferences to identify the answers. In this work, we introduce a simple pipeline for creating and evaluating analogy-based riddles. The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues, and a validator that collects all possible answers the riddle could point to. We use this validator to study whether large language models can recover the full answer set for different riddle types. Our case study shows that while models often guess the main intended answer, they frequently miss other valid interpretations. This highlights the value of riddles as a lightweight tool for examining reasoning coverage and ambiguity handling in language models.", "AI": {"tldr": "The paper presents a pipeline to automatically generate and evaluate analogy-based riddles, then uses them to test how well large language models cover multiple valid answers and handle ambiguity.", "motivation": "To probe the reasoning capabilities and ambiguity handling of large language models using a lightweight, controlled, and creative testbed: analogy-based riddles. Existing evaluations often focus on single correct answers and may overlook whether models can recognize multiple valid interpretations implied by natural language clues.", "method": "They design a modular pipeline: (1) a triples creator that builds structured facts (subject\u2013relation\u2013attribute) about a concept, (2) a semantic mapper that chooses attributes well-suited for analogical clues, (3) a stylized generator that converts these attributes into natural-language riddle clues, and (4) a validator that enumerates all plausible answers consistent with the clues. They then pose these riddles to large language models and compare the models\u2019 predicted answers against the full answer sets produced by the validator, across different riddle types.", "result": "In a case study, large language models typically identify the primary, intended answer to the riddle but often fail to retrieve alternative valid answers that the validator finds. This gap reveals incomplete coverage of the full solution space implied by the clues and shows that model reasoning tends to be narrow and biased toward the most salient or prototypical interpretation.", "conclusion": "Analogy-based riddles, generated and validated via the proposed pipeline, offer a practical way to analyze the breadth of reasoning and ambiguity resolution in large language models. The observed tendency to miss legitimate alternative answers suggests that current models under-represent the full range of meanings supported by natural language, and riddles can serve as an efficient diagnostic tool for improving and evaluating such capabilities."}}
{"id": "2601.19199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19199", "abs": "https://arxiv.org/abs/2601.19199", "authors": ["Libo Sun", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution", "comment": null, "summary": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.", "AI": {"tldr": "MAGNET is a memory-driven adaptive mobile GUI agent that remains robust under UI changes by exploiting stable semantics and task intents.", "motivation": "Mobile GUI agents powered by large foundation models often fail when app interfaces change, because they overfit to historical visual layouts even though the underlying functions and task goals stay the same. There is a need for agents that can generalize across evolving UIs by leveraging more stable structures than raw pixels or specific workflows.", "method": "The authors propose MAGNET, an adaptive agent framework with a dual-level memory system. A stationary memory stores mappings from diverse and changing visual features to stable functional semantics, enabling robust grounding of actions despite UI appearance changes. A procedural memory stores stable task intents and high-level procedures that persist even when workflows are reorganized. They also design a dynamic memory evolution mechanism that continuously updates and refines these memories, prioritizing and strengthening frequently accessed knowledge during online interaction.", "result": "In experiments on the online benchmark AndroidWorld, MAGNET substantially outperforms baseline agents on autonomous task execution. Offline benchmark tests under controlled distribution shifts \u2014 such as altered UIs and workflows \u2014 also show consistent performance gains for MAGNET over baselines, indicating better robustness and adaptability.", "conclusion": "By explicitly modeling and updating stable semantic structures and task intents via dual-level memory, MAGNET can maintain and even improve performance as mobile interfaces evolve. This memory-driven approach enhances generalization and robustness of GUI agents in real-world, frequently changing software environments."}}
{"id": "2601.19278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19278", "abs": "https://arxiv.org/abs/2601.19278", "authors": ["Fuliang Liu", "Xue Li", "Ketai Zhao", "Yinxi Gao", "Ziyan Zhou", "Zhonghui Zhang", "Zhibin Wang", "Wanchun Dou", "Sheng Zhong", "Chen Tian"], "title": "DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference", "comment": null, "summary": "Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the drafting stage itself a performance bottleneck. Inspired by diffusion-based large language models (dLLMs), we propose DART, which leverages parallel generation to reduce drafting latency. DART predicts logits for multiple future masked positions in parallel within a single forward pass based on hidden states of the target model, thereby eliminating autoregressive rollouts in the draft model while preserving a lightweight design. Based on these parallel logit predictions, we further introduce an efficient tree pruning algorithm that constructs high-quality draft token trees with N-gram-enforced semantic continuity. DART substantially reduces draft-stage overhead while preserving high draft accuracy, leading to significantly improved end-to-end decoding speed. Experimental results demonstrate that DART achieves a 2.03x--3.44x wall-clock time speedup across multiple datasets, surpassing EAGLE3 by 30% on average and offering a practical speculative decoding framework. Code is released at https://github.com/fvliang/DART.", "AI": {"tldr": "The paper proposes DART, a speculative decoding framework that speeds up LLM inference by parallel draft generation instead of multi-step autoregressive drafting, achieving 2.03x\u20133.44x speedup over standard decoding and about 30% over EAGLE3.", "motivation": "Speculative decoding can accelerate LLM inference without affecting output quality, but current model-based draft methods like EAGLE3 require multi-step autoregressive rollouts in the draft model. This introduces substantial latency and turns the drafting stage into a bottleneck, limiting real-world speedups. The authors aim to design a draft mechanism that preserves high draft accuracy but with much lower latency, making speculative decoding more practical and efficient.", "method": "The authors design DART, a draft model that, given the target LLM\u2019s hidden states, predicts logits for multiple future token positions in parallel in a single forward pass, instead of autoregressively rolling out token by token. Building on these parallel logit predictions, they develop an efficient tree pruning algorithm that constructs high-quality draft token trees, enforcing N-gram-based semantic continuity to avoid incoherent drafts. The draft model is kept lightweight and tightly coupled to the target model through hidden states, and the speculative decoding pipeline uses these draft trees to propose multiple tokens that the target model can verify in bulk.", "result": "Experiments show that DART greatly reduces the overhead of the drafting phase while maintaining high draft accuracy. Across multiple datasets, DART delivers 2.03x\u20133.44x wall-clock decoding speedups compared to standard greedy or autoregressive decoding with the base LLM. Compared to EAGLE3, a strong prior speculative decoding baseline, DART achieves about 30% additional speedup on average, indicating that parallel drafting and the proposed tree pruning strategy translate into substantial real-world performance gains.", "conclusion": "DART demonstrates that speculative decoding can be significantly accelerated by replacing autoregressive drafting with parallel logit prediction conditioned on the target model\u2019s hidden states. The N-gram-constrained tree pruning algorithm enables construction of accurate draft token trees without heavy rollouts, making the draft stage no longer a bottleneck. The approach provides a practical, high-speed speculative decoding framework that outperforms existing methods like EAGLE3 while remaining lossless in output quality, and the public code release supports further adoption and extension."}}
{"id": "2601.19204", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19204", "abs": "https://arxiv.org/abs/2601.19204", "authors": ["Zhixi Cai", "Fucai Ke", "Kevin Leo", "Sukai Huang", "Maria Garcia de la Banda", "Peter J. Stuckey", "Hamid Rezatofighi"], "title": "MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning", "comment": "ICLR 2026", "summary": "Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.", "AI": {"tldr": "Proposes MATA, a multi-agent hierarchical trainable automaton for visual reasoning that uses a hyper-agent to select among specialized agents, achieving SOTA performance and improved interpretability.", "motivation": "Vision-language models are strong perceptually but their reasoning is opaque and prone to hallucinations, especially on complex, multi-step queries. Existing compositional approaches improve interpretability but typically use fixed, hand-crafted pipelines or single-agent systems that cannot flexibly decide when agents should collaborate or compete. There is a need for a trainable, interpretable control framework that can dynamically orchestrate multiple complementary reasoning agents for visual tasks.", "method": "They design MATA, a multi-agent system structured as a hierarchical finite-state automaton. At the top level, a trainable hyper agent (implemented via an LLM) acts as the transition policy, selecting which agent/state to activate next based on the current shared memory and the query. Each agent corresponds to a state in this automaton and internally runs a small rule-based sub-automaton for low-level, reliable control. All agents operate over a shared memory, recording read/write operations to create a transparent execution trace. To train the hyper agent, they construct transition-trajectory trees from reasoning runs and convert them into memory-to-next-state training pairs, forming the MATA-SFT-90K dataset for supervised finetuning of the LLM-based transition policy.", "result": "The finetuned LLM-based hyper agent learns to understand both the input query and the capabilities of different agents, enabling it to choose an effective sequence of agents for each task. On multiple visual reasoning benchmarks, MATA achieves state-of-the-art performance compared to both monolithic vision-language models and existing compositional reasoning baselines, while providing an interpretable execution history.", "conclusion": "MATA demonstrates that representing a multi-agent visual reasoning system as a hierarchical finite-state automaton, with a trainable hyper agent controlling transitions, can yield both strong performance and improved interpretability. The shared-memory design and rule-based sub-automata provide transparent micro-level control, while supervised finetuning on the MATA-SFT-90K dataset enables effective macro-level orchestration across agents. This approach offers a flexible and scalable framework for multi-agent visual reasoning and outperforms prior monolithic and compositional methods."}}
{"id": "2601.19286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19286", "abs": "https://arxiv.org/abs/2601.19286", "authors": ["Jesus Lovon-Melgarejo", "Jose G. Moreno", "Christine Damase-Michel", "Lynda Tamine"], "title": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction", "comment": "Accepted by WSDM 2026", "summary": "Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.", "AI": {"tldr": "The paper introduces Rewrite-To-Predict (ReToP), an LLM-based framework that jointly trains an EHR rewriter and a clinical predictor to generate task-aware EHR rewrites that improve clinical prediction performance.", "motivation": "Existing LLM-based methods for EHR modeling are mostly task-agnostic: they use LLMs as generic EHR encoders or completion tools without tightly coupling them to downstream prediction objectives. This limits predictive accuracy, especially given EHRs\u2019 high dimensionality, heterogeneity, and sparsity. The authors aim to better exploit LLMs\u2019 parametric medical knowledge by making the representation learning explicitly task-aware and prediction-driven.", "method": "ReToP is an end-to-end framework combining (1) an EHR rewriter powered by an LLM that transforms raw EHRs into rewritten patient narratives, and (2) a clinical predictor trained on these rewrites. To address the absence of labeled EHR rewrites, the authors generate synthetic pseudo-labeled rewrites using clinically driven feature selection strategies, ensuring diversity and relevance. They introduce a Classifier Supervised Contribution (CSC) score that quantifies how much each part of a rewrite helps prediction; this score guides fine-tuning of the rewriter so that it emphasizes features most useful for the prediction tasks.", "result": "On three clinical prediction tasks using the MIMIC-IV dataset, ReToP outperforms strong baseline models that either (a) directly encode EHRs or (b) use LLMs in a task-agnostic fashion. Additional experiments indicate that ReToP generalizes to new datasets and tasks with limited fine-tuning, maintaining clinically faithful rewrites and highlighting task-relevant features.", "conclusion": "Task-aware integration of LLMs with clinical predictors via a rewriting mechanism and CSC-based supervision leads to better use of EHR data and improved clinical prediction performance. ReToP not only boosts accuracy on multiple tasks but also exhibits promising generalization and faithfulness, suggesting that LLM-based EHR rewriting aligned with prediction objectives is a powerful strategy for clinical AI applications."}}
{"id": "2601.19245", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19245", "abs": "https://arxiv.org/abs/2601.19245", "authors": ["Yongxin Deng", "Zhen Fang", "Yixuan Li", "Ling Chen"], "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection", "comment": null, "summary": "Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.", "AI": {"tldr": "The paper proposes SpikeScore, a hallucination detection method that generalizes across domains by exploiting uncertainty spikes in simulated multi-turn dialogues after an LLM\u2019s initial response.", "motivation": "Existing hallucination detectors work well only when training and test data are from the same domain, failing to generalize to new domains, which is problematic for real-world deployment of LLMs. The authors want a hallucination detection framework that can be trained on a single domain yet remain robust across diverse related domains.", "method": "They define the task of generalizable hallucination detection (GHD) and simulate multi-turn dialogues that continue from an LLM\u2019s initial response. By analyzing these dialogues across domains, they discover that hallucinated responses cause larger fluctuations in model uncertainty than factual ones. They formalize this as a new metric, SpikeScore, which measures abrupt uncertainty fluctuations over turns. They provide theoretical analysis showing why SpikeScore separates hallucinated from non-hallucinated responses and build a detection pipeline based on this score.", "result": "SpikeScore exhibits strong cross-domain separability between hallucinated and factual responses in both theory and experiments. Across multiple LLMs and benchmarks, the SpikeScore-based detector outperforms strong baselines and dedicated generalization-oriented methods on cross-domain hallucination detection tasks.", "conclusion": "Uncertainty fluctuation patterns in multi-turn dialogues provide a domain-robust signal for hallucination detection. The proposed SpikeScore leverages this signal to achieve better cross-domain generalization than existing methods, making it a promising approach for deploying LLMs more safely in varied real-world settings."}}
{"id": "2601.19290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19290", "abs": "https://arxiv.org/abs/2601.19290", "authors": ["Yimeng Wang", "Jiaxing Zhao", "Hongbin Xie", "Hexing Ma", "Yuzhen Lei", "Shuangxue Liu", "Xuan Song", "Zichen Zhang", "Haoran Zhang"], "title": "MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning", "comment": null, "summary": "Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.", "AI": {"tldr": "MetaGen is a training-free framework that dynamically generates and adapts agent roles and their interaction topology for multi-agent LLM systems at inference time, improving accuracy\u2013cost tradeoffs on code and reasoning tasks.", "motivation": "Most multi-agent LLM systems use a fixed library of roles (e.g., planner, coder, verifier) and a static interaction graph. This rigidity causes mismatch between roles and specific queries, limits adaptation when new information arises during reasoning, and wastes compute on unnecessary agents, hurting both accuracy and efficiency. There is a need for a more flexible framework that can tailor the role set and collaboration structure to each task instance without retraining the base models.", "method": "MetaGen is a training-free, inference-time orchestration framework. Given a query, it: (1) generates and continually rewrites role specifications, maintaining a dynamic, controllable pool of specialized roles; (2) constructs a minimal backbone execution graph and then instantiates a constrained collaboration topology among roles around this backbone; and (3) during execution, iteratively updates role prompts and adjusts structural decisions (e.g., which agents interact, when to invoke them) using lightweight feedback signals, all without modifying base model weights.", "result": "On code generation and multi-step reasoning benchmarks, MetaGen outperforms strong multi-agent LLM baselines, achieving higher task accuracy for the same or lower inference cost. The experiments demonstrate improved accuracy\u2013cost tradeoffs, indicating more efficient and effective use of multi-agent collaboration compared to fixed-role, static-topology systems.", "conclusion": "Dynamically generating and adapting roles and collaboration topology at inference time\u2014without retraining\u2014can make multi-agent LLM systems both more capable and more efficient. MetaGen shows that a training-free, feedback-informed orchestration layer can better align role specialization and interaction structure with task demands, improving performance and cost over rigid multi-agent designs."}}
{"id": "2601.19249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19249", "abs": "https://arxiv.org/abs/2601.19249", "authors": ["Xingkun Yin", "Hongyang Du"], "title": "GLOVE: Global Verifier for LLM Memory-Environment Realignment", "comment": null, "summary": "Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.", "AI": {"tldr": "Paper proposes GLOVE, a Global Verifier framework for LLM memory that maintains consistency between stored memories and a changing environment via active probing and relative truth, improving success rates under non-stationary conditions.", "motivation": "Existing memory-augmented LLM agents typically assume they can tell whether a memory is valid either via external task-specific supervision (e.g., success/failure signals) or via internal mechanisms like reflection and self-critique. In realistic, dynamically changing environments, these assumptions fail: ground truth supervision is sparse or missing, external feedback may be delayed or noisy, and model self-reflection is unreliable. As environments drift over time, previously correct memories become outdated or inconsistent, degrading agent performance. There is a need for a principled way to detect and correct invalid or obsolete memories without strong supervision or blind trust in model introspection.", "method": "Introduce GLOVE (Global Verifier), a framework that treats truth as relative to the current environment state rather than absolute. GLOVE actively probes the environment to check for inconsistencies between retrieved memories and new observations. When discrepancies are detected, it flags, verifies, and updates memory entries, enabling memory-environment realignment. This process does not rely on explicit ground-truth labels or heavy self-reflection; instead, it leverages comparison between what the memory predicts and what probing actually observes. The method is instantiated in LLM-based agents across several domains, where the verifier orchestrates when and how to probe, and how to modify or replace memory entries based on detected inconsistencies.", "result": "On benchmarks covering web navigation, planning, and control that are augmented with controlled environmental drifts (non-stationarity beyond original problem settings), GLOVE-based agents achieve substantially higher success rates compared to baselines that use conventional memory mechanisms. The experiments demonstrate that GLOVE enables agents to adapt to changes in their environment by maintaining more accurate and up-to-date memories, particularly under non-stationary conditions where other methods degrade.", "conclusion": "GLOVE provides a new design dimension for LLM memory systems by shifting from static or introspection-based validity checks to an active, environment-grounded verification process. By defining a relative notion of truth and continually probing for inconsistencies, the framework supports self-correction of memories without explicit supervision. Empirical gains across non-stationary benchmarks indicate that such global verification is a promising step toward more robust, self-evolving cognitive agents that can maintain reliable long-term behavior in dynamic environments."}}
{"id": "2601.19302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19302", "abs": "https://arxiv.org/abs/2601.19302", "authors": ["Natapong Nitarach", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "title": "Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics", "comment": null, "summary": "Prompting techniques such as Chain-of-Thought (CoT) and Program-of-Thought (PoT) improve LLM mathematical reasoning by structuring intermediate steps in natural language or code. However, applied mathematics problems in domains like finance, physics, and cryptography often require recalling or deriving governing equations, a step that current approaches do not explicitly leverage. We propose Formula-One Prompting (F-1), a two-phase approach that uses mathematical equations as an intermediate representation before adaptive solving. F-1 first formulates governing equations from problem descriptions, then selects a solving strategy among CoT, PoT, or direct computation based on the generated equations, all within a single LLM call. Results across five models and four benchmarks show F-1 outperforms CoT by +5.76% and PoT by +8.42% on average. Crucially, gains are largest in applied domains: +13.30% on FinanceMath over CoT, and within OlympiadBench, larger gains on physics (+2.55%) than pure math (+0.44%). This demonstrates that F-1 is more effective than CoT in applied mathematics problems.", "AI": {"tldr": "The paper introduces Formula-One Prompting (F-1), a two-phase prompting method that first writes down governing equations and then adaptively chooses a solution strategy, yielding better mathematical reasoning performance than Chain-of-Thought and Program-of-Thought, especially in applied domains.", "motivation": "Existing prompting methods like Chain-of-Thought (CoT) and Program-of-Thought (PoT) help LLMs with step-by-step reasoning, but they do not explicitly focus on a critical step in applied mathematics: identifying and formulating the governing equations from problem text. Applied domains such as finance, physics, and cryptography depend heavily on selecting or deriving the right formulas, and current approaches underutilize this structure, leading to suboptimal performance on such tasks.", "method": "The authors propose Formula-One Prompting (F-1), a two-phase prompting framework. In the first phase, the LLM is guided to translate the problem description into explicit mathematical equations that govern the scenario. In the second phase, based on the generated equations, the model adaptively chooses a solution mode: (1) Chain-of-Thought style natural-language reasoning, (2) Program-of-Thought style code-based reasoning, or (3) direct computation. This entire process is executed within a single LLM call. The method is evaluated across five language models and four mathematical reasoning benchmarks.", "result": "Across all tested models and benchmarks, F-1 consistently outperforms existing prompting strategies. On average, it improves over Chain-of-Thought by +5.76% and over Program-of-Thought by +8.42%. The gains are particularly strong in applied settings: on the FinanceMath benchmark, F-1 achieves a +13.30% improvement over CoT; within OlympiadBench, improvements are larger in physics problems (+2.55%) than in pure math problems (+0.44%).", "conclusion": "Formulating explicit governing equations as an intermediate representation and then adaptively selecting a solution strategy makes LLMs more effective at mathematical reasoning, particularly in applied domains. Formula-One Prompting (F-1) outperforms standard CoT and PoT prompting, showing that equation-centric intermediate reasoning is a crucial missing component in current prompting techniques for applied mathematics."}}
{"id": "2601.19306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19306", "abs": "https://arxiv.org/abs/2601.19306", "authors": ["Sijia Li", "Xiaoyu Tan", "Shahir Ali", "Niels Schmidt", "Gengchen Ma", "Xihe Qiu"], "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents", "comment": null, "summary": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.", "AI": {"tldr": "They propose a curiosity-based knowledge retrieval framework for mobile agents on smartphones, using structured \u201cAppCards\u201d to fill knowledge gaps and improve automation success, achieving new SOTA on AndroidWorld.", "motivation": "Current mobile agents for smartphone automation struggle in complex, unseen app environments due to incomplete knowledge of app behaviors and poor generalization. The authors want a way for agents to dynamically acquire and use external, app-specific knowledge during execution instead of relying only on what\u2019s in the backbone model.", "method": "They define an uncertainty-based curiosity score during agent execution. When this score crosses a threshold, the agent automatically retrieves external information (documentation, code repos, past trajectories). This information is normalized into structured AppCards that capture each app\u2019s functional semantics, parameters, interface mappings, and interaction patterns. An enhanced agent architecture is then designed to selectively integrate the relevant AppCards into its step-by-step reasoning and planning, reducing blind spots mid-trajectory.", "result": "On the AndroidWorld benchmark, their framework yields consistent gains across different backbone models, with an average improvement of about 6 percentage points. With GPT-5 as the backbone, they reach a new state-of-the-art success rate of 88.8%. Empirical analyses show that the benefits are strongest for multi-step and cross-application tasks and that the magnitude of improvement depends on which backbone LLM is used.", "conclusion": "Curiosity-triggered retrieval plus structured AppCards can substantially improve robustness and reliability of mobile agents in complex smartphone tasks. AppCards help agents resolve ambiguity, reduce unnecessary exploration, and sustain stable execution trajectories, especially in challenging multi-step and cross-app scenarios. The released task trajectories aim to support further research in this direction."}}
{"id": "2601.19334", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19334", "abs": "https://arxiv.org/abs/2601.19334", "authors": ["Jianzhe Chai", "Yu Zhe", "Jun Sakuma"], "title": "When Benchmarks Leak: Inference-Time Decontamination for LLMs", "comment": null, "summary": "Benchmark-based evaluation is the de facto standard for comparing large language models (LLMs). However, its reliability is increasingly threatened by test set contamination, where test samples or their close variants leak into training data and artificially inflate reported performance. To address this issue, prior work has explored two main lines of mitigation. One line attempts to identify and remove contaminated benchmark items before evaluation, but this inevitably alters the evaluation set itself and becomes unreliable when contamination is moderate or severe. The other line preserves the benchmark and instead suppresses contaminated behavior at evaluation time; however, such interventions often interfere with normal inference and lead to noticeable performance degradation on clean inputs. We propose DeconIEP, a decontamination framework that operates entirely during evaluation by applying small, bounded perturbations in the input embedding space. Guided by a relatively less-contaminated reference model, DeconIEP learns an instance-adaptive perturbation generator that steers the evaluated model away from memorization-driven shortcut pathways. Across multiple open-weight LLMs and benchmarks, extensive empirical results show that DeconIEP achieves strong decontamination effectiveness while incurring only minimal degradation in benign utility.", "AI": {"tldr": "The paper introduces DeconIEP, a framework that reduces benchmark contamination effects in LLM evaluation by applying small embedding-space perturbations during inference, guided by a cleaner reference model, thus lowering contamination-driven performance inflation with minimal loss on clean inputs.", "motivation": "Benchmark-based evaluation of LLMs is unreliable when test data is contaminated\u2014i.e., appears in training\u2014because this inflates performance and misleads model comparison. Existing mitigation either removes contaminated items (changing the benchmark and failing under moderate/severe contamination) or suppresses contamination at inference in ways that hurt normal performance. Hence a new method is needed that works at evaluation time, does not alter benchmarks, and avoids large degradation on uncontaminated inputs.", "method": "They propose DeconIEP, an evaluation-time decontamination framework operating in the input embedding space. A relatively cleaner reference model guides the learning of an instance-adaptive perturbation generator that applies small, bounded perturbations to the evaluated model\u2019s input embeddings. These perturbations are designed to steer the evaluated model away from memorization-based shortcut pathways that arise from prior exposure to benchmark data, while preserving behavior on benign, uncontaminated inputs.", "result": "Across multiple open-weight LLMs and benchmarks, experiments show that DeconIEP effectively reduces contamination-driven performance inflation while only minimally degrading utility on clean inputs. The framework maintains benchmark integrity (no alteration of test sets) and outperforms prior mitigation approaches in the trade-off between decontamination effectiveness and benign performance loss.", "conclusion": "DeconIEP is an effective and practical solution for handling test set contamination in LLM benchmark evaluation. By acting solely during evaluation via small embedding perturbations guided by a less-contaminated reference model, it mitigates memorization effects without modifying benchmarks or substantially harming normal inference, leading to more reliable model comparison under contamination."}}
{"id": "2601.19311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19311", "abs": "https://arxiv.org/abs/2601.19311", "authors": ["Anh Khoa Ngo Ho", "Martin Chauvin", "Simon Gosset", "Philippe Cordier", "Boris Gamazaychikov"], "title": "Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems", "comment": null, "summary": "As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.", "AI": {"tldr": "The paper studies how using smaller language models in multi-agent AI systems affects energy consumption, responsiveness, and output quality, and provides design guidelines for sustainable AI.", "motivation": "As large language models are increasingly used in agentic AI systems, their high energy consumption during inference raises sustainability concerns, motivating an investigation into more energy-efficient alternatives.", "method": "The authors perform a comparative analysis of language models of different sizes in multi-agent, real-world environments, measuring energy usage, responsiveness, and task performance to quantify efficiency\u2013performance trade-offs.", "result": "They find that smaller open-weights language models can significantly reduce energy consumption while maintaining comparable task quality and responsiveness to larger models.", "conclusion": "The paper concludes that sustainable AI systems can be built by carefully selecting model scale and configuring factors like batch size and compute allocation, and it offers practical guidelines for designing scalable, environmentally responsible AI architectures."}}
{"id": "2601.19350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19350", "abs": "https://arxiv.org/abs/2601.19350", "authors": ["Tathagata Raha", "Clement Christophe", "Nada Saadi", "Hamza A Javed", "Marco AF Pimentel", "Ronnie Rajan", "Praveenkumar Kanithi"], "title": "Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity in Text-to-Text Generation", "comment": null, "summary": "Traditional metrics like BLEU and BERTScore fail to capture semantic fidelity in generative text-to-text tasks. We adapt the Cross-Examination Framework (CEF) for a reference-free, multi-dimensional evaluation by treating the source and candidate as independent knowledge bases. CEF generates verifiable questions from each text and performs a cross-examination to derive three interpretable scores: Coverage, Conformity, and Consistency. Validated across translation, summarization and clinical note-generation, our framework identifies critical errors, such as content omissions and factual contradictions, missed by standard metrics. A key contribution is a systematic robustness analysis to select a stable judge model. Crucially, the strong correlation between our reference-free and with-reference modes validates CEF's reliability without gold references. Furthermore, human expert validation demonstrates that CEF mismatching questions align with meaning-altering semantic errors higher than with non-semantic errors, particularly excelling at identifying entity-based and relational distortions.", "AI": {"tldr": "They propose a reference-free evaluation framework (CEF) for generative text-to-text models that uses cross-questioning between source and candidate texts to measure semantic fidelity via Coverage, Conformity, and Consistency, outperforming traditional metrics like BLEU and BERTScore.", "motivation": "Existing automatic metrics such as BLEU and BERTScore often miss important semantic issues in generative text-to-text tasks, like omissions or factual contradictions. There is a need for an evaluation method that better captures semantic fidelity, can work without gold-standard references, and provides interpretable signals about different error types.", "method": "They adapt the Cross-Examination Framework (CEF) by treating the source text and the generated candidate as independent knowledge bases. The system uses an LLM to generate verifiable questions from each text and then cross-examines: questions from the source are answered using the candidate, and vice versa. By comparing answers, they compute three scores: Coverage (how much source information appears in the candidate), Conformity (how well the candidate adheres to the source without adding unsupported content), and Consistency (internal agreement between texts). They perform a robustness analysis to choose a stable judge model and support both reference-free and with-reference evaluation modes.", "result": "Across tasks like machine translation, summarization, and clinical note generation, CEF detects critical semantic errors such as omissions and factual contradictions that standard metrics often miss. Their robustness analysis yields a stable choice of judge model. They find a strong correlation between the reference-free and with-reference modes, indicating that CEF is reliable even without gold references. Human expert studies show that questions flagged as mismatching by CEF correspond more frequently to meaning-altering semantic errors\u2014especially entity-level and relational distortions\u2014than to non-semantic deviations.", "conclusion": "The adapted CEF offers a more semantically faithful, interpretable, and robust evaluation for text-to-text generation than traditional surface- or embedding-based metrics. It can operate effectively without reference texts, provides multidimensional signals (Coverage, Conformity, Consistency), and is validated both quantitatively and through human expert assessments, making it well-suited for high-stakes domains like clinical text generation."}}
{"id": "2601.19337", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19337", "abs": "https://arxiv.org/abs/2601.19337", "authors": ["Sayak Chowdhury", "Meenakshi D'Souza"], "title": "SETA: Statistical Fault Attribution for Compound AI Systems", "comment": "Accepted to CAIN 2026 co-hosted with ICSE 2026", "summary": "Modern AI systems increasingly comprise multiple interconnected neural networks to tackle complex inference tasks. Testing such systems for robustness and safety entails significant challenges. Current state-of-the-art robustness testing techniques, whether black-box or white-box, have been proposed and implemented for single-network models and do not scale well to multi-network pipelines. We propose a modular robustness testing framework that applies a given set of perturbations to test data. Our testing framework supports (1) a component-wise system analysis to isolate errors and (2) reasoning about error propagation across the neural network modules. The testing framework is architecture and modality agnostic and can be applied across domains. We apply the framework to a real-world autonomous rail inspection system composed of multiple deep networks and successfully demonstrate how our approach enables fine-grained robustness analysis beyond conventional end-to-end metrics.", "AI": {"tldr": "The paper introduces a modular, architecture-agnostic robustness testing framework for complex AI systems composed of multiple neural network modules, enabling fine-grained, component-wise robustness analysis and error propagation reasoning, and validates it on a real-world autonomous rail inspection system.", "motivation": "Existing robustness testing methods are designed for single neural network models and do not scale or generalize well to modern AI systems that are composed of multiple interconnected networks. This limitation makes it difficult to understand where and why such systems fail, and how errors propagate between components, which is critical for safety- and reliability-sensitive applications like autonomous inspection or control.", "method": "The authors design a modular robustness testing framework that takes test data and systematically applies a predefined set of perturbations. The framework treats each neural network in a multi-network pipeline as a separate module, enabling (1) component-wise analysis by observing how each individual module behaves under perturbations, and (2) analysis of error propagation by tracking how perturbations and resulting errors move through the pipeline. The framework is intentionally designed to be architecture- and modality-agnostic so it can be used across domains and different types of models.", "result": "Using an autonomous rail inspection system made up of multiple deep neural networks as a case study, the authors show that their framework can expose detailed robustness characteristics of each component and of the pipeline as a whole. It enables the identification of vulnerabilities and performance degradation patterns that are not visible when only using conventional end-to-end metrics for evaluation.", "conclusion": "The paper concludes that modular, component-level robustness testing is both feasible and valuable for complex, multi-network AI systems. Their framework provides richer diagnostic insights than standard end-to-end robustness evaluations, supports analysis of error propagation, and is broadly applicable across different architectures and data modalities, thereby improving the ability to assess and enhance the safety and reliability of modern AI pipelines."}}
{"id": "2601.19360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19360", "abs": "https://arxiv.org/abs/2601.19360", "authors": ["Diego Rossini", "Lonneke van der Plas"], "title": "Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement", "comment": "Accepted at Findings of EACL 2026", "summary": "We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165x fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. We confirm the generalization of our method on the STREUSLE dataset, achieving 78.9% F1. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.", "AI": {"tldr": "This paper proposes an efficient method for identifying multiword expressions (MWEs) using a compact DeBERTa-based token classifier, achieving substantially better F1 scores than a very large LLM while using far fewer parameters.", "motivation": "Large language models have recently been applied to structured NLP tasks like MWE identification, but they are computationally expensive and do not always yield strong performance, especially on tasks needing fine-grained token-level decisions and handling of discontinuous expressions. There is a need for methods that both improve accuracy on benchmark MWE datasets and remain lightweight enough for resource-constrained environments.", "method": "The authors treat MWE detection as binary token-level classification with START/END/INSIDE labels instead of span prediction. They fine-tune a DeBERTa-v3-large model and integrate explicit linguistic features such as noun phrase chunking and dependency information to better capture discontinuous and noun-type MWEs. To combat class imbalance in the training data, they apply oversampling data augmentation for underrepresented positive MWE instances.", "result": "On the CoAM dataset, their approach achieves 69.8% F1, outperforming the previous best system based on Qwen-72B (57.8% F1) by 12 F1 points while using 165 times fewer parameters. On the STREUSLE dataset, the method reaches 78.9% F1, indicating that the approach generalizes beyond a single benchmark.", "conclusion": "The study shows that a carefully engineered smaller model enriched with linguistic features and class-imbalance mitigation can substantially outperform much larger LLMs on structured NLP tasks like MWE identification. This suggests that for many practical deployments, especially those with limited computational resources, targeted architectures may be preferable to generic large-scale LLMs."}}
{"id": "2601.19402", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19402", "abs": "https://arxiv.org/abs/2601.19402", "authors": ["Amit Singh Bhatti", "Vishal Vaddina", "Dagnachew Birru"], "title": "PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems", "comment": null, "summary": "Production LLM deployments serve diverse workloads where cost and quality requirements vary by customer tier, time of day, and query criticality. Model serving systems accept latency SLOs directly. LLM routers do not. They force operators to tune parameters offline and guess what accuracy might result. The relationship between parameters and outcomes is indirect, non-monotonic, and dataset-dependent. Operators need to specify accuracy targets, not infer them from opaque settings. We present PROTEUS (Polymorphic Router for Operational Target Enforcement with Unified SLA), a router that accepts accuracy targets tau as runtime input. PROTEUS uses Lagrangian dual control. A learned dual variable lambda tracks constraint violations during training and conditions the policy network. This lets the router translate specified tau values into routing decisions that satisfy them. A single trained model serves the full accuracy spectrum without retraining.We evaluate on RouterBench (11 models, 405K queries) and SPROUT (14 models, 45K queries). PROTEUS achieves consistent floor compliance where accuracy meets or exceeds tau. The target-response correlation reaches 0.97 to 0.98. The closest baseline, OmniRouter, meets floors only 22% of the time despite also using Lagrangian optimization. PROTEUS operates across tau in [0.85, 0.95] from a single model. On RouterBench it achieves 90.1% accuracy, within 1.3% of oracle. On SPROUT it achieves 94.0% accuracy, within 4.6% of oracle. Cost savings reach 89.8% versus the best fixed model.", "AI": {"tldr": "The paper presents PROTEUS, an LLM routing system that takes accuracy targets as runtime inputs and meets them reliably while reducing costs.", "motivation": "Existing LLM routers require operators to tune low-level parameters offline and only indirectly control accuracy/latency tradeoffs. The mapping from routing parameters to achieved accuracy is opaque, non-monotonic, and dataset-dependent, which makes it hard for production operators who really want to specify clear accuracy targets or SLAs. There is a need for a router that can directly enforce accuracy floors as first-class objectives at runtime, across diverse workloads and customer tiers, while also controlling cost.", "method": "The authors design PROTEUS, a polymorphic router that accepts an explicit accuracy target \u03c4 as a runtime input. They formulate routing with accuracy floors as a constrained optimization problem and solve it using Lagrangian dual control. During training, a learned dual variable \u03bb tracks violations of the accuracy constraint and conditions the routing policy network, effectively teaching the router how to adjust routing decisions to satisfy given \u03c4 values. A single trained router model is used to serve queries across a range of \u03c4 values without retraining, mapping target accuracies to routing choices among multiple LLMs with different cost\u2013quality profiles.", "result": "On the RouterBench benchmark (11 models, 405K queries) and SPROUT (14 models, 45K queries), PROTEUS consistently satisfies accuracy floors, with target\u2013response correlations of 0.97\u20130.98. It operates across \u03c4 in [0.85, 0.95] using one model. On RouterBench it reaches 90.1% accuracy, within 1.3% of an oracle router, and on SPROUT it reaches 94.0% accuracy, within 4.6% of oracle. Compared to always using the best single fixed model, PROTEUS achieves up to 89.8% cost savings while still meeting accuracy targets, and significantly outperforms OmniRouter, the closest baseline, which only meets floors 22% of the time.", "conclusion": "PROTEUS provides a practical, runtime-controllable LLM routing solution that directly enforces accuracy targets as SLAs. By using Lagrangian dual control with a learned dual variable to condition the routing policy, it can reliably meet specified accuracy floors across different datasets and target ranges with a single trained model. This approach yields accuracy close to oracle routing and large cost reductions compared with fixed-model deployment and existing routers that lack robust floor compliance."}}
{"id": "2601.19410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19410", "abs": "https://arxiv.org/abs/2601.19410", "authors": ["Ahrii Kim", "Seong-heum Kim"], "title": "Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?", "comment": null, "summary": "Automatic post-editing (APE) aims to refine machine translations by correcting residual errors. Although recent large language models (LLMs) demonstrate strong translation capabilities, their effectiveness for APE--especially under document-level context--remains insufficiently understood. We present a systematic comparison of proprietary and open-weight LLMs under a naive document-level prompting setup, analyzing APE quality, contextual behavior, robustness, and efficiency.\n  Our results show that proprietary LLMs achieve near human-level APE quality even with simple one-shot prompting, regardless of whether document context is provided. While these models exhibit higher robustness to data poisoning attacks than open-weight counterparts, this robustness also reveals a limitation: they largely fail to exploit document-level context for contextual error correction. Furthermore, standard automatic metrics do not reliably reflect these qualitative improvements, highlighting the continued necessity of human evaluation. Despite their strong performance, the substantial cost and latency overheads of proprietary LLMs render them impractical for real-world APE deployment. Overall, our findings elucidate both the promise and current limitations of LLM-based document-aware APE, and point toward the need for more efficient long-context modeling approaches for translation refinement.", "AI": {"tldr": "The paper evaluates how well large language models perform automatic post-editing of machine translation at the document level, finding proprietary LLMs reach near human-level quality but barely use document context, are more robust than open models, and are currently too costly and slow for practical deployment.", "motivation": "While LLMs are strong translators, it is unclear how effective they are for automatic post-editing, especially when given whole-document context. Existing work lacks a systematic, fair comparison between proprietary and open-weight LLMs for document-aware APE, and we do not yet understand their robustness, contextual use, and efficiency trade-offs.", "method": "The authors conduct a systematic experimental comparison of proprietary and open-weight LLMs used as APE systems under a simple, naive document-level prompting setup. They evaluate APE quality, examine how models use (or fail to use) document-level context, and assess robustness to data poisoning attacks as well as computational cost and latency. They compare automatic metrics with human evaluation to understand metric reliability.", "result": "Proprietary LLMs achieve near human-level APE quality even with one-shot prompts and regardless of document-level context availability. They are more robust to data poisoning than open-weight models but generally fail to leverage document context for resolving contextual translation errors. Automatic evaluation metrics do not consistently capture qualitative improvements seen by humans. Proprietary models also incur high inference cost and latency.", "conclusion": "LLMs, particularly proprietary ones, show strong potential as APE systems, but they underutilize document-level context, automatic metrics are insufficient to assess their real gains, and their current cost and latency make them impractical for deployment at scale. Future work should focus on more efficient, long-context modeling techniques and better evaluation protocols to exploit document-level information for translation refinement."}}
{"id": "2601.19404", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19404", "abs": "https://arxiv.org/abs/2601.19404", "authors": ["Hongzhu Yi", "Xinming Wang", "Zhenghao zhang", "Tianyu Zong", "Yuanxiang Wang", "Jun Xie", "Tao Yu", "Haopeng Jin", "Zhepeng Wang", "Kaixin Xu", "Feng Chen", "Jiahuan Chen", "Yujia Yang", "Zhenyu Guan", "Bingkang Shi", "Jungang Xu"], "title": "RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization", "comment": null, "summary": "Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.", "AI": {"tldr": "RPO is a reinforcement fine-tuning algorithm for LLMs that optimizes only partial reasoning suffixes instead of full trajectories, drastically cutting rollout tokens and training time while preserving performance.", "motivation": "Full-trajectory reinforcement fine-tuning for reasoning tasks is very compute-expensive because the model must generate entire reasoning paths during rollouts. The authors want to understand which parts of the reasoning path truly affect final answer correctness, and then exploit this structure to reduce rollout cost without sacrificing performance.", "method": "They empirically analyze how different segments of reasoning paths influence final accuracy, then design RPO, a plug-and-play reinforcement fine-tuning algorithm that trains on suffixes of reasoning traces. RPO uses an experience cache of partial trajectories; during rollouts, it conditions on cached prefixes and only generates reasoning suffixes, reducing rollout tokens by about 95%. RPO is compatible with existing RL fine-tuning methods like GRPO and DAPO, serving as an optimization wrapper that changes how trajectories are generated rather than the underlying objective.", "result": "In experiments on 1.5B and 7B models, RPO reduces training-time rollout token generation by roughly 95%. End-to-end, it cuts total training time by 90% for a 1.5B model and 72% for a 7B model, while maintaining task performance comparable to standard full-path RL fine-tuning baselines. It accelerates GRPO and DAPO without degrading their effectiveness.", "conclusion": "Partial reasoning optimization via suffix-only rollouts is an effective way to reduce the computational cost of reinforcement fine-tuning for reasoning LLMs. By leveraging an experience cache and focusing training on the most influential parts of the reasoning trajectory, RPO dramatically speeds up training while preserving performance, and can be used as a general plug-in for existing RL fine-tuning algorithms."}}
{"id": "2601.19447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19447", "abs": "https://arxiv.org/abs/2601.19447", "authors": ["V\u00edtor N. Louren\u00e7o", "Aline Paes", "Tillman Weyde", "Audrey Depeige", "Mohnish Dubey"], "title": "KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking", "comment": "Accepted to publication at the 19th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2026", "summary": "Claim verification is a core component of automated fact-checking systems, aimed at determining the truthfulness of a statement by assessing it against reliable evidence sources such as documents or knowledge bases. This work presents KG-CRAFT, a method that improves automatic claim verification by leveraging large language models (LLMs) augmented with contrastive questions grounded in a knowledge graph. KG-CRAFT first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary that is used for veracity assessment by LLMs. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.", "AI": {"tldr": "KG-CRAFT is a claim verification method that builds a knowledge graph from claims and reports, generates contrastive questions from the graph, distills evidence into summaries, and uses LLMs to assess veracity, achieving state-of-the-art results on two datasets.", "motivation": "Automated fact-checking is crucial for assessing the truthfulness of claims, but existing systems struggle to effectively leverage complex structured evidence and guide LLMs to reason over it. There is a need for methods that improve LLM-based claim verification by organizing evidence and prompting more robust, contrastive reasoning to distinguish subtle differences in veracity.", "method": "KG-CRAFT first constructs a knowledge graph using claims and their associated reports as nodes and relations. Based on the structure of this graph, it generates contextually relevant contrastive questions that highlight differences and relationships between pieces of evidence or aspects of claims. These questions are then used to guide LLMs in distilling the underlying reports into focused, evidence-based summaries. The resulting concise summaries, encapsulating key evidence, are then fed to LLMs to perform the final veracity classification of each claim.", "result": "On two real-world claim verification datasets, LIAR-RAW and RAWFC, KG-CRAFT achieves new state-of-the-art predictive performance compared to previous automated fact-checking methods. The evaluations show improved accuracy in determining claim veracity, indicating that the knowledge graph-guided contrastive questioning and summarization pipeline helps LLMs make better fact-checking decisions.", "conclusion": "The study concludes that combining knowledge graph construction with contrastive question generation significantly enhances LLMs' fact-checking abilities. By structuring evidence and explicitly prompting contrastive reasoning, KG-CRAFT enables more accurate and explainable claim verification, as validated through extensive experiments and analyses on real-world datasets."}}
{"id": "2601.19527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19527", "abs": "https://arxiv.org/abs/2601.19527", "authors": ["Temirbolat Maratuly", "Pakizar Shamoi", "Timur Samigulin"], "title": "Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach", "comment": null, "summary": "Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.", "AI": {"tldr": "The paper develops and tests a fuzzy expert control system, supported by a digital twin, to automatically regulate key process parameters in sour water treatment in refineries, improving safety, reliability, and ease of use.", "motivation": "Sour water generated during crude oil processing contains acidic and toxic components like H2S and CO2, which, if untreated, cause serious environmental harm and severe corrosion of pipelines and equipment. Conventional operation requires expert human oversight and exposes workers to hazards. There is a need for an automated, operator-friendly control solution that keeps critical process variables within safe limits while reducing human exposure and enabling water reuse and cost reductions.", "method": "The authors build a fuzzy expert controller that mimics human operator reasoning and connect it to a high-fidelity digital twin of an industrial sour water treatment unit. The digital twin is implemented in Honeywell UniSim Design R492, with valve dynamics identified and modeled in MATLAB via system identification. Real-time data exchange between the simulator and the fuzzy controller is handled using OPC DA. The fuzzy controller uses split-range control across two valves and is evaluated under 21 initial pressure conditions and 5 defuzzification methods (105 scenarios total). A web-based interface for simulation and interaction is implemented in Python using Streamlit. Performance is assessed via standard error metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response measures (overshoot, undershoot, rise/fall time, settling time, steady-state error).", "result": "The fuzzy expert controller successfully maintains key process parameters (e.g., pressure) within desired ranges across a wide set of initial conditions and defuzzification strategies. Quantitative performance metrics and dynamic response indicators demonstrate acceptable tracking quality and stability for all 105 tested scenarios, with sensitivity to the choice of defuzzification method. The integrated digital twin plus web interface allows realistic, real-time testing and operator interaction without impacting the real plant.", "conclusion": "The study shows that a fuzzy expert control strategy, supported by an accurate digital twin and simple web-based interface, can effectively automate sour water treatment operations while remaining intuitive for junior or non-expert operators. The approach enhances safety, reduces operator workload, and offers general-purpose applicability to other industrial processes beyond sour water treatment, provided a suitable digital twin is available."}}
{"id": "2601.19532", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19532", "abs": "https://arxiv.org/abs/2601.19532", "authors": ["Marthe Ballon", "Andres Algaba", "Brecht Verbeken", "Vincent Ginis"], "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge", "comment": "17 pages, 10 figures, 3 tables", "summary": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.", "AI": {"tldr": "Omni-MATH-2 is a cleaned and annotated version of the Omni-MATH benchmark that exposes major issues in both dataset quality and automatic judging for LLM math evaluation.", "motivation": "Benchmarking LLMs on math is currently unreliable because datasets contain errors and ambiguities, and automatic judges often mis-evaluate solutions. The authors want a benchmark that accurately reflects model capabilities and to quantify how much \"noise\" comes from flawed data and judges.", "method": "They manually audit the full Omni-MATH dataset for LaTeX compilability, completeness of information, solvability, and verifiability. They then split it into a clean exact-answer subset and a smaller tagged subset of non-standard problems (proofs, estimations, image-dependent questions). They annotate problems with tags and remove clutter. Using this revised dataset, they compare evaluations produced by GPT-5 mini acting as a judge with those from the original Omni-Judge, and have human experts adjudicate disagreements.", "result": "They built Omni-MATH-2: a clean exact-answer subset of 4181 problems and a tagged non-standard subset of 247 problems. The cleaning significantly reduces dataset-induced noise. Comparing judges shows substantial disagreement on both subsets; in disagreements, human experts find Omni-Judge is incorrect 96.4% of the time, implying it is a very unreliable evaluator, especially for harder problems. They also show that as problem difficulty increases, more capable judges are needed to avoid judge errors overwhelming true performance differences, and that both automatic judges fail to capture key failure modes on tagged problems.", "conclusion": "High-quality math benchmarks for LLMs require both carefully cleaned/annotated datasets and highly capable judges. Omni-MATH-2 improves dataset quality and exposes that commonly used automatic judges like Omni-Judge are largely unreliable, particularly on challenging problems and non-standard formats. Without addressing both dataset noise and judge noise, benchmark scores can severely misrepresent model abilities and obscure real progress."}}
{"id": "2601.19490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19490", "abs": "https://arxiv.org/abs/2601.19490", "authors": ["Ricardo Campos", "Raquel Sequeira", "Sara Nerea", "In\u00eas Cantante", "Diogo Folques", "Lu\u00eds Filipe Cunha", "Jo\u00e3o Canavilhas", "Ant\u00f3nio Branco", "Al\u00edpio Jorge", "S\u00e9rgio Nunes", "Nuno Guimar\u00e3es", "Purifica\u00e7\u00e3o Silvano"], "title": "ClaimPT: A Portuguese Dataset of Annotated Claims in News Articles", "comment": null, "summary": "Fact-checking remains a demanding and time-consuming task, still largely dependent on manual verification and unable to match the rapid spread of misinformation online. This is particularly important because debunking false information typically takes longer to reach consumers than the misinformation itself; accelerating corrections through automation can therefore help counter it more effectively. Although many organizations perform manual fact-checking, this approach is difficult to scale given the growing volume of digital content. These limitations have motivated interest in automating fact-checking, where identifying claims is a crucial first step. However, progress has been uneven across languages, with English dominating due to abundant annotated data. Portuguese, like other languages, still lacks accessible, licensed datasets, limiting research, NLP developments and applications. In this paper, we introduce ClaimPT, a dataset of European Portuguese news articles annotated for factual claims, comprising 1,308 articles and 6,875 individual annotations. Unlike most existing resources based on social media or parliamentary transcripts, ClaimPT focuses on journalistic content, collected through a partnership with LUSA, the Portuguese News Agency. To ensure annotation quality, two trained annotators labeled each article, with a curator validating all annotations according to a newly proposed scheme. We also provide baseline models for claim detection, establishing initial benchmarks and enabling future NLP and IR applications. By releasing ClaimPT, we aim to advance research on low-resource fact-checking and enhance understanding of misinformation in news media.", "AI": {"tldr": "The paper presents ClaimPT, a new European Portuguese dataset of news articles annotated for factual claims, along with baseline models for automatic claim detection to support fact-checking research in a low-resource language.", "motivation": "Manual fact-checking is slow, labor-intensive, and cannot keep up with the rapid spread of online misinformation. Automation requires reliable claim detection, but progress is uneven across languages because most annotated resources are in English. Portuguese lacks accessible, properly licensed datasets, particularly for journalistic content, which constrains research and practical NLP applications for fact-checking.", "method": "The authors partnered with LUSA, the Portuguese News Agency, to collect European Portuguese news articles. They designed a new annotation scheme for factual claims and had two trained annotators independently label each article. A curator then validated all annotations to ensure quality. The final dataset, ClaimPT, contains 1,308 news articles and 6,875 individual claim annotations. The authors also trained and evaluated baseline models for claim detection on this dataset to create initial benchmarks.", "result": "The outcome is ClaimPT, a curated dataset of European Portuguese news articles with high-quality factual claim annotations, totaling 1,308 articles and 6,875 annotations. The authors report baseline performance numbers for claim detection models on this dataset, establishing reference benchmarks for future work, though exact metrics are not provided in the abstract.", "conclusion": "ClaimPT fills an important resource gap for automated fact-checking in Portuguese, particularly in the domain of professional news. By making this annotated dataset and baseline results available, the paper aims to catalyze research in low-resource fact-checking, support the development of NLP and IR tools for claim detection, and improve understanding and handling of misinformation in news media."}}
{"id": "2601.19568", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19568", "abs": "https://arxiv.org/abs/2601.19568", "authors": ["Ke Xu", "Siyang Xiao", "Ming Liang", "Yichen Yu", "Zhixiang Wang", "Jingxuan Xu", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "Learning Adaptive Parallel Execution for Efficient Code Localization", "comment": "13 pages, 4 figures", "summary": "Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\\% redundant invocation rate, which negates parallelism benefits. We propose \\textbf{FuseSearch}, reformulating parallel code localization as a \\textbf{joint quality-efficiency optimization} task. Through defining \\textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\\% file-level and 56.4\\% function-level $F_1$ scores) with 93.6\\% speedup, utilizing 67.7\\% fewer turns and 68.9\\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.", "AI": {"tldr": "FuseSearch is a training and inference framework that makes parallel code search both faster and more accurate by explicitly optimizing for tool efficiency (unique info gain per tool call) rather than just breadth of search.", "motivation": "Automated code localization (finding relevant files/functions for a task or bug) is a major bottleneck in software agents because tool calls for search and retrieval are slow and expensive. Existing agents try to parallelize tool calls but end up with high redundancy (34.9% redundant invocations), which cancels out much of the speed benefit and injects noisy, duplicative context. There is a need for agents that can adaptively decide how many tools to call in parallel and which ones, trading off exploration and efficiency based on the task state.", "method": "The paper introduces FuseSearch, which formalizes parallel code localization as a joint quality-efficiency optimization problem. It defines a metric called tool efficiency (unique information gain per invocation). The authors train an agent in two stages: first with supervised fine-tuning (SFT) and then with reinforcement learning (RL), so that it learns policies for adaptive parallel tool usage. Instead of a fixed number of parallel calls, the agent dynamically adjusts search breadth depending on the current context, starting with broader exploration and moving to narrower refinement as it converges on relevant code. The policy is optimized to maximize both localization performance and efficiency (fewer redundant, low-value tool calls).", "result": "On the SWE-bench Verified benchmark, FuseSearch with a 4B model achieves state-of-the-art-level code localization: 84.7% file-level F1 and 56.4% function-level F1. At the same time, it delivers a 93.6% speedup over baselines while using 67.7% fewer dialogue turns and 68.9% fewer tokens. The tool redundancy rate is substantially reduced, demonstrating that the model has learned to avoid unnecessary parallel queries while still improving, not hurting, task quality.", "conclusion": "Efficiency-aware training for code localization agents not only reduces cost and latency but also improves localization accuracy. By explicitly modeling tool efficiency and learning adaptive parallel search strategies via SFT+RL, FuseSearch turns parallelism from a naive breadth-increasing mechanism into an intelligent exploration\u2013refinement process. This shows that careful optimization of tool usage policies can yield high-performance, cost-effective localization agents, and suggests a broader design pattern for multi-tool LLM systems."}}
{"id": "2601.19503", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19503", "abs": "https://arxiv.org/abs/2601.19503", "authors": ["Wei Huang", "Anda Cheng", "Yinggui Wang"], "title": "GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs", "comment": "Accepted by ICLR2026", "summary": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.", "AI": {"tldr": "GradPruner is a gradient-guided, early-stage pruning method for LLM fine-tuning that removes and merges less important layers to improve both training and inference efficiency, cutting 40% of parameters with only ~1% accuracy loss.", "motivation": "Fine-tuning large language models on downstream tasks is computationally expensive in both training and inference. Existing structured pruning methods typically target inference efficiency only and incur extra overhead (e.g., knowledge distillation, search procedures), making them ill-suited for efficient downstream fine-tuning. There is a need for a method that can jointly accelerate fine-tuning and inference with minimal additional cost and minimal performance degradation.", "method": "The paper proposes GradPruner, a gradient-guided pruning framework applied in the early phase of fine-tuning. During the initial training steps, it accumulates gradients for each parameter and constructs an Initial Gradient Information Accumulation Matrix (IGIA-Matrix) that quantifies layer importance. Based on this matrix, it selects less important layers for pruning. The parameters of pruned layers are sparsified and then merged into neighboring remaining layers. The merging is constrained to parameters with matching signs to avoid destructive interference from sign differences. After pruning and merging, fine-tuning continues on the reduced model for the downstream task.", "result": "Across two different LLM backbones and eight downstream datasets spanning medical, financial, and general benchmarks, GradPruner reduces the parameter count by about 40% while incurring only a 0.99% average drop in accuracy, indicating effective compression with limited performance loss.", "conclusion": "Gradient-based early-stage layer pruning with IGIA-guided selection and sign-aware parameter merging can significantly reduce LLM size while maintaining accuracy, thereby improving both training and inference efficiency in downstream fine-tuning. GradPruner offers a practical approach for resource-efficient adaptation of LLMs, and the released code supports reproducibility and further research."}}
{"id": "2601.19607", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19607", "abs": "https://arxiv.org/abs/2601.19607", "authors": ["Haoyun Li", "Ming Xiao", "Kezhi Wang", "Robert Schober", "Dong In Kim", "Yong Liang Guan"], "title": "ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks", "comment": null, "summary": "Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.", "AI": {"tldr": "The paper proposes ComAgent, a multi-LLM agent framework that autonomously converts high-level 6G network design intents into solver-ready optimization formulations and simulations, outperforming single LLM setups.", "motivation": "Designing 6G networks needs complex cross-layer optimization, but turning high-level human intents into precise mathematical and simulation models is difficult, slow, and requires deep expertise. Existing monolithic LLM approaches lack domain grounding, constraint handling, and reliable verification, limiting their usefulness for real engineering workflows.", "method": "The authors design ComAgent, a multi-agent framework based on several specialized LLMs orchestrated in a closed-loop Perception-Planning-Action-Reflection cycle. Different agents handle literature search, problem decomposition, coding, constraint-aware optimization formulation, scoring, and self-correction. The system iteratively refines the problem formulation and simulation setup until it produces solver-ready code and reproducible experiments that align with the user\u2019s high-level intent.", "result": "In evaluations on complex wireless design tasks, including beamforming optimization, ComAgent reaches performance comparable to human experts and consistently outperforms single, monolithic LLM baselines on a variety of wireless networking problems. It shows improved reliability in generating correct optimization formulations and executable simulations.", "conclusion": "ComAgent effectively bridges the gap between user intent and executable optimization and simulation workflows for 6G design. Its multi-agent, closed-loop architecture offers better domain grounding, constraint awareness, and verification than monolithic LLMs, indicating strong potential to automate and accelerate design processes in emerging wireless networks."}}
{"id": "2601.19507", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19507", "abs": "https://arxiv.org/abs/2601.19507", "authors": ["Xiangyang Zhu", "Yuan Tian", "Zicheng Zhang", "Qi Jia", "Chunyi Li", "Renrui Zhang", "Heng Li", "Zongrui Wang", "Wei Sun"], "title": "Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs", "comment": null, "summary": "Large vision-language models (LVLMs) exhibit remarkable capabilities in cross-modal tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process, static complexity, and limited discriminative power. Thus, they may fail to keep pace with rapidly evolving models and emerging risks. To address these limitations, we propose VLSafetyBencher, the first automated system for LVLM safety benchmarking. VLSafetyBencher introduces four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection agents to construct and select high-quality samples. Experiments validates that VLSafetyBencher can construct high-quality safety benchmarks within one week at a minimal cost. The generated benchmark effectively distinguish safety, with a safety rate disparity of 70% between the most and least safe models.", "AI": {"tldr": "An automated system, VLSafetyBencher, is proposed to build safety benchmarks for large vision-language models (LVLMs) efficiently and adaptively.", "motivation": "Existing LVLM safety benchmarks are hard to build, static, and quickly become outdated, limiting their ability to detect vulnerabilities and emerging risks in rapidly evolving models.", "method": "They design VLSafetyBencher, an automated pipeline composed of four collaborating agents\u2014Data Preprocessing, Generation, Augmentation, and Selection\u2014that together generate and filter diverse, high-quality safety evaluation samples.", "result": "Experiments show that VLSafetyBencher can create high-quality LVLM safety benchmarks in under a week with low cost, and that these benchmarks reveal large differences in safety performance between models (up to a 70% gap between the safest and least safe models).", "conclusion": "Automating safety benchmark construction for LVLMs is feasible and effective; VLSafetyBencher produces strong, discriminative benchmarks that can keep up with quickly evolving models and expose substantial safety disparities among them."}}
{"id": "2601.19622", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.19622", "abs": "https://arxiv.org/abs/2601.19622", "authors": ["Thomas B\u00f6mer", "Nico Koltermann", "Max Disselnmeyer", "Bastian Amberg", "Anne Meyer"], "title": "Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search", "comment": "accepted at EvoStar conference; Code: https://github.com/tb-git-tud/a-ceoh-evolution-of-heuristics?tab=readme-ov-file", "summary": "Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.", "AI": {"tldr": "The paper proposes A-CEoH, a method that uses large language models with code-augmented prompts to automatically generate heuristics for A* search, achieving performance competitive with or better than expert-crafted heuristics on logistics and puzzle domains.", "motivation": "Designing effective heuristic functions for A* is hard, expert-intensive, and problem-specific. With advances in LLMs and evolutionary search, there is an opportunity to automate heuristic design and reduce reliance on manual, handcrafted heuristics while potentially discovering better ones.", "method": "The authors extend the Evolution of Heuristics (EoH) framework by adding a domain-agnostic prompt augmentation that embeds the A* algorithm code into the LLM prompt, enabling stronger in-context learning. This Algorithmic-Contextual EoH (A-CEoH) uses an evolutionary loop: LLMs generate and refine heuristic functions, which are evaluated inside A* on target problems (UPMP and SPP), and the best-performing heuristics are iteratively evolved.", "result": "On two domains\u2014the Unit-Load Pre-Marshalling Problem and the Sliding Puzzle Problem\u2014the heuristics generated by A-CEoH lead to significantly better A* performance than baseline LLM-generated heuristics without algorithmic context, and in several cases they surpass established expert-designed heuristics in search efficiency or solution quality.", "conclusion": "Incorporating algorithm code into LLM prompts within an evolutionary heuristic-generation framework substantially improves the automated design of heuristics for A*, demonstrating that A-CEoH is a powerful, domain-agnostic approach that can rival or exceed expert-crafted heuristics in diverse search problems."}}
{"id": "2601.19578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19578", "abs": "https://arxiv.org/abs/2601.19578", "authors": ["Yuxuan Cai", "Xinyi Lai", "Peng Yuan", "Weiting Liu", "Huajian Li", "Mingda Li", "Xinghua Wang", "Shengxie Zheng", "Yanchao Hao", "Yuyang Yin", "Zheng Wei"], "title": "Yunque DeepResearch Technical Report", "comment": null, "summary": "Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.", "AI": {"tldr": "The paper presents Yunque DeepResearch, a hierarchical multi-agent framework that improves robustness, scalability, and performance for LLM-based deep research tasks.", "motivation": "Existing deep research agents suffer from contextual noise in long tasks, cascading errors due to fragility, and poor modular extensibility, limiting their effectiveness on complex open-ended problems.", "method": "They design a hierarchical architecture with three main components: (1) a Multi-Agent Orchestration System that decomposes tasks and routes subtasks to a pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that turns completed sub-goals into semantic summaries to control context length and reduce noise; and (3) a proactive Supervisor Module that detects anomalies, prunes harmful or irrelevant context, and generally enforces robustness during execution.", "result": "Yunque DeepResearch achieves state-of-the-art results on several agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam, indicating superior performance over prior systems.", "conclusion": "A hierarchical, modular, and supervised multi-agent design with dynamic context management can significantly enhance the robustness and effectiveness of LLM-based deep research agents, and the open-sourced framework enables broader community adoption and extension."}}
{"id": "2601.19752", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19752", "abs": "https://arxiv.org/abs/2601.19752", "authors": ["Minh-Dung Dao", "Quy Minh Le", "Hoang Thanh Lam", "Duc-Trong Le", "Quoc-Viet Pham", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "Agentic Design Patterns: A System-Theoretic Framework", "comment": null, "summary": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.", "AI": {"tldr": "The paper proposes a system-theoretic framework and 12 reusable design patterns to build more robust, modular AI agents, addressing issues like hallucination and brittle reasoning.", "motivation": "Agentic AI systems built on foundation models suffer from hallucinations, weak reasoning, and ad-hoc, brittle architectures. Existing \u201cdesign pattern\u201d work is often informal and lacks a rigorous systems-theoretic basis, making it hard to systematically design, compare, and implement agentic systems. The authors aim to provide a principled, shared methodology and language for engineering robust AI agents.", "method": "The authors develop a system-theoretic decomposition of agentic AI into five interacting functional subsystems: (1) Reasoning & World Model, (2) Perception & Grounding, (3) Action Execution, (4) Learning & Adaptation, and (5) Inter-Agent Communication. From this decomposition and a mapped taxonomy of agentic challenges, they systematically derive 12 agentic design patterns. These patterns are grouped into four categories\u2014Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning\u2014and formalised as reusable architectural solutions. They then apply the framework in a case study of the ReAct framework to show how the patterns expose and repair architectural weaknesses.", "result": "The outcome is: (1) a system-theoretic reference architecture for agentic AI systems, (2) a structured taxonomy of challenges tied to this architecture, and (3) a catalog of 12 concrete agentic design patterns aligned with the taxonomy and subsystems. In the ReAct case study, applying these patterns reveals systemic deficiencies and suggests architectural improvements, illustrating the practical utility of the framework for analysing and redesigning real-world agent systems.", "conclusion": "By grounding agentic AI design in systems theory and codifying recurrent solutions as design patterns, the paper offers a standardised conceptual and practical toolkit for building agents. This should improve modularity, interpretability, and reliability of autonomous systems and facilitate clearer communication and reuse among researchers and engineers when specifying, critiquing, and evolving agent architectures."}}
{"id": "2601.19605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19605", "abs": "https://arxiv.org/abs/2601.19605", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "Andr\u00e9 Freitas"], "title": "Decompose-and-Formalise: Recursively Verifiable Natural Language Inference", "comment": null, "summary": "Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $\u03b8$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.", "AI": {"tldr": "They propose a neuro-symbolic NLI framework that decomposes explanations into verifiable entailment trees and performs local, prover-guided refinement with improved logical formalisation, substantially boosting explanation verification rates and efficiency across tasks and LLMs.", "motivation": "Existing LLM+theorem prover pipelines for natural language inference struggle on realistic, complex inputs because errors in autoformalising text into logic are amplified in long multi-step arguments. A single mismatch can break an entire proof, and current systems typically respond with expensive global regeneration since they cannot easily pinpoint where the reasoning failed from prover feedback. The authors aim to build a more robust, scalable, and faithful refinement process that can handle naturalistic NLI while maintaining accuracy and efficiency.", "method": "They introduce a decompose-and-formalise pipeline. First, each premise\u2013hypothesis pair is decomposed into an entailment tree whose nodes are atomic reasoning steps. Second, the system verifies this tree in a bottom-up manner using a theorem prover, which localises failures to specific nodes or steps. Third, instead of regenerating the whole explanation, the system performs local, diagnostic-guided refinement only where verification fails. Additionally, they design an event-based logical representation enhanced with \u03b8-substitution, enforcing consistent argument-role bindings across the formalisation to increase faithfulness of the autoformalised proofs. They evaluate this across several reasoning tasks and with five different LLM backbones.", "result": "Across multiple reasoning tasks and five different LLMs as backbones, their method attains the highest explanation verification rates among compared approaches. It yields improvements over the prior state of the art by 26.2%, 21.7%, 21.6%, and 48.9% (for different settings or datasets), while also reducing the number of refinement iterations and overall runtime. At the same time, it preserves strong NLI classification accuracy, indicating that the gains in verification and efficiency do not come at the cost of predictive performance.", "conclusion": "Structured decomposition into entailment trees, coupled with bottom-up theorem-prover verification and local, diagnostic-guided refinement, makes neuro-symbolic NLI more robust and scalable to naturalistic inputs. The added \u03b8-substitution mechanism in an event-based logical form improves the faithfulness of mapping text to logic by enforcing consistent argument-role bindings. Collectively, these innovations significantly enhance explanation verification and efficiency without sacrificing NLI accuracy, advancing the practicality of LLM+TP-based reasoning systems."}}
{"id": "2601.19768", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19768", "abs": "https://arxiv.org/abs/2601.19768", "authors": ["Shir Rozenfeld", "Rahul Pankajakshan", "Itay Zloczower", "Eyal Lenga", "Gilad Gressel", "Yisroel Mirsky"], "title": "GAVEL: Towards rule-based safety through activation monitoring", "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.", "AI": {"tldr": "The paper proposes a rule-based, interpretable activation-safety framework (GAVEL) for LLMs that uses composable cognitive elements to detect and govern harmful behaviors with higher precision and customizability.", "motivation": "Existing activation-based safety systems for LLMs are trained on broad misuse datasets and suffer from poor precision, limited ability to adapt to specific domains, and low interpretability, making it hard for practitioners to understand, control, and audit model behavior. The authors want a more precise, transparent, and flexible way to govern LLM behaviors at the activation level.", "method": "They conceptualize model activations as cognitive elements (CEs): fine-grained, semantically interpretable units like \"making a threat\" or \"payment processing.\" Over these CEs, they define logical predicate rules that can be composed to represent nuanced, domain-specific safety policies. The framework detects rule violations from activations in real time, without retraining the base model. They package this into an open-source system, GAVEL, along with an automated tool to help generate rules.", "result": "Empirical evaluations show that their compositional, rule-based activation safety system improves precision over baseline activation-safety methods, better supports domain-specific customization, and provides interpretable signals suitable for inspection and auditing. The system can be updated and refined by modifying rules rather than retraining detectors.", "conclusion": "Modeling activations with interpretable cognitive elements and governing them via explicit, composable rules yields a more precise, customizable, and auditable activation-safety mechanism for LLMs. This rule-based approach offers a scalable path toward transparent AI governance, and the authors support its adoption by releasing the GAVEL framework and an associated rule-creation tool as open source."}}
{"id": "2601.19613", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19613", "abs": "https://arxiv.org/abs/2601.19613", "authors": ["Xinzhong Wang", "Ya Guo", "Jing Li", "Huan Chen", "Yi Tu", "Yijie Hong", "Gongshen Liu", "Huijia Zhu"], "title": "Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs", "comment": "Accepted by ICASSP 2026", "summary": "Key Information Extraction (KIE) from visually-rich documents (VrDs) is a critical task, for which recent Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have demonstrated strong potential. However, their reliance on autoregressive inference, which generates outputs sequentially, creates a significant efficiency bottleneck, especially as KIE tasks often involve extracting multiple, semantically independent fields. To overcome this limitation, we introduce PIP: a Parallel Inference Paradigm for KIE. Our approach reformulates the problem by using \"[mask]\" tokens as placeholders for all target values, enabling their simultaneous generation in a single forward pass. To facilitate this paradigm, we develop a tailored mask pre-training strategy and construct large-scale supervised datasets. Experimental results show that our PIP-models achieve a 5-36x inference speedup with negligible performance degradation compared to traditional autoregressive base models. By substantially improving efficiency while maintaining high accuracy, PIP paves the way for scalable and practical real-world KIE solutions.", "AI": {"tldr": "The paper proposes PIP, a parallel inference paradigm for key information extraction from visually-rich documents, replacing slow autoregressive generation with simultaneous prediction of all target fields via [mask] tokens, achieving large speedups with minimal accuracy loss.", "motivation": "Key Information Extraction from visually-rich documents often uses large language and multimodal models that generate outputs autoregressively. This sequential decoding is inefficient when many fields are independent, creating an inference-time bottleneck that limits real-world scalability. The authors aim to remove this bottleneck and make KIE faster and more practical while preserving accuracy.", "method": "They redesign KIE as a masked prediction problem: all target fields in a document are represented by [mask] tokens, and the model predicts all masked values in one forward pass instead of token-by-token generation. To support this, they introduce a specialized mask-based pre-training scheme and build large-scale supervised datasets aligned with the new formulation. The approach is instantiated on LLM/MLLM backbones to create PIP-models.", "result": "On benchmark KIE tasks, PIP-based models obtain 5\u201336x faster inference compared with their autoregressive counterparts, while exhibiting only negligible degradation in extraction performance. This shows that near-original accuracy can be maintained despite the large reduction in decoding cost.", "conclusion": "Parallel masked prediction is an effective alternative to autoregressive decoding for KIE on visually-rich documents. By leveraging [mask] placeholders and dedicated pre-training plus supervision, PIP substantially boosts inference efficiency with little loss in quality, offering a scalable and practical solution for real-world KIE deployments."}}
{"id": "2601.19793", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19793", "abs": "https://arxiv.org/abs/2601.19793", "authors": ["Shanyv Liu", "Xuyang Yuan", "Tao Chen", "Zijun Zhan", "Zhu Han", "Danyang Zheng", "Weishan Zhang", "Shaohua Cao"], "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing", "comment": null, "summary": "Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.", "AI": {"tldr": "Proposes CASTER, a dynamic router to allocate different-strength models in graph-based multi-agent systems, cutting cost while preserving performance.", "motivation": "Static, uniform allocation of strong models in graph-based MAS is computationally wasteful, especially on trivial subtasks in complex cyclic workflows. There is a need for a principled, dynamic routing mechanism that chooses appropriate model strength per node/task to balance cost and success rate.", "method": "Introduces CASTER, a lightweight, context-aware router with a Dual-Signal Router that fuses semantic embeddings and structural meta-features to estimate task difficulty and select models. Training follows a Cold Start to Iterative Evolution paradigm, where the router improves using on-policy negative feedback from its routing errors, i.e., learning from its own failures over time.", "result": "Across domains including Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity, CASTER achieves up to 72.4% reduction in inference cost relative to strong-model-only baselines, while maintaining comparable success rates, and it consistently outperforms heuristic routing and FrugalGPT-based approaches.", "conclusion": "Dynamic, context-aware routing via CASTER effectively exploits task difficulty variations in graph-based MAS, substantially reducing cost without sacrificing performance, and provides a general, superior alternative to heuristic or prior frugality-focused methods across multiple domains."}}
{"id": "2601.19637", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19637", "abs": "https://arxiv.org/abs/2601.19637", "authors": ["Weicong Liu", "Zixuan Yang", "Yibo Zhao", "Xiang Li"], "title": "RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems", "comment": "18 pages", "summary": "Reviewer assignment is increasingly critical yet challenging in the LLM era, where rapid topic shifts render many pre-2023 benchmarks outdated and where proxy signals poorly reflect true reviewer familiarity. We address this evaluation bottleneck by introducing LR-bench, a high-fidelity, up-to-date benchmark curated from 2024-2025 AI/NLP manuscripts with five-level self-assessed familiarity ratings collected via a large-scale email survey, yielding 1055 expert-annotated paper-reviewer-score annotations. We further propose RATE, a reviewer-centric ranking framework that distills each reviewer's recent publications into compact keyword-based profiles and fine-tunes an embedding model with weak preference supervision constructed from heuristic retrieval signals, enabling matching each manuscript against a reviewer profile directly. Across LR-bench and the CMU gold-standard dataset, our approach consistently achieves state-of-the-art performance, outperforming strong embedding baselines by a clear margin. We release LR-bench at https://huggingface.co/datasets/Gnociew/LR-bench, and a GitHub repository at https://github.com/Gnociew/RATE-Reviewer-Assign.", "AI": {"tldr": "The paper introduces LR-bench, a new up-to-date benchmark with expert self-assessed familiarity ratings for reviewer assignment, and proposes RATE, a reviewer-centric ranking framework using keyword-based reviewer profiles and fine-tuned embeddings that achieves state-of-the-art performance.", "motivation": "Existing reviewer assignment benchmarks are outdated in the fast-evolving LLM/AI landscape, and current proxy signals (like coarse expertise indicators) do not accurately capture true reviewer familiarity, making it hard to evaluate and improve assignment algorithms.", "method": "The authors build LR-bench from 2024\u20132025 AI/NLP manuscripts, collecting five-level self-assessed familiarity scores from experts via a large-scale email survey to obtain 1055 expert annotations. They then design RATE, which creates compact keyword-based profiles from each reviewer's recent publications and fine-tunes an embedding model using weak preference supervision derived from heuristic retrieval signals, allowing direct manuscript-to-reviewer-profile matching and ranking.", "result": "On LR-bench and the CMU gold-standard reviewer assignment dataset, the proposed RATE framework achieves state-of-the-art performance, clearly outperforming strong embedding-based baselines on reviewer-paper matching quality metrics.", "conclusion": "LR-bench provides a high-fidelity, up-to-date benchmark for reviewer assignment evaluation, and the RATE framework offers a more accurate, reviewer-centric approach to matching that generalizes well and sets a new performance bar on both new and existing datasets; the resources are publicly released for the community."}}
{"id": "2601.19824", "categories": ["cs.AI", "cs.HC", "cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.19824", "abs": "https://arxiv.org/abs/2601.19824", "authors": ["Andre Paulino de Lima", "Paula Castro", "Suzana Carvalho Vaz de Andrade", "Rosa Maria Marcucci", "Ruth Caldeira de Melo", "Marcelo Garcia Manzato"], "title": "An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care", "comment": "81 pages, 19 figures, 3 annexes", "summary": "There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.", "AI": {"tldr": "They propose and evaluate an interpretable recommender system for geriatric primary care that uses psychometric data and visual explanations to help clinicians build personalized care plans.", "motivation": "Healthcare recommender systems face obstacles: limited public clinical data, opaque recommendations that clinicians can\u2019t easily interpret, risks and liability of following automated advice, and uncertainty about the actual effectiveness of suggested actions. There is a particular need in gerontological primary care, a growing area with increasing demand for personalized, data-driven care plans that clinicians can still understand and trust.", "method": "They design a recommendation model that exploits the structure of psychometric data (e.g., standardized questionnaires and scales) and generates visual explanations of its recommendations. These visual explanations are explicitly constructed to be faithful to the model\u2019s internal reasoning while remaining understandable to care professionals. They evaluate the model offline on healthcare datasets collected by research partners in Brazil and conduct a user study to assess how interpretable the visual explanations are for practitioners.", "result": "In offline experiments, the proposed model performs competitively (or better) compared with alternative approaches on the collected healthcare datasets. In the user study, care professionals find the visual explanations understandable and useful for supporting their decision-making in constructing personalized care plans.", "conclusion": "The proposed recommender system, grounded in psychometric data and visual, model-faithful explanations, can effectively support gerontological primary care professionals in designing personalized care plans. The empirical and user-study results indicate that this approach can help overcome key barriers to using recommender systems in healthcare and is promising for a healthcare niche expected to grow significantly as populations age."}}
{"id": "2601.19657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19657", "abs": "https://arxiv.org/abs/2601.19657", "authors": ["Zihou Zhang", "Zheyong Xie", "Li Zhong", "Haifeng Liu", "Shaosheng Cao"], "title": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token", "comment": null, "summary": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.", "AI": {"tldr": "The paper analyzes instability in Diffusion Language Models caused by moving attention sinks and proposes a dedicated, position-agnostic sink token to stabilize attention and improve performance.", "motivation": "Diffusion Language Models offer parallel text generation comparable to autoregressive models but suffer from a critical instability called the moving sink phenomenon, where special low-norm sink tokens appear at varying positions across diffusion steps, harming robustness. The authors aim to understand why this happens and to design a simple, reliable mechanism to remove this instability without sacrificing the benefits of diffusion-based generation.", "method": "The authors first analyze attention sinks in DLMs, focusing on token representations in the Transformer value space and how low-norm sink tokens prevent excessive information mixing. They then introduce a modified attention mask that adds an extra special token. This token is constrained to attend only to itself while being visible to all other tokens, thereby serving as a fixed structural sink. They empirically evaluate the impact of this design on model stability and performance, and further probe the token\u2019s representations to verify that its function is structural rather than semantic and does not depend on its position.", "result": "Experiments show that adding a single extra sink token significantly stabilizes the behavior of attention sinks in DLMs, leading to substantial improvements in model performance. Analysis reveals that the new sink token consistently absorbs the sink behavior, regardless of where it is placed, and carries negligible semantic information, confirming that it operates as a purely structural mechanism rather than encoding content.", "conclusion": "The work identifies moving attention sinks as a key source of instability in Diffusion Language Models and shows that they act as a safeguard against information over-mixing. By introducing a simple structural sink token via a modified attention mask, the authors fix the sink\u2019s location, improving robustness and performance. The position-agnostic and semantically neutral nature of the token demonstrates that DLMs benefit from an explicit, dedicated sink, offering a straightforward architectural enhancement for stable parallel text generation."}}
{"id": "2601.19825", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.19825", "abs": "https://arxiv.org/abs/2601.19825", "authors": ["Saikrishna Sudarshan", "Tanay Kulkarni", "Manasi Patwardhan", "Lovekesh Vig", "Ashwin Srinivasan", "Tanmay Tulsidas Verlekar"], "title": "Routing End User Queries to Enterprise Databases", "comment": "6 pages, 2 figures", "summary": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.", "AI": {"tldr": "They propose a reasoning-based reranking strategy to route natural language questions to the right database in multi-database enterprise settings, and it beats embedding-only and direct LLM baselines.", "motivation": "Routing NL queries to the correct database becomes harder as the number of overlapping databases grows and queries are ambiguous; existing benchmarks are too simple, so they extend NL-to-SQL datasets to build realistic tests and study this challenge.", "method": "They extend existing NL-to-SQL datasets to create multi-database benchmarks, then design a modular reasoning-driven reranking approach that explicitly models three aspects: schema coverage (how well a DB schema can answer a query), structural connectivity (how tables/joins support the query), and fine-grained semantic alignment between query and schema; this reranker is used on top of initial candidates (e.g., from embeddings or LLMs).", "result": "Their reasoning-based reranking strategy consistently outperforms purely embedding-based routing and direct LLM prompting across multiple evaluation metrics on the new benchmarks.", "conclusion": "Structured, modular reasoning over schema coverage, structure, and semantic alignment is crucial for accurate query-to-database routing in large, overlapping enterprise repositories, and outperforms naive embedding-only and direct LLM approaches."}}
{"id": "2601.19667", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19667", "abs": "https://arxiv.org/abs/2601.19667", "authors": ["Adam Remaki", "Christel G\u00e9rardin", "Eul\u00e0lia Farr\u00e9-Maduell", "Martin Krallinger", "Xavier Tannier"], "title": "SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking", "comment": null, "summary": "We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.", "AI": {"tldr": "SynCABEL uses large language models to generate synthetic, context-rich training data for biomedical entity linking, achieving state-of-the-art performance with far less manual annotation.", "motivation": "Supervised biomedical entity linking requires large amounts of expert-annotated data, which is expensive and slow to obtain, especially across multiple languages and large ontologies. This data scarcity limits the performance and scalability of current BEL systems. The paper aims to alleviate this bottleneck by replacing much of the manual labeling effort with synthetic supervision generated automatically.", "method": "The authors propose SynCABEL, a framework that uses large language models to create context-rich synthetic training examples for every candidate concept in a target biomedical knowledge base. These synthetic examples are then used to train decoder-only models for BEL. They also employ guided inference strategies to better leverage the synthetic data. Additionally, they design an LLM-as-a-judge evaluation protocol to assess clinical validity beyond strict exact-code matching, which is sensitive to ontology redundancy.", "result": "SynCABEL achieves new state-of-the-art results on three multilingual biomedical entity linking benchmarks: MedMentions (English), QUAERO (French), and SPACCC (Spanish). In data-efficiency studies, SynCABEL matches the performance of fully human-supervised models while using up to 60% less expert-annotated data. Under the LLM-as-a-judge evaluation, SynCABEL is shown to substantially increase the proportion of clinically valid predictions compared to baselines.", "conclusion": "Synthetic, LLM-generated contextual training data can effectively substitute a large portion of manual expert annotation for biomedical entity linking, providing broad supervision across concepts and languages. SynCABEL both improves state-of-the-art accuracy and significantly reduces annotation costs. Standard exact-code evaluation underestimates real clinical utility, and an LLM-as-a-judge protocol better captures clinically valid matches. The authors release their synthetic datasets, models, and code to facilitate reproducibility and further work in this direction."}}
{"id": "2601.19834", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19834", "abs": "https://arxiv.org/abs/2601.19834", "authors": ["Jialong Wu", "Xiaoying Zhang", "Hongyi Yuan", "Xiangcheng Zhang", "Tianhao Huang", "Changjing He", "Chaoyi Deng", "Renrui Zhang", "Youbin Wu", "Mingsheng Long"], "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models", "comment": "Project page: https://thuml.github.io/Reasoning-Visual-World", "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.", "AI": {"tldr": "The paper investigates when and how visual generation in unified multimodal models helps reasoning, proposing that for certain physically grounded tasks visual world models outperform purely verbal ones, and providing a benchmark and experiments to support this.", "motivation": "Although large language models with chain-of-thought prompting achieve strong performance in abstract domains via verbal reasoning, they remain weak in physical and spatial reasoning where humans rely on richer, often visual, internal world models. With the rise of unified multimodal models that can both see and generate images, it is unclear under what conditions visual pathways genuinely improve reasoning beyond text-only CoT. The paper aims to clarify the role and benefits of visual world modeling in multimodal reasoning systems.", "method": "The authors adopt a world-model perspective on chain-of-thought reasoning, formally defining internal world modeling as a core component and distinguishing between verbal and visual world models. They introduce the \"visual superiority hypothesis\" that some tasks, especially those grounded in the physical world, are better served by visual world models. Empirically, they design and curate a new benchmark, VisWorld-Eval, containing tasks that require interleaved visual and verbal chain-of-thought. Using a state-of-the-art unified multimodal model, they run controlled experiments comparing purely verbal CoT, purely visual, and interleaved visual-verbal CoT across different task types.", "result": "On tasks in VisWorld-Eval that are designed to favor or require visual world modeling\u2014typically involving physical and spatial reasoning\u2014the interleaved visual-verbal CoT substantially outperforms purely verbal CoT. For tasks that are more abstract or do not benefit from visual grounding, interleaving visual reasoning offers no consistent advantage over text-only CoT. These results empirically support the visual superiority hypothesis in the appropriate task regimes and delineate the boundaries of visual benefits.", "conclusion": "The study concludes that visual generation within unified multimodal models can serve as a more effective internal world model than purely verbal representations for certain physically grounded reasoning tasks, leading to better performance when used in an interleaved visual-verbal CoT scheme. However, this benefit is task-dependent and does not extend uniformly to all reasoning problems. The work positions multimodal world modeling as a critical direction for building more human-like AI and provides formal analysis, a benchmark, and empirical evidence to guide future research on multimodal reasoning strategies."}}
{"id": "2601.19723", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19723", "abs": "https://arxiv.org/abs/2601.19723", "authors": ["Yifan Wang", "Jichen Zheng", "Jingyuan Sun", "Yunhao Zhang", "Chunyu Ye", "Jixing Li", "Chengqing Zong", "Shaonan Wang"], "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes", "comment": null, "summary": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.", "AI": {"tldr": "The paper builds a framework to simulate specific aphasia subtypes (Broca\u2019s, Wernicke\u2019s) in large language models by selectively perturbing internal components, showing that modular Mixture-of-Experts models yield especially interpretable and localized impairments.", "motivation": "Large language models show human-like language behavior and internal representations, so they may serve as computational models for language cognition and its disorders. The authors want to know whether LLMs can be deliberately manipulated to mimic specific language-production impairments seen in aphasia after focal brain lesions. This would offer a scalable, controllable proxy for testing rehabilitation ideas and for probing how language functions are organized and break down under damage.", "method": "They propose a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components within LLMs. The approach is applied to both modular Mixture-of-Experts (MoE) models and standard dense Transformers through a unified intervention interface. The pipeline: (i) identifies model components statistically linked to Broca\u2019s vs Wernicke\u2019s aphasia phenotypes, (ii) characterizes these components using linguistic probing tasks to interpret their roles, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, then evaluates language performance using Western Aphasia Battery (WAB) subtests summarized in an Aphasia Quotient (AQ). They compare targeted lesions to size-matched random perturbations and contrast MoE vs dense architectures.", "result": "For both MoE and dense models, perturbing components specifically associated with a given aphasia subtype leads to more systematic, aphasia-like degradations in language performance than random perturbations affecting the same number of parameters. The induced deficits track WAB-based measures, yielding graded reductions in Aphasia Quotient consistent with targeted impairments. MoE architectures, due to their modularity, produce more localized and interpretable mappings between aphasia phenotypes and model components, facilitating clearer lesion\u2013deficit relationships.", "conclusion": "Targeted, clinically informed component perturbations can make LLMs exhibit aphasia-like language-production deficits in a controlled, graded manner. Modular Mixture-of-Experts models in particular afford clearer and more localized correspondences between functional components and language behaviors. This suggests that modular LLMs are a promising platform for simulating aphasic language production and for studying how specific language functions fail under targeted disruptions, with potential applications to cognitive neuroscience and rehabilitation research."}}
{"id": "2601.19739", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19739", "abs": "https://arxiv.org/abs/2601.19739", "authors": ["Runjia Zeng", "Qifan Wang", "Qiang Guan", "Ruixiang Tang", "Lifu Huang", "Zhenting Wang", "Xueling Zhang", "Cheng Han", "Dongfang Liu"], "title": "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching", "comment": "ICLR 2026", "summary": "Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/", "AI": {"tldr": "TokenSeek is an instance-aware token selection/ablation plugin for transformer LLMs that substantially reduces fine-tuning memory while preserving or improving performance.", "motivation": "Fine-tuning large language models is memory-hungry primarily due to activation storage, making adaptation to downstream tasks inefficient or infeasible on limited hardware. Existing memory-efficient fine-tuning methods that optimize activations are typically data-agnostic, failing to adaptively focus computation on important tokens per instance, which can lead to unstable or suboptimal training. There is a need for a method that is both memory-efficient and sensitive to instance-level token importance.", "method": "TokenSeek is proposed as a universal plugin for transformer-based models that performs instance-aware token seeking (identifying important tokens) and token ditching (removing or down-weighting unimportant tokens) during fine-tuning. This dynamically reduces the number of tokens whose activations must be stored, thereby lowering memory usage. The method is designed to be model-agnostic, easily integrated into different LLM architectures, and interpretable via its token selection behavior.", "result": "On models such as Llama3.2 1B, TokenSeek achieves large memory savings for fine-tuning, using as little as 14.8% of the original memory consumption, while maintaining comparable or even better task performance compared to standard fine-tuning. The interpretable token selection patterns further demonstrate that the method focuses computation on semantically important tokens.", "conclusion": "Instance-aware token seeking and ditching via TokenSeek offers an effective, widely applicable way to reduce activation memory during LLM fine-tuning, without sacrificing accuracy and sometimes improving it. Its interpretability also sheds light on token efficiency, providing guidance for future work on making LLM training and adaptation more resource-efficient."}}
{"id": "2601.19773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19773", "abs": "https://arxiv.org/abs/2601.19773", "authors": ["Zhuohan Long", "Zhijie Bao", "Zhongyu Wei"], "title": "Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis", "comment": null, "summary": "Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \\rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .", "AI": {"tldr": "They propose and benchmark an interactive framework to evaluate and improve how medical dialogue agents gather clinical evidence during consultations.", "motivation": "Existing medical AI evaluations focus on static inputs or final diagnostic outcomes, ignoring how well an agent interactively asks questions and collects the necessary clinical evidence, which is crucial for safe, realistic consultations.", "method": "They build a simulated consultation environment with a simulated patient and a simulated reporter grounded in atomic clinical evidences, define a new metric called Information Coverage Rate (ICR) to measure how completely an agent collects required evidence, construct the EviMed benchmark covering various diseases, test 10 models of different reasoning strength, and introduce REFINE, a strategy that uses diagnostic verification to iteratively identify uncertainties and drive targeted question-asking and evidence gathering.", "result": "Experiments show that good diagnostic reasoning alone does not ensure thorough information collection, with evidence-gathering being a key bottleneck; the proposed REFINE strategy significantly improves information coverage and overall performance across multiple datasets and enables smaller models to perform strongly when guided by a more capable reasoning model.", "conclusion": "Interactive medical agents must be evaluated and optimized not just for diagnostic accuracy but for the completeness of their evidence-gathering process; the proposed framework, metric ICR, EviMed benchmark, and REFINE strategy together provide an effective way to measure and enhance this capability, including in collaborative model setups."}}
{"id": "2601.19792", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19792", "abs": "https://arxiv.org/abs/2601.19792", "authors": ["Peter Zeng", "Weiling Li", "Amie Paige", "Zhengxiang Wang", "Panagiotis Kaliosis", "Dimitris Samaras", "Gregory Zelinsky", "Susan Brennan", "Owen Rambow"], "title": "LVLMs and Humans Ground Differently in Referential Communication", "comment": "24 pages, 16 figures, preprint", "summary": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.", "AI": {"tldr": "The paper studies how well large vision-language models (LVLMs) can establish common ground with humans in a referential communication task using images that lack obvious names, revealing current limitations in predicting and resolving human referring expressions.", "motivation": "Generative AI agents struggle to collaborate naturally with humans because they lack robust models of common ground and shared context, which are essential for predicting human intent in interactive language use. The authors aim to systematically test this gap in a controlled communication setting.", "method": "They design a factorial referential communication experiment with director\u2013matcher pairs in four configurations: human\u2013human, human\u2013AI, AI\u2013human, and AI\u2013AI. Pairs engage in multi-turn dialogues over repeated rounds to match pictures of objects that do not have clear lexicalized labels. They build and release an online pipeline for collecting such dialogues, along with tools to measure accuracy, efficiency, and lexical overlap, and compile a corpus of 356 dialogues from 89 pairs over four rounds each.", "result": "The collected corpus and analyses show that current LVLMs have notable limitations in interactively resolving referring expressions in this setting, particularly compared with humans. Performance metrics such as matching accuracy, communicative efficiency, and the development of shared lexical conventions reveal these deficits.", "conclusion": "The work demonstrates that LVLMs, despite their generative capabilities, are still poor at modeling common ground and resolving referential expressions in interactive, iterative tasks. The released dataset, pipeline, and evaluation tools provide a benchmark for future research aimed at improving AI agents\u2019 ability to coordinate with humans through language."}}
{"id": "2601.19802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19802", "abs": "https://arxiv.org/abs/2601.19802", "authors": ["Aohua Li", "Yuanshuo Zhang", "Ge Gao", "Bo Chen", "Xiaobing Zhao"], "title": "Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation", "comment": null, "summary": "Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.", "AI": {"tldr": "Proposes a new zero-shot stance detection task that automatically discovers multiple dynamic targets and their stances from social media text, along with a Chinese dataset and LLM-based baselines that perform strongly.", "motivation": "Most stance detection assumes a fixed, predefined target, but in real social media, targets are implicit, multiple, and change over time. This gap makes existing methods poorly suited for realistic applications where the system must first figure out what entities/issues are being discussed before judging stance. The paper aims to make stance detection more practical and closer to real-world usage.", "method": "They define a new task called zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), where models must automatically extract all relevant targets from text and determine stance toward each without prior target lists. They build a Chinese social media dataset for this setting and design multi-dimensional metrics that separately and jointly evaluate target recognition and stance classification. They explore two fine-tuning paradigms for large language models: (1) integrated fine-tuning, where target generation and stance prediction are handled in a single end-to-end model; and (2) two-stage fine-tuning, where one model (or stage) focuses on target generation and another on stance detection. They benchmark several baseline LLMs under these strategies.", "result": "Fine-tuned large language models outperform other baselines on the DGTA task. Specifically, the two-stage fine-tuned Qwen2.5-7B model achieves the best overall target recognition performance with a comprehensive score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B model reaches the highest stance detection F1 score of 79.26%.", "conclusion": "Dynamic, zero-shot stance detection without predefined targets is feasible when leveraging fine-tuned LLMs. The proposed DGTA task, dataset, and metrics establish a more realistic benchmark for stance detection in social media. Different fine-tuning strategies trade off between target recognition quality and stance classification performance, indicating that decomposing the problem into stages can benefit target discovery, while integrated approaches are strong for end-to-end stance prediction."}}
{"id": "2601.19827", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.19827", "abs": "https://arxiv.org/abs/2601.19827", "authors": ["Mahdi Astaraki", "Mohammad Arshi Saloot", "Ali Shiraee Kasmaee", "Hamidreza Mahyar", "Soheila Samiee"], "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering", "comment": "27 pages, 15 figures", "summary": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.", "AI": {"tldr": "This paper studies when and why iterative Retrieval-Augmented Generation (RAG) can beat a strong static RAG baseline that sees all gold evidence at once, finding that staged, synchronized retrieval\u2013reasoning often works better in complex scientific QA.", "motivation": "While RAG helps LLMs access non-parametric knowledge, it is unclear under what conditions iterative retrieval-reasoning loops actually outperform static approaches, especially in scientific domains that require multi-hop reasoning, have sparse and heterogeneous knowledge, and are sensitive to retrieval errors. Existing work lacked controlled, mechanism-level diagnostics comparing iterative RAG to an idealized static upper bound, leaving practitioners without clear guidance on whether and how to design iterative controllers for specialized domains like chemistry.", "method": "The authors conduct a controlled diagnostic study on the ChemKGMultiHopQA dataset, focusing on chemistry questions that truly require retrieval and multi-hop reasoning. They benchmark eleven state-of-the-art LLMs under three conditions: (i) No Context (only parametric knowledge), (ii) Gold Context (all oracle evidence provided at once as an ideal static RAG upper bound), and (iii) Iterative RAG (a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping). They design diagnostics to probe different mechanisms and failure modes, such as retrieval coverage gaps, anchor-carry drop across hops, query quality, composition fidelity of multi-hop reasoning, and calibration of the controller\u2019s stopping decisions. They then compare accuracies and behaviors across models and regimes to understand when iterative staged retrieval helps or fails.", "result": "Across eleven LLMs, Iterative RAG consistently outperforms the Gold Context regime, with improvements up to 25.6 percentage points, particularly for models that were not fine-tuned for explicit reasoning. The staged, synchronized retrieval-reasoning process reduces late-hop failures, alleviates context overload that arises when all evidence is given at once, and allows dynamic correction of early hypothesis drift. However, several failure modes remain prominent: incomplete coverage of all required hops, trajectories where the model latches onto distractor evidence, miscalibrated early stopping that halts before sufficient evidence is gathered, and high rates of composition errors in multi-hop reasoning even when retrieval is perfect.", "conclusion": "Staged iterative retrieval can be more critical to performance than simply providing ideal evidence in a static batch; controlling how and when evidence is introduced significantly affects LLM reasoning in complex scientific QA. The study shows that synchronized iterative RAG can surpass an idealized static Gold Context upper bound and provides a set of diagnostics and practical recommendations for building, tuning, and troubleshooting RAG systems in specialized domains like chemistry. This work lays groundwork for more reliable and controllable iterative retrieval-reasoning frameworks beyond simple one-shot RAG, especially where multi-hop reasoning and heterogeneous evidence are central."}}
{"id": "2601.19847", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19847", "abs": "https://arxiv.org/abs/2601.19847", "authors": ["Fangan Dong", "Zuming Yan", "Xuri Ge", "Zhiwei Xu", "Mengqi Zhang", "Xuanang Chen", "Ben He", "Xin Xin", "Zhumin Chen", "Ying Zhou"], "title": "Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering", "comment": null, "summary": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.", "AI": {"tldr": "They discover specific neurons in LLMs that correlate with correct reasoning and introduce AdaRAS, a test-time method that steers these neurons\u2019 activations to improve reasoning accuracy without extra training or sampling.", "motivation": "LLMs can reason well but often need costly post-training or multiple sampling to perform reliably on hard tasks like math and coding. There is a need for a cheap, test-time method to boost reliability without retraining or heavy sampling.", "method": "1) Empirically analyze neuron activations and identify a small subset whose activity is strongly predictive of reasoning correctness. 2) Define these as Reasoning-Critical Neurons (RCNs) selected using a polarity-aware mean-difference criterion. 3) Propose AdaRAS, which at inference detects and adaptively steers the activations of RCNs, enhancing signals associated with correct reasoning only when needed, while being conservative for already-correct cases.", "result": "Across 10 math and coding benchmarks, AdaRAS consistently improves reasoning reliability, achieving over 13% absolute gains on AIME-24 and AIME-25. It also transfers well across datasets and scales to larger, stronger models, and surpasses post-training approaches without incurring extra training or sampling costs.", "conclusion": "Reasoning performance in LLMs is strongly tied to a small set of identifiable neurons. By adaptively steering these Reasoning-Critical Neurons at test time, AdaRAS provides an efficient and effective way to boost reasoning reliability that outperforms more expensive post-training methods and generalizes across tasks and model scales."}}
{"id": "2601.19871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19871", "abs": "https://arxiv.org/abs/2601.19871", "authors": ["Nicholas Cheng"], "title": "Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection", "comment": "12 pages, 3 figures, 6 tables. Accepted to the NeurIPS 2025 Workshop on Multilingual Representation Learning (Mexico City) and the AAAI 2025 Workshop on Language Models for Under-Resourced Communities (LM4UC). Code and data available at: https://github.com/Nickcheng123/reflective-translation-mt", "summary": "Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.", "AI": {"tldr": "The paper proposes Reflective Translation, a prompt-based framework where a model translates, critiques its own output, and then refines the translation, improving low-resource MT quality without fine-tuning.", "motivation": "Low-resource languages like isiZulu and isiXhosa suffer from poor machine translation performance due to scarce parallel data and tools. At the same time, large language models have shown that self-reflection can enhance reasoning and factual accuracy. The authors are motivated to see whether similar self-reflection mechanisms can systematically improve translation quality for low-resource language pairs without requiring costly training or additional data.", "method": "The authors design Reflective Translation, a three-step, prompt-based pipeline for large language models: (1) generate an initial translation from English into isiZulu or isiXhosa; (2) prompt the model to produce a structured self-critique of this translation, identifying potential errors or weaknesses; and (3) use this critique as additional context to generate a refined second-pass translation. They evaluate this framework on English\u2192isiZulu and English\u2192isiXhosa using the OPUS-100 and NTREX-African test sets. Multiple prompting variants and confidence thresholds are explored to understand how reflection instructions and filtering impact quality. No model fine-tuning is involved, making the method model-agnostic.", "result": "Across both language pairs and datasets, the reflective second-pass translations outperform the initial translations. The authors report average gains of up to +0.22 BLEU and +0.18 COMET between first and second passes. They apply paired nonparametric statistical tests to confirm that these improvements are statistically significant rather than due to random variation. The work also yields a reflection-augmented dataset consisting of source sentences, initial translations, critiques, and refined translations.", "conclusion": "The study concludes that structured self-reflection can reliably improve translation quality for low-resource languages in a zero-fine-tuning, prompt-only setup. Reflective Translation is shown to be a practical, model-agnostic mechanism that yields statistically significant gains in BLEU and COMET for English\u2013isiZulu and English\u2013isiXhosa. The reflection-augmented data produced by the pipeline offers additional value for future supervised learning or detailed error analysis, suggesting that self-critique is a promising direction for enhancing low-resource machine translation."}}
{"id": "2601.19899", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19899", "abs": "https://arxiv.org/abs/2601.19899", "authors": ["Luis Lorenzo", "Marcos Montana-Mendez", "Sergio Figueiras", "Miguel Boubeta", "Cristobal Bernardo-Castineira"], "title": "Evaluation of Oncotimia: An LLM based system for supporting tumour boards", "comment": "9 pages, 2 figures", "summary": "Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.", "AI": {"tldr": "The paper introduces ONCOTIMIA, an AI-powered system that uses large language models and retrieval-augmented generation to automatically populate structured lung cancer tumour board forms from unstructured clinical data, showing about 80% correct field completion with acceptable latency.", "motivation": "Multidisciplinary tumour boards are essential for oncology care but require clinicians to manually extract and structure large amounts of heterogeneous clinical information into standardized forms, creating significant documentation burden and inefficiency. The authors aim to reduce this burden while maintaining or improving data quality and consistency.", "method": "The authors design ONCOTIMIA, a modular clinical tool that integrates generative AI into oncology workflows. The system uses a multi-layer data lake to hold clinical data, hybrid relational and vector databases for structured and semantic access, retrieval-augmented generation to ground LLM outputs in patient-specific data, and a rule-driven adaptive form model that maps LLM outputs into standardized tumour board fields. They deploy six large language models via AWS Bedrock and test them on ten lung cancer cases, evaluating form field completion accuracy and end-to-end latency of the autocompletion process.", "result": "Across the six evaluated LLMs, the system achieves high form completion performance, with the best configuration correctly filling 80% of the tumour board fields and maintaining clinically acceptable response times. Larger and more recent models yield the highest accuracies without causing unacceptable latency.", "conclusion": "LLM-assisted automatic completion of tumour board forms is technically and operationally feasible in lung cancer MDT workflows. ONCOTIMIA can substantially reduce documentation burden while preserving structured data quality, and newer, larger LLMs appear particularly well-suited due to their favorable accuracy-latency trade-off."}}
