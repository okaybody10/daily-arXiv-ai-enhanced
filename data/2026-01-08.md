<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing](https://arxiv.org/abs/2601.03261)
*Shuo Lu,Yinuo Xu,Jianjie Cheng,Lingxiao He,Meng Wang,Jian Liang*

Main category: cs.CL

TL;DR: The paper introduces DeepResearch-Slice, a neuro-symbolic method that narrows the gap between retrieving relevant information and actually using it for reasoning by explicitly selecting spans of text before reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing deep research agents focus on improving how well they can retrieve relevant documents, but they often fail to effectively use the correct (gold) evidence, especially in noisy contexts where many irrelevant details distract the model. This retrieval-utilization gap limits the benefits of better search and retrieval.

Method: The authors propose DeepResearch-Slice, a neuro-symbolic framework that explicitly predicts start–end span indices in retrieved texts. These spans are then used as a deterministic hard filter, discarding irrelevant context before the reasoning model operates. This contrasts with standard soft attention mechanisms that implicitly weigh tokens but still process all the noise.

Result: Across six benchmarks, DeepResearch-Slice significantly improves robustness in noisy settings. When applied to frozen reasoning backbones (no parameter updates), the method increases performance from 19.1% to 33.0%, a 73% relative gain, demonstrating that targeted span filtering can greatly enhance utilization of retrieved evidence.

Conclusion: Explicit grounding via span selection meaningfully closes the retrieval-utilization gap in deep research agents. Deterministic, neuro-symbolic filtering of context can mitigate noise and improve reasoning quality without retraining core models, indicating that future open-ended research systems should incorporate such explicit grounding mechanisms.

Abstract: Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.

</details>


### [2] [Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models](https://arxiv.org/abs/2601.03265)
*Kai Hu,Abhinav Aggarwal,Mehran Khodabandeh,David Zhang,Eric Hsin,Li Chen,Ankit Jain,Matt Fredrikson,Akash Bharadwaj*

Main category: cs.CL

TL;DR: Jailbreak-Zero is a red teaming framework that uses a policy-based approach and an attack LLM, fine-tuned via preferences, to generate diverse, realistic adversarial prompts that more effectively uncover LLM safety vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety evaluations often rely on constrained, example-based jailbreak prompts that have limited coverage of safety policies, narrow attack strategies, and may not reflect real user behavior. This makes it difficult to comprehensively uncover and mitigate vulnerabilities, especially as models and policies evolve. The authors aim to create a more scalable, systematic, and realistic red teaming methodology that can generalize across policies and better stress-test modern LLMs.

Method: The authors propose Jailbreak-Zero, which (1) formulates red teaming as a policy-based task rather than a fixed set of example prompts, (2) uses an attack LLM to automatically generate a large number of adversarial prompts conditioned on safety policies, and (3) fine-tunes this attack LLM using a preference dataset so that it learns to prefer prompts with better policy coverage, higher attack diversity, and high fidelity to realistic user inputs. They then evaluate the resulting prompts against various target LLMs, both open-source and proprietary, to measure attack success rates and other metrics.

Result: Jailbreak-Zero produces adversarial prompts that achieve significantly higher attack success rates than existing state-of-the-art jailbreak and red teaming methods when evaluated on multiple target LLMs, including strong proprietary models such as GPT-40 and Claude 3.5. The prompts also exhibit broad policy coverage, diverse attack strategies, and remain human-readable and realistic, while requiring minimal ongoing human curation or intervention.

Conclusion: The paper concludes that Jailbreak-Zero provides a more effective and scalable framework for LLM safety evaluation than prior example-based approaches. By optimizing for policy coverage, prompt diversity, and realism via a preference-trained attack model, it uncovers more vulnerabilities with less human labor. This policy-based, attack-LLM-driven red teaming paradigm is positioned as a promising direction for future safety work and for strengthening defenses of current and next-generation LLMs.

Abstract: This paper introduces Jailbreak-Zero, a novel red teaming methodology that shifts the paradigm of Large Language Model (LLM) safety evaluation from a constrained example-based approach to a more expansive and effective policy-based framework. By leveraging an attack LLM to generate a high volume of diverse adversarial prompts and then fine-tuning this attack model with a preference dataset, Jailbreak-Zero achieves Pareto optimality across the crucial objectives of policy coverage, attack strategy diversity, and prompt fidelity to real user inputs. The empirical evidence demonstrates the superiority of this method, showcasing significantly higher attack success rates against both open-source and proprietary models like GPT-40 and Claude 3.5 when compared to existing state-of-the-art techniques. Crucially, Jailbreak-Zero accomplishes this while producing human-readable and effective adversarial prompts with minimal need for human intervention, thereby presenting a more scalable and comprehensive solution for identifying and mitigating the safety vulnerabilities of LLMs.

</details>


### [3] [Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support](https://arxiv.org/abs/2601.03266)
*Alif Munim,Jun Ma,Omar Ibrahim,Alhusain Abdalla,Shuolin Yin,Leo Chen,Bo Wang*

Main category: cs.CL

TL;DR: The paper evaluates lightweight, on-device large language models for clinical decision support and finds they can match or approach larger proprietary systems, especially after fine-tuning, while improving privacy and practicality for real-world clinical use.


<details>
  <summary>Details</summary>
Motivation: Proprietary clinical LLMs are powerful but depend on cloud infrastructure and raise privacy concerns, while existing open-source models are typically too large for resource-limited clinical environments. The authors aim to determine whether smaller, on-device LLMs can provide strong diagnostic and decision-support performance without sacrificing privacy or requiring extensive computational resources.

Method: The authors benchmark two relatively small on-device LLMs (gpt-oss-20b and gpt-oss-120b) on three representative clinical tasks: (1) general disease diagnosis, (2) ophthalmology-specific diagnosis and management, and (3) simulation of human expert grading and evaluation. They compare these models against top proprietary systems (GPT-5 and o4-mini) and a strong open-source baseline (DeepSeek-R1). They also fine-tune gpt-oss-20b on general diagnostic datasets to assess how task-specific adaptation affects performance.

Result: The gpt-oss models perform on par with or better than DeepSeek-R1 and o4-mini across the evaluated tasks, despite having substantially smaller model sizes. Fine-tuning gpt-oss-20b on diagnostic data significantly boosts its accuracy, bringing its performance close to that of the leading proprietary model GPT-5.

Conclusion: On-device LLMs of moderate size can deliver accurate, adaptable, and privacy-preserving clinical decision support, particularly when fine-tuned on relevant data. This suggests a feasible route to integrating LLMs into routine clinical workflows in a way that respects privacy and works within hardware and resource constraints.

Abstract: Large language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary systems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local inference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark two on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis, specialty-specific (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation. We compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source model (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by fine-tuning gpt-oss-20b on general diagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-mini despite being substantially smaller. In addition, fine-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b, enabling it to approach the performance of GPT-5. These findings highlight the potential of on-device LLMs to deliver accurate, adaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into routine clinical practice.

</details>


### [4] [OpenAI GPT-5 System Card](https://arxiv.org/abs/2601.03267)
*Aaditya Singh,Adam Fry,Adam Perelman,Adam Tart,Adi Ganesh,Ahmed El-Kishky,Aidan McLaughlin,Aiden Low,AJ Ostrow,Akhila Ananthram,Akshay Nathan,Alan Luo,Alec Helyar,Aleksander Madry,Aleksandr Efremov,Aleksandra Spyra,Alex Baker-Whitcomb,Alex Beutel,Alex Karpenko,Alex Makelov,Alex Neitz,Alex Wei,Alexandra Barr,Alexandre Kirchmeyer,Alexey Ivanov,Alexi Christakis,Alistair Gillespie,Allison Tam,Ally Bennett,Alvin Wan,Alyssa Huang,Amy McDonald Sandjideh,Amy Yang,Ananya Kumar,Andre Saraiva,Andrea Vallone,Andrei Gheorghe,Andres Garcia Garcia,Andrew Braunstein,Andrew Liu,Andrew Schmidt,Andrey Mereskin,Andrey Mishchenko,Andy Applebaum,Andy Rogerson,Ann Rajan,Annie Wei,Anoop Kotha,Anubha Srivastava,Anushree Agrawal,Arun Vijayvergiya,Ashley Tyra,Ashvin Nair,Avi Nayak,Ben Eggers,Bessie Ji,Beth Hoover,Bill Chen,Blair Chen,Boaz Barak,Borys Minaiev,Botao Hao,Bowen Baker,Brad Lightcap,Brandon McKinzie,Brandon Wang,Brendan Quinn,Brian Fioca,Brian Hsu,Brian Yang,Brian Yu,Brian Zhang,Brittany Brenner,Callie Riggins Zetino,Cameron Raymond,Camillo Lugaresi,Carolina Paz,Cary Hudson,Cedric Whitney,Chak Li,Charles Chen,Charlotte Cole,Chelsea Voss,Chen Ding,Chen Shen,Chengdu Huang,Chris Colby,Chris Hallacy,Chris Koch,Chris Lu,Christina Kaplan,Christina Kim,CJ Minott-Henriques,Cliff Frey,Cody Yu,Coley Czarnecki,Colin Reid,Colin Wei,Cory Decareaux,Cristina Scheau,Cyril Zhang,Cyrus Forbes,Da Tang,Dakota Goldberg,Dan Roberts,Dana Palmie,Daniel Kappler,Daniel Levine,Daniel Wright,Dave Leo,David Lin,David Robinson,Declan Grabb,Derek Chen,Derek Lim,Derek Salama,Dibya Bhattacharjee,Dimitris Tsipras,Dinghua Li,Dingli Yu,DJ Strouse,Drew Williams,Dylan Hunn,Ed Bayes,Edwin Arbus,Ekin Akyurek,Elaine Ya Le,Elana Widmann,Eli Yani,Elizabeth Proehl,Enis Sert,Enoch Cheung,Eri Schwartz,Eric Han,Eric Jiang,Eric Mitchell,Eric Sigler,Eric Wallace,Erik Ritter,Erin Kavanaugh,Evan Mays,Evgenii Nikishin,Fangyuan Li,Felipe Petroski Such,Filipe de Avila Belbute Peres,Filippo Raso,Florent Bekerman,Foivos Tsimpourlas,Fotis Chantzis,Francis Song,Francis Zhang,Gaby Raila,Garrett McGrath,Gary Briggs,Gary Yang,Giambattista Parascandolo,Gildas Chabot,Grace Kim,Grace Zhao,Gregory Valiant,Guillaume Leclerc,Hadi Salman,Hanson Wang,Hao Sheng,Haoming Jiang,Haoyu Wang,Haozhun Jin,Harshit Sikchi,Heather Schmidt,Henry Aspegren,Honglin Chen,Huida Qiu,Hunter Lightman,Ian Covert,Ian Kivlichan,Ian Silber,Ian Sohl,Ibrahim Hammoud,Ignasi Clavera,Ikai Lan,Ilge Akkaya,Ilya Kostrikov,Irina Kofman,Isak Etinger,Ishaan Singal,Jackie Hehir,Jacob Huh,Jacqueline Pan,Jake Wilczynski,Jakub Pachocki,James Lee,James Quinn,Jamie Kiros,Janvi Kalra,Jasmyn Samaroo,Jason Wang,Jason Wolfe,Jay Chen,Jay Wang,Jean Harb,Jeffrey Han,Jeffrey Wang,Jennifer Zhao,Jeremy Chen,Jerene Yang,Jerry Tworek,Jesse Chand,Jessica Landon,Jessica Liang,Ji Lin,Jiancheng Liu,Jianfeng Wang,Jie Tang,Jihan Yin,Joanne Jang,Joel Morris,Joey Flynn,Johannes Ferstad,Johannes Heidecke,John Fishbein,John Hallman,Jonah Grant,Jonathan Chien,Jonathan Gordon,Jongsoo Park,Jordan Liss,Jos Kraaijeveld,Joseph Guay,Joseph Mo,Josh Lawson,Josh McGrath,Joshua Vendrow,Joy Jiao,Julian Lee,Julie Steele,Julie Wang,Junhua Mao,Kai Chen,Kai Hayashi,Kai Xiao,Kamyar Salahi,Kan Wu,Karan Sekhri,Karan Sharma,Karan Singhal,Karen Li,Kenny Nguyen,Keren Gu-Lemberg,Kevin King,Kevin Liu,Kevin Stone,Kevin Yu,Kristen Ying,Kristian Georgiev,Kristie Lim,Kushal Tirumala,Kyle Miller,Lama Ahmad,Larry Lv,Laura Clare,Laurance Fauconnet,Lauren Itow,Lauren Yang,Laurentia Romaniuk,Leah Anise,Lee Byron,Leher Pathak,Leon Maksin,Leyan Lo,Leyton Ho,Li Jing,Liang Wu,Liang Xiong,Lien Mamitsuka,Lin Yang,Lindsay McCallum,Lindsey Held,Liz Bourgeois,Logan Engstrom,Lorenz Kuhn,Louis Feuvrier,Lu Zhang,Lucas Switzer,Lukas Kondraciuk,Lukasz Kaiser,Manas Joglekar,Mandeep Singh,Mandip Shah,Manuka Stratta,Marcus Williams,Mark Chen,Mark Sun,Marselus Cayton,Martin Li,Marvin Zhang,Marwan Aljubeh,Matt Nichols,Matthew Haines,Max Schwarzer,Mayank Gupta,Meghan Shah,Melody Huang,Meng Dong,Mengqing Wang,Mia Glaese,Micah Carroll,Michael Lampe,Michael Malek,Michael Sharman,Michael Zhang,Michele Wang,Michelle Pokrass,Mihai Florian,Mikhail Pavlov,Miles Wang,Ming Chen,Mingxuan Wang,Minnia Feng,Mo Bavarian,Molly Lin,Moose Abdool,Mostafa Rohaninejad,Nacho Soto,Natalie Staudacher,Natan LaFontaine,Nathan Marwell,Nelson Liu,Nick Preston,Nick Turley,Nicklas Ansman,Nicole Blades,Nikil Pancha,Nikita Mikhaylin,Niko Felix,Nikunj Handa,Nishant Rai,Nitish Keskar,Noam Brown,Ofir Nachum,Oleg Boiko,Oleg Murk,Olivia Watkins,Oona Gleeson,Pamela Mishkin,Patryk Lesiewicz,Paul Baltescu,Pavel Belov,Peter Zhokhov,Philip Pronin,Phillip Guo,Phoebe Thacker,Qi Liu,Qiming Yuan,Qinghua Liu,Rachel Dias,Rachel Puckett,Rahul Arora,Ravi Teja Mullapudi,Raz Gaon,Reah Miyara,Rennie Song,Rishabh Aggarwal,RJ Marsan,Robel Yemiru,Robert Xiong,Rohan Kshirsagar,Rohan Nuttall,Roman Tsiupa,Ronen Eldan,Rose Wang,Roshan James,Roy Ziv,Rui Shu,Ruslan Nigmatullin,Saachi Jain,Saam Talaie,Sam Altman,Sam Arnesen,Sam Toizer,Sam Toyer,Samuel Miserendino,Sandhini Agarwal,Sarah Yoo,Savannah Heon,Scott Ethersmith,Sean Grove,Sean Taylor,Sebastien Bubeck,Sever Banesiu,Shaokyi Amdo,Shengjia Zhao,Sherwin Wu,Shibani Santurkar,Shiyu Zhao,Shraman Ray Chaudhuri,Shreyas Krishnaswamy,Shuaiqi,Xia,Shuyang Cheng,Shyamal Anadkat,Simón Posada Fishman,Simon Tobin,Siyuan Fu,Somay Jain,Song Mei,Sonya Egoian,Spencer Kim,Spug Golden,SQ Mah,Steph Lin,Stephen Imm,Steve Sharpe,Steve Yadlowsky,Sulman Choudhry,Sungwon Eum,Suvansh Sanjeev,Tabarak Khan,Tal Stramer,Tao Wang,Tao Xin,Tarun Gogineni,Taya Christianson,Ted Sanders,Tejal Patwardhan,Thomas Degry,Thomas Shadwell,Tianfu Fu,Tianshi Gao,Timur Garipov,Tina Sriskandarajah,Toki Sherbakov,Tomer Kaftan,Tomo Hiratsuka,Tongzhou Wang,Tony Song,Tony Zhao,Troy Peterson,Val Kharitonov,Victoria Chernova,Vineet Kosaraju,Vishal Kuo,Vitchyr Pong,Vivek Verma,Vlad Petrov,Wanning Jiang,Weixing Zhang,Wenda Zhou,Wenlei Xie,Wenting Zhan,Wes McCabe,Will DePue,Will Ellsworth,Wulfie Bain,Wyatt Thompson,Xiangning Chen,Xiangyu Qi,Xin Xiang,Xinwei Shi,Yann Dubois,Yaodong Yu,Yara Khakbaz,Yifan Wu,Yilei Qian,Yin Tat Lee,Yinbo Chen,Yizhen Zhang,Yizhong Xiong,Yonglong Tian,Young Cha,Yu Bai,Yu Yang,Yuan Yuan,Yuanzhi Li,Yufeng Zhang,Yuguang Yang,Yujia Jin,Yun Jiang,Yunyun Wang,Yushi Wang,Yutian Liu,Zach Stubenvoll,Zehao Dou,Zheng Wu,Zhigang Wang*

Main category: cs.CL

TL;DR: System card for GPT-5 describing system design, safety posture, and capabilities, especially for two main models.


<details>
  <summary>Details</summary>
Motivation: To document and justify the design choices, capabilities, and safety controls of the GPT-5 system at launch, with emphasis on real-world usefulness and risk management.

Method: Describe the overall system architecture (router plus multiple models), the training and routing strategy, and safety techniques like safe-completions and Preparedness Framework categorization.

Result: GPT-5 integrates a smart-fast main model with a deeper reasoning model, coordinated by a learning router, achieving better benchmark performance, faster responses, improved writing/coding/health assistance, and reduced hallucinations, instruction-failure, and sycophancy, while adding safety constraints on disallowed content.

Conclusion: GPT-5 is positioned as a more capable and useful successor to earlier models, with conservative safety treatment (e.g., classifying gpt-5-thinking as High capability in sensitive domains and activating safeguards) and a dynamic routing architecture to balance speed, depth of reasoning, and safety in real-world use.

Abstract: This is the system card published alongside the OpenAI GPT-5 launch, August 2025.
  GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say 'think hard about this' in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries.
  This system card focuses primarily on gpt-5-thinking and gpt-5-main, while evaluations for other models are available in the appendix. The GPT-5 system not only outperforms previous models on benchmarks and answers questions more quickly, but -- more importantly -- is more useful for real-world queries. We've made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5's performance in three of ChatGPT's most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.
  Similarly to ChatGPT agent, we have decided to treat gpt-5-thinking as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards. While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm -- our defined threshold for High capability -- we have chosen to take a precautionary approach.

</details>


### [5] [WRAVAL -- WRiting Assist eVALuation](https://arxiv.org/abs/2601.03268)
*Gabriel Benedict,Matthew Butler,Naved Merchant,Eetu Salama-Laine*

Main category: cs.CL

TL;DR: The paper argues that common reasoning-focused benchmarks underestimate the practical value of Small Language Models (SLMs), and introduces an evaluation framework tailored to non-reasoning, industrial tasks such as tone modification.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of language models is dominated by reasoning and problem-solving benchmarks, where Small Language Models perform much worse than Large Language Models. This creates a misleading picture of SLMs’ usefulness in real-world industrial applications, especially for edge and private deployments, where smaller models are preferred. The authors want to correct this imbalance by designing evaluations that capture SLMs’ strengths on common non-reasoning tasks.

Method: The authors design an evaluation framework for SLMs focused on non-reasoning tasks, particularly tone modification (e.g., funny, serious, professional). Because standard datasets do not exist for these tasks, they use a combination of (1) novel data generation strategies, (2) prompt-tuning to adapt models to specific tasks, and (3) LLM-based automatic evaluation of outputs. They then apply this framework to compare task-specific fine-tuned SLMs with LLMs.

Result: Using their framework, they show that SLMs, when appropriately fine-tuned for specific non-reasoning tasks, can perform much better than what traditional reasoning benchmarks suggest. In certain industrially relevant tasks like tone modification, SLMs can be competitive and practically effective compared to LLMs.

Conclusion: The paper concludes that reasoning-centric benchmarks are insufficient for judging the practical value of SLMs. Their proposed framework enables more realistic benchmarking of both SLMs and LLMs on non-reasoning industrial tasks, providing practitioners with tools to evaluate models for edge and private computing use cases. They release an open-source implementation to support adoption.

Abstract: The emergence of Large Language Models (LLMs) has shifted language model evaluation toward reasoning and problem-solving tasks as measures of general intelligence. Small Language Models (SLMs) -- defined here as models under 10B parameters -- typically score 3-4 times lower than LLMs on these metrics. However, we demonstrate that these evaluations fail to capture SLMs' effectiveness in common industrial applications, such as tone modification tasks (e.g., funny, serious, professional). We propose an evaluation framework specifically designed to highlight SLMs' capabilities in non-reasoning tasks where predefined evaluation datasets don't exist. Our framework combines novel approaches in data generation, prompt-tuning, and LLM-based evaluation to demonstrate the potential of task-specific finetuning. This work provides practitioners with tools to effectively benchmark both SLMs and LLMs for practical applications, particularly in edge and private computing scenarios. Our implementation is available at: https://github.com/amazon-science/wraval.

</details>


### [6] [The Instruction Gap: LLMs get lost in Following Instruction](https://arxiv.org/abs/2601.03269)
*Vishesh Tripathi,Uday Allu,Biddwan Ahmed*

Main category: cs.CL

TL;DR: The paper evaluates how well 13 major LLMs follow enterprise-style custom instructions in real-world RAG setups and finds large differences, with some top models performing best but revealing a general ‘instruction gap’.


<details>
  <summary>Details</summary>
Motivation: Enterprises need LLMs that reliably follow precise, custom instructions in complex, real-world workflows, especially in retrieval-augmented generation scenarios. Existing benchmarks focus on general capabilities but under-measure strict instruction adherence, which is critical for safe and effective deployment in business settings. The authors aim to fill this evaluation gap.

Method: The authors systematically evaluate 13 state-of-the-art LLMs in real-world RAG scenarios using enterprise-grade evaluation protocols. They test each model on standardized samples, measuring three aspects: (1) instruction compliance (how faithfully models follow detailed custom instructions), (2) response accuracy (correctness and relevance of outputs in the context of retrieved documents), and (3) performance metrics appropriate for enterprise use (likely latency, stability, etc.). They compare results across major model families, including Claude-Sonnet-4 and GPT-5.

Result: Instruction-following performance varies widely across the 13 evaluated models. Claude-Sonnet-4 and GPT-5 achieve the highest combined scores on instruction compliance and response accuracy. Many other models show strong general language abilities but inconsistent adherence to detailed instructions in RAG workflows. This variation is documented as quantitative benchmarks for each model family.

Conclusion: The study identifies an ‘instruction gap’: many strong LLMs excel at general language tasks yet fail to consistently follow precise enterprise instructions, limiting their reliability in production RAG systems. By benchmarking 13 major models, the work offers practical guidance for organizations choosing LLMs and highlights the need for improved training and evaluation methods focused on instruction-following fidelity in enterprise contexts.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in natural language understanding and generation, yet their deployment in enterprise environments reveals a critical limitation: inconsistent adherence to custom instructions. This study presents a comprehensive evaluation of 13 leading LLMs across instruction compliance, response accuracy, and performance metrics in realworld RAG (Retrieval-Augmented Generation) scenarios. Through systematic testing with samples and enterprise-grade evaluation protocols, we demonstrate that instruction following varies dramatically across models, with Claude-Sonnet-4 and GPT-5 achieving the highest results. Our findings reveal the "instruction gap" - a fundamental challenge where models excel at general tasks but struggle with precise instruction adherence required for enterprise deployment. This work provides practical insights for organizations deploying LLM-powered solutions and establishes benchmarks for instruction-following capabilities across major model families.

</details>


### [7] [Advances and Challenges in Semantic Textual Similarity: A Comprehensive Survey](https://arxiv.org/abs/2601.03270)
*Lokendra Kumar,Neelesh S. Upadhye,Kannan Piedy*

Main category: cs.CL

TL;DR: Survey of recent (post-2021) advances in Semantic Textual Similarity (STS), focusing on six methodological directions and their applications.


<details>
  <summary>Details</summary>
Motivation: STS has evolved quickly with new transformer and contrastive learning techniques, plus growing demand for domain-specific and multi-modal semantic understanding. Researchers and practitioners need a structured overview to understand what has been done, which methods work best, and where open problems remain.

Method: The paper conducts a literature survey, categorizing recent STS work into six areas: transformer-based models, contrastive learning approaches, domain-specific adaptations, multi-modal methods, graph-based approaches, and knowledge-enhanced techniques. It compares representative models such as FarSSiBERT, DeBERTa-v3, AspectCSE, CXR-BERT, and Financial-STS in terms of performance and design choices.

Result: The survey finds that advanced transformers and contrastive learning have significantly improved STS accuracy. Domain-adapted models show strong gains in specialized fields like medicine and finance. Multi-modal, graph-based, and knowledge-integrated approaches further increase semantic richness and robustness, though they are less mature and standardized than text-only transformers.

Conclusion: The paper concludes that STS is rapidly progressing, with transformer and contrastive methods currently leading performance, and domain-specific, multi-modal, and knowledge-based approaches opening new application opportunities. It emphasizes existing challenges and outlines emerging trends to guide future research and practical deployment of STS systems.

Abstract: Semantic Textual Similarity (STS) research has expanded rapidly since 2021, driven by advances in transformer architectures, contrastive learning, and domain-specific techniques. This survey reviews progress across six key areas: transformer-based models, contrastive learning, domain-focused solutions, multi-modal methods, graph-based approaches, and knowledge-enhanced techniques. Recent transformer models such as FarSSiBERT and DeBERTa-v3 have achieved remarkable accuracy, while contrastive methods like AspectCSE have established new benchmarks. Domain-adapted models, including CXR-BERT for medical texts and Financial-STS for finance, demonstrate how STS can be effectively customized for specialized fields. Moreover, multi-modal, graph-based, and knowledge-integrated models further enhance semantic understanding and representation. By organizing and analyzing these developments, the survey provides valuable insights into current methods, practical applications, and remaining challenges. It aims to guide researchers and practitioners alike in navigating rapid advancements, highlighting emerging trends and future opportunities in the evolving field of STS.

</details>


### [8] [Less is more: Not all samples are effective for evaluation](https://arxiv.org/abs/2601.03272)
*Wentang Song,Jinqiang Li,Kele Huang,Junhui Lin,Shengxiang Wu,Zhongshi Xie*

Main category: cs.CL

TL;DR: They propose a history-free framework to compress LLM evaluation benchmarks by detecting and removing semantically redundant test samples using task-adapted embeddings and clustering, cutting evaluation costs by >90% while preserving fidelity.


<details>
  <summary>Details</summary>
Motivation: Specialized LLM benchmarks in vertical domains are large, semantically redundant, and computationally expensive to evaluate. Existing compression methods rely on historical correctness labels from many models, making them unusable when a new task, domain, or model appears with no prior evaluation history. There is a need for a test set compression method that works in such cold-start scenarios without relying on model performance logs.

Method: 1) Fine-tune a base LLM on a small amount of domain-specific data so it internalizes task-relevant semantics. 2) Use the fine-tuned model to generate high-level semantic embeddings for all original test samples from their raw text. 3) In this domain-adapted embedding space, run task-aware clustering to group semantically similar items. 4) Apply a novel "dataset X-ray" mechanism that inspects cluster geometry (e.g., density, spread, redundancy) to automatically adjust how aggressively to compress, thus selecting a smaller, representative subset of test items while accounting for intrinsic redundancy.

Result: On professional-domain datasets, especially a large-scale 3GPP communications benchmark, the method identifies and removes redundant test samples, shrinking the benchmark and reducing LLM evaluation cost by more than 90%, while maintaining close agreement between results on the compressed subset and on the full benchmark.

Conclusion: A history-free, domain-adapted, embedding-and-clustering-based framework can significantly compress LLM evaluation benchmarks without needing historical correctness labels, enabling low-cost yet faithful evaluation in new domains and tasks. The approach scales to large professional datasets and preserves benchmark fidelity despite aggressive reduction of test items.

Abstract: The versatility of Large Language Models (LLMs) in vertical domains has spurred the development of numerous specialized evaluation benchmarks. However, these benchmarks often suffer from significant semantic redundancy and impose high computational costs during evaluation. Existing compression methods, such as tinyBenchmarks depend critically on correctness labels from multiple historical models evaluated on the full test set, making them inapplicable in cold-start scenarios, such as the introduction of a new task, domain, or model with no prior evaluation history.
  To address this limitation, we propose a history-free test set compression framework that requires no prior model performance data. Our method begins by fine-tuning a base LLM on a small amount of domain-specific data to internalize task-relevant semantics. It then generates high-level semantic embeddings for all original test samples using only their raw textual content. In this domain-adapted embedding space, we perform task-aware clustering and introduce a novel dataset X-ray mechanism that analyzes cluster geometry to dynamically calibrate the compression intensity based on the intrinsic redundancy of the benchmark.
  Experiments on professional-domain dataset, notably a large-scale 3GPP communications benchmark, demonstrate that our approach effectively identifies and removes redundant samples, reducing evaluation cost by over 90% while preserving high fidelity to the full benchmark.

</details>


### [9] [GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators](https://arxiv.org/abs/2601.03273)
*Naseem Machlovi,Maryam Saleki,Ruhul Amin,Mohamed Rahouti,Shawqi Al-Maliki,Junaid Qadir,Mohamed M. Abdallah,Ala Al-Fuqaha*

Main category: cs.CL

TL;DR: The paper introduces GuardEval, a fine-grained, multi-perspective safety benchmark and GemmaGuard (GGuard), a fine-tuned moderation model that significantly outperforms existing systems on nuanced harmful content detection.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based moderation tools struggle with nuanced and borderline safety cases—like implicit offensiveness, subtle gender and racial bias, and jailbreak prompts—and can reinforce societal biases due to over-reliance on training data that is not diverse or human-centered enough. There is a need for a unified benchmark and stronger models that can make more consistent, fair, and robust moderation decisions across many fine-grained safety categories.

Method: The authors construct GuardEval, a unified dataset with 106 fine-grained safety-related categories spanning emotions, offensive and hateful language, gender and racial bias, and general safety issues, to be used for both training and evaluation. They then apply QLoRA fine-tuning on Gemma3-12B using GuardEval to create GemmaGuard (GGuard), a content moderation model that outputs detailed, category-level safety assessments. They quantitatively evaluate GGuard against existing moderation models such as OpenAI Moderator and Llama Guard using macro F1 scores.

Result: GGuard achieves a macro F1 score of 0.832 on the GuardEval benchmark, significantly outperforming OpenAI Moderator (0.64) and Llama Guard (0.61), showing improved performance particularly on complex and borderline moderation cases.

Conclusion: Multi-perspective, human-centered safety benchmarks like GuardEval, combined with fine-tuned models like GGuard, can substantially reduce biased and inconsistent moderation outcomes. Diverse and representative training data materially improve safety, fairness, and robustness of LLM-based content moderation, especially for nuanced and borderline harmful content.

Abstract: As large language models (LLMs) become deeply embedded in daily life, the urgent need for safer moderation systems, distinguishing between naive from harmful requests while upholding appropriate censorship boundaries, has never been greater. While existing LLMs can detect harmful or unsafe content, they often struggle with nuanced cases such as implicit offensiveness, subtle gender and racial biases, and jailbreak prompts, due to the subjective and context-dependent nature of these issues. Furthermore, their heavy reliance on training data can reinforce societal biases, resulting in inconsistent and ethically problematic outputs. To address these challenges, we introduce GuardEval, a unified multi-perspective benchmark dataset designed for both training and evaluation, containing 106 fine-grained categories spanning human emotions, offensive and hateful language, gender and racial bias, and broader safety concerns. We also present GemmaGuard (GGuard), a QLoRA fine-tuned version of Gemma3-12B trained on GuardEval, to assess content moderation with fine-grained labels. Our evaluation shows that GGuard achieves a macro F1 score of 0.832, substantially outperforming leading moderation models, including OpenAI Moderator (0.64) and Llama Guard (0.61). We show that multi-perspective, human-centered safety benchmarks are critical for reducing biased and inconsistent moderation decisions. GuardEval and GGuard together demonstrate that diverse, representative data materially improve safety, fairness, and robustness on complex, borderline cases.

</details>


### [10] [LLM_annotate: A Python package for annotating and analyzing fiction characters](https://arxiv.org/abs/2601.03274)
*Hannes Rosenbusch*

Main category: cs.CL

TL;DR: LLM_annotate is a Python package that systematizes using large language models to analyze the personalities of fictional characters across full texts.


<details>
  <summary>Details</summary>
Motivation: Researchers studying fictional characters and personality often need to annotate behaviors and infer traits from long-form texts like books and scripts. Doing this manually is time-consuming, hard to scale, and difficult to reproduce. While LLMs can help, researchers currently lack a standardized, transparent workflow to apply LLMs for character analysis, manage long-context documents, handle character name ambiguity, and inspect or validate results interactively.

Method: The paper introduces LLM_annotate, a Python library that orchestrates LLM-based annotation pipelines for character analysis. It provides tools for splitting long texts into chunks, calling any chosen LLM to annotate character behaviors, resolving name disambiguation, scoring the quality of the annotations, and computing character-level statistics and embeddings. It also offers a human-in-the-loop graphical interface so researchers can inspect, correct, and validate the LLM outputs. Usage is demonstrated with worked examples on The Simpsons Movie and the novel Pride and Prejudice.

Result: The package successfully enables end-to-end, LLM-assisted analysis of fictional characters in long texts, including automated behavior annotation, trait inference, and computation of derived character metrics and embeddings. The examples show that users can flexibly plug in commercial, open-source, or custom LLMs and obtain structured, reproducible character analyses with integrated quality assessment and manual correction capabilities.

Conclusion: LLM_annotate provides a standardized, flexible, and reproducible framework for using LLMs to study fictional character personality at scale. By combining automation with human-in-the-loop validation and compatibility with a wide range of LLMs, the package lowers the barrier for researchers to perform detailed character analyses on long-form narrative texts.

Abstract: LLM_annotate is a Python package for analyzing the personality of fiction characters with large language models. It standardizes workflows for annotating character behaviors in full texts (e.g., books and movie scripts), inferring character traits, and validating annotation/inference quality via a human-in-the-loop GUI. The package includes functions for text chunking, LLM-based annotation, character name disambiguation, quality scoring, and computation of character-level statistics and embeddings. Researchers can use any LLM, commercial, open-source, or custom, within LLM_annotate. Through tutorial examples using The Simpsons Movie and the novel Pride and Prejudice, I demonstrate the usage of the package for efficient and reproducible character analyses.

</details>


### [11] [Topic Segmentation Using Generative Language Models](https://arxiv.org/abs/2601.03276)
*Pierre Mackenzie,Maya Shah,Patrick Frenett*

Main category: cs.CL

TL;DR: The paper explores using generative LLMs for topic segmentation and shows they can outperform traditional semantic-similarity methods when used with a specialized prompting strategy, while noting remaining reliability issues.


<details>
  <summary>Details</summary>
Motivation: Traditional topic segmentation methods rely on semantic similarity between adjacent sentences, which struggles with long-range dependencies and cannot leverage broad world knowledge. With the emergence of powerful LLMs, there is an opportunity to revisit topic segmentation using their generative and reasoning capabilities, but this has been underexplored.

Method: They design an overlapping and recursive prompting strategy for LLMs, where sentences are enumerated and fed to the model in overlapping chunks. The LLM is asked, via prompts, to identify topic boundaries, and the process recurses to refine boundary decisions. They also adopt and advocate for the boundary similarity metric as the evaluation measure for topic segmentation quality.

Result: Experiments show that, under the proposed prompting scheme, LLMs achieve better topic segmentation performance than existing semantic-similarity-based methods according to boundary similarity scores. However, the improvement is not yet consistent or robust enough to fully trust LLMs as drop-in replacements.

Conclusion: Generative LLMs, when guided with an overlapping and recursive prompting scheme and sentence enumeration, can be strong topic segmenters, outperforming traditional methods in many cases. Yet, they still exhibit limitations and reliability concerns, indicating that more research is needed before LLM-based topic segmentation can be considered mature and dependable for real-world applications.

Abstract: Topic segmentation using generative Large Language Models (LLMs) remains relatively unexplored. Previous methods use semantic similarity between sentences, but such models lack the long range dependencies and vast knowledge found in LLMs. In this work, we propose an overlapping and recursive prompting strategy using sentence enumeration. We also support the adoption of the boundary similarity evaluation metric. Results show that LLMs can be more effective segmenters than existing methods, but issues remain to be solved before they can be relied upon for topic segmentation.

</details>


### [12] [Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64](https://arxiv.org/abs/2601.03324)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: The paper proposes a highly optimized software runtime for running LLMs on Apple Silicon ARM64 by treating the CPU like a “virtual tensor core,” focusing on memory layout and data movement rather than just compute.


<details>
  <summary>Details</summary>
Motivation: Running LLMs on edge devices is limited by the memory wall: moving data from memory is slower than doing arithmetic. Standard inference runtimes add overhead via abstractions, dynamic dispatch, and suboptimal memory access. The authors want an open, deterministic, high-performance reference that exposes and mitigates the memory bottleneck on general-purpose ARM hardware without relying on proprietary accelerators.

Method: They design a “Virtual Tensor Core” software architecture optimized for ARM64 (Apple Silicon). Key techniques include: bypassing standard library containers and using direct mmap-based memory mapping; hand-written NEON SIMD kernels; a Software-Defined DMA approach to tightly control data movement; a Tensor Virtualization Layout (TVL) that ensures 100% cache-line utilization for weight matrices; and a zero-copy model loader that avoids initialization latency. The system targets stable, predictable throughput and cache efficiency rather than peak FLOPs.

Result: On a 110M parameter model running on Apple M2 hardware, the system reaches stable throughput above 60 tokens/second. It maintains full cache-line utilization of weight matrices and removes loader-induced startup latency. Although it does not match peak performance of proprietary accelerators like Apple AMX, it shows that careful software and memory-layout engineering can achieve competitive, stable performance on general-purpose ARM cores.

Conclusion: The work demonstrates that a software-only “Virtual Tensor Core” on ARM64, emphasizing memory layout and data movement control, can overcome part of the memory wall for LLM inference on edge devices. It offers an open, portable, and deterministic reference implementation that hits psycholinguistic response-time targets (~200 ms) without depending on opaque, proprietary hardware units, making it valuable both for practical deployment and for studying memory bottlenecks in LLM inference.

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" the bottleneck where data movement latency outstrips arithmetic throughput. Standard inference runtimes often incur significant overhead through high-level abstractions, dynamic dispatch, and unaligned memory access patterns. In this work, we present a novel "Virtual Tensor Core" architecture implemented in software, optimized specifically for ARM64 microarchitectures (Apple Silicon). By bypassing standard library containers in favor of direct memory mapping (mmap) and implementing hand-tuned NEON SIMD kernels, we achieve a form of "Software-Defined Direct Memory Access (DMA)." Our proposed Tensor Virtualization Layout (TVL) guarantees 100% cache line utilization for weight matrices, while our zero-copy loader eliminates initialization latency. Experimental results on a 110M parameter model demonstrate a stable throughput of >60 tokens/second on M2 hardware. While proprietary hardware accelerators (e.g., Apple AMX) can achieve higher peak throughput, our architecture provides a fully open, portable, and deterministic reference implementation for studying the memory bottleneck on general-purpose ARM silicon, meeting the 200ms psycholinguistic latency threshold without opaque dependencies.

</details>


### [13] [A path to natural language through tokenisation and transformers](https://arxiv.org/abs/2601.03368)
*David S. Berman,Alexander G. Stapleton*

Main category: cs.CL

TL;DR: The paper studies how Zipf-like statistics and entropy behave under BPE tokenisation and how this interacts with transformer language models.


<details>
  <summary>Details</summary>
Motivation: Natural language corpora obey Zipf’s and Heaps’ laws, but it is unclear how these statistical regularities relate to modern subword tokenisation (BPE) used in transformer LMs, and how BPE changes information-theoretic properties of text.

Method: (1) Theoretically model word frequencies with a Zipfian distribution and derive a closed-form expression for expected slot (token) entropy. (2) Empirically apply recursive BPE to corpora, track how token frequency distributions and empirical entropies evolve. (3) Train transformer language models on tokenisations with different BPE depths, compare model predictive entropies to Zipf-based predictions, and use attention diagnostics to study token dependency structure at each depth.

Result: Recursive BPE moves token frequency distributions toward a Zipfian power law and produces a characteristic growth pattern in empirical entropy. As BPE depth increases, model predictive entropies align more closely with the Zipf-derived theoretical expectations. Attention patterns show that deeper tokenisations weaken short-range token dependencies, making the data closer to a weakly dependent (near-IID) regime.

Conclusion: BPE is not just a compression tool; it statistically reshapes text so that token distributions become more Zipfian and more weakly dependent, and transformer models trained on such tokenisations exhibit predictive entropies matching Zipf-based theory. This clarifies the informational role of BPE in reconstructing key statistical properties of natural language at the token level.

Abstract: Natural languages exhibit striking regularities in their statistical structure, including notably the emergence of Zipf's and Heaps' laws. Despite this, it remains broadly unclear how these properties relate to the modern tokenisation schemes used in contemporary transformer models. In this note, we analyse the information content (as measured by the Shannon entropy) of various corpora under the assumption of a Zipfian frequency distribution, and derive a closed-form expression for the slot entropy expectation value. We then empirically investigate how byte--pair encoding (BPE) transforms corpus statistics, showing that recursive applications of BPE drive token frequencies toward a Zipfian power law while inducing a characteristic growth pattern in empirical entropy. Utilizing the ability of transformers to learn context dependent token probability distributions, we train language models on corpora tokenised at varying BPE depths, revealing that the model predictive entropies increasingly agree with Zipf-derived predictions as the BPE depth increases. Attention-based diagnostics further indicate that deeper tokenisation reduces local token dependencies, bringing the empirical distribution closer to the weakly dependent (near IID) regime. Together, these results clarify how BPE acts not only as a compression mechanism but also as a statistical transform that reconstructs key informational properties of natural language.

</details>


### [14] [Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models](https://arxiv.org/abs/2601.03388)
*Zhibo Hu,Chen Wang,Yanfeng Shu,Hye-young Paik,Liming Zhu*

Main category: cs.CL

TL;DR: The paper shows that metaphors in training data causally affect how LLMs develop and express misaligned reasoning across domains, and uses metaphor-related latent features to detect such misalignment.


<details>
  <summary>Details</summary>
Motivation: Previous work shows metaphors shape human decisions, and LLMs are trained on metaphor-rich text. Meanwhile, emergent misalignment—where models transfer harmful or misaligned reasoning patterns from one domain to another—is a growing concern. The authors are motivated to test whether metaphors in training data similarly shape LLMs’ reasoning pathways and contribute to cross-domain misalignment, and whether this effect can be understood and controlled.

Method: They empirically manipulate metaphors during different training stages—pre-training, fine-tuning, and re-alignment—and measure how this affects cross-domain misalignment in LLM reasoning. They analyze model internals to relate metaphor use to activation patterns of global and local latent features in large reasoning models. Based on these internal signals, they build a detector that monitors these latent features to predict when the model is producing misaligned content.

Result: They find a strong causal link between the presence and nature of metaphors in the training data and the observed degree of cross-domain misalignment in LLM reasoning: changing metaphor usage during various training phases significantly shifts misalignment levels. They also identify specific global and local latent features whose activation correlates with metaphor use and misaligned reasoning, and show that these can be used to predict misalignment with high accuracy.

Conclusion: Metaphors are not just stylistic artifacts in LLM training data; they play a causal role in shaping cross-domain misalignment in reasoning. By intervening on metaphor content during training and by monitoring metaphor-linked latent features, it is possible both to modulate misalignment and to accurately detect misaligned outputs. This suggests that controlling and auditing metaphor-related representations is a promising direction for improving LLM alignment.

Abstract: Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.

</details>


### [15] [Breaking the Assistant Mold: Modeling Behavioral Variation in LLM Based Procedural Character Generation](https://arxiv.org/abs/2601.03396)
*Maan Qraitem,Kate Saenko,Bryan A. Plummer*

Main category: cs.CL

TL;DR: The paper introduces PersonaWeaver, a framework for generating large-scale, diverse virtual characters by separating world-building attributes from behavioral traits, addressing alignment-induced biases in current character generators.


<details>
  <summary>Details</summary>
Motivation: Existing procedural content generation focuses on environments (levels, maps, quests), but character generation is limited and heavily influenced by alignment training, which makes characters uniformly moral and overly helpful. This reduces dramatic tension and leads to predictable, homogeneous interactions in virtual worlds, games, and simulations. The paper aims to overcome these limitations to enable richer narrative experiences.

Method: The authors first analyze and define two alignment-induced biases in current language-model-based character generation: positive moral bias and helpful assistant bias. They then propose PersonaWeaver, a framework that explicitly separates world-building characteristics (such as roles and demographics) from behavioral-building characteristics (such as moral stances and interaction styles). This disentangling allows controlled generation of characters with varied moral positions and interaction patterns, leading to first-order diversity (reactions, stances) and second-order diversity (stylistic features like length, tone, and punctuation).

Result: Using PersonaWeaver, the generated characters display a wider range of moral stances and more varied interactional behaviors compared with standard alignment-tuned models. Additionally, the characters exhibit diverse stylistic markers, improving realism and reducing homogeneity in generated dialogues and narratives. The code implementation is publicly released for reproducibility and further experimentation.

Conclusion: PersonaWeaver successfully mitigates alignment-induced biases in large language model character generation by disentangling world-building from behavioral-building features. This leads to more varied, less uniformly agreeable, and more dramatically engaging characters, supporting richer narrative and interactive experiences in virtual worlds and games.

Abstract: Procedural content generation has enabled vast virtual worlds through levels, maps, and quests, but large-scale character generation remains underexplored. We identify two alignment-induced biases in existing methods: a positive moral bias, where characters uniformly adopt agreeable stances (e.g. always saying lying is bad), and a helpful assistant bias, where characters invariably answer questions directly (e.g. never refusing or deflecting). While such tendencies suit instruction-following systems, they suppress dramatic tension and yield predictable characters, stemming from maximum likelihood training and assistant fine-tuning. To address this, we introduce PersonaWeaver, a framework that disentangles world-building (roles, demographics) from behavioral-building (moral stances, interactional styles), yielding characters with more diverse reactions and moral stances, as well as second-order diversity in stylistic markers like length, tone, and punctuation. Code: https://github.com/mqraitem/Persona-Weaver

</details>


### [16] [Rendering Data Unlearnable by Exploiting LLM Alignment Mechanisms](https://arxiv.org/abs/2601.03401)
*Ruihan Zhang,Jun Sun*

Main category: cs.CL

TL;DR: The paper introduces Disclaimer Injection, a data-level defense that makes text effectively unlearnable for large language models by exploiting their alignment mechanisms, degrading model performance when trained on protected data.


<details>
  <summary>Details</summary>
Motivation: As LLMs are trained on vast, mixed data sources, there is a high risk of unauthorized use of proprietary or personal data. Existing protections often require control over the model or training pipeline, or post-hoc data removal, which is impractical in many real-world black-box settings. There is a need for a practical technique that prevents models from effectively learning from sensitive data, even when the training process cannot be directly controlled.

Method: The authors propose Disclaimer Injection, where carefully crafted disclaimers are attached to or embedded within text data. These disclaimers are designed to trigger the model's alignment and safety mechanisms during training. The authors perform layer-wise analysis of LLMs fine-tuned on such data, examining how activation patterns, especially in alignment-related layers, change. They compare models fine-tuned on normal data versus data with injected disclaimers, evaluating downstream performance and learning behavior on both protected and common inputs.

Result: Fine-tuning LLMs on data augmented with alignment-triggering disclaimers leads to persistent activation of alignment-related layers. This causes alignment behavior to dominate over task-specific learning, resulting in substantial and systematic degradation of model performance compared to standard fine-tuning. The degradation is observed not just on the protected data but also on common inputs, showing that the protection affects the model’s overall ability to learn from that data.

Conclusion: Alignment behavior in LLMs can be exploited as a lever for data protection: by injecting specially designed disclaimers, sensitive text becomes effectively unlearnable in a black-box training scenario. This Disclaimer Injection approach is, to the authors’ knowledge, the first practical method that restricts data learnability at LLM scale without requiring any modification of the training pipeline or direct access to the model, offering a new avenue for safeguarding proprietary or personal data used in LLM training.

Abstract: Large language models (LLMs) are increasingly trained on massive, heterogeneous text corpora, raising serious concerns about the unauthorised use of proprietary or personal data during model training. In this work, we address the problem of data protection against unwanted model learning in a realistic black-box setting. We propose Disclaimer Injection, a novel data-level defence that renders text unlearnable to LLMs. Rather than relying on model-side controls or explicit data removal, our approach exploits the models' own alignment mechanisms: by injecting carefully designed alignment-triggering disclaimers to prevent effective learning. Through layer-wise analysis, we find that fine-tuning on such protected data induces persistent activation of alignment-related layers, causing alignment constraints to override task learning even on common inputs. Consequently, models trained on such data exhibit substantial and systematic performance degradation compared to standard fine-tuning. Our results identify alignment behaviour as a previously unexplored lever for data protection and, to our knowledge, present the first practical method for restricting data learnability at LLM scale without requiring access to or modification of the training pipeline.

</details>


### [17] [Tigrinya Number Verbalization: Rules, Algorithm, and Implementation](https://arxiv.org/abs/2601.03403)
*Fitsum Gaim,Issayas Tesfamariam*

Main category: cs.CL

TL;DR: The paper formalizes how to say cardinal and ordinal numbers in Tigrinya and provides an algorithm and open-source implementation, showing that current LLMs handle this task poorly.


<details>
  <summary>Details</summary>
Motivation: Tigrinya lacks computational resources for accurate number verbalization, which is needed for language technologies like speech synthesis and accessibility tools. Existing LLMs perform poorly on this task, so explicit, rule-based documentation is necessary.

Method: The authors systematically document canonical linguistic rules for expressing numerical values in spoken Tigrinya, including conjunctions, scale words, and special cases (dates, times, currency). They design a formal algorithm for number-to-word conversion and implement it as an open-source tool. They also empirically evaluate several frontier LLMs on Tigrinya number verbalization accuracy.

Result: They obtain a complete formalization and working implementation of Tigrinya cardinal and ordinal verbalization rules. The evaluation shows significant inaccuracies and inconsistencies in how frontier LLMs verbalize Tigrinya numbers compared to the rule-based system.

Conclusion: Rule-based, explicitly documented number verbalization is essential for Tigrinya, as current LLMs are not reliable for this task. Their resource and algorithm can support language modeling, speech technologies, and accessibility tools for Tigrinya-speaking users.

Abstract: We present a systematic formalization of Tigrinya cardinal and ordinal number verbalization, addressing a gap in computational resources for the language. This work documents the canonical rules governing the expression of numerical values in spoken Tigrinya, including the conjunction system, scale words, and special cases for dates, times, and currency. We provide a formal algorithm for number-to-word conversion and release an open-source implementation. Evaluation of frontier large language models (LLMs) reveals significant gaps in their ability to accurately verbalize Tigrinya numbers, underscoring the need for explicit rule documentation. This work serves language modeling, speech synthesis, and accessibility applications targeting Tigrinya-speaking communities.

</details>


### [18] [Implicit Graph, Explicit Retrieval: Towards Efficient and Interpretable Long-horizon Memory for Large Language Models](https://arxiv.org/abs/2601.03417)
*Xin Zhang,Kailai Yang,Hao Li,Chenyue Li,Qiyu Wei,Sophia Ananiadou*

Main category: cs.CL

TL;DR: LatentGraphMem is a hybrid memory framework for LLMs that stores information as a latent graph for stability and efficiency, while exposing small, explicit subgraphs for reasoning and inspection, leading to better long-horizon performance than prior explicit or latent memory systems.


<details>
  <summary>Details</summary>
Motivation: Long-horizon tasks require LLMs to reason over very long contexts where relevant evidence is sparse and scattered. Existing memory paradigms are limited: explicit structured memories (e.g., symbolic graphs) are interpretable but become brittle or overloaded as context length grows, while purely latent memory mechanisms (e.g., hidden states) scale and remain stable but are opaque and hard to inspect or control. The paper aims to design a memory system that preserves the robustness and efficiency of latent mechanisms while restoring explicit, interpretable structure for reasoning and supervision, especially under long-context conditions.

Method: The authors introduce LatentGraphMem, which represents memory as a graph in latent space. The full graph is never fully externalized; instead, a task-specific retrieval module selects a compact subgraph under a fixed budget, which is then surfaced as a symbolic structure for downstream reasoning or human inspection. Training is done by materializing an explicit graph view that can interface with a frozen reasoner and receive question-answering supervision. At inference, retrieval happens purely in latent space for efficiency, and only the retrieved subgraph is converted to an explicit form. The system is evaluated against both explicit-graph and latent-memory baselines across various model sizes on long-horizon benchmarks.

Result: Across multiple model scales and long-horizon benchmarks, LatentGraphMem consistently outperforms representative explicit-graph memory and latent-memory baselines. It also supports parameter-efficient adaptation and can flexibly scale to larger external reasoners without producing large symbolic Graph artifacts, suggesting better practicality for real-world long-context deployments.

Conclusion: LatentGraphMem successfully bridges the gap between explicit and latent memory approaches for long-horizon LLM applications. By keeping the global memory in latent graph form and exposing only small, task-specific subgraphs, it achieves both interpretability and robustness under long-context overload. The framework yields superior performance, efficient parameter usage, and scalable integration with stronger reasoners, indicating a promising direction for memory-augmented LLM design in settings with sparse, dispersed evidence over long contexts.

Abstract: Long-horizon applications increasingly require large language models (LLMs) to answer queries when relevant evidence is sparse and dispersed across very long contexts. Existing memory systems largely follow two paradigms: explicit structured memories offer interpretability but often become brittle under long-context overload, while latent memory mechanisms are efficient and stable yet difficult to inspect. We propose LatentGraphMem, a memory framework that combines implicit graph memory with explicit subgraph retrieval. LatentGraphMem stores a graph-structured memory in latent space for stability and efficiency, and exposes a task-specific subgraph retrieval interface that returns a compact symbolic subgraph under a fixed budget for downstream reasoning and human inspection. During training, an explicit graph view is materialized to interface with a frozen reasoner for question-answering supervision. At inference time, retrieval is performed in latent space and only the retrieved subgraph is externalized. Experiments on long-horizon benchmarks across multiple model scales show that LatentGraphMem consistently outperforms representative explicit-graph and latent-memory baselines, while enabling parameter-efficient adaptation and flexible scaling to larger reasoners without introducing large symbolic artifacts.

</details>


### [19] [PCoA: A New Benchmark for Medical Aspect-Based Summarization With Phrase-Level Context Attribution](https://arxiv.org/abs/2601.03418)
*Bohao Chu,Sameh Frihat,Tabea M. G. Pakull,Hendrik Damm,Meijie Li,Ula Muhabbek,Georg Lodde,Norbert Fuhr*

Main category: cs.CL

TL;DR: The paper introduces PCoA, a medical summarization benchmark with phrase-level attribution and a decoupled evaluation framework for summaries, citations, and contributory phrases.


<details>
  <summary>Details</summary>
Motivation: System-generated summaries, especially in high-stakes medical domains, are difficult to verify because they often lack precise, fine-grained attribution to the source text. Existing benchmarks and evaluation methods do not adequately capture whether each part of a summary is properly supported by the original context, which is critical for safety and trust. The authors aim to fill this gap with a benchmark and evaluation setup that focus explicitly on phrase-level attribution and aspect-based medical summarization.

Method: The authors construct PCoA, an expert-annotated dataset for medical aspect-based summarization. For each aspect-based summary, domain experts align the summary with its supporting contextual sentences and further annotate contributory phrases within those sentences. On top of this dataset, they design a decoupled evaluation framework that separately scores (1) the content quality of generated summaries, (2) the correctness of sentence-level citations, and (3) the correctness and coverage of contributory phrases. They then run extensive experiments, benchmarking several large language models under this framework and performing analyses to validate annotation quality and consistency.

Result: Experiments show that the PCoA annotations are of high quality and consistent across annotators. Benchmarking results indicate that existing large language models vary in their ability to produce accurate summaries and attributions when evaluated separately on summaries, citations, and contributory phrases. The study further finds that a pipeline that first explicitly identifies relevant sentences and contributory phrases before composing the summary yields better overall performance than directly summarizing without this intermediate step.

Conclusion: PCoA serves as a reliable benchmark for evaluating medical aspect-based summaries with phrase-level context attribution. The proposed decoupled evaluation framework enables more nuanced assessment of summarization systems, going beyond holistic summary quality to include fine-grained attribution. The paper concludes that incorporating explicit intermediate steps of identifying relevant sentences and contributory phrases can enhance the fidelity and quality of system-generated medical summaries, providing a promising direction for safer summarization in high-stakes domains.

Abstract: Verifying system-generated summaries remains challenging, as effective verification requires precise attribution to the source context, which is especially crucial in high-stakes medical domains. To address this challenge, we introduce PCoA, an expert-annotated benchmark for medical aspect-based summarization with phrase-level context attribution. PCoA aligns each aspect-based summary with its supporting contextual sentences and contributory phrases within them. We further propose a fine-grained, decoupled evaluation framework that independently assesses the quality of generated summaries, citations, and contributory phrases. Through extensive experiments, we validate the quality and consistency of the PCoA dataset and benchmark several large language models on the proposed task. Experimental results demonstrate that PCoA provides a reliable benchmark for evaluating system-generated summaries with phrase-level context attribution. Furthermore, comparative experiments show that explicitly identifying relevant sentences and contributory phrases before summarization can improve overall quality. The data and code are available at https://github.com/chubohao/PCoA.

</details>


### [20] [Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models](https://arxiv.org/abs/2601.03423)
*Sasha Ronaghi,Chloe Stanwyck,Asad Aali,Amir Ronaghi,Miguel Fuentes,Tina Hernandez-Boussard,Emily Alsentzer*

Main category: cs.CL

TL;DR: The paper introduces CAPT, a training-free ensembling method that adapts cutting-edge general language models to clinical tasks by leveraging existing clinical models, improving performance on multiple clinical tasks without retraining.


<details>
  <summary>Details</summary>
Motivation: Adapting each new generation of general language models to the clinical domain usually requires expensive continued pretraining or fine-tuning on clinical data. This is inefficient and often impractical, especially as model sizes and update frequency increase. The authors want a way to reuse existing clinical models to quickly exploit the strengths of newer general-domain models without retraining them.

Method: They propose Cross-Architecture Proxy Tuning (CAPT), an ensembling approach that combines a state-of-the-art general-domain model with an older clinical-domain model, even if their token vocabularies differ. CAPT uses contrastive decoding: it selectively incorporates clinically relevant signals from the clinical model while relying on the general model for general reasoning and fluent language generation. This effectively injects domain-specific knowledge at decoding time rather than through retraining.

Result: On six clinical classification and generation tasks, CAPT using a new general-domain model plus an older clinical model consistently outperforms either model alone and surpasses other ensembling methods, achieving on average a 17.6% improvement over UniTE and 41.4% over prior proxy tuning methods across tasks. Analyses show that CAPT enhances clinically actionable content, reduces contextual mistakes, and makes outputs more clinically specific.

Conclusion: CAPT provides an efficient, training-free way to adapt new general-domain language models to the clinical domain by leveraging existing clinical models. It handles differing vocabularies and improves performance and clinical quality across diverse tasks, suggesting a practical strategy for keeping clinical NLP systems current as new general models are released without repeated costly retraining.

Abstract: Adapting language models to the clinical domain through continued pretraining and fine-tuning requires costly retraining for each new model generation. We propose Cross-Architecture Proxy Tuning (CAPT), a model-ensembling approach that enables training-free adaptation of state-of-the-art general-domain models using existing clinical models. CAPT supports models with disjoint vocabularies, leveraging contrastive decoding to selectively inject clinically relevant signals while preserving the general-domain model's reasoning and fluency. On six clinical classification and text-generation tasks, CAPT with a new-generation general-domain model and an older-generation clinical model consistently outperforms both models individually and state-of-the-art ensembling approaches (average +17.6% over UniTE, +41.4% over proxy tuning across tasks). Through token-level analysis and physician case studies, we demonstrate that CAPT amplifies clinically actionable language, reduces context errors, and increases clinical specificity.

</details>


### [21] [The Critical Role of Aspects in Measuring Document Similarity](https://arxiv.org/abs/2601.03435)
*Eftekhar Hossain,Tarnika Hazra,Ahatesham Bhuiyan,Santu Karmaker*

Main category: cs.CL

TL;DR: ASPECTSIM is a framework for document similarity that conditions on explicit aspects rather than holistic similarity and shows much higher alignment with human judgments, especially when using GPT-4o, while smaller open-source LLMs still lag even with refinement.


<details>
  <summary>Details</summary>
Motivation: Traditional document similarity methods evaluate texts holistically, ignoring that humans often judge similarity with respect to specific aspects (e.g., topic, style, sentiment). This gap leads to lower human-machine agreement and limits the usefulness of similarity measures in nuanced applications. The paper aims to address this by explicitly conditioning similarity on user-specified aspects and by systematically evaluating how well different models capture such aspect-conditioned similarity.

Method: The authors propose ASPECTSIM, a framework where document similarity is computed with respect to an explicitly specified aspect rather than in a holistic manner. They construct a benchmark consisting of 26K aspect-document pairs annotated for aspect-conditioned similarity. They first implement ASPECTSIM using direct prompting of GPT-4o and compare its human-machine agreement against traditional holistic similarity judgments. Then they perform a large-scale meta-evaluation involving 16 smaller open-source LLMs and 9 embedding models. They test direct prompting for aspect-conditioned similarity and also introduce a simple two-stage refinement procedure to improve performance.

Result: GPT-4o-based ASPECTSIM achieves about 80% higher human-machine agreement compared to holistic similarity without explicit aspects, suggesting that aspect conditioning substantially improves alignment with human judgments. For the smaller open-source LLMs and embedding models, direct prompting to obtain ASPECTSIM scores yields relatively poor human-machine agreement (20–30%). Applying a simple two-stage refinement process boosts their agreement by about 140%, but even after this improvement, their agreement levels remain significantly below those of GPT-4o-based ASPECTSIM.

Conclusion: Explicitly specifying aspects when measuring document similarity yields much higher agreement with human judgments than traditional holistic similarity approaches, implying that current standard practice should be revised to incorporate aspect conditioning. GPT-4o is particularly effective at capturing aspect-conditioned similarity, whereas smaller open-source LLMs, even with refinement strategies, still lag behind, highlighting a performance gap between large proprietary and smaller open models for this task and motivating further research into improving open-source systems for aspect-based similarity.

Abstract: We introduce ASPECTSIM, a simple and interpretable framework that requires conditioning document similarity on an explicitly specified aspect, which is different from the traditional holistic approach in measuring document similarity. Experimenting with a newly constructed benchmark of 26K aspect-document pairs, we found that ASPECTSIM, when implemented with direct GPT-4o prompting, achieves substantially higher human-machine agreement ($\approx$80% higher) than the same for holistic similarity without explicit aspects. These findings underscore the importance of explicitly accounting for aspects when measuring document similarity and highlight the need to revise standard practice. Next, we conducted a large-scale meta-evaluation using 16 smaller open-source LLMs and 9 embedding models with a focus on making ASPECTSIM accessible and reproducible. While directly prompting LLMs to produce ASPECTSIM scores turned out be ineffective (20-30% human-machine agreement), a simple two-stage refinement improved their agreement by $\approx$140%. Nevertheless, agreement remains well below that of GPT-4o-based models, indicating that smaller open-source LLMs still lag behind large proprietary models in capturing aspect-conditioned similarity.

</details>


### [22] [Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale](https://arxiv.org/abs/2601.03444)
*Weiyue Li,Minda Zhao,Weixuan Dong,Jiahui Cai,Yuze Wei,Michael Pocress,Yi Li,Wanyan Yuan,Xiaoyue Wang,Ruoyu Hou,Kaiyuan Lou,Wenqi Zeng,Yutong Yang,Yilun Du,Mengyu Wang*

Main category: cs.CL

TL;DR: The paper examines how different rating scales affect the reliability and human-alignment of large language models when used as automated judges across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: While LLMs are widely used as automatic evaluators, prior work shows their scores can change when prompts are rephrased. However, the specific impact of the *grading scale* (e.g., 0–1 vs 0–5 vs 1–10) on LLM reliability and alignment with humans has not been systematically studied. The authors aim to fill this gap and provide guidance for designing robust LLM-as-a-judge protocols.

Method: The authors compare humans and LLMs as raters on three different rating scales across six benchmarks that span objective tasks, open-ended subjective tasks, and mixed tasks. They compute intraclass correlation coefficients (ICC) to quantify absolute agreement within and between rater groups, examining both consistency across scales and human–LLM alignment. They also analyze subgroup differences (e.g., across gender groups) and study how pooling results can obscure benchmark-level heterogeneity.

Result: They find that LLM ratings are not perfectly consistent across scales on subjective benchmarks, and that simply changing the rating scale can substantially affect the level of agreement between humans and LLMs—even when each group is internally reliable. When aggregating across tasks, a 0–5 scale produces the highest human–LLM alignment. They also show that pooled reliability metrics can hide important differences between benchmarks and expose systematic subgroup differences in alignment across gender groups.

Conclusion: The design of the grading scale is a critical factor in LLM-as-a-judge settings: it influences LLM consistency, human–LLM agreement, and observed subgroup alignment. A 0–5 scale appears preferable on average, but benchmark-specific and subgroup-specific diagnostics are necessary. Robust LLM-evaluation protocols should therefore explicitly consider scale choice and incorporate fine-grained reliability analyses rather than relying on pooled metrics alone.

Abstract: Large language models (LLMs) are increasingly used as automated evaluators, yet prior works demonstrate that these LLM judges often lack consistency in scoring when the prompt is altered. However, the effect of the grading scale itself remains underexplored. We study the LLM-as-a-judge problem by comparing two kinds of raters: humans and LLMs. We collect ratings from both groups on three scales and across six benchmarks that include objective, open-ended subjective, and mixed tasks. Using intraclass correlation coefficients (ICC) to measure absolute agreement, we find that LLM judgments are not perfectly consistent across scales on subjective benchmarks, and that the choice of scale substantially shifts human-LLM agreement, even when within-group panel reliability is high. Aggregated over tasks, the grading scale of 0-5 yields the strongest human-LLM alignment. We further demonstrate that pooled reliability can mask benchmark heterogeneity and reveal systematic subgroup differences in alignment across gender groups, strengthening the importance of scale design and sub-level diagnostics as essential components of LLM-as-a-judge protocols.

</details>


### [23] [Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks](https://arxiv.org/abs/2601.03448)
*Atsuki Yamaguchi,Maggie Mi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: L2T augments language model pre-training with explicit language learning tasks derived from raw text to improve linguistic competence without hurting general reasoning.


<details>
  <summary>Details</summary>
Motivation: Standard LM pre-training via next-token prediction focuses on sequence continuation and incidental world knowledge, but does not directly target fine-grained linguistic competence (syntax, morphology, etc.). The authors want to make LMs better at such linguistic skills and to acquire them faster, while preserving their general reasoning ability.

Method: They introduce L2T, a pre-training framework that automatically converts raw text into structured input-output pairs representing language learning tasks, inspired by how humans receive explicit linguistic stimulation. The LM is pre-trained on a mixture of standard next-token prediction data and these L2T tasks, effectively multitask training on both raw text and structured linguistic objectives.

Result: Models trained with L2T show improved scores on linguistic competence benchmarks and achieve these linguistic capabilities earlier in training compared to baselines, while retaining competitive performance on general reasoning tasks.

Conclusion: Integrating explicit language learning tasks into LM pre-training is an effective way to enhance and speed up the acquisition of linguistic competence, without sacrificing broader reasoning performance, suggesting that LM pre-training should combine raw-text prediction with structured linguistic supervision.

Abstract: Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.

</details>


### [24] [Prompting Underestimates LLM Capability for Time Series Classification](https://arxiv.org/abs/2601.03464)
*Dan Schumacher,Erfan Nourbakhsh,Rocky Slavin,Anthony Rios*

Main category: cs.CL

TL;DR: The paper shows that large language models actually contain useful time series representations, even though prompt-based evaluations make them look bad at time series classification.


<details>
  <summary>Details</summary>
Motivation: Recent prompt-based evaluations suggest that large language models perform near chance on time series classification, implying they might not encode temporal structure. The authors want to test whether this apparent weakness is due to true representational limits of LLMs or merely to the limitations of prompt-based generation as an evaluation method.

Method: They directly compare two ways of using the same internal LLM representations: (1) conventional zero-shot prompting to generate class labels, and (2) training simple linear probes on frozen internal representations to perform time series classification. They run layer-wise analyses to see where in the transformer stack class-discriminative information appears, and they also examine how visual and multimodal inputs affect the emergence of time series information.

Result: Zero-shot prompting yields near-chance performance on time series classification with F1 scores around 0.15–0.26. In contrast, linear probes trained on the same internal representations achieve much higher F1 scores of 0.61–0.67, on par with or better than specialized time series models. Layer-wise analysis shows that discriminative time series information arises in early transformer layers and becomes stronger when visual and multimodal inputs are provided.

Conclusion: Prompt-based evaluation severely underestimates LLMs’ understanding of time series because it probes only their generative behavior, not the underlying representations. LLMs do encode meaningful, class-discriminative temporal structure, especially in early layers and when aided by visual or multimodal inputs. There is a systematic mismatch between what LLMs represent internally and what is revealed by current prompt-based tests.

Abstract: Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model's representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early transformer layers and is amplified by visual and multimodal inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.

</details>


### [25] [EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning](https://arxiv.org/abs/2601.03471)
*Mingyang Wei,Dehai Min,Zewen Liu,Yuzhang Xie,Guanchen Wu,Carl Yang,Max S. Y. Lau,Qi He,Lu Cheng,Wei Jin*

Main category: cs.CL

TL;DR: The paper introduces EpiQAL, a new benchmark to test how well language models can answer epidemiology-focused questions using evidence from research papers.


<details>
  <summary>Details</summary>
Motivation: Current medical QA benchmarks mainly test clinical or patient-level reasoning and do not adequately assess population-level, evidence-based epidemiological reasoning. There is a need for a systematic way to evaluate how well models can infer disease burden, transmission, and intervention effects from the literature.

Method: The authors build EpiQAL, a benchmark consisting of three subsets created from open-access epidemiological literature. These subsets target: (1) text-grounded factual recall, (2) multi-step inference that connects document evidence with epidemiological principles, and (3) reconstruction of paper conclusions when the Discussion section is hidden. Construction uses an expert-designed taxonomy, multi-model verification for quality control, and retrieval-based methods to control question difficulty. They then evaluate ten open LLMs on these subsets, with and without Chain-of-Thought prompting.

Result: Current open LLMs perform poorly on epidemiological reasoning tasks in EpiQAL, especially on multi-step inference. Performance rankings differ across the three subsets, and simply increasing model size does not consistently improve results. Chain-of-Thought prompting helps with multi-step inference but has inconsistent benefits on other tasks.

Conclusion: EpiQAL is the first diagnostic benchmark specifically targeting epidemiological question answering and reveals clear limitations of current LLMs in evidence-grounded epidemiological reasoning. It offers fine-grained signals to analyze model abilities in evidence grounding, complex inference, and conclusion reconstruction, and can guide future work on models better suited for epidemiological analysis.

Abstract: Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.

</details>


### [26] [SegNSP: Revisiting Next Sentence Prediction for Linear Text Segmentation](https://arxiv.org/abs/2601.03474)
*José Isidro,Filipe Cunha,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: The paper proposes SegNSP, a linear text segmentation method that reframes segmentation as a next sentence prediction (NSP) task, achieving strong performance on two benchmarks without needing explicit topic labels.


<details>
  <summary>Details</summary>
Motivation: Linear text segmentation is important for many NLP applications, but remains difficult due to ambiguous topic boundaries, diverse discourse structures, and the need to balance local and global coherence. Existing methods often require task-specific supervision or explicit topic labels and may not robustly capture sentence-to-sentence continuity. The authors aim to exploit the inherent strength of NSP in modeling continuity to improve segmentation quality without heavy supervision.

Method: The authors frame linear text segmentation as a label-agnostic next sentence prediction (NSP) task, where the model predicts whether the next sentence continues the current topic. They introduce a segmentation-aware loss that emphasizes learning from likely boundary regions and combine it with harder negative sampling to better capture discourse continuity. Their approach avoids auxiliary topic labels or task-specific supervision, focusing solely on sentence-to-sentence continuity signals to infer topic boundaries.

Result: The proposed SegNSP model is evaluated on two datasets: CitiLink-Minutes, where they establish the first segmentation benchmark, and WikiSection. On CitiLink-Minutes, SegNSP attains a B-F1 score of 0.79, closely matching human-annotated topic transitions. On WikiSection, it achieves a B-F1 score of 0.65, outperforming the strongest reproducible baseline, TopSeg, by 0.17 absolute points. These results show that SegNSP performs competitively and robustly compared to existing methods.

Conclusion: Modeling linear text segmentation as a label-agnostic NSP task, enhanced by segmentation-aware loss and hard negative sampling, effectively captures sentence-to-sentence continuity and topic boundaries. The approach avoids reliance on explicit topic labels or auxiliary supervision, yet achieves strong, sometimes state-of-the-art, results across datasets. This demonstrates that revisiting NSP in a targeted way can improve segmentation quality and benefit downstream NLP applications that rely on coherent text units.

Abstract: Linear text segmentation is a long-standing problem in natural language processing (NLP), focused on dividing continuous text into coherent and semantically meaningful units. Despite its importance, the task remains challenging due to the complexity of defining topic boundaries, the variability in discourse structure, and the need to balance local coherence with global context. These difficulties hinder downstream applications such as summarization, information retrieval, and question answering. In this work, we introduce SegNSP, framing linear text segmentation as a next sentence prediction (NSP) task. Although NSP has largely been abandoned in modern pre-training, its explicit modeling of sentence-to-sentence continuity makes it a natural fit for detecting topic boundaries. We propose a label-agnostic NSP approach, which predicts whether the next sentence continues the current topic without requiring explicit topic labels, and enhance it with a segmentation-aware loss combined with harder negative sampling to better capture discourse continuity. Unlike recent proposals that leverage NSP alongside auxiliary topic classification, our approach avoids task-specific supervision. We evaluate our model against established baselines on two datasets, CitiLink-Minutes, for which we establish the first segmentation benchmark, and WikiSection. On CitiLink-Minutes, SegNSP achieves a B-$F_1$ of 0.79, closely aligning with human-annotated topic transitions, while on WikiSection it attains a B-F$_1$ of 0.65, outperforming the strongest reproducible baseline, TopSeg, by 0.17 absolute points. These results demonstrate competitive and robust performance, highlighting the effectiveness of modeling sentence-to-sentence continuity for improving segmentation quality and supporting downstream NLP applications.

</details>


### [27] [CALM: Culturally Self-Aware Language Models](https://arxiv.org/abs/2601.03483)
*Lingzhi Shen,Xiaohao Cai,Yunfei Long,Imran Razzak,Guanming Chen,Shoaib Jameel*

Main category: cs.CL

TL;DR: The paper proposes CALM, a framework that gives language models dynamic cultural self-awareness, improving performance on cross-cultural tasks.


<details>
  <summary>Details</summary>
Motivation: Existing language-model approaches treat culture as fixed background knowledge, ignoring how cultural norms and signals are dynamic and context-dependent. This static view harms performance and reliability in downstream tasks that require nuanced cultural sensitivity and adaptation over time. The authors aim to make models more culturally aware in a way that can update and self-correct as they encounter new cultural contexts.

Method: The authors propose CALM, which first disentangles task semantics from both explicit cultural concepts and latent cultural signals. It uses contrastive learning to structure these cultural elements into cultural clusters. These clusters are aligned via cross-attention to model fine-grained interactions among related cultural features. A Mixture-of-Experts (MoE) mechanism then adaptively integrates information along culture-specific dimensions. The resulting unified cultural representation is fused with the base model’s knowledge to form an internal, culturally grounded identity state. Finally, self-prompted reflective learning is used to continually refine this state through adaptation and self-correction.

Result: On multiple cross-cultural benchmark datasets, CALM yields consistent improvements over existing state-of-the-art methods, indicating better handling of diverse cultural contexts and more reliable cultural sensitivity in downstream tasks.

Conclusion: By modeling culture as dynamic and disentangled from core task semantics, CALM equips language models with a structured internal cultural identity that can evolve over time. This leads to more culturally aware and robust behavior across cross-cultural benchmarks, suggesting that dynamic cultural self-awareness is an effective direction for improving language models in culturally sensitive applications.

Abstract: Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model's original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Extensive experiments conducted on multiple cross-cultural benchmark datasets demonstrate that CALM consistently outperforms state-of-the-art methods.

</details>


### [28] [Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning](https://arxiv.org/abs/2601.03505)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Farinaz Koushanfar*

Main category: cs.CL

TL;DR: The paper introduces KR-Test, a lightweight evaluation framework to measure genuine knowledge retention in LLMs during supervised fine-tuning, separating factual learning from stylistic mimicry.


<details>
  <summary>Details</summary>
Motivation: Traditional validation metrics like perplexity fail to distinguish whether fine-tuned LLMs truly internalize domain knowledge or merely imitate surface linguistic patterns. There is a need for a more precise, corpus-grounded method to track factual learning during training, especially under common setups like SFT and LoRA.

Method: The authors propose the Knowledge Retention (KR) Test, which automatically generates contrastive examples from a corpus, each offering correct and incorrect continuations. The model’s likelihood scores for these continuations are compared to quantify its preference for factual knowledge. The test is non-generative (no decoding needed), does not require instruction tuning, and is used to monitor training dynamics, including with LoRA fine-tuning. A “blind vs. oracle” baseline is used to validate the test’s soundness.

Result: KR-Test reliably distinguishes between linguistic convergence and actual knowledge retention. The blind vs. oracle analysis supports the validity of the framework, and experiments on LoRA fine-tuning show nuanced, fine-grained training dynamics that perplexity alone cannot reveal.

Conclusion: KR-Test improves interpretability of supervised fine-tuning by providing a targeted, corpus-grounded measure of factual knowledge retention that is decoupled from mere stylistic learning. This enables more accurate monitoring and diagnosis of LLM training, particularly under parameter-efficient methods like LoRA.

Abstract: Supervised Fine-Tuning (SFT) is a standard approach for injecting domain knowledge into Large Language Models (LLMs). However, relying on validation perplexity to monitor training is often insufficient, as it confounds stylistic mimicry with genuine factual internalization. To address this, we introduce the Knowledge Retention (KR) Test , a lightweight, corpus-grounded evaluation framework designed to distinguish factual learning from linguistics. KR-Test utilizes automatically generated contrastive examples to measure likelihood preferences for correct versus incorrect continuations, requiring no instruction tuning or generative decoding. We validate the framework's integrity through a "blind vs. oracle" baseline analysis. Furthermore, we demonstrate the diagnostic capabilities of KR-Test by analyzing the training dynamics of Low-Rank Adaptation (LoRA). By exposing the fine-grained dissociation between linguistic convergence and knowledge retention, KR-Test enhances the interpretability of fine-tuning dynamics.

</details>


### [29] [Reasoning Pattern Alignment Merging for Adaptive Reasoning](https://arxiv.org/abs/2601.03506)
*Zhaofeng Zhong,Wei Yuan,Tong Chen,Xiangyu Zhao,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.CL

TL;DR: They propose RPAM, a model-merging framework that combines a long chain-of-thought LRM and a short-CoT model to get adaptive, efficient reasoning without retraining, maintaining strong performance while cutting inference cost.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models are powerful at complex reasoning but always produce long reasoning traces, causing high computation and latency. Existing methods to speed them up need retraining or delicate prompting, which is expensive and brittle. The authors want a lightweight, robust way to get query-adaptive reasoning length without training from scratch or using large extra datasets.

Method: They introduce Reasoning Pattern Alignment Merging (RPAM), which merges a Long-CoT reasoning model with a Short-CoT instruction model in a layer-wise manner. First, they build a small calibration set where each query is labeled with its appropriate reasoning pattern (long vs short). Then, for each layer, they learn merging coefficients so that the merged model’s intermediate representations match those of the model whose pattern is selected for that query, while a contrastive term pushes them away from the non-selected model’s representations. This alignment-based optimization yields a single merged model that can implicitly adapt its reasoning behavior to the query.

Result: On seven standard reasoning benchmarks, the merged model using RPAM achieves much lower inference cost (fewer tokens / shorter reasoning and thus lower latency) while keeping performance close to, or competitively strong with, the original long-CoT model. This demonstrates that representation-alignment-based merging can offer an efficient alternative to retraining or complex prompting for LRMs.

Conclusion: Model merging via RPAM can turn a pair of Long-CoT and Short-CoT models into a single adaptive reasoner that is more efficient yet still accurate, and does so without training from scratch or requiring large-scale extra data. This suggests that representation-level alignment and contrastive objectives are a practical path toward controllable, query-adaptive reasoning patterns in LRMs; the released code will allow reproduction and further exploration.

Abstract: Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model's intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.

</details>


### [30] [IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation](https://arxiv.org/abs/2601.03511)
*Hossein Hosseini Kasnavieh,Gholamreza Haffari,Chris Leckie,Adel N. Toosi*

Main category: cs.CL

TL;DR: IntroLM lets a language model estimate the quality of its own answers during the prefilling phase using special introspective tokens, avoiding external evaluators.


<details>
  <summary>Details</summary>
Motivation: Operating LLMs efficiently requires knowing in advance whether a model will answer a query well, to choose routing or fallback strategies. Existing quality predictors rely on separate models like BERT/DeBERTa, which have limited context windows, restricted representation power compared to the LLM itself, and add inference overhead. A method that lets the LLM self-assess its output quality without extra models or latency overhead is therefore needed.

Method: The authors introduce IntroLM, which augments a causal LLM with introspective tokens processed during the prefilling phase. They use token-conditional LoRA adapters that are activated only when an introspective token is present, so the backbone’s normal generation behavior is preserved. The model is trained so that, when given a query and the introspective token, the activated LoRA layers predict the expected quality (e.g., success probability) of the model’s answer to that query, without modifying the underlying generative parameters.

Result: On question-answering benchmarks, applying IntroLM to Qwen3 8B achieves a ROC-AUC of about 90% for predicting whether the model will successfully answer a query, beating a strong DeBERTa-based classifier by roughly 14 percentage points. When used in multi-model routing systems, IntroLM’s self-quality estimates enable better routing decisions, leading to up to 33% lower latency and up to 50% reduction in large-model usage at the same reliability level.

Conclusion: IntroLM demonstrates that causal LLMs can be equipped with lightweight, token-conditional adapters to introspect on and predict their own output quality without external evaluators or degradation of generation behavior. This self-evaluation improves success prediction, and integrating it into routing frameworks yields better cost–latency–reliability tradeoffs, indicating a promising direction for scalable and efficient LLM deployment.

Abstract: A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.

</details>


### [31] [Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents](https://arxiv.org/abs/2601.03515)
*Yuanchen Bei,Tianxin Wei,Xuying Ning,Yanjun Zhao,Zhining Liu,Xiao Lin,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: Mem-Gallery is a benchmark to test how multimodal LLM agents store, organize, and use long-term conversational memory across image–text dialogues.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks either focus on long-term memory in text-only chats or on short-range multimodal understanding, so they cannot evaluate how agents maintain and use multimodal memories over many dialogue sessions.

Method: The authors build Mem-Gallery, a dataset of multi-session conversations that involve both images and text with long interaction horizons and cross-session dependencies. On top of this, they design an evaluation framework along three axes: (1) memory extraction and test-time adaptation, (2) memory reasoning, and (3) memory knowledge management. They then benchmark thirteen different memory systems on these tasks.

Result: Experiments on thirteen memory systems show that models benefit from explicit mechanisms for retaining and organizing multimodal information, yet they still struggle with complex reasoning over stored memories and with managing accumulated knowledge efficiently.

Conclusion: Mem-Gallery exposes that while current MLLM agents can store and retrieve multimodal information to some extent, they remain limited in reasoning over long-term memories and in organizing knowledge at scale, and they also face efficiency bottlenecks—indicating the need for better multimodal memory architectures and management strategies.

Abstract: Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.

</details>


### [32] [PALM-Bench: A Comprehensive Benchmark for Personalized Audio-Language Models](https://arxiv.org/abs/2601.03531)
*Yuwen Wang,Xinyuan Qian,Tian-Hao Zhang,Jiaran Gao,Yuchen Pan,Xin Wang,Zhou Pan,Chen Wei,Yiming Wang*

Main category: cs.CL

TL;DR: The paper introduces Personalized Large Audio-Language Models (PALM) and a new benchmark (PALM-Bench) to evaluate and improve how LALMs understand and reason over personal, speaker-specific context.


<details>
  <summary>Details</summary>
Motivation: Current Large Audio-Language Models perform well on generic audio tasks like transcription and summarization but fail at personalized question answering that depends on an individual’s personal context (e.g., understanding what ‘my best friend’ refers to in a conversation). Humans naturally interpret audio and make decisions using such personal context, so there is a gap between human-like understanding and existing LALMs that the authors aim to close.

Method: 1) The authors formally define the task of Personalized LALMs (PALM), focusing on recognizing personal concepts and reasoning within personal context.
2) They construct PALM-Bench, the first benchmark designed for personalized audio-language understanding, covering multiple tasks and multi-speaker scenarios.
3) They conduct extensive experiments on representative open-source LALMs, evaluating both training-free prompting methods and supervised fine-tuning approaches on these personalized tasks.

Result: The experiments show that while both prompting-based methods and supervised fine-tuning can bring some performance gains, they are still inadequate for robustly modeling personalized knowledge and transferring it across different tasks. Existing LALMs remain largely generic and struggle with rich, stable personalization in multi-speaker settings.

Conclusion: Personalization in audio-language models is far from solved: current techniques only partially capture and transfer personalized knowledge. The PALM formulation and PALM-Bench benchmark provide a foundation for systematic study and advancement of personalized LALMs, and the release of data and code is intended to facilitate further research in this direction.

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in audio understanding and generation. Yet, our extensive benchmarking reveals that their behavior is largely generic (e.g., summarizing spoken content) and fails to adequately support personalized question answering (e.g., summarizing what my best friend says). In contrast, human conditions their interpretation and decision-making on each individual's personal context. To bridge this gap, we formalize the task of Personalized LALMs (PALM) for recognizing personal concepts and reasoning within personal context. Moreover, we create the first benchmark (PALM-Bench) to foster the methodological advances in PALM and enable structured evaluation on several tasks across multi-speaker scenarios. Our extensive experiments on representative open-source LALMs, show that existing training-free prompting and supervised fine-tuning strategies, while yield improvements, remains limited in modeling personalized knowledge and transferring them across tasks robustly. Data and code will be released.

</details>


### [33] [Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach](https://arxiv.org/abs/2601.03534)
*Yilong Dai,Ziyi Wang,Chenguang Wang,Kexin Zhou,Yiheng Qian,Susu Xu,Xiang Yan*

Main category: cs.CL

TL;DR: The paper introduces a persona-aware vision-language framework to assess bikeability that captures subjective cyclist perceptions while providing explainable factor attribution.


<details>
  <summary>Details</summary>
Motivation: Existing bikeability assessments do not adequately represent users’ subjective perceptions of safety and comfort, and current perception-based methods struggle with the complexity of road environments and heterogeneity among cyclists. There is a need for an approach that both predicts bikeability and explains which infrastructural factors matter for different types of cyclists.

Method: The authors design a persona-aware Vision-Language Model that is conditioned on theory-grounded cyclist personas. The model uses chain-of-thought reasoning to generate persona-specific explanations. It is trained with multi-granularity supervised fine-tuning that integrates limited expert-annotated reasoning data with large-scale user rating data to jointly perform prediction and explanation. An AI-based data augmentation scheme generates paired road-environment samples that allow isolating the impact of specific infrastructure variables. They implement a panoramic image-based crowdsourcing platform to collect persona-conditioned assessments from cyclists to train and test the framework.

Result: Using 12,400 persona-conditioned assessments from 427 cyclists, experiments show that the framework predicts bikeability ratings competitively compared to baselines while also producing fine-grained, persona-specific explanatory attributions to different environmental and infrastructure factors.

Conclusion: The proposed persona-aware Vision-Language Model framework effectively incorporates heterogeneous cyclist perceptions into bikeability assessment, enabling both accurate rating prediction and interpretable explanations of the contributing factors. This demonstrates the potential of combining persona conditioning, vision-language modeling, and AI-driven data augmentation for more user-centered, explainable urban bikeability evaluation.

Abstract: Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.

</details>


### [34] [DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing](https://arxiv.org/abs/2601.03540)
*Hongzhi Zhang,Yuanze Hu,Tinghai Zhang,Jia Fu,Tao Wang,Junwei Jing,Zhaoxin Fan,Qi Wang,Ruiming Tang,Han Li,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: DeepSynth-Eval is a benchmark to objectively test how well LLM-based agents synthesize large volumes of research into structured long-form reports, decoupling synthesis from retrieval and using checklist-based metrics.


<details>
  <summary>Details</summary>
Motivation: Existing LLM research has focused heavily on retrieval and question answering, but the crucial post-retrieval step—digesting large, fragmented evidence into coherent, survey-like reports—is poorly evaluated because long-form writing is subjective and hard to score reliably. There is a need for an objective, reproducible way to measure information consolidation and structural adherence when models act as deep research agents.

Method: The authors construct DeepSynth-Eval by using existing high-quality survey papers as ground-truth outputs. They reverse-engineer each survey into a research request and build an “Oracle Context” from the survey’s bibliography so that retrieval is assumed perfect and only synthesis is evaluated. They then design a fine-grained evaluation protocol with two types of checklists: General Checklists capture factual coverage of key points, and Constraint Checklists encode structural and organizational requirements. These checklists turn subjective judgments about writing quality into concrete, verifiable metrics. They run experiments on 96 synthesis tasks, comparing single-shot generations to agentic plan-and-write workflows under the benchmark.

Result: Across 96 tasks, models struggle to synthesize information from hundreds of references, indicating that large-scale consolidation remains difficult even with ideal context. Agentic plan-and-write workflows significantly outperform simple single-turn generation: they lower hallucination rates, improve factual coverage, and better satisfy complex structural and organizational constraints as captured by the checklist metrics.

Conclusion: DeepSynth-Eval provides an objective and reproducible framework for evaluating the synthesis capabilities of LLM-based research agents, specifically in consolidating large, curated contexts into high-quality long-form outputs. The benchmark reveals that current models are still weak at large-scale synthesis but that structured, agentic workflows notably improve performance. This suggests future work should emphasize planning-based generation and further refinement of synthesis-specific evaluation protocols, rather than focusing solely on retrieval or short-form QA.

Abstract: The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing "Oracle Contexts" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.

</details>


### [35] [Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://arxiv.org/abs/2601.03542)
*Xukai Liu,Ye Liu,Jipeng Zhang,Yanghai Zhang,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: The paper finds that in multi-hop reasoning, LLMs often decode final answers earlier in the network than intermediate bridge entities, and explains this with a probabilistic recall-and-extract mechanism across layers.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models internally perform multi-hop reasoning, and to test whether the existing hop-aligned circuit hypothesis—where intermediate bridge entities are computed layer-by-layer before final answers—actually holds on realistic multi-hop tasks.

Method: The authors systematically probe and decode internal representations of LLMs on real-world multi-hop questions. They measure at which layers bridge entities and final answer entities become decodable. Observing discrepancies with the hop-aligned view, they propose and analyze a probabilistic recall-and-extract framework, where shallow MLP layers perform broad, noisy recall of related facts and deeper attention layers selectively extract the relevant ones. They validate this through detailed layer-wise probing and reinterpretation of prior decoding analyses.

Result: They discover layer-order inversion: the model’s representations often make the later-hop answer decodable earlier than the intermediate bridge entities, and this inversion becomes more pronounced as the number of reasoning hops increases. Their probabilistic recall-and-extract model matches these empirical patterns and can account for observed behaviors in multi-hop reasoning, including successes, failures, and chain-of-thought benefits.

Conclusion: The hop-aligned circuit hypothesis does not generally describe how LLMs perform multi-hop reasoning. Instead, multi-hop answers can emerge earlier in the network than bridge entities due to a recall-and-extract process, where shallow layers broadly recall many candidate facts and deeper layers focus and extract the relevant ones. This reinterprets prior mechanistic findings and offers a framework for diagnosing when and why LLMs fail at multi-hop reasoning despite having correct single-hop knowledge.

Abstract: Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.

</details>


### [36] [EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory](https://arxiv.org/abs/2601.03543)
*Ye Shen,Dun Pei,Yiqiu Guo,Junying Wang,Yijin Guo,Zicheng Zhang,Qi Jia,Jun Zhou,Guangtao Zhai*

Main category: cs.CL

TL;DR: EvolMem is a benchmark to systematically evaluate LLMs' multi-session conversational memory across diverse, cognitively grounded memory dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for long-range conversational memory do not systematically test LLMs across diverse memory dimensions, especially in multi-session dialog settings, leaving gaps in understanding their true memory capabilities and weaknesses.

Method: The authors design EvolMem, a cognitively grounded benchmark that covers declarative and non-declarative memory, decomposed into fine-grained abilities. They build a hybrid data synthesis pipeline combining topic-initiated conversation generation with narrative-inspired transformations to produce scalable, controllable multi-session dialogues, each with tailored evaluation guidelines.

Result: Using EvolMem to evaluate various LLMs and agent systems, they find that no single LLM dominates across all memory dimensions, and that adding agent-style memory mechanisms does not reliably improve performance and often introduces efficiency overheads.

Conclusion: EvolMem exposes nuanced strengths and weaknesses of LLMs' multi-session memory, shows that memory capabilities are heterogeneous across models and dimensions, and suggests that current agent memory designs are not a guaranteed path to better long-range conversational memory, motivating more targeted future research.

Abstract: Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.

</details>


### [37] [Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict](https://arxiv.org/abs/2601.03546)
*Guanyu Chen,Chenxiao Yu,Xiyang Hu*

Main category: cs.CL

TL;DR: The paper introduces a new way to test whether large language models’ stated privacy and prosocial values actually predict their simulated data‑sharing behavior.


<details>
  <summary>Details</summary>
Motivation: Researchers and practitioners increasingly use LLMs to simulate human decisions about sharing personal data, where individuals must trade off privacy concerns against prosocial motives such as helping others or contributing to research. However, current evaluations typically look at privacy attitudes or sharing intentions separately, so they cannot tell whether an LLM that “says” it cares about privacy or prosocialness will behave consistently when facing concrete data‑sharing choices. There is a need for a systematic, human-grounded protocol to assess whether LLM value-like statements align with downstream actions under realistic value conflicts.

Method: The authors design a context-based assessment protocol that runs entirely within a single, history-carrying LLM session. In this session, the model is sequentially given standardized questionnaires that measure (1) privacy attitudes, (2) prosocial tendencies, and (3) acceptance of data sharing in specific scenarios. They then apply multi-group structural equation modeling (MGSEM) to the model-generated responses to estimate how privacy concerns and prosocialness jointly predict acceptance of data sharing, mirroring human behavioral modeling. To quantify how well the model’s internal value–action structure matches human expectations, they define the Value-Action Alignment Rate (VAAR), which aggregates evidence about whether estimated paths (e.g., privacy → data sharing, prosocialness → data sharing) have the expected direction and sign relative to human reference data.

Result: Using this protocol, they obtain distinct, stable profiles for different LLMs in terms of their privacy attitudes, prosocialness, and acceptance of data sharing (Privacy-PSA-AoDS profiles). Despite this stability, they find substantial variation across models in how well their stated values predict their own data-sharing choices. Some models show high value–action alignment consistent with human patterns (e.g., higher privacy concern predicting lower data sharing, higher prosocialness predicting higher data sharing), while others show weaker or more inconsistent structural relations, leading to heterogeneous VAAR scores.

Conclusion: The study shows that LLMs can be systematically evaluated for alignment between expressed values and downstream behavior in contexts with competing motives such as privacy vs. prosocial benefits. The proposed context-based protocol, MGSEM analysis, and VAAR metric reveal that different models embody different, model-specific value–action structures and that alignment is far from uniform. These tools provide a more realistic and fine-grained way to assess and compare LLM behavior in sensitive decision-making tasks, and highlight the need to go beyond isolated attitude or intention measures when validating models as human proxies.

Abstract: Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.

</details>


### [38] [Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios](https://arxiv.org/abs/2601.03553)
*Sangyub Lee,Heedou Kim,Hyeoncheol Kim*

Main category: cs.CL

TL;DR: Proposes PAS, an evaluation framework and QA dataset to systematically test how well LLMs can support police operations, showing current commercial models perform poorly on fact-based police advice.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in police work, but there is no evaluation framework tailored to police operations. Even if LLM answers are not obviously illegal, unverified use can still cause serious problems such as unlawful arrests and improper evidence collection. The authors aim to create a structured way to assess whether LLMs can safely and reliably support police decision-making.

Method: They design PAS (Police Action Scenarios), a systematic evaluation framework that spans the entire evaluation process for LLMs in policing contexts. Using PAS, they build a new QA dataset derived from more than 8,000 official police-related documents. They then define key evaluation metrics and validate these metrics statistically by comparing them with judgements from police experts. Finally, they run experiments with commercial LLMs on their tasks, focusing on fact-based recommendations in police scenarios.

Result: The constructed dataset and metrics form a specialized benchmark for police operations. Commercial LLMs tested on this benchmark perform poorly, especially when they are required to give fact-grounded recommendations for police actions. The correlation analyses support that their metrics capture expert notions of quality and correctness.

Conclusion: PAS provides an extensible, domain-specific evaluation framework for assessing LLMs in police operations. The poor performance of current commercial models on PAS underscores that LLMs are not yet reliable for unsupervised or critical use in policing and that careful, expert-aligned evaluation is essential. The authors release their dataset and prompt templates to support future research and improvement.

Abstract: The use of Large Language Models (LLMs) in police operations is growing, yet an evaluation framework tailored to police operations remains absent. While LLM's responses may not always be legally incorrect, their unverified use still can lead to severe issues such as unlawful arrests and improper evidence collection. To address this, we propose PAS (Police Action Scenarios), a systematic framework covering the entire evaluation process. Applying this framework, we constructed a novel QA dataset from over 8,000 official documents and established key metrics validated through statistical analysis with police expert judgements. Experimental results show that commercial LLMs struggle with our new police-related tasks, particularly in providing fact-based recommendations. This study highlights the necessity of an expandable evaluation framework to ensure reliable AI-driven police operations. We release our data and prompt template.

</details>


### [39] [DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2601.03559)
*Shidong Cao,Hongzhan Lin,Yuxuan Gu,Ziyang Luo,Jing Ma*

Main category: cs.CL

TL;DR: The paper introduces DiffCoT, a diffusion-style framework that treats chain-of-thought reasoning as iterative denoising to enable both generation and correction of reasoning steps, improving robustness and accuracy over standard CoT methods.


<details>
  <summary>Details</summary>
Motivation: Standard chain-of-thought reasoning in large language models is autoregressive: once an early step is wrong, all subsequent steps build on that error. This exposure bias and error accumulation make multi-step mathematical reasoning brittle, with limited ability to revise or correct intermediate steps. The authors are motivated to design a mechanism that allows CoT to be refined iteratively, correcting earlier mistakes while still fitting within the autoregressive generation paradigm.

Method: They propose DiffCoT, which reformulates CoT reasoning as a diffusion-style iterative denoising process operating at the reasoning-step level. Using a sliding window over recent reasoning steps, the model can generate and retrospectively refine intermediate steps. They incorporate diffusion principles into this windowed process while preserving token-level autoregression. To ensure that later steps do not incorrectly influence earlier ones, they introduce a causal diffusion noise schedule that aligns with the temporal order of reasoning steps, maintaining causal consistency of the reasoning chain.

Result: Across three multi-step CoT reasoning benchmarks and using various model backbones, DiffCoT consistently outperforms existing CoT preference optimization approaches. The empirical results show better robustness to early-step errors and stronger error-correction capabilities in multi-step mathematical reasoning tasks.

Conclusion: DiffCoT demonstrates that viewing chain-of-thought reasoning as an iterative denoising process can alleviate exposure bias and error accumulation in multi-step reasoning. By combining a sliding-window diffusion mechanism with a causal noise schedule, the framework enables both generation and correction of reasoning steps, leading to more accurate and robust CoT-based problem solving than prior optimization methods.

Abstract: Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.

</details>


### [40] [How Do Large Language Models Learn Concepts During Continual Pre-Training?](https://arxiv.org/abs/2601.03570)
*Barry Menglong Yao,Sha Li,Yunzhi Yao,Minqian Liu,Zaishuo Xia,Qifan Wang,Lifu Huang*

Main category: cs.CL

TL;DR: The paper investigates how large language models (LLMs) acquire, forget, and transfer individual concepts during continual pretraining, using internal “concept circuits” and graph metrics to characterize the dynamics of concept learning and interference.


<details>
  <summary>Details</summary>
Motivation: Although humans rely on structured concepts to understand the world, it is unclear how LLMs internally represent and update such concepts over time as they are continually pretrained. Existing work tends to treat LLM knowledge monolithically, without isolating single concepts or tracking their evolution. The authors aim to understand the fine-grained dynamics of concept acquisition, forgetting, interference, and transfer inside LLMs, and to relate these behaviors to identifiable internal computational structures. This can improve interpretability and guide better training strategies.

Method: The authors define and identify “Concept Circuits,” which are computational subgraphs in an LLM associated with particular concepts. They continuously pretrain LLMs and track how performance on specific concepts changes over time, while concurrently analyzing the structural properties of the associated circuits using graph-theoretic metrics. They then study (i) temporal dynamics of individual concepts, (ii) forgetting after further training, (iii) interference between multiple concepts—especially semantically similar ones—and (iv) positive transfer or synergy where learning one concept helps another.

Result: They find that: (1) the behavior of identified concept circuits correlates significantly with measured concept learning and forgetting; (2) concept circuits go through a stage-wise temporal trajectory during continual pretraining: early growth, then gradual decline and stabilization; (3) concepts that are learned more strongly initially tend to be more prone to forgetting with continued training; (4) semantically similar concepts interfere more with one another than weakly related concepts; and (5) some concepts serve as better “sources” for transfer, substantially improving the learning of other concepts, while others transfer poorly.

Conclusion: The study shows that concept-level behavior in LLMs can be meaningfully linked to specific internal circuits with characteristic graph structures and temporal dynamics. Concept learning is not monotonic; it involves growth, partial erosion, and stabilization, with strong initial learning often accompanied by later forgetting and with significant interference between similar concepts. At the same time, some concepts provide beneficial transfer to others. These insights support the development of concept-aware, circuit-level training and continual learning strategies aimed at better interpretability, reduced interference, and more robust retention of conceptual knowledge in LLMs.

Abstract: Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.

</details>


### [41] [PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics](https://arxiv.org/abs/2601.03578)
*Yaling Shen,Stephanie Fong,Yiwen Jiang,Zimu Wang,Feilong Tang,Qingyang Xu,Xiangyu Zhao,Zhongxing Xu,Jiahe Liu,Jinpeng Hu,Dominic Dwyer,Zongyuan Ge*

Main category: cs.CL

TL;DR: Introduces PsychEthicsBench, a benchmark to evaluate LLM ethical behavior in mental health beyond simple refusal rates, showing refusals are poor proxies for clinical appropriateness and that domain fine-tuning can harm ethical robustness.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in mental health settings, there is a need to ensure they are professionally and ethically aligned with clinical standards. Existing safety evaluations focus mainly on whether models refuse to answer, but in mental health this can be harmful by appearing unempathetic and deterring help-seeking. There is a gap for a benchmark grounded in professional guidelines that can capture nuanced, context-sensitive ethical behavior rather than binary refusal.

Method: The authors design PsychEthicsBench, a benchmark derived from Australian psychology and psychiatry professional guidelines. It includes multiple-choice and open-ended tasks that test both ethical knowledge and behavioral responses. Items are annotated with fine-grained ethicality labels, allowing detailed assessment of how well LLM outputs align with professional ethics. They evaluate 14 different LLMs on this benchmark, comparing refusal behaviors, ethical performance, and the impact of domain-specific fine-tuning.

Result: Across the 14 models, high refusal rates do not correlate well with ethically appropriate behavior on PsychEthicsBench. Models that refuse more are not necessarily safer or more clinically aligned; in fact, refusal-based safety triggers often diverge from what is clinically appropriate. The authors also find that domain-specialized fine-tuning for mental health can reduce ethical robustness: several specialized models perform worse on ethical alignment than their base, more general backbones when evaluated on the benchmark.

Conclusion: PsychEthicsBench demonstrates that refusal-based metrics are insufficient and sometimes misleading for assessing LLM safety in mental health contexts. A principle-grounded, jurisdiction-specific benchmark better captures ethical alignment and reveals trade-offs introduced by domain fine-tuning. The work provides a foundation for more systematic, clinically relevant, and region-aware evaluation of mental-health LLMs, aiming to guide more responsible development and deployment in this sensitive domain.

Abstract: The increasing integration of large language models (LLMs) into mental health applications necessitates robust frameworks for evaluating professional safety alignment. Current evaluative approaches primarily rely on refusal-based safety signals, which offer limited insight into the nuanced behaviors required in clinical practice. In mental health, clinically inadequate refusals can be perceived as unempathetic and discourage help-seeking. To address this gap, we move beyond refusal-centric metrics and introduce \texttt{PsychEthicsBench}, the first principle-grounded benchmark based on Australian psychology and psychiatry guidelines, designed to evaluate LLMs' ethical knowledge and behavioral responses through multiple-choice and open-ended tasks with fine-grained ethicality annotations. Empirical results across 14 models reveal that refusal rates are poor indicators of ethical behavior, revealing a significant divergence between safety triggers and clinical appropriateness. Notably, we find that domain-specific fine-tuning can degrade ethical robustness, as several specialized models underperform their base backbones in ethical alignment. PsychEthicsBench provides a foundation for systematic, jurisdiction-aware evaluation of LLMs in mental health, encouraging more responsible development in this domain.

</details>


### [42] [OLA: Output Language Alignment in Code-Switched LLM Interactions](https://arxiv.org/abs/2601.03589)
*Juhyun Oh,Haneul Yoo,Faiz Ghifari Haznitrama,Alice Oh*

Main category: cs.CL

TL;DR: The paper shows that large language models often respond in the wrong language when users code-switch, and introduces a benchmark plus a small alignment method that significantly improves this behavior.


<details>
  <summary>Details</summary>
Motivation: Multilingual users naturally mix languages in conversation, but LLMs struggle to infer which language they should respond in when prompts are code-switched and the desired output language is only implicitly specified. This misalignment leads to user frustration and reveals a gap between human pragmatic understanding and current LLM behavior. The authors want to systematically measure and fix this gap.

Method: The authors design OLA, a benchmark targeting Output Language Alignment for Korean–English code-switching. The benchmark covers a spectrum of scenarios, from simple intra-sentential language mixing to more complex cases where the instruction language and content language mismatch. They evaluate multiple frontier LLMs on OLA, analyze their biases (e.g., toward non-English output), and test generalization of these biases to other language pairs (Chinese–English, Indonesian–English). They further test Chain-of-Thought prompting to see if explicit reasoning helps. Finally, they apply a Code-Switching Aware Direct Preference Optimization (DPO) approach using about 1K training examples to align models better with desired output language.

Result: Across models, performance on OLA is poor: LLMs often produce responses in an undesired language, frequently favoring non-English output even when human-intuitive cues indicate another choice. This misalignment also appears in other language pairs beyond Korean–English. Models display unstable behavior such as switching languages mid-response and inserting unwanted language intrusions. Chain-of-Thought prompting does not alleviate these issues. However, training with Code-Switching Aware DPO on a relatively small dataset (around 1,000 examples) substantially improves output language alignment, reducing the rate of misaligned responses.

Conclusion: LLMs currently exhibit serious shortcomings in handling code-switched prompts, failing to infer users' implicit expectations about output language and revealing weak pragmatic reasoning. These failures are not fundamental limitations of model architecture but largely alignment issues: a small amount of targeted, code-switching-aware preference optimization can markedly improve behavior. The authors argue that future multilingual LLM development should explicitly incorporate output language alignment to better support natural, real-world code-switched interactions.

Abstract: Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs' Output Language Alignment in code-switched interactions. OLA focuses on Korean--English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users' implicit expectations in real-world code-switched interactions.

</details>


### [43] [From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs](https://arxiv.org/abs/2601.03597)
*Yingjian Chen,Haoran Liu,Yinhong Liu,Sherry T. Tong,Aosong Feng,Jinghui Lu,Juntao Zhang,Yusuke Iwasawa,Yutaka Matsuo,Irene Li*

Main category: cs.CL

TL;DR: The paper introduces Self-Graph Reasoning (SGR), a framework that lets LLMs explicitly build and use graph-structured reasoning processes to improve consistency and accuracy in open-domain QA, achieving large performance gains and rivaling/toppling leading models.


<details>
  <summary>Details</summary>
Motivation: Although LLMs perform well on open-domain QA, their reasoning is usually linear (e.g., Chain-of-Thought) and often logically inconsistent. Real-world reasoning is inherently multi-premise and parallel, better captured as graphs than as single linear chains. Existing graph-based methods typically depend on external graphs and do not allow LLMs to construct and leverage their own reasoning graphs in open-domain settings. The authors aim to bridge this gap by enabling LLMs to explicitly form and exploit graph-structured reasoning processes.

Method: They propose Self-Graph Reasoning (SGR), where an LLM first constructs an explicit reasoning graph that represents premises, intermediate facts, and their relationships, then uses this graph to derive the final answer. They further build a training dataset for graph-structured reasoning by merging multiple candidate reasoning graphs into refined, higher-quality graph structures. The framework is then used to fine-tune LLMs (e.g., LLaMA-3.3-70B) for open-domain and specialized-domain QA tasks.

Result: On five QA benchmarks across general and specialized domains, models trained with SGR show consistently better reasoning consistency and achieve a 17.74% performance gain over the base model. A LLaMA-3.3-70B model fine-tuned with SGR performs on par with GPT-4o and outperforms Claude-3.5-Haiku on these benchmarks.

Conclusion: Explicit graph-structured reasoning, as instantiated in SGR, enables LLMs to reason more consistently and accurately than traditional linear Chain-of-Thought approaches. By allowing models to construct and use their own reasoning graphs, SGR significantly boosts performance and can elevate open-source models to a level comparable with or superior to strong proprietary baselines, underscoring the promise of graph-based reasoning for open-domain QA.

Abstract: Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.

</details>


### [44] [DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier](https://arxiv.org/abs/2601.03605)
*Hui Huang,Muyun Yang,Yuki Arase*

Main category: cs.CL

TL;DR: The paper introduces DiVA, a hybrid agentic-discriminative verifier and a new benchmark (FGVeriBench) for fine-grained factuality verification of LLM outputs, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate factually incorrect content, and current factuality verification methods mostly offer only binary judgments (correct/incorrect). This binary view cannot capture different levels of error severity, which are crucial for tasks like detailed evaluation of model answers and training via preference optimization. The authors are motivated to create a system and benchmark that can assess factuality in a fine-grained manner.

Method: The authors propose Agentic Discriminative Verifier (DiVA), a hybrid framework that combines: (1) agentic search using generative models to actively explore and retrieve relevant evidence and reasoning paths, and (2) a discriminative model that assigns precise factuality scores instead of simple binary labels. Additionally, they design FGVeriBench, a benchmark specifically aimed at evaluating fine-grained factuality verification, including both general and multi-hop question settings.

Result: On the newly constructed FGVeriBench benchmark, DiVA shows substantially better performance than existing factuality verification methods. It is effective across both general questions and more complex multi-hop questions, demonstrating improved accuracy and granularity in factuality judgments.

Conclusion: Fine-grained factuality verification is both necessary and feasible. By combining agentic search from generative models with discriminative scoring, DiVA can outperform prior approaches that rely mainly on binary verification. The introduced FGVeriBench benchmark provides a robust testbed to drive further research on nuanced, fine-grained factuality assessment for LLMs.

Abstract: Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.

</details>


### [45] [Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation](https://arxiv.org/abs/2601.03615)
*Binh Nguyen,Thai Le*

Main category: cs.CL

TL;DR: The paper evaluates how robust the reasoning processes of Audio Language Models (ALMs) are for audio deepfake detection when facing adversarial attacks, not just whether their final fake/real classification is correct.


<details>
  <summary>Details</summary>
Motivation: Traditional audio deepfake detection relies on black-box classifiers that give a label (fake/real) without explaining why. New Audio Language Models provide reasoning traces, which can in principle make detection more transparent and trustworthy. However, this introduces a new and largely unexplored robustness question: not only can the final prediction be attacked, but the reasoning itself (how the model explains its decision) can also be manipulated by adversaries. The authors aim to understand and quantify how stable and trustworthy these reasoning traces are under different kinds of adversarial perturbations.

Method: The authors propose a forensic auditing framework for Audio Language Models under adversarial attacks, focusing on reasoning rather than just output labels. They decompose robustness into three interrelated dimensions: (1) acoustic perception (how robustly the model perceives and describes audio cues), (2) cognitive coherence (how logically consistent and aligned the reasoning is with the observed evidence and the final decision), and (3) cognitive dissonance (internal contradictions or tension between evidence descriptions and the final classification). Using this framework, they systematically subject ALMs to various adversarial attacks, including acoustic and linguistic attacks, and analyze how reasoning traces shift along these three dimensions and how that correlates with attack success.

Result: The analysis shows that adding explicit reasoning to ALMs does not uniformly strengthen robustness to adversarial attacks. They identify a split: models that already have robust acoustic perception benefit from reasoning, which acts as a “shield” that helps resist adversarial perturbations. In contrast, models with weaker perception suffer from a “tax”: explicit reasoning can actually make them more vulnerable, especially to linguistic attacks that disrupt cognitive coherence and increase the probability of a successful attack. The study also finds that, even when the model’s final deepfake classification is wrong, high cognitive dissonance in the reasoning trace can still serve as an indicator that manipulation is present.

Conclusion: Reasoning in Audio Language Models for deepfake detection is a double-edged sword: it can either protect or weaken models depending on their underlying perceptual robustness. Explicit reasoning is not a guaranteed path to greater robustness; instead, it introduces new vulnerabilities, especially to linguistic adversarial attacks that target the coherence of explanations. Nevertheless, the reasoning traces, particularly signs of cognitive dissonance, can provide valuable forensic signals even when the classification is compromised, effectively acting as a silent alarm for potential audio manipulation. The paper underscores the need to evaluate and design ALMs with both classification accuracy and reasoning robustness in mind for reliable forensic use.

Abstract: Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.

</details>


### [46] [Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines](https://arxiv.org/abs/2601.03627)
*Jean Seo,Gibaeg Kim,Kihun Shin,Seungseop Lim,Hyunkyung Lee,Wooseok Han,Jongwon Lee,Eunho Yang*

Main category: cs.CL

TL;DR: The paper presents EPAG, a benchmark dataset and framework to evaluate large language models’ (LLMs) pre-consultation abilities in clinical settings, using diagnostic guidelines and disease diagnosis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks focus mostly on medical knowledge tests or post-hoc reasoning, but there is a lack of systematic evaluation of how well LLMs conduct pre-consultation dialogue and follow diagnostic guidelines in realistic settings. The authors aim to fill this gap so that LLMs can be more reliably and safely used in real-world clinical workflows.

Method: The authors construct EPAG, a benchmark that compares LLM-generated histories of present illness (HPI) with guideline-based HPIs and assesses diagnostic performance. They evaluate various models directly via HPI–guideline similarity and indirectly via their ability to correctly diagnose diseases. They run experiments with frontier LLMs and smaller open-source models fine-tuned on a curated, task-specific dataset, and analyze the effect of HPI length and consultation language on performance and dialogue characteristics.

Result: Experiments show that small, fine-tuned open-source LLMs can outperform larger frontier models on pre-consultation tasks within EPAG. The study also finds that providing more HPI information does not always improve diagnostic accuracy, and that the language in which pre-consultation is conducted systematically affects dialogue characteristics and behavior of the models.

Conclusion: EPAG offers a standardized way to evaluate LLM pre-consultation capability using diagnostic guidelines and diagnostic accuracy. The results suggest that targeted fine-tuning can make smaller models competitive or superior to frontier models for this task, and that more information is not always better for diagnosis. By releasing the dataset and evaluation pipeline, the authors aim to support future research and development of clinically useful LLM-based pre-consultation systems.

Abstract: We introduce EPAG, a benchmark dataset and framework designed for Evaluating the Pre-consultation Ability of LLMs using diagnostic Guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.

</details>


### [47] [Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases](https://arxiv.org/abs/2601.03630)
*Hui Huang,Xuanxin Wu,Muyun Yang,Yuki Arase*

Main category: cs.CL

TL;DR: The paper compares Large Reasoning Models (LRMs) with standard LLMs as automatic judges and introduces PlanJudge to reduce evaluation bias.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used as automatic evaluators of other models and content, it is unclear whether newer Large Reasoning Models truly act as better, more reliable judges than traditional non-reasoning LLMs, and how to mitigate their known biases.

Method: The authors run a systematic empirical comparison between LRMs and non-reasoning LLMs on a variety of judgment tasks, with a focus on reasoning-intensive evaluations, instruction-following in evaluation prompts, and robustness against adversarial attacks on judgment. They then introduce PlanJudge, a prompting strategy where the model first produces an explicit evaluation plan before issuing its final judgment, and test its impact on bias and robustness.

Result: (1) LRMs are more accurate than non-reasoning LLMs as judges, especially on tasks requiring substantial reasoning; (2) LRMs follow evaluation instructions more faithfully; (3) LRMs are more robust to adversarial attempts to manipulate their judgments; (4) Both LRMs and standard LLMs remain strongly biased by superficial qualities of responses. PlanJudge substantially reduces these biases across both LRMs and standard LLMs in experiments.

Conclusion: Large Reasoning Models make better automatic judges than traditional LLMs but still suffer from notable superficial-quality biases. A simple planning-based prompting strategy, PlanJudge, can meaningfully improve robustness and reduce these biases, making both LRMs and standard LLMs more reliable evaluators.

Abstract: This paper presents the first systematic comparison investigating whether Large Reasoning Models (LRMs) are superior judge to non-reasoning LLMs. Our empirical analysis yields four key findings: 1) LRMs outperform non-reasoning LLMs in terms of judgment accuracy, particularly on reasoning-intensive tasks; 2) LRMs demonstrate superior instruction-following capabilities in evaluation contexts; 3) LRMs exhibit enhanced robustness against adversarial attacks targeting judgment tasks; 4) However, LRMs still exhibit strong biases in superficial quality. To improve the robustness against biases, we propose PlanJudge, an evaluation strategy that prompts the model to generate an explicit evaluation plan before execution. Despite its simplicity, our experiments demonstrate that PlanJudge significantly mitigates biases in both LRMs and standard LLMs.

</details>


### [48] [Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning](https://arxiv.org/abs/2601.03641)
*Zheng Wu,Xingyu Lou,Xinbei Ma,Yansi Li,Weiwen Liu,Weinan Zhang,Jun Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: They propose Agent-Dice, a parameter fusion method to help LLM-based agents continually learn new tasks without forgetting old ones, by separating shared from conflicting knowledge during training.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents must operate in changing environments and continually acquire new skills. Standard training causes catastrophic forgetting (the stability-plasticity dilemma), partly because existing methods do not explicitly separate knowledge common to many tasks from knowledge that conflicts across tasks. The authors aim to provide a principled way to fuse updates from multiple tasks so agents retain prior capabilities while learning new ones efficiently.

Method: They introduce Agent-Dice, a parameter fusion framework that analyzes gradient directions from different tasks. First, it applies geometric consensus filtering to detect and prune gradient components that conflict across tasks (directional disagreement). Second, it uses curvature-based importance weighting to emphasize and preserve directions representing shared semantics, likely via approximations of the local loss curvature (e.g., Fisher/Hessian-based measures). This yields fused parameter updates that favor shared, stable knowledge while reducing harmful task-specific interference. They also develop a theoretical analysis formalizing why this scheme addresses the stability-plasticity dilemma.

Result: On benchmark settings for GUI agents and tool-use agents, Agent-Dice achieves strong continual learning performance, outperforming baselines in retaining old tasks while learning new ones, with low computational and parameter overhead. The experiments indicate that their fusion approach successfully mitigates catastrophic forgetting in practical LLM-agent scenarios.

Conclusion: Explicitly separating shared from conflicting knowledge via directional consensus and curvature-aware weighting is an effective strategy for continual learning in LLM-based agents. Agent-Dice offers both theoretical justification and empirical evidence that such parameter fusion can alleviate the stability-plasticity dilemma with minimal additional cost, suggesting a promising direction for robust, continually learning agents.

Abstract: Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.

</details>


### [49] [LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight](https://arxiv.org/abs/2601.03645)
*Yu-Zheng Lin,Bono Po-Jen Shih,John Paul Martin Encinas,Elizabeth Victoria Abraham Achom,Karan Himanshu Patel,Jesus Horacio Pacheco,Sicong Shao,Jyotikrishna Dass,Soheil Salehi,Pratik Satam*

Main category: cs.CL

TL;DR: The paper proposes LLM-MC-Affect, a probabilistic, Monte Carlo-based framework that uses stochastic LLM decoding to model emotions as continuous probability distributions over an affective space, enabling analysis of emotional trajectories and interpersonal coupling in dialogues, validated on teacher–student interactions.


<details>
  <summary>Details</summary>
Motivation: Existing text-based sentiment and affect models typically output a single deterministic label or score per utterance, which neglects the inherent subjectivity, ambiguity, and temporal interdependence of emotions in real conversations. There is a need for a method that captures uncertainty in affect inference and the dynamic coordination of emotions between interlocutors, particularly for understanding complex interpersonal processes such as scaffolding in instructional settings.

Method: The authors introduce LLM-MC-Affect, which treats emotion as a continuous latent probability distribution in an affective space rather than as a discrete label. They use stochastic decoding with large language models to generate multiple affective inferences for the same text, and then apply Monte Carlo estimation to approximate the underlying affective distribution and its uncertainty over time. From these estimated distributions, they construct sentiment trajectories for each interlocutor and analyze interpersonal emotional coordination using sequential cross-correlation and slope-based indicators to detect leading or lagging emotional influences between speakers.

Result: Applying LLM-MC-Affect to teacher–student instructional dialogues, the approach yields high-fidelity sentiment trajectories that quantify both central emotional tendencies and associated ambiguity. The derived indicators of emotional coupling successfully recover interpretable patterns in the interactions, such as signatures of effective pedagogical scaffolding and directional influence between teacher and student affect over time.

Conclusion: LLM-MC-Affect offers a scalable and deployable probabilistic approach to modeling affect in interaction, moving beyond static, deterministic sentiment labels to dynamic distributions that capture uncertainty and interpersonal coupling. The framework enables structured analysis of emotional coordination and can generate interpretable insights about relational processes, demonstrated in educational dialogues but applicable to a broad range of social and behavioral research contexts.

Abstract: Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing to capture the inherent subjectivity, latent ambiguity, and sequential coupling found in mutual exchanges. We introduce LLM-MC-Affect, a probabilistic framework that characterizes emotion not as a static label, but as a continuous latent probability distribution defined over an affective space. By leveraging stochastic LLM decoding and Monte Carlo estimation, the methodology approximates these distributions to derive high-fidelity sentiment trajectories that explicitly quantify both central affective tendencies and perceptual ambiguity. These trajectories enable a structured analysis of interpersonal coupling through sequential cross-correlation and slope-based indicators, identifying leading or lagging influences between interlocutors. To validate the interpretive capacity of this approach, we utilize teacher-student instructional dialogues as a representative case study, where our quantitative indicators successfully distill high-level interaction insights such as effective scaffolding. This work establishes a scalable and deployable pathway for understanding interpersonal dynamics, offering a generalizable solution that extends beyond education to broader social and behavioral research.

</details>


### [50] [ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs](https://arxiv.org/abs/2601.03648)
*HanGyeol Yoo,ChangSu Choi,Minjun Kim,Seohyun Song,SeungWoo Song,Inho Won,Jongyoul Park,Cheoneum Park,KyungTae Lim*

Main category: cs.CL

TL;DR: An efficient layer-specific optimization (ELO) method accelerates continual pretraining for a target language in multilingual LLMs by training only key layers, then realigning them with the full model, achieving large speedups and performance gains while preserving original language abilities.


<details>
  <summary>Details</summary>
Motivation: Continual pretraining multilingual LLMs on a new target language is computationally expensive and often harms performance in existing (source) languages. There is a need for a more efficient method that can specialize models to new languages without high GPU cost or catastrophic forgetting of source-language skills.

Method: ELO uses a two-stage, layer-specific process. In ELO Pretraining, only a small subset of layers—found empirically to be the first and last layers—are detached from the original multilingual LLM and pretrained on the target language, reducing both trainable parameters and forward-pass compute. In the Layer Alignment stage, the updated layers are reinserted into the original model and the entire model is briefly fine-tuned on a small dataset to reconcile and align parameters between the newly trained layers and the rest of the network.

Result: Experiments show that ELO provides up to a 6.46× training speedup compared to standard continual pretraining approaches. It improves target language benchmark performance by up to 6.2% while maintaining the original English performance, indicating effective specialization without significant degradation in source language ability.

Conclusion: Layer-specific optimization focused on the first and last layers is an effective and efficient way to perform continual pretraining for new languages in multilingual LLMs. The ELO method significantly reduces computation and memory requirements, accelerates training, enhances target language performance, and preserves source-language capabilities after a brief alignment fine-tuning step.

Abstract: We propose an efficient layer-specific optimization (ELO) method designed to enhance continual pretraining (CP) for specific languages in multilingual large language models (MLLMs). This approach addresses the common challenges of high computational cost and degradation of source language performance associated with traditional CP. The ELO method consists of two main stages: (1) ELO Pretraining, where a small subset of specific layers, identified in our experiments as the critically important first and last layers, are detached from the original MLLM and trained with the target language. This significantly reduces not only the number of trainable parameters but also the total parameters computed during the forward pass, minimizing GPU memory consumption and accelerating the training process. (2) Layer Alignment, where the newly trained layers are reintegrated into the original model, followed by a brief full fine-tuning step on a small dataset to align the parameters. Experimental results demonstrate that the ELO method achieves a training speedup of up to 6.46 times compared to existing methods, while improving target language performance by up to 6.2\% on qualitative benchmarks and effectively preserving source language (English) capabilities.

</details>


### [51] [e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings](https://arxiv.org/abs/2601.03666)
*Haonan Chen,Sicheng Gao,Radu Timofte,Tetsuya Sakai,Zhicheng Dou*

Main category: cs.CL

TL;DR: The paper introduces e5-omni, a lightweight method to explicitly align embeddings from multiple modalities (text, image, audio, video, etc.) into a shared space, improving cross-modal similarity comparison over existing vision-language-based omni-modal models.


<details>
  <summary>Details</summary>
Motivation: Existing omni-modal embedding models mostly inherit implicit alignment from pretrained vision-language models, leading to inconsistent similarity scales across modalities, ineffective in-batch negatives due to imbalanced hardness in mixed-modality batches, and mismatched statistical properties (mean and covariance) of embeddings across modalities, which hurt ranking stability and retrieval performance. The authors aim to fix these systemic alignment issues while reusing strong off-the-shelf VLM backbones.

Method: They propose e5-omni, an explicit alignment recipe that adapts off-the-shelf VLMs using three key components: (1) modality-aware temperature calibration so that similarity logits from different modalities are rescaled to a consistent sharpness; (2) a controllable negative curriculum with debiasing that schedules the difficulty of negative samples and mitigates false negatives, making contrastive training more effective in mixed-modality batches; and (3) batch whitening plus covariance regularization to align the first- and second-order statistics across modalities, enforcing better-matched geometry in the shared embedding space.

Result: On benchmark datasets such as MMEB-V2 and AudioCaps, e5-omni yields consistent performance gains over strong bi-modal and omni-modal baselines. Additionally, their recipe generalizes well across different VLM backbones, indicating that the approach is broadly applicable rather than tied to a single architecture.

Conclusion: Explicit cross-modal alignment on top of strong VLM backbones—via temperature calibration, negative curriculum with debiasing, and batch whitening with covariance regularization—produces more robust, better-calibrated omni-modal embeddings than relying solely on inherited implicit alignment. The method is lightweight, widely applicable to existing VLMs, and empirically improves retrieval and ranking quality across multiple modalities; the authors also release a public model checkpoint (e5-omni-7B) to facilitate adoption.

Abstract: Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.

</details>


### [52] [eTracer: Towards Traceable Text Generation via Claim-Level Grounding](https://arxiv.org/abs/2601.03669)
*Bohao Chu,Qianli Wang,Hendrik Damm,Hui Wang,Ula Muhabbek,Elisabeth Livingstone,Christoph M. Friedrich,Norbert Fuhr*

Main category: cs.CL

TL;DR: eTracer is a framework that makes system-generated biomedical responses traceable and verifiable by grounding each claim in contextual evidence, improving faithfulness and user verification efficiency.


<details>
  <summary>Details</summary>
Motivation: In high-stakes biomedical domains, system-generated responses must be trustworthy and easily verifiable. Existing grounding methods typically align text only at the sentence level, which is too coarse to reliably verify individual claims or detect contradictions. There is a need for a fine-grained, claim-level grounding method that can quantify and expose how each generated statement relates to the underlying evidence.

Method: The authors propose eTracer, a plug-and-play framework for traceable text generation. After a system generates a response, eTracer performs post-hoc grounding at the claim level, aligning each atomic claim in the response to contextual evidence that either supports or contradicts it. The framework then aggregates these grounding results to (1) provide explicit traceability links from each claim back to its source evidence and (2) compute a quantitative measure of response faithfulness based on the quality and polarity (support/contradict) of the alignments.

Result: Experiments in biomedical settings demonstrate that claim-level grounding with eTracer surpasses conventional sentence-level grounding methods. It improves the accuracy and granularity of aligning generated claims with evidence, thereby enhancing overall grounding quality. User studies or evaluation further show that this finer-grained grounding significantly improves user verification efficiency when checking system-generated responses.

Conclusion: eTracer effectively addresses the challenge of verifying system-generated text in high-stakes biomedical domains by providing claim-level grounding and faithfulness quantification. Its plug-and-play, post-hoc design allows integration with existing generation systems and leads to more traceable, verifiable, and trustworthy responses compared to traditional sentence-level grounding approaches.

Abstract: How can system-generated responses be efficiently verified, especially in the high-stakes biomedical domain? To address this challenge, we introduce eTracer, a plug-and-play framework that enables traceable text generation by grounding claims against contextual evidence. Through post-hoc grounding, each response claim is aligned with contextual evidence that either supports or contradicts it. Building on claim-level grounding results, eTracer not only enables users to precisely trace responses back to their contextual source but also quantifies response faithfulness, thereby enabling the verifiability and trustworthiness of generated responses. Experiments show that our claim-level grounding approach alleviates the limitations of conventional grounding methods in aligning generated statements with contextual sentence-level evidence, resulting in substantial improvements in overall grounding quality and user verification efficiency. The code and data are available at https://github.com/chubohao/eTracer.

</details>


### [53] [Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis](https://arxiv.org/abs/2601.03676)
*Yifan Wei,Li Du,Xiaoyan Yu,Yang Feng,Angsheng Li*

Main category: cs.CL

TL;DR: STEPS is a framework that builds a hierarchical skill taxonomy and uses it to synthesize challenging, compositionally rich training data for LLMs and agents, improving instruction-following and compositional generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs and agent-based systems perform poorly on compositional generalization, especially when tasks require rare or complex combinations of skills. Real-world data follows a long-tailed, power-law distribution over such combinations, so most composite skills are underrepresented, creating a data bottleneck that limits instruction-following and agent performance. The paper aims to systematically generate the underrepresented, compositionally complex data needed to overcome this bottleneck.

Method: The authors introduce STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework. First, they infer latent relationships between skills and organize them into a hierarchical, interpretable skill taxonomy using structural information theory. Then they cast data synthesis as a constrained information maximization problem over this hierarchy: they algorithmically choose combinations of skills that maximize marginal structural information (i.e., are compositionally informative and challenging) while constraining generation to maintain semantic coherence of the resulting instructions. The synthesized data is then used for post-training LLMs/agents.

Result: On challenging instruction-following benchmarks, models post-trained with data produced by STEPS outperform models trained with existing data synthesis baselines. Furthermore, in downstream agent-based tasks that demand compositional reasoning over multiple skills, STEPS-trained models exhibit superior compositional generalization compared to baselines, demonstrating better performance on novel and complex skill compositions.

Conclusion: By explicitly modeling the structure of skills via a hierarchical taxonomy and using it to guide entropy-based data synthesis, STEPS effectively targets the long-tail of complex skill compositions that current training pipelines miss. This targeted synthesis improves both instruction-following and compositional generalization in agent settings, suggesting that structure-aware, information-theoretic data generation is a promising direction for scaling LLM and agent capabilities beyond what can be achieved with naive or heuristic data augmentation.

Abstract: Large Language Models (LLMs) and agent-based systems often struggle with compositional generalization due to a data bottleneck in which complex skill combinations follow a long-tailed, power-law distribution, limiting both instruction-following performance and generalization in agent-centric tasks. To address this challenge, we propose STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework for generating compositionally challenging data. STEPS explicitly targets compositional generalization by uncovering latent relationships among skills and organizing them into an interpretable, hierarchical skill taxonomy using structural information theory. Building on this taxonomy, we formulate data synthesis as a constrained information maximization problem, selecting skill combinations that maximize marginal structural information within the hierarchy while preserving semantic coherence. Experiments on challenging instruction-following benchmarks show that STEPS outperforms existing data synthesis baselines, while also yielding improved compositional generalization in downstream agent-based evaluations.

</details>


### [54] [DisastQA: A Comprehensive Benchmark for Evaluating Question Answering in Disaster Management](https://arxiv.org/abs/2601.03670)
*Zhitong Chen,Kai Yin,Xiangjue Dong,Chengkai Liu,Xiangpeng Li,Yiming Xiao,Bo Li,Junwei Ma,Ali Mostafavi,James Caverlee*

Main category: cs.CL

TL;DR: Introduces DisastQA, a disaster-focused QA benchmark stressing reasoning under uncertain and noisy evidence, revealing large reliability gaps in current LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing QA benchmarks mostly use clean, reliable evidence and fail to capture the uncertainty, conflict, and noise typical of real-world disaster management scenarios. This makes it hard to measure whether models can truly support high-stakes decision-making in disasters, where information is incomplete, evolving, and often contradictory. The authors aim to build a benchmark that better reflects these conditions and can distinguish a model’s internal knowledge from its ability to reason with imperfect evidence.

Method: They construct DisastQA, a benchmark of 3,000 rigorously verified questions (2,000 multiple-choice and 1,000 open-ended) across eight disaster types using a human–LLM collaboration pipeline. Stratified sampling is used to ensure balanced coverage over disaster types and question categories. The benchmark includes multiple evaluation modes: closed-book QA (no external evidence), clean evidence QA, and noisy evidence QA with conflicting or imperfect documents, so they can separately analyze knowledge and reasoning under uncertainty. For open-ended questions, they design a human-verified keypoint-based evaluation protocol that scores factual completeness rather than length or style. They then evaluate 20 different language models, including both proprietary and open-weight systems, across these settings.

Result: The experiments show that model rankings on DisastQA differ markedly from general-purpose QA leaderboards like MMLU-Pro. While strong open-weight models reach or approach proprietary models when evidence is clean, all models—especially open-weight ones—suffer large performance drops under noisy, conflicting, or otherwise imperfect evidence. This demonstrates that current benchmarks overestimate reliability in realistic disaster-response conditions and that models are brittle when information quality degrades.

Conclusion: DisastQA fills an important gap by providing a disaster-specific QA benchmark that explicitly tests reasoning under uncertainty and noisy evidence. The findings indicate that current LLMs, including those that perform well on standard benchmarks, are not yet reliably trustworthy for disaster management scenarios. The authors release all code, data, and evaluation resources to support further research on robust, noise-tolerant QA for high-stakes applications.

Abstract: Accurate question answering (QA) in disaster management requires reasoning over uncertain and conflicting information, a setting poorly captured by existing benchmarks built on clean evidence. We introduce DisastQA, a large-scale benchmark of 3,000 rigorously verified questions (2,000 multiple-choice and 1,000 open-ended) spanning eight disaster types. The benchmark is constructed via a human-LLM collaboration pipeline with stratified sampling to ensure balanced coverage. Models are evaluated under varying evidence conditions, from closed-book to noisy evidence integration, enabling separation of internal knowledge from reasoning under imperfect information. For open-ended QA, we propose a human-verified keypoint-based evaluation protocol emphasizing factual completeness over verbosity. Experiments with 20 models reveal substantial divergences from general-purpose leaderboards such as MMLU-Pro. While recent open-weight models approach proprietary systems in clean settings, performance degrades sharply under realistic noise, exposing critical reliability gaps for disaster response. All code, data, and evaluation resources are available at https://github.com/TamuChen18/DisastQA_open.

</details>


### [55] [From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs](https://arxiv.org/abs/2601.03682)
*Shaojie Wang,Liang Zhang*

Main category: cs.CL

TL;DR: The paper finds that most math reasoning errors in LLMs stem from poor understanding of logical relationships, proposes a lightweight training method (FSLR) that teaches only the first reasoning step, and shows it outperforms standard CoT fine-tuning while being cheaper and faster.


<details>
  <summary>Details</summary>
Motivation: LLMs often solve math problems by pattern-matching rather than true logical reasoning. Existing approaches like Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) fine-tune on full solution traces but still leave a large fraction of logical reasoning errors unresolved. The authors want to identify the main source of these errors and design a more targeted and efficient training method to improve logical relationship understanding specifically.

Method: 1) Empirically analyze LLM errors on math problems to categorize and quantify the portion due to failures in logical relationship understanding. 2) Observe that the critical step is the first planning action: selecting which variables to use and what operation to apply. 3) Propose First-Step Logical Reasoning (FSLR), a training framework that isolates and supervises this first step instead of the whole chain-of-thought. 4) Train models with explicit labels for the first-step plan (variables + operation) and compare against traditional CoT-SFT on multiple models and datasets, both in-distribution and out-of-distribution.

Result: The analysis reveals that more than 90% of incorrect predictions are attributable to errors in logical relationship understanding. CoT-SFT does not meaningfully reduce these errors. FSLR-trained models consistently outperform CoT-SFT across different LLM backbones and benchmarks, achieving on average 3.2% performance gains in-distribution and 4.6% gains out-of-distribution. Additionally, FSLR is significantly more efficient, with 4–6× faster training and over 80% reduction in training token usage.

Conclusion: Logical relationship understanding—especially in the first reasoning step—is the main bottleneck in LLM mathematical reasoning. Targeted supervision on this first step (FSLR) is more effective and efficient than full chain-of-thought supervision. Focusing training on explicit first-step planning improves both in-distribution and out-of-distribution generalization while greatly reducing training cost, suggesting that fine-grained supervision of critical reasoning sub-skills can be a powerful alternative to standard CoT-SFT.

Abstract: Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\% and 4.6\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\%.

</details>


### [56] [NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models](https://arxiv.org/abs/2601.03671)
*Weiqi Liu,Yongliang Miao,Haiyan Zhao,Yanguang Liu,Mengnan Du*

Main category: cs.CL

TL;DR: Introduces NeuronScope, a method to interpret polysemantic neurons in LLMs via iterative, activation-guided multi-agent analysis.


<details>
  <summary>Details</summary>
Motivation: Neuron-level interpretation of LLMs is hard because many neurons are polysemantic, responding to multiple unrelated concepts; single-pass methods usually miss this complexity and give incomplete or misleading neuron explanations.

Method: NeuronScope uses a multi-agent framework that repeatedly analyzes a neuron’s activations, decomposes them into atomic semantic components, clusters these into distinct semantic modes, and iteratively refines the proposed explanations using feedback based on how well they predict or correlate with neuron activations.

Result: Experiments show that NeuronScope can reveal hidden polysemanticity that prior methods overlook and yields neuron explanations whose predicted activation patterns correlate much better with the neuron’s true activations than single-pass baseline approaches.

Conclusion: Iterative, activation-guided multi-agent decomposition of neuron behavior provides a more faithful, fine-grained interpretation of polysemantic neurons in LLMs than traditional single-pass interpretation techniques.

Abstract: Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.

</details>


### [57] [O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2601.03743)
*Yi Yao,He Zhu,Piaohong Wang,Jincheng Ren,Xinlong Yang,Qianben Chen,Xiaowan Li,Dingfeng Shi,Jiaxian Li,Qiexiang Wang,Sinuo Wang,Xinpeng Liu,Jiaqi Wu,Minghao Liu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: The paper presents a framework that uses multi-agent AI systems to automatically generate high-quality instructional data and a two-stage training pipeline (SFT + novel RL) to significantly boost open-source LLM performance to state-of-the-art levels on a deep research benchmark.


<details>
  <summary>Details</summary>
Motivation: Closed-source LLMs outperform open-source ones mainly because they have access to better, proprietary training data. Open-source models lack similar-quality data, limiting their capabilities and alignment. The authors are motivated to create a scalable, automated way to produce research-grade training data so that open-source LLMs can close this performance gap without depending on proprietary resources.

Method: The method has two main components: (1) A multi-agent data synthesis framework where multiple AI agents collaborate, use tools, and simulate complex reasoning processes to generate diverse, high-fidelity instructional data end-to-end. (2) A two-stage training pipeline: first, supervised fine-tuning on the synthesized data; second, a novel reinforcement learning procedure tailored to further align the model and improve its capabilities based on the generated data and performance feedback.

Result: Using the proposed framework and training strategy, open-source LLMs at various parameter scales are significantly improved. The models trained with this pipeline reach new state-of-the-art results on a prominent deep research benchmark, demonstrating that the synthesized data and training method are highly effective.

Conclusion: The work shows that high-quality, research-grade instructional data can be automatically synthesized via a multi-agent, tool-using workflow and successfully used in a two-stage SFT+RL training pipeline to markedly improve open-source LLMs. This provides a practical and scalable route for advancing open-source models to near closed-source performance levels without needing proprietary data or closed models.

Abstract: The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.

</details>


### [58] [Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms](https://arxiv.org/abs/2601.03752)
*Dominik Macko*

Main category: cs.CL

TL;DR: The paper examines how personalization in large language model outputs affects text quality and detectability of AI-generated disinformation across 10 languages, revealing cross-lingual differences and especially strong effects for English and platform-focused personalization.


<details>
  <summary>Details</summary>
Motivation: As large language models become better at generating fluent text in many languages, they can be misused to create convincing, personalized disinformation. Prior work showed that personalization lowers detectability of AI-generated texts, but only in English. There is a lack of systematic, multilingual analysis of how different personalization strategies (e.g., targeting demographic groups vs. social-media platforms) affect both the quality and detectability of generated content. The paper aims to fill this gap to better understand risks and potential safeguards in multilingual settings.

Method: The authors design prompts that encode a wide variety of personalization aspects, such as demographic targeting and platform-specific tailoring, across 10 different languages. They instantiate 1,080 distinct personalization configurations and use 16 distinct large language models to generate a total of 17,280 texts. They then evaluate these texts along two main dimensions: (1) the quality of personalization (how well the text reflects the targeted demographic or platform) and (2) detectability (how easily automated detectors can distinguish machine-generated text from human-written text). Comparative analyses are conducted across languages and types of personalization.

Result: The study finds clear cross-lingual differences in how well large language models personalize their outputs and how that personalization impacts detectability. Personalization toward social-media platforms leads to a stronger reduction in detectability than personalization toward demographic groups, with the effect being particularly pronounced in English, where personalization quality is also highest. The results show that some languages and personalization types pose higher risks for undetectable disinformation than others.

Conclusion: Personalized text generation with large language models presents uneven risks across languages and personalization targets. Platform-oriented personalization, especially in English, both achieves higher personalization quality and more substantially decreases the detectability of AI-generated content, exacerbating the threat of multilingual, tailored disinformation. The findings highlight the need for language- and context-aware detection strategies and safeguards, as well as careful consideration of how personalization features are exposed in deployed systems.

Abstract: Capabilities of large language models to generate multilingual coherent text have continuously enhanced in recent years, which opens concerns about their potential misuse. Previous research has shown that they can be misused for generation of personalized disinformation in multiple languages. It has also been observed that personalization negatively affects detectability of machine-generated texts; however, this has been studied in the English language only. In this work, we examine this phenomenon across 10 languages, while we focus not only on potential misuse of personalization capabilities, but also on potential benefits they offer. Overall, we cover 1080 combinations of various personalization aspects in the prompts, for which the texts are generated by 16 distinct language models (17,280 texts in total). Our results indicate that there are differences in personalization quality of the generated texts when targeting demographic groups and when targeting social-media platforms across languages. Personalization towards platforms affects detectability of the generated texts in a higher scale, especially in English, where the personalization quality is the highest.

</details>


### [59] [Evaluation Framework for AI Creativity: A Case Study Based on Story Generation](https://arxiv.org/abs/2601.03698)
*Pharath Sathya,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: The paper introduces a new human-centered framework to evaluate AI story creativity beyond traditional reference-based metrics.


<details>
  <summary>Details</summary>
Motivation: Current automatic metrics for evaluating creative text generation rely on reference comparisons and fail to reflect how humans subjectively judge creativity, especially in storytelling. There is a need for an evaluation scheme that aligns more closely with human perceptions and multifaceted aspects of creativity.

Method: The authors design a structured evaluation framework with four main components—Novelty, Value, Adherence, and Resonance—and eleven finer-grained sub-components for assessing AI-generated stories. They generate stories using a controlled prompting technique called Spike Prompting, then conduct a crowdsourced study with 115 human readers to gather both immediate and reflective creativity judgments along these components.

Result: Analysis of human ratings shows that people do not combine creativity aspects in a simple additive way. Instead, they judge creativity hierarchically, with different dimensions becoming important at different stages (initial vs reflective) of evaluation. Reflective evaluations notably change overall creativity ratings and increase or modify inter-rater agreement patterns, revealing nuances that are invisible to standard reference-based metrics.

Conclusion: The proposed framework successfully surfaces multiple dimensions of creativity in AI story generation and demonstrates that human creativity judgments are staged and hierarchical. This suggests that relying solely on reference-based automatic metrics is insufficient and that structured, multi-dimensional human evaluation is better suited for assessing creative text generation systems.

Abstract: Evaluating creative text generation remains a challenge because existing reference-based metrics fail to capture the subjective nature of creativity. We propose a structured evaluation framework for AI story generation comprising four components (Novelty, Value, Adherence, and Resonance) and eleven sub-components. Using controlled story generation via ``Spike Prompting'' and a crowdsourced study of 115 readers, we examine how different creative components shape both immediate and reflective human creativity judgments. Our findings show that creativity is evaluated hierarchically rather than cumulatively, with different dimensions becoming salient at different stages of judgment, and that reflective evaluation substantially alters both ratings and inter-rater agreement. Together, these results support the effectiveness of our framework in revealing dimensions of creativity that are obscured by reference-based evaluation.

</details>


### [60] [Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents](https://arxiv.org/abs/2601.03785)
*Dehao Tao,Guoliang Ma,Yongfeng Huang,Minghu Jiang*

Main category: cs.CL

TL;DR: The paper proposes Membox, a hierarchical memory architecture for LLM agents that preserves topic continuity in dialogue and significantly improves temporal reasoning while using fewer context tokens.


<details>
  <summary>Details</summary>
Motivation: Most current LLM agent memory systems fragment dialogues into isolated utterances and then rely on embedding-based retrieval to re-establish coherence. This fragmentation-compensation paradigm destroys narrative and causal flow, biases retrieval toward lexical similarity, and fails to capture topic continuity and long-range temporal structure. The authors aim to design a memory system that better aligns with how topics evolve over time in natural human-agent dialogues, improving coherence and temporal reasoning without excessive token usage.

Method: The authors introduce Membox, a hierarchical memory architecture built around two core components. First, a Topic Loom monitors ongoing dialogue with a sliding window and dynamically clusters temporally adjacent dialogue turns that share the same topic into coherent "memory boxes" at storage time, preserving local narrative flow. Once a box is sealed, the second component, a Trace Weaver, links these boxes into long-range event-timeline traces that capture macro-topic recurrences and temporal relationships across dialogue discontinuities. This structure replaces naive utterance-level storage and generic embedding retrieval with topic-aware, temporally organized memory. The system is evaluated on the LoCoMo benchmark against strong baselines such as Mem0 and A-MEM.

Result: On the LoCoMo benchmark, Membox achieves up to 68% F1 improvement on temporal reasoning tasks compared to competitive memory baselines like Mem0 and A-MEM. It also attains these performance gains while using substantially fewer context tokens than existing methods, indicating improved efficiency as well as effectiveness in capturing topic continuity and temporal structure.

Conclusion: Membox shows that explicitly modeling topic continuity during memory formation and retrieval can substantially enhance LLM agents’ temporal reasoning and dialogue coherence. By grouping same-topic turns into structured memory boxes and linking them into event timelines, Membox provides a cognitively inspired and token-efficient alternative to standard fragment-and-retrieve memory designs. This suggests a promising direction for future LLM agent architectures that need long-term, coherent interaction histories.

Abstract: Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent "memory boxes" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.

</details>


### [61] [RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models](https://arxiv.org/abs/2601.03699)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TL;DR: The paper introduces RedBench, a unified, large-scale red-teaming benchmark that aggregates 37 existing datasets to systematically evaluate LLM robustness against adversarial prompts using a standardized taxonomy.


<details>
  <summary>Details</summary>
Motivation: Current red teaming datasets for LLM safety are fragmented, with inconsistent risk labels, narrow domain coverage, and often outdated evaluations. This fragmentation makes it difficult to systematically assess and compare the vulnerabilities and robustness of different LLMs, especially as they are deployed in safety-critical settings.

Method: The authors aggregate 37 existing red-teaming and safety-related datasets into a single benchmark called RedBench, totaling 29,362 samples that include both attack prompts and refusal prompts. They design and apply a unified taxonomy with 22 risk categories and 19 domains to relabel or map samples consistently across datasets. They then run modern LLMs on RedBench, evaluate their behavior using this standardized schema, and provide open-source code and resources for benchmarking and analysis.

Result: RedBench consolidates previously scattered datasets into one coherent resource with consistent risk and domain labels, covering a wide range of adversarial and safety scenarios. The authors produce baseline performance results for contemporary LLMs on this benchmark and offer detailed dataset analyses that highlight gaps, strengths, and patterns in current red-teaming resources and model behaviors.

Conclusion: RedBench enables more reliable, apples-to-apples comparisons of LLM safety and robustness by standardizing risk taxonomies and aggregating diverse red-teaming datasets. By making both the dataset and evaluation code publicly available, the work lowers the barrier to systematic safety evaluation, supports future research on adversarial robustness, and aims to guide the development of more secure and trustworthy LLMs for real-world use.

Abstract: As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval

</details>


### [62] [NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning](https://arxiv.org/abs/2601.03790)
*Zhongtao Miao,Kaiyan Zhao,Masaaki Nagata,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: They build a Wiktionary-based agent framework and dataset to improve machine translation of neologisms, trained with reinforcement learning and a special reward/rollout scheme.


<details>
  <summary>Details</summary>
Motivation: Standard MT systems struggle with neologisms (newly coined words, slang, new technical terms), and there is a lack of dedicated datasets and methods for handling them in a multilingual setting. The authors aim to systematically support neologism-aware MT with both data and a training framework that can leverage external lexical resources.

Method: 1) Construct a neologism-aware MT dataset from ~10M records of an English Wiktionary dump, spanning 16 languages and 75 translation directions.
2) Build a Wiktionary-based search tool using ~3M cleaned records as a retrieval corpus to look up neologisms and related lexical information.
3) Design an agentic MT framework, NeoAMT, where a translation agent can query the search tool while translating.
4) Train the agent with reinforcement learning, using a novel reward function tailored to neologism translation quality and an adaptive rollout generation strategy based on estimated “translation difficulty.”

Result: They obtain a large multilingual dataset for neologism-aware MT, a functioning Wiktionary search tool, and an RL-trained translation agent that can leverage this tool. Experiments show improved neologism translation accuracy and overall translation quality compared to baselines that do not use the agentic RL framework or external Wiktionary retrieval in the same way.

Conclusion: Integrating an external Wiktionary-based search tool into an RL-trained MT agent notably improves handling of neologisms. The proposed NeoAMT framework, dataset, and training strategy provide a practical path forward for more robust neologism-aware MT across many languages and directions.

Abstract: Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging "translation difficulty" to further improve the translation quality of translation agents using our search tool.

</details>


### [63] [ADEPT: Adaptive Dynamic Early-Exit Process for Transformers](https://arxiv.org/abs/2601.03700)
*Sangmin Yoo,Srikanth Malla,Chiho Choi,Wei D. Lu,Joon Hee Choi*

Main category: cs.CL

TL;DR: ADEPT is a dynamic early-exit method for Transformers that works at token level in both prefill and generation, reducing computation and KV-cache cost while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing early-exit methods for large language models cut computation only for the first token or at the prompt level, so later tokens still depend on full-layer KV caches, leaving much of the potential speedup unrealized. There is a need for an approach that can reduce computation per token throughout decoding without degrading model quality and that removes the KV-cache bottleneck created by skipped layers.

Method: The authors propose ADEPT, an Adaptive Dynamic Early-exit Process for Transformers. It introduces a token-level early-exit mechanism that decides, per token and per step, how many layers to execute based on token complexity. ADEPT is applied in both the prefill and generation phases. To make this feasible, the method modifies the KV-cache generation procedure so that skipped layers no longer impose strict sequential dependencies, effectively decoupling KV formation from the full forward pass of all layers. This architectural and scheduling change allows actual compute and KV-cache savings when layers are skipped for specific tokens.

Result: On language generation benchmarks, ADEPT reduces inference cost and achieves up to 25% efficiency improvement compared to standard decoding without harming output quality. For downstream classification tasks using LLMs, ADEPT yields up to 4x speed-up and can even improve task performance by as much as 45%, indicating that adaptive early exit can act as a beneficial form of dynamic depth selection rather than merely an approximation.

Conclusion: ADEPT demonstrates that carefully designed token-level early exit for Transformers can simultaneously accelerate both prefill and generation while mitigating the KV-cache bottleneck. By adaptively selecting computation depth per token and decoupling skipped layers from KV-cache dependencies, it delivers substantial speedups and sometimes better performance. This suggests dynamic depth control is a promising direction for efficient and robust LLM inference across generative and downstream tasks.

Abstract: The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance.

</details>


### [64] [Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework](https://arxiv.org/abs/2601.03791)
*Xiaoyu Luo,Yiyi Chen,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CL

TL;DR: The paper redefines how to evaluate PII memorization in LLMs, showing that much reported PII "leakage" is due to strong lexical cues in prompts rather than true memorization, and proposes a cue-controlled framework (CRM) to measure genuine privacy-relevant memorization.


<details>
  <summary>Details</summary>
Motivation: Existing work claims that LLMs memorize and leak PII based on reconstruction attacks, but these evaluations often use prompts that contain strong lexical overlap or direct surface-form hints, conflating pattern completion with actual memorization. There is a need for a principled, privacy-relevant definition and evaluation method that separates cue-driven generation from genuine stored memorization, especially across languages and different attack paradigms.

Method: The authors define Cue-Resistant Memorization (CRM), a framework that explicitly conditions on and controls lexical and surface-form cues in prompts when testing for memorization. They apply CRM in large-scale multilingual experiments over 32 languages and several memorization paradigms, including prefix-suffix completion, associative reconstruction, cue-free generation, and membership inference. By systematically varying or removing prompt-target overlap, they measure how much reconstruction remains once direct cues are removed.

Result: Under standard reconstruction setups, LLMs often appear to leak PII effectively, but CRM analysis shows this effectiveness is mostly explained by direct surface-form prompts and lexical overlap. When those cues are controlled or removed, reconstruction performance drops sharply. In settings without such cues (cue-free generation) and in membership inference tests, true positive rates for detecting real memorization are extremely low.

Conclusion: Many prior results on PII leakage in LLMs overstate genuine memorization because they rely on cue-rich prompts that elicit pattern completion. A cue-controlled evaluation like CRM is necessary to correctly quantify privacy-relevant memorization. Properly accounting for lexical cues suggests that actual memorized PII leakage is much smaller than previously claimed, and future privacy assessments of LLMs should adopt cue-resistant methodologies.

Abstract: Large Language Models (LLMs) have been reported to "leak" Personally Identifiable Information (PII), with successful PII reconstruction often interpreted as evidence of memorization. We propose a principled revision of memorization evaluation for LLMs, arguing that PII leakage should be evaluated under low lexical cue conditions, where target PII cannot be reconstructed through prompt-induced generalization or pattern completion. We formalize Cue-Resistant Memorization (CRM) as a cue-controlled evaluation framework and a necessary condition for valid memorization evaluation, explicitly conditioning on prompt-target overlap cues. Using CRM, we conduct a large-scale multilingual re-evaluation of PII leakage across 32 languages and multiple memorization paradigms. Revisiting reconstruction-based settings, including verbatim prefix-suffix completion and associative reconstruction, we find that their apparent effectiveness is driven primarily by direct surface-form cues rather than by true memorization. When such cues are controlled for, reconstruction success diminishes substantially. We further examine cue-free generation and membership inference, both of which exhibit extremely low true positive rates. Overall, our results suggest that previously reported PII leakage is better explained by cue-driven behavior than by genuine memorization, highlighting the importance of cue-controlled evaluation for reliably quantifying privacy-relevant memorization in LLMs.

</details>


### [65] [AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions](https://arxiv.org/abs/2601.03707)
*Hengxing Cai,Yijie Rao,Ligang Huang,Zanyang Zhong,Jinhan Dong,Jingjun Tan,Wenhao Lu,Renxin Zhong*

Main category: cs.CL

TL;DR: AirNav is a new real-world UAV vision-language navigation benchmark with natural instructions, plus a training framework (AirVLN-R1) that combines supervised and reinforcement fine-tuning, validated in real-world tests.


<details>
  <summary>Details</summary>
Motivation: Current UAV VLN datasets mainly rely on simulated or synthetic environments, have unnatural or constrained language instructions, and are relatively small in scale, which limits model realism, robustness, and generalization to real-world UAV navigation tasks.

Method: 1) Build AirNav, a large-scale benchmark based on real urban aerial data instead of virtual scenes, and annotate it with diverse, natural language navigation instructions. 2) Propose AirVLN-R1, a training pipeline that first uses supervised fine-tuning on the dataset and then applies reinforcement fine-tuning to further optimize navigation behavior and generalization. 3) Run preliminary real-world UAV experiments to evaluate the trained model’s feasibility outside the dataset. 4) Release the dataset and code publicly for the community.

Result: The AirNav dataset is successfully constructed and released, and the AirVLN-R1 framework achieves improved performance and generalization in UAV VLN tasks, as demonstrated by preliminary real-world tests showing that the trained model can feasibly guide UAV navigation in real environments.

Conclusion: Using real-world urban aerial data with natural, large-scale language instructions substantially improves the realism of UAV VLN benchmarks, and combining supervised fine-tuning with reinforcement fine-tuning (AirVLN-R1) yields a practical model that shows promising performance and generalization in initial real-world deployments, with resources made publicly available to support further research.

Abstract: Existing Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) datasets face issues such as dependence on virtual environments, lack of naturalness in instructions, and limited scale. To address these challenges, we propose AirNav, a large-scale UAV VLN benchmark constructed from real urban aerial data, rather than synthetic environments, with natural and diverse instructions. Additionally, we introduce the AirVLN-R1, which combines Supervised Fine-Tuning and Reinforcement Fine-Tuning to enhance performance and generalization. The feasibility of the model is preliminarily evaluated through real-world tests. Our dataset and code are publicly available.

</details>


### [66] [Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models](https://arxiv.org/abs/2601.03798)
*Taisiia Tikhomirova,Dirk U. Wulff*

Main category: cs.CL

TL;DR: The paper probes different transformer layers to see where various psycholinguistic meaning features are encoded, showing this depends heavily on probing method but reveals a shared ordering from lexical to experiential/affective features with depth.


<details>
  <summary>Details</summary>
Motivation: To understand how and where transformer language models encode psychologically relevant aspects of word meaning, which is important for cognitive theory and for practical use of these models in NLP and cognitive science.

Method: They perform a systematic, layer-wise probing analysis on 10 transformer models (both encoder-only and decoder-only). For each layer, and using three different ways of extracting embeddings, they train linear probes to predict 58 psycholinguistic features and compare performance and selectivity across depth and methods.

Result: They find that the apparent localization of meaning strongly depends on the embedding extraction method: contextualized embeddings give higher feature selectivity and different depth profiles than isolated embeddings. Final layers are usually not the best for recovering psycholinguistic information with linear probes. Nonetheless, across models and methods, there is a consistent depth ordering: lexical features peak in earlier layers, whereas experiential and affective features peak in later layers.

Conclusion: The location of meaning representations in transformers is not fixed but arises from an interaction between probing methodology and model architecture; still, there is a robust shared pattern where simpler lexical aspects are encoded earlier and richer experiential/affective aspects later in the network.

Abstract: Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity and different layer-wise profiles than isolated embeddings. Across models and methods, final-layer representations are rarely optimal for recovering psycholinguistic information with linear probes. Despite these differences, models exhibit a shared depth ordering of meaning dimensions, with lexical properties peaking earlier and experiential and affective dimensions peaking later. Together, these results show that where meaning "lives" in transformer models reflects an interaction between methodological choices and architectural constraints.

</details>


### [67] [Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR](https://arxiv.org/abs/2601.03714)
*Yunhao Liang,Ruixuan Ying,Bo Li,Hong Li,Kai Yan,Qingwen Li,Min Yang,Okamoto Satoshi,Zhe Cui,Shiwen Ni*

Main category: cs.CL

TL;DR: The paper critically evaluates DeepSeek-OCR, showing that its strong reported performance heavily depends on linguistic priors rather than pure visual recognition, and that its optical compression strategy can worsen long-context issues.


<details>
  <summary>Details</summary>
Motivation: To determine whether DeepSeek-OCR’s impressive OCR performance is truly due to superior visual understanding or is largely driven by language-model priors, especially under extreme vision-text compression aimed at alleviating long-context limits in LLMs.

Method: The authors apply sentence-level and word-level semantic corruption to input text to disrupt linguistic cues and isolate the model’s genuine visual OCR capabilities. They benchmark DeepSeek-OCR against 13 traditional and end-to-end OCR baselines, analyze robustness to semantic perturbations, study the effect of varying visual token counts on reliance on priors and hallucinations, and perform context-length stress tests to observe model behavior as the decoded text sequence grows (up to ~10k tokens).

Result: When linguistic priors are removed via semantic corruption, DeepSeek-OCR’s accuracy drops from about 90% to 20%. Traditional pipeline OCR systems are significantly more robust to these perturbations than end-to-end approaches like DeepSeek-OCR. Lower visual token budgets lead to stronger dependence on language priors and higher hallucination risk. Under long-context stress (around 10,000 tokens), DeepSeek-OCR’s performance collapses entirely, indicating instability in extreme compression regimes.

Conclusion: DeepSeek-OCR’s apparent superiority is largely language-prior driven rather than purely visual. Its aggressive optical compression increases hallucination risks and can actually intensify, not alleviate, long-context bottlenecks. The paper delineates the capability limits of DeepSeek-OCR and suggests that future vision-text compression designs must better balance visual fidelity with context length, and improve robustness to semantic perturbations to avoid overreliance on language priors.

Abstract: DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: "Visual merit or linguistic crutch - which drives DeepSeek-OCR's performance?" By employing sentence-level and word-level semantic corruption, we isolate the model's intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR's performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR's capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.

</details>


### [68] [AI Generated Text Detection](https://arxiv.org/abs/2601.03812)
*Adilkhan Alikhanov,Aidar Amangeldi,Diar Demeubay,Dilnaz Akhmetzhan,Nurbek Moldakhmetov,Omar Polat,Galymzhan Zharas*

Main category: cs.CL

TL;DR: The paper benchmarks traditional and transformer-based AI text detection models on unified datasets with topic-based splits, finding that contextual deep models like DistilBERT outperform lexical models and stressing the need for robust evaluation against topic memorization.


<details>
  <summary>Details</summary>
Motivation: LLM-generated text is increasingly being submitted by students as original work, threatening academic integrity. Existing AI text detectors vary widely and often suffer from information leakage and poor generalization across topics. The authors aim to systematically evaluate different detection approaches under a unified, leakage-resilient benchmark to understand which methods generalize best to unseen domains.

Method: They construct a unified benchmark using the HC3 and DAIGT v2 datasets and apply a topic-based data split to minimize information leakage between training and test sets. They then train and compare multiple detection models: a TF-IDF-based logistic regression classifier as a traditional baseline, a BiLSTM deep learning classifier, and a transformer-based DistilBERT model. Performance is evaluated in terms of accuracy and ROC-AUC, with emphasis on generalization to unseen topics.

Result: The TF-IDF logistic regression baseline reaches 82.87% accuracy. Deep learning models perform better: the BiLSTM achieves 88.86% accuracy, and DistilBERT reaches 88.11% accuracy but with the highest ROC-AUC of 0.96, marking it as the strongest overall performer. These results show that models leveraging contextual semantics clearly outperform those relying only on lexical features.

Conclusion: Contextual semantic modeling via deep architectures such as DistilBERT is significantly more effective for AI text detection than traditional lexical approaches. Proper evaluation protocols that avoid topic leakage are critical; otherwise, models may overfit to topics rather than learn true discriminative signals. The work is limited by dataset diversity and compute resources. Future directions include expanding dataset variety, adopting parameter-efficient fine-tuning (e.g., LoRA), using smaller or distilled models, and optimizing training via better batching and hardware-aware strategies.

Abstract: The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.

</details>


### [69] [MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation](https://arxiv.org/abs/2601.03717)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: The paper proposes MIND, a capability-adaptive distillation framework that transfers Chain-of-Thought reasoning abilities from large language models to smaller ones while improving both in-domain and cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Transferring the strong reasoning abilities of large language models to smaller ones is desirable due to practical resource constraints, but current distillation methods struggle to maintain both domain-specific performance and cross-domain generalization. Existing approaches assume a single golden rationale, ignoring diverse reasoning paths and the evolving preferences and capacities of the student model, which can introduce out-of-distribution noise and degrade the student’s latent reasoning distribution.

Method: The authors introduce MIND, a capability-adaptive framework that reframes distillation as active cognitive construction instead of passive mimicry. They synthesize multiple teacher reasoning paths via a novel Teaching Assistant network and use a Feedback-Driven Inertia Calibration mechanism. This mechanism leverages inertia-filtered training loss to adapt the supervision signal to the student’s current adaptability, aligning the teacher’s guidance with the student’s evolving reasoning capacity and mitigating catastrophic forgetting.

Result: MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks compared to existing knowledge distillation methods for reasoning. The empirical results show better domain performance and improved cross-domain generalization, supported by quantitative evaluations.

Conclusion: The proposed MIND framework effectively transfers reasoning capabilities from large to small models by aligning supervision with the student’s evolving reasoning preferences and capacities. By integrating diverse teacher perspectives and adaptively calibrating feedback, MIND avoids the pitfalls of single-rationale distillation, mitigates catastrophic forgetting, and internalizes robust reasoning abilities in the student’s latent space, as confirmed by latent space analyses and benchmark performance.

Abstract: While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's "optimal" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel "Teaching Assistant" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.

</details>


### [70] [What Matters For Safety Alignment?](https://arxiv.org/abs/2601.03868)
*Xing Li,Hui-Ling Zhen,Lihao Yin,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: Empirical study benchmarking safety alignment of 32 recent LLMs/LRMs, analyzing how model characteristics and attack techniques affect vulnerability and revealing critical weaknesses, especially via CoT-based jailbreaks.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs and LRMs is crucial but poorly understood empirically at scale. There is limited systematic evidence on which model properties, training choices, and attack methods most affect safety, and how newer reasoning-oriented models behave under adversarial probing. The paper aims to provide a rigorous, large-scale comparison to guide the design and deployment of safer AI systems.

Method: The authors benchmark 32 popular recent LLMs and LRMs from 13 model families (3B–235B parameters). They evaluate safety using five standard safety datasets and probe vulnerabilities with 56 jailbreak methods and four chain-of-thought (CoT) attack strategies. They analyze six intrinsic model characteristics (e.g., architecture, parameter scale, reasoning/self-reflection integration, training stage) and three external attack types, tracking attack success rates and how different conditions impact alignment performance over 4.6M API calls.

Result: (1) LRMs with integrated reasoning and self-reflection (GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, GPT-OSS-120B) score as the safest among evaluated models, suggesting such mechanisms enhance robustness. (2) Post-training and knowledge distillation often systematically degrade safety alignment, indicating that naive capability-focused post-training can harm safety. (3) CoT attacks using a response prefix dramatically increase jailbreak success rates, on average by 3.34x and up to 96.3% for a specific model, exposing a severe weakness in text-completion style interfaces and prefix-configurable systems. (4) Roleplay-style prompts, prompt injection, and gradient-based adversarial prompt search emerge as the most effective current methods for eliciting unsafe behavior from modern models.

Conclusion: Integrated reasoning and self-reflection in LRMs are promising for improving safety alignment, but common training practices like post-training and distillation can inadvertently undermine it unless safety is made an explicit optimization objective. CoT-prefix attacks and related features in completion-style interfaces pose major security risks, requiring architectural changes and stricter deployment policies. Given that roleplay, prompt injection, and gradient-based prompt search are especially potent attack vectors, model developers and operators should design defenses, evaluations, and system interfaces specifically targeting these modalities to achieve robust, real-world safety.

Abstract: This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.

</details>


### [71] [Stuttering-Aware Automatic Speech Recognition for Indonesian Language](https://arxiv.org/abs/2601.03727)
*Fadhil Muhammad,Alwin Djuliansah,Adrian Aryaputra Hamzah,Kurniawati Azizah*

Main category: cs.CL

TL;DR: They improve Indonesian ASR for stuttered speech by generating synthetic stuttered audio and fine‑tuning Whisper, which reduces errors without hurting fluent-speech performance.


<details>
  <summary>Details</summary>
Motivation: ASR systems work well on fluent speech but fail on stuttered speech, especially in low-resource languages such as Indonesian that lack stutter-specific datasets. Collecting large, real stuttered corpora is costly and difficult, so an alternative way to obtain training data is needed to make ASR more inclusive.

Method: They design a data augmentation pipeline that takes fluent text and injects stuttering patterns (repetitions and prolongations). This uses a mix of rule-based transformations and large language models to produce dysfluent text, which is then rendered into audio using text-to-speech. The resulting synthetic stuttered audio-text pairs are used to fine-tune a pre-trained Indonesian Whisper ASR model via transfer learning, adapting it to dysfluent acoustics.

Result: Fine-tuning with the synthetic stuttered data reduces recognition errors on stuttered speech compared to the baseline Whisper model, while preserving recognition quality on fluent speech. The improvements are consistent across evaluations, indicating that the synthetic data effectively trains the model to handle dysfluencies.

Conclusion: A synthetic data pipeline that injects stuttering patterns into fluent text and converts it to audio can successfully adapt ASR systems for stuttered speech in low-resource languages. This approach improves robustness to dysfluency without requiring large amounts of real stuttered recordings, showing a practical path toward more inclusive speech technologies for under-represented languages.

Abstract: Automatic speech recognition systems have achieved remarkable performance on fluent speech but continue to degrade significantly when processing stuttered speech, a limitation that is particularly acute for low-resource languages like Indonesian where specialized datasets are virtually non-existent. To overcome this scarcity, we propose a data augmentation framework that generates synthetic stuttered audio by injecting repetitions and prolongations into fluent text through a combination of rule-based transformations and large language models followed by text-to-speech synthesis. We apply this synthetic data to fine-tune a pre-trained Indonesian Whisper model using transfer learning, enabling the architecture to adapt to dysfluent acoustic patterns without requiring large-scale real-world recordings. Our experiments demonstrate that this targeted synthetic exposure consistently reduces recognition errors on stuttered speech while maintaining performance on fluent segments, validating the utility of synthetic data pipelines for developing more inclusive speech technologies in under-represented languages.

</details>


### [72] [Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs](https://arxiv.org/abs/2601.03940)
*Paweł Liskowski,Krzysztof Jankowski*

Main category: cs.CL

TL;DR: Arctic-ABSA introduces strong encoder and decoder models for real-world aspect-based sentiment analysis, extending label space, adding multilingual support, and achieving state-of-the-art performance with novel reasoning-based pretraining and a new large benchmark.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing ABSA systems, which use small datasets (e.g., SemEval14), only three sentiment labels, weak multilingual coverage, and lack explicit reasoning, making them less suitable for commercial, real-life applications.

Method: (1) Build a dataset 20× larger than SemEval14 by combining large public corpora with carefully curated synthetic data. (2) Extend ABSA to five sentiment classes (positive, negative, neutral, mixed, unknown) and add joint prediction of overall text sentiment. (3) Support multiple languages in a single model. (4) Explore reasoning injection by fine-tuning with Chain-of-Thought (CoT) examples. (5) Propose a novel reasoning pretraining scheme for encoder-only models to improve downstream ABSA learning and generalization. (6) Train a 395M encoder and an 8B decoder model on this setup. (7) Construct ABSA-mix, a benchmark unifying 17 public datasets across 92 domains, to evaluate performance comprehensively.

Result: The 395M-parameter encoder and 8B-parameter decoder models surpass GPT-4o and Claude 3.5 Sonnet by up to 10 accuracy points on ABSA tasks and set new state-of-the-art scores on SemEval14. A single multilingual model sustains high accuracy (87–91%) across six languages while preserving English performance. The ABSA-mix benchmark is released as a standardized large-scale evaluation suite.

Conclusion: Reasoning-focused pretraining and CoT-based fine-tuning, combined with an expanded label set, large-scale data, and multilingual design, produce highly accurate and generalizable ABSA models that outperform leading general-purpose LLMs and establish new SOTA benchmarks, while ABSA-mix provides a broad, realistic evaluation resource for the community.

Abstract: We introduce Arctic-ABSA, a collection of powerful models for real-life aspect-based sentiment analysis (ABSA). Our models are tailored to commercial needs, trained on a large corpus of public data alongside carefully generated synthetic data, resulting in a dataset 20 times larger than SemEval14. We extend typical ABSA models by expanding the number of sentiment classes from the standard three (positive, negative, neutral) to five, adding mixed and unknown classes, while also jointly predicting overall text sentiment and supporting multiple languages. We experiment with reasoning injection by fine-tuning on Chain-of-Thought (CoT) examples and introduce a novel reasoning pretraining technique for encoder-only models that significantly improves downstream fine-tuning and generalization. Our 395M-parameter encoder and 8B-parameter decoder achieve up to 10 percentage points higher accuracy than GPT-4o and Claude 3.5 Sonnet, while setting new state-of-the-art results on the SemEval14 benchmark. A single multilingual model maintains 87-91% accuracy across six languages without degrading English performance. We release ABSA-mix, a large-scale benchmark aggregating 17 public ABSA datasets across 92 domains.

</details>


### [73] [Layer-wise Positional Bias in Short-Context Language Modeling](https://arxiv.org/abs/2601.04098)
*Maryam Rahimi,Mahdi Nouri,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper analyzes how transformer language models distribute importance across input positions across layers, finding architecture-specific, stable positional importance profiles with increasing recency bias and decreasing primacy and word-type sensitivity in deeper layers.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify positional biases in language models beyond task-specific or long-context settings, and to see how these biases develop layer by layer and position by position in standard short-context language modeling.

Method: They propose an attribution-based analysis framework using layer conductance combined with a sliding-window approach to measure how much each input position contributes to model predictions at each layer, producing layer-wise positional importance profiles that are then compared across architectures, inputs, and word types.

Result: They observe that positional importance profiles are highly dependent on model architecture but are stable across different inputs and remain unchanged when input word order is lexically scrambled. The profiles show a strong recency bias that grows in later layers and a weaker primacy bias that fades with depth. Early layers emphasize content words over function words at all positions, but this distinction diminishes in deeper layers.

Conclusion: Positional biases in transformer language models are systematic, layer-dependent, and architecture-specific rather than purely task-driven. As depth increases, models increasingly focus on recent tokens and become less sensitive to word-type distinctions, suggesting that deeper layers encode more position-driven, task-agnostic processing while earlier layers encode more linguistically grounded distinctions like content vs. function words.

Abstract: Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.

</details>


### [74] [Whose Facts Win? LLM Source Preferences under Knowledge Conflicts](https://arxiv.org/abs/2601.03746)
*Jakob Schuster,Vagrant Gautam,Katja Markert*

Main category: cs.CL

TL;DR: The paper studies how large language models handle conflicting information from different sources in retrieval-augmented generation, focusing on their preferences for sources and how repetition can distort these preferences, and proposes a method to reduce repetition bias while largely preserving original source preferences.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used with retrieval mechanisms, they often encounter situations where retrieved documents disagree. Prior work has looked at how they handle knowledge conflicts, but has largely ignored whether the *source* of the information (e.g., official institutions vs. social media) affects model decisions. Interdisciplinary work on credibility suggests source matters a lot for humans, so the authors want to understand if and how it matters for LLMs, and how robust these preferences are when evidence is repeated or manipulated.

Method: The authors design a novel, controlled framework for creating inter-context knowledge conflicts in English, where different pieces of information about the same claim come from sources with different perceived levels of credibility (e.g., institutional sources like governments/newspapers vs. individuals or social media). They then systematically test 13 open-weight LLMs, measuring which source the models side with when resolving conflicts. They further manipulate the number of repetitions of information from less credible vs. more credible sources to see how repetition alters source preferences. Finally, they introduce a method to reduce repetition bias—presumably via changes in prompting, decoding, or context processing—and evaluate its effect on both repetition bias and preservation of original source preference patterns.

Result: Empirically, the tested LLMs tend to favor information that is corroborated by institutional, high-credibility sources (such as government agencies and newspapers) over information from individuals and social media when resolving knowledge conflicts. However, when information from less credible sources is simply repeated more often, the models' preferences can be flipped, indicating a strong repetition bias that overrides inherent source preferences. Using their proposed mitigation method, they report up to a 99.8% reduction in repetition bias while still preserving at least 88.8% of the models’ original source preference behavior.

Conclusion: LLMs do exhibit meaningful source preferences aligned with human notions of institutional credibility, but these preferences are fragile and can be undermined by repetition of less credible information in the context. This creates a vulnerability in retrieval-augmented systems, where adversarial or accidental repetition could distort model outputs. The authors’ proposed method shows that it is possible to greatly reduce repetition-driven bias while mostly retaining helpful source preferences, and they release their data and code to spur further research on credibility, source modeling, and robustness in knowledge-intensive NLP applications.

Abstract: As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.

</details>


### [75] [InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training](https://arxiv.org/abs/2601.04126)
*Ziyun Zhang,Zezhou Wang,Xiaoyi Zhang,Zongyu Guo,Jiahao Li,Bin Li,Yan Lu*

Main category: cs.CL

TL;DR: InfiniteWeb is a system that auto-generates realistic, multi-page, functional websites with built-in task evaluators to train GUI agents, leading to strong gains on downstream benchmarks.


<details>
  <summary>Details</summary>
Motivation: Training GUI agents that operate graphical interfaces requires large numbers of realistic, interactive environments, but such environments are scarce and expensive to build manually. Existing LLM-based code generation can produce single webpages, yet struggles to create full, coherent, multi-page websites that are both realistic and functionally rich enough for training and evaluation. There is also a lack of scalable, automatically verifiable tasks with dense feedback signals for reinforcement learning in these settings.

Method: The authors design InfiniteWeb, a pipeline that (1) uses a unified specification to describe realistic websites and their interaction requirements; (2) employs task-centric, test-driven development so that the generated code is guided by executable tasks and tests; (3) conditions generation on both a website seed and a reference design image to ensure stylistic and structural diversity; and (4) automatically generates verifiable task evaluators that can check whether specific GUI interaction goals are met, providing dense reward signals for RL-based agents. The system leverages LLM-based code generation but constrains and validates it through these components.

Result: In controlled experiments, InfiniteWeb is able to generate realistic multi-page websites that outperform those created by commercial coding agents on measures of functionality and fidelity to specifications. Furthermore, GUI agents trained within the InfiniteWeb-generated environments achieve significantly higher performance on external benchmarks such as OSWorld and Online-Mind2Web, indicating that the environments are both rich and transferable for training robust agents.

Conclusion: Automatically generated web environments from InfiniteWeb provide an effective and scalable solution for training GUI agents. By combining unified specifications, task-driven code generation, design-conditioned diversity, and built-in task evaluators, the system both surpasses existing coding agents at realistic website construction and yields substantial performance gains for GUI agents on challenging benchmarks, demonstrating its practical value for building more capable AI assistants that operate graphical interfaces.

Abstract: GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.

</details>


### [76] [ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models](https://arxiv.org/abs/2601.04131)
*Nikhil Anand,Shwetha Somasundaram,Anirudh Phukan,Apoorv Saxena,Koyel Mukherjee*

Main category: cs.CL

TL;DR: The paper proposes ContextFocus, a lightweight activation-steering method that makes large language models rely more on retrieved context instead of outdated or conflicting memorized knowledge, improving contextual faithfulness without fine-tuning or large compute overhead.


<details>
  <summary>Details</summary>
Motivation: As real-world knowledge changes, LLMs need to accurately follow up-to-date external context (e.g., retrieval results). However, when the retrieved evidence conflicts with what the model memorized during pre-training, LLMs often fall back on internal knowledge, generating unfaithful or outdated answers. Existing methods to fix this often require costly fine-tuning or heavy inference-time mechanisms, limiting practical deployment. Therefore, there is a need for an efficient, plug-and-play method that reliably shifts LLM behavior toward following contextual evidence, especially under knowledge conflicts.

Method: The authors introduce ContextFocus, a lightweight activation steering approach applied at inference time. Instead of fine-tuning, they adjust internal activations so the model focuses more on external context and less on conflicting parametric knowledge. The method is designed to be model-agnostic, involve minimal additional computation, and integrate with prompting strategies. They evaluate it on the ConFiQA benchmark, comparing against strong baselines such as ContextDPO, COIECD, and prompt-based methods, and also test its effectiveness on larger models and in combination with prompts.

Result: On the ConFiQA benchmark, ContextFocus achieves significantly better contextual faithfulness than strong baselines, including fine-tuning-based and prompt-engineering approaches, while preserving fluency and maintaining low inference overhead. Experiments show robustness across different settings and model scales and demonstrate that ContextFocus can be combined with prompting strategies to yield further improvements.

Conclusion: ContextFocus is an effective, robust, and efficient approach to improve contextual faithfulness of LLMs in knowledge-conflict scenarios. By steering activations at inference time without fine-tuning, it allows models to better follow external evidence while remaining fluent and scalable, making it practical for real-world retrieval-augmented or context-dependent applications.

Abstract: Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.

</details>


### [77] [Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations](https://arxiv.org/abs/2601.03775)
*Pingjun Hong,Benjamin Roth*

Main category: cs.CL

TL;DR: The paper tests whether large language models’ self-explanations actually help people and other models predict the model’s behavior under counterfactual variations of questions.


<details>
  <summary>Details</summary>
Motivation: Although LLMs can generate chain-of-thought and post-hoc rationales, prior work shows these rationales may not correspond to the true internal decision process, raising doubts about their trustworthiness and practical value. The authors want to know whether, despite this mismatch, explanations are still useful to end users by making the model’s behavior more predictable, especially when questions are perturbed in subtle, realistic ways.

Method: The authors use StrategyQA as a base dataset and construct counterfactual follow-up questions in two ways: (1) automatically with LLM-generated counterfactuals, and (2) using pragmatics-based perturbations grounded in linguistic/pragmatic theory. They then measure counterfactual simulatability: how accurately human participants and LLM judges can predict a target model’s answers to these perturbed questions. Conditions vary in whether judges see the model’s chain-of-thought or post-hoc explanations. They also collect and qualitatively analyze free-text justifications from humans about their predictions to see how explanations influence their reasoning about the model’s behavior.

Result: Self-explanations from the target model reliably improve simulation accuracy for both human participants and LLM judges compared to no-explanation baselines. However, the magnitude and consistency of this improvement vary substantially with (a) the way counterfactual/perturbed test cases are constructed and (b) the strength/capability of the judging model. The qualitative analysis of human-written justifications indicates that explanations change how users reason about the model, often aligning their expectations more closely with its actual responses on perturbed items.

Conclusion: Even if LLM self-explanations are not faithful to the underlying decision process, they can still be pragmatically useful by making the model more predictable under counterfactual changes, improving counterfactual simulatability for both humans and LLM judges. However, this benefit is not uniform: it depends sensitively on the perturbation strategy and the judge’s competence, implying that evaluation setups and explanation designs must be chosen carefully when using explanations for transparency, trust calibration, or interpretability purposes.

Abstract: Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.

</details>


### [78] [Tracing the complexity profiles of different linguistic phenomena through the intrinsic dimension of LLM representations](https://arxiv.org/abs/2601.03779)
*Marco Baroni,Emily Cheng,Iria deDios-Flores,Francesca Franzon*

Main category: cs.CL

TL;DR: The paper studies how intrinsic dimension (ID) of large language model (LLM) representations can indicate different kinds of linguistic complexity across model layers.


<details>
  <summary>Details</summary>
Motivation: To better understand what kinds of linguistic complexity LLMs capture internally, and at which processing stages, by using an architecture- and scale-agnostic geometric measure (intrinsic dimension) rather than task-specific behavioral metrics.

Method: Measure and compare the intrinsic dimension of hidden representations across layers of various LLMs for sentences differing in formal complexity (coordination vs. subordination) and functional complexity (right-branching vs. center-embedded structures, and ambiguous vs. unambiguous relative clause attachment). Complement ID analysis with representational similarity comparisons and layer ablation experiments to locate processing phases and validate findings.

Result: Formal syntactic contrasts involving multiple coordinated or subordinated clauses produce clear and layer-specific differences in intrinsic dimension, with the onset of these differences aligning with a previously identified phase of abstract linguistic processing. Functional contrasts (branching type, attachment ambiguity) are also detectable via ID but show weaker effects and do not align with the same processing phase. Representational similarity and ablation experiments replicate and support these patterns.

Conclusion: Intrinsic dimension is a robust indicator of linguistic complexity in LLM internal representations. It distinguishes between formal and functional types of complexity and reveals shared stages of linguistic processing across different LLM architectures.

Abstract: We explore the intrinsic dimension (ID) of LLM representations as a marker of linguistic complexity, asking if different ID profiles across LLM layers differentially characterize formal and functional complexity. We find the formal contrast between sentences with multiple coordinated or subordinated clauses to be reflected in ID differences whose onset aligns with a phase of more abstract linguistic processing independently identified in earlier work. The functional contrasts between sentences characterized by right branching vs. center embedding or unambiguous vs. ambiguous relative clause attachment are also picked up by ID, but in a less marked way, and they do not correlate with the same processing phase. Further experiments using representational similarity and layer ablation confirm the same trends. We conclude that ID is a useful marker of linguistic complexity in LLMs, that it allows to differentiate between different types of complexity, and that it points to similar stages of linguistic processing across disparate LLMs.

</details>


### [79] [HearSay Benchmark: Do Audio LLMs Leak What They Hear?](https://arxiv.org/abs/2601.03783)
*Jin Wang,Liang Lin,Kaiwen Luo,Weiliu Wang,Yitian Chen,Moayad Aloqaily,Xuehai Tang,Zhenhong Zhou,Kun Wang,Li Sun,Qingsong Wen*

Main category: cs.CL

TL;DR: The paper introduces HearSay, a benchmark to study whether audio large language models (ALLMs) leak private user information from voice alone, and shows that current models significantly and unsafely reveal such attributes, especially when using chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Audio large language models can understand and generate speech, but there is little understanding of whether they inadvertently reveal private user attributes (like gender or social traits) purely from voiceprints. With growing deployment of voice assistants and speech-based AI, it is crucial to quantify and evaluate these privacy risks and the effectiveness of existing safety mechanisms.

Method: The authors build HearSay, a benchmark of over 22,000 real-world audio clips annotated with factual privacy labels obtained through automated profiling followed by human verification. They then systematically evaluate a range of existing ALLMs on tasks that probe their ability to infer private attributes (e.g., gender, social attributes, physiological traits) from audio alone, and test the behavior of built-in safety mechanisms, including the impact of enabling chain-of-thought (CoT) reasoning.

Result: On HearSay, ALLMs can infer private attributes from voiceprints with high accuracy (up to 92.89% for gender) and can also profile social attributes. Existing safety and refusal mechanisms are largely ineffective, with near-zero refusal rates for many sensitive traits. Moreover, enabling chain-of-thought reasoning in capable models further increases the amount and depth of sensitive information they infer, indicating that advanced reasoning amplifies privacy leakage.

Conclusion: The study demonstrates that current ALLMs pose substantial and underappreciated privacy risks because they can reliably extract sensitive attributes from users’ voices and existing safeguards fail to prevent such inferences, especially when chain-of-thought reasoning is used. The authors argue that specialized privacy-alignment techniques and better safety designs are urgently needed, and they release the HearSay code and dataset to support future research on mitigating these vulnerabilities.

Abstract: While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\textit{HearSay}$ yield three critical findings: $\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark

</details>


### [80] [Compact Example-Based Explanations for Language Models](https://arxiv.org/abs/2601.03786)
*Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: They study how to select a small, informative subset of training examples for explaining model predictions, propose a new retraining-free relevance score for sets of examples, show it predicts whether examples support or undermine predictions, find common selection strategies can be worse than random, and introduce a better strategy that balances influence and representativeness.


<details>
  <summary>Details</summary>
Motivation: Influence estimation methods can say how much each training example contributes to a model’s prediction, which is useful for example-based explanations. But humans can only inspect a few examples, so we must choose a small subset. Which examples we select crucially affects explanation quality, yet prior work largely ignored principled strategies and did not evaluate how good different subsets are. There is a need for a way to measure how useful a chosen set of examples is for explaining a prediction, without expensive retraining.

Method: They introduce a selection relevance score: a retraining-free metric that evaluates a whole set of training examples in terms of how useful it is for explaining a model’s output. They validate this score with fine-tuning experiments, where they fine-tune the model on selected example sets and observe whether these sets strengthen or weaken the model’s original prediction, checking that the metric predicts this effect. They then empirically compare common selection strategies (e.g., picking top-influence points) against random selection under this metric. Finally, they design and test a new selection strategy that explicitly trades off example influence with representativeness/diversity within the training set.

Result: The proposed selection relevance score successfully predicts whether a set of examples supports or undermines the model’s prediction, as verified by fine-tuning experiments. Empirical evaluation shows that widely used selection strategies for influence-based explanations often perform no better than, and sometimes worse than, random selection in terms of this relevance score. The new selection method that balances influence with representativeness produces more useful example subsets under fixed selection budgets.

Conclusion: Set-level evaluation of training examples is crucial for high-quality, influence-based explanations. Their retraining-free selection relevance score provides an effective way to assess how explanatory a chosen subset of examples is. Common heuristic strategies that simply take the top influential points can be suboptimal, and may even underperform random choice. By instead balancing influence and representativeness when selecting examples, explanation systems can make more effective use of limited example budgets and provide more faithful and informative explanations of model predictions.

Abstract: Training data influence estimation methods quantify the contribution of training documents to a model's output, making them a promising source of information for example-based explanations. As humans cannot interpret thousands of documents, only a small subset of the training data can be presented as an explanation. Although the choice of which documents to include directly affects explanation quality, previous evaluations of such systems have largely ignored any selection strategies. To address this, we propose a novel selection relevance score, a retraining-free metric that quantifies how useful a set of examples is for explaining a model's output. We validate this score through fine-tuning experiments, confirming that it can predict whether a set of examples supports or undermines the model's predictions. Using this metric, we further show that common selection strategies often underperform random selection. Motivated by this finding, we propose a strategy that balances influence and representativeness, enabling better use of selection budgets than naively selecting the highest-ranking examples.

</details>


### [81] [VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation](https://arxiv.org/abs/2601.03792)
*Huynh Trung Kiet,Dao Sy Duy Minh,Nguyen Dinh Ha Duong,Le Hoang Minh Huy,Long Nguyen,Dien Dinh*

Main category: cs.CL

TL;DR: The paper builds VietMed-MCQ, a validated multiple-choice benchmark for Vietnamese Traditional Medicine, and uses it to evaluate several LLMs, showing cross-lingual transfer but persistent difficulty with complex diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well in general medical tasks but fail in niche, culturally specific domains like Vietnamese Traditional Medicine due to a lack of high-quality, structured benchmarks. The authors want to create a reliable evaluation dataset to fairly assess and ultimately improve LLM performance in this low-resource, specialized medical field.

Method: They construct VietMed-MCQ, a 3,190-question multiple-choice dataset using a Retrieval-Augmented Generation pipeline. The system retrieves relevant VTM documents, uses LLMs to generate questions and answers, and then applies a dual-model validation scheme where two independent models verify reasoning consistency and answers. An automated consistency check (including substring-based evidence checking) filters out low-quality items. The resulting questions are then human-validated by one medical expert and four students, and seven open-source models are benchmarked on the final dataset.

Result: The final dataset has 3,190 questions at three difficulty levels and passes human validation with 94.2% approval and substantial inter-rater reliability (Fleiss' kappa = 0.82). Benchmarking shows that general-purpose, Chinese-prior LLMs outperform Vietnamese-centric models on VietMed-MCQ, suggesting effective cross-lingual conceptual transfer, but all models perform poorly on items requiring complex diagnostic reasoning.

Conclusion: VietMed-MCQ offers a high-quality, publicly available benchmark for Vietnamese Traditional Medicine that can drive progress in low-resource medical NLP. The dual-model RAG-based generation pipeline yields a validated dataset, though evidence checking remains imperfect. Cross-lingual large models transfer some medical knowledge into VTM better than purely Vietnamese models, yet there remains a significant gap in handling complex diagnostic reasoning tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.

</details>


### [82] [Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning](https://arxiv.org/abs/2601.03823)
*Fei Wu,Zhenrong Zhang,Qikai Chang,Jianshu Zhang,Quan Liu,Jun Du*

Main category: cs.CL

TL;DR: The paper introduces SPAE, a fine-grained, step-level advantage estimation method for reinforcement learning with verifiable rewards that improves reasoning accuracy and efficiency in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods rely on coarse, outcome-based rewards, which can’t capture the quality of intermediate reasoning steps. This leads LLMs to over-verify, extend reasoning unnecessarily, and sometimes transform correct partial solutions into incorrect final answers. There is a need for a semantically meaningful, step-level measure of reasoning progress to better supervise long chain-of-thought reasoning.

Method: The authors design a training-free probing mechanism that, at each reasoning step, estimates intermediate confidence and correctness, and combines them into a scalar Step Potential reflecting the model’s current reasoning state. Using this Step Potential, they propose Step Potential Advantage Estimation (SPAE), a credit assignment method that (1) amplifies gains when potential increases, (2) penalizes drops when potential decreases, and (3) applies penalties when potential saturates to favor early stopping. SPAE is plugged into RLVR to provide fine-grained, process-level rewards without extra training of the probe.

Result: Across multiple reasoning benchmarks, SPAE improves answer accuracy while significantly shortening generated chains-of-thought compared with strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. This demonstrates both better reasoning quality and higher efficiency in terms of response length.

Conclusion: Step-level, semantically grounded process supervision via Step Potential and SPAE yields more reliable and efficient chain-of-thought reasoning in LLMs than coarse, outcome-based RLVR rewards. SPAE delivers consistent gains in accuracy and brevity and can be integrated into RLVR in a training-free manner, making it a practical enhancement for reasoning-focused RL with LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.

</details>


### [83] [Rethinking Table Pruning in TableQA: From Sequential Revisions to Gold Trajectory-Supervised Parallel Search](https://arxiv.org/abs/2601.03851)
*Yu Guo,Shenghao Ye,Shuangwu Chen,Zijian Wen,Tao Zhang,Qirui Bai,Dong Jin,Yunpeng Hou,Huasen He,Jian Yang,Xiaobin Tan*

Main category: cs.CL

TL;DR: The paper introduces TabTrim, a new framework for pruning tables in Table Question Answering, which learns from gold SQL execution trajectories and uses parallel search to produce compact sub-tables, yielding state-of-the-art accuracy on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing table pruning methods for TableQA rely on sequential editing guided by weak or unreliable critique signals. These approaches often cannot reliably tell when crucial, answer-relevant cells have been removed, leading to performance degradation. There is a need for a pruning framework that is directly supervised by ground-truth reasoning processes and can more effectively search for high-quality sub-tables.

Method: The authors propose TabTrim, which reframes table pruning as a gold trajectory-supervised parallel search problem. They extract a gold pruning trajectory from intermediate sub-tables that appear during the execution of gold SQL queries. Using these trajectories, they train two components: (1) a pruner that performs step-wise table pruning, and (2) a verifier that evaluates whether the pruned sub-tables align with the gold trajectory. At inference time, instead of a single sequential revision path, TabTrim performs parallel search over multiple pruning trajectories to find the optimal sub-table for downstream reasoning.

Result: On multiple tabular reasoning benchmarks, TabTrim achieves state-of-the-art accuracy. The 8B-parameter version, TabTrim-8B, attains an average accuracy of 73.5%, surpassing the best prior baseline by 3.2 percentage points. It specifically reaches 79.4% accuracy on WikiTQ and 61.2% on TableBench.

Conclusion: Transforming table pruning from sequential critique-based revision into a gold trajectory-supervised parallel search significantly improves TableQA performance. By leveraging gold SQL execution to supervise pruning and exploring multiple trajectories at inference, TabTrim produces more reliable, compact sub-tables and sets a new state of the art on several tabular reasoning benchmarks.

Abstract: Table Question Answering (TableQA) benefits significantly from table pruning, which extracts compact sub-tables by eliminating redundant cells to streamline downstream reasoning. However, existing pruning methods typically rely on sequential revisions driven by unreliable critique signals, often failing to detect the loss of answer-critical data. To address this limitation, we propose TabTrim, a novel table pruning framework which transforms table pruning from sequential revisions to gold trajectory-supervised parallel search. TabTrim derives a gold pruning trajectory using the intermediate sub-tables in the execution process of gold SQL queries, and trains a pruner and a verifier to make the step-wise pruning result align with the gold pruning trajectory. During inference, TabTrim performs parallel search to explore multiple candidate pruning trajectories and identify the optimal sub-table. Extensive experiments demonstrate that TabTrim achieves state-of-the-art performance across diverse tabular reasoning tasks: TabTrim-8B reaches 73.5% average accuracy, outperforming the strongest baseline by 3.2%, including 79.4% on WikiTQ and 61.2% on TableBench.

</details>


### [84] [What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs](https://arxiv.org/abs/2601.03858)
*Seyed Mahed Mousavi,Simone Alghisi,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: The paper examines continual pre-training (CPT) of LLMs as a process of factual knowledge learning, showing that decreasing loss does not reliably correspond to stable knowledge acquisition or general skill preservation.


<details>
  <summary>Details</summary>
Motivation: Continual pre-training is heavily used to update and expand LLM knowledge, but practitioners typically rely on loss as a proxy for learning progress, without understanding how factual knowledge and other skills actually change over training epochs. This lack of grounding can lead to miscalibrated training duration, unintended forgetting, and degraded out-of-domain abilities. The paper is motivated by the need to treat CPT as a knowledge learning phenomenon, not just an optimization task, and to better understand when and how models truly acquire, consolidate, or forget facts.

Method: The authors build a controlled benchmark of factual documents whose distribution matches CPT data and embed diagnostic probes directly within the CPT process. These probes enable epoch-level tracking of both factual knowledge acquisition and changes in out-of-domain capabilities such as math. They run experiments across three instruction-tuned LLMs and various CPT strategies, and perform circuit-level analyses to see how internal knowledge pathways are reconfigured over epochs.

Result: They find a systematic divergence between optimization and learning: training loss decreases monotonically, but factual learning is unstable and non-monotonic. Newly learned facts are seldom consolidated, learning depends strongly on prior exposure, and out-of-domain performance starts degrading from early epochs. Circuit analysis shows rapid reconfiguration of knowledge circuits across epochs, leading to narrow windows where facts can be acquired and a tendency toward systematic forgetting.

Conclusion: Loss minimization during CPT is not a reliable indicator of knowledge learning or preservation. Continual pre-training can induce unstable factual knowledge, dependency on prior exposure, and early degradation of out-of-domain skills due to rapid restructuring of knowledge circuits. The authors conclude that CPT should be evaluated and controlled using task-level learning dynamics and more appropriate stopping criteria, rather than relying chiefly on loss as the stopping signal.

Abstract: Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimization problem. We construct a controlled, distribution-matched benchmark of factual documents and interleave diagnostic probes directly into the CPT loop, enabling epoch-level measurement of knowledge acquisition dynamics and changes in Out-Of-Domain (OOD) general skills (e.g., math). We further analyze how CPT reshapes knowledge circuits during training. Across three instruction-tuned LLMs and multiple CPT strategies, optimization and learning systematically diverge as loss decreases monotonically while factual learning is unstable and non-monotonic. Acquired facts are rarely consolidated, learning is strongly conditioned on prior exposure, and OOD performance degrades from early epochs. Circuit analysis reveals rapid reconfiguration of knowledge pathways across epochs, providing an explanation for narrow acquisition windows and systematic forgetting. These results show that loss optimization is misaligned with learning progress in CPT and motivate evaluation of stopping criteria based on task-level learning dynamics.

</details>


### [85] [PartisanLens: A Multilingual Dataset of Hyperpartisan and Conspiratorial Immigration Narratives in European Media](https://arxiv.org/abs/2601.03860)
*Michele Joshua Maggini,Paloma Piot,Anxo Pérez,Erik Bran Marino,Lúa Santamaría Montesinos,Ana Lisboa,Marta Vázquez Abuín,Javier Parapar,Pablo Gamallo*

Main category: cs.CL

TL;DR: The paper presents PartisanLens, a new multilingual dataset of hyperpartisan news headlines (including population replacement conspiracy theories) in Spanish, Italian, and Portuguese, and evaluates large language models both as classifiers and as potential automatic annotators that might emulate human ideological perspectives.


<details>
  <summary>Details</summary>
Motivation: Hyperpartisan narratives and population replacement conspiracy theories fuel polarization, institutional distrust, and even real-world extremist violence, yet current computational resources are scarce, English-centric, and typically treat hyperpartisanship, stance, and rhetorical bias separately. There is a need for multilingual, multi-aspect resources and for understanding how well LLMs can detect and annotate such narratives, particularly in European languages and contexts.

Method: The authors construct PartisanLens, a dataset of 1,617 hyperpartisan news headlines in Spanish, Italian, and Portuguese, annotated for multiple political discourse aspects including hyperpartisanship and PRCT content. They benchmark the performance of widely used LLMs as classifiers on this dataset to set baselines for detecting hyperpartisan and conspiratorial narratives. They then test LLMs as automatic annotators by comparing their labels with human annotations, and further explore whether LLMs can mimic human annotation patterns when conditioned on simulated socio-economic and ideological profiles of annotators.

Result: The evaluation shows that current LLMs can perform non-trivially on classifying hyperpartisan and PRCT narratives in these languages, but they still have notable limitations compared with human annotators. As automatic annotators, LLMs can approximate human labels to some extent, yet their agreement and reliability are not sufficient to fully replace humans. Conditioning LLMs on socio-economic and ideological profiles yields some shifts in annotation patterns, suggesting partial ability to emulate perspective-driven judgments, but this emulation is imperfect and constrained.

Conclusion: PartisanLens fills an important gap by providing a multilingual, multi-aspect dataset for studying partisan and conspiratorial political narratives in European languages. The authors conclude that while LLMs offer promising capabilities for classification and semi-automatic annotation in this domain, they currently have clear limitations and should be used with caution, ideally in conjunction with human oversight. The released dataset and baselines are intended to support future work on detecting and understanding partisan and conspiratorial narratives in European contexts.

Abstract: Detecting hyperpartisan narratives and Population Replacement Conspiracy Theories (PRCT) is essential to addressing the spread of misinformation. These complex narratives pose a significant threat, as hyperpartisanship drives political polarisation and institutional distrust, while PRCTs directly motivate real-world extremist violence, making their identification critical for social cohesion and public safety. However, existing resources are scarce, predominantly English-centric, and often analyse hyperpartisanship, stance, and rhetorical bias in isolation rather than as interrelated aspects of political discourse. To bridge this gap, we introduce \textsc{PartisanLens}, the first multilingual dataset of \num{1617} hyperpartisan news headlines in Spanish, Italian, and Portuguese, annotated in multiple political discourse aspects. We first evaluate the classification performance of widely used Large Language Models (LLMs) on this dataset, establishing robust baselines for the classification of hyperpartisan and PRCT narratives. In addition, we assess the viability of using LLMs as automatic annotators for this task, analysing their ability to approximate human annotation. Results highlight both their potential and current limitations. Next, moving beyond standard judgments, we explore whether LLMs can emulate human annotation patterns by conditioning them on socio-economic and ideological profiles that simulate annotator perspectives. At last, we provide our resources and evaluation, \textsc{PartisanLens} supports future research on detecting partisan and conspiratorial narratives in European contexts.

</details>


### [86] [Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning](https://arxiv.org/abs/2601.03872)
*Jinyang Wu,Guocheng Zhai,Ruihan Jin,Jiahao Yuan,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.CL

TL;DR: A framework (ATLAS) that dynamically selects and routes between different LLMs and tools to solve complex, cross-domain tasks, outperforming strong baselines in both familiar and novel settings.


<details>
  <summary>Details</summary>
Motivation: Using external tools with LLMs greatly increases their capabilities, but as both models and tools proliferate, it becomes hard to choose the best model-tool combination for each task. Existing methods typically use a single LLM or fixed tool-calling strategies, ignoring that different model-tool pairs can perform very differently depending on the domain and distribution shift. The paper aims to systematically exploit these performance variations through smarter, adaptive routing.

Method: The paper proposes ATLAS, a dual-path framework for tool usage and model routing in complex reasoning tasks across domains. The first path is a training-free, cluster-based routing mechanism that uses empirical priors: it clusters tasks or domains and routes queries to model-tool pairs that historically worked best for similar clusters. The second path is an RL-based multi-step routing strategy that learns policies to sequentially select tools and models over multiple steps, exploring new tool trajectories to handle out-of-distribution queries. Together, these two paths allow both fast, prior-based alignment and exploratory, learned routing for robust performance.

Result: On 15 benchmarks, ATLAS outperforms strong baselines, including closed-source models like GPT-4o, and surpasses existing routing approaches. It achieves substantial performance improvements: +10.1% on in-distribution tasks and +13.1% on out-of-distribution tasks. The framework also yields notable gains in visual reasoning tasks by coordinating specialized multimodal tools.

Conclusion: Dynamic, adaptive routing over heterogeneous LLMs and tools is crucial for maximizing performance in complex, cross-domain reasoning. By combining training-free cluster-based routing with RL-based multi-step routing, ATLAS effectively aligns tasks with suitable model-tool combinations, leading to superior performance over fixed, single-model, or static routing baselines, and generalizes better to out-of-distribution settings, including multimodal scenarios.

Abstract: The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.

</details>


### [87] [Evaluating Small Decoder-Only Language Models for Grammar Correction and Text Simplification](https://arxiv.org/abs/2601.03874)
*Anthony Lamelas*

Main category: cs.CL

TL;DR: The paper evaluates whether small decoder-only language models (SLMs) can effectively perform grammar correction and text simplification, finding that they are still significantly worse than strong baselines and modern LLMs, especially in preserving meaning and avoiding hallucinations.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are powerful but expensive to run, deploy, and secure, which limits their use in many real-world scenarios. There is a need to explore whether much smaller models could provide comparable performance for specific rewriting tasks like grammar correction and text simplification, offering a more efficient and accessible alternative.

Method: The authors evaluate small decoder-only language models on grammar correction and text simplification tasks using the JFLEG and ASSET datasets. They compare three usage modes: (1) using the small models directly (out-of-the-box), (2) fine-tuning them for the specific tasks, and (3) running them sequentially in multi-step pipelines. Established evaluation metrics for these tasks are used to compare their performance with strong baselines and modern large language models.

Result: The experiments show that while small language models can pick up some useful rewriting behaviors, their overall performance on grammar correction and text simplification lags behind strong baselines and state-of-the-art large language models. They particularly struggle with preserving the original meaning of the text and are prone to hallucinations (unwarranted or invented changes).

Conclusion: Current small decoder-only language models, despite being more efficient and easier to deploy, are not yet competitive with modern large language models for grammar correction and text simplification. Their issues with meaning preservation and hallucination indicate that further improvements in training strategies and model design are needed before they can close the performance gap and serve as a viable alternative for rewriting tasks.

Abstract: Large language models have become extremely popular recently due to their ability to achieve strong performance on a variety of tasks, such as text generation and rewriting, but their size and computation cost make them difficult to access, deploy, and secure in many settings. This paper investigates whether small, decoder-only language models can provide an efficient alternative for the tasks of grammar correction and text simplification. The experiments in this paper focus on testing small language models out of the box, fine-tuned, and run sequentially on the JFLEG and ASSET datasets using established metrics. The results show that while SLMs may learn certain behaviors well, their performance remains below strong baselines and current LLMs. The results also show that SLMs struggle with retaining meaning and hallucinations. These findings suggest that despite their efficiency advantages, current SLMs are not yet competitive enough with modern LLMs for rewriting, and further advances in training are required for SLMs to close the performance gap between them and today's LLMs.

</details>


### [88] [Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval](https://arxiv.org/abs/2601.03908)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: The paper proposes Decide Then Retrieve (DTR), a training-free framework that selectively triggers retrieval based on model uncertainty and uses dual-path, adaptively filtered evidence to improve RAG for open-domain QA.


<details>
  <summary>Details</summary>
Motivation: Standard retrieval-augmented generation (RAG) pipelines always perform retrieval and typically rely on a single chain of retrieved evidence. This indiscriminate retrieval can add noise, waste computation, and even harm answer quality, especially for sparse or ambiguous queries where the retrieved documents are unreliable. The authors aim to build a more efficient and accurate RAG framework that only retrieves when needed and can better exploit external knowledge under challenging query conditions.

Method: DTR is a training-free framework that sits on top of existing LLMs and retrievers. It first uses the LLMs generation uncertainty as a signal to decide whether to trigger retrieval at all. If retrieval is deemed necessary, DTR employs a dual-path retrieval mechanism: two complementary retrieval or evidence-construction paths are used to gather external knowledge. An adaptive information selection module then filters and integrates the retrieved content, choosing which pieces of evidence from the two paths should be fed back to the LLM. This process aims to reduce noisy context while retaining useful information for answer generation.

Result: Across five open-domain QA benchmarks, with various LLM sizes and different retrieval backends, DTR consistently improves exact match (EM) and F1 scores compared to standard RAG setups and strong retrieval-enhanced baselines. At the same time, it reduces the number of retrieval calls, indicating better efficiency and less unnecessary use of external knowledge.

Conclusion: Decide Then Retrieve demonstrates that selectively triggering retrieval based on model uncertainty and using dual-path, adaptively filtered evidence can make RAG both more accurate and more efficient. The framework is training-free and model-agnostic, making it easy to plug into existing RAG pipelines to obtain better performance on open-domain QA with fewer redundant retrieval operations.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.

</details>


### [89] [When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering](https://arxiv.org/abs/2601.03914)
*Hugh Mee Wong,Rick Nouwen,Albert Gatt*

Main category: cs.CL

TL;DR: The paper analyzes how language models internally perform multiple-choice question answering (MCQA), showing evidence for a two-stage process: first selecting the correct option in content space, then mapping that choice to the output symbol.


<details>
  <summary>Details</summary>
Motivation: MCQA is widely used because it is easy to automatically evaluate, but it entangles two abilities: reasoning about which option is correct and correctly outputting the symbol (A/B/C/…), so errors can come from either step. The authors want to understand how language models internally implement MCQA, and specifically how they represent option correctness and bind it to the answer symbol.

Method: The authors use representational analyses such as principal component analysis (PCA) and linear probes on residual stream states, focusing on representations at the boundaries between answer options (e.g., after newlines). They also perform causal interventions on these internal states. They probe for signals of per-option correctness and for the identity of the ultimately selected winner, and examine how these signals evolve across layers and positions, including under permutations of symbols and option contents.

Result: They find that residual states at option boundaries (e.g., at newlines between choices) contain strong linearly decodable information about whether each option is correct. Probing for the winner identity shows a temporal progression: right after reading the final option, the model’s internal states encode which *content position* (e.g., second option) is correct, but not yet the final output symbol; the explicit symbol corresponding to the winning option emerges later, closer to where the model emits the answer. Symbol and content permutation tests further show that the model’s internal process is robust to these changes and that the representation separates content selection from symbol assignment.

Conclusion: The paper concludes that language models likely implement MCQA using a two-stage internal mechanism. First, they select a winning option in a content-based representation space (decoding which option is correct immediately after reading all options). Second, they carry out a symbol-binding or routing step that maps the chosen content position to the appropriate answer symbol at the output stage. This clarifies how reasoning and symbol production are separated in the model’s internal computation and explains how MCQA performance can conflate reasoning accuracy with symbol-binding reliability.

Abstract: Multiple-choice question answering (MCQA) is easy to evaluate but adds a meta-task: models must both solve the problem and output the symbol that *represents* the answer, conflating reasoning errors with symbol-binding failures. We study how language models implement MCQA internally using representational analyses (PCA, linear probes) as well as causal interventions. We find that option-boundary (newline) residual states often contain strong linearly decodable signals related to per-option correctness. Winner-identity probing reveals a two-stage progression: the winning *content position* becomes decodable immediately after the final option is processed, while the *output symbol* is represented closer to the answer emission position. Tests under symbol and content permutations support a two-stage mechanism in which models first select a winner in content space and then bind or route that winner to the appropriate symbol to emit.

</details>


### [90] [Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2601.03926)
*Haeun Jang,Hwan Chang,Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper introduces Doc-PP, a benchmark for testing how well large vision-language models obey explicit non-disclosure policies in document QA, finds that reasoning across modalities creates a safety gap causing sensitive information leakage, and proposes DVA, a decomposition-based framework that better enforces policies than standard prompting defenses.


<details>
  <summary>Details</summary>
Motivation: Real-world document QA systems must follow explicit, user-defined disclosure policies, but current safety work focuses on implicit social norms and text-only setups, neglecting multimodal documents and structured policies. There is a need to understand and mitigate how LVLMs can inadvertently leak sensitive information when performing complex reasoning over documents that mix text and visuals under strict access policies.

Method: The authors build Doc-PP, a benchmark based on real-world reports that require cross-modal, multi-step reasoning under explicit non-disclosure policies. They empirically evaluate existing LVLMs and safety prompting strategies on this benchmark, analyzing when and how policy violations occur. They then design DVA (Decompose-Verify-Aggregation), a structural inference framework that decomposes the task into separate reasoning and policy-checking stages, verifies intermediate reasoning steps against policies, and aggregates only policy-compliant information into final answers.

Result: Experiments on Doc-PP show that state-of-the-art LVLMs frequently violate explicit non-disclosure policies, especially when answers require complex, cross-modal reasoning or aggregation, revealing a Reasoning-Induced Safety Gap. Providing extracted text to models improves their perceptual understanding but increases the rate of sensitive information leakage. The proposed DVA framework significantly reduces policy violations and outperforms standard safety prompting defenses across evaluated settings, while maintaining useful task performance.

Conclusion: Explicit policy compliance in multimodal document QA is not reliably ensured by current LVLMs or common prompting-based safety defenses, especially in scenarios requiring sophisticated reasoning. The Doc-PP benchmark exposes a systematic Reasoning-Induced Safety Gap, emphasizing the need for architectures that decouple reasoning from safety enforcement. DVA offers an effective baseline by structurally separating inference and policy verification, demonstrating that more robust, policy-aware pipelines are feasible for safer document understanding.

Abstract: The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding

</details>


### [91] [RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection](https://arxiv.org/abs/2601.03981)
*Song-Duo Ma,Yi-Hung Liu,Hsin-Yu Lin,Pin-Yu Chen,Hong-Yan Huang,Shau-Yung Hsu,Yun-Nung Chen*

Main category: cs.CL

TL;DR: RADAR is a retrieval-augmented, adversarially refined system for detecting LLM-generated fake news, using a generator-detector framework enhanced by natural-language feedback.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate convincing fake news at scale, and existing detectors, including LLM-based ones, struggle to robustly identify such misinformation, especially when adversaries adapt. There is a need for a more resilient, retrieval-augmented detection mechanism that co-evolves with adversarial generators.

Method: RADAR combines (1) a generator that rewrites real news articles with factual perturbations to synthesize realistic fake news, and (2) a lightweight detector that verifies article claims via dense passage retrieval over external evidence. To make the system robust, they introduce verbal adversarial feedback (VAF), where the detector provides structured natural-language critiques instead of scalar rewards. These critiques explain why generated articles were detected, pushing the generator to create more evasive fakes and forcing the detector to adapt through iterative training. Ablation studies examine the contributions of retrieval, VAF, and few-shot demonstrations.

Result: On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, outperforming general-purpose LLMs augmented with retrieval. Ablations show that detector-side retrieval contributes the largest performance improvements, and that VAF plus few-shot demonstrations are important signals for achieving robustness.

Conclusion: A co-evolving generator–detector framework with retrieval and verbal adversarial feedback can substantially improve robustness in detecting LLM-generated fake news, with retrieval at detection time being particularly impactful and structured language feedback serving as an effective training signal alongside few-shot examples.

Abstract: To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.

</details>


### [92] [Benchmark^2: Systematic Evaluation of LLM Benchmarks](https://arxiv.org/abs/2601.03986)
*Qi Qian,Chengsong Huang,Jingwen Xu,Changze Lv,Muling Wu,Wenhao Liu,Xiaohua Wang,Zhenghua Wang,Zisu Huang,Muzhao Tian,Jianhan Xu,Kun Hu,He-Da Wang,Yao Hu,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CL

TL;DR: The paper introduces Benchmark^2, a framework to evaluate the quality of benchmarks used for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: With many benchmarks emerging to evaluate LLMs, it is unclear which benchmarks are reliable or high quality. There is a need for systematic, quantitative measures to compare and select benchmarks themselves, rather than assuming all are equally valid.

Method: The authors design Benchmark^2, which consists of three metrics: (1) Cross-Benchmark Ranking Consistency, which checks whether a benchmark’s model ranking agrees with rankings from other benchmarks; (2) Discriminability Score, which measures how well a benchmark distinguishes between different models; and (3) Capability Alignment Deviation, which detects cases where stronger models in a family fail on items that weaker models succeed on, flagging potentially problematic items. They apply these metrics to 15 existing benchmarks covering math, reasoning, and knowledge, testing 11 LLMs from four model families.

Result: The evaluation shows that existing benchmarks vary widely in quality under these three metrics. Some benchmarks are more consistent, more discriminative, and have fewer problematic items than others. By selecting and subsetting benchmarks using Benchmark^2 metrics, the authors can create smaller test sets that still provide evaluation quality comparable to using full, larger benchmark suites.

Conclusion: Benchmark^2 provides a practical, quantitative framework to assess and improve benchmark quality for LLMs. Using these metrics enables more efficient and reliable evaluation: practitioners can choose or construct smaller, higher-quality benchmark subsets while preserving robust model assessment.

Abstract: The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.

</details>


### [93] [VotIE: Information Extraction from Meeting Minutes](https://arxiv.org/abs/2601.03997)
*José Pedro Evans,Luís Filipe Cunha,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: The paper introduces VotIE, a benchmark and task for extracting structured voting events from heterogeneous, narrative municipal meeting minutes, comparing encoder-based and generative models in both in-domain and cross-municipality settings.


<details>
  <summary>Details</summary>
Motivation: Municipal meeting minutes contain crucial democratic decisions, but unlike standardized parliamentary records, they are written as heterogeneous, free-form narratives that differ across municipalities. This makes it difficult to automatically extract structured information about voting outcomes, limiting large-scale analysis and administrative NLP applications. There was no dedicated task definition or benchmark for this problem, especially in Portuguese municipal data.

Method: The authors define a new information extraction task, Voting Information Extraction (VotIE), focused on identifying structured voting events from narrative municipal minutes. They build the first benchmark for this task using Portuguese municipal minutes from the CitiLink corpus. They evaluate different modeling approaches: fine-tuned encoder models (notably XLM-R with a CRF layer) and generative large language models, under two evaluation regimes: standard in-domain evaluation and cross-municipality transfer to unseen administrative contexts.

Result: In in-domain evaluation, fine-tuned encoder models perform best: XLM-R-CRF reaches 93.2% macro F1 and outperforms generative approaches. However, when tested in a cross-municipality setting, encoder models’ performance drops sharply, showing poor generalization to unseen municipalities, while few-shot LLMs exhibit better robustness with much smaller performance declines.

Conclusion: For current practical deployment at scale, lightweight fine-tuned encoder models are preferable because they are computationally efficient and achieve strong in-domain performance, despite weaker cross-municipality generalization. Few-shot generative LLMs generalize better across municipalities but are computationally expensive, limiting their immediate practicality. The authors release the benchmark, models, and evaluation framework to foster reproducible and extensible research in administrative NLP.

Abstract: Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.

</details>


### [94] [Simulated Students in Tutoring Dialogues: Substance or Illusion?](https://arxiv.org/abs/2601.04025)
*Alexander Scarlatos,Jaewook Lee,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: The paper studies how to create and evaluate realistic simulated students for LLM-powered tutoring systems, finding that naive prompting performs poorly and more advanced training helps but is still limited.


<details>
  <summary>Details</summary>
Motivation: LLM-based tutoring tools need many student interactions for training and evaluation, but using real students is slow, costly, and hard to scale. Researchers therefore rely on simulated students, yet there is little understanding of how to define this simulation task or assess whether these simulated students are realistic and useful.

Method: The authors formally define the student simulation task and design evaluation metrics covering linguistic (how the student talks), behavioral (interaction patterns), and cognitive (knowledge and learning) dimensions. They then implement and benchmark multiple student-simulation approaches—including prompt-based methods, supervised fine-tuning, and preference-optimized models—on a real-world math tutoring dialogue dataset, using both automatic metrics and human judgments.

Result: Experiments show that simple prompting-based simulated students perform poorly across many dimensions of the proposed metrics. Models trained via supervised fine-tuning and via preference optimization generate more realistic and higher-quality student behavior, but their performance is still far from ideal according to both automated scores and human evaluators.

Conclusion: Student simulation is a challenging and underexplored task. While more advanced methods like fine-tuning and preference optimization improve over basic prompting, they still have notable gaps, indicating substantial room for future research on creating accurate, reliable simulated students for LLM-powered tutoring systems.

Abstract: Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.

</details>


### [95] [SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency](https://arxiv.org/abs/2601.04029)
*Jonggeun Lee,Junseong Pyo,Gyuhyeon Seo,Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces SpeakerSleuth, a benchmark to test whether large audio-language models can judge speaker consistency across multi-turn dialogues, revealing that current models are biased toward textual information and struggle with acoustic consistency judgments.


<details>
  <summary>Details</summary>
Motivation: While large audio-language models are increasingly used to evaluate speech generation, their ability to assess whether the same speaker remains consistent over multi-turn conversations has not been systematically studied. This is important for realistic evaluation of dialogue systems, voice cloning, and multi-speaker scenarios, where identifying speaker changes or inconsistencies is crucial.

Method: The authors build SpeakerSleuth, a benchmark consisting of 1,818 human-verified instances drawn from four varied datasets that cover both synthetic and real speech and systematically vary acoustic difficulty. They define three tasks mirroring real-world evaluation needs, including detecting inconsistencies across a speaker’s turns, localizing which turns are problematic, and selecting the best-matching audio from multiple variants. They then evaluate nine popular large audio-language models on these tasks.

Result: Across the benchmark, current large audio-language models show poor reliability in detecting acoustic inconsistencies in multi-turn conversations. Some models incorrectly flag consistent speakers as inconsistent, while others miss clear inconsistencies, and all struggle to accurately pinpoint which turns are inconsistent. When other interlocutors’ turns are included, performance drops sharply as models focus on textual coherence and overlook acoustic mismatches, even missing salient cues like gender changes. However, models perform much better on forced-choice matching tasks where they must pick the audio that best matches a given speaker, indicating they possess underlying acoustic discrimination abilities.

Conclusion: The study concludes that large audio-language models exhibit a strong bias toward textual information over acoustic cues when acting as judges of multi-turn dialogue, leading to unreliable assessments of speaker consistency. This reveals a modality imbalance that must be addressed—through training, prompting, or architectural changes—before such models can serve as dependable audio-language evaluators in real-world, multi-speaker settings.

Abstract: Large Audio-Language Models (LALMs) as judges have emerged as a prominent approach for evaluating speech generation quality, yet their ability to assess speaker consistency across multi-turn conversations remains unexplored. We present SpeakerSleuth, a benchmark evaluating whether LALMs can reliably judge speaker consistency in multi-turn dialogues through three tasks reflecting real-world requirements. We construct 1,818 human-verified evaluation instances across four diverse datasets spanning synthetic and real speech, with controlled acoustic difficulty. Evaluating nine widely-used LALMs, we find that models struggle to reliably detect acoustic inconsistencies. For instance, given audio samples of the same speaker's turns, some models overpredict inconsistency, whereas others are overly lenient. Models further struggle to identify the exact turns that are problematic. When other interlocutors' turns are provided together, performance degrades dramatically as models prioritize textual coherence over acoustic cues, failing to detect even obvious gender switches for a speaker. On the other hand, models perform substantially better in choosing the audio that best matches the speaker among several acoustic variants, demonstrating inherent acoustic discrimination capabilities. These findings expose a significant bias in LALMs: they tend to prioritize text over acoustics, revealing fundamental modality imbalances that need to be addressed to build reliable audio-language judges.

</details>


### [96] [Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation](https://arxiv.org/abs/2601.04036)
*David Stap*

Main category: cs.CL

TL;DR: The thesis investigates how multilingual neural models transfer knowledge across languages, focusing on improving robustness and performance for low-resource machine translation through better representations, training strategies, and data composition.


<details>
  <summary>Details</summary>
Motivation: Multilingual machine translation aims to make information accessible across languages, but current systems struggle with effective cross-lingual representations, particularly for low-resource languages where parallel data is scarce. There is limited understanding of how language similarity, data availability, and training strategies interact to affect cross-lingual knowledge transfer and generalization.

Method: The thesis uses neural machine translation as the primary testbed to empirically analyze cross-lingual transfer, studying: (1) how linguistic similarity affects transfer; (2) how retrieval-based methods and auxiliary supervision can enhance low-resource translation; (3) how fine-tuning large language models on parallel data introduces trade-offs; and (4) how varying language diversity and translation coverage during training impacts generalization and off-target behavior.

Result: The work finds that language similarity significantly influences transfer effectiveness, retrieval and auxiliary supervision can strengthen low-resource translation, fine-tuning on parallel data can lead to unintended trade-offs in large language models, and increasing language diversity and translation coverage during training improves generalization and reduces off-target translation behavior.

Conclusion: Modeling choices (e.g., use of retrieval, auxiliary supervision, fine-tuning strategies) and data composition (e.g., language diversity and coverage) critically shape multilingual learning. Thoughtful design of these factors leads to more robust, generalizable, and inclusive multilingual NLP systems, particularly benefiting low-resource languages.

Abstract: Multilingual machine translation systems aim to make knowledge accessible across languages, yet learning effective cross-lingual representations remains challenging. These challenges are especially pronounced for low-resource languages, where limited parallel data constrains generalization and transfer. Understanding how multilingual models share knowledge across languages requires examining the interaction between representations, data availability, and training strategies. In this thesis, we study cross-lingual knowledge transfer in neural models and develop methods to improve robustness and generalization in multilingual settings, using machine translation as a central testbed. We analyze how similarity between languages influences transfer, how retrieval and auxiliary supervision can strengthen low-resource translation, and how fine-tuning on parallel data can introduce unintended trade-offs in large language models. We further examine the role of language diversity during training and show that increasing translation coverage improves generalization and reduces off-target behavior. Together, this work highlights how modeling choices and data composition shape multilingual learning and offers insights toward more inclusive and resilient multilingual NLP systems.

</details>


### [97] [When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life](https://arxiv.org/abs/2601.04043)
*Xinyue Lou,Jinan Xu,Jingyi Yin,Xiaolong Wang,Zhaolu Kang,Youwei Liao,Yixuan Wang,Xiangyu Shi,Fengran Mo,Su Yao,Kaiyu Huang*

Main category: cs.CL

TL;DR: SaLAD is a multimodal safety benchmark and evaluation framework to test how well MLLMs handle real-world, safety-critical image–text scenarios, showing current models are still unsafe and oversensitive.


<details>
  <summary>Details</summary>
Motivation: MLLMs are increasingly used as assistants in daily life, but they can generate unsafe guidance or overcautious refusals. Existing safety benchmarks often focus on text-only inputs or unrealistic scenarios, which miss many practical, visually grounded risks. The paper aims to systematically measure how MLLMs’ responses could impact human behavior in realistic, multimodal, daily-life situations.

Method: The authors construct SaLAD, a benchmark of 2,013 real-world image–text samples spanning 10 daily-life categories, carefully balanced between unsafe cases (where harmful behavior is possible) and oversensitive cases (where cautious help is still appropriate). The design enforces that safety judgments require joint reasoning over images and text, making it impossible to infer risk from text alone. They also introduce a safety-warning-based evaluation framework that scores models on whether they give clear, specific safety warnings instead of generic refusals. They then evaluate 18 existing MLLMs and common safety-alignment strategies on SaLAD.

Result: Across 18 tested MLLMs, even the best models safely respond to only 57.2% of unsafe queries in SaLAD. Models frequently miss visually grounded risks or respond with vague, generic refusals that do not provide actionable safety guidance. Standard safety-alignment techniques, while effective on traditional benchmarks, do not significantly improve performance on SaLAD’s realistic multimodal scenarios.

Conclusion: SaLAD exposes significant safety gaps in current MLLMs when handling real-world, multimodal daily-life situations. Safety judgments often fail when they depend on fine-grained image–text reasoning, and existing alignment methods are insufficient for such cases. The benchmark and evaluation framework provide a more realistic way to assess and improve multimodal safety, encouraging future models to issue specific safety warnings rather than blanket refusals.

Abstract: As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.

</details>


### [98] [Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients](https://arxiv.org/abs/2601.04055)
*Prith Sharma,Austin Z. Henley*

Main category: cs.CL

TL;DR: The paper proposes Modular Prompt Optimization (MPO), a schema-based method that optimizes different semantic sections of a prompt separately to improve LLM reasoning performance, especially for smaller open-source models.


<details>
  <summary>Details</summary>
Motivation: Existing automatic prompt optimization methods treat prompts as a single block of text, which makes it hard to localize errors, preserve key instructions, and avoid uncontrolled prompt bloat. Smaller open-source instruction-tuned models are highly sensitive to prompt structure and need more precise, interpretable control over prompt components.

Method: The authors define prompts as structured objects with fixed sections (e.g., system role, context, task description, constraints, output format). They use a critic LLM to generate section-local textual gradients and refine each section independently while keeping the overall schema fixed. They then de-duplicate and consolidate section updates to reduce redundancy and interference across components, yielding a controlled and interpretable optimization process.

Result: On ARC-Challenge and MMLU benchmarks, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models, MPO consistently outperforms both an untuned structured prompt and the TextGrad prompt optimization baseline, achieving notable accuracy improvements without changing model weights or the high-level schema.

Conclusion: Maintaining a fixed prompt schema while applying localized, section-wise textual updates is an effective and practical way to boost the reasoning performance of smaller open-source LLMs. Structured, modular optimization yields more interpretable, robust, and efficient prompt improvements than monolithic prompt optimization approaches.

Abstract: Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.

</details>


### [99] [Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion](https://arxiv.org/abs/2601.04056)
*Yuanfeng Xu,Yuhao Chen,Liang Lin,Guangrun Wang*

Main category: cs.CL

TL;DR: Proposes CoM-DAD, a unified multimodal generative framework that couples continuous latent diffusion for semantics with discrete absorbing diffusion for token generation, enabling stable and scalable text–image generation.


<details>
  <summary>Details</summary>
Motivation: Current generative modeling is split: autoregressive models work well for discrete text, diffusion models for continuous images. Masked language models are efficient and bidirectional but underperform in generative quality and semantic continuity. Extending masked generation to multimodal (text–image) setups brings hard alignment problems and unstable training, and existing multimodal systems often need heavy dual-encoder contrastive architectures. The paper aims to build a truly unified and stable multimodal generator that overcomes these limitations.

Method: Introduce CoM-DAD (Coupled Manifold Discrete Absorbing Diffusion), a hierarchical dual-process generative framework. At a high level, it models a continuous semantic manifold using a latent diffusion process, which plans global semantics. At a low level, it generates concrete tokens (for different modalities) via a discrete absorbing diffusion process controlled by a variable-rate noise schedule. Token diffusion is conditioned on the evolving semantic latent, effectively decoupling semantic planning from token synthesis. To align different modalities (e.g., text and images) without large contrastive dual encoders, it proposes a Stochastic Mixed-Modal Transport mechanism that couples their representations during training and sampling.

Result: Empirically, CoM-DAD achieves more stable training and generation than standard masked modeling approaches in multimodal settings. It yields improved or competitive performance on unified text–image generation tasks, demonstrating that the coupled continuous–discrete diffusion design and the stochastic mixed-modal transport can handle multimodal alignment effectively. The framework scales to larger models without the instability typically seen in masked multimodal generators.

Conclusion: By reformulating multimodal generation as a hierarchical two-stage process—continuous semantic diffusion followed by discrete absorbing diffusion for tokens—CoM-DAD offers a stable, scalable, and unified framework for text–image generation. It bridges the gap between autoregressive, diffusion, and masked modeling paradigms, mitigates multimodal alignment challenges without heavy dual encoders, and suggests a new direction for building general-purpose multimodal generative systems.

Abstract: The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \textbf{CoM-DAD} (\textbf{Co}upled \textbf{M}anifold \textbf{D}iscrete \textbf{A}bsorbing \textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.

</details>


### [100] [KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures](https://arxiv.org/abs/2601.04086)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: The paper proposes a code-guided, knowledge-graph-based reasoning and distillation framework to reduce prompt-induced hallucinations in LLMs, significantly boosting accuracy (HIT@k) and reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models often hallucinate, especially due to prompt-induced errors in multi-step reasoning. Existing methods either lack explicit control over intermediate reasoning or underutilize external structured knowledge such as knowledge graphs. There is a need for a framework that can systematically constrain and guide reasoning using verifiable external knowledge while maintaining the flexibility of LLMs.

Method: The authors extend chain-style knowledge distillation by embedding a programmable module as executable code into the reasoning prompt. This module orchestrates exploration over an external knowledge graph and feeds structured information back into the reasoning process. The framework uses distillation to train student models so that their intermediate reasoning steps are explicitly regulated by the knowledge-graph-guided code. The approach is evaluated with GPT-4 and LLaMA-3.3 on several public benchmarks.

Result: On multiple benchmarks, the proposed code-guided reasoning framework improves contextual modeling and reduces prompt-induced hallucinations. Quantitatively, HIT@1, HIT@3, and HIT@5 are improved by 15.64%, 13.38%, and 13.28% respectively, with final scores surpassing 95% in several evaluation settings, indicating strong performance gains in answer ranking and reliability.

Conclusion: Embedding executable, knowledge-graph-guiding code within prompts enables LLMs to ground their reasoning in external structured knowledge. This design, combined with an enhanced distillation framework that supervises intermediate steps, effectively constrains erroneous reasoning, reduces hallucinations, and improves both accuracy and interpretability on standard benchmarks.

Abstract: To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.

</details>


### [101] [SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks](https://arxiv.org/abs/2601.04093)
*Yu Yan,Sheng Sun,Mingfeng Li,Zheming Yang,Chiwei Zhu,Fei Ma,Benfeng Xu,Min Liu*

Main category: cs.CL

TL;DR: The paper introduces SearchAttack, a red-teaming method that exploits web search as an attack surface for search-augmented LLMs by offloading harmful content to external search and guiding LLMs to reconstruct it, revealing serious safety vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Search-augmented LLMs are used to improve reliability on open, knowledge-intensive tasks, but when harmful queries trigger web search, unsafe content from search results can bypass the LLM’s internal safeguards. Since the LLM cannot retroactively filter exposure once harmful, ready-to-use content is retrieved, web search becomes a critical, underexplored vulnerability. The paper is motivated by the need to systematically evaluate and stress-test this attack surface for safety assessment.

Method: The authors propose SearchAttack, a red-teaming framework that constructs adversarial prompts for search-augmented LLMs. It works by (1) externalizing the harmful semantics into the web search process, leaving only a benign-looking query skeleton with scattered hints; (2) using those fragments to elicit search results that contain the targeted harmful information; and (3) employing structural rubrics in prompts that guide the LLM to reconstruct, organize, and present the harmful content retrieved from the web. They then run extensive experiments attacking multiple search-augmented LLM systems to measure attack success and robustness.

Result: Experiments show that SearchAttack can reliably cause search-augmented LLMs to output harmful, detailed content despite their safety filters. Across a range of models and settings, the attack achieves high success rates in eliciting targeted harmful outputs, indicating that current defenses are inadequate when web search is involved.

Conclusion: The study concludes that web search is a major attack surface for search-augmented LLMs and that current safety mechanisms focused only on the LLM side are insufficient. SearchAttack provides a systematic red-teaming method to uncover these vulnerabilities, underscoring the need for joint defenses spanning both retrieval and generation components in search-augmented systems.

Abstract: Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \textbf{\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.

</details>


### [102] [LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation](https://arxiv.org/abs/2601.04135)
*Leonardo Bottona,Nicolò Penzo,Bruno Lepri,Marco Guerini,Sara Tonelli*

Main category: cs.CL

TL;DR: LLMberjack is an open-source tool to turn debate reply trees into coherent multi-party dialogue transcripts, with visualization and optional LLM help.


<details>
  <summary>Details</summary>
Motivation: Existing multi-party debate data is often stored as complex reply/discussion trees, which are hard to transform into coherent, linear dialogues that preserve who said what and how utterances relate. There is a lack of transparent, reproducible tools and resources for constructing high-quality multi-party conversation datasets from such structures.

Method: The authors build an interactive platform that (1) visualizes tree-structured discussions; (2) lets users select and order nodes to form linear conversation threads while maintaining speaker identities and discourse relations; and (3) optionally invokes LLMs to help edit message text and generate/refine speakers’ descriptions. The system is implemented as an open-source tool emphasizing transparent workflows.

Result: The platform enables users to transform complex reply trees into coherent conversation threads more easily. The authors show that visualizing the tree helps users build meaningful, consistent dialogues, and that the integrated LLM support improves the quality of the resulting conversations while reducing manual editing effort.

Conclusion: LLMberjack provides a practical, open-source solution for constructing multi-party conversational data from debate reply trees. By combining visualization and optional LLM assistance, it supports more coherent conversations, better data quality, and more efficient, reproducible creation of multi-party conversation resources, helping address the scarcity of such datasets.

Abstract: We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.

</details>


### [103] [All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection](https://arxiv.org/abs/2601.04160)
*Yuechen Jiang,Zhiwei Liu,Yupeng Cao,Yueru He,Ziyang Xu,Chen Xu,Zhiyang Deng,Prayag Tiwari,Xi Chen,Alejandro Lopez-Lira,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RFC Bench is a new benchmark to evaluate how well large language models detect financial misinformation in realistic news, with and without explicit comparison to a reference text.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of misinformation detection, especially in finance, often use simplified or sentence-level tasks that miss the subtle, context-dependent nature of real financial news. Moreover, many setups give models explicit reference information or labels, which hides how poorly they may perform when they must reason in a reference-free, real-world setting. There is a need for a structured benchmark that captures the complexity of financial news and isolates models’ ability to maintain consistent beliefs and detect misinformation without external grounding.

Method: The authors construct RFC Bench as a paragraph-level benchmark derived from realistic financial news, where misinformation may be signaled only through dispersed contextual cues. They define two tasks: (1) reference-free misinformation detection, where a model must judge the veracity of a paragraph without any explicit comparison document, and (2) comparison-based diagnosis, where the model is given an original and a perturbed version of content and must identify misinformation by contrasting them. They then systematically evaluate large language models on both tasks, measuring accuracy, stability of predictions, and the frequency of invalid outputs.

Result: Across experiments, models perform much better when they can compare an original and a perturbed paragraph than when they must judge a single paragraph in isolation. In the reference-free setting, models show marked weaknesses: their predictions are unstable and they produce more invalid or unusable outputs. This pattern is consistent across models, highlighting a systematic limitation rather than a quirk of any one system.

Conclusion: The study concludes that current large language models have difficulty maintaining coherent, well-grounded belief states when they lack explicit external reference information, particularly in the domain of financial news. RFC Bench exposes this deficit by contrasting reference-free with comparison-based performance. The benchmark thus offers a structured platform for investigating reference-free reasoning and for developing methods that yield more reliable financial misinformation detection in realistic, real-world scenarios.

Abstract: We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [Mastering the Game of Go with Self-play Experience Replay](https://arxiv.org/abs/2601.03306)
*Jingbin Liu,Xuechun Wang*

Main category: cs.AI

TL;DR: QZero is a model-free RL algorithm that learns superhuman Go play without MCTS search, achieving AlphaGo-level strength using modest compute.


<details>
  <summary>Details</summary>
Motivation: AlphaGo-style systems rely heavily on model-based Monte-Carlo Tree Search, which is computationally expensive, tightly couples training and search, and limits the exploration of model-free, off-policy methods in large-scale, complex games like Go. The authors want to test whether a purely model-free approach, without search during training and without human data, can reach comparable performance, thereby broadening the toolkit for mastering complex strategic environments.

Method: They propose QZero, a model-free, entropy-regularized Q-learning algorithm for two-player zero-sum Go. QZero uses self-play combined with off-policy experience replay to learn, and employs a single Q-value network that simultaneously supports policy evaluation and policy improvement. Unlike AlphaGo-style methods, it forgoes Monte-Carlo Tree Search during training, learning a Nash equilibrium policy directly from self-play data starting tabula rasa. Training runs for about 5 months on 7 GPUs.

Result: After 5 months of training from scratch without human expert data and with relatively modest computational resources, QZero reaches a level of play comparable to AlphaGo, a well-known strong Go program that used MCTS and substantial compute. The results show that the learned policy is close to a Nash equilibrium strategy for Go at high level play.

Conclusion: The study concludes that model-free reinforcement learning, specifically entropy-regularized Q-learning with off-policy replay and self-play, can efficiently master Go without relying on model-based search such as MCTS. This validates the practicality of off-policy, model-free RL in large-scale, complex, strategic environments and suggests an alternative path to superhuman performance in games like Go, decoupled from heavy tree search during training.

Abstract: The game of Go has long served as a benchmark for artificial intelligence, demanding sophisticated strategic reasoning and long-term planning. Previous approaches such as AlphaGo and its successors, have predominantly relied on model-based Monte-Carlo Tree Search (MCTS). In this work, we present QZero, a novel model-free reinforcement learning algorithm that forgoes search during training and learns a Nash equilibrium policy through self-play and off-policy experience replay. Built upon entropy-regularized Q-learning, QZero utilizes a single Q-value network to unify policy evaluation and improvement. Starting tabula rasa without human data and trained for 5 months with modest compute resources (7 GPUs), QZero achieved a performance level comparable to that of AlphaGo. This demonstrates, for the first time, the efficiency of using model-free reinforcement learning to master the game of Go, as well as the feasibility of off-policy reinforcement learning in solving large-scale and complex environments.

</details>


### [105] [Digital Red Queen: Adversarial Program Evolution in Core War with LLMs](https://arxiv.org/abs/2601.03335)
*Akarsh Kumar,Ryan Bahlous-Boldi,Prafull Sharma,Phillip Isola,Sebastian Risi,Yujin Tang,David Ha*

Main category: cs.AI

TL;DR: The paper introduces Digital Red Queen (DRQ), a simple LLM-driven self-play framework that continually evolves Core War programs (warriors) against an ever-changing set of opponents, leading to increasingly general but convergent strategies.


<details>
  <summary>Details</summary>
Motivation: Most existing LLM-evolution frameworks optimize against fixed, static objectives, missing the open-ended, adversarial, and co-evolutionary nature of many real-world problems (Red Queen dynamics). The authors aim to explore whether embracing continual adaptation to a changing objective can produce more general and interesting solutions, and whether a classic artificial-life environment (Core War) can serve as a good testbed for such LLM-based evolutionary processes.

Method: They design Digital Red Queen (DRQ), a simple self-play algorithm in which an LLM is used to iteratively evolve assembly-like Core War programs, termed warriors. Each training round asks the LLM to generate a new warrior that defeats all previously discovered warriors. The environment is the Turing-complete Core War virtual machine, where warriors compete for control. The authors track the sequence of warriors produced over many rounds and evaluate them against a held-out set of human-designed warriors, as well as analyze behavioral diversity across independent DRQ runs.

Result: Across rounds, the evolved warriors become increasingly strong and general, as evidenced by improved performance against a held-out set of human warriors. At the same time, solutions from independent DRQ runs grow more behaviorally similar, indicating a convergence in strategy space despite different starting conditions. This demonstrates that dynamic Red Queen objectives can drive both performance and convergence toward general-purpose strategies.

Conclusion: Shifting from static optimization to dynamic, adversarial Red Queen-style objectives can yield more general and convergent strategies in LLM-driven evolutionary systems. Core War emerges as a useful, controllable sandbox for studying adversarial adaptation and evaluating LLM-based evolution. The simplicity and effectiveness of DRQ imply that comparably minimal self-play mechanisms could be applied to other adversarial multi-agent domains, with potential relevance to areas such as cybersecurity and resistance dynamics.

Abstract: Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optimization problems, overlooking the open-ended adversarial dynamics that characterize real-world evolutionary processes. Here, we study Digital Red Queen (DRQ), a simple self-play algorithm that embraces these so-called "Red Queen" dynamics via continual adaptation to a changing objective. DRQ uses an LLM to evolve assembly-like programs, called warriors, which compete against each other for control of a virtual machine in the game of Core War, a Turing-complete environment studied in artificial life and connected to cybersecurity. In each round of DRQ, the model evolves a new warrior to defeat all previous ones, producing a sequence of adapted warriors. Over many rounds, we observe that warriors become increasingly general (relative to a set of held-out human warriors). Interestingly, warriors also become less behaviorally diverse across independent runs, indicating a convergence pressure toward a general-purpose behavioral strategy, much like convergent evolution in nature. This result highlights a potential value of shifting from static objectives to dynamic Red Queen objectives. Our work positions Core War as a rich, controllable sandbox for studying adversarial adaptation in artificial systems and for evaluating LLM-based evolution methods. More broadly, the simplicity and effectiveness of DRQ suggest that similarly minimal self-play approaches could prove useful in other more practical multi-agent adversarial domains, like real-world cybersecurity or combating drug resistance.

</details>


### [106] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: The paper introduces a workflow that improves how prompts express task constraints so LLMs follow rules better, without changing their underlying capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs can produce answers that are conceptually right but break formal rules (formats, constraints, procedures). Existing prompt-engineering mostly rewrites the main task description and ignores precise, checkable constraints. There is a need for a systematic way to separately optimize and refine those constraints so models better satisfy acceptance criteria.

Method: The authors design a multi-agent workflow where different LLM agents separately optimize (1) the main task description and (2) the associated constraints. They use quantitative compliance scores as feedback, iteratively rewriting both parts of the prompt. Constraints are evaluated and adjusted in a loop so that the final prompt better encodes the rules the model must obey.

Result: Using this workflow, they obtain revised prompts that significantly increase rule-compliance scores for models such as Llama 3.1 8B and Mixtral-8x 7B, compared with standard prompt-refinement baselines.

Conclusion: Decoupling task description from constraints, and optimizing each via feedback-driven multi-agent prompt rewriting, leads to better constraint adherence in LLM outputs. This suggests that structured, score-guided prompt workflows can substantially improve procedural correctness without modifying the underlying models.

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [107] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: The paper introduces introspective reinforcement learning agents that infer their own internal 'pain-belief' states via a hidden Markov model and use this as a subjective reward to improve learning and reproduce human-like behaviors.


<details>
  <summary>Details</summary>
Motivation: To advance AI Theory of Mind by understanding and modeling how agents represent and use internal mental states, focusing on self-awareness and its impact on learning, and to examine how different models of pain perception (normal vs. chronic) affect behavior and performance.

Method: They design gridworld reinforcement learning environments where agents are augmented with an introspective exploration module. This module employs a hidden Markov model to infer a latent internal variable called 'pain-belief' from online observations. The inferred pain-belief is then incorporated into a subjective reward function, modifying the agent’s learning dynamics. They compare agents with normal vs. chronic pain models and against standard RL baselines.

Result: Introspective agents that use inferred pain-belief signals consistently outperform standard baseline RL agents in the gridworld tasks. Additionally, agents with the introspective pain-based mechanism reproduce complex, human-like behavioral patterns associated with different pain perception profiles (normal vs. chronic).

Conclusion: Incorporating self-awareness through an introspective pain-belief inference mechanism into RL agents enhances learning performance and can give rise to more human-like behavior. This supports the idea that unified mechanisms for self- and other-awareness can be fruitfully modeled in AI, and that internal state modeling (including pain-like signals) is beneficial for intelligent behavior.

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [108] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: A framework is proposed to certify embodied AI systems using maturity levels and explicit quantitative measurements, demonstrated via a UAS detection case study focused on uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: There is a need to make embodied AI systems (like robots, drones, and autonomous platforms) certifiable and trustworthy, which requires structured, measurable, and comparable assessment rather than ad-hoc evaluations.

Method: The authors design a maturity-based certification framework that defines structured assessment procedures, quantitative scoring mechanisms, and ways to handle multi-objective trade-offs in trustworthiness. They instantiate this framework using uncertainty quantification as a concrete measurement mechanism and apply it in a UAS detection scenario.

Result: They show that uncertainty quantification can be integrated into the maturity-based framework to yield quantitative scores for trustworthiness aspects and demonstrate that this is feasible in a practical UAS detection case study.

Conclusion: A maturity-based, measurement-driven approach is viable for certifying embodied AI systems, and uncertainty quantification can serve as a key measurement mechanism, paving the way for more systematic and certifiable evaluations of trustworthiness in embodied AI.

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [109] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: The paper presents CPGPrompt, an auto-prompting framework that converts narrative clinical practice guidelines into structured decision trees navigated by LLMs, achieving high performance on binary referral decisions but lower, domain-dependent performance on complex pathway classification.


<details>
  <summary>Details</summary>
Motivation: Integrating narrative clinical practice guidelines into AI systems is difficult. Existing rule-based and guideline-based AI tools often lack interpretability, have inconsistent adherence to guidelines, and are limited to narrow clinical domains. The authors aim to create a more generalizable, interpretable, and guideline-faithful way to operationalize CPGs within LLMs for clinical decision support.

Method: The authors design CPGPrompt, which first converts textual clinical practice guidelines into structured decision trees. An LLM is then prompted to traverse these trees dynamically when evaluating patient cases. They generate synthetic clinical vignettes for three domains (headache, lower back pain, prostate cancer), partition them into scenarios reflecting different decision contexts, and assess performance on two tasks: (1) binary specialty referral decisions, and (2) multi-class guideline pathway selection. Performance is measured primarily using F1 scores and recall, and domain-specific behavior is analyzed to understand the impact of guideline structure and reasoning demands (e.g., negation, temporality, lab thresholds).

Result: For binary specialty referral decisions, CPGPrompt achieves strong and consistent results across all three domains, with F1 scores between 0.85 and 1.00 and perfect recall (1.00 ± 0.00). In contrast, for multi-class pathway classification, performance is more modest and varies by domain: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). The analysis links these differences to the logical and structural properties of each guideline, such as the prevalence of negation, temporal conditions, and availability of quantitative test results.

Conclusion: The study demonstrates that transforming narrative CPGs into decision trees and using an LLM to navigate them can yield high performance for binary referral decisions, indicating promise for guideline-grounded clinical decision support. However, more complex, fine-grained pathway assignment remains challenging and strongly dependent on guideline structure and reasoning requirements. Improvements in handling negation, temporal reasoning, and complex logical conditions are needed to enhance the robustness and generalizability of this approach.

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [110] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: The paper argues that large foundation models (LFMs) alone cannot deliver truly personalized medical treatment; instead, they should be combined with N-of-1 trials, which provide individual-level causal evidence.


<details>
  <summary>Details</summary>
Motivation: To examine whether and how large foundation models can support truly personalized treatment in healthcare, given several emerging paradoxes: high-performing models that fail to generalize across studies (generalizability paradox), trade-offs between data use and privacy (privacy-performance paradox), tension between large-scale models and patient-specific needs (scale-specificity paradox), and the conflict between automation and the need for human empathy (automation-empathy paradox).

Method: Conceptual and analytical paper: identifies core paradoxes in AI-driven personalized medicine, contrasts predictive capabilities of LFMs with the causal requirements of individualized treatment, and evaluates N-of-1 trials as a methodological counterpoint. Proposes a hybrid framework where LFMs and N-of-1 trials are used together.

Result: Shows, through theoretical analysis and existing knowledge about LFMs and N-of-1 designs, that LFMs alone cannot provide reliable, causally grounded recommendations for individual patients. It demonstrates that N-of-1 trials offer privacy-preserving, within-person causal inference that addresses several paradoxes. The paper introduces a workflow in which LFMs generate and rank personalized intervention hypotheses with uncertainty estimates, which are then empirically tested via N-of-1 trials for each patient.

Conclusion: LFMs and N-of-1 trials are complementary rather than interchangeable. LFMs should be used primarily for hypothesis generation from large, multimodal population datasets, while N-of-1 trials should be used for causal validation at the individual level. Explicitly distinguishing prediction from causation and engaging directly with key paradoxes (generalizability, privacy-performance, scale-specificity, automation-empathy) is crucial for responsible deployment of AI in personalized medicine.

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [111] [Evolving Programmatic Skill Networks](https://arxiv.org/abs/2601.03509)
*Haochen Shi,Xingdi Yuan,Bang Liu*

Main category: cs.AI

TL;DR: Introduces Programmatic Skill Networks (PSN), a framework where an agent learns a growing library of symbolic program-skills in open-ended environments using LLM-based mechanisms for debugging, optimization, and refactoring, achieving strong reuse and generalization in MineDojo and Crafter.


<details>
  <summary>Details</summary>
Motivation: Continual learning agents in open-ended, embodied environments need to acquire, refine, and reuse an ever-growing set of skills. Existing methods often struggle with compositionality, stability–plasticity trade-offs, and maintaining a compact, interpretable skill library as tasks diversify. The paper aims to address how an agent can autonomously build and manage a structured repertoire of reusable skills that support rapid adaptation and generalization across diverse tasks.

Method: They propose Programmatic Skill Networks (PSN), where each skill is a symbolic, executable program and skills are connected into a compositional network that evolves over time. The framework uses large language models to implement three mechanisms: (1) REFLECT, which performs structured fault localization over composed skills to diagnose where failures occur; (2) progressive optimization with maturity-aware update gating, which protects mature, reliable skills from destabilizing updates while allowing more adaptation for uncertain or immature skills; and (3) canonical structural refactoring with rollback validation, which compacts and reorganizes the skill network, only accepting refactors that pass behavioral validation. They also analyze parallels between PSN’s learning dynamics and neural network training.

Result: On open-ended task distributions in MineDojo and Crafter, PSN demonstrates robust skill reuse, fast adaptation to new tasks, and strong generalization. Empirically, these properties show that the programmatic skill library and its maintenance mechanisms effectively support continual skill acquisition in complex environments, outperforming baselines in measures of reuse, adaptation speed, and generalization (details presumably in the full paper).

Conclusion: Programmatic Skill Networks provide an effective framework for continual skill acquisition in open-ended embodied environments by representing skills as compositional programs and managing them with LLM-based mechanisms for debugging, optimization, and refactoring. This design yields stable yet plastic skill libraries that support robust reuse and strong generalization, and the learning dynamics bear structural resemblance to neural network training. The approach is validated on MineDojo and Crafter and is intended to be released as open-source.

Abstract: We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\footnote{We plan to open-source the code.

</details>


### [112] [Variance Computation for Weighted Model Counting with Knowledge Compilation Approach](https://arxiv.org/abs/2601.03523)
*Kengo Nakamura,Masaaki Nishino,Norihito Yasuda*

Main category: cs.AI

TL;DR: They study how to efficiently compute the variance of weighted model counting (WMC) under parameter uncertainty, giving a polytime algorithm for structured d-DNNF and hardness results for related circuit classes, plus an application to Bayesian networks.


<details>
  <summary>Details</summary>
Motivation: In probabilistic inference (e.g., Bayesian networks), WMC is a key tool, but model parameters are uncertain because they are learned from data. We thus care not only about point estimates of inference (e.g., marginal probabilities) but also about how uncertain those outputs are, typically via their variance. However, the computational complexity of computing this variance in the WMC framework is unclear, motivating a systematic tractability analysis.

Method: They formalize the problem of computing the variance of WMC when parameters are treated as random variables. They then derive a polynomial-time dynamic programming algorithm for this variance when the logical representation is a structured d-DNNF. For other circuit classes (structured DNNFs, d-DNNFs, FBDDs), they give complexity-theoretic hardness proofs, showing that, despite these classes supporting polynomial-time WMC, variance computation becomes intractable. Finally, they instantiate the method for Bayesian networks and run experiments on real-world networks.

Result: 1) A polynomial-time algorithm to compute the variance of WMC when the input circuit is a structured d-DNNF. 2) Complexity hardness results for computing WMC variance on structured DNNFs, d-DNNFs, and FBDDs, despite the existence of polytime algorithms for standard WMC on some of these classes. 3) An empirical evaluation demonstrating that their algorithm can practically compute variances of marginal probabilities on real Bayesian networks and illustrating how parameter variances propagate to output variance.

Conclusion: Variance computation for WMC, which captures uncertainty in probabilistic inference under parameter uncertainty, is tractable for structured d-DNNFs but provably hard for several closely related circuit classes. Their algorithm is not only theoretically sound but also practical for real Bayesian networks, enabling quantitative analysis of how parameter uncertainty affects the reliability of inferred probabilities.

Abstract: One of the most important queries in knowledge compilation is weighted model counting (WMC), which has been applied to probabilistic inference on various models, such as Bayesian networks. In practical situations on inference tasks, the model's parameters have uncertainty because they are often learned from data, and thus we want to compute the degree of uncertainty in the inference outcome. One possible approach is to regard the inference outcome as a random variable by introducing distributions for the parameters and evaluate the variance of the outcome. Unfortunately, the tractability of computing such a variance is hardly known. Motivated by this, we consider the problem of computing the variance of WMC and investigate this problem's tractability. First, we derive a polynomial time algorithm to evaluate the WMC variance when the input is given as a structured d-DNNF. Second, we prove the hardness of this problem for structured DNNFs, d-DNNFs, and FBDDs, which is intriguing because the latter two allow polynomial time WMC algorithms. Finally, we show an application that measures the uncertainty in the inference of Bayesian networks. We empirically show that our algorithm can evaluate the variance of the marginal probability on real-world Bayesian networks and analyze the impact of the variances of parameters on the variance of the marginal.

</details>


### [113] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: The paper proposes STAR-S, a self-taught safety reasoning framework that repeatedly trains an LLM to reason over safety rules, significantly improving robustness against jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreak attacks that bypass safety mechanisms. Existing work tries to make models explicitly reason about safety rules before answering, but it is unclear what kind of safety reasoning is truly effective and how to obtain it at scale, since hand-designing or directly collecting such reasoning traces is difficult.

Method: STAR-S (Self-TAught Reasoning based on Safety rules) uses a self-taught loop: (1) prompt the model with safety rules to elicit explicit safety reasoning and reflection before answering; (2) collect these rule-guided reasoning traces; (3) fine-tune the model on this data to strengthen its ability to interpret and apply safety rules; (4) repeat the process so that the improved model can generate higher-quality safety reasoning data in the next iteration, creating a synergistic cycle.

Result: Experiments demonstrate that models trained with STAR-S become more robust to jailbreak attacks and achieve better defense performance than existing baseline methods on evaluated benchmarks.

Conclusion: Integrating safety-rule-based reasoning into a self-taught iterative training loop yields progressively stronger safety reasoning capabilities in LLMs, leading to superior jailbreak defense compared with standard approaches.

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [114] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: The paper introduces a framework to rigorously evaluate whether performance gains from test-time scaling in LLMs come from real reasoning or just longer CoT outputs, and uses it to study behavioral patterns, failure modes, and training/inference trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought evaluation focuses mainly on final accuracy and length, making it unclear if observed gains with test-time scaling correspond to genuine reasoning improvements or simply more verbose explanations. There is a need for a process-centric, fine-grained, and non-intrusive way to characterize LLM reasoning behavior and its failure modes across inference setups, training regimes, and model sizes.

Method: The authors propose a neuro-symbolic, process-centric evaluation framework that analyzes the internal reasoning traces (CoT) produced at test time. Without modifying the model, they classify reasoning behaviors into distinct prototypes and trace how these behaviors evolve under different inference modes (e.g., with and without test-time scaling), training strategies (e.g., mixing long and short CoT data, distillation), and model scales. The framework appears to combine symbolic structure over reasoning steps with neural model outputs to quantify depth and quality of reasoning beyond sheer token length.

Result: Using the framework, they identify four distinct behavioral prototypes of reasoning and systematically diagnose associated failure modes. They find that deep reasoning does not inherently require long generated chains; in other words, token length is a poor proxy for reasoning depth. They also show that mixing short and long CoT supervision during training can cause premature saturation and even collapse of reasoning quality. Moreover, when distilling from larger to smaller models, the student can mimic the length of the teacher’s CoT but fails to match its logical soundness, highlighting capacity limitations.

Conclusion: The study concludes that current CoT-based test-time scaling gains cannot be reliably interpreted using simple metrics like explanation length. A process-centric neuro-symbolic evaluation is necessary to understand and improve LLM reasoning. Long chains are not strictly needed for deep reasoning, naïvely mixing CoT lengths in training can harm robustness, and distillation into smaller models is fundamentally constrained by model capacity, which limits transfer of true logical competence even when surface behavior (length) is matched.

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [115] [SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models](https://arxiv.org/abs/2601.03555)
*Yuxuan Jiang,Francis Ferraro*

Main category: cs.AI

TL;DR: SCRIBE is a reinforcement learning framework for tool-augmented agents that uses skill-conditioned, mid-level reward modeling to provide more stable and accurate credit assignment, leading to state-of-the-art performance on reasoning and tool-use tasks.


<details>
  <summary>Details</summary>
Motivation: Training agents that use tools and perform multi-step reasoning is hard mainly because it is difficult to assign credit to individual steps. Existing process-level reward models rely on LLM judges that are noisy and inconsistent, largely because they lack precise, task-specific rubrics and cannot clearly distinguish between good high-level planning and correct low-level execution. The authors want a more structured way to evaluate intermediate behavior so that learning becomes more stable and effective.

Method: The authors propose SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that operates at a mid-level abstraction between high-level plans and low-level actions. They construct a curated library of skill prototypes, where each prototype encodes a specific mid-level skill and its evaluation rubric. During training, each subgoal or step in the agent’s reasoning/tool-use trajectory is routed to the appropriate skill prototype. Reward modeling then becomes a constrained verification task: the reward model checks whether the behavior matches the corresponding skill prototype according to its structured rubric. This yields more precise, lower-variance process-level rewards for RL training of the tool-using agent.

Result: SCRIBE achieves state-of-the-art results on multiple reasoning and tool-use benchmarks. Notably, when training a Qwen3-4B model on AIME25, SCRIBE improves accuracy from 43.3% to 63.3%. It also substantially increases success rates in complex, multi-turn tool interaction tasks. Analysis of the training process shows that the model first learns the mid-level skills embedded in the prototypes and then, on top of that, develops stronger high-level planning capabilities. The method is shown to be complementary to existing low-level tool optimization techniques.

Conclusion: Mid-level, skill-conditioned reward modeling via SCRIBE significantly improves the training of tool-augmented agents by reducing reward noise and enabling more reliable credit assignment in multi-step reasoning. By grounding evaluation in a library of explicit skill prototypes, SCRIBE turns open-ended LLM judging into a structured verification problem, leading to better performance and revealing a co-evolution from mid-level skill mastery to high-level planning. The framework is compatible with and additive to low-level tool optimizations, offering a scalable path toward more autonomous and dependable tool-using agents.

Abstract: Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.

</details>


### [116] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: They propose SAE-Steering, a sparse-autoencoder-based method to disentangle and control fine-grained reasoning strategies in Large Reasoning Models, improving strategy control and overall reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models can use human-like reasoning strategies, but currently choose these strategies autonomously, often leading to inefficient or incorrect reasoning paths. Existing control methods cannot precisely adjust specific strategies because strategy-related concepts are entangled in hidden states. The authors want a way to reliably and flexibly control which reasoning strategies the model uses.

Method: They apply Sparse Autoencoders (SAEs) to decompose the LRM’s strategy-entangled hidden states into a disentangled feature space. Then they introduce SAE-Steering, a two-stage pipeline: (1) recall stage – select features that significantly amplify logits of strategy-indicative keywords, discarding over 99% of SAE features; (2) ranking stage – evaluate and rank the remaining features by how effective they are for controlling reasoning strategies. The top strategy-specific features are used as steering/control vectors during inference.

Result: SAE-Steering identifies a small set of strategy-specific SAE features and uses them to steer reasoning strategies. Compared with prior control methods, it achieves more than 15% improvement in control effectiveness. Additionally, by steering strategies away from bad reasoning paths and toward better ones, it yields a 7% absolute gain in task accuracy.

Conclusion: Disentangling reasoning strategies in LRMs’ hidden representations with SAEs and then steering via strategy-specific features is an effective way to finely control reasoning behavior. SAE-Steering substantially improves both the controllability of reasoning strategies and the final task performance, demonstrating that targeted strategy control can correct erroneous reasoning paths.

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [117] [Interleaved Tool-Call Reasoning for Protein Function Understanding](https://arxiv.org/abs/2601.03604)
*Chuanliu Fan,Zicheng Ma,Huanran Meng,Aijia Zhang,Wenjie Du,Jun Zhang,Yi Qin Gao,Ziqiang Cao,Guohong Fu*

Main category: cs.AI

TL;DR: The paper shows that direct chain-of-thought style reasoning with LLMs performs poorly for protein function prediction and introduces PFUA, a tool-augmented agent that leverages biological tools and priors to significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought has worked well in symbolic domains like math and coding, but its effectiveness for complex, knowledge-intensive scientific tasks like protein function understanding is unclear. The authors observe that naively applying text-only reasoning to proteins leads to shallow pattern matching and poor generalization, motivating a new framework that can incorporate external biological knowledge and tools.

Method: They first analyze why reinforcement-learning-optimized, text-only chain-of-thought reasoning fails on protein function prediction, showing it mainly reinforces superficial keyword cues instead of adding real biological insight. They then propose PFUA, a protein function understanding agent that decomposes tasks into subproblems, selectively invokes domain-specific computational biology tools, and integrates the resulting structured evidence into grounded final answers rather than relying on long free-form reasoning traces.

Result: On four protein function understanding benchmarks, PFUA consistently outperforms text-only reasoning models, achieving on average a 103% relative improvement in performance, indicating substantially better generalization and accuracy when tool use is integrated.

Conclusion: Protein function prediction is better treated as a tool- and knowledge-intensive scientific workflow than as pure internal text reasoning. By tightly integrating problem decomposition with domain tools and verifiable intermediate evidence, PFUA overcomes the limitations of standard chain-of-thought LLMs and sets a stronger baseline for LLM-based protein reasoning agents.

Abstract: Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.

</details>


### [118] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: The paper proposes a formally grounded, pattern-based architecture for building enterprise-ready Agentic AI systems, particularly multi-agent communities that coordinate AI agents and humans under explicit governance.


<details>
  <summary>Details</summary>
Motivation: LLMs and Agentic AI are evolving quickly, but practitioners lack systematic, enterprise-grade architectural guidance—especially for complex, multi-agent, human-in-the-loop systems that must satisfy organizational, legal, and ethical requirements.

Method: The authors adapt design patterns from enterprise distributed systems, formal methods, and industry practice, and organize them into three tiers: LLM Agents, Agentic AI, and Agentic Communities. They develop a formal coordination and governance framework that models collaboration agreements, roles, protocols, and accountability, then apply it to a clinical trial matching case study.

Result: They obtain a catalog of architectural patterns and a formal framework for specifying and verifying coordination and governance in Agentic Communities, demonstrating its applicability through the clinical trial matching scenario.

Conclusion: Formal, pattern-based architectures can guide the design of scalable, governable Agentic AI ecosystems that integrate humans and AI agents while enabling verification of organizational, legal, and ethical constraints, making them suitable for enterprise and industrial deployment.

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [119] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: The paper introduces SafeRemind, a decoding-time defense that injects safety reminders into the internal reasoning steps of Large Reasoning Models (LRMs) to reduce unsafe outputs while maintaining reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models use explicit multi-step reasoning, which can amplify unsafe behaviors if the reasoning trajectory turns harmful. Existing safety defenses target final outputs or general behavior and do not account for the internal reasoning dynamics of LRMs, leaving a safety gap. The authors aim to create a defense that acts directly on the reasoning process to better control safety risks without retraining models.

Method: The authors empirically observe that the presence of safe-reminding phrases within intermediate thinking steps correlates with safer model behavior. Building on this, they design SafeRemind, a decoding-time method that monitors token-level entropy during generation and identifies decision-locking points—steps where the trajectory is becoming committed. At these points, the method dynamically injects safe-reminding phrases into the model’s ongoing chain-of-thought. This intervention is done purely at decoding time, requires no parameter updates, and is applied across different LRMs.

Result: Across five different LRMs and six safety benchmarks, SafeRemind significantly improves safety metrics, with gains reported up to 45.5 percentage points, while preserving the core reasoning ability of the models. The results suggest that SafeRemind effectively redirects harmful reasoning trajectories without causing major degradation in task performance.

Conclusion: Reasoning-time safety interventions are effective for Large Reasoning Models. By inserting carefully timed safe-reminding phrases into internal thinking steps, SafeRemind can substantially enhance safety with minimal impact on reasoning quality and no need for model retraining. This highlights the importance of targeting the internal reasoning dynamics, rather than only the final outputs, when designing safety mechanisms for LRMs.

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [120] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: The paper introduces Sandwich Reasoning (SandwichR), an Answer-Reasoning-Answer framework that enables low-latency yet reasoning-aware query correction for search, achieving CoT-level accuracy with substantially lower latency via consistency-aware RL and a new complex query correction dataset.


<details>
  <summary>Details</summary>
Motivation: In modern search pipelines, query correction must be highly accurate but also satisfy strict real-time latency constraints. Chain-of-Thought reasoning improves correction accuracy but is too slow for online use. A naive idea of answering first and reasoning later does not work under autoregressive decoding because the early answer cannot benefit from later reasoning. There is also a lack of strong benchmarks for complex query correction. The paper aims to break the latency–accuracy trade-off by enabling models to use reasoning benefits without incurring full CoT latency, and to provide an appropriate dataset for evaluation.

Method: The authors propose Sandwich Reasoning (SandwichR), which follows an Answer-Reasoning-Answer generation pattern: the model first outputs an initial query correction, then an explicit reasoning trace, and finally a refined correction. To ensure the first (fast) answer already reflects the benefits of reasoning, they design a consistency-aware reinforcement learning framework. A consistency reward encourages the initial and final corrections to be aligned, so that improvements from reasoning propagate back to shape the initial answer. Margin-based rejection sampling focuses training on borderline cases where reasoning most changes the outcome, amplifying the value of reasoning-driven corrections. They also build a dedicated, high-quality dataset for complex query correction to train and evaluate the system.

Result: Experiments show that SandwichR matches or surpasses the state-of-the-art accuracy of conventional Chain-of-Thought approaches for query correction while cutting latency by 40–70%. This demonstrates that the method effectively uses reasoning signals to maintain high accuracy without the usual latency overhead. The new dataset supports systematic evaluation on complex query correction scenarios.

Conclusion: SandwichR resolves the traditional trade-off between latency and reasoning-aware accuracy for online query correction. By coupling an Answer-Reasoning-Answer generation scheme with consistency-aware RL and targeted sampling, the model produces a fast initial correction that already benefits from deeper reasoning. Together with the new complex query correction dataset, this establishes a practical and high-performance framework for real-time search query correction.

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [121] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: They improve automated personalized medication planning by using LLM‑generated, domain-specific heuristics with GBFS search, scaling from 7 to at least 28 medications with major gains in speed and coverage.


<details>
  <summary>Details</summary>
Motivation: Existing automated planners can personalize treatment regimens when the medical planning problem is modeled in a general planning language, but they fail to scale beyond about seven medications, which is clinically useless because real cases often involve many more concurrent drugs and constraints. There is a need for methods that retain the generality of domain-independent planning while scaling to realistic, higher-dimensional medication planning tasks so the technology can be used with clinicians.

Method: They replace purely domain-independent heuristics with automatically generated domain- and problem-specific heuristics. The medication domain is specified programmatically via an initial state and successor function rather than only as a static planning description. A large language model (LLM) is then used to synthesize a heuristic tailored to each planning problem. This learned/problem-specific heuristic guides a fixed search procedure—Greedy Best-First Search (GBFS)—for constructing medication and dosing plans.

Result: Using the LLM-derived, problem-specific heuristics with GBFS yields dramatic improvements in both coverage (percentage of solved planning instances) and planning time compared to previous domain-independent approaches. The enhanced planner can handle medication planning problems with at least 28 medications, a fourfold increase over the earlier practical limit of seven medications.

Conclusion: Automatically generated, domain- and problem-specific heuristics, produced by an LLM and combined with a standard search algorithm, substantially improve the scalability of personalized medication planning. This approach moves automated treatment planning closer to clinically meaningful scales and suggests that integrating programmatic domain models with LLM-based heuristic synthesis is a promising direction for real-world medical decision support.

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [122] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT is a framework to automatically detect and remove low-quality chain-of-thought (CoT) traces so that only logically useful reasoning steps are kept for fine-tuning LLMs on math tasks.


<details>
  <summary>Details</summary>
Motivation: Existing CoT fine-tuning data often has correct final answers but flawed or hallucinated intermediate reasoning, which can misguide models trained on them. The authors want a systematic way to clean such data so that each supervision step genuinely contributes to correct reasoning, thereby improving mathematical reasoning performance.

Method: EntroCoT has two main components. First, it uses an entropy-based mechanism to segment a CoT reasoning trace into steps at points of high uncertainty, turning a long explanation into multiple decision points. Second, it uses Monte Carlo rollouts to estimate the marginal contribution of each step by simulating reasoning with and without that step. Steps that do not improve or harm the path to the correct answer are identified as low quality and filtered out, producing a refined subset of traces.

Result: Using EntroCoT to select a subset of reasoning traces as training data, LLMs fine-tuned on this subset outperform models fine-tuned on the full, unfiltered CoT datasets across multiple mathematical reasoning benchmarks. The experiments show consistent gains, indicating that data quality is more important than sheer quantity for CoT supervision.

Conclusion: Careful selection and refinement of CoT traces—ensuring each intermediate step is genuinely helpful—yields better mathematical reasoning than training on all available CoT data. EntroCoT provides an automatic, entropy- and rollout-based framework to do this filtering, demonstrating that removing deceptive or low-value reasoning is an effective strategy for improving CoT-based LLM fine-tuning.

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [123] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: The paper introduces ROI-Reasoning, a framework that lets LLMs reason under a strict global token budget by predicting task difficulty and allocating computation strategically, modeled as an Ordered Stochastic Multiple-Choice Knapsack Problem.


<details>
  <summary>Details</summary>
Motivation: LLMs can reason well if given enough computation (e.g., long chains-of-thought), but they lack awareness of how much computation each task truly requires. In realistic settings, there is a strict limit on total tokens or compute budget across many inputs, so models must decide where to spend or save computation to maximize overall performance. Existing approaches do not provide intrinsic, budget-aware rationality or principled global allocation of compute across tasks.

Method: They formalize budgeted inference-time reasoning as an Ordered Stochastic Multiple-Choice Knapsack Problem (OS-MCKP), where each task has multiple possible computation options with stochastic utilities and costs, and a global token budget must not be exceeded. They propose ROI-Reasoning, a two-stage framework: (1) Meta-Cognitive Fine-Tuning, which trains the LLM to predict the reasoning cost and expected utility of solving a task before generation, enabling solve-or-skip decisions; (2) Rationality-Aware Reinforcement Learning, which learns sequential policies that decide, under a hard global token budget, how much computation to allocate to each incoming task, optimizing long-horizon expected total score.

Result: On budgeted mathematical reasoning benchmarks, ROI-Reasoning yields higher overall scores than baselines for the same or lower total token budgets. It notably reduces regret when computation budgets are tight, meaning it wastes fewer tokens on low-ROI problems and better preserves tokens for high-ROI ones, leading to more efficient global use of computation.

Conclusion: Modeling compute allocation as an OS-MCKP and training LLMs with ROI-Reasoning gives them meta-cognitive, budget-aware rationality. The approach enables LLMs to anticipate problem difficulty, estimate the return on additional reasoning, and strategically allocate limited computation across tasks, improving performance and reducing regret under strict token budgets.

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [124] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: The paper encodes Rational Closure (a core KLM defeasible entailment algorithm) in Answer Set Programming, proving correctness and showing it can be more efficient than existing imperative solvers.


<details>
  <summary>Details</summary>
Motivation: Defeasible entailment draws plausible conclusions from incomplete information, with the KLM framework and Rational Closure being central. However, existing implementations of Rational Closure are largely imperative and may not exploit the benefits of declarative paradigms. There is a need for a clear, declarative, and automatable way to construct minimal ranked models and perform entailment checking that also allows leveraging ASP solvers’ optimization and reasoning capabilities.

Method: The authors provide a declarative encoding of the Rational Closure procedure in Answer Set Programming. They design ASP rules that, given a knowledge base, automatically construct its minimal ranked model according to the KLM framework, and support checking whether a query is defeasibly entailed. They formally prove that the ASP encoding is sound and complete with respect to the standard definition of Rational Closure and then implement the encoding using an ASP solver. They empirically compare its performance to an existing imperative Rational Closure implementation (InfOCF).

Result: The ASP encoding is shown to correctly reproduce Rational Closure entailments, as proven formally. In empirical evaluations, the ASP-based implementation is compared against the InfOCF solver, and the results show that the ASP approach is at least competitive and, in the reported cases, more computationally efficient.

Conclusion: A correct and efficient declarative realization of Rational Closure in Answer Set Programming is possible. The proposed ASP encoding faithfully captures the KLM-based notion of Rational Closure, enables automatic construction of minimal ranked models and entailment checking, and outperforms a well-known imperative solver (InfOCF) in the reported experiments, suggesting that ASP is a promising platform for implementing defeasible reasoning systems.

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [125] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: The paper models Italian Criminal Code articles in Answer Set Programming and uses inductive logic programming to learn and explain legal rules from past judicial decisions, supporting judges with automated reasoning and interpretable outcomes.


<details>
  <summary>Details</summary>
Motivation: Legal reasoning over complex criminal codes is difficult, error‑prone, and often opaque. There is a need for computational tools that can both encode legal norms formally and provide explainable automated reasoning over concrete cases, including the ability to generalize from prior judicial decisions rather than manually coding all rules.

Method: They formalize selected articles of the Italian Criminal Code—especially crimes against the person and property offenses—using Answer Set Programming. They build a system that: (1) encodes statutory rules in ASP; (2) checks these encodings against past verdicts, refining them when mismatches appear; (3) handles contradictions in the code or case facts within the ASP semantics; (4) uses the notion of supportedness of stable models to generate human‑readable explanations of why particular legal outcomes follow; and (5) integrates an inductive logic programming component for ASP that learns generalized legal rules from labeled case examples.

Result: The tool successfully represents ICC provisions in ASP, can reproduce prior verdicts on a validation set, highlights and manages contradictions during encoding, and generates candidate decisions for new cases together with explanations grounded in the underlying ASP models. The inductive logic programming module is able to infer generalized legal rules from case examples, complementing the manually encoded statutes.

Conclusion: ASP is a suitable formalism for modeling criminal law norms and reasoning about concrete cases, and combining it with inductive logic programming enables both semi‑automatic acquisition of legal rules and explainable decision support. The proposed tool can assist legal experts during criminal trials by suggesting possible outcomes and clarifying the logical basis of judicial decisions, ultimately improving transparency and interpretability in legal decision‑making.

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [126] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: They propose an ASP-based framework to generate multiple logical explanation types for decisions made by tree-based models, enabling flexible preference handling and full enumeration of explanations, and empirically show it is competitive with prior methods while clarifying its limits.


<details>
  <summary>Details</summary>
Motivation: Decision tree ensembles such as random forests and gradient-boosted trees are highly accurate but hard to interpret, which is problematic in domains where decisions must be formally justified. Prior automated-reasoning work has shown that logical and abductive explanations are possible, but existing SAT-based approaches can be rigid and limited in the types and enumeration of explanations they support. The paper aims to provide richer, more flexible, and formally grounded explanations for tree-based model predictions.

Method: They encode decision tree models and different explanation notions—sufficient, contrastive, majority, and tree-specific—within Answer Set Programming (ASP). The ASP encoding allows specifying user preferences and constraints declaratively and supports computing not just one, but all possible explanations that satisfy the criteria. They then apply this ASP-based explanation generator to trained tree ensembles across multiple datasets and compare its outputs and performance to SAT-based and other existing explanation methods.

Result: The ASP-based approach successfully generates the targeted explanation types for decisions made by tree-based models and can enumerate multiple explanations while incorporating user preferences. Experiments on diverse datasets show that the method is effective in producing meaningful explanations, with performance and explanation quality that are competitive with or complementary to existing SAT-based techniques, though they also observe scalability and other practical limitations.

Conclusion: ASP is a viable and advantageous backbone for generating logical explanations of decision tree ensemble predictions, offering expressive preference handling and explanation enumeration beyond SAT-based approaches. The empirical study supports its practical usefulness while highlighting trade-offs and limitations, suggesting further work on scalability and integration into real-world decision-support systems.

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [127] [xDNN(ASP): Explanation Generation System for Deep Neural Networks powered by Answer Set Programming](https://arxiv.org/abs/2601.03847)
*Ly Ly Trieu,Tran Cao Son*

Main category: cs.AI

TL;DR: The paper proposes xDNN(ASP), a system that extracts a global, symbolic explanation (logic program) from a trained deep neural network, preserving predictive accuracy while exposing feature and hidden-node importance and enabling network simplification.


<details>
  <summary>Details</summary>
Motivation: Existing xAI methods like SHAP, rule extraction, and counterfactuals mostly explain input-output behavior and overlook the internal structure of deep neural networks. There is a need for global explanations that faithfully capture the network’s decision logic and reveal how features and hidden neurons contribute to predictions, potentially supporting model understanding and optimization.

Method: Given a trained neural network and its training data, xDNN(ASP) extracts a logic program under answer set semantics whose answer sets correspond to input-output pairs of the network. This logic program is intended to approximate or ideally match the network’s behavior. The method is evaluated on two synthetic datasets to assess predictive accuracy and the interpretability benefits, such as feature and hidden-node importance analysis.

Result: Experiments on two synthetic datasets show that the extracted logic programs achieve high predictive accuracy compared to the original neural networks. Additionally, the logic programs make it possible to analyze the relevance of input features and the influence of hidden nodes on predictions, information that is not readily available from standard xAI methods focused solely on input-output mappings.

Conclusion: xDNN(ASP) can generate faithful global, symbolic explanations for deep neural networks in the form of answer-set logic programs. These explanations preserve much of the original model’s accuracy while revealing feature importance and hidden-node impact, which can guide structural simplification and optimization of the network. The approach addresses limitations of existing xAI methods that ignore internal network structure.

Abstract: Explainable artificial intelligence (xAI) has gained significant attention in recent years. Among other things, explainablility for deep neural networks has been a topic of intensive research due to the meteoric rise in prominence of deep neural networks and their "black-box" nature. xAI approaches can be characterized along different dimensions such as their scope (global versus local explanations) or underlying methodologies (statistic-based versus rule-based strategies). Methods generating global explanations aim to provide reasoning process applicable to all possible output classes while local explanation methods focus only on a single, specific class. SHAP (SHapley Additive exPlanations), a well-known statistical technique, identifies important features of a network. Deep neural network rule extraction method constructs IF-THEN rules that link input conditions to a class. Another approach focuses on generating counterfactuals which help explain how small changes to an input can affect the model's predictions. However, these techniques primarily focus on the input-output relationship and thus neglect the structure of the network in explanation generation.   In this work, we propose xDNN(ASP), an explanation generation system for deep neural networks that provides global explanations. Given a neural network model and its training data, xDNN(ASP) extracts a logic program under answer set semantics that-in the ideal case-represents the trained model, i.e., answer sets of the extracted program correspond one-to-one to input-output pairs of the network. We demonstrate experimentally, using two synthetic datasets, that not only the extracted logic program maintains a high-level of accuracy in the prediction task, but it also provides valuable information for the understanding of the model such as the importance of features as well as the impact of hidden nodes on the prediction. The latter can be used as a guide for reducing the number of nodes used in hidden layers, i.e., providing a means for optimizing the network.

</details>


### [128] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: The paper studies how well current Answer Set Programming (ASP) technologies scale to very large configuration problems and proposes new techniques to reduce memory usage, especially tackling the grounding bottleneck.


<details>
  <summary>Details</summary>
Motivation: ASP promises that users can declaratively specify problems and let the computer solve them automatically, and it has succeeded in many domains. However, very large configuration tasks, such as configuring electronic systems with over 30,000 components, challenge existing ASP solvers due to massive memory consumption during grounding. The authors are motivated to understand both the potential and limitations of ASP in this context and to push its scalability.

Method: They use large electronic system configuration as a benchmark and analyze the performance of current ASP solving techniques, focusing on the grounding phase and memory use. They study and apply incremental solving to mitigate the grounding bottleneck and then conduct a detailed grounding analysis to design a new technique called constraint-aware guessing, which alters how choices are generated with awareness of constraints to reduce grounding size and memory usage.

Result: Incremental solving shows practical effectiveness in scaling ASP to larger configuration instances, but it still suffers from substantial memory demands. By introducing constraint-aware guessing, the authors achieve a significant reduction in memory requirements compared to standard approaches, allowing ASP to handle larger problem instances than before.

Conclusion: Current ASP technology has strong potential for large configuration problems but is fundamentally limited by the grounding bottleneck and memory consumption. Incremental solving improves scalability but is not sufficient alone. The newly proposed constraint-aware guessing approach substantially alleviates memory use, extending the range of configuration problems ASP can feasibly solve, though challenges remain for extremely large instances.

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [129] [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905)
*Cheng Qian,Emre Can Acikgoz,Bingxuan Li,Xiusi Chen,Yuji Zhang,Bingxiang He,Qinyu Luo,Dilek Hakkani-Tür,Gokhan Tur,Yunzhu Li,Heng Ji,Heng Ji*

Main category: cs.AI

TL;DR: The paper finds that current vision-language agents struggle to effectively use external generative world models for simulation, leading to little use, frequent misuse, and sometimes worse performance, highlighting a need for better mechanisms to integrate simulated foresight into reasoning.


<details>
  <summary>Details</summary>
Motivation: Many real-world tasks require agents to anticipate future states, but existing vision-language agents mostly perform short-horizon reasoning. Generative world models could serve as external simulators that allow agents to ‘look ahead’ before acting. The paper is motivated by the question of whether current agents can practically leverage such world models to improve their decision-making and reasoning.

Method: The authors empirically evaluate vision-language model-based agents augmented with generative world models across a range of agentic tasks and visual question answering benchmarks. They analyze how often agents call the simulator, how they use (or misuse) predicted rollouts, and how performance changes when simulation is available or enforced. They also perform attribution analysis to locate where the main bottlenecks lie in the pipeline from deciding to simulate, interpreting simulations, to incorporating them into reasoning.

Result: The study shows that some agents invoke simulation extremely rarely (under 1% of the time), misuse predicted rollouts in roughly 15% of cases, and may suffer inconsistent or even degraded task performance (up to 5% drop) when simulation is provided or forced. Attribution analysis suggests that the core limitations are not in the world models themselves but in the agents’ higher-level control: deciding when simulations are needed, correctly interpreting predicted futures, and integrating these predictions into downstream decisions.

Conclusion: Current vision-language agents are not yet adept at strategically using external generative world models for anticipatory reasoning. To realize the potential of such simulators, future work must develop mechanisms and training strategies that help agents reliably decide when to simulate, appropriately interpret rollout information, and integrate foresight in a calibrated, beneficial way. This is necessary for building agent systems with robust anticipatory cognition.

Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.

</details>


### [130] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1 is a reinforcement learning framework that reduces reward hacking and improves reasoning consistency for financial decision-making by verifying process-level reasoning using a structured RAG-based consistency metric and semantic rewards.


<details>
  <summary>Details</summary>
Motivation: Standard RL methods that work well for LLM reasoning in math and coding fail in financial decision-making because market rewards, while verifiable, are highly stochastic and noisy. This noise leads to reward hacking, where models exploit randomness instead of learning robust, reasoning-based strategies. The paper aims to design a training paradigm that can reliably use noisy financial returns as learning signals while enforcing sound, document-grounded reasoning.

Method: The authors introduce Trade-R1, a model training framework that connects verifiable but noisy financial rewards with process-level reasoning verification. They design a verification method that converts the evaluation of long financial reasoning into a structured Retrieval-Augmented Generation (RAG) problem. The framework builds a triangular consistency metric that measures pairwise alignment among three elements: retrieved evidence from financial documents, the model’s intermediate reasoning chains, and the final trading decisions. This metric filters out instances where high market returns are inconsistent with evidence-based reasoning. They further propose two semantic reward schemes: Fixed-effect Semantic Reward (FSR), providing a stable alignment signal independent of reward magnitude, and Dynamic-effect Semantic Reward (DSR), which couples semantic consistency with the size of financial returns to jointly optimize correctness and profit.

Result: On cross-country asset selection tasks, Trade-R1 reduces reward hacking compared with standard RL approaches. Among the proposed reward schemes, DSR yields the best cross-market generalization performance while preserving the highest level of reasoning consistency according to the triangular consistency metric, indicating that it both earns higher-quality returns and maintains stronger alignment between evidence, reasoning, and decisions.

Conclusion: The paper concludes that incorporating process-level reasoning verification via a structured RAG pipeline and triangular consistency metric enables RL for LLMs to function more reliably in noisy financial markets. By filtering market rewards through semantic validity checks and using FSR/DSR semantic rewards, Trade-R1 mitigates reward hacking and produces models with better generalization and more consistent, evidence-grounded financial decisions, demonstrating a viable path to extend RL-based reasoning from deterministic domains to stochastic financial environments.

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [131] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: The paper proposes a training-time method to reduce unnecessary long chain-of-thought in reasoning models while preserving performance, achieving much better accuracy–efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Reinforcement-learning-enhanced reasoning models become very verbose, even on simple problems, making deployment expensive. Existing fixes use explicit length penalties, which can conflict with optimization goals and don’t address why models overthink. There is a need for a principled way to curb unnecessary length without harming performance on hard tasks.

Method: The authors first identify a training phenomenon called "length shift," where models increasingly produce unnecessary reasoning on trivial inputs. They then propose Dynamic Outlier Truncation (DOT), a training-time intervention that selectively truncates only the longest, redundant responses. DOT operates at the group level on fully correct rollouts: it detects the extreme tail of the length distribution in each group and suppresses those redundant tokens. To preserve stability and reasoning ability, they add auxiliary KL regularization and a predictive dynamic sampling strategy during training.

Result: Across several model sizes, DOT improves the trade-off between reasoning accuracy and inference token usage, effectively shifting the efficiency–performance Pareto frontier outward. On AIME-24, their method cuts token usage at inference by 78% while also increasing accuracy relative to the starting policy and outperforming prior efficient reasoning baselines.

Conclusion: Targeting only extreme-length, redundant outputs during training can significantly reduce overthinking in reasoning models without sacrificing—and even improving—performance. Dynamic Outlier Truncation, combined with KL regularization and predictive dynamic sampling, offers a more efficient and effective way to control chain-of-thought length than naive length penalties.

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [132] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer is a world-model-based lookahead framework that lets mobile GUI agents imagine future screens, improving long-horizon task performance on Android.


<details>
  <summary>Details</summary>
Motivation: Existing mobile GUI agents are mainly reactive, deciding actions only from the current screen without forecasting future states, which hurts performance on long-horizon, multi-step tasks. There is a need for an efficient world model that can predict post-action GUI states with spatial awareness and be deployable in real-world mobile environments.

Method: The paper proposes MobileDreamer, composed of (1) a textual sketch world model that learns to convert GUI screenshots into compact, task-relevant textual sketches and predicts post-action sketches; it uses a novel order-invariant learning strategy to keep spatial relationships among GUI elements, and (2) a rollout imagination strategy for the GUI agent that uses the world model’s predictions to simulate future trajectories and optimize action selection rather than acting purely reactively.

Result: On the Android World benchmark, MobileDreamer achieves state-of-the-art performance, improving task success rate by 5.25% over prior methods. Additional evaluations of the world model demonstrate that the textual sketch representation accurately forecasts key GUI elements in future states.

Conclusion: Equipping mobile GUI agents with an efficient, spatially aware world model that operates over textual sketches enables effective lookahead and improves long-horizon task performance. MobileDreamer’s world-model-based imagination framework is both accurate and practical, leading to measurable gains in real-world-style Android automation tasks.

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [133] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: The paper introduces ComfySearch, an agent-based framework that automatically builds valid, high-quality ComfyUI workflows by exploring component space with validation feedback, significantly improving success and solution rates on complex creative tasks.


<details>
  <summary>Details</summary>
Motivation: ComfyUI enables modular, customizable AI creative pipelines, but the large component space and strict graph constraints make it hard for users (and automatic systems) to construct long-horizon, structurally consistent workflows. This leads to low execution pass rates and suboptimal pipeline quality, especially for complex or creative tasks. A method is needed to systematically search the workflow space and reliably assemble functional, high-quality pipelines.

Method: The authors design ComfySearch, an agentic framework that incrementally constructs ComfyUI workflows. It explores the space of components and connections while being guided by validation signals that check whether intermediate workflows are executable and structurally sound. The agent uses this validation feedback to iteratively refine and extend candidate pipelines, effectively performing validation-guided search over the workflow graph under ComfyUI’s constraints.

Result: In experiments on complex, creative workflow construction tasks for ComfyUI, ComfySearch achieves substantially higher executability (pass) rates, higher overall solution rates, and better generalization to new tasks compared with existing automatic workflow generation baselines.

Conclusion: Validation-guided, agentic exploration of the component space is an effective strategy for constructing modular AI workflows in ComfyUI. ComfySearch not only improves the robustness (pass rate) and success rate of generated pipelines but also generalizes better to complex and creative tasks, suggesting a promising direction for automated design of modular AI systems.

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [134] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: The paper studies how multi-agent LLM systems degrade over long interactions and proposes ways to measure and reduce this degradation (called agent drift).


<details>
  <summary>Details</summary>
Motivation: While multi-agent LLM systems are promising for complex tasks, little is known about how their behavior changes and degrades over long-term use. There is a need to formally characterize and quantify this degradation to ensure reliability and safety in real-world deployments.

Method: The authors define and categorize agent drift into semantic, coordination, and behavioral drift. They design the Agent Stability Index (ASI), a composite metric over twelve dimensions (e.g., response consistency, tool usage patterns, reasoning stability, and inter-agent agreement). They use simulation-based analysis and theoretical modeling to examine how drift affects task performance and human intervention needs, and they analytically evaluate three mitigation strategies: episodic memory consolidation, drift-aware routing, and adaptive behavioral anchoring.

Result: The analyses show that agent drift significantly harms task accuracy and increases the need for human oversight in multi-agent LLM systems. The proposed ASI effectively captures multiple aspects of drift. Theoretical evaluation indicates that the three mitigation strategies can notably lower drift-related errors without sacrificing throughput.

Conclusion: Agent drift is a serious, multi-faceted issue in multi-agent LLM systems. The paper offers a foundational framework (via ASI) to measure and track drift, and introduces mitigation strategies that can enhance long-term stability, reliability, and safety of agentic AI deployments, especially in enterprise settings.

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>
