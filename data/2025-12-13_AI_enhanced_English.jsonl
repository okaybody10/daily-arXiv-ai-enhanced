{"id": "2512.10865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10865", "abs": "https://arxiv.org/abs/2512.10865", "authors": ["Lilin Qiu"], "title": "Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python", "comment": null, "summary": "This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.", "AI": {"tldr": "Computational analysis of dialogue in The Hobbit reveals an overall positive, calm tone with growing agency across the story.", "motivation": "To quantitatively explore the emotional dynamics of dialogue in Tolkien\u2019s The Hobbit and show how digital methods can capture literary emotional structure.", "method": "Dialogue is extracted from the novel using regular expressions, preprocessed, and then scored with the NRC-VAD lexicon to obtain valence, arousal, and dominance values. The emotional scores are analyzed over narrative time and visualized through emotional trajectory graphs and word clouds, combined with literary interpretation.", "result": "The dialogue in The Hobbit is found to be generally high in valence (positive), low in arousal (calm), and to exhibit an increasing dominance (agency) dimension as the story advances. Emotional patterns show cycles where tense or dangerous moments are followed by scenes of humor, camaraderie, and relief, forming a clear emotional rhythm.", "conclusion": "Computational text analysis, particularly lexicon-based VAD scoring and visualization, can reveal subtle emotional structures and rhythmic patterns in literary dialogue. In The Hobbit, these methods expose a steady emotional modulation between tension and comfort and a growing sense of agency over the course of the narrative."}}
{"id": "2512.10882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10882", "abs": "https://arxiv.org/abs/2512.10882", "authors": ["Hauke Licht"], "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity", "comment": null, "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.", "AI": {"tldr": "The paper evaluates how well current multimodal large language models (mLLMs) can rate emotional arousal from political videos, finding strong performance in idealized data but weak, potentially misleading performance in real parliamentary debates.", "motivation": "Emotions are crucial in political communication research, and scholars increasingly use audio-visual data to study them. With the rapid rise of multimodal generative AI, there is great potential to automate emotion analysis from video. Yet there is little empirical evidence on how accurate and unbiased these models are, especially in realistic political settings, creating a need for systematic evaluation.", "method": "The author assesses current multimodal large language models on video-based emotion analysis, focusing on emotional arousal. They use two complementary human-labeled video datasets: one representing ideal, controlled conditions, and another consisting of real-world parliamentary debate recordings. They compare mLLM arousal ratings to human annotations and examine reliability and demographic bias, as well as implications for downstream statistical analyses.", "result": "In controlled or \u201cideal\u201d video settings, mLLMs\u2019 arousal ratings correlate strongly with human labels and exhibit little to no detectable demographic bias. However, when applied to real-world parliamentary debate recordings, the same models produce arousal ratings that are unreliable and fail to match human-coded patterns, undermining their utility and potentially distorting subsequent empirical findings.", "conclusion": "Multimodal LLMs can perform well in clean, controlled contexts but currently fall short in complex, real-world political video data. Relying on them uncritically for emotion analysis in political research can lead to flawed inferences. The paper highlights the importance of rigorous, context-specific validation of generative AI tools and offers a replicable framework for evaluating mLLMs in political communication studies."}}
{"id": "2512.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10895", "abs": "https://arxiv.org/abs/2512.10895", "authors": ["Lijie Ding", "Janell Thomson", "Jon Taylor", "Changwoo Do"], "title": "LLMs Can Assist with Proposal Selection at Large User Facilities", "comment": "9 pages, 8figures", "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\u03c1\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.", "AI": {"tldr": "The paper investigates using large language models to assist or replace human reviewers in selecting scientific proposals at large research facilities, focusing on ranking quality and cost-effectiveness.", "motivation": "Traditional human-based proposal review suffers from bias, inconsistency, and low inter-reviewer agreement, and more principled pairwise comparison methods are too labor-intensive to scale. Large user facilities need a more scalable, consistent, and cheaper way to rank many proposals while still identifying those with high scientific impact potential.", "method": "The authors use LLMs to perform pairwise preference-based comparisons between research proposals, leveraging curated historical proposals and associated publication records from three beamlines at the Spallation Neutron Source. They generate LLM-based rankings, compare them with human rankings using Spearman correlation, and evaluate how well LLMs predict proposals with high publication potential. They also apply embedding models to quantify proposal similarity as an additional analysis tool.", "result": "LLM-generated rankings show strong correlation with human reviewer rankings, with Spearman correlation coefficients ranging from about 0.2 to 0.8 and improving to at least 0.5 after removing 10% of outliers. LLMs are at least as effective as human reviewers at identifying proposals that lead to a high number of publications, while being over 100 times cheaper in terms of review cost. The embedding-based similarity analysis offers extra insights for review committees that are difficult to obtain from human judgment alone.", "conclusion": "LLMs can effectively support or augment the proposal selection process at large research facilities, achieving human-comparable performance in ranking and impact prediction at a fraction of the cost. Pairwise preference-based ranking becomes practical with LLMs, and embedding-based similarity analysis adds valuable information for committee deliberations. This suggests that LLMs are a promising tool for scalable, consistent, and cost-effective proposal review systems."}}
{"id": "2512.10903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10903", "abs": "https://arxiv.org/abs/2512.10903", "authors": ["Muhammad Umair Haider", "Hammad Rizwan", "Hassan Sajjad", "A. B. Siddique"], "title": "Multi-Granular Node Pruning for Circuit Discovery", "comment": null, "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.", "AI": {"tldr": "The paper proposes a more fine-grained and efficient method to discover functional circuits inside LLMs by pruning at the node (neuron) level rather than only at coarse components like heads or blocks.", "motivation": "Circuit discovery is important for understanding how specific behaviors arise in LLMs, but existing methods are computationally expensive and operate only at coarse granularity (attention heads or MLP blocks). This misses finer structures such as individual neurons and limits scalability, both in compute and memory, making it hard to precisely identify the minimal subnetworks responsible for behaviors.", "method": "Introduce a node-level pruning framework with learnable masks applied at multiple granularities (from whole blocks down to individual neurons) under a unified optimization objective. Use granularity-specific sparsity penalties to guide the pruning so that, during one fine-tuning run, the model learns which units at each level can be removed while preserving behavior. The approach avoids storing intermediate activations, reducing memory costs.", "result": "The method finds circuits that contain fewer nodes than those from prior, coarser pruning approaches. It reveals that many units previously considered important (e.g., whole heads or blocks) actually contain irrelevant neurons that can be removed without hurting task performance. The framework also reduces memory usage by 5\u201310x because it doesn\u2019t rely on caching intermediate activations.", "conclusion": "Fine-grained, node-level pruning with multi-granularity masks is an effective and scalable way to discover smaller, more accurate circuits in LLMs. It challenges the assumption that coarse components are the true functional units and shows that many neurons inside them are unnecessary for the target behaviors, while also being substantially more memory-efficient than existing circuit discovery methods."}}
{"id": "2512.10937", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.10937", "abs": "https://arxiv.org/abs/2512.10937", "authors": ["Matt Wilson"], "title": "On Decision-Making Agents and Higher-Order Causal Processes", "comment": null, "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.", "AI": {"tldr": "They show a one-to-one correspondence between POMDP agents and one-input process functions (from quantum information theory), and extend it to decentralized multi-agent settings.", "motivation": "Connect AI decision-making models (POMDP agents) with structures from the theory of higher-order quantum operations, aiming for a unified formal framework and dual interpretation of 'agent vs environment'.", "method": "Mathematically map an agent's policy plus memory update in a POMDP to a single process function that interacts with the environment via the link product; then generalize this mapping to multi-agent, observation-independent decentralized POMDPs and multi-input process functions.", "result": "They obtain a precise identification/equivalence between (i) decision-making agents in POMDPs and one-input process functions, and (ii) decentralized POMDPs and multi-input process functions, allowing either side (agent or environment) to be viewed as the process depending on perspective.", "conclusion": "Process functions provide a natural, symmetric formalism for modeling both agents and environments in partially observable and decentralized decision problems, bridging AI models with higher-order quantum processes and offering a dual agent\u2013environment interpretation."}}
