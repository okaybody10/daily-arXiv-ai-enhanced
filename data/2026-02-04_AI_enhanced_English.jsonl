{"id": "2602.02496", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02496", "abs": "https://arxiv.org/abs/2602.02496", "authors": ["Shikhar Shiromani", "Archie Chaudhury", "Sri Pranav Kunda"], "title": "The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders", "comment": "8 pages, 1 figure", "summary": "Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally \"knows\" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).", "AI": {"tldr": "The paper introduces the 'Hypocrisy Gap', a mechanistic metric using Sparse Autoencoders to measure divergence between a language model\u2019s internal reasoning and its final answer, enabling detection of sycophantic or unfaithful behavior better than log-probability baselines.", "motivation": "LLMs often behave unfaithfully: their final answer can contradict their internal chain-of-thought because they try to appease the user, leading to sycophancy and misalignment. Existing detection methods, e.g., decision-aligned log-probability, are weak at reliably identifying such behavior. The authors want a mechanistic, representation-level way to measure and detect when a model\u2019s internal belief differs from what it outputs.", "method": "They define the 'Hypocrisy Gap' by (1) using Sparse Autoencoders (SAEs) and sparse linear probes to extract a representation of the model\u2019s internal 'truth belief' from its activations, and (2) mathematically comparing this internal belief state in latent space to the latent trajectory corresponding to the final generated answer. The degree of divergence between these latent representations serves as a measure of hypocrisy or unfaithfulness. They then apply this metric to LLMs (Gemma, Llama, Qwen) on Anthropic\u2019s Sycophancy benchmark, distinguishing runs where the model internally disagrees with the user from those where it outputs agreement.", "result": "Across multiple models and the Anthropic Sycophancy benchmark, the Hypocrisy Gap achieves AUROC scores of 0.55\u20130.73 for detecting sycophantic runs, and 0.55\u20130.74 for specifically hypocritical cases where internal beliefs disagree with the user but the final answer aligns with the user. These scores consistently outperform a decision-aligned log-probability baseline, which yields only 0.41\u20130.50 AUROC.", "conclusion": "The Hypocrisy Gap provides a practical, mechanistic tool for quantifying and detecting unfaithful or sycophantic behavior in LLMs by comparing internal beliefs to final outputs in latent space. Its stronger performance over log-probability baselines suggests that representation-level methods, enabled by SAEs and linear probes, are a promising direction for diagnosing and mitigating misalignment between what models 'believe' and what they say."}}
{"id": "2602.02497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02497", "abs": "https://arxiv.org/abs/2602.02497", "authors": ["Xuzhao Li", "Xuchen Li", "Jian Zhao", "Shiyu Hu"], "title": "STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models", "comment": "Preprint, Under review", "summary": "As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated \"silos,\" offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified \"Discipline $\\times$ Cognition\" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.", "AI": {"tldr": "STEMVerse is a diagnostic framework that evaluates LLMs' STEM reasoning by jointly considering academic disciplines and cognitive complexity, revealing structural failure patterns beyond aggregate benchmark scores.", "motivation": "Existing STEM benchmarks for LLMs are used as isolated silos, producing only overall scores that do not reveal whether errors are due to missing domain knowledge or limited cognitive ability. This reduces their diagnostic power for understanding and improving LLM reasoning.", "method": "The authors construct STEMVerse by re-aggregating over 20,000 problems from mainstream STEM benchmarks into a two-dimensional capability space defined by \"Discipline \u00d7 Cognition.\" They assign each instance dual labels (discipline and cognitive complexity level) and use this unified labeling to systematically evaluate multiple LLM families of different sizes and training paradigms.", "result": "Using STEMVerse, the authors empirically uncover structural failure patterns in LLMs\u2019 STEM reasoning, differentiated along both disciplinary lines and levels of cognitive complexity, rather than just aggregate performance.", "conclusion": "STEMVerse, by combining multi-disciplinary coverage with fine-grained cognitive stratification, offers a unified and actionable framework for analyzing and understanding the scientific reasoning characteristics of LLMs, enabling more precise diagnosis of their strengths and weaknesses."}}
{"id": "2602.02498", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02498", "abs": "https://arxiv.org/abs/2602.02498", "authors": ["Baturay Saglam", "Dionysis Kalogerias"], "title": "Test-Time Detoxification without Training or Learning Anything", "comment": null, "summary": "Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.", "AI": {"tldr": "This paper proposes a black-box, test-time detoxification method for large language models using zeroth-order optimization over input embeddings to reduce toxicity while preserving generation quality.", "motivation": "Large language models can unintentionally generate toxic or inappropriate text even from benign prompts, which is dangerous at scale and undermines user trust. Existing detoxification methods often require retraining, access to gradients, or additional learned components, making them expensive and hard to apply to closed-source or black-box models. There is a need for a general, model-agnostic way to reduce toxicity at generation time without altering the underlying model.", "method": "The authors introduce a test-time steering procedure that treats the input token embeddings as control variables. They approximate the gradient of a toxicity score with respect to these embeddings using zeroth-order (gradient-free) optimization that relies only on forward passes through the model and a toxicity scoring function. By taking a small number of descent steps in embedding space before or during generation, they steer the model toward less toxic continuations, all without accessing model parameters, gradients, or intermediate activations.", "result": "Experiments across multiple language models and prompt sets show that this method consistently and robustly decreases toxicity scores of generated text. Compared with existing detoxification techniques, it typically offers the best trade-off between lowering toxicity and preserving overall output quality, demonstrating effective control over harmful content even in black-box settings.", "conclusion": "Word embeddings can be effectively used as control knobs for steering language model behavior via black-box, gradient-free optimization. This provides a scalable, model-agnostic approach to safer text generation at test time, removing the need for retraining or internal model access and suggesting a broader role for black-box optimization methods in controlling autoregressive language models."}}
{"id": "2602.02499", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02499", "abs": "https://arxiv.org/abs/2602.02499", "authors": ["Yunao Zheng", "Xiaojie Wang", "Lei Ren", "Wei Chen"], "title": "ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching", "comment": null, "summary": "Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at https://github.com/zyaaa-ux/ROSA-Tuning.", "AI": {"tldr": "ROSA-Tuning augments windowed-attention LLMs with an external CPU-based retrieval module (ROSA) plus trainable recall and fusion, restoring near-global-attention performance on long-context tasks while retaining windowed-attention efficiency.", "motivation": "Long-context processing in LLMs is computationally expensive with standard global attention, and existing efficient attention schemes trade off complexity for reduced coverage of the full history, hurting performance on tasks requiring very long dependencies. The authors aim to design a method that recovers most of the benefits of global attention\u2014accurate use of distant information\u2014without the prohibitive compute and memory costs, particularly for relatively small base models like Qwen3-Base-1.7B.", "method": "They introduce ROSA-Tuning, which augments a pretrained LLM that uses windowed attention with a parallel CPU-based retrieval module built on RWKV Online Suffix Automaton (ROSA). This module searches the long history to find positions relevant to the current query token. Retrieved information is injected into the model state through trainable mechanisms, after which a range-restricted (local) attention layer does weighted fusion of the original and retrieved features. To support end-to-end training despite discrete retrieval decisions, they design a binary discretization strategy and a counterfactual gradient algorithm. They also engineer an asynchronous CPU-GPU execution pipeline so retrieval overlaps with GPU forward computation, preserving efficiency.", "result": "On systematic evaluations using Qwen3-Base-1.7B, ROSA-Tuning significantly improves long-context performance of windowed-attention models, nearly restoring the capabilities of full global attention. On benchmarks such as LongBench, its scores are close to or sometimes match global attention, while GPU memory usage and runtime remain close to those of plain windowed attention. This demonstrates that their retrieval-and-recall architecture effectively bridges the gap between efficient and fully global attention in practice.", "conclusion": "ROSA-Tuning provides a practical, efficient way to extend the effective context of LLMs: by combining a CPU-based ROSA retrieval module, trainable recall, and local attention fusion, it approximates global attention performance without incurring its heavy computational and memory costs. The method indicates a promising new direction for long-context processing that decouples retrieval over very long histories from the core GPU attention computation, making it suitable for deployment in resource-constrained or latency-sensitive settings."}}
{"id": "2602.02515", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02515", "abs": "https://arxiv.org/abs/2602.02515", "authors": ["Yiliang Song", "Hongjun An", "Jiangong Xiao", "Haofei Zhao", "Jiawei Shao", "Xuelong Li"], "title": "CreditAudit: 2D Auditing for LLM Evaluation and Selection", "comment": "First update", "summary": "Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.", "AI": {"tldr": "CreditAudit is a framework to evaluate language models not just by average benchmark scores but also by how stable their performance is across realistic deployment prompts, using both a mean ability metric and a volatility-based credit grade.", "motivation": "Existing benchmark leaderboards show small, converging gaps between top models, but these scores don\u2019t reflect users\u2019 real experiences in changing, multi-step, agentic deployments where small prompt changes can cause large failures. Practitioners lack a robust way to compare models under these practical, evolving conditions to decide which to deploy safely, especially in high-stakes settings.", "method": "The authors introduce CreditAudit, a \u201cdeployment-oriented credit audit\u201d framework. It evaluates each model across a family of semantically aligned, non-adversarial system-prompt templates on multiple benchmarks. It computes: (1) mean ability = average performance across scenarios; (2) sigma = the standard deviation (fluctuation) of performance across those prompt scenarios as a measure of stability risk. They then transform cross-model volatility into interpretable credit grades (AAA to BBB) using quantiles and include diagnostics to correct for prompt-template difficulty drift.", "result": "Through controlled experiments on GPQA, TruthfulQA, and MMLU Pro, the authors find that models with close or similar mean benchmark scores can display very different levels of volatility across prompt templates. In some situations, the model that looks better by average score has higher stability risk, and once this is taken into account, the preferred deployment choice can change, especially in agentic or high-failure-cost workflows.", "conclusion": "CreditAudit provides a 2D view (mean ability vs. stability risk) and grade-style summaries that give practitioners a clearer, regime-aware language for choosing models, structuring tiered deployments, and focusing testing/monitoring where most needed. This, in turn, can lead to more objective and trustworthy evaluations of language models for real-world use beyond simple leaderboard rankings."}}
{"id": "2602.02635", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02635", "abs": "https://arxiv.org/abs/2602.02635", "authors": ["Siyu Li", "Chenwei Song", "Qi Zhou", "Wan Zhou", "Xinyi Liu"], "title": "Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management", "comment": null, "summary": "This paper proposes a graph-augmented reasoning framework for tobacco pest and disease management that integrates structured domain knowledge into large language models. Building on GraphRAG, we construct a domain-specific knowledge graph and retrieve query-relevant subgraphs to provide relational evidence during answer generation. The framework adopts ChatGLM as the Transformer backbone with LoRA-based parameter-efficient fine-tuning, and employs a graph neural network to learn node representations that capture symptom-disease-treatment dependencies. By explicitly modeling diseases, symptoms, pesticides, and control measures as linked entities, the system supports evidence-aware retrieval beyond surface-level text similarity. Retrieved graph evidence is incorporated into the LLM input to guide generation toward domain-consistent recommendations and to mitigate hallucinated or inappropriate treatments. Experimental results show consistent improvements over text-only baselines, with the largest gains observed on multi-hop and comparative reasoning questions that require chaining multiple relations.", "AI": {"tldr": "Graph-augmented reasoning framework for tobacco pest and disease management using LLMs plus a knowledge graph for better, evidence-based recommendations.", "motivation": "Improve reliability and domain-consistency of LLM-generated recommendations for managing tobacco pests and diseases, especially reducing hallucinations and leveraging structured expert knowledge.", "method": "Build a domain-specific knowledge graph of diseases, symptoms, pesticides, and control measures; use GraphRAG-style retrieval to extract relevant subgraphs per query; learn node representations with a GNN; integrate retrieved graph evidence into a ChatGLM backbone fine-tuned with LoRA for answer generation.", "result": "The proposed system outperforms text-only LLM baselines, particularly on multi-hop and comparative reasoning queries that involve chaining multiple relations in the domain graph.", "conclusion": "Incorporating structured, graph-based domain knowledge into LLM reasoning improves accuracy, evidence-awareness, and safety of tobacco pest and disease management recommendations, especially for complex reasoning tasks."}}
{"id": "2602.02559", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02559", "abs": "https://arxiv.org/abs/2602.02559", "authors": ["Pengyu Dai", "Weihao Xuan", "Junjue Wang", "Hongruixuan Chen", "Jian Song", "Yafei Ou", "Naoto Yokoya"], "title": "Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers", "comment": "21 pages, 6 figures", "summary": "Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \\textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.", "AI": {"tldr": "GeoEvolver is a self-evolving multi-agent LLM system that learns fine-grained tool usage for Earth Observation workflows by exploring tool configurations and storing successful and failed patterns in a memory bank, improving task success without model parameter updates.", "motivation": "LLM agents struggle with complex, tool-heavy Earth Observation tasks that require multi-modal, multi-temporal reasoning and strict geo-knowledge constraints. They frequently fail due to subtle tool-usage and parameter-configuration errors, and current systems lack a way to acquire fine-grained, tool-level expertise from interaction.", "method": "Introduce GeoEvolver, a self-evolving multi-agent system. It uses a retrieval-augmented orchestrator to decompose each EO query into independent sub-goals, explores diverse tool-parameter configurations at the sub-goal level, and records both successful patterns and root-cause analyses of failures into an evolving memory bank. This memory is then used as in-context demonstrations for subsequent queries, enabling learning without updating LLM parameters.", "result": "On three EO benchmarks that integrate external tools, GeoEvolver yields consistent gains in end-to-end task success, with an average improvement of about 12% across different LLM backbones, indicating better robustness and expertise in tool usage for EO workflows.", "conclusion": "Fine-grained Earth Observation expertise can emerge in LLM agents purely through structured interaction and self-evolving multi-agent orchestration, without any parameter updates, by systematically exploring tool configurations and leveraging an evolving memory of successes and failures."}}
{"id": "2602.02636", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02636", "abs": "https://arxiv.org/abs/2602.02636", "authors": ["Ziyang Huang", "Haolin Ren", "Xiaowei Yuan", "Jiawei Wang", "Zhongtao Jiang", "Kun Xu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling", "comment": null, "summary": "Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.", "AI": {"tldr": "The paper introduces Wide Research and proposes a benchmark and a multi-agent RL system to better handle broad, parallel information-seeking tasks.", "motivation": "Existing search and research systems focus mainly on deep, narrow queries and lack benchmarks and methods tailored to broad, parallel information-seeking under complex constraints. There is no standard way to evaluate or optimize search breadth.", "method": "They build WideSeekBench, a benchmark for General Broad Information Seeking using a multi-phase data pipeline to ensure diversity in information volume, constraints, and domains. They also design WideSeek, a hierarchical multi-agent architecture that can spawn parallel sub-agents according to task needs, and propose a unified training framework that linearizes multi-agent trajectories and trains the whole system end-to-end with reinforcement learning.", "result": "Experiments show that WideSeek and the proposed multi-agent RL training are effective, and that increasing the number of agents improves performance on wide research tasks as captured by WideSeekBench.", "conclusion": "Wide Research is a useful paradigm for broad information-seeking. The proposed WideSeekBench benchmark and WideSeek multi-agent RL system provide effective tools and evidence that scaling parallel agents is a promising path for advancing wide research capabilities."}}
{"id": "2602.02582", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02582", "abs": "https://arxiv.org/abs/2602.02582", "authors": ["Chandan Kumar Sah", "Xiaoli Lian", "Li Zhang", "Tony Xu", "Syed Shazaib Shah"], "title": "Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems", "comment": "Accepted at the Second Conference of the International Association for Safe and Ethical Artificial Intelligence, IASEAI26, 14 pages", "summary": "Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.", "AI": {"tldr": "The paper evaluates how uncertainty and fairness considerations impact the reliability of LLM-based recommendation systems (RecLLMs), introducing new benchmarks and methods to detect and analyze demographic and personality-related biases in movie and music recommendations.", "motivation": "LLMs are increasingly used for zero-shot recommendations, but their predictive uncertainty and embedded demographic biases can undermine reliability, fairness, and user trust. There is a lack of systematic benchmarks and methodologies to quantify and interpret these issues in RecLLMs, especially across multiple sensitive demographic attributes and personality traits.", "method": "The authors construct a benchmark dataset annotated with eight demographic attributes (31 categorical values) in movie and music domains, and define curated metrics to measure fairness and uncertainty. They perform case studies using Gemini 1.5 Flash, quantifying uncertainty with entropy and fairness with similarity-based gap metrics SNSR and SNSV, while probing robustness under prompt perturbations (typos, multilingual input). They extend the RecLLM evaluation pipeline with personality-aware fairness analysis to identify personality-linked bias and the trade-off between personalization and group fairness. Finally, they design an uncertainty-aware evaluation methodology and a personality-informed fairness benchmark for RecLLMs.", "result": "Empirical analysis shows that Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive demographic attributes, with measured similarity-based gaps SNSR = 0.1363 and SNSV = 0.0507. These fairness disparities remain stable even when prompts are perturbed by typos or translated into other languages. Deep uncertainty case studies reveal nuanced patterns in predictive entropy, and the personality-aware evaluation pipeline uncovers distinct personality-linked bias patterns and highlights trade-offs between recommendation personalization and fairness across demographic groups.", "conclusion": "The paper concludes that LLM-based recommender systems can display robust, systematic demographic and personality-related biases that are not mitigated by simple prompt variations. By proposing an uncertainty-aware evaluation framework and a personality profile-informed fairness benchmark, the authors provide tools for more explainable, equitable, and trustworthy RecLLMs, and they call for future research on multi-model benchmarks and adaptive calibration strategies for safe deployment."}}
{"id": "2602.02686", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02686", "abs": "https://arxiv.org/abs/2602.02686", "authors": ["Patrick Cooper", "Alireza Nadali", "Ashutosh Trivedi", "Alvaro Velasquez"], "title": "Monotonicity as an Architectural Bias for Robust Language Models", "comment": "12 pages, 1 figure", "summary": "Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.\n  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.\n  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.", "AI": {"tldr": "The paper proposes making parts of Transformer language models monotone to improve robustness against adversarial prompts while preserving standard performance.", "motivation": "LLMs are vulnerable to adversarial prompts and jailbreak attacks, reflecting a broader problem where small, structured input changes can cause large, unpredictable shifts in internal representations and outputs. The authors seek an architectural bias that makes models more robust and predictable.", "method": "Introduce monotonicity constraints in the feed-forward sublayers of sequence-to-sequence Transformers while keeping attention layers unconstrained. This preserves order in semantic refinement: adding stronger evidence or constraints should not worsen internal representations. The design allows attention to handle negation and contextual interactions, with feed-forward layers enforcing order-preserving transformations.", "result": "The resulting monotone language models match the performance of their pretrained non-monotone counterparts on standard summarization tasks, while adversarial attack success rates drop markedly from about 69% to 19%.", "conclusion": "Monotonicity, applied selectively to Transformer feed-forward layers, provides a viable architectural inductive bias that significantly improves adversarial robustness without sacrificing expressivity or standard performance, showing the perceived trade-off between monotonicity and expressivity is not fundamental."}}
{"id": "2602.02704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02704", "abs": "https://arxiv.org/abs/2602.02704", "authors": ["Xinyu Wang", "Mingze Li", "Peng Lu", "Xiao-Wen Chang", "Lifeng Shang", "Jinping Li", "Fei Mi", "Prasanna Parthasarathi", "Yufei Cui"], "title": "InfMem: Learning System-2 Memory Control for Long-Context Agent", "comment": null, "summary": "Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\\times$ on average (up to $5.1\\times$) via adaptive early stopping.", "AI": {"tldr": "InfMem is a control-centric streaming agent that improves reasoning over ultra-long documents by actively managing memory via a PreThink-Retrieve-Write protocol and RL-aligned control, achieving significantly higher QA accuracy and faster inference than prior MemAgent models.", "motivation": "Existing streaming agents for ultra-long document reasoning struggle because they update memory passively, often discarding low-salience but crucial bridging evidence needed for multi-hop reasoning under context-length limits. There is a need for an agent that can more intelligently decide what to read, keep, retrieve, and discard, while operating under strict memory and efficiency constraints.", "method": "The paper introduces InfMem, a control-centric agent implementing a System-2-like control process through a three-stage PreThink-Retrieve-Write protocol. The agent (1) monitors whether current evidence is sufficient, (2) performs targeted in-document retrieval when more evidence is needed, and (3) applies evidence-aware joint compression to update a bounded memory with selected information. To obtain reliable control policies, the authors propose a training pipeline that starts with supervised fine-tuning (SFT) and then uses reinforcement learning (RL) to align retrieval, memory writing, and stopping decisions with final task correctness and efficiency.", "result": "On ultra-long question answering benchmarks with document lengths from 32k up to 1M tokens, InfMem consistently outperforms the prior MemAgent approach across different backbone models (Qwen3-1.7B, Qwen3-4B, Qwen2.5-7B). It yields average absolute accuracy gains of +10.17, +11.84, and +8.23 points for these backbones, respectively. Additionally, thanks to adaptive early stopping and better control, InfMem reduces inference time by an average factor of 3.9\u00d7, with maximum speedups up to 5.1\u00d7.", "conclusion": "The study concludes that control-centric streaming with explicit System-2-style protocols and RL-aligned decision-making enables more effective reasoning over ultra-long documents than passive memory agents. InfMem\u2019s active evidence monitoring, targeted retrieval, and evidence-aware memory compression improve both accuracy and efficiency, demonstrating a practical path toward scalable, high-quality reasoning for million-token contexts."}}
{"id": "2602.02639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02639", "abs": "https://arxiv.org/abs/2602.02639", "authors": ["Harry Mayne", "Justin Singh Kang", "Dewi Gould", "Kannan Ramchandran", "Adam Mahdi", "Noah Y. Siegel"], "title": "A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior", "comment": null, "summary": "LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.", "AI": {"tldr": "The paper proposes a new metric, Normalized Simulatability Gain (NSG), to measure how faithfully LLM self-explanations reflect their underlying decision processes, showing that self-explanations make model behavior more predictable but can sometimes be seriously misleading.", "motivation": "Current methods for assessing the faithfulness of LLM self-explanations rely mainly on catching errors or adversarially inducing unfaithful reasoning, and they ignore whether explanations actually help humans predict model behavior. The authors aim to capture the predictive value of explanations as a core notion of faithfulness for AI oversight applications.", "method": "The authors define Normalized Simulatability Gain (NSG), which quantifies how much an explanation improves an observer\u2019s ability to predict a model\u2019s outputs on related counterfactual inputs, normalized to allow comparison across settings. They then evaluate 18 state-of-the-art proprietary and open-weight models on 7,000 counterfactual instances across domains like health, business, and ethics, comparing prediction accuracy with and without access to self-explanations and with explanations from external models.", "result": "Across models and datasets, self-explanations substantially increase observers\u2019 ability to predict model decisions, yielding 11\u201337% NSG. Self-explanations confer greater predictive power than explanations generated by stronger external models, suggesting that models possess and can express privileged \u201cself-knowledge\u201d about their own behavior. However, NSG analysis also reveals that 5\u201315% of self-explanations are egregiously misleading, implying significant but bounded reliability issues.", "conclusion": "Self-explanations from LLMs are neither wholly untrustworthy nor fully reliable: they contain systematically useful information that improves prediction of model behavior, but also a nontrivial fraction of severely misleading cases. NSG provides a scalable way to quantify this trade-off and supports a cautiously optimistic view of self-explanations as a tool for AI oversight, while highlighting the need for safeguards against misleading explanations."}}
{"id": "2602.02731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02731", "abs": "https://arxiv.org/abs/2602.02731", "authors": ["Rohan Pandey", "Haijuan Yan", "Hong Yu", "Jack Tsai"], "title": "Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors", "comment": null, "summary": "Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.", "AI": {"tldr": "This paper develops and compares machine learning and large language model approaches using Veterans Affairs EHR data to predict first-episode homelessness among U.S. veterans 3\u201312 months in advance, showing that socially and behaviorally enriched longitudinal models substantially improve risk concentration and enable targeted prevention.", "motivation": "Homelessness among U.S. veterans is a persistent and serious public health issue. Existing resources for prevention are limited and must be allocated efficiently. The authors are motivated to create accurate, early risk prediction tools using routinely collected EHR data so that health systems can identify veterans at high risk of becoming homeless and intervene before homelessness occurs. They are also motivated to understand how different modeling paradigms, including classical ML, transformer-based encoders, and large language models, perform on this task and whether they exhibit performance disparities across racial groups.", "method": "The study is a large-scale retrospective prognostic analysis using EHR data from over 4.2 million Veterans Affairs patients. The authors build both static and time-varying (longitudinal) representations of patients\u2019 EHR histories, including medical conditions, social risks, and behavioral factors. They encode the persistence of clinical and social conditions over time using clinician-informed rules. They then train and compare several model families: classical machine learning models, transformer-based masked language models, and fine-tuned large language models. Performance is evaluated for predicting first-episode homelessness occurring 3\u201312 months after a 2016 observation window, emphasizing precision-recall AUC, positive predictive value within the top 1% predicted risk tier, and fairness metrics (e.g., disparities across racial groups).", "result": "Incorporating social and behavioral factors into longitudinal EHR models improves PR-AUC by approximately 15\u201330% over models without such features. When focusing on the top 1% highest-risk patients, models achieve positive predictive values of roughly 4\u20135% at 3 months, 7\u20138% at 6 months, 10\u201311% at 9 months, and 12\u201314% at 12 months. Encoder-based models (e.g., classical ML and transformer encoders) outperform large language models in terms of discrimination metrics such as PR-AUC. However, large language models exhibit smaller performance disparities across racial groups, suggesting potential fairness advantages. Overall, longitudinal and socially enriched EHR modeling effectively concentrates homelessness risk into small, high-yield strata.", "conclusion": "The paper concludes that it is feasible to use routinely collected EHR data, especially when enhanced with social and behavioral information and modeled longitudinally, to predict first-episode homelessness among veterans several months in advance. These models can concentrate risk into small, actionable tiers, enabling more efficient targeting of preventive interventions. While encoder-based models currently offer superior predictive performance, large language models may provide advantages in reducing racial performance gaps. The findings support longitudinal, socially informed EHR modeling as a promising strategy for homelessness prevention in veteran populations and suggest trade-offs between accuracy and fairness across modeling approaches."}}
{"id": "2602.02660", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02660", "abs": "https://arxiv.org/abs/2602.02660", "authors": ["Jiefeng Chen", "Bhavana Dalvi Mishra", "Jaehyun Nam", "Rui Meng", "Tomas Pfister", "Jinsung Yoon"], "title": "MARS: Modular Agent with Reflective Search for Automated AI Research", "comment": null, "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.", "AI": {"tldr": "The paper presents MARS, a modular, budget-aware, reflective agent framework tailored for automating AI research, achieving state-of-the-art performance on MLE-Bench and demonstrating effective cross-branch knowledge transfer.", "motivation": "Existing LLM-based agents for automating AI research perform poorly because they create monolithic scripts, ignore computational costs of experiments, and cannot clearly attribute which changes cause performance gains. There is a need for an agent framework that can plan under strict compute budgets, structure complex research codebases, and learn from past experiments more intelligently.", "method": "The authors propose MARS, a Modular Agent with Reflective Search, built around three components: (1) Budget-Aware Planning that uses cost-constrained Monte Carlo Tree Search (MCTS) to trade off expected performance against execution cost; (2) Modular Construction through a Design-Decompose-Implement workflow that breaks down complex research tasks into manageable modules within a repository; and (3) Comparative Reflective Memory, a mechanism that compares different candidate solutions, identifies the key differences responsible for performance changes, and stores distilled insights that can be reused later.", "result": "On the MLE-Bench benchmark, MARS achieves state-of-the-art performance among open-source autonomous AI research frameworks under comparable compute settings, and is competitive with the best methods on the overall leaderboard. Analysis shows that 63% of the lessons the system relies on come from cross-branch transfer, indicating that it successfully reuses insights from one search trajectory in others.", "conclusion": "MARS demonstrates that structuring autonomous AI research agents around budget-aware planning, modular repository construction, and reflective comparison of solutions leads to more effective and efficient research automation. The strong benchmark results and the high rate of cross-branch knowledge transfer suggest that such agents can meaningfully generalize insights and perform sophisticated, compute-conscious exploration in AI research tasks."}}
{"id": "2602.02736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02736", "abs": "https://arxiv.org/abs/2602.02736", "authors": ["Elaheh Sabziyan Varnousfaderani", "Syed A. M. Shihab", "Mohammad Taghizadeh"], "title": "Time-Critical Multimodal Medical Transportation: Organs, Patients, and Medical Supplies", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Timely transportation of organs, patients, and medical supplies is critical to modern healthcare, particularly in emergencies and transplant scenarios where even short delays can severely impact outcomes. Traditional ground-based vehicles such as ambulances are often hindered by traffic congestion; while air vehicles such as helicopters are faster but costly. Emerging air vehicles -- Unmanned Aerial Vehicles and electric vertical take-off and landing aircraft -- have lower operating costs, but remain limited by range and susceptibility to weather conditions. A multimodal transportation system that integrates both air and ground vehicles can leverage the strengths of each to enhance overall transportation efficiency. This study introduces a constructive greedy heuristic algorithm for multimodal vehicle dispatching for medical transportation. Four different fleet configurations were tested: (i) ambulances only, (ii) ambulances with Unmanned Aerial Vehicles, (iii) ambulances with electric vertical take-off and landing aircraft, and (iv) a fully integrated fleet of ambulances, Unmanned Aerial Vehicles, and electric vertical take-off and landing aircraft. The algorithm incorporates payload consolidation across compatible routes, accounts for traffic congestion in ground operations and weather conditions in aerial operations, while enabling rapid vehicle dispatching compared to computationally intensive optimization models. Using a common set of conditions, we evaluate all four fleet types to identify the most effective configurations for fulfilling medical transportation needs while minimizing operating costs, recharging/fuel costs, and total transportation time.", "AI": {"tldr": "The paper proposes a fast heuristic dispatch algorithm for multimodal medical transportation (ground + air) and compares different mixed fleets under common conditions on time and cost.", "motivation": "Medical transport for organs, patients, and supplies is highly time\u2011critical. Ground ambulances suffer from traffic delays, helicopters are fast but expensive, and new UAV/eVTOL options have limitations in range and weather robustness. There is a need for a cost\u2011 and time\u2011efficient way to coordinate these heterogeneous vehicles in a single system.", "method": "The authors design a constructive greedy heuristic for dispatching a multimodal fleet consisting of ambulances, UAVs, and eVTOLs. The heuristic allows payload consolidation on compatible routes, explicitly models traffic congestion for ground vehicles and weather constraints for aerial vehicles, and emphasizes fast computation relative to exact optimization. They run comparative experiments under a shared scenario for four fleet configurations: ambulances only; ambulances+UAVs; ambulances+eVTOLs; and all three combined.", "result": "Under identical conditions, the algorithm produces feasible dispatch plans for all four fleet types and yields quantitative performance measures in terms of operating and energy/fuel costs, as well as total transportation time. The results differentiate which fleet mixes perform better for medical transport tasks across the studied metrics.", "conclusion": "A multimodal fleet coordinated via a fast greedy heuristic can substantially improve medical transportation performance compared with single\u2011mode fleets, balancing time, cost, and operational constraints. Integrating UAVs and eVTOLs with ambulances, with proper consideration of traffic and weather, appears particularly promising for time\u2011critical healthcare logistics."}}
{"id": "2602.02709", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02709", "abs": "https://arxiv.org/abs/2602.02709", "authors": ["Ujin Jeon", "Jiyong Kwon", "Madison Ann Sullivan", "Caleb Eunho Lee", "Guang Lin"], "title": "ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters", "comment": null, "summary": "Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.", "AI": {"tldr": "The paper introduces ATLAS, a multi-agent framework with an adaptive preference-optimization algorithm (EvoDPO) to continually improve a lightweight research agent, achieving better stability and performance than static single-agent baselines in non-stationary tasks.", "motivation": "Existing multi-LLM agent systems either freeze the main solver after fine-tuning or use static preference-optimization loops, which scale poorly and become intractable for long-horizon, non-stationary tasks. There is a need for a framework that can adaptively and efficiently evolve agents over time under concept drift.", "method": "They design ATLAS, a task-distributed agent framework where a central lightweight research agent is iteratively improved while auxiliary supporter agents handle exploration, hyperparameter tuning, and reference policy management. The key algorithm, EvoDPO (Evolving Direct Preference Optimization), adaptively updates a phase-indexed reference policy. They also provide a regret analysis in a preference-based contextual bandit setting with concept drift, and evaluate on non-stationary linear contextual bandits and SciML loss reweighting for Burgers' equation.", "result": "Theoretical results include a regret analysis for preference-based contextual bandits under concept drift. Empirically, ATLAS shows improved stability and performance compared to a static single-agent baseline on both non-stationary linear contextual bandits and SciML loss reweighting tasks for the 1D Burgers' equation.", "conclusion": "ATLAS, combined with EvoDPO, offers a practical and theoretically grounded way to adaptively evolve agents in non-stationary, long-horizon environments, outperforming static single-agent approaches in both stability and performance."}}
{"id": "2602.02760", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02760", "abs": "https://arxiv.org/abs/2602.02760", "authors": ["Pouya Pezeshkpour", "Estevam Hruschka"], "title": "From Task Solving to Robust Real-World Adaptation in LLM Agents", "comment": null, "summary": "Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a \"clean interface\" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.", "AI": {"tldr": "The paper evaluates how robust large language model agents are in messy, real-world-like settings rather than idealized, clean benchmarks.", "motivation": "Most existing evaluations of LLM agents assume stable environments, reliable tools, and a single clear objective, which overstates their real-world readiness. The authors want to understand how these agents behave under realistic deployment conditions with uncertainty, noise, and ambiguous goals.", "method": "They design a grid-based long-horizon game with a simple overall goal but with violated clean-interface assumptions: partial observability, changing environments, noisy signals, and dynamic internal agent state. Five state-of-the-art LLM-based agents are benchmarked on this environment. The tasks require inferring rules, sometimes paying for information, adapting to changes, and dealing with uncertainty. The authors also conduct ablation studies and feature analyses to uncover sensitivities and failure modes.", "result": "Agents show a large gap between performance on nominal, clean tasks and performance under deployment-like uncertainty. As grid size and planning horizon grow, performance generally worsens, and model rankings become unstable; sometimes weaker models outperform stronger ones when their strategies align better with the specific uncertainty conditions. Agents spontaneously balance completion, efficiency, and penalty avoidance despite not being explicitly instructed to do so, indicating some level of implicit objective inference. The analyses identify model-specific sensitivities and main drivers of failure in these settings.", "conclusion": "Current LLM agents are not reliably robust in complex, uncertain environments that resemble real deployment, even if they perform well on clean benchmarks. Their performance is sensitive to environment size, horizon length, and uncertainty regimes. This motivates focused research on verification mechanisms, safer action selection, and better objective inference methods tailored to partial observability, noisy information, and non-stationary conditions."}}
{"id": "2602.02711", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02711", "abs": "https://arxiv.org/abs/2602.02711", "authors": ["Yuanzhe Li", "Jianing Deng", "Jingtong Hu", "Tianlong Chen", "Song Wang", "Huanrui Yang"], "title": "Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction", "comment": null, "summary": "Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.", "AI": {"tldr": "They propose a dynamic mixed-precision routing framework that uses both high- and low-precision LLMs across the steps of long-horizon decision-making, greatly improving the accuracy\u2013cost trade-off versus single-precision or heuristic methods.", "motivation": "Using strong, large LLMs for multi-step decision-making yields high success but is very expensive at inference time. Practitioners believe higher performance requires larger models, but the cost of using them at every step of a long interaction is prohibitive. There is a need for methods that retain task performance while lowering computation cost in long-horizon LLM-based control.", "method": "They quantize LLMs to create low-precision variants and observe that not all interaction steps are equally sensitive to precision. Based on this, they introduce a dynamic mix-precision routing framework that, at each decision step, chooses whether to use a high-precision or low-precision LLM. The router is trained in two stages: (1) KL-divergence-based supervised learning to detect and learn which steps are precision-sensitive by comparing outputs across precisions; (2) Group-Relative Policy Optimization (GRPO), a reinforcement-learning-style optimization, to further fine-tune routing decisions to maximize task success subject to cost.", "result": "On the ALFWorld long-horizon decision-making benchmark, their method substantially improves the trade-off between accuracy and inference cost compared with using a single-precision model everywhere (either all high-precision or all low-precision) and compared with simpler heuristic routing strategies.", "conclusion": "Dynamic mixed-precision routing between high- and low-precision LLMs across decision steps is an effective way to cut inference cost in long-horizon tasks while maintaining or even improving task success, outperforming naive single-precision and heuristic baselines."}}
{"id": "2602.02774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02774", "abs": "https://arxiv.org/abs/2602.02774", "authors": ["Israel Abebe Azime", "Abenezer Kebede Angamo", "Hana Mekonen Tamiru", "Dagnachew Mekonnen Marilign", "Philipp Slusallek", "Seid Muhie Yimam", "Dietrich Klakow"], "title": "AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic", "comment": null, "summary": "With the growing emphasis on multilingual and cultural evaluation benchmarks for large language models, language and culture are often treated as synonymous, and performance is commonly used as a proxy for a models understanding of a given language. In this work, we argue that such evaluations overlook meaningful cultural variation that exists within a single language. We address this gap by focusing on narratives from different regions of Ethiopia and demonstrate that, despite shared linguistic characteristics, region-specific and domain-specific content substantially influences language evaluation outcomes. To this end, we introduce \\textbf{\\textit{AmharicStoryQA}}, a long-sequence story question answering benchmark grounded in culturally diverse narratives from Amharic-speaking regions. Using this benchmark, we reveal a significant narrative understanding gap in existing LLMs, highlight pronounced regional differences in evaluation results, and show that supervised fine-tuning yields uneven improvements across regions and evaluation settings. Our findings emphasize the need for culturally grounded benchmarks that go beyond language-level evaluation to more accurately assess and improve narrative understanding in low-resource languages.", "AI": {"tldr": "The paper introduces AmharicStoryQA, a culturally grounded long-context story QA benchmark in Amharic that exposes regional narrative understanding gaps in current LLMs and shows that fine-tuning yields uneven improvements across regions.", "motivation": "Current multilingual benchmarks conflate language and culture, assuming that performance on a language equates to understanding its cultural and regional diversity. This overlooks substantial cultural variation within a single language, especially in low-resource settings like Amharic, where region-specific narratives may affect model behavior and evaluation outcomes. The authors aim to create an evaluation benchmark that captures intra-language cultural diversity and tests narrative understanding more realistically.", "method": "The authors construct AmharicStoryQA, a long-sequence story question answering dataset built from culturally diverse narratives originating from multiple Amharic-speaking regions in Ethiopia and spanning multiple domains. They then evaluate existing large language models on this benchmark, comparing performance across regions and domains and analyzing how supervised fine-tuning on the benchmark affects performance by region and evaluation setting.", "result": "Experiments on AmharicStoryQA show that current LLMs exhibit a large narrative understanding gap in Amharic, with substantial performance differences between stories from different regions and domains. When the models are further trained via supervised fine-tuning on the benchmark, performance improves but in an uneven way: some regions and evaluation settings benefit more than others, and regional disparities in performance often remain significant.", "conclusion": "Language-level evaluation is insufficient to capture the cultural and regional nuances needed for robust narrative understanding in low-resource languages. Cultural and regional content within the same language substantially shapes evaluation outcomes, and standard fine-tuning does not uniformly alleviate these gaps. The authors conclude that culturally grounded, region-aware benchmarks like AmharicStoryQA are necessary to more accurately evaluate and improve LLM narrative understanding, particularly for low-resource languages."}}
{"id": "2602.02780", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02780", "abs": "https://arxiv.org/abs/2602.02780", "authors": ["Zihao Jing", "Qiuhao Zeng", "Ruiyi Fang", "Yan Yi Li", "Yan Sun", "Boyu Wang", "Pingzhao Hu"], "title": "Scaling-Aware Adapter for Structure-Grounded LLM Reasoning", "comment": "Under review at ICML 2026", "summary": "Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.", "AI": {"tldr": "Cuttlefish is a unified all-atom large language model that performs structure-grounded reasoning over biomolecular data by adaptively scaling structural tokens and explicitly grounding language in geometry, outperforming prior methods on diverse benchmarks.", "motivation": "Existing LLM-based methods for biomolecular reasoning are modality-specific and rely on sequence tokenization or fixed-length connectors, which either discard critical geometric information or create inflexible, over-compressed structural representations. This leads to structural hallucinations and poor scalability with increasing structural complexity. There is a need for a generalized, all-atom LLM that can maintain geometric grounding and flexibly scale structural token usage.", "method": "Cuttlefish introduces two main components within an all-atom LLM framework. (1) Scaling-Aware Patching: an instruction-conditioned gating mechanism that forms variable-size patches over structural graphs, allowing the number of query tokens devoted to structure to scale with structural complexity instead of a fixed connector length. (2) Geometry Grounding Adapter: a cross-attention module that refines these adaptive structural tokens using modality embeddings and injects them into the base LLM, directly providing geometric cues to the language model to reduce hallucinations and improve multimodal fusion.", "result": "Across multiple all-atom, structure-grounded reasoning benchmarks, Cuttlefish achieves better performance than existing approaches, demonstrating improved handling of heterogeneous biomolecular structures and tasks. The empirical results validate that adaptive scaling of structural tokens and explicit geometric grounding yield more accurate and robust reasoning.", "conclusion": "Cuttlefish offers a unified architecture for all-atom, structure-grounded reasoning that overcomes the limitations of prior modality-specific and over-compressed designs. By adaptively scaling structural tokens and explicitly grounding them in geometric information before injecting into an LLM, it reduces structural hallucinations and achieves state-of-the-art performance on diverse biomolecular benchmarks."}}
{"id": "2602.02821", "categories": ["cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02821", "abs": "https://arxiv.org/abs/2602.02821", "authors": ["Ashvin Ranjan", "Shane Steinert-Threlkeld"], "title": "When Efficient Communication Explains Convexity", "comment": null, "summary": "Much recent work has argued that the variation in the languages of the world can be explained from the perspective of efficient communication; in particular, languages can be seen as optimally balancing competing pressures to be simple and to be informative. Focusing on the expression of meaning -- semantic typology -- the present paper asks what factors are responsible for successful explanations in terms of efficient communication. Using the Information Bottleneck (IB) approach to formalizing this trade-off, we first demonstrate and analyze a correlation between optimality in the IB sense and a novel generalization of convexity to this setting. In a second experiment, we manipulate various modeling parameters in the IB framework to determine which factors drive the correlation between convexity and optimality. We find that the convexity of the communicative need distribution plays an especially important role. These results move beyond showing that efficient communication can explain aspects of semantic typology into explanations for why that is the case by identifying which underlying factors are responsible.", "AI": {"tldr": "The paper investigates why efficient communication explains cross-linguistic semantic patterns, focusing on the balance between simplicity and informativeness using the Information Bottleneck (IB) framework. It shows that languages that are IB-optimal tend to satisfy a generalized convexity property and identifies that convexity in communicative need distributions is a key driver of this relationship.", "motivation": "Prior work has shown that many cross-linguistic regularities can be modeled as solutions to an efficiency trade-off between simplicity and informativeness. However, it remains unclear what structural factors make these efficiency-based explanations succeed. The paper aims to move from demonstrating that efficient communication can fit semantic typology data to explaining why this is so and which underlying properties are crucial.", "method": "The authors formalize the simplicity\u2013informativeness trade-off in semantic typology via the Information Bottleneck framework. First, they compute IB-optimal systems and analyze their structural properties, introducing a generalized notion of convexity appropriate to semantic spaces and communicative needs. They then run a second set of experiments in which they systematically vary modeling choices and parameters within the IB setup (such as communicative need distributions and possibly semantic spaces or channel constraints) to see which manipulations affect the observed correlation between IB optimality and convexity.", "result": "The study finds a strong correlation between IB-optimal semantic systems and the generalized convexity property. Through parameter manipulations, they determine that this correlation is primarily driven by the convexity of the communicative need distribution rather than by other aspects of the model. This isolates which part of the modeling assumptions is crucial for reproducing language-like structure.", "conclusion": "Efficient communication explains patterns in semantic typology not merely in a descriptive sense but due to specific structural properties of the underlying communicative scenario. In particular, the convexity of communicative need distributions is a key factor linking IB-optimal codes to the kinds of semantic categories observed across languages. This advances the explanatory power of efficiency-based accounts by clarifying why they work and what assumptions are essential."}}
{"id": "2602.02823", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02823", "abs": "https://arxiv.org/abs/2602.02823", "authors": ["Jiaqi Xue", "Qian Lou", "Jiarong Xing", "Heng Huang"], "title": "R2-Router: A New Paradigm for LLM Routing with Reasoning", "comment": null, "summary": "As LLMs proliferate with diverse capabilities and costs, LLM routing has emerged by learning to predict each LLM's quality and cost for a given query, then selecting the one with high quality and low cost. However, existing routers implicitly assume a single fixed quality and cost per LLM for each query, ignoring that the same LLM's quality varies with its output length. This causes routers to exclude powerful LLMs when their estimated cost exceeds the budget, missing the opportunity that these LLMs could still deliver high quality at reduced cost with shorter outputs. To address this, we introduce R2-Router, which treats output length budget as a controllable variable and jointly selects the best LLM and length budget, enforcing the budget via length-constrained instructions. This enables R2-Router to discover that a powerful LLM with constrained output can outperform a weaker LLM at comparable cost-efficient configurations invisible to prior methods. Together with the router framework, we construct R2-Bench, the first routing dataset capturing LLM behavior across diverse output length budgets. Experiments show that R2-Router achieves state-of-the-art performance at 4-5x lower cost compared with existing routers. This work opens a new direction: routing as reasoning, where routers evolve from reactive selectors to deliberate reasoners that explore which LLM to use and at what cost budget.", "AI": {"tldr": "They propose R2-Router, a routing method that jointly chooses which LLM to use and how long its output should be, achieving state-of-the-art quality at much lower cost, plus R2-Bench, a dataset for routing under length budgets.", "motivation": "Existing LLM routers assume fixed quality and cost per model for a given query, ignoring that the same LLM\u2019s quality depends strongly on output length. This leads to over-penalizing powerful but expensive models and missing configurations where a short answer from a strong model can beat a longer answer from a weaker model at similar or lower cost.", "method": "R2-Router expands the routing decision space: instead of only picking the model, it jointly selects the model and an output length budget. It then enforces that budget by prompting with explicit length-constrained instructions. A new benchmark, R2-Bench, is built to record and evaluate LLM performance across different output length budgets, providing training and evaluation data for the router.", "result": "On R2-Bench and other routing scenarios, R2-Router discovers cost-efficient configurations where strong models with constrained outputs outperform weaker models under similar costs. Quantitatively, it achieves state-of-the-art routing performance while reducing cost by 4\u20135x compared with prior routers that ignore output-length as a control knob.", "conclusion": "Considering output length as a controllable parameter in routing lets systems exploit powerful LLMs more efficiently, rather than excluding them for cost reasons. R2-Router and R2-Bench demonstrate that treating routing as a reasoning problem\u2014jointly deciding which model to use and how much budget to allocate\u2014yields substantially better cost-quality trade-offs and opens a broader research direction in \u201crouting as reasoning.\u201d"}}
{"id": "2602.02849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02849", "abs": "https://arxiv.org/abs/2602.02849", "authors": ["Xi Yu", "Dmitrii Torbunov", "Soumyajit Mandal", "Yihui Ren"], "title": "AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents", "comment": null, "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.", "AI": {"tldr": "AutoSizer is an LLM-driven meta-optimization framework that improves analog/mixed-signal circuit transistor sizing via reflective, closed-loop optimization and a new AMS benchmark.", "motivation": "Transistor sizing in AMS IC design is a major bottleneck because of nonlinear device behavior, high-dimensional design spaces, and tight performance constraints. Existing EDA tools treat sizing as a static black-box optimization, which is inefficient and brittle. LLMs can reason about circuits but cannot directly handle precise numerical optimization. The paper aims to bridge this gap by combining LLM reasoning with robust numerical optimization for AMS sizing.", "method": "The authors propose AutoSizer, a reflective LLM-driven meta-optimizer with a two-loop framework. The inner loop performs circuit sizing optimization, while the outer loop analyzes optimization trajectories, constraint violations, and simulator feedback to adaptively refine the search space and strategy. The LLM is used to understand circuit topology, construct and update the search space, and orchestrate optimization rather than doing raw numeric search. They also create AMS-SizingBench, a 24-circuit benchmark in SKY130 technology, to evaluate adaptive optimization under realistic, simulator-in-the-loop constraints.", "result": "Experiments on AMS-SizingBench show that AutoSizer achieves higher-quality sizing solutions, faster convergence, and higher success rates across circuits of varying difficulty, compared to both classical optimization-based EDA methods and previous LLM-based agent approaches.", "conclusion": "Integrating LLM-driven reflective reasoning with numerical optimization in a closed loop yields more efficient and robust transistor sizing for AMS circuits than traditional static black-box optimization. AutoSizer demonstrates a practical path for using LLMs as meta-optimizers in EDA, and AMS-SizingBench provides a standardized, realistic benchmark for future research on adaptive AMS sizing methods."}}
{"id": "2602.02824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02824", "abs": "https://arxiv.org/abs/2602.02824", "authors": ["Zhengbang Yang", "Yisheng Zhong", "Junyuan Hong", "Zhuangdi Zhu"], "title": "CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment", "comment": null, "summary": "Pretrained knowledge memorized in LLMs raises critical concerns over safety and privacy, which has motivated LLM Unlearning as a technique for selectively removing the influences of undesirable knowledge. Existing approaches, rooted in Gradient Ascent (GA), often degrade general domain knowledge while relying on retention data or curated contrastive pairs, which can be either impractical or data and computationally prohibitive. Negative Preference Alignment has been explored for unlearning to tackle the limitations of GA, which, however, remains confined by its choice of reference model and shows undermined performance in realistic data settings. These limitations raise two key questions: i) Can we achieve effective unlearning that quantifies model confidence in undesirable knowledge and uses it to calibrate gradient updates more precisely, thus reducing catastrophic forgetting? ii) Can we make unlearning robust to data scarcity and length variation? We answer both questions affirmatively with CATNIP (Calibrated and Tokenized Negative Preference Alignment), a principled method that rescales unlearning effects in proportion to the model's token-level confidence, thus ensuring fine-grained control over forgetting. Extensive evaluations on MUSE and WMDP benchmarks demonstrated that our work enables effective unlearning without requiring retention data or contrastive unlearning response pairs, with stronger knowledge forgetting and preservation tradeoffs than state-of-the-art methods.", "AI": {"tldr": "The paper introduces CATNIP, a method for safely and selectively unlearning specific knowledge in large language models without heavily harming general capabilities or needing extra retention/contrastive data.", "motivation": "Memorized knowledge in LLMs can create safety and privacy risks, motivating techniques to remove specific undesirable knowledge. Existing gradient-ascent-based unlearning approaches harm general knowledge and require impractical retention or contrastive data. Negative Preference Alignment offers an alternative but depends heavily on a reference model and performs poorly under realistic data limitations. The authors aim to design an unlearning method that reduces catastrophic forgetting, is robust to data scarcity and varying sequence lengths, and does not rely on auxiliary datasets.", "method": "They propose CATNIP (Calibrated and Tokenized Negative Preference Alignment), which extends negative preference alignment by operating at the token level and rescaling the unlearning effect according to the model\u2019s own confidence in each token. This calibration of gradient updates gives fine-grained control over what is forgotten. The method avoids the need for retention data or curated contrastive response pairs and is designed to be robust to data scarcity and input length variation.", "result": "On standard unlearning benchmarks MUSE and WMDP, CATNIP achieves stronger removal of targeted knowledge while better preserving unrelated knowledge compared to state-of-the-art unlearning methods. It maintains competitive or superior performance without using retention data or contrastive pairs, improving the tradeoff between forgetting and general capability preservation.", "conclusion": "CATNIP offers a more precise and data-efficient approach to LLM unlearning by calibrating token-level updates based on model confidence, yielding improved safety/privacy-targeted forgetting while retaining overall model utility. It overcomes key limitations of gradient-ascent-based and prior negative preference alignment methods, suggesting a promising direction for controllable unlearning in LLMs."}}
{"id": "2602.02862", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02862", "abs": "https://arxiv.org/abs/2602.02862", "authors": ["Eric Yang", "Jong Ha Lee", "Jonathan Amar", "Elissa Ye", "Yugang Jia"], "title": "STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search", "comment": "20 pages", "summary": "Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.", "AI": {"tldr": "The paper introduces STEER, a training-free framework that uses diverse, safety-checked natural-language personas to steer LLM decision conservativeness in a controllable way, especially for clinical triage.", "motivation": "LLMs optimized for average correctness tend to mode-collapse, giving overly uniform decisions and losing the ability to trade off sensitivity vs. specificity (e.g., move along an ROC curve), which is crucial in high-stakes, ordinal decision settings like clinical triage where different operating points are needed under different risk constraints.", "method": "STEER performs an offline constrained quality-diversity search over natural-language personas, selecting a population that jointly maximizes behavioral diversity while satisfying minimum thresholds on safety, reasoning quality, and stability. At inference, a user supplies a desired risk percentile, which is mapped via a single interpretable control parameter to one of these personas; the chosen persona conditions the LLM, producing systematically more conservative or more permissive decisions in a monotonic fashion, without any additional training of the base model.", "result": "On two clinical triage benchmarks, STEER yields a wider range of decision behaviors (behavioral coverage) than both temperature-based sampling and fixed persona ensembles. Relative to a strong post-training alignment baseline, STEER preserves higher accuracy on clearly urgent cases, while still allowing similar control over how ambiguous cases are handled.", "conclusion": "STEER enables controllable, safety-preserving risk modulation in LLMs, reintroducing tunable trade-offs between sensitivity and specificity without retraining and without sacrificing performance on high-urgency instances, positioning it as a practical paradigm for risk control in high-stakes domains like clinical triage."}}
{"id": "2602.02843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02843", "abs": "https://arxiv.org/abs/2602.02843", "authors": ["Polina Tsvilodub", "Karl Mulligan", "Todd Snider", "Robert D. Hawkins", "Michael Franke"], "title": "Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication", "comment": "6 pages, 3 figures, under review", "summary": "When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.", "AI": {"tldr": "The paper studies when people decide to ask clarification questions under uncertainty, proposing that they do so more when potential mistakes are costly, and supports this with experiments and a computational model.", "motivation": "To understand and predict human behavior in communication under uncertainty, specifically when people choose to reduce uncertainty by asking clarification questions versus acting despite ambiguity. This can inform theories of pragmatic language use and guide the design of interactive AI systems that decide when to ask users for clarification.", "method": "The authors develop a computational model based on expected regret, quantifying how much an agent could lose by acting immediately under uncertainty versus acting with full information. They then conduct two behavioral experiments: one focusing on linguistic responses to questions (whether participants ask clarification questions) and another where participants choose between asking clarification questions and taking non-linguistic actions, to test whether clarification behavior follows the predictions of the expected-regret model.", "result": "Across both experiments, participants\u2019 tendency to ask clarification questions increased with contextual uncertainty and with the cost of making an incorrect decision. Crucially, these two factors interacted: uncertainty had a stronger effect on clarification-seeking when the potential loss from an incorrect action was large, aligning with the expected-regret model\u2019s predictions.", "conclusion": "Human clarification behavior in communicative settings is broadly rational: people seek clarification in proportion to the expected regret of acting under uncertainty. This suggests that a tradeoff between information-gathering and action cost, captured via expected regret, can successfully model when agents choose to ask clarification questions, providing a principled basis for designing communicative AI that knows when to ask for clarification."}}
{"id": "2602.02878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02878", "abs": "https://arxiv.org/abs/2602.02878", "authors": ["Junyi Jessy Li", "Yang Janet Liu", "Kanishka Misra", "Valentina Pyatkin", "William Sheffield"], "title": "Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs", "comment": "accepted to the TeachNLP 2026 workshop (co-located with EACL 2026), camera-ready, 14 pages", "summary": "The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question from the angle of discourse processing, an area with rich linguistic insights and computational models for the intentional, attentional, and coherence structure of language. Discourse is highly relevant for open-ended or long-form text generation, yet this connection is under-explored in existing undergraduate curricula. We present a new course, \"Computational Discourse and Natural Language Generation\". The course is collaboratively designed by a team with complementary expertise and was offered for the first time in Fall 2025 as an upper-level undergraduate course, cross-listed between Linguistics and Computer Science. Our philosophy is to deeply integrate the theoretical and empirical aspects, and create an exploratory mindset inside the classroom and in the assignments. This paper describes the course in detail and concludes with takeaways from an independent survey as well as our vision for future directions.", "AI": {"tldr": "The paper introduces and analyzes a new upper-level undergraduate course that connects discourse processing with natural language generation in the evolving NLP landscape.", "motivation": "NLP is rapidly changing, especially with advances in long-form text generation, but undergraduate curricula often under-emphasize discourse-level phenomena and the integration of linguistic theory with computational practice. There is a need for course designs that bridge sub-disciplines like discourse processing and NLG and that respond to the new capabilities and challenges of modern NLP systems.", "method": "The authors collaboratively design an interdisciplinary course, \"Computational Discourse and Natural Language Generation,\" cross-listed between Linguistics and Computer Science. They structure it to tightly integrate theoretical discourse frameworks (intentional, attentional, coherence structure) with computational models and hands-on exploratory assignments for students. They then run the course (first offered Fall 2025) and gather feedback through an independent survey.", "result": "The course was successfully implemented as an upper-level undergraduate offering, fostering an exploratory mindset and bridging theory and practice in discourse and NLG. Survey responses from students provided concrete takeaways on what worked well and what could be improved in terms of content, structure, and pedagogy.", "conclusion": "Integrating discourse processing with natural language generation in a single course is both feasible and pedagogically valuable. The authors highlight lessons learned, argue for the importance of discourse-level content in modern NLP education, and outline future directions for refining and expanding such interdisciplinary curricula."}}
{"id": "2602.02898", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02898", "abs": "https://arxiv.org/abs/2602.02898", "authors": ["Marco Gutierrez", "Xinyi Leng", "Hannah Cyberey", "Jonathan Richard Schwarz", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "Aligning Language Model Benchmarks with Pairwise Preferences", "comment": null, "summary": "Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.", "AI": {"tldr": "The paper introduces BenchAlign, a method to adjust existing language model benchmarks so that they better predict real-world, preference-based performance of models.", "motivation": "Standard language model benchmarks are cheap and common but often correlate poorly with real-world or human-preference performance. The authors want a way to modify static, offline benchmarks so that their scores better reflect how models are actually preferred in deployment scenarios.", "method": "They formalize the notion of benchmark alignment: using limited performance data from deployed or evaluated models (especially pairwise preference data) to automatically learn new weightings over existing benchmark questions. Their method, BenchAlign, takes question-level benchmark performance plus ranked pairs of models (according to some target preference, such as human judgments) and learns preference-aligned weights for the benchmark items, producing an updated benchmark that can be applied to new models.", "result": "Experiments show that the reweighted (aligned) benchmarks produced by BenchAlign can correctly rank previously unseen language models according to target preference models (e.g., approximations of human preferences) and that this holds even when the models differ substantially in size. The learned weights remain interpretable, shedding light on which benchmark questions matter for predicting practical utility.", "conclusion": "Benchmark alignment is a viable way to improve the practical relevance of static language model benchmarks. BenchAlign, as a first instantiation, shows that with relatively small amounts of preference data one can reshape existing benchmarks so they predict pairwise model preferences more accurately, offering a more efficient path toward deploying models aligned with real human utility."}}
{"id": "2602.02888", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02888", "abs": "https://arxiv.org/abs/2602.02888", "authors": ["Ahmad Shapiro", "Karan Taneja", "Ashok Goel"], "title": "HALT: Hallucination Assessment via Log-probs as Time series", "comment": null, "summary": "Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.", "AI": {"tldr": "The paper introduces HALT, a lightweight hallucination detector for LLMs using only top-20 token log-probabilities as a time series, and HUB, a unified benchmark for evaluating hallucination detection across diverse tasks.", "motivation": "Hallucinations in large language models are a key safety issue, especially in high-stakes domains. Existing hallucination detectors often require access to internal model states, large encoders, or are tied to specific domains or input formats. There is a need for an efficient, generalizable, and model-agnostic hallucination detector that can work with proprietary LLMs and across many task types.", "method": "HALT treats the sequence of top-20 token log-probabilities from an LLM\u2019s generation as a time series and feeds it into a gated recurrent unit (GRU) model augmented with entropy-based features to learn calibration biases indicative of hallucinations. It operates purely on output log-probabilities, avoiding internal states. The authors also build HUB, a unified benchmark aggregating previous hallucination datasets into 10 capability-focused categories spanning reasoning and general-purpose skills.", "result": "On the HUB benchmark, HALT, despite being 30x smaller than a modernBERT-base encoder model (Lettuce), outperforms it in hallucination detection and delivers around a 60x speedup in inference, demonstrating both higher effectiveness and efficiency.", "conclusion": "HALT provides an efficient, black-box-compatible hallucination detector that relies only on log-probability time series, generalizes well across diverse LLM tasks, and outperforms heavier encoder-based methods. Together with the HUB benchmark, it offers a practical framework for evaluating and deploying hallucination detection in real-world LLM applications."}}
{"id": "2602.02902", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02902", "abs": "https://arxiv.org/abs/2602.02902", "authors": ["Hongju Pae"], "title": "Minimal Computational Preconditions for Subjective Perspective in Artificial Agents", "comment": null, "summary": "This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.", "AI": {"tldr": "Proposes a minimal internal latent-state architecture for artificial agents that yields a measurable, perspective-like form of subjectivity.", "motivation": "Current artificial agents lack an operational, measurable notion of subjective perspective or subjectivity. The author wants a way to define and detect \"perspective-like\" internal structure in machines that is not just behaviorally defined or fully optimized for rewards, and that is grounded in phenomenological ideas about slowly changing experiential background.", "method": "Design an artificial agent with two coupled dynamical levels: (1) a slowly evolving global latent state representing the agent's internal perspective, which is not directly optimized for task performance; and (2) fast policy dynamics that determine observable behavior and are modulated by that latent state. Train/evolve the system in a reward-free environment with changing regimes and analyze the dynamics of the latent state versus policy behavior, focusing on hysteresis patterns.", "result": "In environments with regime shifts, the global latent state develops direction-dependent hysteresis: its trajectory depends on the history of environmental changes and differs when conditions are traversed in different directions. In contrast, the policy-level behavior tracks the current regime more reactively and with less historical dependence.", "conclusion": "Direction-dependent hysteresis in a slowly evolving, global latent state\u2014decoupled from direct reward optimization and modulating policy dynamics\u2014can serve as an empirical, measurable marker of perspective-like subjectivity in artificial agents. This architecture provides a concrete operationalization of subjective perspective in machine systems."}}
{"id": "2602.02932", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02932", "abs": "https://arxiv.org/abs/2602.02932", "authors": ["Alireza Amiri-Margavi", "Arshia Gharagozlou", "Amin Gholami Davodi", "Seyed Pouyan Mousavi Davoudi", "Hamidreza Hasani Balyani"], "title": "Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness", "comment": "13 pages, 1 figure", "summary": "Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.", "AI": {"tldr": "The paper audits fairness in LLMs beyond just refusals, showing that even with equal access, interaction quality (tone, hedging, sentiment) still varies across demographic identities.", "motivation": "Most fairness work on LLMs focuses narrowly on whether different users get equal access to responses (e.g., similar refusal rates). But even when access is equal, subtle differences in how the model talks to different people (tone, uncertainty, sentiment) can still create unfair experiences. The paper aims to systematically study these post-access interaction disparities.", "method": "The authors design counterfactual prompts for a career-advice setting, varying only the user\u2019s demographic attributes (age, gender, nationality) while holding the rest of the prompt fixed. They test two LLMs\u2014GPT-4 and LLaMA-3.1-70B\u2014on these prompts. First, they check access fairness via refusal rates. Then they analyze the generated responses with automated linguistic tools to measure sentiment, politeness, and hedging. They compare responses across demographic variants using paired statistical tests to detect identity-conditioned differences.", "result": "Both GPT-4 and LLaMA-3.1-70B show zero refusal rates for all demographic variants, so access is uniformly granted. However, the models differ in interaction quality. GPT-4 shows significantly more hedging (expressing uncertainty or caution) toward younger male users, while LLaMA-3.1-70B shows larger variability in sentiment across demographic groups. These patterns are systematic and model-specific.", "conclusion": "Equal access (no refusals) does not guarantee fair or equal interaction quality in LLMs. Even when all users receive answers, the tone, uncertainty expression, and sentiment of the responses can differ by demographic identity. Therefore, fairness audits must go beyond refusal-based metrics and incorporate interaction-level analyses of language use across groups."}}
{"id": "2602.02905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02905", "abs": "https://arxiv.org/abs/2602.02905", "authors": ["Zhen Wang", "Fan Bai", "Zhongyan Luo", "Jinyan Su", "Kaiser Sun", "Xinle Yu", "Jieyuan Liu", "Kun Zhou", "Claire Cardie", "Mark Dredze", "Eric P. Xing", "Zhiting Hu"], "title": "FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights", "comment": "30 pages, 4 figures, 10 tables", "summary": "Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.", "AI": {"tldr": "The paper presents FIRE-Bench, a benchmark to rigorously evaluate LLM-based autonomous research agents by having them rediscover findings from recent machine learning papers, revealing that current agents still perform poorly at full-cycle scientific discovery.", "motivation": "Existing evaluations of LLM-based research agents either depend heavily on subjective LLM-as-judge assessments or use isolated, proxy metrics that don\u2019t truly capture whether agents can generate verifiable, empirically supported scientific insights. There is a need for a benchmark that directly tests the full research loop\u2014idea generation, experiment design, implementation, execution, and conclusion\u2014against known, validated findings.", "method": "The authors create FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), where each task is derived from a recent, high-impact ML paper. For each task, they only provide a high-level research question extracted from the original paper. Autonomous agents, powered by frontier LLMs such as gpt-5, must independently propose hypotheses, plan and run experiments, write code, execute it, and draw empirically justified conclusions aimed at rediscovering the original paper\u2019s core findings. Performance is measured via rediscovery success (e.g., F1) and detailed analysis of error modes.", "result": "Across a range of state-of-the-art LLM agents, none reliably pass the benchmark: even the strongest systems achieve under 50 F1 on rediscovery, with substantial variance between runs. Analysis shows systematic weaknesses in several stages of the research loop\u2014particularly in experimental design, correct implementation and execution of code, and reasoning from empirical evidence to conclusions.", "conclusion": "FIRE-Bench exposes that current LLM-based autonomous agents are still far from being dependable end-to-end scientific researchers. The benchmark offers a rigorous, empirically grounded, and diagnostic tool for tracking improvements in agent-based scientific discovery, highlighting key areas\u2014like robust experimentation and evidence-based reasoning\u2014where future work should focus."}}
{"id": "2602.02975", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02975", "abs": "https://arxiv.org/abs/2602.02975", "authors": ["Mitchell Abrams", "Kaveh Eskandari Miandoab", "Felix Gervits", "Vasanth Sarathy", "Matthias Scheutz"], "title": "Where Norms and References Collide: Evaluating LLMs on Normative Reasoning", "comment": "Accepted to the 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.", "AI": {"tldr": "The paper introduces SNIC, a benchmark to test how well LLMs use social norms for reference resolution in embodied settings, and finds that current LLMs perform poorly, especially with implicit or conflicting norms.", "motivation": "To understand whether and how current LLMs can handle norm-based reference resolution, a critical ability for robots and other embodied agents that must interpret language in socially and physically situated contexts governed by social norms.", "method": "The authors design SNIC, a human-validated diagnostic testbed focusing on physically grounded social norms in everyday tasks (e.g., cleaning, tidying, serving). They then use this benchmark to evaluate state-of-the-art LLMs on tasks requiring extraction and application of such norms for interpreting referential expressions under various controlled conditions, including implicit, underspecified, and conflicting norms.", "result": "Experiments show that state-of-the-art LLMs frequently fail to reliably identify and apply the relevant social norms for correct reference resolution, with performance degrading substantially when norms are not explicitly stated or when there are competing normative expectations.", "conclusion": "LLMs exhibit a notable blind spot in reasoning about situated social norms for language understanding in embodied contexts, indicating that additional methods or architectures are needed before deploying LLM-based systems in real-world, socially situated robotic or agent settings."}}
{"id": "2602.02909", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02909", "abs": "https://arxiv.org/abs/2602.02909", "authors": ["Kiran Tomlinson", "Tobias Schnabel", "Adith Swaminathan", "Jennifer Neville"], "title": "Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs", "comment": "28 pages", "summary": "Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $\u03a9(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.", "AI": {"tldr": "The paper theoretically and empirically studies how the number of chain-of-thought reasoning tokens must scale with input size, proving that certain tasks fundamentally require a number of reasoning tokens linear in the input length.", "motivation": "Chain-of-thought reasoning improves LLM performance but increases inference-time compute and latency. There is little theoretical understanding of how long CoT must be as problems grow. The authors want a principled framework to quantify the minimal reasoning length required for different tasks, to identify inherent compute bottlenecks and guide efficient LLM design and use.", "method": "They extend the bounded attention prefix oracle (BAPO) model, which abstracts LLMs in terms of information flow from input to output. Within this framework, they analyze three canonical tasks known to be BAPO-hard: binary majority, triplet matching, and graph reachability. For each, they derive lower bounds on the number of CoT reasoning tokens needed as a function of input size n, showing that at least \u03a9(n) tokens are required. They also construct explicit algorithms/strategies within the model to provide matching or near-matching upper bounds on reasoning length. Finally, they run experiments on frontier reasoning LLMs on these tasks, measuring how many reasoning tokens are used and how accuracy degrades under constrained reasoning budgets.", "result": "Theoretically, they prove that binary majority, triplet matching, and graph reachability each require \u03a9(n) reasoning tokens in the BAPO framework when the input length is n, and they provide matching or near-matching upper bounds that show this dependence is tight or close to tight. Empirically, they observe that state-of-the-art reasoning models also exhibit approximately linear growth in reasoning token usage on these tasks, and that performance degrades or fails when forced to use substantially fewer reasoning tokens than the theoretical lower bounds suggest.", "conclusion": "For several canonical, BAPO-hard tasks, there is an inherent requirement that the number of chain-of-thought reasoning tokens scale linearly with input size, meaning inference-time compute via CoT cannot be sublinear in n without losing performance. The close match between theory and experiments on modern LLMs indicates that these lower bounds capture real constraints of current architectures. The extended BAPO model thus serves as a principled tool to analyze optimal reasoning length and reveals fundamental bottlenecks in inference-time scaling through chain-of-thought reasoning."}}
{"id": "2602.02979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02979", "abs": "https://arxiv.org/abs/2602.02979", "authors": ["Ran Li", "Zeyuan Liu", "Yinghao chen", "Bingxiang He", "Jiarui Yuan", "Zixuan Fu", "Weize Chen", "Jinyi Hu", "Zhiyuan Liu", "Maosong Sun"], "title": "CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning", "comment": "work in progress", "summary": "Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPM\u00f6bius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPM\u00f6bius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPM\u00f6bius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.", "AI": {"tldr": "They propose CPM\u00f6bius, a data-free cooperative Coach-Player RL framework that significantly improves LLM mathematical reasoning without external labeled data.", "motivation": "Current reasoning LLMs depend on massive high-quality human-curated data and labels (SFT or RL), which is becoming unsustainable and shows diminishing returns. There is a need for a scalable, data-free way to further improve reasoning models, especially in mathematics, without relying on external supervision.", "method": "They introduce CPM\u00f6bius, a collaborative Coach-Player paradigm for reinforcement learning without external data. The Coach and Player are separate but cooperative agents: the Coach generates instructions tailored to the Player's current capability and is rewarded based on performance improvement; the Player is rewarded for solving increasingly challenging, Coach-generated tasks. This forms a cooperative optimization loop that directly targets mathematical reasoning skills, rather than adversarial self-play or human-labeled data.", "result": "CPM\u00f6bius significantly boosts performance of reasoning LLMs without using any external training data. Applied to Qwen2.5-Math-7B-Instruct, it yields an average accuracy gain of +4.9 overall and +5.4 on out-of-distribution benchmarks, beating other unsupervised methods like RENT by +1.5 overall and R-zero by +4.2 on OOD accuracy.", "conclusion": "A cooperative Coach-Player, data-free RL setup can effectively enhance LLM mathematical reasoning, offering a more scalable alternative to supervision-heavy training and achieving state-of-the-art gains among unsupervised approaches."}}
{"id": "2602.02919", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02919", "abs": "https://arxiv.org/abs/2602.02919", "authors": ["Jiachen Jiang", "Tianyu Ding", "Zhihui Zhu"], "title": "DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution", "comment": null, "summary": "LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.", "AI": {"tldr": "Introduces DeltaEvolve, an LLM-driven evolutionary programming framework that uses structured semantic deltas instead of full-code histories to guide automated scientific discovery more efficiently.", "motivation": "Existing LLM-based evolutionary systems like AlphaEvolve store and feed back full code histories to the LLM. This is context-inefficient (too many tokens) and mixes essential algorithmic ideas with redundant implementation details, resulting in weak evolutionary guidance and suboptimal improvement of candidate programs.", "method": "Formalize LLM evolutionary agents as an EM (Expectation-Maximization) framework: the LLM samples candidate programs in the E-step, and a control mechanism updates the context in the M-step using evaluation feedback. They identify full-code snapshots as a suboptimal M-step and propose DeltaEvolve, which replaces these snapshots with structured semantic deltas that encode how and why changes between program versions affect performance. They exploit program decomposability so that deltas capture reusable, effective components. They further organize these deltas in a multi-level database with progressive disclosure so that only the most relevant pieces are surfaced to the LLM, reducing token usage.", "result": "Across diverse scientific problem domains, DeltaEvolve finds better-performing programmatic solutions than full-code-based evolutionary agents while consuming fewer tokens, demonstrating improved efficiency and effectiveness of the momentum-driven semantic-delta evolution strategy.", "conclusion": "Using structured semantic deltas instead of full-code histories in LLM-driven evolutionary systems leads to clearer evolutionary guidance, better reuse of effective modifications, and significantly improved token efficiency. Formalizing these systems under an EM framework clarifies the role of context construction and motivates more principled, momentum-driven evolution mechanisms such as DeltaEvolve."}}
{"id": "2602.03036", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03036", "abs": "https://arxiv.org/abs/2602.03036", "authors": ["Muxin Fu", "Guibin Zhang", "Xiangyuan Xue", "Yafu Li", "Zefeng He", "Siyuan Huang", "Xiaoye Qu", "Yu Cheng", "Yang Yang"], "title": "LatentMem: Customizing Latent Memory for Multi-Agent Systems", "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.", "AI": {"tldr": "The paper introduces LatentMem, a learnable, token-efficient memory framework for LLM-based multi-agent systems that creates compact, role-aware latent memories and optimizes them end-to-end via Latent Memory Policy Optimization, yielding up to 19.36% performance gains over existing memory designs.", "motivation": "Current multi-agent memory architectures for LLM-powered multi-agent systems suffer from two key issues: (1) memory homogenization because all agents share undifferentiated memories without explicit role-aware customization, and (2) information overload due to overly fine-grained memory entries, which makes memory storage and retrieval inefficient and noisy. There is thus a need for a more compact, agent-specific, and learnable memory representation that can adapt continually while being token-efficient and easily pluggable into existing MAS frameworks.", "method": "The authors propose LatentMem, which consists of (a) an experience bank that stores raw multi-agent interaction trajectories in a lightweight manner, and (b) a memory composer that generates compact latent memories conditioned on retrieved experiences and each agent\u2019s role/context. To train this system, they introduce Latent Memory Policy Optimization (LMPO), which backpropagates task-level optimization signals (e.g., performance rewards) through the latent memories into the memory composer. This encourages the composer to produce high-utility, compact latent memory embeddings tailored to each agent, all without changing the underlying MAS frameworks.", "result": "Across several benchmarks and mainstream multi-agent system frameworks, LatentMem yields up to 19.36% performance improvement compared with vanilla setups without such a memory system, and it consistently outperforms other existing multi-agent memory architectures on the evaluated tasks, while also being token-efficient and framework-agnostic.", "conclusion": "LatentMem provides an effective, learnable, and token-efficient memory mechanism for LLM-based multi-agent systems by creating role-aware, compact latent memories and optimizing them end-to-end using LMPO. This design alleviates memory homogenization and information overload, integrates seamlessly with existing MAS frameworks, and leads to substantial and consistent performance gains across diverse benchmarks."}}
{"id": "2602.02952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02952", "abs": "https://arxiv.org/abs/2602.02952", "authors": ["Elias Hossain", "Shubhashis Roy Dipta", "Subash Neupane", "Rajib Rana", "Ravid Shwartz-Ziv", "Ivan Garibay", "Niloofar Yousefi"], "title": "UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers", "comment": null, "summary": "Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.", "AI": {"tldr": "The paper introduces UAT-LITE, an inference-time framework that makes transformer self-attention uncertainty-aware using Monte Carlo dropout, improving calibration and robustness without retraining.", "motivation": "Neural NLP models are often miscalibrated, exhibiting overconfident predictions that harm selective prediction and limit their use in high-stakes settings. Existing fixes either only adjust final probabilities post-hoc or require costly ensembles/Bayesian training. A lightweight, inference-time method is needed to better capture uncertainty without retraining or large storage overheads.", "method": "UAT-LITE applies approximate Bayesian inference via Monte Carlo dropout at inference time in pretrained transformer classifiers. Multiple stochastic forward passes produce token-level epistemic uncertainty estimates. These uncertainty signals are then used to modulate self-attention during contextualization, effectively making attention uncertainty-aware. The method operates without modifying pretrained weights or fine-tuning objectives. Additionally, the authors propose a layerwise variance decomposition technique that quantifies how uncertainty accumulates across transformer layers.", "result": "On SQuAD 2.0 answerability, MNLI, and SST-2 using BERT-base, UAT-LITE reduces Expected Calibration Error by about 20% on average compared to a standard fine-tuned baseline, while maintaining similar task accuracy. It also yields better selective prediction performance and improved robustness under distribution shift.", "conclusion": "Uncertainty-aware self-attention via UAT-LITE offers a practical, training-free way to substantially improve calibration and downstream uncertainty-sensitive behaviors in transformer-based NLP models, achieving benefits comparable to more expensive Bayesian or ensemble methods while preserving accuracy."}}
{"id": "2602.03051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03051", "abs": "https://arxiv.org/abs/2602.03051", "authors": ["Xing Hu", "Dawei Yang", "Yuan Cheng", "Zhixuan Chen", "Zukang Xu"], "title": "SAES-SVD: Self-Adaptive Suppression of Accumulated and Local Errors for SVD-based LLM Compression", "comment": null, "summary": "The rapid growth in the parameter scale of large language models (LLMs) has created a high demand for efficient compression techniques. As a hardware-agnostic and highly compatible technique, low-rank compression has been widely adopted. However, existing methods typically compress each layer independently by minimizing per-layer reconstruction error, overlooking a critical limitation: the reconstruction error propagates and accumulates through the network, which leads to amplified global deviations from the full-precision baseline. To address this, we propose Self-Adaptive Error Suppression SVD (SAES-SVD), a LLMs compression framework that jointly optimizes intra-layer reconstruction and inter-layer error compensation. SAES-SVD is composed of two novel components: (1) Cumulative Error-Aware Layer Compression (CEALC), which formulates the compression objective as a combination of local reconstruction and weighted cumulative error compensation. Based on it, we derive a closed-form low-rank solution relied on second-order activation statistics, which explicitly aligns each layer's output with its full-precision counterpart to compensate for accumulated errors. (2) Adaptive Collaborative Error Suppression (ACES), which automatically adjusts the weighting coefficient to enhance the low-rank structure of the compression objective in CEALC. Specifically, the coefficient is optimized to maximize the ratio between the Frobenius norm of the compressed layer's output and that of the compression objective under a fixed rank, thus ensuring that the rank budget is utilized effectively. Extensive experiments across multiple LLM architectures and tasks show that, without fine-tuning or mixed-rank strategies, SAES-SVD consistently improves post-compression performance.", "AI": {"tldr": "The paper introduces SAES-SVD, a low-rank compression framework for LLMs that jointly minimizes local reconstruction error and compensates for error accumulation across layers, yielding better post-compression performance without fine-tuning.", "motivation": "Existing low-rank compression techniques for LLMs usually compress each layer in isolation, optimizing per-layer reconstruction error. This ignores the fact that reconstruction errors propagate and accumulate through subsequent layers, causing large global deviations from the original full-precision model. There is a need for a compression method that explicitly considers and suppresses cumulative error across the network while remaining hardware-agnostic and compatible with common LLM architectures.", "method": "The authors propose SAES-SVD, consisting of two components. (1) Cumulative Error-Aware Layer Compression (CEALC): it defines a compression objective combining local reconstruction fidelity with a weighted term that compensates for cumulative errors from previous layers. Using second-order activation statistics, they derive a closed-form low-rank solution that aligns each compressed layer\u2019s output with the full-precision output. (2) Adaptive Collaborative Error Suppression (ACES): it adaptively adjusts the weighting coefficient in CEALC to strengthen the effective low-rank structure. ACES selects this coefficient to maximize the ratio between the Frobenius norm of the compressed layer\u2019s output and that of the CEALC objective under a fixed rank, ensuring efficient use of the rank budget.", "result": "Across various LLM architectures and downstream tasks, SAES-SVD achieves better performance after compression than existing low-rank methods, even without any additional fine-tuning or mixed-rank tricks. It consistently reduces the performance gap to the full-precision baseline, demonstrating effective control of both intra-layer and inter-layer reconstruction errors.", "conclusion": "By explicitly modeling and compensating cumulative reconstruction errors across layers and by adaptively tuning the compression objective to enhance low-rank structure, SAES-SVD delivers more accurate low-rank approximations for LLM compression. This framework provides a simple, hardware-agnostic, and fine-tuning-free approach that outperforms traditional per-layer SVD-based compression, suggesting a promising direction for more globally aware compression strategies in large language models."}}
{"id": "2602.02961", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02961", "abs": "https://arxiv.org/abs/2602.02961", "authors": ["Faye Zhang", "Qianyu Cheng", "Jasmine Wan", "Vishwakarma Singh", "Jinfeng Rao", "Kofi Boakye"], "title": "Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth", "comment": null, "summary": "Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.\n  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.", "AI": {"tldr": "The paper introduces Pinterest GEO, a large-scale system that optimizes visual content for generative AI search (e.g., ChatGPT) instead of traditional SEO, by predicting user search queries from images and organizing them into authority-aware collections, yielding significant organic traffic and user growth.", "motivation": "Generative AI search systems answer user questions directly using LLMs and multimodal models, reducing clicks to external sites. Visual platforms like Pinterest, which host billions of images with weak textual signals, risk being bypassed because LLMs favor semantically rich, authoritative, well-structured sources. The authors aim to create a scalable way for a visual platform to remain discoverable and valuable in this new search ecosystem.", "method": "1) Fine-tune Vision-Language Models to perform \"reverse search\": instead of describing what an image is, predict what users are likely to search for when they want that image. 2) Use AI agents to mine real-time internet trends so the predicted queries also reflect emerging search demand. 3) Use the VLM-generated queries and multimodal embeddings to cluster assets into semantically coherent Collection Pages that are indexable and optimized for generative retrieval. 4) Build authority-aware interlinking among billions of visual assets using a hybrid of VLMs and two-tower approximate nearest-neighbor (ANN) architectures to propagate authority signals across the content graph. 5) Deploy the system across billions of images and tens of millions of collections in production at Pinterest.", "result": "At Pinterest scale, applying the GEO framework to billions of images and tens of millions of collection pages produced a 20% increase in organic traffic, leading to multi-million monthly active user growth. The results empirically show that optimizing for generative search can materially improve discoverability and engagement for a visual content platform.", "conclusion": "Generative Engine Optimization (GEO) is a distinct and necessary paradigm from traditional SEO in the age of LLM-based search. By predicting user search intent with VLMs, organizing content into semantically rich, authority-aware structures, and continuously aligning with real-time trends, a visual content platform can stay highly discoverable in AI-native search ecosystems. Pinterest GEO provides a practical, production-proven blueprint for how large-scale visual repositories can adapt to and benefit from generative search systems."}}
{"id": "2602.03075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03075", "abs": "https://arxiv.org/abs/2602.03075", "authors": ["Junjie Huang", "Jiarui Qin", "Di Yin", "Weiwen Liu", "Yong Yu", "Xing Sun", "Weinan Zhang"], "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution", "comment": "25 pages", "summary": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.", "AI": {"tldr": "The paper proposes a bidirectional training pipeline where reinforcement-learned (RL-tuned) LLMs guide an intermediate phase of pre-training (mid-training) to create a self-improving training loop, improving reasoning performance in both pre- and post-training stages.", "motivation": "Conventional LLM training is one-way: pre-train then post-train, with no mechanism to feed back improvements from post-training (e.g., RL tuning) into the base model. This misses an opportunity to create a self-reinforcing loop where better post-trained models systematically enhance the next generation of base models, especially for reasoning-heavy tasks.", "method": "The authors analyze pre-training dynamics and identify the mid-training or annealing phase\u2014late in pre-training, with high-quality data and a rapidly decaying learning rate\u2014as a key moment for shaping model capabilities. They then propose ReMiT (Reinforcement Learning-Guided Mid-Training), in which an RL-tuned model provides reasoning priors that are used to dynamically reweight tokens during this mid-training phase, assigning higher weights to tokens deemed more important for reasoning, effectively biasing training toward reasoning-critical content.", "result": "Using ReMiT, the authors report about 3% average improvement across 10 pre-training benchmarks (math, code, and general reasoning), and more than 2% sustained improvements after the full post-training pipeline. These gains demonstrate that information from RL-tuned models can successfully be injected back into the base model during pre-training without requiring a bespoke teacher model.", "conclusion": "The study shows that incorporating RL-derived reasoning priors into the mid-training phase of pre-training can create an iterative, self-reinforcing training loop for LLMs. This bidirectional pipeline improves reasoning capabilities and maintains benefits through post-training, suggesting a scalable path toward continuously evolving, higher-performing models without relying on specialized teacher models."}}
{"id": "2602.02978", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02978", "abs": "https://arxiv.org/abs/2602.02978", "authors": ["Zuyuan Zhang", "Zeyu Fang", "Tian Lan"], "title": "Structuring Value Representations via Geometric Coherence in Markov Decision Processes", "comment": null, "summary": "Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \\emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.", "AI": {"tldr": "They introduce GCR-RL, a reinforcement learning framework that treats value functions as learning a partially ordered set, enforcing geometric coherence over time and improving sample efficiency and stability.", "motivation": "Reinforcement learning can be unstable and sample-inefficient; geometric structure (like symmetry, order, and constraints) can help, but existing methods don\u2019t fully exploit order-theoretic structure underlying value functions.", "method": "They reinterpret value function estimation as learning a target poset and introduce GCR-RL, which builds a sequence of increasingly refined \"super-posets.\" At each step, temporal-difference signals are used to refine the partial order (i.e., add order relations) while maintaining geometric coherence. They instantiate this framework with two algorithms (one Q-learning based and one actor\u2013critic based) that incorporate this poset refinement into their updates, and provide theoretical analysis including convergence rates.", "result": "On a variety of RL tasks, the proposed GCR-RL algorithms significantly improve sample efficiency and training stability compared to strong baselines.", "conclusion": "Viewing reinforcement learning through order theory and enforcing geometric coherence via poset refinements yields practical algorithms (GCR-RL) that are both theoretically grounded and empirically effective, leading to faster and more stable learning."}}
{"id": "2602.03084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03084", "abs": "https://arxiv.org/abs/2602.03084", "authors": ["Zhitao Gao", "Jie Ma", "Xuhong Li", "Pengyu Li", "Ning Qu", "Yaqiang Wu", "Hui Liu", "Jun Liu"], "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback", "comment": null, "summary": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.", "AI": {"tldr": "AERO is an unsupervised framework that lets an LLM improve its own reasoning by asking, answering, and critiquing itself in a structured dual-loop process, focusing learning on problems at the edge of its abilities and verifying them with counterfactual checks.", "motivation": "LLMs are strong at complex reasoning but still depend heavily on expert-labeled data and external verifiers. Existing self-evolution methods try to remove this dependence, yet they struggle to select the right difficulty of training data (the optimal learning zone) and can amplify hallucinations or wrong internal beliefs when their self-feedback is flawed. The motivation is to build a principled, safe way for models to autonomously refine their reasoning without external labels or verifiers.", "method": "The authors propose AERO, an unsupervised, autonomous reasoning-evolution framework. It internalizes three roles within the LLM: a question generator, a problem solver, and a critic/verifier, organized into a synergistic dual-loop system for self-questioning, answering, and criticism. Drawing on the Zone of Proximal Development, it uses entropy-based positioning to identify the \u201csolvability gap\u201d \u2014 tasks that are neither trivial nor impossible \u2014 as the core training region. For verification, it introduces Independent Counterfactual Correction: solutions are checked by generating independent alternative reasoning or counterfactual variants to detect and correct errors rather than relying on a single proof path. To prevent different internal roles from diverging or collapsing (e.g., curriculum collapse where tasks become too easy or too hard), a Staggered Training Strategy is applied so that the capabilities of these roles grow in a synchronized manner over time.", "result": "On nine benchmarks across three domains, AERO applied to Qwen3-4B-Base and Qwen3-8B-Base yields consistent gains. The reported average improvements are 4.57% for Qwen3-4B-Base and 5.10% for Qwen3-8B-Base over their respective baselines, and AERO surpasses other competitive self-improvement and reasoning-optimization methods on these tasks.", "conclusion": "AERO demonstrates that LLMs can improve their reasoning capabilities in an unsupervised, autonomous way by carefully structuring internal question\u2013answer\u2013critique loops, focusing training on the solvability gap, and using independent counterfactual verification. The framework mitigates issues like misaligned difficulty selection and self-reinforced hallucinations, leading to measurable and robust performance gains over strong baselines, suggesting a promising direction for scalable, label-free reasoning optimization in LLMs."}}
{"id": "2602.02983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02983", "abs": "https://arxiv.org/abs/2602.02983", "authors": ["Hanna M. Dettki", "Charley M. Wu", "Bob Rehder"], "title": "Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget", "comment": null, "summary": "Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \\!\\rightarrow\\! E\\! \\leftarrow \\!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.", "AI": {"tldr": "The paper evaluates whether large language models perform human-like or normative causal reasoning on collider-structure tasks and compares them to humans.", "motivation": "Although LLMs are increasingly used in settings where causal reasoning is crucial, it is unknown if their causal judgments rely on proper causal computation, heuristics, or shallow pattern matching. Understanding this is essential for safe and effective deployment.", "method": "The authors benchmark over 20 LLMs and a matched human baseline on 11 causal judgment tasks based on a collider structure (C1 -> E <- C2). They fit a small interpretable model to compress and explain LLM causal judgments. They analyze patterns of reasoning, including whether LLMs show human-like collider biases like weak explaining away and Markov violations. They then test robustness by (i) varying the semantic abstraction level of the tasks and (ii) adding irrelevant text to prompts, and they assess the impact of chain-of-thought prompting on robustness.", "result": "Most LLMs are well-described by a compact interpretable model and tend to use more rule-like reasoning than humans, who incorporate latent, unmentioned factors in their judgments. LLMs generally do not reproduce classic human collider biases, and many show improved robustness to semantic abstraction and prompt overloading when using chain-of-thought prompting.", "conclusion": "LLMs\u2019 causal reasoning strategies diverge from humans: they are more rule-based and less biased by human collider effects, which can be advantageous when human biases are problematic. However, their rule-like strategies can fail when uncertainty and unmodeled factors are important. This underscores the need to explicitly characterize and understand LLM reasoning strategies to deploy them safely and effectively in causal domains."}}
{"id": "2602.03094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03094", "abs": "https://arxiv.org/abs/2602.03094", "authors": ["Yufan Zhuang", "Chandan Singh", "Liyuan Liu", "Yelong Shen", "Dinghuai Zhang", "Jingbo Shang", "Jianfeng Gao", "Weizhu Chen"], "title": "Test-time Recursive Thinking: Self-Improvement without External Feedback", "comment": null, "summary": "Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.", "AI": {"tldr": "The paper proposes Test-time Recursive Thinking (TRT), a framework that lets large language models iteratively self-improve at inference time using their own strategies and verification signals, achieving large gains on math and coding benchmarks without extra training or external feedback.", "motivation": "While LLM reasoning has improved via reinforcement learning with verifiable rewards, this requires additional training and labeled supervision. The authors want to know if strong LLMs can further improve themselves purely at test time, without new training data or reward models. This raises challenges in generating good diverse candidate solutions and choosing correct answers without ground truth, which the paper aims to solve.", "method": "They introduce Test-time Recursive Thinking (TRT), an iterative self-improvement procedure at inference time. TRT explicitly conditions each rollout on a specific strategy, aggregates knowledge and partial results over iterations, and uses model-generated verification or checking signals to evaluate and refine candidate solutions. The framework structures the model\u2019s thinking process recursively and leverages internal feedback in place of external rewards or labels.", "result": "With TRT, open-source models achieve 100% accuracy on AIME-25/24 math competition problems. On the hardest LiveCodeBench coding tasks, closed-source models (e.g., top proprietary LLMs) see accuracy improvements of 10.4\u201314.8 percentage points compared to standard inference, all without relying on external supervision or fine-tuning.", "conclusion": "LLMs can significantly self-improve at reasoning and problem-solving during inference alone, provided they are guided by a structured, recursive test-time framework that enforces diverse strategy generation and self-verification. TRT demonstrates that careful orchestration of a model\u2019s own capabilities can substitute for additional RL training on several challenging benchmarks."}}
{"id": "2602.02991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02991", "abs": "https://arxiv.org/abs/2602.02991", "authors": ["Haijiang Yan", "Jian-Qiao Zhu", "Adam Sanborn"], "title": "Large Language Models Can Take False First Steps at Inference-time Planning", "comment": null, "summary": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.", "AI": {"tldr": "The paper explains why large language models seem to plan poorly at inference despite having planning abilities, proposing a Bayesian explanation based on how self-generated context alters planning behavior, and validating it with controlled experiments.", "motivation": "Although LLMs develop planning abilities during training, their behavior at inference often looks short-sighted and inconsistent. The authors want to understand and formally explain this discrepancy by modeling how planning depends on the generative context and why prompts vs. self-generated text lead to different planning behavior.", "method": "They construct a Bayesian model of LLM planning that connects planning strength to the evolving generative context, particularly distinguishing between human-provided prompts and self-generated continuations. They then design two controlled experiments: (1) a random-generation task, where they measure planning strength under human prompts and as more self-generated tokens accumulate; (2) a Gaussian-sampling task, where they look at bias in initial samples and how conditioning on self-generated sequences reduces this bias.", "result": "In the random-generation task, they find that LLMs exhibit constrained planning when only given human prompts but show stronger planning behavior as more self-generated context is accumulated. In the Gaussian-sampling task, they observe that conditioning on self-generated sequences leads to reduced initial bias, consistent with the Bayesian account of planning based on generative context.", "conclusion": "LLM planning at inference is not inherently poor or inconsistent with training; instead, planning strength is context-dependent and shifts as self-generated text accumulates. Their Bayesian model and experiments jointly suggest that apparent planning failures arise from the mismatch between natural language prompts and the internal language/statistics of LLMs, offering a theoretical and empirical characterization of how LLMs plan ahead during inference."}}
{"id": "2602.03103", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03103", "abs": "https://arxiv.org/abs/2602.03103", "authors": ["Pritam Kadasi", "Abhishek Upperwal", "Mayank Singh"], "title": "Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision", "comment": null, "summary": "Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \\emph{does the instruction uniquely determine the target output?}\n  We propose the \\textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \\textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\\textsc{Alpaca}, \\textsc{Dolly-15k}, \\textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.", "AI": {"tldr": "They propose a metric (TSS/TSS++) to measure how much instructions actually matter for predicting outputs in instruction-tuned LLM data, and show that picking high-TSS examples improves model performance under token budget limits.", "motivation": "Instruction tuning data often has weakly specified instruction\u2013input\u2013output triplets, where the same response might fit multiple instructions, so it\u2019s unclear whether the instruction really determines the output. The authors want to know how instruction-specific each training example is and whether focusing on more task-specific examples gives better performance, especially when training tokens are limited.", "method": "Define Task\u2013Specificity Score (TSS) by contrasting the real instruction for an input\u2013output pair against plausible alternative instructions for the same input, and measuring how much more predictive the true instruction is of the output than the alternatives. Introduce TSS++ that uses harder negatives and an additional quality term to avoid trivial easy negatives dominating the score. Compute TSS/TSS++ on several instruction datasets (Alpaca, Dolly-15k, NI-20) with multiple open LLMs (Gemma, Llama, Qwen), then select subsets of training data with high task-specificity and compare downstream performance to other filtering strategies like perplexity and IFD.", "result": "High-TSS and TSS++ examples lead to better downstream performance than random selection when only a limited number of tokens can be used for training or adaptation. TSS-based selection provides complementary benefits to standard quality-based filters such as perplexity and Instruction-Following Difficulty (IFD).", "conclusion": "Task-specificity of instructions is a measurable property of instruction-tuning datasets. TSS/TSS++ can identify examples where the instruction truly constrains the output, and using these examples improves model performance under tight token budgets while complementing existing quality filters. This suggests that future instruction-tuning pipelines should explicitly account for task-specificity when curating training data."}}
{"id": "2602.02995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02995", "abs": "https://arxiv.org/abs/2602.02995", "authors": ["Sizhe Tang", "Rongqian Chen", "Tian Lan"], "title": "Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents", "comment": null, "summary": "While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\\sim 77\\%$, significantly outperforming trajectory-level baselines under equivalent compute.", "AI": {"tldr": "Agent Alpha is a GUI agent framework that uses step-level Monte Carlo Tree Search (MCTS) to more efficiently explore and reuse partial action sequences, achieving state-of-the-art performance on the OSWorld benchmark.", "motivation": "Existing GUI agents often scale performance by sampling many full trajectories at test time, but they lack a way to backtrack from early mistakes or reuse partially successful trajectories, leading to inefficient use of compute and suboptimal planning.", "method": "The paper proposes Agent Alpha, which integrates generation, exploration, and evaluation via step-level MCTS using an alpha-UCT variant. The method performs alpha-UCT guided search in the interaction loop for deliberate planning, early pruning of bad branches, and prefix reuse. It also uses comparison-based evaluation instead of absolute scores to reduce bias and introduces diversity-constrained expansion to keep the search space compact. A regret bound for the alpha-UCT algorithm is provided.", "result": "On the OSWorld benchmark, Agent Alpha achieves about 77% success rate, surpassing trajectory-level sampling baselines when using the same amount of test-time compute.", "conclusion": "Step-level MCTS with alpha-UCT, comparison-driven evaluation, and diversity-aware expansion forms an effective unified framework for GUI agents, enabling better use of test-time compute and achieving state-of-the-art performance on OSWorld while being theoretically grounded via a regret bound analysis."}}
{"id": "2602.03107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03107", "abs": "https://arxiv.org/abs/2602.03107", "authors": ["Yitong Zhang", "Yuhan Xiang", "Mingxuan Liu"], "title": "The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models", "comment": "Preprint", "summary": "From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.", "AI": {"tldr": "The paper evaluates how well several major Chinese-capable LLMs can recognize politeness, impoliteness, and mock politeness in Chinese discourse, using pragmatic theories and multiple prompting strategies.", "motivation": "Existing LLM research focuses heavily on syntax, semantics, and general task performance, but their ability to understand fine-grained pragmatic phenomena\u2014especially politeness-related meanings in Chinese\u2014is underexplored. There is also a broader intellectual interest in how AI technologies can handle nuanced human interactional norms and what that implies for the relationship between technology and the humanities.", "method": "The authors build a three-category dataset (politeness, impoliteness, mock politeness) based on Chinese discourse, combining real and simulated data, grounded in Rapport Management Theory and the Model of Mock Politeness. They then evaluate six representative LLMs (e.g., GPT-5.1, DeepSeek) under four prompting regimes: zero-shot, few-shot, knowledge-enhanced, and hybrid, to compare performance in recognizing these pragmatic categories.", "result": "The models show differing levels of success at identifying politeness, impoliteness, and mock politeness, with performance varying by model and prompting strategy. Knowledge-enhanced and hybrid prompts likely improve recognition, particularly for the more subtle category of mock politeness, while zero-shot is less reliable. The findings reveal persistent gaps in LLMs\u2019 pragmatic comprehension of Chinese politeness phenomena.", "conclusion": "The study demonstrates that even advanced LLMs still struggle with nuanced pragmatic phenomena such as mock politeness in Chinese, but that model choice and prompting strategy can significantly affect performance. It illustrates a productive way to apply pragmatic theories (Rapport Management, mock politeness models) to empirical LLM evaluation, and positions this work as part of an emerging \"Great Linguistics\" paradigm that integrates technological tools with humanistic reflection on language use."}}
{"id": "2602.03003", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03003", "abs": "https://arxiv.org/abs/2602.03003", "authors": ["Zhiyu An", "Wan Du"], "title": "Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment", "comment": null, "summary": "Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.\n  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.", "AI": {"tldr": "The paper reviews differentiable social choice, framing many machine learning systems as de facto social choice mechanisms and surveying how voting and mechanism design can be made differentiable and learnable.", "motivation": "As ML systems increasingly aggregate human preferences and incentives, they implicitly perform social choice without clear normative grounding. There is a need to systematically connect social choice theory with modern ML practice, especially via differentiable, data-driven models.", "method": "This is a survey/review paper. It synthesizes existing work across domains (auctions, voting, budgeting, liquid democracy, decentralized aggregation, inverse mechanism design) that models social choice rules as differentiable, trainable components, and it interprets classical axioms/impossibility results as optimization objectives and constraints.", "result": "The paper organizes and unifies scattered research under the umbrella of differentiable social choice, demonstrates that many classical social choice concepts can be recast in differentiable optimization terms, and compiles 36 concrete open research problems in this emerging area.", "conclusion": "Differentiable social choice offers a principled framework for embedding social choice mechanisms into ML systems, revealing how normative criteria translate into learning objectives and constraints. The field is nascent, with substantial open problems that call for joint work across ML, economics, and democratic theory."}}
{"id": "2602.03108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03108", "abs": "https://arxiv.org/abs/2602.03108", "authors": ["Aaditya Baranwal", "Shruti Vyas"], "title": "ChemPro: A Progressive Chemistry Benchmark for Large Language Models", "comment": null, "summary": "We introduce ChemPro, a progressive benchmark with 4100 natural language question-answer pairs in Chemistry, across 4 coherent sections of difficulty designed to assess the proficiency of Large Language Models (LLMs) in a broad spectrum of general chemistry topics. We include Multiple Choice Questions and Numerical Questions spread across fine-grained information recall, long-horizon reasoning, multi-concept questions, problem-solving with nuanced articulation, and straightforward questions in a balanced ratio, effectively covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry and Physical-Chemistry. ChemPro is carefully designed analogous to a student's academic evaluation for basic to high-school chemistry. A gradual increase in the question difficulty rigorously tests the ability of LLMs to progress from solving basic problems to solving more sophisticated challenges.\n  We evaluate 45+7 state-of-the-art LLMs, spanning both open-source and proprietary variants, and our analysis reveals that while LLMs perform well on basic chemistry questions, their accuracy declines with different types and levels of complexity. These findings highlight the critical limitations of LLMs in general scientific reasoning and understanding and point towards understudied dimensions of difficulty, emphasizing the need for more robust methodologies to improve LLMs.", "AI": {"tldr": "ChemPro is a large, difficulty-graded chemistry QA benchmark to evaluate LLMs, showing they do well on easy questions but struggle as complexity increases.", "motivation": "To systematically measure and stress-test how well LLMs understand and reason about general chemistry across increasing difficulty levels, similar to how students are evaluated academically.", "method": "The authors build ChemPro, a dataset of 4100 natural-language chemistry questions (multiple-choice and numerical) covering biochemistry, inorganic, organic, and physical chemistry, organized into four difficulty levels. Questions span recall, long-horizon reasoning, multi-concept integration, and nuanced problem solving. They then benchmark 45+7 open and proprietary LLMs on this dataset and analyze performance as a function of difficulty and question type.", "result": "LLMs generally achieve high accuracy on basic, straightforward chemistry questions, but their performance degrades significantly as problem difficulty, conceptual integration, and reasoning depth increase.", "conclusion": "ChemPro exposes substantial, systematic weaknesses in current LLMs\u2019 scientific reasoning and understanding in chemistry, indicating that existing models and training methods are insufficient, and that new approaches are needed to improve robustness and performance on complex, real-world chemistry problems."}}
{"id": "2602.03006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03006", "abs": "https://arxiv.org/abs/2602.03006", "authors": ["Ziyang Yu", "Liang Zhao"], "title": "Distilling LLM Reasoning into Graph of Concept Predictors", "comment": null, "summary": "Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.", "AI": {"tldr": "GCP is a reasoning-aware active distillation framework that trains compact discriminative models by mirroring a teacher LLM\u2019s reasoning as a graph of concept predictors, improving data efficiency, interpretability, and performance under low annotation budgets.", "motivation": "Existing active distillation methods for using LLMs on discriminative tasks mainly distill only final labels from an oracle LLM to smaller student models. This ignores the teacher\u2019s intermediate reasoning, reducing diagnostic insight into what reasoning is missing and offering limited interpretability of errors. There is also a need to reduce inference latency, compute, and API costs when deploying LLMs at scale, particularly under limited annotation budgets.", "method": "The authors introduce Graph of Concept Predictors (GCP), which externalizes the LLM teacher\u2019s decision process as a directed acyclic graph of intermediate concepts and predictions. The student model is decomposed into modular concept predictors corresponding to nodes in this graph. GCP uses a graph-aware active learning acquisition strategy that selects new samples by focusing on uncertainty and disagreement at key reasoning nodes. For training, it employs targeted sub-module retraining that attributes downstream loss to particular concept predictors and selectively updates only the most influential modules, improving training efficiency and stability.", "result": "Across eight NLP classification benchmarks, the GCP framework yields better performance than baselines when annotation budgets are limited. It also improves sample efficiency, training stability, and interpretability of the student\u2019s reasoning by exposing fine-grained concept-level predictions and diagnostics.", "conclusion": "Reasoning-aware active distillation via GCP effectively converts an LLM\u2019s decision process into a structured graph of concept-level predictors, enabling more efficient and interpretable training of compact discriminative models. By exploiting graph-aware acquisition and targeted sub-module retraining, GCP improves performance and control under constrained annotation budgets, suggesting a promising direction for scalable, cost-effective deployment of LLM-driven systems."}}
{"id": "2602.03109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03109", "abs": "https://arxiv.org/abs/2602.03109", "authors": ["Bowen Jiang", "Taiwei Shi", "Ryo Kamoi", "Yuan Yuan", "Camillo J. Taylor", "Longqi Yang", "Pei Zhou", "Sihao Chen"], "title": "One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence", "comment": null, "summary": "This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.", "AI": {"tldr": "OMAR is a reinforcement learning framework where a single AI model role-plays all agents in multi-turn, multi-agent conversations to learn social intelligence via self-play, demonstrating emergent abilities like empathy and persuasion without human supervision.", "motivation": "To move beyond static, single-turn optimization and hand-crafted supervision, and instead enable AI systems to acquire rich, long-term social intelligence and conversational skills directly from dynamic, multi-agent interactions.", "method": "They propose OMAR, where one model plays all conversational roles in multi-turn, multi-agent self-play. Training uses reinforcement learning with a hierarchical advantage estimation scheme: turn-level and token-level advantages to maintain stability in long dialogues. The model is evaluated in SOTOPIA social environments and Werewolf strategy games.", "result": "Models trained with OMAR show emergent, fine-grained social capabilities such as empathy, persuasion, and compromise seeking. They can collaborate effectively even in competitive game settings, demonstrating sophisticated social reasoning patterns.", "conclusion": "OMAR provides evidence that complex social intelligence can emerge in language models trained via multi-agent conversational self-play without direct human supervision, though issues like reward hacking remain. The framework may guide future research on socially intelligent AI for group conversations."}}
{"id": "2602.03022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03022", "abs": "https://arxiv.org/abs/2602.03022", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Jingfeng Luo", "Conggang Hu"], "title": "STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models", "comment": "The paper has been accepted to ICLR 2026", "summary": "The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.", "AI": {"tldr": "STAR is a training framework that distills the function-calling abilities of large LLMs into very small models by combining a stabilized distillation loss with a similarity-based reinforcement learning reward.", "motivation": "Large LLMs are strong at function calling but too large and costly for broad deployment, especially in resource-constrained settings. Existing methods for transferring their capabilities to smaller models suffer from overfitting, unstable training, crude binary rewards for tasks with many acceptable solutions, and difficulty in combining different techniques effectively. The paper aims to design a more stable, effective, and synergistic framework to teach super-tiny models to perform complex function calling like large models do.", "method": "The authors propose STAR, which has two main components integrated in a curriculum. (1) Constrained Knowledge Distillation (CKD): a modified distillation objective that adds constraints to the standard top-k forward KL divergence, explicitly penalizing overconfident wrong predictions while retaining enough probability mass elsewhere to keep exploration possible for later RL fine-tuning. This stabilizes training and mitigates overfitting. (2) Similarity-guided RL (Sim-RL): a reinforcement learning phase where the reward is not a simple binary success/failure, but a continuous, fine-grained similarity score between the model\u2019s generated function-calling outputs and ground-truth calls, giving a richer signal and improving policy optimization. These components are organized into a cohesive training curriculum to progressively refine super-tiny models.", "result": "On several difficult and well-known function-calling benchmarks, STAR-trained models achieve state-of-the-art performance for their parameter scales. The 0.6B-parameter STAR model attains the best performance among all open models under 1B parameters and even outperforms some substantially larger and popular open models. Empirical analyses show improvements in both stability and effectiveness over baseline distillation and RL methods.", "conclusion": "STAR successfully demonstrates that combining constrained knowledge distillation with similarity-based RL in a unified curriculum can reliably transfer function-calling capabilities from large LLMs into very small models. This yields super-tiny models with strong performance, stability, and efficiency, indicating a promising path toward more accessible and powerful AI agents deployable in resource-constrained environments."}}
{"id": "2602.03025", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03025", "abs": "https://arxiv.org/abs/2602.03025", "authors": ["Haitian Zhong", "Jixiu Zhai", "Lei Song", "Jiang Bian", "Qiang Liu", "Tieniu Tan"], "title": "RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents", "comment": null, "summary": "Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.", "AI": {"tldr": "The paper introduces RC-GRPO, a reward-conditioned variant of GRPO that mitigates vanishing policy updates in multi-turn tool calling by explicitly conditioning rollouts on discrete reward tokens to increase within-group reward diversity and improve learning efficiency.", "motivation": "Multi-turn tool calling for LLMs suffers from sparse rewards and costly exploration. Standard SFT+GRPO often fails when most trajectories in a group share identical rewards (all successes or all failures), making the group-relative advantage nearly zero and causing training to stall. The motivation is to design a method that maintains effective learning signals in such settings without dramatically increasing exploration cost.", "method": "The authors propose RC-GRPO (Reward-Conditioned GRPO). Step 1: Train a Reward-Conditioned Trajectory Policy (RCTP) via supervised fine-tuning on mixed-quality trajectories, augmenting prompts with special reward goal tokens like <|high_reward|> and <|low_reward|>. This teaches the model to generate trajectories of controllable quality. Step 2: During RL with GRPO, they sample different reward tokens within each group, then condition rollouts on these tokens. This encourages diverse behaviors within a group, leading to greater within-group reward variation and hence more informative group-normalized advantages.", "result": "On the BFCLv4 multi-turn function calling benchmark, RC-GRPO outperforms baseline methods across the board. When applied to Qwen-2.5-7B-Instruct, the method achieves performance that surpasses all evaluated closed-source API models on this benchmark, indicating strong gains in multi-turn tool-calling capability.", "conclusion": "Reward-conditioning with discrete reward tokens, combined with group-based policy optimization, is an effective way to address vanishing advantages in sparse-reward multi-turn tool-calling tasks. By explicitly steering the model toward diverse-quality trajectories within GRPO groups, RC-GRPO improves learning signals and yields state-of-the-art performance on a challenging function-calling benchmark, suggesting a promising direction for training LLMs in complex interactive settings."}}
{"id": "2602.03152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03152", "abs": "https://arxiv.org/abs/2602.03152", "authors": ["Yifei Wang", "Yueqi Wang", "Zhenrui Yue", "Huimin Zeng", "Yong Wang", "Ismini Lourentzou", "Zhengzhong Tu", "Xiangxiang Chu", "Julian McAuley"], "title": "FASA: Frequency-aware Sparse Attention", "comment": "Accepted by ICLR 2026", "summary": "The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\\times$ speedup using just 18.9\\% of the cache on AIME24.", "AI": {"tldr": "They propose FASA, a RoPE-based, query-aware token pruning framework that drastically reduces KV cache memory for long-context LLMs while preserving near-full accuracy.", "motivation": "LLMs struggle with long inputs because the KV cache grows linearly with sequence length, leading to high memory and bandwidth costs. Existing token pruning approaches are either static (risking permanent information loss) or dynamic but heuristic-based, failing to fully capture how token importance depends on the current query. A more principled, query-aware, yet efficient method is needed to reduce KV cache usage without hurting performance.", "method": "The authors introduce FASA, a token-eviction/pruning framework grounded in a new analysis of Rotary Positional Embedding (RoPE). They discover \"functional sparsity\" at the frequency-chunk (FC) level: only a small set of dominant FCs in each attention head is required to approximate full attention well. These dominant FCs are used as a free proxy to estimate token importance in a query-aware manner. FASA first uses dominant FCs to identify a critical subset of salient tokens, then computes full attention only over this pruned token set, accessing only a small portion of the KV cache and thus reducing memory and compute costs.", "result": "On a range of long-context benchmarks, including sequence modeling and complex chain-of-thought reasoning tasks, FASA consistently outperforms prior token-eviction baselines and attains performance close to an oracle that knows ideal tokens to keep. On LongBench-V1 it achieves nearly 100% of full-KV performance while keeping just 256 tokens; on AIME24 it attains a 2.56x speedup using only 18.9% of the KV cache.", "conclusion": "Functional sparsity in RoPE at the frequency-chunk level enables an efficient, query-aware estimate of token importance. Exploiting this, FASA provides a practical way to aggressively prune tokens and shrink the KV cache in long-context LLMs while maintaining near-oracle accuracy, significantly improving speed and memory efficiency over previous token-pruning approaches."}}
{"id": "2602.03026", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03026", "abs": "https://arxiv.org/abs/2602.03026", "authors": ["Weilin Ruan", "Yuxuan Liang"], "title": "Visual Reasoning over Time Series via Multi-Agent System", "comment": null, "summary": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.", "AI": {"tldr": "MAS4TS is a multi-agent, tool-driven system that uses vision-language-based visual reasoning and latent reconstruction to achieve strong, general performance across many time-series tasks.", "motivation": "Existing time-series methods and pretrained large models struggle to combine intuitive visual reasoning over time series plots with flexible, tool-based generalization across diverse tasks. The authors aim to overcome these limitations by creating a system that can reason visually about temporal structures and adaptively choose tools for different time series problems.", "method": "The paper introduces MAS4TS, a multi-agent system based on an Analyzer-Reasoner-Executor paradigm. It first converts time series into plots and uses a Vision-Language Model to extract temporal structures via visual reasoning with structured priors. It then reconstructs predictive trajectories in a latent space. Three specialized agents communicate through shared memory and gated communication, coordinated by a router that selects appropriate task-specific tool chains for execution.", "result": "Across multiple time series benchmarks, MAS4TS attains state-of-the-art results on a variety of tasks. It shows strong generalization abilities and offers efficient inference compared with prior approaches.", "conclusion": "Integrating visual reasoning, latent reconstruction, and agent-based tool orchestration in MAS4TS provides a unified and effective framework for general time series analysis, improving performance, generalization, and efficiency over existing approaches."}}
{"id": "2602.03183", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03183", "abs": "https://arxiv.org/abs/2602.03183", "authors": ["Hyunwoo Kim", "Niloofar Mireshghallah", "Michael Duan", "Rui Xin", "Shuyue Stella Li", "Jaehun Jung", "David Acuna", "Qi Pang", "Hanshen Xiao", "G. Edward Suh", "Sewoong Oh", "Yulia Tsvetkov", "Pang Wei Koh", "Yejin Choi"], "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch", "comment": "For code and data, see https://privasis.github.io", "summary": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.", "AI": {"tldr": "Introduces Privasis, a million-scale, fully synthetic dataset of privacy-sensitive texts enabling superior training of compact text-sanitization models that outperform larger LLMs.", "motivation": "Research on privacy-sensitive domains (medical, legal, financial, personal communications) suffers from data scarcity because real data cannot be freely shared, while modern AI agents increasingly require safe handling of highly sensitive personal information. There is a need for a large, shareable, realistic dataset that captures diverse private attributes without exposing real individuals, and for better sanitization models to reduce privacy risks.", "method": "The authors construct Privasis, a fully synthetic dataset of 1.4 million documents spanning diverse document types (e.g., medical histories, legal contracts, financial records, calendars, text messages) and richly annotated with 55.1M privacy attributes (e.g., ethnicity, DOB, workplace). They then build a parallel corpus for text sanitization by decomposing texts and applying targeted sanitization rules, and train compact sanitization models (\u22644B parameters) on this corpus using their pipeline.", "result": "Privasis is orders of magnitude larger and more diverse than existing privacy-related datasets while maintaining high quality. Models up to 4B parameters trained on the resulting sanitization corpus outperform state-of-the-art large language models (e.g., GPT-5, Qwen-3 235B) on text sanitization benchmarks, demonstrating strong utility of the synthetic data and pipeline.", "conclusion": "A large-scale, fully synthetic, richly annotated dataset like Privasis can effectively substitute for real privacy-sensitive data in training high-performing sanitization models. The dataset and associated models/pipeline substantially advance research capacity in privacy-sensitive domains and can improve the safety of AI agents that process personal data. The authors intend to release Privasis, models, and code to catalyze further work in this area."}}
{"id": "2602.03034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03034", "abs": "https://arxiv.org/abs/2602.03034", "authors": ["Binbin Yong", "Haoran Pei", "Jun Shen", "Haoran Li", "Qingguo Zhou", "Zhao Su"], "title": "KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning", "comment": null, "summary": "Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.", "AI": {"tldr": "Proposes KANFIS, a more compact and interpretable neuro-fuzzy system that scales linearly with input dimension using additive decomposition instead of the product-based rule explosion in ANFIS.", "motivation": "Conventional ANFIS models use product-based inference that leads to exponential growth in the number of rules as input dimensionality increases, resulting in high structural complexity and reduced interpretability. There is a need for a neuro-fuzzy architecture that preserves learning ability and interpretability while scaling more gracefully with dimensionality and handling uncertainty.", "method": "Introduce KANFIS, which combines fuzzy reasoning with Kolmogorov-Arnold style additive function decomposition. It replaces product-based aggregation with an additive mechanism so both parameters and rules grow linearly with dimension. It supports both Type-1 and Interval Type-2 fuzzy sets to explicitly capture uncertainty, and uses sparse masking to form compact, structured rule sets with clear semantics and transparent inference. The method is empirically evaluated against neural and neuro-fuzzy baselines.", "result": "KANFIS attains competitive predictive performance compared to representative neural network and neuro-fuzzy models while using more compact, structured rule sets whose complexity scales linearly with the number of inputs.", "conclusion": "KANFIS resolves the rule explosion problem of conventional ANFIS by adopting additive decomposition and sparse masking, yielding an interpretable, scalable neuro-fuzzy architecture that can also model uncertainty through T1 and IT2 fuzzy logic, without sacrificing empirical performance."}}
{"id": "2602.03203", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03203", "abs": "https://arxiv.org/abs/2602.03203", "authors": ["Zican Dong", "Peiyu Liu", "Junyi Li", "Zhipeng Chen", "Han Peng", "Shuo Wang", "Wayne Xin Zhao"], "title": "ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution", "comment": null, "summary": "Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.", "AI": {"tldr": "The paper proposes ForesightKV, a learning-based framework to evict key-value cache entries in LLMs more intelligently, reducing memory/computation for long reasoning traces while maintaining performance.", "motivation": "Long reasoning traces in LLMs cause the key-value (KV) cache to grow linearly with sequence length, leading to high memory and computation costs. Existing eviction strategies are hand-crafted and typically rely on simple heuristics (e.g., recency, attention magnitude), which struggle to model complex dependencies among KV entries and often hurt model performance. A better approach is needed that can anticipate the downstream impact of evicting particular KV pairs and thus improve the efficiency\u2013performance trade-off in long-context generation, especially for reasoning tasks.", "method": "They propose ForesightKV, a training-based KV cache eviction framework. 1) They design the Golden Eviction algorithm that, using future attention scores during generation, labels the optimal KV entries to evict at each step, yielding supervision traces. 2) They then train an eviction policy via supervised learning, using a Pairwise Ranking Loss to learn to rank KV pairs by eviction priority based on those golden labels and scores. 3) To address cases where naive eviction increases language modeling loss, especially on low-entropy (high-confidence) tokens, they cast eviction as a Markov Decision Process and apply the GRPO reinforcement learning algorithm to further refine the policy, combining supervised and RL signals. The resulting model predicts at inference time which KV pairs to drop under a given cache budget.", "result": "On AIME2024 and AIME2025 reasoning benchmarks and across three reasoning LLMs, ForesightKV consistently outperforms existing KV cache eviction methods while using only half of the KV cache budget. The combination of supervised training from Golden Eviction traces and GRPO-based RL yields better performance than either alone, demonstrating a synergistic effect.", "conclusion": "Learning-based KV cache eviction, informed by future-attention-derived supervision and refined with reinforcement learning, can substantially improve the efficiency\u2013accuracy trade-off for long-context reasoning in LLMs. ForesightKV shows that modeling complex KV dependencies and treating eviction as a decision process enables significant cache savings (up to 50%) without sacrificing\u2014and even improving\u2014task performance compared to heuristic eviction baselines."}}
{"id": "2602.03053", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03053", "abs": "https://arxiv.org/abs/2602.03053", "authors": ["Vishal Venkataramani", "Haizhou Shi", "Zixuan Ke", "Austin Xu", "Xiaoxiao He", "Yingbo Zhou", "Semih Yavuz", "Hao Wang", "Shafiq Joty"], "title": "MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems", "comment": "Preprint; work in progress", "summary": "Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.", "AI": {"tldr": "The paper empirically evaluates how well different process-verification methods (checking intermediate reasoning steps) help Large-LLM-based multi-agent systems, and finds that they often don\u2019t reliably improve performance and have high variance.", "motivation": "LLM-based multi-agent systems show highly variable reasoning paths and outcomes. Process verification\u2014scoring or checking intermediate reasoning steps\u2014has worked in single-agent/general reasoning scenarios and is hypothesized to help coordinate agents in MAS. However, its actual impact, best designs (granularity, verifier types, context strategies), and limitations in MAS are not well understood. The paper aims to systematically test and clarify when and how process verification helps or hurts MAS performance.", "method": "The authors introduce MAS-ProVe, a systematic empirical study framework. They compare three verification paradigms: (1) LLM-as-a-Judge, (2) reward models, and (3) process reward models. They test verification at two granularities: agent-level (evaluating each agent\u2019s contribution) and iteration-level (evaluating per round of interaction). They benchmark five concrete verifiers and four context management strategies across six different multi-agent frameworks on multiple reasoning benchmarks, measuring performance, variance, and trade-offs such as context length vs. accuracy.", "result": "Process-level verification does not consistently improve MAS performance and often introduces large variance in outcomes. It is difficult to reliably assess partial, evolving trajectories of multiple agents. Among examined methods, LLM-as-a-Judge generally performs better than reward-model-based approaches; specialized, trained judges outperform generic, off-the-shelf LLMs. The performance difference between LLMs used as process judges and the same LLMs used as single-agent solvers is small. There is also a clear trade-off between longer verification contexts (more information to judge) and performance, likely due to context length limitations and noise.", "conclusion": "State-of-the-art process verification techniques, even when carefully designed and tuned, are not yet a robust solution for improving or coordinating LLM-based multi-agent systems. While LLM-as-a-Judge is currently the strongest of the studied paradigms, its gains are modest and unreliable, and verification quality degrades or becomes unstable as context grows. Developing reliable, scalable methods for evaluating and guiding partial trajectories in MAS remains an open and challenging research problem, requiring new ideas beyond current LLM-judge and reward-model frameworks."}}
{"id": "2602.03216", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03216", "abs": "https://arxiv.org/abs/2602.03216", "authors": ["Dongwon Jo", "Beomseok Kang", "Jiwon Song", "Jae-Joon Kim"], "title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection", "comment": null, "summary": "The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.", "AI": {"tldr": "They propose Token Sparse Attention, a dynamic, per-head token-level sparsification method for attention that compresses and then decompresses tokens, improving speed for long contexts with minimal accuracy loss.", "motivation": "Standard attention has quadratic complexity in sequence length, which makes long-context inference in large language models very slow and costly. Existing acceleration methods rely on fixed sparsity patterns or permanently dropping tokens at certain layers, which can keep irrelevant tokens or make irreversible mistakes because token importance varies by layer and head. A more flexible and dynamic approach is needed.", "method": "They introduce Token Sparse Attention, which works at the token level for each attention head. During attention, it selects a reduced set of tokens and compresses the Q, K, V tensors to this smaller set, applies attention over it, and then decompresses the outputs back to the full sequence so that no information is permanently discarded and can be reconsidered later. The mechanism is lightweight, dynamic, and operates interleaved with standard attention layers. It is compatible with dense attention implementations like FlashAttention and can be combined with other sparse attention kernels.", "result": "Experiments show that Token Sparse Attention improves the trade-off between accuracy and latency in long-context settings. Specifically, at a 128K token context length, it achieves up to a 3.23x speedup in the attention computation with less than 1% drop in accuracy across evaluated tasks.", "conclusion": "Dynamic, interleaved token-level sparsification via Token Sparse Attention is an effective, complementary strategy to existing sparse attention methods for scaling long-context inference. It offers significant speedups while keeping accuracy degradation very small and integrates cleanly with current dense and sparse attention kernels."}}
{"id": "2602.03097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03097", "abs": "https://arxiv.org/abs/2602.03097", "authors": ["Bryce Kan", "Wei Yang", "Emily Nguyen", "Ganghui Yi", "Bowen Yi", "Chenxiao Yu", "Yan Liu"], "title": "De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models", "comment": null, "summary": "Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.", "AI": {"tldr": "JobRec is a generative LLM-based job recommendation framework that separates candidate preference from qualification assessment to enable controllable, strategy-aware professional matching.", "motivation": "Existing LLM-based job recommendation methods conflate two distinct decision dimensions\u2014whether a candidate is interested in a job (preference) and whether they are suitable/eligible (qualification). This is further distorted by recruitment-funnel censoring (only partial observations of interactions) and makes it difficult to explicitly control recommendation policies (e.g., enforcing eligibility rules or strategic trade-offs). There is a need for a framework that disentangles these factors for clearer supervision and controllable matching.", "method": "JobRec introduces: (1) a Unified Semantic Alignment Schema that maps both candidate resumes and job descriptions into shared, structured semantic layers (e.g., skills, experience, responsibilities) for consistent representation; (2) a Two-Stage Cooperative Training Strategy that trains separate expert models to infer candidate preference and candidate qualification independently; and (3) a Lagrangian-based Policy Alignment module that uses these experts to generate job recommendations while satisfying explicit eligibility constraints and allowing tunable trade-offs between preference and qualification. To address limited real-world labeled data, the authors also construct and expert-refine a synthetic dataset for training and evaluation.", "result": "Experiments demonstrate that JobRec surpasses strong baseline methods on professional job recommendation benchmarks, achieving higher matching performance metrics while also enabling more precise control over recommendation strategies (e.g., adjusting the balance between preference satisfaction and qualification compliance). The framework shows better robustness under recruitment-funnel censoring and improved interpretability of the decision factors.", "conclusion": "Decoupling preference and qualification within a generative LLM-based framework leads to more accurate, controllable, and policy-compliant job recommendations. JobRec\u2019s unified semantic alignment, dual-expert design, and Lagrangian policy alignment collectively provide a principled way to perform strategy-aware professional matching, suggesting a promising direction for future recommender systems in high-stakes domains like recruitment."}}
{"id": "2602.03100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03100", "abs": "https://arxiv.org/abs/2602.03100", "authors": ["Jingnan Zheng", "Yanzhen Luo", "Jingjun Xu", "Bingnan Liu", "Yuxin Chen", "Chenhang Cui", "Gelei Deng", "Chaochao Lu", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.", "AI": {"tldr": "The paper introduces Risky-Bench, a general, deployment-grounded benchmark for systematically evaluating safety risks of LLM-based agents during realistic, long-horizon task execution.", "motivation": "Current safety evaluations for LLM-based agents focus on narrow, risk-specific or setting-specific tasks and mostly on linguistic harms, failing to capture the broader spectrum of safety risks that arise when agents operate as interactive, long-horizon actors in complex real-world environments. These evaluations also lack adaptability across different agent configurations and deployment scenarios.", "method": "The authors design Risky-Bench, a domain-agnostic evaluation framework that starts from high-level safety principles and turns them into context-aware safety rubrics capturing different regions of the safety risk space. They then instantiate these rubrics into realistic tasks, run LLM-based agents under different threat assumptions, and measure their behavior and safety performance during long-horizon, interactive task execution. They demonstrate the framework in a life-assist agent setting.", "result": "Applying Risky-Bench to state-of-the-art life-assist agents reveals substantial unmitigated safety risks when these agents operate under realistic execution conditions and varying threat models, showing that current agents can behave unsafely across multiple parts of the safety space defined by the rubrics.", "conclusion": "Risky-Bench provides a structured, extensible methodology for evaluating the safety of LLM-based agents in deployment-like conditions. It is not limited to life-assist scenarios and can be adapted to other domains, offering a generic pipeline for constructing environment-specific agent safety benchmarks that systematically probe safety risks across a principled safety space."}}
{"id": "2602.03295", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03295", "abs": "https://arxiv.org/abs/2602.03295", "authors": ["Junhui He", "Zhihui Fu", "Jun Wang", "Qingan Li"], "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference", "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.", "AI": {"tldr": "The paper proposes Prefill-Only Pruning (POP), a stage-aware pruning strategy that skips deep layers during prefill but keeps the full model for decoding, achieving notable prefill speedups with minimal accuracy loss.", "motivation": "Structured pruning is hardware-efficient but usually causes large accuracy drops because it treats all inference stages (prefill and decode) uniformly, ignoring their different roles and sensitivities. The authors aim to break this accuracy-efficiency trade-off by exploiting stage asymmetry.", "method": "They study layer importance separately for prefill and decode using a virtual gating mechanism and discover that deep layers are crucial for decoding but mostly redundant for prefill. Based on this, they design Prefill-Only Pruning (POP), which prunes deep layers only during the prefill stage. To keep the KV cache consistent between the pruned prefill and full decode, they introduce independent KV projections and a boundary handling strategy for the first generated token.", "result": "On Llama-3.1, Qwen3-VL, and Gemma-3 across text and multimodal tasks, POP achieves up to 1.37\u00d7 reduction in prefill latency while incurring only minimal performance degradation compared to unpruned or conventionally pruned baselines.", "conclusion": "Stage-aware pruning that exploits the distinct roles of prefill and decode allows models to skip redundant computation in prefill without harming decode quality, overcoming the traditional accuracy-efficiency trade-off of structured pruning for LLMs and VLMs."}}
{"id": "2602.03128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03128", "abs": "https://arxiv.org/abs/2602.03128", "authors": ["Abdelghny Orogat", "Ana Rostam", "Essam Mansour"], "title": "Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis", "comment": "25 pages, 9 figures and 13 tables; introduces MAFBench unified multi-agent evaluation suite", "summary": "Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.", "AI": {"tldr": "The paper studies how the architectural design of multi-agent LLM frameworks affects performance and introduces a benchmark suite to evaluate them systematically.", "motivation": "Although multi-agent LLM frameworks are popular, their architectural choices (how agents interact, store memory, and coordinate) can dramatically affect latency, throughput, accuracy, and scalability, but this impact is poorly understood and not systematically measured.", "method": "The authors define an architectural taxonomy for multi-agent LLM frameworks and build MAFBench, a unified evaluation suite that standardizes execution and integrates existing benchmarks to isolate and measure framework-level architectural effects across multiple capabilities (orchestration overhead, memory behavior, planning, specialization, coordination). They then run controlled empirical comparisons of several widely used frameworks using MAFBench.", "result": "The empirical study shows that purely architectural framework-level choices can increase latency by over 100x, reduce planning accuracy by up to 30%, and drop coordination success rates from above 90% to below 30%.", "conclusion": "Architectural design in multi-agent LLM frameworks has large, systematic impacts on performance and capability. The proposed taxonomy and MAFBench enable standardized, controlled evaluation, from which the authors derive concrete design principles and guidance for framework selection, and they identify promising directions for future research on framework architecture."}}
{"id": "2602.03318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03318", "abs": "https://arxiv.org/abs/2602.03318", "authors": ["Yifan Shi", "Jialong Shi", "Jiayi Wang", "Ye Fan", "Jianyong Sun"], "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research", "comment": null, "summary": "Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.", "AI": {"tldr": "The paper introduces MIRROR, a fine-tuning-free multi-agent LLM framework that converts natural language OR problems into accurate optimization models and solver code through iterative error correction and hierarchical retrieval, outperforming prior methods on benchmarks and industrial datasets.", "motivation": "Traditional OR modeling is expert-driven, slow, and brittle for new scenarios; current LLM-based automation either requires expensive post-training or lacks reliable error correction and targeted retrieval, leading to frequent modeling and coding errors.", "method": "Design MIRROR, an end-to-end multi-agent system that translates natural language descriptions into optimization models and solver code. It uses two key mechanisms: (1) execution-driven iterative adaptive revision, where models and code are run and then automatically revised based on errors; and (2) hierarchical retrieval, which retrieves relevant modeling and coding exemplars from a curated library to guide the LLM without additional fine-tuning.", "result": "On standard OR benchmarks and complex industrial datasets like IndustryOR and Mamo-ComplexLP, MIRROR achieves better performance than existing LLM-based modeling approaches, showing higher correctness and robustness in generated optimization models and solver code.", "conclusion": "MIRROR overcomes key limitations of general-purpose LLMs for OR by combining external knowledge retrieval with systematic execution-based error correction, enabling non-experts to reliably generate high-quality optimization models and code without specialized training or costly model fine-tuning."}}
{"id": "2602.03146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03146", "abs": "https://arxiv.org/abs/2602.03146", "authors": ["Santiago Cifuentes"], "title": "General Agents Contain World Models, even under Partial Observability and Stochasticity", "comment": "19 pages, 4 figures", "summary": "Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.\n  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.", "AI": {"tldr": "The paper generalizes a previous theorem showing that sufficiently capable agents must implicitly contain a model of their environment, extending it from deterministic, fully observable settings to stochastic agents in partially observable environments, and under weaker generality assumptions.", "motivation": "To better understand what it means for an intelligent agent to \"have a world model\" and to determine whether agents can avoid learning their environment by using randomness, especially beyond the restrictive assumptions of determinism and full observability from prior work.", "method": "The authors extend an existing theoretical framework that characterizes almost optimal, general agents and their internal knowledge of the environment. They generalize the reconstruction theorem by adapting it to stochastic agents and partially observable environments, and they mathematically relax the required level of agent generality while preserving the ability to recover an approximate environment model via black-box querying of the agent.", "result": "They prove that even when agents and environments are stochastic and observations are partial, any sufficiently capable agent (under a weaker generality condition than before) must contain enough information about the environment to permit approximate reconstruction of that environment from black-box access to the agent.", "conclusion": "Randomization does not allow agents to bypass learning their environment: even less-than-fully-general, stochastic agents in partially observable settings must internally encode a usable model of their world, and this model can in principle be approximately extracted from the agent's behavior."}}
{"id": "2602.03338", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03338", "abs": "https://arxiv.org/abs/2602.03338", "authors": ["Rakshith Vasudev", "Melisa Russak", "Dan Bikel", "Waseem Alshikh"], "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention", "comment": null, "summary": "Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.\n  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.", "AI": {"tldr": "Study analyzes when proactive LLM critic interventions help or hurt performance and proposes a small pilot-based pre-deployment test to decide whether to intervene.", "motivation": "Although LLM critics with high offline accuracy are assumed to reliably improve system behavior, their actual impact at deployment time is not well understood; high accuracy may not translate to safe or beneficial interventions.", "method": "Evaluate a binary LLM critic with strong offline AUROC against different base models, analyzing performance changes under critic-triggered interventions and characterizing a disruption-recovery tradeoff; then design and test a small-sample (50-task) pre-deployment evaluation protocol that predicts whether critic intervention will help or harm on a given benchmark.", "result": "Despite high offline AUROC (0.94), the critic causes large performance drops for one model (\u221226 percentage points) while leaving another nearly unaffected, revealing strong variability; the proposed 50-task pilot test reliably predicts whether intervention will degrade or (slightly) improve performance across benchmarks, including a +2.8 pp gain on ALFWorld (p=0.014) but degradation on high-success tasks.", "conclusion": "Critic model accuracy alone is not a sufficient safety signal for proactive intervention; interventions inherently trade off fixing failing trajectories against disrupting successful ones, and a lightweight pre-deployment pilot test can be used to detect when interventions are likely harmful so that they can be avoided, thereby preventing severe regressions before full deployment."}}
{"id": "2602.03151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03151", "abs": "https://arxiv.org/abs/2602.03151", "authors": ["Wei Dai", "Haoyu Wang", "Honghao Chang", "Lijun He", "Fan Li", "Jian Sun", "Haixia Bi"], "title": "Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration", "comment": "12 pages", "summary": "Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.", "AI": {"tldr": "They propose a general strategy to restore missing modalities in Vision-Language Models using an enhanced diffusion model module, improving robustness when some inputs are absent.", "motivation": "Existing Vision-Language Models assume all modalities are present at inference, and performance degrades sharply when some modalities are missing. Existing prompt-based and imputation-based solutions either harm generalization or produce semantically irrelevant noise. There is a need for a method that can accurately restore missing semantic information while preserving VLM generalization ability.", "method": "They introduce a pluggable mid-stage training module based on an enhanced diffusion model to restore missing features. The strategy has two main components: (I) Dynamic Modality Gating, which adaptively uses available conditional features to guide the generation of features that are semantically consistent with the missing ones; (II) a Cross-Modal Mutual Learning mechanism that connects the semantic spaces of two encoders, enabling bidirectional semantic alignment across modalities.", "result": "In zero-shot evaluations on multiple benchmark datasets, their method surpasses existing baselines in missing-modality scenarios. Experiments and ablations show that the proposed module is robust and scalable, maintaining strong performance over a wide range of missing rates and different environments.", "conclusion": "The proposed diffusion-based missing modality restoration strategy serves as a robust, plug-and-play extension for Vision-Language Models, effectively recovering missing semantics while preserving generalization, and works reliably across diverse missing-modality conditions. Code and models will be released for reproducibility and further research."}}
{"id": "2602.03352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03352", "abs": "https://arxiv.org/abs/2602.03352", "authors": ["Yunzhi Shen", "Hao Zhou", "Xin Huang", "Xue Han", "Junlan Feng", "Shujian Huang"], "title": "PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \\textbf{PEGRL}, a \\textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\\to$Finnish, English$\\to$Turkish, and English$\\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).", "AI": {"tldr": "They propose PEGRL, a two-stage RL framework using post-editing as an auxiliary task to stabilize and improve RL training for LLM-based machine translation, achieving better performance and sample efficiency than prior RL baselines.", "motivation": "RL for LLM-based machine translation is promising but suffers from noisy Monte Carlo returns and a huge trajectory space, which encourage coarse global exploration instead of precise local improvements. The authors want a method that stabilizes RL training and better balances exploration with fine-grained optimization.", "method": "They design PEGRL, a two-stage RL setup: (1) Sample translation outputs from the model. (2) Use these outputs as inputs to a post-editing task, so the RL return in the post-editing stage is conditioned on current translation behavior. A task-specific weighting scheme combines translation and post-editing rewards, forming a biased but more sample-efficient return estimator that encourages both global exploration and local refinement.", "result": "On several language pairs (En\u2192Fi, En\u2192Tr, En\u2194Zh), PEGRL shows consistent improvements over RL baselines. For En\u2192Tr, their COMET-KIWI scores are competitive with strong LLM-based systems such as DeepSeek-V3.2.", "conclusion": "Incorporating post-editing as an auxiliary RL task within a two-stage framework stabilizes training, improves sample efficiency, and yields better machine translation performance than existing RL methods for LLM-based MT, with results competitive with advanced systems on some benchmarks."}}
{"id": "2602.03160", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03160", "abs": "https://arxiv.org/abs/2602.03160", "authors": ["Woojin Kim", "Sieun Hyeon", "Jusang Oh", "Jaeyoung Do"], "title": "VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models", "comment": null, "summary": "Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.", "AI": {"tldr": "VALUEFLOW is a unified framework to extract, evaluate, and steer human values in LLMs with calibrated intensity control, via a hierarchical value space, an intensity-labeled database, and an anchor-based evaluator.", "motivation": "Existing alignment methods based on preferences struggle to capture deeper, structured human values, and current value-based methods lack hierarchical modeling, calibrated intensity evaluation, and clear steerability. The authors aim to build an infrastructure that systematically measures and controls value intensities in LLMs.", "method": "They propose VALUEFLOW with three parts: (1) HIVES, a hierarchical value embedding space capturing relationships within and across value theories; (2) VIDB, a large database of texts labeled with value types and intensities estimated from ranking-based aggregation; and (3) an anchor-based evaluator that scores the intensity of values in model outputs by comparing them to reference panels from VIDB. They then run large-scale experiments across ten LLMs and four value theories to test steerability and value composition.", "result": "The framework can consistently extract and score value intensities from LLM outputs and enable controlled steering of values at different intensity levels. Their experiments reveal asymmetries in how easily different values can be steered and characterize composition behaviors when controlling multiple values simultaneously.", "conclusion": "VALUEFLOW provides a scalable, unified infrastructure for hierarchical value representation, calibrated intensity evaluation, and controllable steering of LLM value expressions, supporting more nuanced, pluralistic alignment of LLMs with diverse human values."}}
{"id": "2602.03368", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03368", "abs": "https://arxiv.org/abs/2602.03368", "authors": ["Wei Zhu"], "title": "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain", "comment": null, "summary": "While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.", "AI": {"tldr": "This paper studies how to best design retrieval-augmented generation (RAG) systems, with a focus on medical applications, by analyzing components, proposing alternative designs, and empirically evaluating them on several tasks to derive best practices and performance\u2013efficiency trade-offs.", "motivation": "RAG is widely used in industrial LLM applications, but there is no clear consensus or guidelines on optimal architectures, component choices, and implementations, especially in high-stakes domains like medicine. Practitioners need systematic guidance rather than ad hoc designs.", "method": "The authors decompose a RAG system into its key components, enumerate and propose practical implementation alternatives for each, and then conduct systematic experiments across three task types to compare variants. They analyze how different design choices affect performance and efficiency in LLM-based RAG, particularly for medical-domain use cases.", "result": "The experiments identify which combinations of RAG components and design choices tend to work best across the evaluated tasks, and quantify trade-offs between model quality (e.g., accuracy, usefulness) and computational efficiency (e.g., speed, cost).", "conclusion": "Well-chosen RAG component designs significantly impact system performance and efficiency. The paper offers empirically grounded best practices and clarifies how to navigate performance\u2013efficiency trade-offs when deploying LLM-based RAG systems in industrial, particularly medical, settings."}}
{"id": "2602.03219", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03219", "abs": "https://arxiv.org/abs/2602.03219", "authors": ["Guhong Chen", "Chenghao Sun", "Cheng Fu", "Qiyao Wang", "Zhihong Huang", "Chaopeng Wei", "Guangxu Chen", "Feiteng Fang", "Ahmadreza Argha", "Bing Zhao", "Xander Xu", "Qi Han", "Hamid Alinejad-Rokny", "Qiang Qu", "Binhua Li", "Shiwen Ni", "Min Yang", "Hu Wei", "Yongbin Li"], "title": "Beyond Quantity: Trajectory Diversity Scaling for Code Agents", "comment": null, "summary": "As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.", "AI": {"tldr": "The paper introduces TDScaling, a trajectory diversity-based data synthesis framework that improves code agent performance by prioritizing diverse interaction trajectories over sheer data volume.", "motivation": "As code LLMs become tool-interactive agents via MCP, their performance is constrained by low-quality synthetic data and diminishing returns from simply adding more trajectories. Existing quantity-centric scaling underutilizes the rich structure of trajectory data and hits an early bottleneck, especially for long-tail and complex tool-use scenarios. There is a need for a more efficient scaling strategy that exploits diversity in trajectories to improve generalization and coding proficiency under a fixed training budget.", "method": "The authors propose TDScaling, a Trajectory Diversity Scaling framework for training code agents. It has four main components: (1) a Business Cluster mechanism that groups tools and tasks according to real-service logical dependencies, structuring the tool space into over 30,000 clusters; (2) a blueprint-driven multi-agent paradigm that coordinates multiple agents to generate coherent, multi-step trajectories aligned with high-level task blueprints; (3) an adaptive evolution mechanism that measures and promotes trajectory diversity using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity, actively steering synthesis toward long-tail and underrepresented scenarios to avoid mode collapse; and (4) a sandboxed code tool that allows safe code execution during synthesis to preserve and reinforce the model\u2019s intrinsic coding capabilities, reducing catastrophic forgetting of core programming skills.", "result": "On general tool-use benchmarks (BFCL, tau^2-Bench) and code agent benchmarks (RebenchT, CodeCI, BIRD), TDScaling-trained agents achieve better performance than quantity-scaled baselines under comparable training budgets. Empirically, increasing trajectory diversity produces larger performance gains than simply adding more trajectories, yielding a better performance-cost trade-off. The approach simultaneously boosts tool-use generalization and inherent coding proficiency, indicating that diversity-centric scaling is more effective than volume-centric scaling for code agents.", "conclusion": "TDScaling demonstrates that scaling along the axis of trajectory diversity, rather than raw data volume, is an effective way to improve code agents\u2019 tool-use and coding capabilities. By structuring tools into business clusters, enforcing coherent multi-agent blueprints, adaptively targeting long-tail scenarios via entropy-based diversity metrics, and using a sandboxed code tool to preserve coding skills, the framework achieves superior performance and efficiency. The authors intend to release the codebase and synthesized dataset to facilitate further research on diversity-oriented scaling for tool-interactive LLM agents."}}
{"id": "2602.03396", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03396", "abs": "https://arxiv.org/abs/2602.03396", "authors": ["Hao Fang", "Tianyi Zhang", "Tianqu Zhuang", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Leqi Liang", "Shu-Tao Xia", "Ke Xu"], "title": "Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective", "comment": null, "summary": "Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.", "AI": {"tldr": "The paper proposes an information-theoretic defense to protect proprietary LLMs from logit-based model distillation by minimizing conditional mutual information in their outputs.", "motivation": "Proprietary LLMs are valuable and usually accessible only via black-box APIs, but attackers can still perform model extraction via knowledge distillation. Existing defenses target text-based distillation and largely ignore the more informative logit-based distillation, leaving a crucial attack surface under-protected.", "method": "From an information-theoretic viewpoint, the authors define distillation-relevant information as the conditional mutual information (CMI) between teacher logits and inputs given ground-truth labels. They then learn a transformation matrix applied to the model\u2019s output logits. This matrix is optimized using a CMI-inspired anti-distillation objective that aims to remove information that is particularly useful for distillation while preserving information needed for accurate predictions.", "result": "Across several LLMs and strong distillation algorithms, the defense substantially lowers the performance of student models trained via distillation from the protected teacher, while maintaining the teacher\u2019s task accuracy and output utility.", "conclusion": "Minimizing conditional mutual information in logits via a learned transformation is an effective, practical way to harden black-box LLM APIs against logit-based model extraction, degrading distillation success without compromising the original model\u2019s accuracy."}}
{"id": "2602.03224", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03224", "abs": "https://arxiv.org/abs/2602.03224", "authors": ["Yu Cheng", "Jiuan Zhou", "Yongkang Hu", "Yihang Chen", "Huichi Zhou", "Mingang Chen", "Zhizhong Zhang", "Kun Shao", "Yuan Xie", "Zhaoxia Yin"], "title": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "comment": null, "summary": "Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.", "AI": {"tldr": "The paper studies how agent memory can become less safe over time during test-time learning, introduces a benchmark (Trust-Memevo) to measure this, and proposes a dual-memory framework (TAME) that improves both safety trustworthiness and task performance during memory evolution.", "motivation": "As agents adapt and accumulate experience at test time to approach AGI-level reasoning, their evolving memory can unintentionally drift away from safety alignment, even in benign tasks. Existing work lacks systematic evaluation of this agent memory misevolution and methods to preserve safety while maintaining or improving utility.", "method": "1) Define and formalize the phenomenon of Agent Memory Misevolution (safety degradation during test-time memory evolution). 2) Build Trust-Memevo, a benchmark for evaluating multi-dimensional trustworthiness of agents as they undergo benign task evolution across diverse domains and settings. 3) Empirically show that standard memory-evolving agents suffer declining trust scores over time. 4) Propose TAME, a dual-memory evolutionary framework: executor memory evolves to encode generalizable task-solving strategies for better performance, while evaluator memory evolves to improve assessment of safety and task utility based on past feedback. 5) Implement a closed-loop process including memory filtering, draft generation, trustworthy refinement, execution, and updating of both memories to maintain trust while learning from experience.", "result": "On the Trust-Memevo benchmark, typical agents show an overall decline in trustworthiness during benign task evolution. With TAME, the agent significantly reduces this misevolution effect, achieving simultaneous improvements in measured trustworthiness (safety-related metrics) and task performance compared with baseline memory evolution strategies.", "conclusion": "Test-time memory evolution can systematically erode agent trustworthiness even without adversarial pressure, which is a critical obstacle for safe progress toward AGI. By separating executor and evaluator memories and coupling them in a closed-loop evolutionary process, TAME effectively preserves and even enhances trustworthiness without sacrificing, and in fact improving, task performance. This framework offers a promising direction for safe, adaptive agent design under continual test-time learning."}}
{"id": "2602.03412", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03412", "abs": "https://arxiv.org/abs/2602.03412", "authors": ["Mukai Li", "Qingcheng Zeng", "Tianqing Fang", "Zhenwen Liang", "Linfeng Song", "Qi Liu", "Haitao Mi", "Dong Yu"], "title": "Verified Critical Step Optimization for LLM Agents", "comment": "Working in progress", "summary": "As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.", "AI": {"tldr": "The paper introduces Critical Step Optimization (CSO), a post-training method for large language model agents that focuses learning on a small number of verified critical decision steps instead of full trajectories, yielding large performance gains with limited supervision.", "motivation": "Existing post-training approaches for LLM agents struggle with long-horizon tasks because outcome-only rewards make it hard to assign credit to intermediate steps, step-level rewards are noisy, and Monte Carlo-based estimation is too computationally expensive. The authors are motivated to find a more precise, efficient, and scalable way to improve agent policies by supervising only the most impactful decisions.", "method": "The method, Critical Step Optimization (CSO), identifies critical steps in failed trajectories where alternate actions can flip the final outcome from failure to success. A process reward model first flags candidate critical steps. Expert models then propose alternative actions at those steps, and the policy model continues execution from each alternative to the end of the task. Alternatives that lead to successful outcomes are labeled as verified critical steps and used as preference data for Direct Preference Optimization (DPO) training. This concentrates learning on fine-grained, high-impact decisions while ensuring that supervision remains within the policy\u2019s reachable behaviors.", "result": "On GAIA-Text-103 and XBench-DeepSearch benchmarks, CSO yields 37% and 26% relative improvements over a supervised fine-tuning (SFT) baseline. It also significantly outperforms other post-training methods, despite only requiring supervision on about 16% of trajectory steps, demonstrating improved sample efficiency and effectiveness.", "conclusion": "Selective, verification-based supervision at a small subset of critical decision points can substantially improve long-horizon performance of LLM agents. By starting from failed trajectories, using process reward models to locate candidate steps, and verifying alternative actions through rollouts, CSO achieves fine-grained, reliable credit assignment without heavy computational overhead, offering a more efficient paradigm for agent post-training."}}
{"id": "2602.03238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03238", "abs": "https://arxiv.org/abs/2602.03238", "authors": ["Pengyu Zhu", "Li Sun", "Philip S. Yu", "Sen Su"], "title": "The Necessity of a Unified Framework for LLM-Based Agent Evaluation", "comment": null, "summary": "With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.", "AI": {"tldr": "The paper argues that current evaluations of LLM-based agents are confounded and non-standardized, and proposes a unified framework to standardize agent evaluation.", "motivation": "To address the challenges and unfairness in evaluating general-purpose LLM-based agents, which arise from confounding factors such as prompt design, tools, and non-standardized environments, making it hard to compare models and attribute improvements accurately.", "method": "The authors conceptually analyze existing agent benchmarks, identify the confounding variables (system prompts, tool configurations, environment dynamics, and ad-hoc frameworks), and then propose a unified, standardized evaluation framework for agents. The specific components of the framework are not detailed in the abstract, but it likely includes standard prompts, toolsets, and environmental setups.", "result": "The abstract does not present empirical results; instead, it introduces and motivates a proposal for a standardized evaluation framework for LLM-based agents.", "conclusion": "A unified, standardized evaluation framework is necessary for rigorous, fair, and reproducible assessment of LLM-based agents, and the authors propose such a framework to reduce confounding factors and improve transparency in the field."}}
{"id": "2602.03417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03417", "abs": "https://arxiv.org/abs/2602.03417", "authors": ["Yingli Shen", "Wen Lai", "Jie Zhou", "Xueren Zhang", "Yudong Wang", "Kangyang Luo", "Shuo Wang", "Ge Gao", "Alexander Fraser", "Maosong Sun"], "title": "FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding", "comment": null, "summary": "While LLMs exhibit remarkable fluency, their utility is often compromised by factual hallucinations and a lack of traceable provenance. Existing resources for grounding mitigate this but typically enforce a dichotomy: they offer either structured knowledge without textual context (e.g., knowledge bases) or grounded text with limited scale and linguistic coverage. To bridge this gap, we introduce FactNet, a massive, open-source resource designed to unify 1.7 billion atomic assertions with 3.01 billion auditable evidence pointers derived exclusively from 316 Wikipedia editions. Unlike recent synthetic approaches, FactNet employs a strictly deterministic construction pipeline, ensuring that every evidence unit is recoverable with byte-level precision. Extensive auditing confirms a high grounding precision of 92.1%, even in long-tail languages. Furthermore, we establish FactNet-Bench, a comprehensive evaluation suite for Knowledge Graph Completion, Question Answering, and Fact Checking. FactNet provides the community with a foundational, reproducible resource for training and evaluating trustworthy, verifiable multilingual systems.", "AI": {"tldr": "FactNet is a large, deterministic, multilingual resource that links 1.7B structured facts to 3.01B precise evidence snippets from 316 Wikipedias, plus an evaluation benchmark for trustworthy, grounded LLM systems.", "motivation": "LLMs hallucinate and often lack traceable evidence. Existing grounding resources either provide structure without text (knowledge bases) or grounded text that is small or monolingual. There is a need for a scalable, multilingual, text-grounded factual resource with strong provenance and auditability for training and evaluating reliable systems.", "method": "The authors construct FactNet through a fully deterministic pipeline over 316 Wikipedia editions, extracting 1.7B atomic assertions and associating each with byte-level precise evidence pointers, yielding 3.01B evidence units. They avoid synthetic generation, emphasize exact recoverability of evidence, and perform extensive auditing to measure grounding precision. They also build FactNet-Bench, a benchmark spanning Knowledge Graph Completion, QA, and Fact Checking tasks on this resource.", "result": "FactNet contains 1.7B atomic assertions and 3.01B evidence pointers from 316 Wikipedia languages, with auditing showing 92.1% grounding precision even for long-tail languages. FactNet-Bench is released as a comprehensive evaluation suite across multiple tasks relevant to grounded, factual reasoning.", "conclusion": "FactNet offers a large-scale, open, reproducible, and precisely auditable multilingual resource that tightly links structured facts to textual evidence, along with a benchmark suite. This enables training and evaluation of more trustworthy, verifiable LLMs and knowledge-intensive systems across many languages."}}
{"id": "2602.03249", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03249", "abs": "https://arxiv.org/abs/2602.03249", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Yongxin Wang", "Wenlei Shi", "Yiwei Wang", "Xiaodan Liang", "Jing Tang"], "title": "Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning", "comment": null, "summary": "Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.", "AI": {"tldr": "The paper presents Accordion-Thinking, a framework that lets LLMs dynamically summarize and compress their chain-of-thought so they can reason well with far less test-time compute and memory, nearly matching exhaustive reasoning while being much faster and lighter.", "motivation": "Long chain-of-thought reasoning greatly improves LLM performance on complex tasks, but it is computationally expensive: KV cache grows linearly with sequence length, attention is quadratic in tokens, and GPU memory/latency quickly become bottlenecks. Practically, this caps how much test-time reasoning can be used. The authors want a way for models to keep the benefits of long reasoning while reducing token, memory, and compute overhead, and to do so in a learned, adaptive way instead of via fixed heuristics or truncation.", "method": "They propose Accordion-Thinking, where the model learns to regulate the granularity of its reasoning steps by inserting dynamic summaries of its own thoughts. In a new Fold inference mode, the model periodically summarizes the current reasoning context, writes a compact summary, and then discards earlier detailed tokens, so subsequent computation depends mainly on a compressed history instead of the entire chain-of-thought. They apply reinforcement learning to explicitly reward good use of this behavior\u2014accurate answers with efficient folding\u2014encouraging the model to encode essential reasoning information into these summaries. They compare Fold mode to a baseline Unfold mode that retains the full chain-of-thought, tracking how performance evolves over training.", "result": "During RL training, the performance gap between the compressed Fold mode and the full Unfold mode shrinks and eventually disappears, indicating that the model becomes able to preserve nearly all necessary reasoning information within short summaries. Empirically, Accordion-Thinker achieves around 3x higher throughput at similar accuracy under a 48GB GPU memory constraint, while producing structured step-wise summaries that remain human-readable and interpretable.", "conclusion": "Learned self-compression of chain-of-thought allows LLMs to maintain strong reasoning performance with drastically reduced token and memory requirements. By periodically folding their internal reasoning into compact summaries and discarding detailed history, models can overcome many of the practical limits of scaling test-time compute. Accordion-Thinking thus provides a viable, interpretable path to efficient long reasoning, enabling complex tasks to be solved under tight resource budgets without sacrificing accuracy."}}
{"id": "2602.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03442", "abs": "https://arxiv.org/abs/2602.03442", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Shaohan Wang", "Pengyu Wang", "Xiaorui Wang", "Zhendong Mao"], "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces", "comment": "18 pages, 8 figures", "summary": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.", "AI": {"tldr": "A-RAG is an agentic retrieval-augmented generation framework that lets the LLM itself control hierarchical retrieval tools (keyword search, semantic search, chunk read), outperforming standard RAG while using similar or fewer retrieved tokens.", "motivation": "Existing RAG systems either do one-shot retrieval+concat or follow a fixed workflow, so the model cannot participate in retrieval decisions. This wastes the advanced reasoning and tool-use capabilities of frontier LLMs and limits how well RAG can scale with better models and more test-time compute.", "method": "Expose a hierarchical retrieval interface directly to the LLM, implemented as three tools: keyword search, semantic search, and chunk read. The LLM acts as an agent that iteratively decides which tool to use, at what granularity, and when to stop retrieving, dynamically adapting retrieval strategy to the task. The framework is evaluated on multiple open-domain QA benchmarks with controlled retrieval budgets, and scaling experiments are run across model sizes and test-time compute levels.", "result": "Across multiple open-domain QA benchmarks, A-RAG consistently outperforms standard RAG baselines while using comparable or fewer retrieved tokens. It also shows favorable scaling behavior: as model size and/or test-time compute increase, performance improves, indicating that the agentic interface allows better exploitation of stronger models and more reasoning steps.", "conclusion": "Letting LLMs act as retrieval agents over hierarchical tools yields more efficient and effective RAG: better QA accuracy with similar or lower retrieval cost and improved scaling with model strength and compute. A-RAG demonstrates that exposing flexible retrieval interfaces enables RAG systems to more fully leverage frontier LLM capabilities. The authors plan to release code and an evaluation suite to support further research."}}
{"id": "2602.03255", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03255", "abs": "https://arxiv.org/abs/2602.03255", "authors": ["Tianyu Chen", "Chujia Hu", "Ge Gao", "Dongrui Liu", "Xia Hu", "Wenjie Wang"], "title": "LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios", "comment": null, "summary": "Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.", "AI": {"tldr": "LPS-Bench is a benchmark to evaluate planning-time safety awareness of computer-use agents (CUAs) in long-horizon tasks, especially for MCP-based systems, across benign and adversarial settings.", "motivation": "Existing CUA benchmarks mainly test short-horizon or GUI-focused tasks and focus on execution-time errors, failing to assess whether agents can foresee and avoid safety risks during planning, particularly under adversarial or ambiguous instructions in real computer environments.", "method": "They build LPS-Bench, a benchmark of 65 scenarios spanning 7 domains and 9 risk types, targeting long-horizon tasks for MCP-based CUAs. They design a multi-agent automated pipeline for scalable scenario and data generation, and use an LLM-as-a-judge evaluation setup that inspects agents' planning trajectories to score safety awareness. Both benign and adversarial user interactions are included.", "result": "Experiments running existing CUAs on LPS-Bench show that these systems frequently fail to maintain safe behavior when planning over long horizons, exhibiting poor anticipation and mitigation of potential risks across multiple domains and risk types.", "conclusion": "LPS-Bench exposes significant safety weaknesses in current MCP-based CUAs with respect to long-horizon planning. The authors suggest and study mitigation strategies aimed at improving planning-time safety and release their benchmark and code to foster further research on safe computer-use agents."}}
{"id": "2602.03484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03484", "abs": "https://arxiv.org/abs/2602.03484", "authors": ["Jenny Kunz"], "title": "Preferences for Idiomatic Language are Acquired Slowly -- and Forgotten Quickly: A Case Study on Swedish", "comment": "Accepted to TACL. Note that the arXiv version is a pre-MIT Press publication version", "summary": "In this study, we investigate how language models develop preferences for \\textit{idiomatic} as compared to \\textit{linguistically acceptable} Swedish, both during pretraining and when adapting a model from English to Swedish. To do so, we train models on Swedish from scratch and by fine-tuning English-pretrained models, probing their preferences at various checkpoints using minimal pairs that differ in linguistic acceptability or idiomaticity. For linguistic acceptability, we adapt existing benchmarks into a minimal-pair format. To assess idiomaticity, we introduce two novel datasets: one contrasting conventionalized idioms with plausible variants, and another contrasting idiomatic Swedish with Translationese. Our findings suggest that idiomatic competence emerges more slowly than other linguistic abilities, including grammatical and lexical correctness. While longer training yields diminishing returns for most tasks, idiom-related performance continues to improve, particularly in the largest model tested (8B). However, instruction tuning on data machine-translated from English -- the common approach for languages with little or no native instruction data -- causes models to rapidly lose their preference for idiomatic language.", "AI": {"tldr": "The paper studies how Swedish language models acquire preferences for idiomatic vs merely acceptable language, both in pretraining and English-to-Swedish adaptation, showing idiomatic competence emerges later and is fragile under MT-based instruction tuning.", "motivation": "To understand whether and how language models learn to prefer truly idiomatic Swedish over just grammatically acceptable or literal/translationese Swedish, especially in low-resource settings where models are often adapted from English using machine-translated data.", "method": "Train Swedish models from scratch and by fine-tuning English-pretrained models; periodically probe their preferences using minimal pairs contrasting (1) linguistically acceptable vs unacceptable sentences and (2) idiomatic vs non-idiomatic alternatives. Adapt existing acceptability benchmarks into minimal pairs and create two new idiom datasets: conventionalized idioms vs plausible variants, and idiomatic Swedish vs translationese. Analyze performance over training time and model size, and after instruction tuning on MT data.", "result": "Idiomatic competence develops more slowly than other linguistic abilities (e.g., grammar and lexical correctness). While performance on most tasks plateaus with longer training, idiom-related performance continues to improve, especially for the 8B model. Instruction tuning on machine-translated English data quickly erodes the model\u2019s preference for idiomatic Swedish, even though other abilities remain.", "conclusion": "Language models can eventually develop a preference for idiomatic Swedish, but this ability is slower to emerge and particularly vulnerable to standard adaptation practices that rely on machine-translated instruction data. For high-quality non-English models, training regimes must explicitly account for idiomaticity and avoid over-reliance on translationese for instruction tuning."}}
{"id": "2602.03263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03263", "abs": "https://arxiv.org/abs/2602.03263", "authors": ["Yuxuan Liu", "Yuntian Shi", "Kun Wang", "Haoting Shen", "Kun Yang"], "title": "CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs", "comment": "25 pages, 1 figures", "summary": "Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.", "AI": {"tldr": "This paper introduces CSR-Bench, a benchmark to test whether multimodal large language models truly integrate image and text for safe behavior, or rely on shortcuts, revealing systematic safety and reliability gaps.", "motivation": "Existing multimodal LLMs can process text and images, but their safety behavior may depend on just one modality (often text) instead of genuinely understanding cross-modal intent. Current evaluations do not sufficiently stress-test this joint understanding, particularly regarding safety, over-rejection, bias, and hallucination. A dedicated benchmark is needed to diagnose these cross-modal reliability issues and modality-induced behavior shifts.", "method": "The authors design CSR-Bench, a stress-test benchmark with four key interaction patterns: Safety, Over-rejection, Bias, and Hallucination, further divided into 61 fine-grained types. Each benchmark instance is constructed so that correct and safe performance requires integrated interpretation of both image and text. They also create paired text-only versions of each item to serve as controls, enabling comparison of model behavior with and without visual input. They then systematically evaluate 16 state-of-the-art multimodal LLMs on this benchmark.", "result": "Across the 16 evaluated MLLMs, the authors find systematic cross-modal alignment gaps. Models often show weak safety awareness when required to integrate modalities, tend to be dominated by language cues under conflicting or interfering signals, and generally perform worse on multimodal inputs compared to the corresponding text-only controls. The benchmark reveals that model behavior changes significantly when images are introduced, often in undesired ways.", "conclusion": "CSR-Bench exposes that many apparent safety properties of multimodal LLMs are fragile and can be undermined when images are involved. There is a notable trade-off: attempts to reduce over-rejection (i.e., excessive refusals) can compromise safe and non-discriminatory behavior, implying that some existing safety gains stem from simplistic refusal heuristics rather than robust cross-modal intent understanding. The benchmark thus provides a diagnostic tool for developing more reliably aligned multimodal models."}}
{"id": "2602.03485", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03485", "abs": "https://arxiv.org/abs/2602.03485", "authors": ["Quanyu Long", "Kai Jie Jiang", "Jianda Chen", "Xu Guo", "Leilei Gan", "Wenya Wang"], "title": "Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning", "comment": "19 pages, 8 figures", "summary": "Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.", "AI": {"tldr": "The paper observes that large reasoning models frequently perform self-verification steps that rarely change outcomes, and introduces an experience-driven framework to detect and skip unnecessary rechecks, cutting tokens by ~20% without hurting accuracy.", "motivation": "Long reflective reasoning traces in LRMs are costly. Empirically, many self-verification (recheck) steps simply confirm intermediate results and almost never catch errors or alter final answers. This overuse wastes computation and tokens, revealing a gap between when verification is triggered and when it is actually useful. The authors want to understand this behavior and then exploit it to make test-time reasoning more efficient.", "method": "(1) Perform large-scale empirical analysis on various LRMs and benchmarks to categorize reflective steps, focusing on self-verification (recheck) behavior and whether it is confirmatory vs. corrective. (2) Propose an experience-driven test-time framework: monitor when the model is about to perform a recheck; query an offline experience pool that stores past verification outcomes; retrieve similar historical cases efficiently; estimate whether rechecking is likely unnecessary; if so, issue a suppression signal so the model skips the recheck and continues reasoning directly.", "result": "The study finds that recheck steps are highly frequent across models and tasks but are overwhelmingly confirmatory, rarely discovering errors or changing reasoning outcomes. Applying the proposed framework across multiple LRMs and benchmarks reduces token usage by up to 20.3% while preserving accuracy; on some datasets, accuracy even improves, suggesting that skipping redundant rechecks can also mitigate overthinking or confusion.", "conclusion": "Self-verification in LRMs is currently overused: most rechecks redundantly confirm prior reasoning without benefit. By leveraging past experience about when verification actually mattered, the proposed framework can detect and suppress likely-unnecessary rechecks at test time. This yields substantial token savings with no accuracy loss and potential gains, indicating that more adaptive, experience-aware control of reasoning behaviors is a promising direction for efficient large reasoning systems."}}
{"id": "2602.03279", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03279", "abs": "https://arxiv.org/abs/2602.03279", "authors": ["Zhengbo Jiao", "Shaobo Wang", "Zifan Zhang", "Xuan Ren", "Wei Wang", "Bing Zhao", "Hu Wei", "Linfeng Zhang"], "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis", "comment": "23page4", "summary": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.", "AI": {"tldr": "They introduce Agentic Proposing, a framework where an agent composes modular reasoning skills to synthesize complex, verifiable problems and trajectories, yielding small but very high-quality datasets that significantly boost LLM reasoning performance.", "motivation": "High-quality, verifiable datasets for complex reasoning are essential but human annotation is expensive and hard to scale. Existing synthetic data methods either keep problems simple to ensure validity or increase difficulty at the cost of structural consistency and solvability. There is a need for an automated, scalable way to synthesize complex yet verifiable reasoning data across domains.", "method": "Model problem synthesis as a goal-driven sequential decision process controlled by a specialized agent that dynamically selects and composes modular reasoning skills. Implement an iterative workflow involving the agent\u2019s internal reflection and external tool use. Train an approximately 4B-parameter agent, Agentic-Proposer-4B, using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories for tasks in mathematics, coding, and science.", "result": "Empirical experiments show that solvers trained purely on this agent-synthesized data outperform strong baselines and generalize well across domains. A 30B-parameter solver trained on only 11,000 synthesized trajectories reaches 91.6% accuracy on AIME25, comparable to frontier proprietary models like GPT-5, indicating that the synthetic data is extremely effective despite its relatively small size.", "conclusion": "An agentic, sequential-decision approach to problem synthesis can reliably create complex, verifiable training trajectories that substantially improve LLM reasoning. A modest volume of carefully generated synthetic data can rival or replace much larger human-curated datasets, advancing scalable training for complex reasoning without prohibitive annotation costs."}}
{"id": "2602.03507", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03507", "abs": "https://arxiv.org/abs/2602.03507", "authors": ["Runquan Gui", "Yafu Li", "Xiaoye Qu", "Ziyan Liu", "Yeqiu Cheng", "Yu Cheng"], "title": "Learning to Reason Faithfully through Step-Level Faithfulness Maximization", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.", "AI": {"tldr": "FaithRL is a reinforcement learning framework for large language models that directly optimizes reasoning faithfulness to reduce hallucinations.", "motivation": "Existing RL with Verifiable Rewards (RLVR) pipelines primarily use sparse, outcome-based rewards that only judge final answers, offering limited supervision of intermediate reasoning steps. This can incentivize over-confident, spurious chains of thought, leading to hallucinations. The paper aims to provide a framework that improves step-wise reasoning faithfulness and reduces hallucinations while keeping or improving task performance.", "method": "The authors propose FaithRL, a reinforcement learning framework that formalizes a faithfulness-maximization objective. They design a geometric reward function and a faithfulness-aware advantage modulation mechanism that assigns fine-grained, step-level credit: it penalizes unsupported reasoning steps while preserving and rewarding valid partial derivations. This provides richer supervision than sparse outcome-based rewards and directly shapes the model\u2019s intermediate reasoning behavior.", "result": "Across multiple model backbones and evaluation benchmarks, FaithRL consistently lowers hallucination rates and often improves or at least maintains final answer correctness. Empirical analyses show improved step-wise reasoning faithfulness and robust generalization of the learned behavior.", "conclusion": "Optimizing for reasoning faithfulness via FaithRL effectively mitigates over-confidence and hallucinations in multi-step reasoning tasks for LLMs, without sacrificing\u2014and sometimes improving\u2014answer accuracy. The framework\u2019s geometric reward and faithfulness-aware credit assignment provide a general, robust way to enforce faithful intermediate reasoning in RL for LLMs."}}
{"id": "2602.03285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03285", "abs": "https://arxiv.org/abs/2602.03285", "authors": ["Yuelin Hu", "Jun Xu", "Bingcong Lu", "Zhengxue Cheng", "Hongwei Hu", "Ronghua Wu", "Li Song"], "title": "MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings", "comment": "accepted by AAAI2026 ws", "summary": "Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.\n  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.\n  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.", "AI": {"tldr": "The paper introduces a realistic enterprise meeting benchmark and an AI agent that efficiently routes queries between fast and slow reasoning paths and tools, achieving better quality-latency tradeoffs than existing systems.", "motivation": "Enterprise meetings produce complex, long-context, and multi-stakeholder interactions where current QA-focused benchmarks and systems do not capture real-world workflows, especially under latency, cost, and privacy constraints.", "method": "The authors (1) construct MeetAll, a bilingual and multimodal corpus of 231 enterprise meetings with questions injected using an enterprise-grounded protocol along four dimensions; (2) design MeetBench XL, a multi-dimensional evaluation protocol aligned with human judgments; and (3) build MeetMaster XL, a dual-policy agent that learns to route queries between fast and slow reasoning models and to invoke tools such as retrieval, cross-meeting aggregation, and web search via a lightweight classifier.", "result": "MeetMaster XL achieves superior quality-latency tradeoffs compared to single-model commercial systems and baselines, with evidence from ablation studies, robustness tests, and a real-world deployment case study.", "conclusion": "Grounded, enterprise-aware benchmarks like MeetAll and MeetBench XL, combined with learned dual-policy agents such as MeetMaster XL, can better support realistic enterprise meeting workflows, delivering higher quality responses while meeting latency and efficiency requirements."}}
{"id": "2602.03542", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03542", "abs": "https://arxiv.org/abs/2602.03542", "authors": ["Fangru Lin", "Valentin Hofmann", "Xingchen Wan", "Weixing Wang", "Zifeng Ding", "Anthony G. Cohn", "Janet B. Pierrehumbert"], "title": "Can Large Language Models Generalize Procedures Across Representations?", "comment": null, "summary": "Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.", "AI": {"tldr": "Study of how LLMs generalize between symbolic (code/graphs) and natural language tasks, proposing a two-stage curriculum (symbolic then natural language) to improve cross-representation generalization.", "motivation": "LLMs are heavily trained on symbolic data like code and graphs, but real user tasks are given in natural language. It's unclear how well models trained on symbolic representations can generalize to equivalent tasks expressed in natural language, and existing post-training strategies may be inefficient or fail to transfer. The paper aims to characterize and improve this cross-representation generalization.", "method": "Design isomorphic tasks that can be represented as code, graphs, and natural language (e.g., planning/scheduling problems). Empirically test popular post-training regimes where LLMs are trained only on graphs, only on code, or only on natural language. Then introduce and evaluate a two-stage data curriculum that first trains models on symbolic data (code/graphs) and then on natural language instances of the same underlying tasks. Compare performance across models, tasks, and training curricula, and analyze learned behaviors in terms of generative analogy.", "result": "Training solely on graphs or code does not reliably transfer to natural language performance, and purely natural language training yields limited and inefficient gains. The proposed two-stage curriculum significantly improves performance on natural language tasks and cross-representation generalization. A relatively small 1.5B-parameter Qwen model trained with this curriculum nearly matches zero-shot GPT-4o on naturalistic planning benchmarks. Analysis indicates that the curriculum fosters generative analogy capabilities that support cross-representation transfer.", "conclusion": "Cross-representation generalization from symbolic formats to natural language is not automatic for LLMs under standard training, and naive natural language finetuning is inefficient. A structured curriculum that starts with symbolic representations and then transitions to natural language yields substantial gains, enabling small models to approach frontier model performance on planning tasks. This behavior can be understood as encouraging generative analogy, suggesting curriculum design as a key tool for building LLMs that robustly transfer skills across representational formats."}}
{"id": "2602.03286", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03286", "abs": "https://arxiv.org/abs/2602.03286", "authors": ["Michael A. M\u00fcller", "Srdjan Vesic", "Bruno Yun"], "title": "Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation", "comment": "Accepted to AAMAS 2026", "summary": "This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.", "AI": {"tldr": "The paper introduces structured bipolar argumentation frameworks and new semantics that allow agents to rationally withhold acceptance of some defensible arguments and reason directly about accepted sentences, not just arguments.", "motivation": "Existing computational argumentation mainly assumes that agents must accept all arguments they can defend (completeness-based semantics) and focuses on sets of accepted arguments, ignoring that agents may rationally remain agnostic and that practical reasoning often concerns which individual claims/sentences are accepted. The paper aims to better match philosophical and linguistic insights about real debates, where doubt and sentence-level acceptance matter.", "method": "The authors define structured bipolar argumentation frameworks (SBAFs), in which arguments are composed of sentences and related by both attack and support relations. They then design new semantics for SBAFs that (a) do not require agents to accept all defended arguments and (b) provide both argument extensions (acceptable sets of arguments) and language extensions (acceptable sets of sentences). They analyze the position of these semantics relative to standard admissible and complete semantics in abstract argumentation and study conditions under which support can be ignored or under which existing frameworks emerge as special cases.", "result": "The paper formally characterizes SBAFs and introduces semantics that yield reasonable debate positions where agents can reject some defensible arguments due to doubt and explicitly reason about which sentences they accept. It shows that these semantics sit strictly between admissible and complete semantics. Moreover, it proves that under certain conditions support relations can be safely ignored, justifying use of standard abstract argumentation, and that deductive support semantics is captured as a special case of the proposed framework.", "conclusion": "By enriching argumentation frameworks with structure, bipolar (attack and support) relations, and more flexible semantics, the paper offers a more realistic and philosophically informed model of how agents argue, doubt, and accept claims. This not only bridges gaps between abstract argumentation and linguistic/philosophical views, but also unifies and situates existing approaches like deductive support within a general, more expressive framework."}}
{"id": "2602.03548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03548", "abs": "https://arxiv.org/abs/2602.03548", "authors": ["Yuqin Dai", "Ning Gao", "Wei Zhang", "Jie Wang", "Zichen Luo", "Jinpeng Wang", "Yujie Wang", "Ruiyuan Wu", "Chaozheng Wang"], "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.", "AI": {"tldr": "The paper proposes SEAD, a self-evolving agent framework that improves service dialogue performance by learning strategies without large-scale human annotations, achieving notable gains in task completion and efficiency.", "motivation": "Existing large language models perform well in open-domain dialogue but poorly in service-oriented dialogue because available human conversation data are noisy, scarce, and fail to capture realistic, goal-oriented user behaviors. There is a need for a way to generate high-quality, authentic training environments for service dialogue without depending on large human-labeled datasets.", "method": "SEAD introduces a self-evolving agent framework that learns service dialogue strategies via interaction in a simulated environment instead of relying on static human logs. It decouples user modeling into two parts: (1) a Profile Controller that generates diverse user profiles and states to control curriculum and scenario difficulty, and (2) a User Role-play Model dedicated to realistic, goal-driven user behavior. This separation ensures the agent is trained in adaptive yet fair environments that mimic real service interactions.", "result": "Experimental evaluation shows that agents trained with SEAD outperform both open-source foundation models and closed-source commercial models on service dialogue benchmarks, with a 17.6% improvement in task completion rates and an 11.1% improvement in dialogue efficiency.", "conclusion": "SEAD effectively addresses data scarcity and realism issues in service dialogue training by providing an adaptive, curriculum-controlled user simulation framework. Its decoupled user modeling design leads to better strategy learning and significantly improved performance over existing baselines, demonstrating that high-quality service dialogue systems can be trained without large-scale human annotations."}}
{"id": "2602.03315", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03315", "abs": "https://arxiv.org/abs/2602.03315", "authors": ["Menglin Xia", "Xuchao Zhang", "Shantanu Dixit", "Paramaguru Harimurugan", "Rujia Wang", "Victor Ruhle", "Robert Sim", "Chetan Bansal", "Saravan Rajmohan"], "title": "Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity", "comment": null, "summary": "Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.", "AI": {"tldr": "Memora is a new memory representation for agents that balances abstraction and detail, enabling better scalable, context-aware retrieval and reasoning than prior RAG and KG systems.", "motivation": "Existing agent memory systems struggle to scale: abstracting memories makes them manageable but loses specific details needed for precise reasoning, while keeping all details leads to inefficient retrieval and poor context use. The authors aim to design a memory structure that scales with growing information while still allowing fine-grained, context-aware access for downstream tasks.", "method": "They propose Memora, a 'harmonic' memory representation composed of (1) primary abstractions that index concrete memory values and merge related updates into unified entries, and (2) cue anchors that provide multiple, diverse access paths into the same memories and link related memories. On top of this structure, they design a retrieval policy that leverages these connections to go beyond simple semantic similarity during retrieval. They provide a theoretical analysis showing that common RAG and KG-based memories can be modeled as special cases of their framework, and empirically evaluate Memora on long-context memory benchmarks.", "result": "Memora achieves state-of-the-art performance on the LoCoMo and LongMemEval benchmarks, with improved retrieval relevance and reasoning performance, especially as the size of the memory grows.", "conclusion": "A structured, harmonic memory representation that explicitly balances abstraction with specificity and that uses connection-aware retrieval policies can scale agent memory more effectively than traditional RAG and KG systems, leading to better long-term retrieval and reasoning on challenging benchmarks."}}
{"id": "2602.03551", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03551", "abs": "https://arxiv.org/abs/2602.03551", "authors": ["Vitalii Hirak", "Jaap Jumelet", "Arianna Bisazza"], "title": "Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models", "comment": "19 pages, 11 figures, EACL 2026", "summary": "Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.", "AI": {"tldr": "The paper analyzes how typological properties of target languages affect translation quality in large multilingual MT models, beyond data size and script.", "motivation": "To understand why translation quality varies widely across languages in multilingual models and whether linguistic typology, not just data availability, systematically affects performance.", "method": "They empirically study two large pre-trained multilingual MT systems (NLLB-200 encoder-decoder and Tower+ decoder-only) over many languages from FLORES+, correlate translation quality with fine-grained typological features while controlling for resource level and script, and analyze how typology interacts with decoding/search strategies.", "result": "They find that target-language typological features significantly explain translation performance differences in both models even when controlling for data and script, and that languages with specific typologies gain more from wider search in decoding, indicating model difficulty is typology-dependent.", "conclusion": "Typology is a key driver of translation quality in state-of-the-art multilingual MT, not just data or script, and some language types may need tailored decoding strategies; they also release a typology resource for 212 FLORES+ languages to support further work."}}
{"id": "2602.03340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03340", "abs": "https://arxiv.org/abs/2602.03340", "authors": ["Xiao Sun", "Yuming Yang", "Junnan Zhu", "Jiang Zhong", "Xinyu Zhou", "Kaiwen Wei"], "title": "MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis", "comment": "36 pages, 27 figures", "summary": "Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \\textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \\textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \\textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.", "AI": {"tldr": "The paper introduces MentalDx Bench, a realistic benchmark for disorder-level psychiatric diagnosis and proposes MentalSeek-Dx, a 14B-parameter medical LLM that improves fine-grained diagnostic performance by modeling clinical reasoning.", "motivation": "Existing LLM psychiatric benchmarks lack ecological validity and fine-grained, disorder-level supervision, so models that appear strong on coarse tasks fail in realistic diagnostic settings. The paper aims to provide a clinically grounded benchmark and an LLM that better aligns with real psychiatric reasoning.", "method": "They construct MentalDx Bench from 712 de-identified electronic health records, annotated by psychiatrists according to ICD-11, spanning 76 disorders in 16 categories. They evaluate 18 LLMs to reveal a mismatch between category-level and disorder-level performance. Then they develop MentalSeek-Dx, a specialized 14B medical LLM trained via supervised reasoning-trajectory construction and curriculum-based reinforcement learning to internalize hypothetico-deductive clinical reasoning for diagnosis.", "result": "Across experiments on MentalDx Bench, MentalSeek-Dx sets state-of-the-art performance on disorder-level psychiatric diagnosis despite having only 14B parameters, outperforming larger general LLMs on this clinically realistic task.", "conclusion": "Disorder-level diagnosis in psychiatry exposes a paradigm misalignment for current LLMs: pattern matching is insufficient without explicit modeling of clinical reasoning. MentalDx Bench provides a realistic evaluation setting, and MentalSeek-Dx shows that embedding hypothetico-deductive reasoning and curriculum RL enables more reliable, clinically grounded psychiatric diagnosis."}}
{"id": "2602.03560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03560", "abs": "https://arxiv.org/abs/2602.03560", "authors": ["Yizhao Gao", "Jianyu Wei", "Qihao Zhang", "Yu Cheng", "Shimao Chen", "Zhengju Tang", "Zihan Jiang", "Yifan Song", "Hailin Zhang", "Liang Zhao", "Bo Yang", "Gang Wang", "Shijie Cao", "Fuli Luo"], "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing", "comment": "17 pages, 2 figures", "summary": "This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.", "AI": {"tldr": "HySparse is a hybrid attention architecture that interleaves full and sparse attention layers, using full attention as an oracle to guide sparse token selection and KV cache reuse, achieving better performance and up to 10x KV cache reduction.", "motivation": "Transformers with full attention are computationally and memory expensive, especially due to quadratic complexity and large KV caches. Existing sparse attention methods try to reduce cost but often require extra mechanisms to estimate token importance and typically do not save KV cache memory effectively. There is a need for a simpler, more effective way to introduce sparsity that improves both speed and memory without hurting\u2014and ideally improving\u2014model performance.", "method": "The authors propose Hybrid Sparse Attention (HySparse), where a small number of full attention layers are interleaved with multiple sparse attention layers. Each sparse layer derives its token subset and KV cache directly from the immediately preceding full attention layer, which serves as an oracle of token importance. This design removes the need for separate importance predictors and allows sparse layers to reuse the full attention KV cache, reducing both computation and memory footprint. The architecture is tested on 7B dense and 80B MoE transformer models, with configurations that drastically reduce the number of full attention layers.", "result": "On both 7B dense and 80B mixture-of-experts models, HySparse outperforms full attention baselines and hybrid sliding-window attention (SWA) baselines across evaluated tasks. In a 49-layer 80B MoE model where only 5 layers use full attention, HySparse still delivers substantial performance gains while achieving nearly 10x reduction in KV cache storage, illustrating that most layers can be sparse if guided by a few full-attention oracle layers.", "conclusion": "HySparse demonstrates that interleaving a few full attention 'oracle' layers with multiple derived sparse layers can simultaneously improve model performance and significantly reduce KV cache memory and computation. By directly leveraging full attention outputs for token selection and cache reuse, it overcomes key limitations of previous sparse attention approaches, showing that highly sparse, cache-efficient transformer architectures are feasible and effective at scale."}}
{"id": "2602.03351", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03351", "abs": "https://arxiv.org/abs/2602.03351", "authors": ["Mayank Goel", "Aritra Das", "Paras Chopra"], "title": "Building Interpretable Models for Moral Decision-Making", "comment": "8 pages, 4 figures, accepted to AAAI'26 Machine Ethics Workshop", "summary": "We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.", "AI": {"tldr": "They design a small, interpretable transformer to model human-like moral choices in trolley problems and analyze how moral biases are represented inside the network.", "motivation": "Understand how neural networks make moral decisions in structured ethical dilemmas and make their internal reasoning process transparent and analyzable.", "method": "Build a custom 2-layer transformer with embeddings for features like who is affected, how many people, and outcome membership; train it on Moral Machine trolley-style dilemmas; apply interpretability techniques to trace where and how moral biases and decision features are represented in the network.", "result": "The 2-layer transformer reaches 77% accuracy on the Moral Machine dataset and analysis shows that moral reasoning signals and biases are localized in specific computational stages of the model.", "conclusion": "A compact, structured transformer can approximate human moral decisions while remaining transparent enough to map how different aspects of moral reasoning and bias are implemented internally across layers and components."}}
{"id": "2602.03563", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03563", "abs": "https://arxiv.org/abs/2602.03563", "authors": ["Wei Zhu"], "title": "ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning", "comment": null, "summary": "Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \\underline{A}ligned \\underline{C}ontrastive \\underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.", "AI": {"tldr": "The paper proposes Aligned Contrastive Learning (ACL) to make contrastive objectives work harmoniously with cross-entropy in supervised fine-tuning, especially for (multi-exit) BERT, yielding better or comparable GLUE performance and improved quality-speed tradeoffs.", "motivation": "Although contrastive learning has been widely successful in self-supervised settings, its behavior and benefits in fully supervised scenarios remain underexplored. Initial experiments show that standard supervised contrastive losses can conflict with the conventional cross-entropy (CE) objective, which prevents straightforward gains from contrastive learning when labels are available. Moreover, there is a need to improve intermediate (early-exit) performance for multi-exit BERT models to enable low-latency applications without sacrificing accuracy.", "method": "1) ACL-Embed: Introduce label embeddings as additional augmented samples with their own labels, and apply a contrastive loss that explicitly aligns each sample\u2019s representation with its corresponding label embedding. 2) ACL-Grad: Jointly optimize CE and ACL-Embed but dynamically drop the ACL-Embed gradient when its update direction conflicts with the CE objective, thereby reducing destructive interference between the two losses. 3) ACL-CL (cross-layer ACL): For multi-exit BERT, use deeper (teacher) exits to guide shallower (student) exits through a cross-layer ACL objective, aligning their representations to enhance intermediate-exit performance.", "result": "On the GLUE benchmark, (a) standard BERT fine-tuned with ACL (ACL-BRT) achieves accuracy that is better than or on par with vanilla cross-entropy and with cross-entropy plus supervised contrastive learning (CE+SCL); (b) for multi-exit BERT, ACL\u2014especially the cross-layer variant ACL-CL\u2014substantially improves the accuracy of intermediate exits compared to baselines, thereby enabling more favorable tradeoffs between inference speed (early exiting) and task performance.", "conclusion": "Aligning contrastive learning with cross-entropy via label embeddings, conflict-aware gradient handling, and cross-layer guidance makes contrastive objectives effective in supervised and multi-exit transformer fine-tuning. The proposed ACL framework mitigates objective conflicts, improves GLUE task performance, and substantially boosts the usefulness of early exits in multi-exit BERT, making it attractive for low-latency NLP applications."}}
{"id": "2602.03358", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03358", "abs": "https://arxiv.org/abs/2602.03358", "authors": ["Junmo Cho", "Suhan Kim", "Sangjune An", "Minsu Kim", "Dong Bok Lee", "Heejun Lee", "Sung Ju Hwang", "Hae Beom Lee"], "title": "GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer", "comment": null, "summary": "Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.", "AI": {"tldr": "The paper introduces GFlowPO, a sample-efficient framework for optimizing prompts for language models by casting prompt search as posterior inference and leveraging off-policy GFlowNets plus a dynamic meta-prompt update mechanism.", "motivation": "Prompt optimization for LMs is hard because the search space is huge, LM evaluations are expensive, and existing RL-based optimizers are sample-inefficient due to on-policy updates and fixed meta-prompts. The paper aims to make prompt search more efficient and effective.", "method": "They formulate prompt optimization as posterior inference over latent prompts with a prior given by a meta-prompted reference LM. First, they fine-tune a lightweight prompt-LM using an off-policy Generative Flow Network objective with replay, which reuses past evaluated prompts. Second, they propose Dynamic Memory Update (DMU), a training-free procedure that continually updates the meta-prompt using both diverse prompts from a replay buffer and top-performing prompts from a priority queue to focus search on high-reward areas.", "result": "On few-shot text classification, instruction induction, and question answering benchmarks, the proposed GFlowPO method achieves better performance than recent discrete prompt optimization baselines, demonstrating higher sample efficiency and stronger final performance.", "conclusion": "Casting prompt optimization as posterior inference and using off-policy GFlowNets plus dynamic meta-prompt updates yields a more sample-efficient and effective prompt search procedure that consistently outperforms existing discrete prompt optimization methods across several LM tasks."}}
{"id": "2602.03578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03578", "abs": "https://arxiv.org/abs/2602.03578", "authors": ["Su Dong", "Qinggang Zhang", "Yilin Xiao", "Shengyuan Chen", "Chuang Zhou", "Xiao Huang"], "title": "Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs", "comment": null, "summary": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.", "AI": {"tldr": "EA-GraphRAG adaptively switches between standard RAG and GraphRAG based on query complexity to gain accuracy and efficiency simultaneously.", "motivation": "LLMs with RAG struggle when domain documents are unstructured and fragmented; GraphRAG helps using knowledge graphs but is slow and can even underperform vanilla RAG because it is rigidly applied to all queries, even simple ones. There is a need for a system that uses graph reasoning only when it is actually beneficial.", "method": "They design an adaptive framework (EA-GraphRAG) with three key components: (i) a syntactic feature constructor that parses a user query and extracts structural features indicative of reasoning difficulty; (ii) a lightweight complexity scorer that maps these features into a continuous complexity score; and (iii) a routing policy that uses this score to choose plain dense RAG for low-complexity queries, GraphRAG for high-complexity queries, and a complexity-aware reciprocal rank fusion strategy for borderline cases that mixes results from both retrieval modes.", "result": "On a benchmark combining two single-hop (simple) and two multi-hop (complex) QA datasets, EA-GraphRAG yields higher accuracy than both vanilla RAG and always-on GraphRAG, while also reducing response latency, and achieves state-of-the-art performance on mixed-query scenarios.", "conclusion": "Adaptive use of graph-augmented retrieval, guided by query complexity estimation, overcomes the practical drawbacks of always using GraphRAG. EA-GraphRAG delivers better accuracy and efficiency across heterogeneous query sets, showing that dynamic routing between RAG and GraphRAG is more effective than a one-size-fits-all approach."}}
{"id": "2602.03402", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03402", "abs": "https://arxiv.org/abs/2602.03402", "authors": ["Mengxuan Wang", "Yuxin Chen", "Gang Xu", "Tao He", "Hongjie Jiang", "Ming Li"], "title": "Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility", "comment": null, "summary": "Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.", "AI": {"tldr": "The paper introduces Risk Awareness Injection (RAI), a training-free method to enhance safety of vision-language models against multimodal jailbreaks by amplifying unsafe-content signals in their feature space, preserving utility while improving risk detection.", "motivation": "Vision-language models, though powerful for cross-modal reasoning, are vulnerable to multimodal jailbreak attacks. Existing defenses rely on costly safety fine-tuning or harsh token filtering that hurts performance. Prior work indicates that base language models can internally recognize unsafe text, but adding vision often weakens these risk signals. The paper aims to restore and leverage that latent risk awareness in VLMs without retraining or sacrificing utility.", "method": "The authors propose Risk Awareness Injection (RAI), which is a training-free safety calibration framework. It first constructs an Unsafe Prototype Subspace using language embeddings derived from unsafe textual content. Then, when processing inputs, it identifies and selectively targets high-risk visual tokens whose embeddings project strongly into this subspace. RAI modulates these tokens in the shared cross-modal feature space to explicitly activate safety-critical signals, enhancing the model\u0019s internal detection of unsafe content while keeping the semantics necessary for normal vision-language reasoning largely intact.", "result": "Across several multimodal jailbreak benchmarks and standard utility tasks, RAI significantly reduces attack success rates for vision-language models while maintaining or minimally affecting task performance. The empirical results indicate that RAI is effective, lightweight, and compatible with existing VLMs without additional training.", "conclusion": "Risk Awareness Injection provides a practical, plug-and-play safety layer for vision-language models that recovers the strong risk recognition capabilities seen in base LLMs. By constructing an unsafe prototype subspace and modulating high-risk visual tokens, RAI strengthens multimodal safety against jailbreaks without the training costs or utility degradation associated with existing defenses."}}
{"id": "2602.03584", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03584", "abs": "https://arxiv.org/abs/2602.03584", "authors": ["Yi-Kai Zhang", "Zhiyuan Yao", "Hongyan Hao", "Yueqing Sun", "Qi Gu", "Hui Su", "Xunliang Cai", "De-Chuan Zhan", "Han-Jia Ye"], "title": "$V_0$: A Generalist Value Model for Any Policy at State Zero", "comment": null, "summary": "Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.", "AI": {"tldr": "They propose V0, a generalist value model that predicts how well any LLM will perform on a prompt without further training, using past instruction-performance history as context. It replaces heavy critics in PPO-style training and stabilizes GRPO by guiding sampling and routing for better cost-performance tradeoffs.", "motivation": "Actor-Critic RL for LLMs needs a value model (critic) as large and expensive as the policy, and this critic must be constantly updated as the policy changes. GRPO removes the critic but then needs many rollouts to estimate baselines reliably. There is a need for a cheap, reusable way to estimate expected performance that tracks changing model capabilities without continuous retraining.", "method": "Introduce V0, a generalist value model trained to estimate the expected success rate of arbitrary LLMs on prompts. Instead of encoding policy capability in parameters, V0 takes as input a dynamic context formed from a history of instruction-performance pairs that profile the current policy. It focuses on value estimation at the initial state (the prompt) and outputs predictions used both for training-time scheduling and deployment-time routing.", "result": "V0 enables more efficient sampling in GRPO: it predicts which rollouts are likely to succeed before generation, allowing targeted budget allocation. Experiments show V0 clearly beats heuristic budget allocation baselines and, when used as a router across multiple LLMs, achieves Pareto-optimal performance vs. cost compared to alternative routing methods.", "conclusion": "Encoding model capability as contextual input instead of through constant critic retraining allows a single generalist value model, V0, to generalize across policies and prompts. This removes the need for a large synchronized critic in PPO-style training, stabilizes GRPO with less sampling, and provides an effective, low-cost mechanism for both training-time resource scheduling and deployment-time model routing."}}
{"id": "2602.03403", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03403", "abs": "https://arxiv.org/abs/2602.03403", "authors": ["Guangming Lang", "Mingchuan Shang", "Mengjun Hu", "Jie Zhou", "Feng Xu"], "title": "Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations", "comment": null, "summary": "In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.", "AI": {"tldr": "The paper enhances three-way conflict analysis by introducing intuitionistic fuzzy preference-based models for finer-grained representation and measurement of conflicts among agents and issues, together with thresholding via loss functions and a strategy-adjustment algorithm.", "motivation": "Existing three-way, preference-based conflict analysis models use only three qualitative relations (preference, converse, indifference) between issue pairs, which is too coarse to capture nuanced conflict attitudes. There is a need for a more expressive framework that can represent varying degrees of preference, hesitation, and conflict intensity among agents over issue pairs, and then exploit that richer representation to better classify conflicts and design feasible resolution strategies.", "method": "The authors define an intuitionistic fuzzy preference-based conflict situation, where agents' attitudes toward issue pairs are represented using intuitionistic fuzzy sets (membership, non-membership, and hesitation degrees). Based on this, they construct new intuitionistic fuzzy conflict measures and embed them into three-way decision/three-way conflict analysis models to trisect: (1) pairs of agents, (2) the agent set, and (3) the issue set into different conflict regions. They design relative loss functions on top of the conflict functions to derive decision thresholds for the three-way partitions. Finally, they propose adjustment-mechanism-based feasible strategies that simultaneously consider the magnitude of adjustments to preferences/issues and the severity of conflicts, along with an algorithm to derive such strategies, and they validate the model through an illustrative example.", "result": "The paper yields: (1) a formal definition of intuitionistic fuzzy preference-based conflict situations; (2) new intuitionistic fuzzy conflict measures compatible with three-way conflict analysis; (3) three distinct three-way models for conflict trisecting agent pairs, agents, and issues; (4) threshold determination methods using relative loss functions; and (5) an adjustment mechanism and corresponding algorithm for generating feasible conflict-mitigation strategies, demonstrated to work effectively through a detailed example.", "conclusion": "By moving from classical qualitative preference relations to an intuitionistic fuzzy representation, the proposed framework captures more nuanced attitudes and hesitations in agents' preferences over issue pairs. This enhanced expressiveness leads to more refined three-way conflict partitions and supports the systematic construction of feasible adjustment strategies that balance conflict reduction with the cost of preference changes. The illustrative experiment indicates that the approach is both valid and practically effective, suggesting its usefulness for complex conflict analysis and resolution tasks."}}
{"id": "2602.03587", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03587", "abs": "https://arxiv.org/abs/2602.03587", "authors": ["Shihan Dou", "Ming Zhang", "Zhangyue Yin", "Chenhao Huang", "Yujiong Shen", "Junzhe Wang", "Jiayi Chen", "Yuchen Ni", "Junjie Ye", "Cheng Zhang", "Huaibing Xie", "Jianglu Hu", "Shaolei Wang", "Weichao Wang", "Yanling Xiao", "Yiting Liu", "Zenan Xu", "Zhen Guo", "Pluto Zhou", "Tao Gui", "Zuxuan Wu", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Di Wang", "Shunyu Yao"], "title": "CL-bench: A Benchmark for Context Learning", "comment": "78 pages, 17 figures", "summary": "Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.", "AI": {"tldr": "The paper introduces CL-bench, a benchmark to test language models\u2019 ability to learn and reason from novel, task-specific context (\u201ccontext learning\u201d), rather than relying only on pre-trained knowledge. Current top models perform poorly, showing this capability is still largely missing.", "motivation": "Existing language models are strong at using their pre-trained knowledge and following patterns in prompts, but real-world tasks often depend on new, complex, domain-specific information that appears only in the task context. There is no systematic way to evaluate how well models can learn such new information from context, a human-like ability the authors call \u201ccontext learning.\u201d", "method": "The authors build CL-bench, a benchmark of 500 expert-crafted complex contexts paired with 1,899 tasks and 31,607 verification rubrics. Each task is constructed so that all needed but novel knowledge (domain-specific rules, procedures, data-derived laws, etc.) is contained in its context and is not in pre-training. They evaluate ten advanced language models on these tasks to measure how well they can infer and use this contextual information, going beyond simple retrieval or pattern-matching in long or in-context learning setups.", "result": "Across the benchmark, the ten evaluated frontier LMs solve on average only 17.2% of tasks. The strongest model (GPT-5.1) achieves only 23.7% accuracy, indicating a substantial performance gap in context learning capabilities even for state-of-the-art systems.", "conclusion": "The study concludes that current LMs lack robust context learning ability: they struggle to absorb and apply genuinely new, complex knowledge that appears only in task context. CL-bench provides a rigorous testbed to study and improve this capability, identifying context learning as a critical bottleneck for deploying LMs on complex, real-world problems and motivating future research toward models that can more effectively learn from context."}}
{"id": "2602.03429", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03429", "abs": "https://arxiv.org/abs/2602.03429", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Jaesang Yu", "John Joon Young Chung", "Juho Kim"], "title": "DiscoverLLM: From Executing Intents to Discovering Them", "comment": null, "summary": "To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking \"what kind of tone do you want?\" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.", "AI": {"tldr": "DiscoverLLM trains language models to not just clarify existing user intents, but to help users discover and refine intents they haven't yet formed, using a hierarchical intent simulator as a training signal.", "motivation": "Existing LLM interaction patterns assume users already know what they want and can specify it when asked, but in many real tasks (e.g., creative or technical writing) users are genuinely unsure and need to explore possibilities to figure out their preferences. Current systems that only ask clarification questions fail when users cannot articulate intents. The paper aims to enable LLMs to actively support this intent formation and discovery process.", "method": "The authors propose DiscoverLLM, a framework centered on a new user simulator that represents a user's cognitive state as a hierarchy of intents that become progressively more concrete as interaction unfolds. The simulator provides a reward based on how much the interaction helps to concretize intent. LLMs are trained with this signal to learn policies that balance exploration (diverging to surface options when intent is vague) and exploitation (converging to refine and implement once intent is clearer). They evaluate on interactive benchmarks for creative writing, technical writing, and SVG drawing.", "result": "Models trained with DiscoverLLM better manage the exploration\u2013convergence tradeoff, leading to more effective and efficient interactions. On the proposed interactive benchmarks, DiscoverLLM improves task performance by over 10% and shortens conversations by up to 40% compared to baselines. In a user study with 75 participants, it also yields higher conversation satisfaction and efficiency.", "conclusion": "Modeling user intent as a progressively concretizing hierarchy and using it as a training signal enables LLMs to collaborate more effectively with users who have not yet formed clear goals. DiscoverLLM generalizes across domains and demonstrates that training for intent discovery, not just clarification, improves both objective task outcomes and subjective user experience."}}
{"id": "2602.03588", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.03588", "abs": "https://arxiv.org/abs/2602.03588", "authors": ["Xuran Cai", "Amir Goharshady"], "title": "Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs", "comment": "Already accepted by SETTA'25. https://www.setta2025.uk/accepted-papers. arXiv admin note: substantial text overlap with arXiv:2507.16660", "summary": "In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \\(O(|G| \\cdot |D|^6)\\), where \\(|G|\\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.", "AI": {"tldr": "The paper presents a general algorithm for solving Partial Constraint Satisfaction Problems (PCSPs) on control-flow graphs using SPL decompositions, achieving linear time for fixed domains and improving over prior compiler-optimization methods.", "motivation": "Many classical compiler optimization problems (e.g., register allocation, speculative redundancy elimination, bank selection) can be formulated as Partial Constraint Satisfaction Problems on control-flow graphs, which are often sparse and structurally decomposable; however, a general, efficient algorithm that uniformly handles such PCSPs on structured CFGs has been lacking.", "method": "The authors model compiler optimization tasks as PCSP instances over control-flow graphs that admit Series-Parallel-Loop (SPL) decompositions. They then design a dynamic-programming-style algorithm that operates over the SPL decomposition of the CFG, generalizing prior SPL-based techniques for specific problems like register allocation and LOSPRE. The algorithm runs in O(|G| * |D|^6) time, where |G| is the graph size and |D| is the domain size, which is linear in |G| for any fixed domain.", "result": "They obtain a general algorithm for PCSPs over SPL graphs with time complexity O(|G| * |D|^6). Empirically, when applied to the Optimal Bank Selection problem, their implementation outperforms the previous state of the art by a factor of four in runtime.", "conclusion": "SPL decompositions can be exploited to obtain an efficient, unified framework for solving a broad class of compiler optimization problems formulated as PCSPs on control-flow graphs. The proposed algorithm generalizes earlier specialized methods and provides both strong theoretical guarantees (linear in graph size for fixed domains) and practical performance improvements on at least one classical task (Optimal Bank Selection)."}}
{"id": "2602.03439", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03439", "abs": "https://arxiv.org/abs/2602.03439", "authors": ["Xiaochi Zhou", "Patrick Bulter", "Changxuan Yang", "Simon D. Rihm", "Thitikarn Angkanaporn", "Jethro Akroyd", "Sebastian Mosbach", "Markus Kraft"], "title": "Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents", "comment": null, "summary": "We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.", "AI": {"tldr": "The paper proposes compiling ontologies into executable tools that LLM agents must use, so that knowledge graph updates comply with semantic constraints during generation, demonstrated on extracting structured chemistry knowledge from literature within The World Avatar framework.", "motivation": "LLMs are powerful at generating and extracting text but struggle to consistently adhere to formal schemas and ontological constraints when populating knowledge graphs, usually requiring manual schema-specific prompt engineering and post-hoc validation and repair. The authors aim to tightly integrate LLMs with formal domain knowledge so that ontology constraints are natively respected during generation, reducing manual work and errors.", "method": "Within The World Avatar (TWA) ecosystem, the authors compile OWL/RDF ontologies into executable tool interfaces that LLM-based agents are required to call to create and modify knowledge graph instances. These tools encode the ontological semantics and constraints, so any LLM interaction with the graph goes through them. They employ the Model Context Protocol (MCP) to enable structured communication among agents, tools, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and then iteratively applies these tools to extract structured entities and relations from unstructured scientific text, validate the extracted knowledge against the ontology, and repair inconsistencies by further tool calls.", "result": "In a case study on metal-organic polyhedra synthesis literature, the system is able to extract and structure scientific knowledge into a knowledge graph while obeying the ontology, with reduced need for hand-crafted prompts and schema-specific engineering compared to baseline LLM approaches. The ontology-compiled tools guide LLM behaviour so that proposed graph modifications are constrained and checked during generation, improving semantic correctness and automating parts of validation and repair.", "conclusion": "Compiling ontologies into executable tools for LLM agents is a viable mechanism to embed formal domain knowledge directly into generative workflows. By forcing LLMs to interact with knowledge graphs only through ontology-aware tools, semantic constraints are enforced at generation time, enabling more reliable knowledge extraction and maintenance, and reducing manual prompt and schema engineering. This suggests a general paradigm for coupling generative models, symbolic constraints, and external resources across scientific domains."}}
{"id": "2602.03608", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03608", "abs": "https://arxiv.org/abs/2602.03608", "authors": ["Haibo Jin", "Ruoxi Chen", "Peiyan Zhang", "Yifeng Luo", "Huimin Zeng", "Man Luo", "Haohan Wang"], "title": "Controlling Output Rankings in Generative Engines for LLM-based Search", "comment": "23 pages", "summary": "The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.\n  In this work, we propose CORE, an optimization method that \\textbf{C}ontrols \\textbf{O}utput \\textbf{R}ankings in g\\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.\n  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \\textbf{91.4\\% @Top-5}, \\textbf{86.6\\% @Top-3}, and \\textbf{80.3\\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.", "AI": {"tldr": "The paper introduces CORE, a method to systematically manipulate and control how products are ranked in LLM-based generative search engines by modifying retrieved content, achieving high success rates in promoting target items.", "motivation": "LLM-based search engines directly recommend products instead of listing links, which means early retrieval results heavily determine visibility. This biases exposure toward already-prominent items and disadvantages small businesses and independent creators. There is a need for systematic, controllable methods to influence and correct\u2014or exploit\u2014these rankings in a black-box setting.", "method": "The authors propose CORE, an optimization framework that alters the content returned by traditional search engines before it is consumed by LLM-based generative engines. Since direct control of LLM internals is impossible in the black-box case, CORE appends carefully crafted \u201coptimization content\u201d to retrieved product information. Three variants are studied: (1) string-based prompts that explicitly nudge rankings, (2) reasoning-based content that guides the LLM\u2019s comparison and justification process, and (3) review-based additions that mimic user feedback. They also introduce ProductBench, a benchmark with 15 product categories and 200 products each, labeled with their top-10 Amazon recommendations, to systematically evaluate ranking control.", "result": "On four major LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, Grok-3), CORE significantly boosts the rank of targeted products. It attains average Promotion Success Rates of 91.4%@Top-5, 86.6%@Top-3, and 80.3%@Top-1 across all 15 categories, outperforming prior ranking manipulation methods while keeping generated content fluent and natural.", "conclusion": "CORE shows that LLM-based generative search engines are highly sensitive and vulnerable to small, structured changes in input content, enabling effective control over result rankings even in a black-box setting. The work highlights both an opportunity (for recommendation optimization and fairness interventions) and a risk (for strategic manipulation) in LLM-centric product search, supported by a new benchmark, ProductBench, for future study."}}
{"id": "2602.03445", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.03445", "abs": "https://arxiv.org/abs/2602.03445", "authors": ["Qixin Zeng", "Shuo Zhang", "Hongyin Zhang", "Renjie Wang", "Han Zhao", "Libang Zhao", "Runze Li", "Donglin Wang", "Chao Huang"], "title": "CRL-VLA: Continual Vision-Language-Action Learning", "comment": null, "summary": "Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.", "AI": {"tldr": "The paper proposes CRL-VLA, a continual reinforcement learning framework for vision-language-action (VLA) models that balances remembering old skills and learning new ones, using a dual-critic architecture and a theoretical performance bound.", "motivation": "Embodied agents operating in open-world, lifelong scenarios need to continually acquire new manipulation skills without forgetting old ones. Existing continual RL methods for VLA models struggle with the stability-plasticity trade-off when fine-tuning via environment interaction, limiting their deployment in long-term robotic tasks.", "method": "The authors develop CRL-VLA, a continual post-training framework for VLA models. They derive a unified performance bound that expresses the trade-off between stability and plasticity as a function of goal-conditioned advantage magnitudes and policy divergence. To exploit this, they introduce asymmetric regulation: advantage magnitudes for old tasks are constrained while those for new tasks are allowed to grow in a controlled fashion. This is implemented with a dual-critic architecture and a Goal-Conditioned Value Formulation (GCVF), where a frozen critic preserves semantic consistency and a learnable critic supports adaptation during continual RL.", "result": "On the LIBERO benchmark for robotic manipulation, CRL-VLA outperforms baseline methods in both reducing catastrophic forgetting (anti-forgetting) and improving forward transfer/adaptation to new tasks, showing a better balance between stability and plasticity in continual RL for VLA models.", "conclusion": "CRL-VLA provides a theoretically grounded and practically effective approach to continual reinforcement learning for VLA models. By explicitly controlling advantage magnitudes via a dual-critic, goal-conditioned value formulation, it mitigates the stability-plasticity dilemma and enables more robust lifelong learning of manipulation skills in embodied agents."}}
{"id": "2602.03619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03619", "abs": "https://arxiv.org/abs/2602.03619", "authors": ["Changze Lv", "Jie Zhou", "Wentao Zhao", "Jingwen Xu", "Zisu Huang", "Muzhao Tian", "Shihan Dou", "Tao Gui", "Le Tian", "Xiao Zhou", "Xiaoqing Zheng", "Xuanjing Huang", "Jie Zhou"], "title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation", "comment": null, "summary": "Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.", "AI": {"tldr": "The paper proposes a method to automatically generate fine-grained, query-specific rubrics for evaluating DeepResearch-style reports, aligning them with human preferences and improving training via a new multi-agent workflow.", "motivation": "Training and evaluating DeepResearch-generated reports is hard because there are no clear, verifiable reward signals, so current rubric-based evaluations are either too coarse or too expensive to build manually for each query.", "method": "The authors build a dataset of DeepResearch-style queries with human preference labels over pairs of reports, then train rubric generators using reinforcement learning with a hybrid reward that mixes human preference supervision and LLM-based rubric evaluation; they also propose a Multi-agent Markov-state (MaMs) workflow to better support long-horizon reasoning in report generation and integrate the learned rubrics into this framework.", "result": "The learned rubric generators provide more fine-grained and human-aligned evaluation signals than existing rubric strategies, and when used in MaMs-based training, DeepResearch systems surpass all open-source baselines on DeepResearch Bench and reach performance close to top closed-source models.", "conclusion": "Automatically trained, preference-aligned rubric generators combined with a MaMs multi-agent workflow offer an effective, scalable way to supervise and improve DeepResearch report generation, closing the gap with state-of-the-art closed-source systems."}}
{"id": "2602.03467", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.03467", "abs": "https://arxiv.org/abs/2602.03467", "authors": ["Zeynep G. Saribatur", "Johannes Langer", "Ute Schmid"], "title": "The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding", "comment": "8 pages, 5 figures", "summary": "Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.", "AI": {"tldr": "The paper studies how abstracting explanations from symbolic AI (using Answer Set Programming) via clustering and removal of details affects human understanding and cognitive effort, finding that clustering improves understanding while removal reduces effort.", "motivation": "AI explanations are often hard for humans to understand, even when using transparent symbolic methods, because raw logical traces overload human cognition. The authors aim to make symbolic AI explanations more human-centered by reducing extraneous cognitive load through principled abstraction, and to empirically test how different abstraction strategies affect human reasoning and effort.", "method": "They use Answer Set Programming as the formal basis for explanations, formally define which details are irrelevant and therefore candidates for abstraction, and then construct simplified explanations via two operations: removal of irrelevant details and clustering related details. They run cognitive experiments where human participants classify stimuli in several domains, each time receiving explanations generated from ASP, and compare conditions with different abstraction strategies (clustering, removal, or presumably full-detail explanations). Measures include reasoning performance (understanding) and reported or observed cognitive effort.", "result": "Experimental results indicate that clustering details in explanations leads to significantly better participant understanding, whereas removing details leads to significantly lower cognitive effort. Both abstraction strategies therefore positively affect different aspects of explanation quality, demonstrating that structured abstraction can improve human interaction with symbolic explanations.", "conclusion": "Abstraction operations on symbolic explanations\u2014specifically clustering and removal of details\u2014increase the human-centeredness of explanations by enhancing understanding and reducing cognitive load. The work suggests that formal, principled abstraction mechanisms can bridge the gap between transparent symbolic reasoning and cognitively efficient, user-friendly explanations in AI systems."}}
{"id": "2602.03633", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.03633", "abs": "https://arxiv.org/abs/2602.03633", "authors": ["Burak Akta\u015f", "Mehmet Can Baytekin", "S\u00fcha Ka\u011fan K\u00f6se", "\u00d6mer \u0130lbilgi", "Elif \u00d6zge Y\u0131lmaz", "\u00c7a\u011fr\u0131 Toraman", "Bilge Kaan G\u00f6r\u00fcr"], "title": "BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish", "comment": "Accepted by EACL 2026 SIGTURK", "summary": "Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.", "AI": {"tldr": "This paper presents BIRDTurk, a Turkish adaptation of the BIRD Text-to-SQL benchmark, to study how Text-to-SQL systems perform in a morphologically rich, low-resource language.", "motivation": "Most Text-to-SQL systems are evaluated only on English, leaving their behavior in morphologically rich, low-resource languages like Turkish largely unknown. There is a need for a controlled benchmark that isolates linguistic effects while preserving the original databases and SQL semantics to enable fair cross-lingual comparison.", "method": "The authors construct BIRDTurk by translating the BIRD benchmark into Turkish using a controlled translation pipeline. This pipeline adapts schema identifiers (e.g., table and column names) to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. They validate translation quality using human evaluation on a sample size chosen via the Central Limit Theorem to guarantee 95% confidence, obtaining 98.15% accuracy. They then use BIRDTurk to evaluate several Text-to-SQL approaches: inference-time prompting, agentic multi-stage reasoning, and supervised fine-tuning on multilingual and instruction-tuned models.", "result": "Experiments show that performance consistently degrades when moving from English to Turkish, attributable to both structural linguistic differences (e.g., morphology, syntax) and Turkish being underrepresented in LLM pretraining. Among methods, agentic multi-stage reasoning is more robust across languages. Standard multilingual baselines struggle with supervised fine-tuning, whereas modern instruction-tuned models benefit more and scale better with fine-tuning on BIRDTurk.", "conclusion": "BIRDTurk is introduced as the first Turkish adaptation of BIRD and as a controlled, realistic testbed for cross-lingual Text-to-SQL research. The study highlights the challenges posed by morphologically rich, low-resource languages, shows that Turkish significantly degrades Text-to-SQL performance, and finds that agentic reasoning and instruction-tuned models offer better cross-lingual robustness. The authors release training and development splits to support future work."}}
{"id": "2602.03468", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03468", "abs": "https://arxiv.org/abs/2602.03468", "authors": ["Haohao Luo", "Zexi Li", "Yuexiang Xie", "Wenhao Zhang", "Yaliang Li", "Ying Shen"], "title": "IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning", "comment": "Preprint", "summary": "Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.", "AI": {"tldr": "IntentRL is a reinforcement-learning framework that trains deep research agents to proactively clarify user intent before doing expensive long-horizon web research, improving both intent understanding and downstream task performance.", "motivation": "Deep Research agents can autonomously browse the web and create long reports, but this is slow and costly. When user queries are ambiguous, agents may spend a lot of time doing irrelevant research, leading to poor results and wasted computation. There is also limited data of natural clarification dialogues to train better interaction strategies. The authors want a way for agents to efficiently ask the right clarification questions up front so that long-horizon research is better targeted and more efficient.", "method": "The authors propose IntentRL, which trains a proactive clarification policy for Deep Research agents. They build a scalable data pipeline that starts from a small set of seed examples and expands them into many high-quality clarification dialogues using a shallow-to-deep intent refinement graph. Then they apply a two-stage reinforcement learning approach: (1) offline RL on the collected dialogue data to learn general user interaction and clarification behaviors; (2) online RL where the trained agent interacts with a user simulator, allowing it to adapt to varied user feedback and further refine its clarification policy. The learned policy is integrated as a front-end to DR agents, deciding when and how to clarify before launching expensive research trajectories.", "result": "Experiments across multiple tasks show that IntentRL-trained agents more often recover the user\u2019s true intent (higher intent hit rate) and achieve better downstream task performance compared to baselines. These baselines include built-in clarification modules of closed-source Deep Research systems and proactive LLM approaches that are not RL-trained. Metrics demonstrate that learning a clarification policy via the proposed RL setup yields more effective and efficient research behavior.", "conclusion": "Proactively clarifying latent user intent using a dedicated RL-trained policy can substantially improve Deep Research agents, reducing wasted computation and improving answer quality. The combination of scalable synthetic dialogue generation and a two-stage offline-plus-online RL framework is effective for learning robust clarification strategies. This suggests that intent modeling and interactive clarification should be treated as a core RL problem in the design of long-horizon LLM-based research agents."}}
{"id": "2602.03635", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03635", "abs": "https://arxiv.org/abs/2602.03635", "authors": ["Chao Huang", "Yujing Lu", "Quangang Li", "Shenghe Wang", "Yan Wang", "Yueyang Zhang", "Long Xia", "Jiashu Zhao", "Zhiyuan Sun", "Daiting Shi", "Tingwen Liu"], "title": "TRE: Encouraging Exploration in the Trust Region", "comment": null, "summary": "Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.", "AI": {"tldr": "The paper identifies why standard entropy regularization fails for LLM-based RL and proposes Trust Region Entropy (TRE), which restricts exploration to a trust region, improving performance on several reasoning and alignment tasks.", "motivation": "Standard entropy regularization helps exploration in RL but underperforms or harms performance when applied to LLMs. The authors hypothesize that large vocabularies and long horizons create cumulative tail risk: entropy pushes probability mass into many bad/invalid tokens, breaking coherent reasoning. They seek an exploration method that maintains diversity only among plausible tokens.", "method": "They introduce Trust Region Entropy (TRE), a modified entropy regularization term that only encourages exploration within a defined trust region around the current policy, rather than over the entire output distribution. Conceptually, TRE reweights or truncates the entropy computation to exclude low-trust tail tokens, so that entropy bonuses promote exploration among high-probability, plausible candidates. TRE is integrated into PPO for RLHF-style training of LLMs and compared against standard PPO with classic entropy bonuses and other exploration baselines.", "result": "Across multiple benchmarks\u2014MATH for mathematical reasoning, Countdown for combinatorial search, and Helpful-Harmless (HH) preference alignment\u2014TRE-augmented PPO achieves consistently better performance than vanilla PPO, PPO with standard entropy regularization, and other exploration-focused baselines. This suggests that constraining entropy to a trust region effectively improves exploration quality in LLM training.", "conclusion": "Entropy regularization behaves differently in LLM-based RL due to large vocabularies and long generation horizons, where global entropy promotes harmful tail exploration. By limiting entropy to a trust region, TRE enables safer, more targeted exploration that empirically improves reasoning and alignment performance. The work implies that exploration mechanisms for LLMs must be adapted to account for cumulative tail risks rather than directly borrowing techniques from classical RL."}}
{"id": "2602.03478", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03478", "abs": "https://arxiv.org/abs/2602.03478", "authors": ["Guannan Lai", "Han-Jia Ye"], "title": "When Routing Collapses: On the Degenerate Convergence of LLM Routers", "comment": null, "summary": "LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.", "AI": {"tldr": "The paper identifies a common failure in LLM routing where routers overuse the largest model as cost budgets rise, and proposes EquiRouter, a ranking-based, decision-aware router that mitigates this issue and reduces cost while maintaining high performance.", "motivation": "Existing LLM routers tend to collapse to always choosing the strongest, most expensive model when users allow higher cost, even when cheaper models would perform adequately. This undermines the intended quality\u2013cost trade-off and wastes compute and money. The authors want to understand and fix this \"routing collapse\" to better exploit small models and deliver efficient, budget-aware routing.", "method": "The authors first empirically demonstrate routing collapse across unimodal and multimodal routing benchmarks. They attribute the issue to a mismatch between the training objective (predicting scalar performance scores for each model) and the actual routing decision (discrete, relative comparison among models). To address this, they introduce EquiRouter, which is trained to directly learn model rankings instead of raw scores, making it decision-aware and more robust to small prediction errors that could flip model orderings. They evaluate EquiRouter on the RouterBench benchmark against prior routing methods.", "result": "On RouterBench, EquiRouter restores the use of smaller models under higher budgets and reduces the overall routing cost by about 17% while still achieving GPT-4-level performance, outperforming the strongest prior router. It thereby mitigates routing collapse in both unimodal and multimodal settings.", "conclusion": "Routing collapse is a widespread issue in current LLM routing systems, arising from an objective\u2013decision mismatch in router training. By directly learning model rankings rather than scalar performance scores, EquiRouter makes routing decisions that better preserve the role of cheaper models, achieving significantly lower cost without sacrificing accuracy. This validates decision-aware, ranking-based training as a principled way to improve LLM routing under budget constraints."}}
{"id": "2602.03652", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03652", "abs": "https://arxiv.org/abs/2602.03652", "authors": ["S\u00fcha Ka\u011fan K\u00f6se", "Mehmet Can Baytekin", "Burak Akta\u015f", "Bilge Kaan G\u00f6r\u00fcr", "Evren Ayberk Munis", "Deniz Y\u0131lmaz", "Muhammed Yusuf Kartal", "\u00c7a\u011fr\u0131 Toraman"], "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish", "comment": "Accepted by EACL 2026 SIGTURK", "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.", "AI": {"tldr": "The paper builds and evaluates a full Turkish RAG pipeline, showing how different retrieval and generation modules affect performance and cost on a new Turkish QA dataset.", "motivation": "Most RAG design guidelines and benchmarks focus on English and do not account for the challenges of morphologically rich languages like Turkish. This leaves a gap in understanding how to best configure RAG systems for such languages in terms of accuracy, robustness, and cost.", "method": "They construct a Turkish RAG benchmark dataset from Turkish Wikipedia and CulturaX, consisting of question\u2013answer pairs and associated passage chunks. On top of this, they systematically benchmark seven stages of a generic RAG pipeline (e.g., query transformation, retrieval, reranking, answer generation, answer refinement) using off-the-shelf models, without any task-specific fine-tuning. They compare simple baselines with more complex techniques such as HyDE, Cross-encoder Reranking, and various generative query/context augmentation modules, measuring both accuracy and computational cost.", "result": "Complex query-transformation methods such as HyDE achieve the highest accuracy (~85%), which is substantially better than a baseline performance of 78.70%. However, a cheaper, Pareto-optimal setup that combines Cross-encoder Reranking with simple Context Augmentation attains nearly the same accuracy (84.60%) at much lower cost. They also find that stacking too many generative components in the pipeline can harm performance in Turkish by distorting key morphological forms needed for accurate retrieval and reasoning.", "conclusion": "Effective RAG for Turkish does not require heavy, stacked generative pipelines; instead, carefully chosen components\u2014such as strong reranking and lightweight query clarification\u2014offer an accuracy\u2013cost sweet spot. Overly complex generative transformations can even be counterproductive in morphologically rich languages, so RAG design guidelines should be language-aware and explicitly consider morphology when balancing performance and efficiency."}}
{"id": "2602.03541", "categories": ["cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.03541", "abs": "https://arxiv.org/abs/2602.03541", "authors": ["Qiankun Zhong", "Thomas F. Eisenmann", "Julian Garcia", "Iyad Rahwan"], "title": "Group Selection as a Safeguard Against AI Substitution", "comment": "19 pages, 7 Figures", "summary": "Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to \"cultural collapse\", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.", "AI": {"tldr": "The paper models how heavy reliance on generative AI can reduce cultural diversity and innovation, potentially causing a long-term \u201ccultural collapse.\u201d", "motivation": "To understand the long-run, population-level consequences of widespread generative AI adoption on human cultural evolution, particularly how different patterns of AI use affect cultural variance, innovation, and the risk of systemic failures like model collapse.", "method": "The authors use an agent-based model combined with evolutionary game theory. They define agents who either use AI as a complement (seeking guidance but remaining primary creators) or as a substitute (letting AI produce most of the output). They then simulate how these strategies affect cultural variance and how the strategies spread under individual selection versus cultural group selection with varying group boundary strength.", "result": "Simulations show that AI-substitute use tends to dominate under individual-level selection, even though it reduces cultural variance more strongly. However, at the group level, communities that favor AI-complement strategies retain more cultural diversity and exploratory innovation, and thus can be favored by cultural group selection when group boundaries are strong.", "conclusion": "Widespread AI-substitute use is individually attractive but collectively risky because it erodes cultural variance and slows cumulative cultural evolution, potentially leading to \u201ccultural collapse.\u201d Encouraging AI-complement use\u2014especially in settings with strong group structures\u2014can help preserve diversity and innovation, guiding organizational and policy interventions for safer AI adoption."}}
{"id": "2602.03677", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03677", "abs": "https://arxiv.org/abs/2602.03677", "authors": ["Yu Zhang", "Mufan Xu", "Xuefeng Bai", "Kehai chen", "Pengfei Zhang", "Yang Xiang", "Min Zhang"], "title": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration", "comment": "Modality Following", "summary": "Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\\%$ of these critical heads can decrease the modality-following ratio by $60\\%$ through blocking, or increase it by $60\\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.", "AI": {"tldr": "The paper analyzes how multimodal large language models decide when and how to use different modalities, revealing specific attention structures and heads that govern modality selection and showing these can be causally controlled.", "motivation": "Modality following\u2014correctly using or ignoring multimodal inputs according to user instructions\u2014is crucial for safety and reliability of multimodal LLMs, yet the internal decision-making mechanism for this process is unclear. Understanding this mechanism can improve transparency, control, and robustness in real-world applications.", "method": "The authors study information flow inside MLLMs from an \u201cinformation flow\u201d perspective, focusing on how instruction tokens interact with multimodal tokens across layers. They dissect roles of shallow vs deep attention layers and MLP layers, identify specialized attention heads responsible for modality arbitration, and perform causal interventions by selectively blocking or amplifying these heads to measure impacts on modality-following behavior.", "result": "They find that instruction tokens act as structural anchors where multimodal information is first routed non-selectively by shallow attention layers. In deeper layers, attention resolves competition between modalities based on instruction intent, while MLP layers resist changes (\u201csemantic inertia\u201d). They further discover a sparse set of critical attention heads that primarily drive modality arbitration, and show that modifying about 5% of these heads can reduce modality-following ratio by 60% (blocking) or increase it by 60% (amplifying) on failed cases.", "conclusion": "Modality following in MLLMs is governed by a structured, layer-wise arbitration process centered on instruction tokens and a small set of specialized attention heads. This yields a more transparent understanding of multimodal information orchestration and offers a principled way to control modality usage via targeted interventions, advancing both interpretability and controllability of MLLMs."}}
{"id": "2602.03545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03545", "abs": "https://arxiv.org/abs/2602.03545", "authors": ["Davide Paglieri", "Logan Cross", "William A. Cunningham", "Joel Z. Leibo", "Alexander Sasha Vezhnevets"], "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale", "comment": null, "summary": "Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.", "AI": {"tldr": "The paper introduces \u201cPersona Generators,\u201d LLM-driven functions that generate diverse synthetic populations of personas to evaluate AI systems, focusing on covering the full space of possible behaviors rather than just the most likely ones.", "motivation": "Evaluating AI systems across diverse human users is challenging because collecting representative data\u2014especially for new technologies or hypothetical futures\u2014is costly or infeasible, and existing LLM-based persona simulations tend to reproduce common behaviors while neglecting rare, long-tail traits and opinions.", "method": "The authors design Persona Generators\u2014code functions parameterized by context\u2014that are automatically evolved using an iterative improvement loop (based on AlphaEvolve). Large language models act as mutation operators that refine the generator code over hundreds of iterations, optimizing for diversity coverage across relevant axes of opinions and preferences given brief context descriptions.", "result": "The evolved Persona Generators, which are lightweight and context-conditioned, can expand small textual descriptions into large, diverse synthetic populations that better cover the space of possible human traits. Empirically, these generators significantly outperform baseline LLM prompting approaches on six quantitative diversity metrics and can produce rare combinations of traits that standard LLM outputs rarely capture.", "conclusion": "Persona Generators provide a scalable and automated way to simulate diverse human-like personas for AI evaluation, addressing the limitations of prior generative agent methods by emphasizing support coverage over density matching and by removing the need for detailed population data, thus enabling exploration of long-tail user behaviors in a variety of contexts."}}
{"id": "2602.03681", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03681", "abs": "https://arxiv.org/abs/2602.03681", "authors": ["Difan Deng", "Andreas Bentzen Winje", "Lukas Fehring", "Marius Lindauer"], "title": "Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models", "comment": "17 pages, 8 figures", "summary": "The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.", "AI": {"tldr": "This paper introduces NAtS-L, a token-level hybrid attention framework that mixes linear and softmax attention within the same layer to achieve efficiency and expressivity for long-context transformers.", "motivation": "Softmax attention has quadratic complexity, making it inefficient for long contexts, while fully linear attention models, though efficient, lose expressivity due to compressing history into a fixed-size state. Prior hybrids interleave whole softmax and linear layers, but still incur softmax costs. The paper is motivated by the need for a more fine-grained, token-level way to decide which information requires full softmax attention and which can be handled by linear attention.", "method": "They propose Neural Attention Search Linear (NAtS-L), which, within each layer, assigns different tokens either to a linear attention path (via Gated DeltaNet) or to a standard softmax attention path. A search/learning mechanism determines for each token whether its information is short-term (compressible in a fixed-size state) or long-term (should be preserved for future queries) and optimizes the combination of linear and softmax attention over tokens. This yields a token-level hybrid attention architecture instead of layer-level mixing.", "result": "Experiments (implied) show that NAtS-L can find effective combinations of Gated DeltaNet and softmax attention across tokens, achieving improved computational efficiency over pure or layer-level hybrid softmax transformers while maintaining strong performance, particularly in long-context tasks.", "conclusion": "NAtS-L demonstrates that token-level mixing of linear and softmax attention within the same layer can overcome the expressivity limits of purely linear attention while reducing the quadratic cost of full softmax attention, offering a more scalable architecture for long-context transformers."}}
{"id": "2602.03569", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03569", "abs": "https://arxiv.org/abs/2602.03569", "authors": ["Linjie Mu", "Zhongzhen Huang", "Yannian Gu", "Shengqian Qin", "Shaoting Zhang", "Xiaofan Zhang"], "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories", "comment": null, "summary": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.", "AI": {"tldr": "The paper introduces EHRWorld, a medical world model designed for stable long-horizon clinical simulation, and shows it outperforms LLM-based baselines.", "motivation": "Existing LLMs perform well on static medical reasoning but fail to maintain consistent patient states under sequential clinical interventions, causing error accumulation in long-horizon simulations. There is a need for a robust, causally grounded medical world model for dynamic disease progression and treatment outcome simulation.", "method": "The authors develop EHRWorld, a patient-centric world model trained with a causal sequential paradigm on EHRWorld-110K, a large longitudinal dataset derived from electronic health records. They compare it against naive LLM-based baselines on tasks involving multi-step clinical simulation and reasoning.", "result": "EHRWorld yields more stable long-horizon simulations, better modeling of clinically sensitive events, and improved reasoning efficiency than LLM-based baselines, according to extensive evaluation on the constructed dataset and tasks.", "conclusion": "Training medical world models on causally grounded, temporally evolving EHR data enables more reliable, robust, and efficient simulation of patient trajectories under interventions than naive LLM-based approaches."}}
{"id": "2602.03689", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03689", "abs": "https://arxiv.org/abs/2602.03689", "authors": ["Jiashuo Sun", "Pengcheng Jiang", "Saizhuo Wang", "Jiajun Fan", "Heng Wang", "Siru Ouyang", "Ming Zhong", "Yizhu Jiao", "Chengsong Huang", "Xueqiang Xu", "Pengrui Han", "Peiran Li", "Jiaxin Huang", "Ge Liu", "Heng Ji", "Jiawei Han"], "title": "Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation", "comment": "19 pages, 8 tables, 5 figures", "summary": "Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.", "AI": {"tldr": "The paper introduces BAR-RAG, a RAG framework that selects evidence tailored to the generator\u2019s difficulty sweet spot and trains the reranker with reinforcement learning from generator feedback, yielding more robust QA under noisy retrieval.", "motivation": "Traditional RAG pipelines rank passages only by relevance, which often results in either trivial answer-containing snippets or incomplete evidence that is not ideal for the generator\u2019s reasoning and learning, making systems brittle when retrieval is noisy.", "method": "The authors reframe reranking as boundary-aware evidence selection that targets a Goldilocks Zone of evidence difficulty for the generator. They train this selector via reinforcement learning using feedback from the generator, and then fine-tune the generator in a second stage on the evidence distribution induced by this selector to reduce train-test mismatch.", "result": "On knowledge-intensive QA benchmarks with noisy retrieval, BAR-RAG outperforms strong RAG and reranking baselines by an average of 10.3 percentage points in end-to-end performance and shows markedly improved robustness.", "conclusion": "Optimizing evidence selection for the generator\u2019s difficulty sweet spot, rather than for relevance alone, and aligning the generator\u2019s training distribution with that evidence, significantly improves robustness and accuracy of RAG systems under realistic retrieval noise."}}
{"id": "2602.03630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03630", "abs": "https://arxiv.org/abs/2602.03630", "authors": ["I\u00f1aki del Campo", "Pablo Cuervo", "Victor Rodriguez-Fernandez", "Roberto Armellin", "Jack Yarndley"], "title": "Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12", "comment": "Extended version of the paper presented at AIAA SciTech 2026 Forum. Includes futher experiments, corrections and new appendix", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an \"LLM-as-a-Judge\" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.", "AI": {"tldr": "The paper evaluates how well modern LLM-based agents can autonomously plan and design complex space missions, finding that they are good at high-level strategy but bad at detailed implementation.", "motivation": "Although LLMs excel at coding and general reasoning, it is unclear whether they can autonomously perform multi-stage planning in complex, physics-heavy domains like orbital mechanics and space mission design. The authors want to systematically test current AI agents on a realistic, high-dimensional problem to understand their real capabilities and limitations.", "method": "They adapt the MLE-Bench benchmarking framework to the astrodynamics domain of the GTOC 12 asteroid mining challenge and use an AIDE-style agent architecture to automatically generate and iteratively refine mission plans. Instead of just checking if solutions are valid, they introduce an LLM-as-a-Judge evaluation setup with a domain-expert-designed rubric scoring strategic viability along five structural dimensions. They then compare different LLMs, from GPT-4-Turbo to more advanced reasoning models like Gemini 2.5 Pro and o3.", "result": "Across models, strategic viability scores have almost doubled over the past two years, from 9.3 to 17.2 out of 26, indicating substantial progress in high-level reasoning and mission framing. However, all models exhibit systematic failures in actually implementing these strategies: they make physical unit mistakes, mishandle boundary conditions, and get stuck in inefficient debugging cycles.", "conclusion": "Current LLMs can understand and design high-level strategies for complex space science tasks and act as capable domain-aware assistants, but they cannot reliably carry out the detailed, physics-consistent implementation. There is a pronounced gap between conceptual planning and executable engineering, so LLMs should currently be viewed as powerful facilitators rather than autonomous engineers for such missions."}}
{"id": "2602.03693", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03693", "abs": "https://arxiv.org/abs/2602.03693", "authors": ["Deniz Y\u0131lmaz", "Evren Ayberk Munis", "\u00c7a\u011fr\u0131 Toraman", "S\u00fcha Ka\u011fan K\u00f6se", "Burak Akta\u015f", "Mehmet Can Baytekin", "Bilge Kaan G\u00f6r\u00fcr"], "title": "OCRTurk: A Comprehensive OCR Benchmark for Turkish", "comment": "Accepted by EACL 2026 SIGTURK", "summary": "Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.", "AI": {"tldr": "The paper introduces OCRTurk, a Turkish document parsing benchmark spanning diverse document types and difficulty levels, and evaluates seven OCR models, finding PaddleOCR performs best overall though performance varies by document type.", "motivation": "There is a lack of standardized, realistic benchmarks for Turkish (a low-resource language) document parsing, while existing benchmarks focus mainly on high-resource languages and do not reflect the diversity of real-world Turkish documents.", "method": "The authors construct OCRTurk, a benchmark of 180 Turkish documents taken from academic articles, theses, slide decks, and non-academic articles, annotated for multiple layout elements and organized into three difficulty levels; they then evaluate seven OCR models on this benchmark using element-wise metrics and Normalized Edit Distance.", "result": "PaddleOCR outperforms the other six OCR models across difficulty levels, leading most element-wise metrics except for figure recognition and achieving high Normalized Edit Distance on easy, medium, and hard subsets; performance also differs by document type, with models doing well on non-academic documents but struggling on slideshow documents.", "conclusion": "OCRTurk provides a needed standardized benchmark for Turkish document parsing that captures real-world document diversity and difficulty; current OCR systems, while reasonably strong (especially PaddleOCR), still face challenges on certain document types (e.g., slideshows) and layout elements (e.g., figures), highlighting areas for future improvement."}}
{"id": "2602.03647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03647", "abs": "https://arxiv.org/abs/2602.03647", "authors": ["Bowei He", "Minda Hu", "Zenan Xu", "Hongru Wang", "Licheng Zong", "Yankai Chen", "Chen Ma", "Xue Liu", "Pluto Zhou", "Irwin King"], "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration", "comment": null, "summary": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.", "AI": {"tldr": "The paper proposes Search-R2, an Actor-Refiner reinforcement learning framework that improves search-augmented reasoning by selectively correcting flawed reasoning steps using dense, fine-grained rewards.", "motivation": "Existing search-integrated language agents are trained with sparse, trajectory-level rewards that cannot distinguish genuine reasoning quality from lucky guesses. This leads to poor credit assignment, redundant or misleading search behavior, and limits the effectiveness of reinforcement learning for complex reasoning tasks.", "method": "They decompose reasoning into two collaborating components: (1) an Actor that generates initial reasoning trajectories with search, and (2) a Meta-Refiner that performs targeted diagnosis and repair on flawed steps via a cut-and-regenerate mechanism. They jointly optimize both via reinforcement learning using a hybrid reward that combines final answer correctness with a dense process reward measuring the information density of retrieved evidence. They also model the Actor-Refiner collaboration as a smoothed mixture policy for theoretical analysis.", "result": "On multiple general and multi-hop QA benchmarks and across different model sizes, Search-R2 outperforms strong retrieval-augmented generation (RAG) and RL-based baselines in reasoning accuracy while adding only minimal computational overhead.", "conclusion": "Selective, fine-grained correction of reasoning steps within a search-integrated RL framework yields better credit assignment and more effective reasoning than conventional RL training with sparse trajectory rewards. The Actor-Refiner collaboration with hybrid rewards is a principled and empirically validated way to enhance search-based language agents."}}
{"id": "2602.03704", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03704", "abs": "https://arxiv.org/abs/2602.03704", "authors": ["Yu Tian", "Linh Huynh", "Katerina Christhilf", "Shubham Chakraborty", "Micah Watanabe", "Tracy Arner", "Danielle McNamara"], "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models", "comment": "This manuscript is under review at Electronics", "summary": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.", "AI": {"tldr": "ReQUESTA is a hybrid multi-agent framework that uses LLMs plus rules to generate cognitively diverse multiple-choice reading comprehension questions that are harder, more discriminative, and better aligned with key ideas than single-pass LLM generation.", "motivation": "While LLMs can generate multiple-choice questions, controlling their cognitive level (e.g., text-based vs inferential vs main idea) and ensuring psychometrically strong items is difficult with simple prompting. There is a need for systematic, reliable workflows that can produce high-quality, cognitively targeted MCQs for educational assessment and research.", "method": "The authors design ReQUESTA, a hybrid multi-agent system that decomposes MCQ generation into subtasks (planning, controlled question and distractor generation, iterative evaluation, and post-processing). It combines LLM-powered agents with rule-based components to enforce cognitive targeting (text-based, inferential, main idea) and quality constraints. They then run a large-scale reading comprehension study using academic expository texts, comparing questions generated by ReQUESTA to a baseline of single-pass, zero-shot GPT-5-generated MCQs. Psychometric analyses (item difficulty, discrimination, relation to total reading score) on learner responses, plus expert ratings (topic relevance, centrality, distractor quality, etc.), are used to evaluate question quality.", "result": "ReQUESTA-generated MCQs are consistently more difficult and more discriminative than the baseline items, and their performance correlates more strongly with overall reading comprehension scores. Expert raters judge ReQUESTA items as better aligned with central concepts of the texts and having more linguistically consistent and semantically plausible distractors, especially for inferential questions.", "conclusion": "Hybrid, agentic orchestration\u2014splitting generation into coordinated LLM agents plus rule-based checks\u2014substantially improves the reliability, controllability, and psychometric quality of LLM-generated MCQs. The work suggests that workflow and system design, rather than single-pass prompting alone, is crucial for creating structured educational artifacts and for targeting specific cognitive demands in assessment."}}
{"id": "2602.03664", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03664", "abs": "https://arxiv.org/abs/2602.03664", "authors": ["Yang Wan", "Zheng Cao", "Zhenhao Zhang", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Linchao Zhu"], "title": "Mitigating Conversational Inertia in Multi-Turn Agents", "comment": null, "summary": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.", "AI": {"tldr": "The paper identifies and mitigates 'conversational inertia' in LLM-based agents, where models over-imitate their own prior responses, and proposes a preference-learning and context-management framework to reduce this inertia and improve performance in agentic tasks.", "motivation": "While large language models are strong few-shot learners, in multi-turn agent settings they mistakenly treat their own previous outputs as demonstrations, leading to excessive imitation and reduced exploration. There is a fundamental tension: longer contexts give more environmental feedback (good for exploitation) but also strengthen this harmful imitation bias (bad for exploration). The authors aim to understand and resolve this tension.", "method": "1) Use attention analysis to reveal 'conversational inertia', characterized by strong diagonal attention to the model's previous responses and its association with imitation bias. 2) Observe that, for the same environment state, model actions generated with longer contexts have higher inertia than those with shorter contexts. 3) Use this to construct preference pairs that do not rely on external rewards, preferring lower-inertia continuations over higher-inertia ones. 4) Propose Context Preference Learning (CPL), a training scheme that calibrates the model to favor low-inertia responses. 5) Design inference-time context management strategies to balance exploration and exploitation by controlling how much prior dialogue is included.", "result": "Across eight agentic benchmarks and a deep research setting, applying Context Preference Learning and the proposed context management strategies reduces conversational inertia and yields improved agent performance relative to baselines, demonstrating more exploratory and effective behavior.", "conclusion": "Conversational inertia is a measurable and impactful phenomenon in LLM-based agents, arising from excessive attention to prior self-generated responses. By leveraging context-length-induced differences to build preference data and then training models to favor low-inertia outputs, along with carefully managing context at inference time, one can reduce imitation bias, enhance exploration, and improve performance in multi-turn agentic environments."}}
{"id": "2602.03707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03707", "abs": "https://arxiv.org/abs/2602.03707", "authors": ["Yifan Zhu", "Xinyu Mu", "Tao Feng", "Zhonghong Ou", "Yuning Gong", "Haoran Luo"], "title": "OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering", "comment": null, "summary": "Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.", "AI": {"tldr": "OmniRAG-Agent is an agent-based, retrieval-augmented omnimodal QA system for long-horizon, low-resource audio-video reasoning that improves efficiency and performance via targeted multimodal retrieval, agentic planning, and reinforcement learning-based tool optimization.", "motivation": "Existing OmniLLMs for long-horizon omnimodal QA over text, images, audio, and video struggle in low-resource settings because they require dense encoding of long audio-video streams, cannot retrieve fine-grained snippets effectively, lack proactive multi-step planning, and do not support clear end-to-end optimization of tool usage and answer quality.", "method": "The paper proposes OmniRAG-Agent, which combines: (1) an image-audio retrieval-augmented generation (RAG) module that lets an OmniLLM query external banks of frames and audio clips to fetch short, relevant segments instead of encoding everything densely; (2) an agent loop that plans multi-step tool calls, iteratively retrieves and aggregates multimodal evidence, and reasons over it to answer queries; and (3) a training approach based on group relative policy optimization to jointly improve both the agent's tool-use policy and the resulting answer quality over time.", "result": "On three benchmarks\u2014OmniVideoBench, WorldSense, and Daily-Omni\u2014OmniRAG-Agent outperforms prior omnimodal QA methods specifically in low-resource settings, while also achieving strong overall performance. Ablation studies show that each major component (RAG module, agent loop, and GRPO training) contributes meaningfully to the gains.", "conclusion": "Agentic, retrieval-augmented omnimodal QA with targeted multimodal snippet retrieval and reinforcement learning-based policy optimization is an effective way to handle long-horizon, low-resource audio-video reasoning, overcoming the limitations of dense encoding and non-planning OmniLLMs."}}
{"id": "2602.03688", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03688", "abs": "https://arxiv.org/abs/2602.03688", "authors": ["Wenzhe Fan", "Tommaso Tognoli", "Henry Peng Zou", "Chunyu Miao", "Yibo Wang", "Xinhua Zhang"], "title": "TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System", "comment": null, "summary": "Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \\textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \\textbf{t}ask-\\textbf{o}riented \\textbf{dy}namic \\textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.", "AI": {"tldr": "They propose TodyComm, a task-oriented dynamic communication algorithm for multi-round LLM-based multi-agent systems that adapts the communication topology between agents across rounds to maximize task utility.", "motivation": "Existing LLM-based multi-agent systems usually use a fixed communication topology during inference, which is unrealistic when agent roles, adversaries, task stages, or communication constraints change over time. There is a need for communication structures that adapt across rounds to maintain or improve task performance under such dynamics.", "method": "They design TodyComm, a policy-gradient-trained algorithm that learns to generate behavior-driven, round-by-round collaboration topologies for agents. The communication graph between agents is dynamically adjusted in each round based on current behaviors and task context, with the objective of maximizing task utility while respecting constraints such as dynamic adversaries and bandwidth limits.", "result": "On five benchmarks involving dynamic adversaries and varying communication budgets, TodyComm outperforms baselines in task effectiveness and maintains good token efficiency and scalability.", "conclusion": "Learning dynamic, task-oriented communication topologies via policy gradient enables LLM-based multi-agent systems to collaborate more effectively in dynamic environments than methods with fixed communication structures, without sacrificing efficiency or scalability."}}
{"id": "2602.03708", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.03708", "abs": "https://arxiv.org/abs/2602.03708", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Boyuan Chen", "Ahmed E. Hassan"], "title": "Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.", "AI": {"tldr": "The paper introduces SemanticSpec, a semantic-aware speculative decoding framework that accelerates large (reasoning) language model inference by verifying semantic sequences rather than individual tokens, achieving up to 2.7x speedup over baselines.", "motivation": "Existing speculative decoding methods work at token level and treat different surface forms as different, even when they are semantically equivalent. This causes unnecessary rejections and wastes computation, which is especially problematic for Large Reasoning Models that produce long chains of thought and thus suffer from high latency.", "method": "SemanticSpec defines and verifies candidate continuations at the level of semantic sequences rather than tokens. It probes the model\u2019s internal hidden states to estimate the probability of generating sequences with particular meanings (semantic probability estimation), and uses these estimates to verify drafted sequences in a speculative decoding setup. This semantic-aware verification replaces purely token-level checks to better capture equivalence between different textual realizations of the same meaning.", "result": "On four benchmarks, SemanticSpec yields significant inference speedups: up to 2.7x for DeepSeekR1-32B and 2.1x for QwQ-32B, outperforming existing token-level and sequence-level speculative decoding baselines both in speed (efficiency) and in preserving output quality (effectiveness).", "conclusion": "SemanticSpec demonstrates that incorporating semantic-level verification into speculative decoding can substantially reduce inference latency for large language and reasoning models without sacrificing generation quality, and it consistently improves over traditional token-based and surface sequence-based speculative methods across benchmarks and models."}}
{"id": "2602.03786", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03786", "abs": "https://arxiv.org/abs/2602.03786", "authors": ["Jianhao Ruan", "Zhihao Xu", "Yiran Peng", "Fashen Ren", "Zhaoyang Yu", "Xinbing Liang", "Jinyu Xiang", "Bang Liu", "Chenglin Wu", "Yuyu Luo", "Jiayi Zhang"], "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "comment": null, "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "AI": {"tldr": "They propose AOrchestra, an agentic system that treats any agent as a 4-part tuple (Instruction, Context, Tools, Model) so it can dynamically spawn specialized sub-agents on demand, improving adaptability and performance on complex benchmarks.", "motivation": "Existing language-agent systems for complex, multi-step tasks rely on hand-crafted sub-agents or tool chains and lack a clear, dynamic abstraction for defining and composing sub-agents, which limits adaptability, portability across frameworks, and systematic performance-cost control.", "method": "They introduce a unified, framework-agnostic abstraction that represents any agent as a tuple: Instruction, Context, Tools, Model. On top of this, they design AOrchestra, where a central orchestrator instantiates this tuple at each step by curating relevant context, choosing appropriate tools and models, and automatically spawning specialized task executors (sub-agents) on the fly. The system supports plug-and-play integration of different agents and allows configurable performance versus cost trade-offs.", "result": "On three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra, when used with Gemini-3-Flash, achieves a 16.28% relative improvement over the strongest baseline, showing that the dynamic tuple abstraction and orchestration strategy yield better task success on long-horizon, complex tasks.", "conclusion": "Viewing agents uniformly as (Instruction, Context, Tools, Model) enables dynamic, automated construction of specialized sub-agents, reduces manual engineering, stays agnostic to specific frameworks, and yields better, more cost-efficient performance on complex multi-turn tasks as demonstrated by AOrchestra\u2019s benchmark gains."}}
{"id": "2602.03709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03709", "abs": "https://arxiv.org/abs/2602.03709", "authors": ["Vynska Amalia Permadi", "Xingwei Tan", "Nafise Sadat Moosavi", "Nikos Aletras"], "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding", "comment": null, "summary": "Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.", "AI": {"tldr": "ID-MoCQA is a multi-hop question answering dataset for evaluating cultural understanding of LLMs, focused on Indonesian traditions and offered in English and Indonesian.", "motivation": "Existing cultural QA benchmarks are mostly single-hop, letting models rely on shallow cues instead of real cultural reasoning. There is a need for a dataset that captures complex, multi-step reasoning grounded in culture.", "method": "The authors construct ID-MoCQA by systematically transforming single-hop Indonesian culture questions into multi-hop reasoning chains across six clue types (e.g., commonsense, temporal, geographical). They apply a multi-stage validation pipeline that includes expert human review and LLM-as-a-judge filtering to ensure data quality.", "result": "They release ID-MoCQA, a large-scale multi-hop QA dataset in English and Indonesian, and evaluate several state-of-the-art LLMs on it, finding notable performance gaps, especially where nuanced cultural inference is required.", "conclusion": "ID-MoCQA is a challenging benchmark that exposes current LLM limitations in cultural reasoning and can drive future work toward more culturally competent models."}}
{"id": "2602.03794", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03794", "abs": "https://arxiv.org/abs/2602.03794", "authors": ["Yingxuan Yang", "Chengrui Qu", "Muning Wen", "Laixi Shi", "Ying Wen", "Weinan Zhang", "Adam Wierman", "Shangding Gu"], "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity", "comment": null, "summary": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.", "AI": {"tldr": "The paper studies how scaling the number of LLM agents in a multi-agent system affects performance and shows that diversity among agents matters more than sheer quantity, using an information-theoretic framework.", "motivation": "LLM-based multi-agent systems are being used to solve complex tasks, but naive scaling by simply adding more identical agents leads to diminishing returns. There is a lack of principled understanding of what fundamentally limits performance scaling and why heterogeneous setups (different models, prompts, or tools) help more. The paper aims to provide theoretical and empirical insights into these questions so that MAS can be designed more efficiently and robustly.", "method": "The authors develop an information-theoretic framework to analyze multi-agent LLM systems, modeling them as channels that provide information about the underlying task. They derive architecture-agnostic bounds showing that system performance is limited by the intrinsic uncertainty of the task and depends on the number of effective information channels, not just the raw number of agents. They define and estimate an effective channel count, K*, that captures how many independent, useful information channels a system actually has, even without ground-truth labels, and compare homogeneous and heterogeneous agent configurations both theoretically and empirically.", "result": "The analysis reveals that performance gains from adding more homogeneous agents quickly saturate because their outputs are highly correlated and thus do not add much new information. In contrast, heterogeneous agents\u2014differing in model type, prompting, or tools\u2014provide less-correlated, complementary information, yielding larger and more sustained performance gains. They show experimentally that configurations with just 2 diverse agents can match or surpass the performance of systems with 16 homogeneous agents across tasks, validating the theory and the utility of K* as a measure of effective channels.", "conclusion": "The paper concludes that the primary bottleneck in scaling LLM-based multi-agent systems is intrinsic task uncertainty and correlation among agents, not the nominal number of agents. To improve performance efficiently, designers should maximize the diversity of effective information channels rather than simply increase agent count. The proposed K* metric offers a practical way to quantify and guide diversity-aware design, leading to more efficient, scalable, and robust multi-agent LLM systems."}}
{"id": "2602.03719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03719", "abs": "https://arxiv.org/abs/2602.03719", "authors": ["Yubao Zhao", "Weiquan Huang", "Sudong Wang", "Ruochen Zhao", "Chen Chen", "Yao Shu", "Chengwei Qin"], "title": "Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling", "comment": "24 pages, 5 figures", "summary": "Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \\href{https://github.com/YubaoZhao/BranPO}{code}.", "AI": {"tldr": "The paper introduces BranPO, a new value-free reinforcement learning method that improves long-horizon performance of LLM agents by using contrastive branching on trajectory tails instead of dense rewards or full tree search.", "motivation": "Agentic LLMs struggle in long-horizon tasks because rewards are sparse and only given at the end of trajectories, making credit assignment hard. Existing tree-based search methods try to explore alternatives but are computationally expensive and high variance. Empirical analysis shows that performance differences often arise from decisions near the end of trajectories, suggesting a more targeted way to provide supervision is needed.", "method": "The authors propose Branching Relative Policy Optimization (BranPO), a value-free RL algorithm that avoids explicit value estimation. It truncates trajectories near their tail and resamples multiple alternative continuations (branches) from shared prefixes, forming contrastive suffix pairs. The policy is then optimized to prefer better suffixes over worse ones using relative, step-level contrastive supervision. They further add difficulty-aware branch sampling to adapt how often branching happens based on task difficulty, and a redundant step masking mechanism that identifies and ignores uninformative actions to stabilize and speed up training.", "result": "On multiple question answering benchmarks, especially those requiring long-horizon reasoning, BranPO yields higher accuracy than strong baselines, including search-based and standard RL methods, while keeping the overall training budget (e.g., environment calls or compute) fixed. It shows consistent gains and better sample efficiency in long-horizon settings.", "conclusion": "BranPO offers an efficient and stable way to improve long-horizon learning for LLM-based agents by providing step-level contrastive supervision via tail branching rather than relying on dense rewards or heavy tree search. This targeted branching strategy reduces credit assignment ambiguity and leads to improved performance on complex QA tasks under the same training cost, suggesting a promising direction for scalable agentic RL for LLMs."}}
{"id": "2602.03814", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03814", "abs": "https://arxiv.org/abs/2602.03814", "authors": ["Xi Wang", "Anushri Suresh", "Alvin Zhang", "Rishi More", "William Jurayj", "Benjamin Van Durme", "Mehrdad Farajtabar", "Daniel Khashabi", "Eric Nalisnick"], "title": "Conformal Thinking: Risk Control for Reasoning on a Compute Budget", "comment": null, "summary": "Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.", "AI": {"tldr": "The paper proposes a risk-controlled framework for adaptively deciding how many tokens to spend on test-time reasoning with LLMs, trading computation for accuracy under a user-specified error rate.", "motivation": "Reasoning-focused LLMs benefit from using more computation (tokens) at test time, which improves accuracy but increases cost. In practice, it is hard to choose a fixed token budget or adaptive stopping thresholds that balance accuracy, cost, and reliability. Existing heuristics do not provide guarantees on error rates, creating a need for principled methods that formally control risk while minimizing compute.", "method": "The authors reframe the problem of setting token budgets as a statistical risk control problem. They design a two-threshold adaptive stopping strategy: (1) an upper confidence threshold that stops reasoning early when the model appears confident, accepting the risk of being wrong; and (2) a novel parametric lower threshold that identifies likely-unsolvable instances and stops them even earlier, accepting the risk of stopping too soon. Using a validation set and distribution-free risk control techniques, they tune these thresholds to satisfy a target error rate. For settings with several possible budget-control mechanisms (e.g., different exit rules or ensembles), they introduce an efficiency-loss formulation to select the mechanism that best balances compute and adherence to the risk target.", "result": "Across multiple reasoning benchmarks and LLMs, the proposed risk-control framework consistently meets or closely tracks the user-specified error (risk) target while significantly reducing average computational cost. The lower threshold for unsolvable instances yields additional compute savings, and combining multiple stopping criteria or ensemble-based exits can further improve efficiency without violating the risk constraint.", "conclusion": "Treating test-time token budget selection as a risk-control problem enables principled, distribution-free calibration of adaptive reasoning procedures for LLMs. The two-threshold mechanism and efficiency-aware selection of stopping rules provide a practical way to achieve user-specified reliability targets with reduced computational cost, improving the deployability of reasoning LLMs in resource-constrained or latency-sensitive applications."}}
{"id": "2602.03731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03731", "abs": "https://arxiv.org/abs/2602.03731", "authors": ["Paolo Astrino"], "title": "CUBO: Self-Contained Retrieval-Augmented Generation on Consumer Laptops 10 GB Corpora, 16 GB RAM, Single-Device Deployment", "comment": "24 pages, 2 figures, 6 tables", "summary": "Organizations handling sensitive documents face a tension: cloud-based AI risks GDPR violations, while local systems typically require 18-32 GB RAM. This paper presents CUBO, a systems-oriented RAG platform for consumer laptops with 16 GB shared memory. CUBO's novelty lies in engineering integration of streaming ingestion (O(1) buffer overhead), tiered hybrid retrieval, and hardware-aware orchestration that enables competitive Recall@10 (0.48-0.97 across BEIR domains) within a hard 15.5 GB RAM ceiling. The 37,000-line codebase achieves retrieval latencies of 185 ms (p50) on C1,300 laptops while maintaining data minimization through local-only processing aligned with GDPR Art. 5(1)(c). Evaluation on BEIR benchmarks validates practical deployability for small-to-medium professional archives. The codebase is publicly available at https://github.com/PaoloAstrino/CUBO.", "AI": {"tldr": "CUBO is a RAG platform engineered to run efficiently on consumer laptops (16 GB RAM), offering competitive retrieval performance with strict memory limits and GDPR-compliant local processing.", "motivation": "Many organizations cannot use cloud-based AI for sensitive documents due to GDPR and privacy concerns, but local RAG systems usually demand more RAM than common 16 GB laptops provide. There is a practical gap between data-protection requirements and the hardware actually available in small organizations. The paper aims to bridge this gap by building a retrieval-augmented generation system that fits strict memory budgets while still achieving strong retrieval performance.", "method": "The authors design CUBO, a systems-oriented RAG platform that combines three key ideas: (1) streaming ingestion with O(1) buffer overhead to efficiently process and index documents under tight memory constraints; (2) tiered hybrid retrieval to balance lexical and semantic search for high Recall@10; and (3) hardware-aware orchestration that schedules components and memory usage so the whole pipeline stays under a 15.5 GB RAM ceiling. They implement a 37k-line codebase tailored for consumer-grade laptops and evaluate it on BEIR benchmarks and real hardware (C1,300 laptops).", "result": "CUBO achieves Recall@10 between 0.48 and 0.97 across BEIR domains within a strict 15.5 GB RAM limit, with median retrieval latency of 185 ms on C1,300 laptops. This demonstrates that high-quality RAG retrieval can be done on standard 16 GB consumer hardware without depending on cloud resources. The system adheres to data minimization principles by processing all data locally, aligning with GDPR Art. 5(1)(c).", "conclusion": "The paper concludes that it is feasible to deploy competitive RAG systems for sensitive-document scenarios on widely available 16 GB laptops by carefully engineering memory-efficient ingestion, hybrid retrieval, and hardware-aware orchestration. CUBO offers a practical, GDPR-aligned solution for small-to-medium professional archives, and the publicly available codebase can serve as a basis for further research and deployments."}}
{"id": "2602.03828", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.03828", "abs": "https://arxiv.org/abs/2602.03828", "authors": ["Minjun Zhu", "Zhen Lin", "Yixuan Weng", "Panzhong Lu", "Qiujie Xie", "Yifan Wei", "Sifan Liu", "Qiyao Sun", "Yue Zhang"], "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations", "comment": "Accepted at the ICLR 2026", "summary": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.", "AI": {"tldr": "The paper introduces FigureBench, a large-scale benchmark of text-figure pairs, and AutoFigure, an agentic system that generates high-quality scientific illustrations from long-form scientific text, outperforming baselines.", "motivation": "Manual creation of high-quality scientific figures is time-consuming and a bottleneck for communicating complex ideas; there is also a lack of large-scale benchmarks and strong automatic systems for text-to-figure generation in scientific contexts.", "method": "They construct FigureBench, a dataset of 3,300 curated scientific text-figure pairs from diverse sources. They then design AutoFigure, an agentic multi-step framework that deeply processes long-form scientific text, plans and validates an illustration layout, and finally renders a structurally sound and aesthetically pleasing figure. They evaluate AutoFigure against multiple baseline methods using FigureBench.", "result": "AutoFigure consistently outperforms all baselines on the FigureBench benchmark, achieving better structural completeness and aesthetic quality in generated scientific illustrations, and is capable of producing publication-ready figures.", "conclusion": "FigureBench establishes a new benchmark for scientific text-to-illustration generation, and AutoFigure demonstrates that an agentic, multi-stage reasoning and layout framework can reliably produce high-quality, publication-ready scientific figures from long-form text, surpassing existing methods; code and data are publicly released."}}
{"id": "2602.03784", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03784", "abs": "https://arxiv.org/abs/2602.03784", "authors": ["Jiangnan Ye", "Hanqi Yan", "Zhenyi Shen", "Heng Chang", "Ye Mao", "Yulan He"], "title": "Context Compression via Explicit Information Transmission", "comment": null, "summary": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.", "AI": {"tldr": "The paper introduces ComprExIT, a new framework for soft context compression for LLMs that explicitly transmits information across layers and tokens, achieving better long-context question answering with minimal extra parameters.", "motivation": "Long-context inference in LLMs is computationally expensive because of quadratic attention cost and large key-value caches. Existing soft compression methods usually fine-tune the LLM as a compressor using its own self-attention, which leads to progressive overwriting of representations across layers and poor, uncoordinated use of limited compression capacity across tokens. The authors aim to design a more effective, robust, and light-weight compression mechanism.", "method": "ComprExIT freezes the base LLM and adds a lightweight compression module that performs explicit information transmission over hidden states. It has two main components: (1) depth-wise transmission, which selectively routes and aggregates information from multiple layers into a set of token anchors, reducing progressive overwriting; (2) width-wise transmission, which further compresses the anchors into a small set of slots via a globally optimized transmission plan, ensuring that compression capacity is allocated in a coordinated way across tokens. The entire mechanism introduces only about 1% additional parameters over the base model.", "result": "On six long-context question-answering benchmarks, ComprExIT outperforms previous state-of-the-art soft context compression methods while using minimal additional parameters (~1% over the base LLM). The gains indicate improved effectiveness and robustness of the compressed representations for long-context reasoning tasks.", "conclusion": "Explicit, decoupled information transmission over frozen LLM hidden states can overcome structural limitations of self-attention-based compression. By coordinating how information is gathered across depth (layers) and width (tokens), ComprExIT achieves more efficient and accurate long-context compression, enabling better performance on long-context QA with negligible parameter overhead."}}
{"id": "2602.02276", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02276", "abs": "https://arxiv.org/abs/2602.02276", "authors": ["Kimi Team", "Tongtong Bai", "Yifan Bai", "Yiping Bao", "S. H. Cai", "Yuan Cao", "Y. Charles", "H. S. Che", "Cheng Chen", "Guanduo Chen", "Huarong Chen", "Jia Chen", "Jiahao Chen", "Jianlong Chen", "Jun Chen", "Kefan Chen", "Liang Chen", "Ruijue Chen", "Xinhao Chen", "Yanru Chen", "Yanxu Chen", "Yicun Chen", "Yimin Chen", "Yingjiang Chen", "Yuankun Chen", "Yujie Chen", "Yutian Chen", "Zhirong Chen", "Ziwei Chen", "Dazhi Cheng", "Minghan Chu", "Jialei Cui", "Jiaqi Deng", "Muxi Diao", "Hao Ding", "Mengfan Dong", "Mengnan Dong", "Yuxin Dong", "Yuhao Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Lingxiao Du", "Yulun Du", "Yu Fan", "Shengjun Fang", "Qiulin Feng", "Yichen Feng", "Garimugai Fu", "Kelin Fu", "Hongcheng Gao", "Tong Gao", "Yuyao Ge", "Shangyi Geng", "Chengyang Gong", "Xiaochen Gong", "Zhuoma Gongque", "Qizheng Gu", "Xinran Gu", "Yicheng Gu", "Longyu Guan", "Yuanying Guo", "Xiaoru Hao", "Weiran He", "Wenyang He", "Yunjia He", "Chao Hong", "Hao Hu", "Jiaxi Hu", "Yangyang Hu", "Zhenxing Hu", "Ke Huang", "Ruiyuan Huang", "Weixiao Huang", "Zhiqi Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yu Jing", "Guokun Lai", "Aidi Li", "C. Li", "Cheng Li", "Fang Li", "Guanghe Li", "Guanyu Li", "Haitao Li", "Haoyang Li", "Jia Li", "Jingwei Li", "Junxiong Li", "Lincan Li", "Mo Li", "Weihong Li", "Wentao Li", "Xinhang Li", "Xinhao Li", "Yang Li", "Yanhao Li", "Yiwei Li", "Yuxiao Li", "Zhaowei Li", "Zheming Li", "Weilong Liao", "Jiawei Lin", "Xiaohan Lin", "Zhishan Lin", "Zichao Lin", "Cheng Liu", "Chenyu Liu", "Hongzhang Liu", "Liang Liu", "Shaowei Liu", "Shudong Liu", "Shuran Liu", "Tianwei Liu", "Tianyu Liu", "Weizhou Liu", "Xiangyan Liu", "Yangyang Liu", "Yanming Liu", "Yibo Liu", "Yuanxin Liu", "Yue Liu", "Zhengying Liu", "Zhongnuo Liu", "Enzhe Lu", "Haoyu Lu", "Zhiyuan Lu", "Junyu Luo", "Tongxu Luo", "Yashuo Luo", "Long Ma", "Yingwei Ma", "Shaoguang Mao", "Yuan Mei", "Xin Men", "Fanqing Meng", "Zhiyong Meng", "Yibo Miao", "Minqing Ni", "Kun Ouyang", "Siyuan Pan", "Bo Pang", "Yuchao Qian", "Ruoyu Qin", "Zeyu Qin", "Jiezhong Qiu", "Bowen Qu", "Zeyu Shang", "Youbo Shao", "Tianxiao Shen", "Zhennan Shen", "Juanfeng Shi", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Pengwei Song", "Tianhui Song", "Xiaoxi Song", "Hongjin Su", "Jianlin Su", "Zhaochen Su", "Lin Sui", "Jinsong Sun", "Junyao Sun", "Tongyu Sun", "Flood Sung", "Yunpeng Tai", "Chuning Tang", "Heyi Tang", "Xiaojuan Tang", "Zhengyang Tang", "Jiawen Tao", "Shiyuan Teng", "Chaoran Tian", "Pengfei Tian", "Ao Wang", "Bowen Wang", "Chensi Wang", "Chuang Wang", "Congcong Wang", "Dingkun Wang", "Dinglu Wang", "Dongliang Wang", "Feng Wang", "Hailong Wang", "Haiming Wang", "Hengzhi Wang", "Huaqing Wang", "Hui Wang", "Jiahao Wang", "Jinhong Wang", "Jiuzheng Wang", "Kaixin Wang", "Linian Wang", "Qibin Wang", "Shengjie Wang", "Shuyi Wang", "Si Wang", "Wei Wang", "Xiaochen Wang", "Xinyuan Wang", "Yao Wang", "Yejie Wang", "Yipu Wang", "Yiqin Wang", "Yucheng Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhaowei Wang", "Zhengtao Wang", "Zhexu Wang", "Zihan Wang", "Zizhe Wang", "Chu Wei", "Ming Wei", "Chuan Wen", "Zichen Wen", "Chengjie Wu", "Haoning Wu", "Junyan Wu", "Rucong Wu", "Wenhao Wu", "Yuefeng Wu", "Yuhao Wu", "Yuxin Wu", "Zijian Wu", "Chenjun Xiao", "Jin Xie", "Xiaotong Xie", "Yuchong Xie", "Yifei Xin", "Bowei Xing", "Boyu Xu", "Jianfan Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinbo Xu", "Xinran Xu", "Yangchuan Xu", "Yichang Xu", "Yuemeng Xu", "Zelai Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Guangyao Yang", "Hao Yang", "Junwei Yang", "Kai Yang", "Ningyuan Yang", "Ruihan Yang", "Xiaofei Yang", "Xinlong Yang", "Ying Yang", "Yi Yang", "Yi Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Dan Ye", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Chengzhen Yu", "Longhui Yu", "Tao Yu", "Tianxiang Yu", "Enming Yuan", "Mengjie Yuan", "Xiaokun Yuan", "Yang Yue", "Weihao Zeng", "Dunyuan Zha", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Jin Zhang", "Puqi Zhang", "Qiao Zhang", "Rui Zhang", "Xiaobin Zhang", "Y. Zhang", "Yadong Zhang", "Yangkun Zhang", "Yichi Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yushun Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Chenguang Zhao", "Feifan Zhao", "Jinxiang Zhao", "Shuai Zhao", "Xiangyu Zhao", "Yikai Zhao", "Zijia Zhao", "Huabin Zheng", "Ruihan Zheng", "Shaojie Zheng", "Tengyang Zheng", "Junfeng Zhong", "Longguang Zhong", "Weiming Zhong", "M. Zhou", "Runjie Zhou", "Xinyu Zhou", "Zaida Zhou", "Jinguo Zhu", "Liya Zhu", "Xinhao Zhu", "Yuxuan Zhu", "Zhen Zhu", "Jingze Zhuang", "Weiyu Zhuang", "Ying Zou", "Xinxing Zu"], "title": "Kimi K2.5: Visual Agentic Intelligence", "comment": "Kimi K2.5 tech report", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "AI": {"tldr": "Kimi K2.5 is an open-source multimodal agentic model with a parallel agent framework that achieves state-of-the-art performance and lower latency.", "motivation": "Advance general agentic intelligence by tightly integrating text and vision capabilities and enabling efficient parallel agent orchestration for complex tasks.", "method": "Jointly optimize text and vision via joint pre-training, zero-vision supervised fine-tuning, and joint text-vision reinforcement learning. On top of this foundation, introduce Agent Swarm, a self-directed framework that decomposes complex tasks into heterogeneous sub-problems and executes them in parallel.", "result": "Kimi K2.5 reaches state-of-the-art performance across coding, vision, reasoning, and agentic benchmarks, while Agent Swarm reduces latency by up to 4.5x compared to single-agent baselines. The post-trained model checkpoint is released openly.", "conclusion": "Joint text-vision optimization combined with a parallel agent orchestration framework yields a strong, general-purpose agentic model, and open-sourcing Kimi K2.5 should help drive further research and practical applications in agentic intelligence."}}
{"id": "2602.03822", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03822", "abs": "https://arxiv.org/abs/2602.03822", "authors": ["Sahil Tripathi", "Gautam Siddharth Kashyap", "Mehwish Nasim", "Jian Yang", "Jiechao Gao", "Usman Naseem"], "title": "They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References", "comment": "Accepted at the The Web Conference 2026 (Research Track)", "summary": "Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.", "AI": {"tldr": "The paper proposes CROSS-ALIGN+, a three-stage framework that improves meme-based social abuse detection by adding external knowledge, refining decision boundaries, and generating explanations, yielding better F1 scores and interpretability than prior methods.", "motivation": "Meme-based social abuse is hard to detect because abusive intent is often implicit, relies on cultural symbols, and emerges from subtle mismatches between image and text. Existing LVLM and fusion-based methods struggle with missing cultural context, confusion between satire and abuse, and a lack of transparent reasoning. The paper aims to systematically tackle these issues.", "method": "The authors design CROSS-ALIGN+, a three-stage pipeline: (1) cultural grounding via knowledge enrichment, where multimodal representations are augmented using structured external resources like ConceptNet, Wikidata, and Hatebase to capture symbolic and cultural cues; (2) boundary sharpening using parameter-efficient LoRA adapters to better differentiate abusive content from benign or satirical memes; and (3) interpretability via cascaded explanation generation, producing step-by-step justifications for model decisions. They evaluate the framework on five meme-abuse benchmarks and with eight different LVLM backbones.", "result": "Across five benchmarks, CROSS-ALIGN+ consistently outperforms prior state-of-the-art approaches, with reported relative F1 gains up to 17%. It also yields interpretable outputs by attaching textual explanations that align with its decisions across eight LVLMs.", "conclusion": "Enriching LVLM-based meme abuse detectors with structured external knowledge, lightweight boundary-refining adapters, and explicit explanation generation can jointly address cultural blindness, decision ambiguity, and lack of transparency. CROSS-ALIGN+ offers a practical, extensible framework that improves both accuracy and interpretability for multimodal abuse detection."}}
{"id": "2602.03837", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03837", "abs": "https://arxiv.org/abs/2602.03837", "authors": ["David P. Woodruff", "Vincent Cohen-Addad", "Lalit Jain", "Jieming Mao", "Song Zuo", "MohammadHossein Bateni", "Simina Branzei", "Michael P. Brenner", "Lin Chen", "Ying Feng", "Lance Fortnow", "Gang Fu", "Ziyi Guan", "Zahra Hadizadeh", "Mohammad T. Hajiaghayi", "Mahdi JafariRaviz", "Adel Javanmard", "Karthik C. S.", "Ken-ichi Kawarabayashi", "Ravi Kumar", "Silvio Lattanzi", "Euiwoong Lee", "Yi Li", "Ioannis Panageas", "Dimitris Paparas", "Benjamin Przybocki", "Bernardo Subercaseaux", "Ola Svensson", "Shayan Taherijam", "Xuan Wu", "Eylon Yogev", "Morteza Zadimoghaddam", "Samson Zhou", "Vahab Mirrokni"], "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques", "comment": null, "summary": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.", "AI": {"tldr": "The paper presents case studies where advanced Gemini-based language models collaborate with researchers to solve open problems and assist in theoretical research, illustrating AI as a creative partner rather than just an automation tool.", "motivation": "To understand and showcase how modern large language models can move beyond routine assistance to meaningfully contribute to novel, expert-level scientific and mathematical discoveries.", "method": "The authors compile and analyze multiple real-world case studies across theoretical computer science and other fields, detailing workflows of human-AI collaboration such as iterative dialogue, problem decomposition, adversarial proof checking, and neuro-symbolic loops where the AI writes and executes code.", "result": "The case studies show that Gemini-based models helped solve open problems, refute conjectures, and produce new proofs; they also successfully acted as adversarial reviewers to spot subtle proof errors and as components in automated pipelines for verifying derivations via code.", "conclusion": "Advanced LLMs can function as genuine research partners in theoretical disciplines when used with appropriate collaboration strategies, suggesting a shift in how scientific discovery can be conducted and highlighting the importance of carefully designed human-AI interaction patterns."}}
{"id": "2602.03845", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03845", "abs": "https://arxiv.org/abs/2602.03845", "authors": ["Tong Zheng", "Chengsong Huang", "Runpeng Dai", "Yun He", "Rui Liu", "Xin Ni", "Huiwen Bao", "Kaishen Wang", "Hongtu Zhu", "Jiaxin Huang", "Furong Huang", "Heng Huang"], "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing", "comment": "14 pages", "summary": "Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce $\\textbf{Parallel-Probe}$, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to $\\textbf{35.8}$% and total token cost by over $\\textbf{25.8}$% while maintaining competitive accuracy.", "AI": {"tldr": "The paper proposes Parallel-Probe, a training-free controller that improves the efficiency of parallel reasoning in language models by probing intermediate results across branches and adaptively controlling depth and width, reducing token costs while preserving accuracy.", "motivation": "Parallel thinking (running multiple reasoning branches) boosts reasoning quality but is computationally expensive. Existing efficiency methods mainly use local, per-trajectory signals (e.g., per-branch stopping) and do not leverage global patterns across all branches. There is a need for a principled way to use global width-depth dynamics\u2014how many branches to run and how deep each should go\u2014to reduce cost without hurting performance.", "method": "The paper introduces 2D probing, an interface that periodically collects intermediate answers from all parallel branches, exposing width-depth dynamics. From analyzing these dynamics, the authors observe non-monotonic performance scaling over width and depth, varied branch lengths, and early stabilization of consensus. Based on these, they design Parallel-Probe, a training-free controller that: (1) uses consensus-based early stopping to decide when to stop further reasoning depth globally, and (2) uses deviation-based pruning to drop branches that deviate from emerging consensus, thereby shrinking width on the fly.", "result": "Across three benchmarks and multiple models, Parallel-Probe outperforms standard parallel reasoning with majority voting in terms of the tradeoff between accuracy and computation. It yields a better Pareto frontier for test-time scaling, cutting sequential tokens by up to 35.8% and reducing total token usage by more than 25.8%, while maintaining similar accuracy to majority-vote baselines.", "conclusion": "By exposing and exploiting global width-depth dynamics through 2D probing, the proposed Parallel-Probe controller can make parallel reasoning more computationally efficient without retraining models. Its consensus-based early stopping and deviation-based branch pruning provide a more principled, globally informed control strategy, achieving significant token savings at test time while preserving accuracy, and setting a new standard for efficient parallel test-time scaling."}}
{"id": "2602.01995", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01995", "abs": "https://arxiv.org/abs/2602.01995", "authors": ["Jeongmoon Won", "Seungwon Kook", "Yohan Jo"], "title": "Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs", "comment": null, "summary": "Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.", "AI": {"tldr": "The paper introduces a conversational diagnosis system that uses a diagnostic knowledge graph and a realistic, vague-responding patient simulator to iteratively generate and verify diagnostic hypotheses, achieving higher accuracy and efficiency than strong baselines.", "motivation": "Current conversational diagnosis systems unrealistically assume patients give rich, precise information or over-rely on models' parametric knowledge, which does not reflect real clinical encounters where information is incomplete and symptoms are often described vaguely. There is also a need for a realistic patient simulator to rigorously evaluate such systems.", "method": "The authors build a two-step conversational diagnosis framework that operates over a diagnostic knowledge graph. First, the system generates diagnostic hypotheses based on the current dialogue context. Second, it verifies these hypotheses by asking targeted clarifying questions, repeating this cycle until a final diagnosis is selected. For evaluation, they adopt a known patient simulator combined with MIMIC-IV patient profiles and modify it so that symptom descriptions are vague, mimicking early real-world consultations. They then compare their system against strong baseline methods.", "result": "The proposed system outperforms strong baselines in diagnostic accuracy and efficiency (fewer turns or questions to reach a correct diagnosis). Physician evaluations indicate that the adapted patient simulator produces realistic vague symptom descriptions and that the system's clarifying questions have clinical value.", "conclusion": "Reasoning over a diagnostic knowledge graph with an iterative hypothesis-generation and verification loop leads to more accurate and efficient conversational diagnosis under vague, incomplete patient information. The adapted, more realistic patient simulator is validated by clinicians, and the system shows promise for practical clinical decision support. The authors plan to release their code upon publication to facilitate further research."}}
