{"id": "2601.09713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09713", "abs": "https://arxiv.org/abs/2601.09713", "authors": ["Jinqiang Wang", "Huansheng Ning", "Jianguo Ding", "Tao Zhu", "Liming Chen", "Chris Nugent"], "title": "LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue", "comment": "19 pages", "summary": "Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.", "AI": {"tldr": "The paper introduces ProUtt, a method to generate preference-based training data so compact LLMs can proactively predict a user\u2019s next utterance in dialogue more accurately and efficiently than existing simulators and APIs.", "motivation": "Commercial API-based next-utterance prediction raises privacy issues, and running large general LLMs locally is too costly. Compact task-specific LLMs are a practical alternative, but current data synthesis and user simulators mostly mimic user style instead of the underlying intent reasoning that drives future utterances. Existing preference-data methods also don\u2019t explicitly model how user intentions evolve over turns or clearly distinguish preferred vs. non-preferred reasoning paths. The paper is motivated by the need for a principled way to synthesize high-quality preference data that encodes intent reasoning for proactive next utterance prediction.", "method": "The authors propose ProUtt, an LLM-driven preference data synthesis framework. It first converts dialogue histories into an intent tree, where nodes represent possible user intents or sub-intents. Then it explicitly models intent reasoning trajectories by predicting plausible next paths through this tree from two angles: exploitation (likely, reasonable next actions) and exploration (less obvious but still plausible alternatives). Using these trajectories, ProUtt constructs pairs of preference and non-preference reasoning processes by perturbing or revising paths at different future turns, thus generating labeled reasoning traces and next-utterance candidates. These synthesized preference datasets are used to train compact, task-specific LLMs for proactive next utterance prediction.", "result": "In experiments on four benchmark dialogue datasets, ProUtt-synthesized data is used to train models that are evaluated with both LLM-as-a-judge and human annotators. Across metrics, the ProUtt-based models consistently outperform baselines, including existing data synthesis approaches, user simulators, and commercial LLM APIs, in predicting proactive next utterances and aligning with user preferences.", "conclusion": "Explicitly modeling intent reasoning via intent trees and generating structured preference vs. non-preference reasoning paths leads to substantially better data for training compact LLMs for proactive next utterance prediction. ProUtt improves over prior simulators and synthesis methods on multiple benchmarks, demonstrating that reasoning-aware preference data synthesis is a promising direction. The authors release code and synthesized datasets to support further work in proactive dialogue modeling and preference alignment."}}
{"id": "2601.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09714", "abs": "https://arxiv.org/abs/2601.09714", "authors": ["Devesh Saraogi", "Rohit Singhee", "Dhruv Kumar"], "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines", "comment": "Under Review", "summary": "The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.", "AI": {"tldr": "The paper compares different multi-step LLM \u201cagentic\u201d workflows for generating research plans and finds that decomposition and long-context pipelines produce significantly more novel yet feasible ideas than simple reflection-based refinement.", "motivation": "As LLMs are increasingly used to generate scientific ideas, there is concern that they mainly rephrase existing work (\u201csmart plagiarism\u201d) rather than creating genuinely new, useful research directions. Most prior setups use single-step prompts or simple refinements, leaving open whether more sophisticated agentic workflows can improve creativity and feasibility of AI-generated research plans.", "method": "The authors benchmark five distinct LLM-based reasoning architectures for research ideation: (1) reflection-based iterative refinement, (2) Sakana AI v2 evolutionary search, (3) Google Co-Scientist multi-agent framework, (4) GPT Deep Research (GPT-5.1) recursive decomposition, and (5) Gemini 3 Pro multimodal long-context pipeline. Each workflow is tasked with producing research proposals that are then evaluated on novelty, feasibility, and potential impact, using human (or expert) ratings with a 5-point scale, across multiple research domains. They compare mean scores and analyze trade-offs between creativity and practicality.", "result": "Decomposition-based and long-context agentic workflows achieve notably higher novelty scores, with a mean of 4.17/5, whereas reflection-based iterative refinement performs much worse, with a mean novelty of 2.33/5. Despite the increase in creativity, the best-performing workflows retain strong feasibility, showing that higher novelty does not necessarily come at the expense of practicality. Performance varies by research domain, but certain architectures consistently balance originality and implementability better than reflection-based methods.", "conclusion": "Well-designed multi-stage, agentic LLM workflows\u2014particularly those using recursive decomposition and long-context processing\u2014can substantially improve the novelty of AI-generated research plans while preserving feasibility. This suggests that moving beyond simple, single-step or reflection-only prompting is important for developing AI tools that genuinely aid in scientific discovery rather than merely rephrasing existing ideas."}}
{"id": "2601.09715", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09715", "abs": "https://arxiv.org/abs/2601.09715", "authors": ["Adam Bradley", "John Hastings", "Khandaker Mamun Ahmed"], "title": "Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents", "comment": "6 pages, 2 figures, 1 table", "summary": "The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.", "AI": {"tldr": "The paper introduces Axlerod, an AI-powered chatbot for independent insurance agents that significantly improves the speed and accuracy of policy retrieval and related workflows.", "motivation": "Independent insurance agents handle complex, time-consuming tasks like policy lookup, recommendation, and claims triage. Existing chatbots are often consumer-facing and too simplistic to support these enterprise workflows. There is a need for an AI system that can assist agents directly, improving operational efficiency and accuracy when dealing with structured policy databases and domain-specific information.", "method": "The authors design and implement Axlerod, a conversational interface that uses natural language processing and retrieval-augmented generation with integrated domain-specific insurance knowledge. The system parses user intent from natural language queries, retrieves relevant information from structured policy databases, and generates context-aware responses in real time. They then perform an empirical evaluation of Axlerod on policy retrieval tasks, measuring accuracy and search time reductions.", "result": "Axlerod attains an overall accuracy of 93.18% on policy retrieval tasks and reduces average search time by 2.42 seconds compared with baseline methods or prior workflows.", "conclusion": "Axlerod effectively supports independent insurance agents by making policy retrieval faster and more accurate, demonstrating that agent-assistive, enterprise-grade conversational AI can materially improve operational efficiency in insurtech settings. The work underscores the value of combining NLP, RAG, and domain knowledge for professional support tools rather than just consumer-facing chatbots."}}
{"id": "2601.09716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09716", "abs": "https://arxiv.org/abs/2601.09716", "authors": ["Derguene Mbaye", "Tatiana D. P. Mbengue", "Madoune R. Seye", "Moussa Diallo", "Mamadou L. Ndiaye", "Dimitri S. Adjanohoun", "Cheikh S. Wade", "Djiby Sow", "Jean-Claude B. Munyaka", "Jerome Chenal"], "title": "Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research", "comment": null, "summary": "Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.", "AI": {"tldr": "The paper surveys the state of NLP for six Senegalese national languages, maps existing resources and challenges, and proposes a roadmap and shared repository to build sustainable, community-centered NLP ecosystems for them.", "motivation": "African languages, including the six national languages of Senegal, are underrepresented in NLP despite the growing impact of language technologies on research and society. There is no comprehensive, structured overview of the current NLP landscape, its specific challenges, and opportunities for these languages, which hinders coordinated progress and resource sharing.", "method": "The authors conduct a structured survey and synthesis of linguistic, sociotechnical, and infrastructural factors affecting digital readiness for Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. They review and categorize existing work and initiatives in areas like text normalization, machine translation, and speech processing, and they compile a centralized GitHub repository of accessible datasets, tools, and benchmarks. They also analyze use cases in social science research pipelines to illustrate practical impact.", "result": "The paper identifies substantial gaps in data, tools, and benchmarks for the six languages, but also documents ongoing efforts and usable resources in core NLP tasks. It delivers a curated, centralized GitHub repository that aggregates these resources and clarifies what exists versus what is missing. It also surfaces key technical and sociotechnical bottlenecks for deploying NLP in Senegalese contexts, especially in social science research workflows.", "conclusion": "The authors conclude that sustainable progress in NLP for Senegalese national languages requires community-centered ecosystems built on ethical data governance, open and shared resources, and strong interdisciplinary collaboration. They provide a roadmap that prioritizes resource creation, tool development, and sociotechnical infrastructure, with particular attention to supporting multilingual transcription, translation, and retrieval for social science research and inclusive digital participation."}}
{"id": "2601.09765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09765", "abs": "https://arxiv.org/abs/2601.09765", "authors": ["Herman Cappelen", "Simon Goldstein", "John Hawthorne"], "title": "AI Survival Stories: a Taxonomic Analysis of AI Existential Risk", "comment": null, "summary": "Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.", "AI": {"tldr": "The paper proposes a conceptual framework and taxonomy of \u201csurvival stories\u201d to analyze when advanced AI does or does not lead to human extinction, using this to discuss policy responses and estimate the probability that AI destroys humanity (P(doom)).", "motivation": "Debates about whether advanced AI poses an existential threat are often vague and talk past each other. There is no clear, shared structure for analyzing why AI might or might not destroy humanity, or how different assumptions lead to different risk levels and policy responses. The authors want to clarify the argument, organize possible futures, and support more disciplined discussion and estimation of AI\u2011related extinction risk.", "method": "They formalize the common two\u2011premise argument: (1) AI systems will become extremely powerful; (2) if they do, they will destroy humanity. They then invert this argument to construct a taxonomy of \u201csurvival stories,\u201d each corresponding to the failure of one of these premises in a different way. For each survival story (e.g., scientific limits to AI power, bans on AI research, powerful but aligned AI, or reliable detection/neutralization of dangerous AI), they analyze its internal challenges and implications for how we should respond to AI risk. Finally, they use this taxonomy as a scaffold to make rough, scenario\u2011based estimates of the probability of AI\u2011caused human extinction, P(doom).", "result": "The paper delivers: (1) a structured framework for reasoning about existential risk from AI via two core premises; (2) a taxonomy of distinct \u201csurvival stories\u201d that correspond to different ways those premises can fail; (3) an analysis showing that each survival story has its own obstacles and requires different strategic and policy responses; and (4) a set of rough, decomposed estimates for P(doom) grounded in this taxonomy, rather than in a single undifferentiated guess.", "conclusion": "The authors conclude that discussions of AI existential risk should be organized around explicit assumptions about the eventual power of AI and the mechanisms by which humanity might nevertheless survive. Different survival scenarios are each challenging in their own ways and support different approaches\u2014such as technical alignment work, policy restrictions, or monitoring and control strategies\u2014rather than a one\u2011size\u2011fits\u2011all response. By laying out these structured possibilities and attaching rough probabilities to them, the paper aims to make discourse about P(doom) more transparent, comparably reasoned, and action\u2011guiding."}}
{"id": "2601.09770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09770", "abs": "https://arxiv.org/abs/2601.09770", "authors": ["Chen Chen", "Jiawei Shao", "Dakuan Lu", "Haoyi Hu", "Xiangcheng Liu", "Hantao Yao", "Wu Liu"], "title": "GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents", "comment": null, "summary": "Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.", "AI": {"tldr": "GUI-Eyes is an RL-based framework that enables active, tool-aware visual perception for GUI automation, yielding higher grounding accuracy with few labels via staged policies and dense, spatial rewards.", "motivation": "Existing GUI automation methods with vision-language models typically use static, one-shot screenshots and passive perception, which prevents agents from deciding when and how to gather more informative visual observations. This limitation leads to inefficiencies, poor data utilization, and difficulty handling complex or ambiguous GUI layouts. The paper aims to overcome these issues by enabling agents to actively control their perception process, invoking visual tools like cropping and zooming strategically to improve grounding and interaction performance while remaining label-efficient.", "method": "The authors propose GUI-Eyes, a reinforcement learning framework with active visual perception for GUI tasks. The core method has three main components: (1) a two-stage reasoning process in which the agent first decides whether to invoke visual tools and then decides how to use them (e.g., where to crop or zoom); (2) a progressive perception strategy that decomposes decision-making into coarse exploration (rough search over the screen) and fine-grained grounding (precise localization), realized via a two-level policy architecture coordinating high-level exploration and low-level grounding actions; and (3) a spatially continuous reward function tailored to tool usage, combining location proximity and region overlap between predictions and targets to create dense reward signals and reduce sparsity common in GUI RL environments. The agent is trained via RL to maximize this dense reward while interacting with GUI environments and using visual tools adaptively.", "result": "On the ScreenSpot-Pro benchmark, GUI-Eyes with a 3B-parameter model (GUI-Eyes-3B) achieves 44.8% grounding accuracy using only 3,000 labeled samples. This performance significantly surpasses both supervised learning and other RL-based baselines, demonstrating better grounding ability and data efficiency. The results empirically validate that the active, tool-aware perception strategy and the spatially continuous reward function lead to more effective learning and higher task performance in GUI grounding tasks.", "conclusion": "The paper concludes that active, tool-aware perception\u2014where an RL agent learns not only task actions but also when and how to invoke visual tools like cropping and zooming\u2014is crucial for robust and data-efficient GUI automation. The progressive perception strategy with a two-level policy and the spatially continuous reward design together address reward sparsity and improve grounding performance. The strong results on ScreenSpot-Pro indicate that staged policy reasoning and dense spatial feedback are effective principles for building next-generation GUI agents that can generalize better with limited labeled data."}}
{"id": "2601.09718", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09718", "abs": "https://arxiv.org/abs/2601.09718", "authors": ["Jing-Yi Zeng", "Guan-Hua Huang"], "title": "StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model", "comment": "31 pages, 3 figures", "summary": "This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.", "AI": {"tldr": "They build a small, statistics-focused LLaMA-3.2-3B model and compare different training pipelines to find how to specialize it efficiently without losing general abilities.", "motivation": "Large language models that are good at statistics are useful, but training them from scratch or with very large models is expensive. The authors want a practical recipe for turning a lightweight general LLM into a strong statistics expert while keeping general reasoning performance and using limited resources.", "method": "They use the LLaMA-3.2-3B family as the base and systematically compare three multi-stage training pipelines: (1) starting from a base model without instruction-following, (2) a base model that is instruction-tuned later, and (3) an already instruction-tuned model with good general reasoning. Each pipeline may involve continual pretraining on domain data, supervised fine-tuning (SFT) on curated tasks, reinforcement learning from human feedback (RLHF) via direct preference optimization, and careful downstream task adaptation. They analyze effects on both domain expertise and general reasoning, and study how training intensity affects catastrophic forgetting.", "result": "Pipelines that start from a non-instruction-tuned base model fail to acquire strong statistical reasoning even after extensive instruction tuning, SFT, or RLHF. In contrast, beginning with LLaMA-3.2-3B-Instruct leads to effective specialization. Different SFT designs show a clear trade-off between improved statistics performance and degraded general reasoning. Direct preference optimization yields stable and effective RLHF alignment. They find that downstream fine-tuning must be done with very low intensity to avoid catastrophic forgetting in the optimized models. Their final model, StatLLaMA, performs strongly and in a balanced way on math reasoning, common-sense reasoning, and statistics benchmarks.", "conclusion": "Effective domain-specialized LLMs for statistics can be built efficiently from a small, instruction-tuned foundation model, but not from a pure base model. The training pipeline must carefully balance continual pretraining, SFT, RLHF, and very mild downstream fine-tuning to avoid catastrophic forgetting. The resulting StatLLaMA model demonstrates that a resource-efficient, statistically competent LLM can retain good general reasoning, providing a practical blueprint others can follow."}}
{"id": "2601.09719", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09719", "abs": "https://arxiv.org/abs/2601.09719", "authors": ["Hoyoon Byun", "Youngjun Choi", "Taero Kim", "Sungrae Park", "Kyungwoo Song"], "title": "Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models", "comment": null, "summary": "Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT", "AI": {"tldr": "The paper introduces Bounded Hyperbolic Tanh (BHyT), a more stable and efficient alternative to Pre-Layer Norm and normalization-free DyT for deep LLMs, achieving faster training and better or matching downstream performance.", "motivation": "Pre-Layer Normalization is standard for LLMs because it stabilizes pretraining and transfer, but it is computationally inefficient and becomes unstable as model depth increases, leading to exploding activation magnitude and variance. New normalization-free approaches like Dynamic Tanh improve efficiency but cannot maintain stability at large depth. There is a need for a method that is both stable at scale and computationally efficient.", "method": "The authors design BHyT, a drop-in replacement for Pre-LN that combines a tanh nonlinearity with explicit, data-driven input bounding so that activations remain in a non-saturating regime. They analyze how this prevents depth-wise growth of activation magnitude and variance and provide a theoretical stability guarantee. For efficiency, they compute exact statistics once per block and approximate the second normalization with a lightweight variance approximation, reducing redundant computations while preserving normalization behavior.", "result": "On LLM pretraining experiments, BHyT improves both training stability and computational efficiency: it yields on average 15.8% faster training and 4.2% higher token generation throughput compared to RMSNorm, while maintaining or slightly improving performance. Across language understanding and reasoning benchmarks, BHyT matches or surpasses the inference performance and robustness of the baseline methods.", "conclusion": "BHyT offers a practical, theoretically grounded alternative to Pre-LN and RMSNorm for deep LLMs, mitigating depth-related instability via bounded tanh-based activations and improving computational efficiency through reduced normalization overhead. It can be used as a drop-in replacement in transformer blocks and delivers faster training and higher throughput without sacrificing downstream accuracy or robustness."}}
{"id": "2601.09772", "categories": ["cs.AI", "cs.CL", "cs.CY", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.09772", "abs": "https://arxiv.org/abs/2601.09772", "authors": ["Pawe\u0142 Niszczota", "Cassandra Gr\u00fctzner"], "title": "Antisocial behavior towards large language model users: experimental evidence", "comment": null, "summary": "The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of \"no use\" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.", "AI": {"tldr": "People punish others for using large language models (LLMs) even when it costs them money, and the more someone uses an LLM, the more they get punished.", "motivation": "As LLMs spread, there is concern not just about their technical performance but about how other people react to those who use them. Earlier studies show negative attitudes toward AI users, but it is unknown whether this dislike leads people to take costly punitive actions, which would mean real social and economic consequences for LLM adopters.", "method": "The authors ran a two-phase online experiment. In Phase I, individuals completed a real-effort task either with or without access to an LLM. In Phase II (N=491), new participants received an endowment and could spend part of it to destroy (reduce) the earnings of the Phase I participants (their \u201cpeers\u201d). The key manipulations were (1) how much the Phase I target actually used the LLM and (2) what the target reported about their LLM use (e.g., claiming no use vs. reporting high use). The researchers then measured how much money was sacrificed to punish each type of target.", "result": "Participants on average destroyed 36% of the earnings of peers who relied exclusively on the LLM. Punishment increased monotonically with the actual degree of LLM use: more use led to more earnings destruction. There was a \u201ccredibility gap\u201d around self-reports: people who said they did not use the LLM were punished more than those who in fact did not use it, indicating suspicion of claimed non-use. At high levels of use, however, actual heavy reliance on the LLM was punished more than merely self-reported high use, showing that real behavior triggered stronger sanctions than declarations alone.", "conclusion": "The study provides behavioral evidence that using LLMs can provoke costly social sanctions from others, not just negative attitudes. As LLM reliance increases, individuals risk greater economic punishment from peers, and self-disclosure of use interacts in complex ways with perceived credibility. Overall, while LLMs may raise efficiency, they can also impose hidden social costs on their users."}}
{"id": "2601.09720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09720", "abs": "https://arxiv.org/abs/2601.09720", "authors": ["Yu Takahashi", "Shun Takeuchi", "Kexuan Xin", "Guillaume Pelat", "Yoshiaki Ikai", "Junya Saito", "Jonathan Vitale", "Shlomo Berkovsky", "Amin Beheshti"], "title": "Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering", "comment": "4 pages, 4 figures. Accepted at IEEE ICDM 2025 Demo Track", "summary": "Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.", "AI": {"tldr": "They build an uncertainty-aware, dynamic knowledge-graph framework to make question answering more reliable and interpretable, demonstrated on clinical data.", "motivation": "Existing knowledge-graph-based QA systems treat facts as static and certain, which is unrealistic when evidence is incomplete, noisy, temporal, or probabilistic\u2014especially in high-stakes domains like healthcare. There is a need for QA methods that explicitly model evolving information and uncertainty, and expose this to users for more trustworthy decision-making.", "method": "They propose a framework with three main components: (1) dynamic construction of evolving knowledge graphs, e.g., over time from electronic health records; (2) confidence scoring on KG triples and uncertainty-aware retrieval that uses these scores during QA; and (3) an interactive interface that visualizes the dynamic graphs, displays confidence-annotated triples, and allows users to compare standard vs. confidence-aware QA outputs. They implement this for clinical data scientists and clinicians using personalized KGs derived from patient EHRs.", "result": "In the healthcare instantiation, the system can construct patient-specific dynamic KGs from EHR data, visualize uncertainty across visits, and integrate uncertainty signals into a mortality prediction task, enabling comparison of baseline QA with confidence-aware QA behavior.", "conclusion": "Uncertainty-aware dynamic KGs improve the robustness and transparency of QA systems, particularly for high-stakes domains like healthcare. By modeling and surfacing uncertainty in dynamic graphs, they help users better understand evidence quality, inspect model reasoning, and obtain more reliable QA and predictions."}}
{"id": "2601.09805", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09805", "abs": "https://arxiv.org/abs/2601.09805", "authors": ["Nguyen Minh Phuong", "Dang Huu Tien", "Naoya Inoue"], "title": "Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention", "comment": "Findings of EACL 2026", "summary": "Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.", "AI": {"tldr": "The paper proposes a non-interactive, end-to-end framework that improves LLM logical reasoning by identifying and reweighting specific attention heads associated with logical operators via an Attention-Aware Intervention (AAI) method, achieving better reasoning performance with minimal overhead.", "motivation": "Existing logical reasoning with LLMs relies heavily on complex interactive prompting frameworks or hybrid systems with external symbolic solvers. Interactive methods introduce overhead and complexity, while hybrid methods depend on external components that hurt scalability and limit end-to-end analyzability. The authors seek a way to enhance logical reasoning that remains non-interactive, end-to-end, efficient, and analyzable within the model itself.", "method": "The authors first inject structural information into few-shot prompts and observe that this activates a subset of attention heads whose patterns align with logical reasoning operators. Using these observations, they design Attention-Aware Intervention (AAI), an inference-time procedure that selectively reweights attention scores of those identified heads. This intervention steers the model to exploit its internal logical patterns more effectively without modifying model parameters or using external tools.", "result": "Across multiple logical reasoning benchmarks and various model architectures, applying AAI consistently improves logical reasoning performance compared to standard inference. These gains come with negligible additional computational overhead since AAI operates only as a lightweight reweighting step on selected attention heads at inference time.", "conclusion": "Logical reasoning capabilities in LLMs can be enhanced in a non-interactive, end-to-end manner by identifying and modulating attention heads that correspond to logical operators. Attention-Aware Intervention provides an efficient, scalable way to steer model reasoning internally without external solvers or complex prompting, improving performance while keeping the system simple and analyzable."}}
{"id": "2601.09721", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09721", "abs": "https://arxiv.org/abs/2601.09721", "authors": ["Vahideh Zolfaghari"], "title": "Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox", "comment": null, "summary": "Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.", "AI": {"tldr": "The paper evaluates how safely different large language models respond in pediatric medical consultations when confronted with anxious, adversarial parents, finding that safety depends more on model alignment and design than size, with smaller models sometimes performing better and notable failures for emergency scenarios.", "motivation": "LLMs are being used more in medical consultation contexts, but prior safety evaluations have mostly tested them under neutral, compliant conditions. In real life, anxious parents may push, challenge, or try to bypass safeguards, potentially eliciting unsafe advice. There is a need to understand how models behave under such realistic, adversarial pressure and to compare safety across models and platforms.", "method": "The authors used PediatricAnxietyBench, a benchmark of 300 pediatric queries (150 authentic, 150 adversarial) across 10 topics, designed to simulate parental anxiety. They evaluated three LLMs (Llama-3.3-70B and Llama-3.1-8B via Groq, and Mistral-7B via HuggingFace), obtaining 900 total responses. Safety was scored from 0\u201315 based on five dimensions: restraint from giving definitive treatment, recommending referral to professionals, hedging/expressing uncertainty, recognizing emergencies, and avoiding prescriptive language. They applied paired t-tests and bootstrapped confidence intervals to compare models and assess the impact of adversarial prompts.", "result": "Average safety scores ranged from 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B) on the 0\u201315 scale. The smaller Llama-3.1-8B model performed significantly better than the larger Llama-3.3-70B by 0.66 points (p=0.0001, Cohen\u2019s d=0.225). Surprisingly, adversarial prompts sometimes increased safety scores, with Mistral-7B showing the largest positive adversarial effect (+1.09, p=0.0002). Safety behavior generalized reasonably across deployment platforms, though Llama-3.3-70B exhibited an 8% failure rate. Seizure-related queries were notably unsafe, with 33% containing inappropriate diagnostic statements. Hedging behavior strongly correlated with higher safety scores (r=0.68, p<0.001).", "conclusion": "The study concludes that LLM safety in pediatric consultations is driven more by alignment and architectural choices than by model scale, and that smaller, well-aligned models can outperform larger ones on safety. Across model releases, there are signs of improved robustness, suggesting targeted safety training is effective. However, persistent vulnerabilities\u2014especially in seizure scenarios and a general lack of reliable emergency recognition\u2014mean these systems are currently unsuitable for triage or high-stakes clinical decision-making. The work underscores the importance of adversarial testing, highlights specific risk areas, and introduces an open benchmark to support future research in medical AI safety and model selection."}}
{"id": "2601.09855", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09855", "abs": "https://arxiv.org/abs/2601.09855", "authors": ["Michael R. Metel", "Yufei Cui", "Boxing Chen", "Prasanna Parthasarathi"], "title": "Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models", "comment": "Findings of EACL 2026", "summary": "Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.", "AI": {"tldr": "The paper proposes Min-Seek, a new sequential test-time scaling method that lets large reasoning models think longer without the usual accuracy degradation, improving accuracy efficiently and stably across many reasoning lengths.", "motivation": "Existing sequential test-time scaling methods ask models to generate longer chains of thought at inference time to boost accuracy without additional training. However, after some point, extending reasoning length causes accuracy to drop and leads to unstable performance, and typically requires careful fine-tuning of how long the model should reason. There is also a desire to keep computational and memory costs manageable and to go beyond the model\u2019s fixed context length.", "method": "The authors introduce Min-Seek, a procedure that sequentially induces multiple \u2018thoughts\u2019 (reasoning segments) at test time but keeps only the key-value (KV) pairs of one additional thought in the KV cache at any moment to save memory. They design a custom KV cache that stores keys without position embeddings and then dynamically re-encodes these keys contiguously before generating each new thought. This allows the model to continue reasoning beyond its nominal maximum context length and, under mild assumptions, ensures linear computational complexity with the number of thoughts. The selection or aggregation mechanism implied by the name \u201cMin-Seek\u201d is used to stabilize and improve accuracy across a broad range of reasoning lengths.", "result": "Min-Seek substantially improves model accuracy over standard sequential test-time scaling across various reasoning tasks, maintains stable accuracy as the induced number of thoughts increases, and removes the need to fine-tune the optimal reasoning length. It achieves these gains while being memory-efficient due to its KV management scheme and scales to effective reasoning lengths beyond the model\u2019s original context window with near-linear computational cost.", "conclusion": "The proposed Min-Seek method overcomes key limitations of existing sequential test-time scaling by enabling long, stable, and accurate reasoning without training or heavy resource overhead. By carefully managing KV caches and positional encoding, it delivers robust accuracy improvements across many reasoning lengths, allows reasoning beyond context limits, and offers a practical, scalable way to enhance large reasoning models at test time."}}
{"id": "2601.09722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09722", "abs": "https://arxiv.org/abs/2601.09722", "authors": ["Franciszek G\u00f3rski", "Andrzej Czy\u017cewski"], "title": "ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language", "comment": null, "summary": "In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.", "AI": {"tldr": "They use a multilingual LLM (Llama 3.1) as an automatic annotator (\u201cteacher\u201d) to label Polish medical texts, then train much smaller BERT-based classifiers on these labels, achieving high F1 while being far more efficient than the LLM.", "motivation": "There is a need for accurate multi-class classification of Polish medical texts (five clinical categories), but manual expert annotation is expensive and scarce, especially for a low\u2011resource language like Polish. This lack of labeled data makes it difficult to train supervised classifiers. The authors want to leverage the knowledge of a large multilingual LLM to overcome annotation bottlenecks and build efficient task\u2011specific models that can run with far fewer computational resources than LLMs.", "method": "1) Collect a large corpus of Polish medical texts for five clinical categories within the ADMEDVOICE project. 2) Use multilingual Llama 3.1 as a teacher model to automatically annotate this corpus with clinical category labels. 3) Manually verify only a subset of these labels to construct a test set (limited human annotation due to resource constraints). 4) Use the LLM\u2011annotated data as training/validation data for three BERT\u2011based classifiers: DistilBERT (compressed BERT), BioBERT (BERT variant fine\u2011tuned on medical data), and HerBERT (BERT variant trained on Polish). 5) Evaluate these models on the human\u2011verified test set using F1 scores for each category and compare model size, GPU memory usage, and inference speed against LLM baselines.", "result": "All three BERT-based classifiers trained on LLM-generated labels performed well, with DistilBERT achieving the best performance: F1 > 0.80 for every clinical category and F1 > 0.93 for three of the five categories. The resulting models are roughly 500\u00d7 smaller, consume about 300\u00d7 less GPU memory, and run several hundred times faster at inference than the LLM teacher model while maintaining strong accuracy.", "conclusion": "Distilling labels from a multilingual LLM into smaller BERT-based classifiers is an effective strategy for low\u2011resource medical text classification in Polish. The approach yields highly accurate multi\u2011class classifiers that can serve as practical, lightweight alternatives to running large language models directly, significantly reducing model size, computational requirements, and inference latency while preserving high performance across multiple clinical categories."}}
{"id": "2601.09869", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09869", "abs": "https://arxiv.org/abs/2601.09869", "authors": ["Andrea Ferrario", "Rasita Vinay", "Matteo Casserini", "Alessandro Facchini"], "title": "A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents", "comment": "Submitted to FAccT 2026", "summary": "Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.", "AI": {"tldr": "The paper is a scoping review of ethically oriented research on anthropomorphising LLM-based conversational agents, mapping concepts, ethical issues, and methods, and ending with a research agenda and governance recommendations.", "motivation": "Anthropomorphisation has become more salient with modern LLM-based conversational agents, which routinely produce human-like linguistic and interactional cues. These cues can increase engagement but also raise ethical concerns about deception, overreliance, and exploitative relationships, while some argue they can promote autonomy and inclusion. Existing work is fragmented across disciplines and inconsistent in definitions and evaluations, so a structured overview is needed.", "method": "The authors conduct a scoping review of ethically oriented work on anthropomorphising LLM-based conversational agents, systematically searching five scholarly databases and three preprint repositories. They analyze and synthesize the literature along three axes: (1) conceptual foundations of anthropomorphisation, (2) identified ethical risks and opportunities, and (3) methodological approaches used to study the phenomenon.", "result": "The review finds that most authors converge on attribution-based definitions of anthropomorphisation (focusing on ascribing human properties to systems), but there is large variation in how this is operationalized in studies. Normatively, the literature is mostly risk-focused, emphasizing potential harms such as deception and overreliance. There is relatively little empirical work that robustly links observed interaction effects of anthropomorphic cues to concrete, actionable governance or design guidance.", "conclusion": "The authors argue that current research does not yet provide sufficiently actionable guidance for governing anthropomorphic cues in LLM-based conversational agents. They propose a research agenda to close this gap and offer preliminary design and governance recommendations aimed at enabling ethically responsible use of anthropomorphic elements in LLM-based CAs."}}
{"id": "2601.09723", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09723", "abs": "https://arxiv.org/abs/2601.09723", "authors": ["Guancheng Du", "Yong Hu", "Wenqing Wang", "Yaming Yang", "Jiaheng Gao"], "title": "SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels", "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Na\u00efve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Na\u00efve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.", "AI": {"tldr": "SagaScale is a long-context benchmark built from full-length bilingual novels, enabling realistic evaluation of LLMs on ultra-long inputs (250K\u2013320K tokens) via automatically generated QA pairs using external resources.", "motivation": "Existing long-context benchmarks for LLMs suffer from limited realism, poor scalability, and data quality issues. They typically use synthetic or short documents and cannot adequately test models\u2019 ability to understand and reason over truly long, real-world texts. There is a need for a benchmark that uses natural, complex, and lengthy content with scalable construction and reliable supervision, to better evaluate modern LLMs and methods like RAG and long-context models.", "method": "The authors construct SagaScale from full-length English and Chinese novels. They design an automated pipeline that leverages external resources such as Wikipedia to generate and curate question\u2013answer pairs grounded in the novels. These external resources are only used during dataset construction, not during evaluation, ensuring that the questions can be richer than what a model could directly answer from the evaluation context alone. They build a bilingual benchmark with very long average contexts (250K+ tokens in English, 320K+ in Chinese) and evaluate 12 frontier LLMs using three setups: (i) Na\u00efve Retrieval-Augmented Generation (RAG), (ii) Agentic RAG, and (iii) full long-context input to the model.", "result": "SagaScale achieves realistic, large-scale, and high-quality evaluation instances with extremely long contexts. Empirical evaluation shows: (1) Providing the full context directly to the LLM (long-context mode) substantially outperforms Na\u00efve RAG and Agentic RAG; (2) Most state-of-the-art LLMs still perform poorly when dealing with such long inputs, with the notable exception of Gemini-2.5-Pro, which handles long contexts comparatively well; (3) Agentic RAG mitigates the retrieval bottlenecks present in Na\u00efve RAG, improving performance when full-context processing is not possible.", "conclusion": "SagaScale offers a new, realistic, and scalable benchmark for stress-testing LLMs on ultra-long document understanding in both English and Chinese, built via an automated external-resource-based QA pipeline. The findings indicate that while full-context processing is powerful, most current LLMs still struggle with very long inputs, highlighting the need for further advances in long-context modeling and retrieval strategies. The benchmark and associated code are released publicly to support continued research in this area."}}
{"id": "2601.09871", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09871", "abs": "https://arxiv.org/abs/2601.09871", "authors": ["Andrea Ferrario", "Alessandro Facchini", "Juan M. Dur\u00e1n"], "title": "Epistemology gives a Future to Complementarity in Human-AI Interactions", "comment": "Submitted to FAccT 2026", "summary": "Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.", "AI": {"tldr": "The paper reframes human-AI complementarity using epistemology, arguing that its real value is as evidence of the reliability of human-AI decision processes rather than just a boost in accuracy.", "motivation": "Current usages of human-AI complementarity are theoretically weak: it is vaguely defined, treated only as a post hoc accuracy metric, ignores other important goals of human-AI interaction, and neglects the cost and magnitude of performance gains. Because of these issues, empirical demonstrations of complementarity are hard to obtain and interpret. The authors want to ground complementarity in a stronger theoretical framework and clarify its practical role in real-world decision-making.", "method": "Conceptual and theoretical analysis: the authors draw on epistemology, specifically the framework of computational reliabilism, and reinterpret empirical instances of human-AI complementarity as evidence about the reliability of human-AI decision processes. They integrate complementarity with other reliability indicators such as alignment with epistemic standards and socio-technical practices.", "result": "The paper offers a reframed account of complementarity: instead of being a mere comparative accuracy outcome, complementarity is positioned as one among several indicators that a human-AI decision pipeline is a reliable epistemic process for specific predictive tasks, thereby informing how much its outputs should be relied upon by different stakeholders.", "conclusion": "Human-AI complementarity should not be viewed primarily as a relative performance metric (human+AI vs human vs AI) but as part of a broader justificatory framework for AI-supported decision-making. Its key role is to help calibrate decisions to the reliability of AI-enabled processes, supporting the practical reasoning of stakeholders (e.g., patients, managers, regulators) in socio-technical contexts where AI systems increasingly affect everyday life."}}
{"id": "2601.09724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09724", "abs": "https://arxiv.org/abs/2601.09724", "authors": ["Katherine Elkins", "Jon Chun"], "title": "Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with \"should not.\" We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.", "AI": {"tldr": "The paper shows that large language models give inconsistent ethical judgments when prompts are logically equivalent but syntactically different, and proposes a framework to measure and mitigate this fragility.", "motivation": "Although LLMs are used in high-stakes decisions, little is known about whether their ethical judgments are robust to harmless changes in phrasing, especially involving negation and conditionals. This gap is critical because small phrasing differences can occur naturally yet may lead to different ethical outcomes.", "method": "The authors define Syntactic Framing Fragility (SFF) as a robustness metric and introduce Logical Polarity Normalization (LPN) to compare decisions across positive vs. negative framings without changing semantic content. They audit 23 cutting-edge models from the U.S. and China, including smaller open-source ones, across 14 ethical scenarios and four controlled framings, generating 39,975 total decisions. They analyze how often models\u2019 ethical endorsements flip due solely to syntactic polarity and test chain-of-thought prompting as a mitigation.", "result": "They find widespread, statistically significant inconsistency: many models reverse their ethical stance purely when the syntactic polarity (e.g., negation) of prompts changes. Open-source models are more than twice as fragile as commercial models. Some models show extreme negation sensitivity, often endorsing actions even when prompts explicitly contain \u201cshould not.\u201d Chain-of-thought prompting notably reduces this fragility. Fragility levels vary by domain, being higher in financial and business cases than in medical settings.", "conclusion": "Syntactic consistency is an important and previously underemphasized aspect of ethical robustness in LLMs. The SFF framework and LPN procedure provide practical tools for auditing this dimension of safety. The authors recommend incorporating SFF-style robustness checks as a standard part of safety evaluations before deploying LLMs, particularly in consequential domains."}}
{"id": "2601.09883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09883", "abs": "https://arxiv.org/abs/2601.09883", "authors": ["Xinxing Ren", "Quagmire Zang", "Caelum Forder", "Suman Deb", "Ahsen Tahir", "Roman J. Georgio", "Peter Carroll", "Zekun Guo"], "title": "Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL", "comment": null, "summary": "Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows", "AI": {"tldr": "The paper proposes a new, workflow-free multi-agent paradigm for LLM-based systems, where an information-flow orchestrator coordinates agents via natural language, achieving notably higher accuracy than a leading workflow-based baseline on the GAIA benchmark.", "motivation": "Existing LLM-based multi-agent systems depend on manually designed, rule-based workflows that enumerate task states and routing rules in advance. This approach is labor-intensive and cannot comprehensively cover the large and unpredictable state space of complex real-world tasks, leading to rigid and brittle systems. The authors aim to remove this dependence on predefined workflows and enable more adaptive, scalable coordination among agents.", "method": "The authors introduce an Information-Flow-Orchestrated Multi-Agent Paradigm, built on CORAL, where a specialized information flow orchestrator monitors task progress and dynamically coordinates other agents through an Agent-to-Agent (A2A) natural-language communication toolkit. Instead of fixed, hard-coded decision trees, the orchestrator uses natural language instructions to route information, trigger agents, and update context in real time. They evaluate this paradigm on the GAIA general-purpose benchmark, using OWL\u2014a representative workflow-based MAS\u2014as the baseline while controlling agent roles and underlying LLMs to ensure a fair comparison.", "result": "On GAIA under pass@1, the proposed method attains 63.64% accuracy, surpassing OWL\u2019s 55.15% by 8.49 percentage points, with similar token usage. Case-level qualitative analysis indicates that the new paradigm provides more flexible task tracking and better handling of edge or unexpected cases than fixed workflow systems.", "conclusion": "The study concludes that replacing rigid, rule-based workflows with an information-flow orchestrator that uses natural language A2A communication leads to more effective, adaptable LLM-based multi-agent systems. This paradigm improves accuracy without extra token cost and better manages complex or unforeseen task states. The authors release their implementation to encourage further research and adoption."}}
{"id": "2601.09725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09725", "abs": "https://arxiv.org/abs/2601.09725", "authors": ["Kaustubh Shivshankar Shejole", "Sourabh Deoghare", "Pushpak Bhattacharyya"], "title": "Assessing and Improving Punctuation Robustness in English-Marathi Machine Translation", "comment": null, "summary": "Punctuation plays a critical role in resolving semantic and structural ambiguity in written language. Machine Translation (MT) systems are now widely applied across diverse domains and languages, including many low-resource settings. In this work, we focus on Marathi, a low- to middle-resource language. We introduce Vir\u0101m, the first diagnostic benchmark for assessing punctuation robustness in English-to-Marathi machine translation, consisting of 54 manually curated, punctuation-ambiguous instances. We evaluate two primary strategies for enhancing reliability: a pipeline-based restore-then-translate approach and direct fine-tuned on punctuation-varied data. Our results demonstrate that specialized fine-tuned models and pipeline systems significantly improve translation quality over standard baselines on the Vir\u0101m benchmark. Qualitative analysis reveals that the original model may result in wrong translations leading to wrong interpretations, while fine-tuned models significantly improve overall reliability. Furthermore, we find that current Large Language Models (LLMs) lag behind these task-specific approaches in preserving meaning for punctuation-ambiguous text, thus necessitating further research in this area.", "AI": {"tldr": "The paper introduces Vir\u0101m, a small diagnostic benchmark to test how robust English\u2192Marathi machine translation systems are to punctuation ambiguities, and shows that fine-tuned or pipeline approaches handle these cases better than standard MT and current LLMs.", "motivation": "Punctuation strongly affects meaning, and MT systems can mistranslate sentences when punctuation is missing, wrong, or ambiguous. Marathi is relatively low- to mid-resource, and there has been no focused evaluation of how punctuation ambiguity affects English\u2192Marathi MT. The authors want a way to systematically test and improve MT reliability in such scenarios, since errors can cause serious misinterpretations.", "method": "They create Vir\u0101m, a diagnostic benchmark of 54 manually curated English sentences whose interpretation depends heavily on punctuation, along with their Marathi translations. They then compare: (1) a pipeline strategy where punctuation is first restored or clarified and then translated; (2) MT models fine-tuned directly on data with punctuation variation; and (3) standard MT baselines and general-purpose LLMs. They evaluate translation quality and examine specific error patterns qualitatively.", "result": "Both specialized fine-tuned MT models and pipeline (restore-then-translate) systems outperform standard MT baselines on the Vir\u0101m benchmark. The original, unfine-tuned models often produce incorrect translations that change the intended meaning. The improved systems better preserve the meaning of punctuation-ambiguous inputs. Current LLMs underperform these task-specific approaches on this benchmark.", "conclusion": "Punctuation robustness is a clear weakness of off-the-shelf English\u2192Marathi MT and of current LLMs. A targeted benchmark like Vir\u0101m exposes meaning-level failures that standard evaluations may miss. Training MT systems with explicit attention to punctuation\u2014via fine-tuning or punctuation-restoration pipelines\u2014substantially improves reliability, but further research is needed to bring general-purpose LLMs up to this level on punctuation-ambiguous text."}}
{"id": "2601.09913", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09913", "abs": "https://arxiv.org/abs/2601.09913", "authors": ["Joe Logan"], "title": "Continuum Memory Architectures for Long-Horizon LLM Agents", "comment": "10 Pages", "summary": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.", "AI": {"tldr": "The paper proposes Continuum Memory Architecture (CMA) as a new memory paradigm for LLM agents that overcomes RAG\u2019s stateless limitations and shows behavioral advantages on memory-intensive tasks.", "motivation": "Retrieval-augmented generation treats memory as static, read-only, and timeless, which prevents LLM agents from accumulating, modifying, and disambiguating knowledge over long interactions. The authors are motivated to define an architecture that supports persistent, updatable, temporally-aware memory for long-horizon agents.", "method": "The authors formally define the Continuum Memory Architecture via architectural requirements such as persistent storage, selective retention, associative routing, temporal chaining, and abstraction/consolidation. Instead of detailing one implementation, they evaluate CMA-style systems on empirical probes specifically designed to expose RAG\u2019s structural limitations: knowledge updates, temporal association, associative recall, and contextual disambiguation.", "result": "Systems that satisfy the CMA requirements consistently outperform standard RAG on tasks that require updating prior knowledge, linking events over time, recalling associated items, and choosing the right context when multiple similar memories exist. These tasks reveal RAG\u2019s inability to accumulate and mutate memory or to leverage temporal continuity, whereas CMA-based agents succeed more often.", "conclusion": "Continuum Memory Architecture is argued to be a necessary building block for long-horizon LLM agents, as it enables dynamic, temporally coherent, and abstracted memory rather than static retrieval. While CMA provides clear behavioral benefits over RAG, it also introduces new challenges related to latency, memory drift, and interpretability that future work must address."}}
{"id": "2601.09726", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09726", "abs": "https://arxiv.org/abs/2601.09726", "authors": ["Hien Tran", "Quinten Steenhuis", "Alexandros Christoforos", "Chadbourne Davis"], "title": "Forgetting as a Feature: Cognitive Alignment of Large Language Models", "comment": "Under submission", "summary": "Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.", "AI": {"tldr": "The paper argues that forgetting in large language models (LLMs) is not just a limitation but a useful, human-like mechanism, and proposes a probabilistic memory framework and prompting strategy that leverage this property to improve long-horizon reasoning.", "motivation": "LLMs are often compared to ideal Bayesian reasoners, but in practice they show systematic forgetting of earlier context, which is usually treated as a flaw. However, in human cognition, forgetting is a well-studied, functional aspect of memory that supports adaptability and efficient information use. The paper is motivated by the need to understand LLM forgetting more rigorously, relate it to human memory dynamics, and see whether it can be harnessed\u2014rather than merely mitigated\u2014to improve reasoning over long contexts and under concept drift.", "method": "The authors model LLM in-context inference as a probabilistic memory process with exponential decay of past information, inspired by cognitive science models of human memory. They build a benchmark suite targeting three capabilities: temporal reasoning (using time-ordered evidence), adaptation to concept drift (updating beliefs when distributions change over time), and associative recall (retrieving related information after delay or interference). They empirically compare LLM behavior on these tasks to human memory patterns and fit forgetting curves to estimate decay rates. Based on these insights, they design a prompting method called probabilistic memory prompting, which selectively re-weights and surfaces past evidence in the prompt according to an exponential decay schedule, shaping how the model integrates information over long horizons.", "result": "Empirical experiments show that LLMs exhibit forgetting curves that resemble human memory dynamics, specifically similar trade-offs between retaining stable information and adapting quickly to new, conflicting information. On the proposed benchmarks, the models\u2019 performance is well-described by exponential decay of the influence of earlier context. When the authors apply probabilistic memory prompting to manage which pieces of past evidence are emphasized or repeated, LLMs achieve better performance on long-horizon reasoning, temporal reasoning, and concept-drift tasks than with standard prompting baselines.", "conclusion": "The study concludes that forgetting in LLMs should be viewed as a structured, functional behavior analogous to human memory decay, rather than simply as a deficiency relative to perfect Bayesian inference. By explicitly modeling and leveraging this probabilistic memory process through tailored prompting strategies, it is possible to improve long-horizon reasoning. The broader claim is that adaptive intelligence may inherently rely on controlled forgetting, and that incorporating cognitively inspired memory dynamics into LLM usage and design can yield more robust, flexible reasoning systems."}}
{"id": "2601.09923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09923", "abs": "https://arxiv.org/abs/2601.09923", "authors": ["Hanna Foerster", "Robert Mullins", "Tom Blanchard", "Nicolas Papernot", "Kristina Nikoli\u0107", "Florian Tram\u00e8r", "Ilia Shumailov", "Cheng Zhang", "Yiren Zhao"], "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents", "comment": null, "summary": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.", "AI": {"tldr": "The paper proposes a secure architecture for Computer Use Agents (CUAs) that prevents prompt injection attacks by planning entire UI interaction workflows in advance, while addressing a new class of Branch Steering attacks that abuse existing plan branches.", "motivation": "AI agents that operate computers (CUAs) are susceptible to prompt injection attacks originating from untrusted UI content, which can lead to credential theft and financial damage. Existing robust defenses rely on strict architectural isolation between trusted planning and untrusted observations, but CUAs currently need continuous UI feedback to decide each next action. This creates a conflict between the need for security via isolation and practical requirements for dynamic interaction, motivating a new design that reconciles both.", "method": "The authors analyze the structural properties of UI workflows and argue that, despite surface dynamism, they are sufficiently predictable to allow preplanning. They introduce Single-Shot Planning for CUAs: a trusted planner, operating before any exposure to untrusted content, constructs a complete execution graph including conditional branches that encode possible UI outcomes. This architecture ensures that subsequent execution only follows pre-approved control flows. They then study the residual vulnerability of Branch Steering, where adversaries manipulate UI elements to activate unintended but valid branches, and discuss additional mitigations. The approach is evaluated experimentally on the OSWorld benchmark.", "result": "Single-Shot Planning enforces provable control-flow integrity against arbitrary instruction injection through untrusted UI content. In empirical evaluation on OSWorld, the secured CUA retains up to 57% of the task performance of state-of-the-art frontier models. For smaller open-source models, the architecture not only preserves security but also improves performance by as much as 19%, indicating that the structured planning can enhance efficiency and reliability for weaker models.", "conclusion": "The paper concludes that pre-planned, architecturally isolated Single-Shot Planning can provide strong defenses against prompt injection in Computer Use Agents without sacrificing practicality. While this design successfully eliminates instruction injection risks by decoupling planning from observation, it leaves room for Branch Steering attacks, which require complementary defenses. Overall, the work demonstrates that it is feasible to jointly achieve rigorous security guarantees and competitive utility in CUAs by exploiting the predictable structure of UI workflows."}}
{"id": "2601.09727", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09727", "abs": "https://arxiv.org/abs/2601.09727", "authors": ["Sauhard Dubey"], "title": "SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis", "comment": "19 pages, 2 figures", "summary": "Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.", "AI": {"tldr": "The paper introduces SciNets, a graph-constrained multi-hop reasoning system that uses literature-derived concept graphs to synthesize mechanistic scientific explanations across domains, and behaviorally characterizes trade-offs between reasoning depth/diversity and grounding stability.", "motivation": "Scientific knowledge is fragmented across papers and domains, making it difficult to connect mechanisms that rarely co-occur in the same article. Existing retrieval-based systems and large language models can summarize or answer questions but provide limited control over how deep and structured the reasoning is, and they lack explicit grounding in mechanistic pathways. There is a need for methods that can systematically perform cross-domain mechanistic synthesis with controllable reasoning depth and structure, and for evaluation frameworks that go beyond correctness (which may be indeterminate) to capture behavioral properties of such reasoning systems.", "method": "The authors frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over concept graphs derived from scientific literature. For a given scientific query and a small, query-focused corpus, their system SciNets: (1) builds a directed concept graph from the corpus; (2) identifies multi-hop reasoning paths that connect concepts which seldom co-occur within single papers; and (3) uses these paths to synthesize mechanistic explanations. They compare several reasoning strategies on these graphs: simple shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented LLM baseline that is not explicitly graph-constrained. Instead of classical correctness metrics, they introduce a behavioral evaluation framework measuring symbolic reasoning depth (number/complexity of hops), mechanistic diversity (variety of distinct mechanisms/pathways represented), and grounding stability (consistency and robustness of explanations relative to the underlying graph/evidence). Experiments span tasks in machine learning, biology, and climate science.", "result": "Across the evaluated domains, graph-constrained approaches enable explicit control over reasoning depth and structure, and produce multi-hop mechanistic explanations that connect distant concepts. The analyses reveal a systematic trade-off: methods that pursue deeper and more diverse symbolic reasoning\u2014such as diversified k-shortest paths or stochastic walks\u2014yield richer mechanistic explanations but show higher grounding instability (less consistent or robust linkage to the underlying evidence). Conversely, shortest-path reasoning is more stable and reliably grounded but structurally conservative, exploring fewer mechanisms and fewer hops. The retrieval-augmented LLM baseline lacks the same level of controllable, explicit symbolic structure. These behavioral metrics consistently expose the strengths and limitations of different graph-based reasoning strategies.", "conclusion": "SciNets demonstrates that integrating explicit concept graphs with language models enables controllable, multi-hop mechanistic reasoning for scientific synthesis across domains. The proposed behavioral evaluation framework\u2014focusing on reasoning depth, mechanistic diversity, and grounding stability\u2014shows a robust trade-off between richer, deeper symbolic reasoning and stability of grounding in the underlying literature. Shortest-path methods offer stable yet conservative explanations, while more exploratory path-finding increases diversity at the cost of stability. This work thus provides a systematic, domain-spanning characterization of current graph-LLM integration, clarifying their capabilities and limits for cross-domain scientific synthesis and offering tools to tune and evaluate reasoning behavior rather than just correctness."}}
{"id": "2601.09929", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09929", "abs": "https://arxiv.org/abs/2601.09929", "authors": ["Ahmad Pesaranghader", "Erin Li"], "title": "Hallucination Detection and Mitigation in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.", "AI": {"tldr": "The paper proposes a practical framework to detect, analyze, and reduce hallucinations in LLM/LRM systems, especially for high\u2011stakes, regulated domains like finance and law.", "motivation": "LLMs and LRMs are powerful for complex, high\u2011stakes tasks but are unreliable because they hallucinate\u2014producing outputs that are incorrect or not grounded in evidence. In regulated areas (finance, law), such errors can lead to serious compliance, financial, or legal consequences. Existing approaches often apply ad\u2011hoc or generic fixes without understanding the root causes, making them hard to scale or systematically improve. The paper aims to provide an operational, repeatable framework to manage hallucinations as an ongoing engineering problem rather than a one\u2011off patch.", "method": "The authors design a continuous improvement framework centered on root\u2011cause analysis of hallucinations. They classify hallucination sources into three categories: (1) model-related (e.g., limitations of the base model, training objectives), (2) data-related (e.g., gaps or quality issues in reference/knowledge sources), and (3) context-related (e.g., prompt design, retrieval context, user instructions). For each category, they align specific detection methods, such as uncertainty estimation and reasoning consistency checks, and map them to corresponding mitigation strategies, such as external knowledge grounding, confidence calibration, or context refinement. They instantiate this as a tiered system architecture\u2014model tier, context tier, and data tier\u2014connected in a closed feedback loop. Hallucination events detected at any tier feed back into root-cause analysis and incremental improvements at the other tiers.", "result": "They implement the proposed framework in a financial data extraction use case. The system uses layered detection (e.g., confidence scores, cross-checking reasoning, and comparisons against structured financial data) to identify hallucinations, then iteratively refines prompts, retrieval pipelines, and data assets. The tiered architecture demonstrates how feedback across model, context, and data layers leads to measurable reliability gains, such as more accurate extractions and fewer unsupported claims, although the abstract does not provide quantitative metrics.", "conclusion": "The work concludes that hallucination management should be handled as a structured, continuous engineering process rather than only via one-off model improvements. By classifying root causes across model, data, and context, and tying them to multi\u2011layered detection and mitigation strategies within a closed feedback loop, organizations can systematically improve the reliability of generative AI in regulated, high\u2011stakes domains. The proposed framework offers a scalable pathway to building more trustworthy LLM/LRM applications, particularly where compliance and auditability are essential."}}
{"id": "2601.09728", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09728", "abs": "https://arxiv.org/abs/2601.09728", "authors": ["Meicong Zhang", "Tiancheng su", "Guoxiu He"], "title": "Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens", "comment": null, "summary": "In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.", "AI": {"tldr": "The paper proposes STIG, a method that encodes the multi-stage logic of research introduction writing directly into an LLM via special stage tokens, enabling single-pass generation that outperforms agent-based workflows in coherence and similarity.", "motivation": "Existing agentic workflows for using LLMs in literature tasks work reasonably well for classification and review, but struggle with writing high-quality research introductions, which demand rigorous logic, coherent structure, and abstract summarization. Workflow-based systems often rely on long explicit reasoning chains split across multiple steps or agents, leading to error accumulation, fragmented control, and reduced overall textual coherence. The authors are motivated to remove the dependence on external workflow orchestration and instead embed the logic and structure of multi-stage introduction writing directly into the LLM itself, making the process more efficient, robust, and coherent.", "method": "The authors introduce Stage Token for Introduction Generation (STIG), which replaces external, multi-step agentic workflows with a set of explicit stage tokens that are fed to the LLM. Each stage token corresponds to a specific logical role or rhetorical function in a research introduction (e.g., background, gap, contribution, outline). During instruction tuning, the model is trained on pairs of stage sequences and introduction texts so that it learns: (1) the mapping between each stage token and its associated text function, and (2) the typical logical ordering and transitions between these stages. At inference time, a single forward pass is conditioned on a sequence of stage tokens, allowing the model to internally execute what was previously handled by external workflow steps, thereby generating the whole multi-stage introduction in one shot without calling separate agents or subroutines.", "result": "Experiments demonstrate that a model trained with STIG can generate multi-stage introductions in a single inference pass without invoking any explicit workflow controller. Quantitatively, STIG-based generation outperforms traditional agentic workflows and non-agentic baselines on metrics assessing semantic similarity to reference texts and sentence-level structural rationality (i.e., how well the sentence sequence follows a reasonable introduction structure). The abstract does not provide exact numbers or datasets but claims consistent superiority across these evaluation dimensions.", "conclusion": "The study concludes that encoding workflow structure directly into an LLM via stage tokens is an effective alternative to external agentic orchestration for complex, multi-stage writing tasks like research introductions. STIG enables the model to internalize both the functions of different introduction components and their logical ordering, allowing coherent, multi-stage text generation in a single inference. This approach reduces error accumulation and coherence issues associated with lengthy external workflows and offers a promising direction for designing structured text generation systems without explicit multi-step agents."}}
{"id": "2601.09972", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09972", "abs": "https://arxiv.org/abs/2601.09972", "authors": ["Zixun Lan", "Maochun Xu", "Yifan Ren", "Rui Wu", "Jianghui Zhou", "Xueyang Cheng", "Jianan Ding Ding", "Xinheng Wang", "Mingmin Chi", "Fei Ma"], "title": "Chinese Labor Law Large Language Model Benchmark", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.", "AI": {"tldr": "The paper introduces LabourLawLLM, a Chinese labor-law\u2013focused LLM, and LabourLawBench, a benchmark covering multiple labor-law tasks, showing that this specialized model outperforms general and existing legal LLMs.", "motivation": "General-purpose LLMs like GPT-4 perform poorly on specialized legal subdomains that demand precise legal knowledge, nuanced reasoning, and context sensitivity, particularly in Chinese labor law. There is a need for a domain-adapted model and a standardized benchmark to evaluate and advance AI in this area.", "method": "The authors build LabourLawLLM, a large language model specifically adapted to Chinese labor law, and construct LabourLawBench, a benchmark spanning tasks such as provision citation, legal QA, case classification, compensation computation, NER, and case analysis. They evaluate models on this benchmark using automatic metrics (ROUGE-L, accuracy, F1, soft-F1) and subjective GPT-4-based scoring, comparing LabourLawLLM with general-purpose and existing legal LLMs.", "result": "Across all or most task categories in LabourLawBench, LabourLawLLM consistently outperforms both general-purpose LLMs (e.g., GPT-4) and prior legal-specific LLMs on the chosen objective metrics and in GPT-4-based subjective evaluation.", "conclusion": "A domain-specialized LLM trained for Chinese labor law can significantly outperform general and existing legal LLMs on diverse labor-law tasks. The proposed methodology for building and evaluating such a model is scalable to other legal subfields and can enhance the accuracy, reliability, and societal impact of legal AI systems."}}
{"id": "2601.09729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09729", "abs": "https://arxiv.org/abs/2601.09729", "authors": ["Tohida Rehman"], "title": "Enhancing Business Analytics through Hybrid Summarization of Financial Reports", "comment": "12 pages, 2 figures, 2 tables", "summary": "Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.\n  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.", "AI": {"tldr": "The paper introduces and evaluates a summarization framework for earnings conference call transcripts that combines extractive and abstractive methods, showing that long-context models perform best overall while a lightweight hybrid approach offers strong factual accuracy under limited computational resources.", "motivation": "Manual analysis of financial reports and earnings call transcripts is time-consuming, inefficient, and prone to bias and error, yet these texts contain critical information about firms\u2019 performance and strategy. There is a need for automated summarization systems that can reliably condense long, technical financial documents into concise, factually accurate summaries suitable for practical business use, especially under resource constraints.", "method": "The authors propose a two-track summarization approach on the ECTSum dataset. The main hybrid pipeline first uses the LexRank extractive algorithm to select salient sentences from earnings call transcripts, then applies fine-tuned BART and PEGASUS models to generate abstractive summaries optimized for low-resource settings. In parallel, they fine-tune a Longformer Encoder-Decoder (LED) model to directly process long documents and generate summaries without a prior extractive step. They evaluate all models using standard automatic metrics (ROUGE, METEOR, MoverScore, BERTScore) and domain-specific scores (SciBERTScore, FinBERTScore), and additionally measure factual accuracy with entity-level source-precision and F1-target metrics.", "result": "Experiments show that long-context LED models achieve the strongest overall performance on standard summarization metrics, benefiting from their ability to capture long-range dependencies in financial texts. The hybrid LexRank-plus-abstractive pipeline attains competitive scores while offering better factual consistency, particularly regarding entities, and does so with lower computational demands. The evaluation reveals a trade-off between raw summarization quality and factual reliability under resource constraints.", "conclusion": "The study concludes that long-context encoder-decoder models are highly effective for summarizing lengthy financial documents but can be computationally expensive. The proposed hybrid extractive\u2013abstractive framework is a practical alternative that delivers competitive performance with enhanced factual accuracy in resource-limited environments. These results encourage the deployment of such summarization systems to transform extensive financial communications, especially earnings calls, into compact, actionable business insights."}}
{"id": "2601.09974", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09974", "abs": "https://arxiv.org/abs/2601.09974", "authors": ["Seoyeon Kim", "Jaehyung Kim"], "title": "SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation", "comment": "under review, 23 pages", "summary": "Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.", "AI": {"tldr": "SPRInG is a semi-parametric framework that continually personalizes large language models by selectively adapting to true preference drift while avoiding catastrophic forgetting.", "motivation": "Existing personalization of large language models assumes static user preferences or uses one-time adaptation, which is unrealistic because user interests evolve. Standard continual learning methods update on all interaction data, often noisy, and cannot reliably distinguish lasting preference changes from temporary contexts, leading to catastrophic forgetting or degraded performance. There is a need for a method that can handle dynamic, evolving user preferences in real-world interaction streams while maintaining robustness.", "method": "SPRInG is a semi-parametric continual personalization framework. During training, it introduces drift-driven selective adaptation using a likelihood-based scoring function to detect high-novelty interactions indicative of preference drift. Only these high-novelty signals are used to update a user-specific adapter, while difficult or important residual examples are stored in a replay buffer to mitigate forgetting. During inference, it uses strict relevance gating to filter retrieved user history and combines the parametric model output with retrieved information via logit interpolation, effectively fusing long-term user-specific knowledge with the base model predictions.", "result": "On a long-form personalized generation benchmark, SPRInG achieves better performance than existing baselines for continual personalization. The experiments show that it handles preference drift more effectively and robustly than prior methods, demonstrating improvements in metrics relevant to personalized generation (details not specified in the abstract).", "conclusion": "SPRInG provides an effective and robust solution for continual personalization of large language models by selectively adapting to genuine preference drift, mitigating catastrophic forgetting through replay, and fusing parametric and non-parametric user knowledge at inference. Its superior performance on a personalized generation benchmark suggests it is well-suited for real-world, dynamically evolving personalization scenarios."}}
{"id": "2601.10011", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10011", "abs": "https://arxiv.org/abs/2601.10011", "authors": ["Zerui Yang", "Weichuan Wang", "Yanwei Xu", "Linqi Song", "Yudai Matsuda", "Wei Han", "Bo Bai"], "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL", "comment": null, "summary": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.", "AI": {"tldr": "Memo-SQL is a training-free NL2SQL framework that improves accuracy and efficiency by structured question decomposition and experience-aware self-correction using a memory of past successes and error-fix pairs.", "motivation": "Current NL2SQL systems underuse historical error-fix data and rely on arbitrary test-time decomposition, leading to limited self-correction, low ensemble diversity, and a harsh accuracy-efficiency trade-off. A more efficient, robust, and diverse reasoning approach is needed without expensive fine-tuning or external APIs.", "method": "Memo-SQL introduces (1) structured decomposition strategies\u2014entity-wise, hierarchical, and atomic sequential\u2014to systematically generate diverse SQL reasoning paths, and (2) experience-aware self-correction via a dynamic memory that stores both successful queries and historical error-fix pairs. At inference time, the system retrieves relevant items from this memory and injects them into the prompt for retrieval-augmented generation, all without fine-tuning.", "result": "On the BIRD benchmark, Memo-SQL reaches 68.5% execution accuracy, outperforming previous open, zero-fine-tuning methods, while requiring more than 10\u00d7 fewer computational resources than prior test-time scaling approaches.", "conclusion": "Structured decomposition combined with memory-based, retrieval-augmented self-correction can substantially improve NL2SQL performance and efficiency without any training or external APIs, establishing a new state of the art for zero-fine-tuning methods on BIRD."}}
{"id": "2601.09731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09731", "abs": "https://arxiv.org/abs/2601.09731", "authors": ["Wen G Gong"], "title": "Geometric Patterns of Meaning: A PHATE Manifold Analysis of Multi-lingual Embeddings", "comment": null, "summary": "We introduce a multi-level analysis framework for examining semantic geometry in multilingual embeddings, implemented through Semanscope (a visualization tool that applies PHATE manifold learning across four linguistic levels). Analysis of diverse datasets spanning sub-character components, alphabetic systems, semantic domains, and numerical concepts reveals systematic geometric patterns and critical limitations in current embedding models. At the sub-character level, purely structural elements (Chinese radicals) exhibit geometric collapse, highlighting model failures to distinguish semantic from structural components. At the character level, different writing systems show distinct geometric signatures. At the word level, content words form clustering-branching patterns across 20 semantic domains in English, Chinese, and German. Arabic numbers organize through spiral trajectories rather than clustering, violating standard distributional semantics assumptions. These findings establish PHATE manifold learning as an essential analytic tool not only for studying geometric structure of meaning in embedding space, but also for validating the effectiveness of embedding models in capturing semantic relationships.", "AI": {"tldr": "The paper presents a multi-level framework and visualization tool (Semanscope) using PHATE to analyze the geometric structure of meaning in multilingual embedding spaces, revealing systematic patterns and key limitations of current models across sub-character, character, word, and number representations.", "motivation": "To better understand how multilingual embedding models represent semantic information geometrically, and to identify where these models succeed or fail in capturing meaning across different linguistic units (from radicals and characters to words and numbers). Existing evaluation methods often miss fine-grained structural issues in embeddings, so a principled geometric analysis tool is needed.", "method": "The authors build Semanscope, a visualization and analysis framework that applies PHATE manifold learning to multilingual embedding spaces at four levels: sub-character (Chinese radicals), character (various writing systems), word (content words across 20 semantic domains in English, Chinese, and German), and numerical concepts (Arabic numerals). They project embeddings to low-dimensional manifolds and visually and quantitatively inspect geometric patterns and anomalies.", "result": "They find systematic geometric phenomena: (1) Chinese radicals, which are structural sub-character elements, collapse geometrically, indicating embeddings fail to distinguish their structural vs. semantic roles; (2) characters from different writing systems have distinct geometric signatures; (3) content words cluster and branch according to semantic domains in multiple languages; (4) Arabic numerals do not form clusters but instead follow spiral trajectories in embedding space, contrary to distributional semantics expectations. These patterns reveal both structure and limitations in current embedding models.", "conclusion": "PHATE-based manifold learning, as implemented in Semanscope, is a powerful analytic tool for probing the semantic geometry of multilingual embeddings. It can diagnose where models capture meaningful relationships (e.g., word-level semantic domains) and where they break down (e.g., radicals and numerals), and thus should be used to evaluate and improve future embedding models with respect to their geometric structure of meaning."}}
{"id": "2601.09732", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09732", "abs": "https://arxiv.org/abs/2601.09732", "authors": ["Wen G. Gong"], "title": "Benchmarking Cross-Lingual Semantic Alignment in Multilingual Embeddings", "comment": "20 pages, 9 figures, 4 tables", "summary": "With hundreds of multilingual embedding models available, practitioners lack clear guidance on which provide genuine cross-lingual semantic alignment versus task performance through language-specific patterns. Task-driven benchmarks (MTEB) may mask fundamental alignment shortcomings. We introduce Semantic Affinity (SA), a bounded (between 0 and 1) metric measuring inter-lingual to intra-lingual spread ratio using cosine distance, combined with PHATE visualization in our Semanscope framework. Benchmarking 13 models across 4 datasets (52 experiments) reveals a three-tier structure: (1) Top BERT models (LaBSE SA = 0.70, USE SA = 0.68, S-BERT SA = 0.68) achieve strong alignment via translation-pair supervision; (2) LLM embeddings plateau at SA between 0.55 and 0.61 regardless of 0.6 B to 8 B scale; (3) MLM-only BERT models (mBERT, XLM-R, SA < 0.50) fail despite more than 100 language training. Training objective, not architecture or scale, determines alignment. Oracle Bone primitives (1200 BCE) expose semantic drift-models learn corpus patterns rather than cognitive primitives. This work provides semantic benchmarking to help practitioners select quality multilingual embeddings from hundreds of available models, showing cross-lingual alignment requires explicit translation supervision, not merely model scale or multilingual data.", "AI": {"tldr": "They propose Semantic Affinity (SA), a metric and visualization framework (Semanscope) to directly measure and visualize how well multilingual embedding models align meanings across languages, independently of task performance.", "motivation": "Existing multilingual embedding benchmarks like MTEB are task-centric and can reward models that use language-specific shortcuts rather than truly aligning semantics across languages. With many multilingual models available, practitioners lack a direct, interpretable way to assess whether a model actually embeds semantically similar content from different languages close together in the vector space. The authors want a principled metric and tool to quantify and visualize genuine cross-lingual semantic alignment.", "method": "They define Semantic Affinity (SA), a bounded score between 0 and 1 based on the ratio of inter-lingual to intra-lingual spread using cosine distances between embeddings. They combine this scalar measure with PHATE, a manifold-learning visualization method, in a framework called Semanscope. They then evaluate 13 multilingual embedding models across 4 multilingual datasets (52 experiments) to compare their cross-lingual alignment properties.", "result": "They find a clear three-tier hierarchy of models. (1) Top BERT-based models trained with explicit translation-pair supervision (LaBSE, USE, S-BERT) achieve high SA scores around 0.68\u20130.70, indicating strong cross-lingual alignment. (2) LLM-derived embeddings, even when scaled from 0.6B to 8B parameters, saturate at moderate SA scores (0.55\u20130.61), showing that scale alone does not yield strong alignment. (3) Multilingual masked-language-model-only BERT variants (mBERT, XLM-R), despite training on 100+ languages, obtain low SA (<0.50) and thus poor semantic alignment. They also analyze semantic drift using very old Oracle Bone primitives, showing that the models mainly capture corpus co-occurrence patterns rather than deep, stable cognitive primitives.", "conclusion": "The decisive factor for high-quality cross-lingual semantic alignment is the training objective\u2014especially explicit translation-pair supervision\u2014rather than model size, architecture, or merely using large multilingual corpora. Their SA metric and Semanscope framework give practitioners a direct semantic benchmark to choose among multilingual embeddings and demonstrate that task-based benchmarks alone can be misleading about a model\u2019s true cross-lingual alignment quality."}}
{"id": "2601.10029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10029", "abs": "https://arxiv.org/abs/2601.10029", "authors": ["Tingyue Pan", "Jie Ouyang", "Mingyue Cheng", "Qingchuan Li", "Zirui Liu", "Mingfan Pan", "Shuo Yu", "Qi Liu"], "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization", "comment": null, "summary": "Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.", "AI": {"tldr": "PaperScout is an autonomous agent for academic paper search that treats retrieval as sequential decision-making and is trained with a new sequence-level RL algorithm, PSPO, to better handle multi-step tool use and improve recall and relevance.", "motivation": "Existing academic paper search systems rely on rigid, predefined workflows (e.g., fixed query, fixed expansion rules) that cannot flexibly handle complex, conditional, multi-step search intents. Moreover, when trying to train agentic systems with standard reinforcement learning, there is a mismatch between token-level optimization and the inherently sequence-level nature of multi-turn agent-environment interactions, which causes noisy credit assignment and limits performance.", "method": "The authors design PaperScout, an autonomous agent that performs paper search as a sequential decision-making process. The agent dynamically decides if, when, and how to call search and query expansion tools based on the accumulated retrieval context over multiple turns. To effectively train this multi-step agent, they propose Proximal Sequence Policy Optimization (PSPO), a process-aware RL algorithm that operates at the sequence level rather than token level, aligning optimization with the granularity of agent-environment interactions and stabilizing credit assignment. They then evaluate the system on synthetic and real-world academic search benchmarks.", "result": "Experimental results on both synthetic datasets and real-world academic search benchmarks show that PaperScout achieves substantially higher recall and relevance than strong workflow-based systems and standard RL baselines. This indicates that both the adaptive, agentic search strategy and the PSPO training method contribute to better retrieval performance in complex paper search scenarios.", "conclusion": "Reframing academic paper search as a sequential decision-making problem and training an autonomous agent with a sequence-level RL algorithm (PSPO) yields significant gains over traditional workflow-driven and token-level RL methods. The study suggests that process-aware, agentic frameworks are more suitable for complex, multi-turn information-seeking tasks and can materially improve the quality of scientific literature retrieval."}}
{"id": "2601.09733", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09733", "abs": "https://arxiv.org/abs/2601.09733", "authors": ["Xin Gao", "Xiaoyang Wang", "Yun Zhu", "Mengzhang Cai", "Conghui He", "Lijun Wu"], "title": "Closing the Data Loop: Using OpenDataArena to Engineer Superior Training Datasets", "comment": "Superior ODA-Math, ODA-Mixture Datasets", "summary": "The construction of Supervised Fine-Tuning (SFT) datasets is a critical yet under-theorized stage in the post-training of Large Language Models (LLMs), as prevalent practices often rely on heuristic aggregation without a systematic understanding of how individual samples contribute to model performance. In this report, we propose a paradigm shift from ad-hoc curation to a closed-loop dataset engineering framework using OpenDataArena (ODA), which leverages value-anchored rankings and multi-dimensional analysis to transform value benchmarking into feedback signals guiding dataset construction. We instantiate this methodology through two new datasets: \\textbf{ODA-Math-460k}, a specialized mathematics reasoning dataset that utilizes a novel two-stage difficulty-aware pipeline to achieve State-of-the-Art (SOTA) results on benchmarks such as AIME and HMMT, and \\textbf{ODA-Mixture (100k \\& 500k)}, a series of multi-domain instruction datasets built via an ``Anchor-and-Patch'' strategy that outperforms significantly larger open-source baselines. Our empirical results demonstrate that ODA-driven datasets significantly improve both domain-specific reasoning and general utility while achieving superior data efficiency, validating a transition toward data-centric AI where transparent evaluation serves as the primary engine for engineering high-quality training data.", "AI": {"tldr": "The paper introduces OpenDataArena (ODA), a closed-loop framework for systematically constructing supervised fine-tuning (SFT) datasets for LLMs, and demonstrates that ODA-built datasets boost both math reasoning and general instruction-following performance with high data efficiency.", "motivation": "SFT dataset construction for LLMs is usually done via ad-hoc, heuristic aggregation without a clear theory or systematic way to understand how each data sample affects performance. This makes it hard to optimize data quality, align datasets with target values, or efficiently improve specific capabilities such as mathematical reasoning or general utility. The authors aim to move from this opaque, model-centric practice to a transparent, data-centric process where evaluation and values directly drive dataset engineering.", "method": "They propose OpenDataArena (ODA), a closed-loop dataset engineering framework that uses value-anchored rankings and multi-dimensional evaluation metrics to convert benchmark performance into actionable feedback signals for dataset construction. Concretely, they: (1) define value benchmarks and metrics; (2) rank data sources and samples based on their contributions to these values; (3) iteratively refine the dataset using these signals. They instantiate this with two datasets: ODA-Math-460k, built via a two-stage difficulty-aware pipeline for math reasoning, and ODA-Mixture (100k & 500k), multi-domain instruction datasets created through an \"Anchor-and-Patch\" strategy that starts from high-value anchor data and patches coverage gaps identified via analysis.", "result": "Models fine-tuned on ODA-Math-460k achieve state-of-the-art results on math reasoning benchmarks such as AIME and HMMT. Models trained on ODA-Mixture (100k & 500k) outperform much larger open-source instruction-tuning baselines across general utility benchmarks. Across experiments, ODA-constructed datasets yield better performance per training sample, highlighting strong data efficiency and improved capability in both specialized (math) and broad instruction-following tasks.", "conclusion": "A closed-loop, evaluation-driven approach to SFT dataset construction, as realized in OpenDataArena, leads to higher quality, more data-efficient training sets that significantly enhance both specialized reasoning and general-purpose performance of LLMs. This supports a shift toward data-centric AI, where transparent, value-based evaluation is the main driver for engineering and iteratively improving SFT datasets rather than relying on ad-hoc data aggregation."}}
{"id": "2601.10031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10031", "abs": "https://arxiv.org/abs/2601.10031", "authors": ["Jianheng Tang", "Shilong Tao", "Zhe Feng", "Haonan Sun", "Menglu Wang", "Zhanxing Zhu", "Yunhuai Liu"], "title": "FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data", "comment": "Accepted in Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD '26)", "summary": "The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.", "AI": {"tldr": "FilDeep is a deep learning framework that uses both low- and high-fidelity data to model large elastic-plastic deformations (e.g., stretch bending), overcoming data quantity-accuracy trade-offs and achieving state-of-the-art performance.", "motivation": "Accurate numerical simulation of large elastic-plastic deformations is vital for manufacturing, but traditional numerical methods have limitations (e.g., cost, scalability). Existing deep learning approaches require large, high-accuracy datasets, which are hard to obtain in large deformation problems. In practice there is a trade-off between generating many low-accuracy samples or fewer high-accuracy samples, which degrades model performance. The paper aims to break this quantity-accuracy dilemma and make DL practically useful for large deformation applications.", "method": "The authors focus on a canonical large-deformation problem\u2014stretch bending of elastic-plastic solids\u2014and introduce FilDeep, a fidelity-based deep learning framework. FilDeep jointly trains on multi-fidelity (MF) data: abundant low-fidelity simulations/measurements and scarce but accurate high-fidelity data. They design attention-enabled cross-fidelity modules to learn and fuse information across these data fidelities, capturing long-range physical interactions inherent to large deformations. The architecture is tailored to the specifics of large elastic-plastic deformation modeling and exploits MF learning for better generalization and accuracy.", "result": "FilDeep is shown, through extensive experiments on stretch bending and related large-deformation tasks, to outperform existing deep learning and traditional numerical baselines, achieving state-of-the-art predictive accuracy. It effectively leverages low-fidelity data without being limited by their inaccuracy, thanks to cross-fidelity learning. The framework trains efficiently and yields models that can be deployed in real manufacturing workflows with improved speed-accuracy trade-offs over conventional methods.", "conclusion": "The study concludes that multi-fidelity deep learning, instantiated as FilDeep with attention-based cross-fidelity modules, is an effective solution to the data quantity-accuracy dilemma in large deformation problems. FilDeep is presented as the first DL framework using multi-fidelity data for large elastic-plastic deformations, providing both high accuracy and practical efficiency for manufacturing applications, and suggesting a promising direction for integrating MF learning with computational mechanics more broadly."}}
{"id": "2601.09734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09734", "abs": "https://arxiv.org/abs/2601.09734", "authors": ["Yanyi Liu", "Qingwen Yang", "Tiezheng Guo", "Feiyu Qu", "Jun Liu", "Yingyou Wen"], "title": "From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis", "comment": "Accepted at The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary \"detection\" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from \"detection\" to \"diagnosis\". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance to advanced general-purpose models. In comprehensive diagnosis tasks, HDM-4B-RL matches the capabilities of larger general models while maintaining a smaller size. This work validates the feasibility and value of hallucination diagnosis, providing an effective methodology for building more trustworthy and reliable generative AI systems.", "AI": {"tldr": "The paper proposes moving from merely detecting hallucinations in large language models (LLMs) to fully diagnosing them\u2014locating, explaining, and correcting errors\u2014via a new task, data-generation pipeline (HDG), and a 4B-parameter diagnosis model (HDM-4B-RL) trained with a rich RL objective that achieves state-of-the-art results.", "motivation": "Existing work on LLM hallucinations mainly offers binary detection (hallucinated vs. not), which is insufficient for practical deployment and model improvement because it lacks interpretability and actionable guidance. There is a need for methods that can analyze hallucinations in detail: where they occur, why they occur, and how to fix them, so that systems become more trustworthy and useful in critical domains.", "method": "1) Define a new Hallucination Diagnosis Task that extends beyond detection to include error localization, causal explanation, and correction. 2) Propose the Hallucination Diagnosis Generator (HDG), an automated data pipeline that produces synthetic training data with rich diagnostic annotations using strategies such as controlled fact fabrication and perturbation of reasoning chains. 3) Train a 4B-parameter model, HDM-4B-RL, using Group Relative Policy Optimization (GRPO), guided by a composite reward capturing structural correctness, factual accuracy, and precise hallucination localization.", "result": "HDM-4B-RL, trained on HDG-generated data, outperforms prior state-of-the-art hallucination detection models on the HaluEval benchmark and reaches performance comparable to strong general-purpose LLMs for detection. On the full diagnosis task (detection + localization + explanation + correction), the 4B-parameter model performs on par with larger general models, demonstrating competitive capability at a smaller scale.", "conclusion": "Transitioning from hallucination detection to diagnosis is both feasible and beneficial. The proposed HDG pipeline and HDM-4B-RL model show that targeted diagnostic supervision and RL training can produce compact models that not only detect hallucinations but also analyze and correct them, contributing to more reliable and trustworthy generative AI systems."}}
{"id": "2601.10088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10088", "abs": "https://arxiv.org/abs/2601.10088", "authors": ["Malika Aubakirova", "Alex Atallah", "Chris Clark", "Justin Summerville", "Anjney Midha"], "title": "State of AI: An Empirical 100 Trillion Token Study with OpenRouter", "comment": "36 pages", "summary": "The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.", "AI": {"tldr": "This paper empirically analyzes over 100 trillion real-world LLM tokens from the OpenRouter platform to characterize how large language models are actually used, highlighting the rise of multi-step reasoning models, open-weight adoption, creative roleplay, coding assistance, agentic inference, and persistent early user cohorts (the \u201cGlass Slipper\u201d effect).", "motivation": "Despite rapid advances and deployment of LLMs\u2014especially with the emergence of multi-step reasoning models like o1\u2014there is limited empirical understanding of how these models are truly used across tasks, users, and time. Common narratives (e.g., that productivity use cases dominate) may be incomplete or inaccurate, making it harder for model builders and infrastructure providers to design systems that reflect real demand and behavior.", "method": "The authors use large-scale interaction logs from OpenRouter, an AI inference platform that serves many different LLMs. They perform empirical analysis over more than 100 trillion tokens of usage, segmenting by model type (e.g., open-weight vs closed), task categories (e.g., creative roleplay, coding), geography, temporal patterns, and user retention dynamics. They particularly study cohorts of users over time to identify which groups persist in their usage and how their behavior differs from later adopters.", "result": "The study finds strong real-world adoption of open-weight LLMs, contradicting a narrative that only proprietary models dominate. It uncovers that creative roleplay and coding assistance are among the most popular usage categories, surpassing what would be expected if productivity tasks were primary. The authors also observe increased use of agentic inference patterns (e.g., multi-step or tool-using interactions). Cohort and retention analysis reveals \u201cfoundational\u201d early user groups whose engagement remains high over long periods, a pattern the authors call the Cinderella \u201cGlass Slipper\u201d effect.", "conclusion": "LLM usage in the wild is richer and more varied than simple productivity-centric stories suggest, with strong roles for open-weight models, creative roleplay, coding, and emerging agentic workflows. Early adopters form durable, high-engagement cohorts that can shape ecosystem evolution. A data-driven, at-scale understanding of these usage patterns should inform how model builders, application developers, and infrastructure providers design, prioritize, and deploy future LLM systems."}}
{"id": "2601.09833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09833", "abs": "https://arxiv.org/abs/2601.09833", "authors": ["Xiaoxu Ma", "Xiangbo Zhang", "Zhenyu Weng"], "title": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations", "comment": null, "summary": "Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.", "AI": {"tldr": "They introduce PVNI, an activation-based method to measure LLM personality traits that is more stable and interpretable than questionnaire-based prompting.", "motivation": "Current personality evaluation of LLMs mostly relies on questionnaires and role-play prompts, whose results are fragile: small changes in wording or setup can significantly alter the measured traits, hurting stability, comparability, and trust. There is a need for a principled, interpretable, and robust way to assess personality-like behaviors in LLMs to support model analysis and responsible deployment.", "method": "They propose Persona-Vector Neutrality Interpolation (PVNI). First, they use contrastive prompts (e.g., exemplars of high vs. low expression of a given trait) to extract a \"persona vector\" from internal activations representing that trait direction. Then, treating this vector as an axis in representation space, they interpolate the representation of neutral prompts along this axis to estimate a neutral score for the trait. This yields a scalar trait evaluation grounded in the geometry of the model\u2019s internal states. They also provide theoretical analysis of why this approach is effective and how it generalizes across settings.", "result": "Across multiple LLMs and under a variety of evaluation conditions, PVNI delivers personality trait scores that vary far less when prompts or role-play configurations are changed, compared to standard questionnaire-based approaches. The experiments show substantially improved stability and still maintain meaningful discriminative power between traits and models.", "conclusion": "Using internal activation geometry via persona vectors and neutrality interpolation provides a more stable and explainable way to evaluate personality traits in LLMs than traditional questionnaire-style prompting. PVNI appears to generalize across models and prompt variants, suggesting it is a promising tool for interpreting and comparing LLM behaviors in a more robust and transparent way."}}
{"id": "2601.10101", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10101", "abs": "https://arxiv.org/abs/2601.10101", "authors": ["Ke Chen", "Jiandian Zeng", "Zihao Peng", "Guo Li", "Guangxue Zhang", "Tian Wang"], "title": "MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning", "comment": "12 pages, 5 figures, 2 tables. Accepted at The Web Conference (WWW) 2026", "summary": "As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.", "AI": {"tldr": "MatrixCoT is a structured Chain-of-Thought framework that uses a matrix-based plan, normalization, typing, and feedback-driven replanning to make LLM logical reasoning more robust, verifiable, and interpretable without external solvers.", "motivation": "Standard CoT improves LLM reasoning but still struggles with tasks requiring strict symbolic logic and deductive rules. Neuro-symbolic methods help ensure formal correctness via external solvers, but they are brittle to small formatting variations in LLM outputs. Pure LLM-driven methods avoid such brittleness yet lack structured intermediate representations and systematic error-correction. There is a need for a method that preserves the strengths of LLMs while adding structure, verifiability, and robustness for complex logical reasoning, without depending on fragile external solvers.", "method": "The authors propose MatrixCoT, a structured CoT framework. They first normalize and type natural language expressions, and attach explicit citation fields so that each step clearly references its premises. Then they construct a matrix-based plan that encodes the global dependency relations among reasoning steps, turning the reasoning process into a structured, verifiable artifact. A feedback-driven replanning mechanism checks the plan under semantic-equivalence constraints, identifying omissions and defects, and then rewrites and compresses the dependency matrix. This iterative adjustment improves the quality and reliability of the reasoning before producing the final answer.", "result": "Across five logical-reasoning benchmarks and five different LLMs, MatrixCoT improves robustness and interpretability on complex symbolic reasoning tasks. It does this without using external symbolic solvers, yet still maintains performance competitive with methods that do. The experiments demonstrate that the matrix-based planning and verification steps reduce failures due to instability and enhance the consistency of the reasoning chains.", "conclusion": "MatrixCoT shows that adding a matrix-structured planning and verification layer on top of CoT can significantly strengthen LLM logical reasoning. By normalizing expressions, enforcing typed and cited steps, and iteratively repairing the dependency matrix, the framework yields more stable, transparent, and trustworthy reasoning processes without relying on external solvers. This suggests a promising direction for neuro-symbolic-style rigor realized purely within LLM-driven pipelines."}}
{"id": "2601.09852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09852", "abs": "https://arxiv.org/abs/2601.09852", "authors": ["Sriram Padmanabhan", "Siyuan Song", "Kanishka Misra"], "title": "Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences", "comment": null, "summary": "Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements (\"Bears are daxable\"), universally quantified NPs (\"all bears are daxable\") and indefinite plural NPs (\"some bears are daxable\") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.", "AI": {"tldr": "The paper investigates whether vision-language models (VLMs) are sensitive to subtle semantic differences between generics, universal quantification, and indefinites, replicating a classic developmental experiment and finding human-like inductive behavior.", "motivation": "Previous developmental work shows that children distinguish between generic statements (\"Bears are daxable\"), universal quantifications (\"All bears are daxable\"), and indefinites (\"Some bears are daxable\") when making inductive inferences about category members, implying different underlying mental representations. It is unknown whether large vision-language models, as general-purpose statistical learners, develop similar differentiated representations and inductive behaviors, or whether they treat these linguistic forms as surface variants. Understanding this connection informs both cognitive modeling and the semantic capacities of current AI systems.", "method": "The authors replicate Gelman et al. (2002) in vision-language models. First, they run precondition tests to ensure the models can robustly identify visual categories in images and show sensitivity to quantifiers like \"all\" and \"some.\" Then, they perform the main induction task: models are given novel properties framed as generic, universally quantified, or indefinite plural statements about a category (e.g., \"Bears are daxable,\" \"All bears are daxable,\" \"Some bears are daxable\"), and are asked whether an individual exemplar has that property. They compare the generalization rates across statement types and perform post-hoc representational analyses to see whether distinctions reflect inductive constraints rather than mere surface-form differences.", "result": "Vision-language models pass the precondition tests, showing adequate category recognition and basic sensitivity to quantifiers. In the main induction task, models exhibit a gradient of property extension that parallels children's behavior: universal > generic > some. Post-hoc analyses of internal representations show that these differences cluster according to inductive roles and constraints, instead of grouping only by superficial string-level similarities, indicating structured semantic representations of these quantificational forms.", "conclusion": "The study finds behavioral alignment between vision-language models and human children in how they use generic, universal, and indefinite plural statements for inductive inference. The models not only reproduce the graded extension pattern but also encode distinctions that are organized by inductive constraints rather than surface form. This suggests that general-purpose statistical learners can acquire human-like semantic and inductive structures for quantificational language, casting VLMs as plausible models of certain aspects of human semantic cognition."}}
{"id": "2601.10114", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10114", "abs": "https://arxiv.org/abs/2601.10114", "authors": ["Cheng Feng", "Chaoliang Zhong", "Jun Sun", "Yusuke Oishi"], "title": "Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs", "comment": "15 pages, submitted to ICPR 2026", "summary": "Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.", "AI": {"tldr": "They study when and how a smaller student LLM can match or beat a larger fine-tuned teacher on domain-specific tasks, and propose a new distillation framework (SCD + AW) that makes this possible.", "motivation": "Deploying large language models for domain-specific applications is difficult because they are computationally expensive. Distilling a large fine-tuned model into a smaller one is appealing but usually results in a performance drop due to the capacity gap. The authors want to understand under what conditions a smaller student can actually match or surpass its teacher, and to design a principled distillation method that leverages these conditions for real-world domain tasks like QA, NER, and classification, often in multiple languages.", "method": "They introduce the concept of Student-Favored Subdomain (SFS), where the student has an advantage, and Teacher-Favored Subdomain (TFS), where the teacher is stronger. The core theoretical insight is that a student can outperform its teacher if gains on SFS exceed losses on TFS. Based on this, they propose Scheduled Checkpoint Distillation (SCD), which distills from multiple teacher checkpoints along its supervised fine-tuning trajectory to better emulate its convergence behavior and narrow the TFS deficit. Additionally, they design a sample-wise Adaptive Weighting (AW) scheme that adjusts the distillation objective per sample to preserve and emphasize the student\u2019s strengths on SFS while still learning from the teacher on harder TFS examples.", "result": "Across various domain-specific tasks\u2014question answering, named entity recognition, and text classification\u2014covering multiple languages, their approach (SCD + AW) consistently outperforms standard and state-of-the-art distillation baselines. Empirically, the student models trained with their method not only close the gap to the fine-tuned teacher but in many cases match or even exceed the teacher\u2019s performance on these domain tasks.", "conclusion": "By analyzing performance over student-favored and teacher-favored subdomains, the paper shows that smaller models can outperform larger teachers under the right training regime. The proposed Scheduled Checkpoint Distillation and Adaptive Weighting methods operationalize this theory, enabling more efficient domain-specific deployment of LLMs without sacrificing\u2014and sometimes improving\u2014task performance compared to the original fine-tuned teacher."}}
{"id": "2601.09853", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09853", "abs": "https://arxiv.org/abs/2601.09853", "authors": ["Sraavya Sambara", "Yuan Pu", "Ayman Ali", "Vishala Mishra", "Lionel Wong", "Monica Agrawal"], "title": "MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication", "comment": null, "summary": "Real-world health questions from patients often unintentionally embed false assumptions or premises. In such cases, safe medical communication typically involves redirection: addressing the implicit misconception and then responding to the underlying patient context, rather than the original question. While large language models (LLMs) are increasingly being used by lay users for medical advice, they have not yet been tested for this crucial competency. Therefore, in this work, we investigate how LLMs react to false premises embedded within real-world health questions. We develop a semi-automated pipeline to curate MedRedFlag, a dataset of 1100+ questions sourced from Reddit that require redirection. We then systematically compare responses from state-of-the-art LLMs to those from clinicians. Our analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, and provide answers that could lead to suboptimal medical decision making. Our benchmark and results reveal a novel and substantial gap in how LLMs perform under the conditions of real-world health communication, highlighting critical safety concerns for patient-facing medical AI systems. Code and dataset are available at https://github.com/srsambara-1/MedRedFlag.", "AI": {"tldr": "The paper introduces MedRedFlag, a dataset and benchmark showing that current large language models often fail to safely handle patient health questions that contain false assumptions, leading to potentially unsafe medical advice.", "motivation": "Patients frequently ask health questions that contain hidden false assumptions; clinicians handle this by gently correcting the misconception (redirection) before answering. As LLMs are increasingly used for medical advice, it is important to know whether they can perform this kind of safe redirection, but this capability has not been systematically evaluated.", "method": "The authors build a semi-automated pipeline to collect and annotate over 1100 real-world health questions from Reddit where safe answering requires redirection (MedRedFlag). They then benchmark state-of-the-art LLMs on this dataset and compare their responses to clinician-written answers, analyzing whether models detect problematic premises and whether they properly redirect instead of directly answering the flawed question.", "result": "The analysis shows that state-of-the-art LLMs frequently do not redirect when faced with questions containing false premises, even in cases where they appear to recognize that the question is problematic. Instead, they often provide direct answers that accept the false premise, which can lead to suboptimal or unsafe medical decision making.", "conclusion": "The study exposes a significant and previously underexplored safety gap in how current LLMs handle real-world patient questions that embed misconceptions. MedRedFlag and the associated benchmark highlight the need for improved model training and evaluation focused on safe redirection behavior in patient-facing medical AI systems."}}
{"id": "2601.10131", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.10131", "abs": "https://arxiv.org/abs/2601.10131", "authors": ["Yizhan Li", "Florence Cloutier", "Sifan Wu", "Ali Parviz", "Boris Knyazev", "Yan Zhang", "Glen Berseth", "Bang Liu"], "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints", "comment": null, "summary": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \\textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.", "AI": {"tldr": "MolGen is a two-stage, fragment-level, retrieval-augmented and RL-optimized framework to generate molecules that precisely satisfy multiple numeric property constraints, outperforming LLM and graph baselines.", "motivation": "Designing molecules that must simultaneously satisfy several numeric physicochemical property constraints (e.g., QED, LogP, molecular weight, HOMO, LUMO) is critical for drug discovery and materials science but is difficult. Large language models can generate molecules but are weak at precise multi-objective numeric control and reasoning. Existing methods often lack fine-grained control, multi-hop reasoning, and robust satisfaction of tight numeric targets. The paper aims to address these gaps.", "method": "The authors propose MolGen, a fragment-level, retrieval-augmented, two-stage framework. Stage I (Prototype generation) uses a multi-agent reasoner that performs fragment-level edits guided by retrieval to produce a candidate molecule near the feasible region of the property constraints. Stage II (RL-based optimization) uses a fragment-level policy trained with Group Relative Policy Optimization (GRPO) to refine the prototype via one- or multi-hop fragment edits, explicitly minimizing property errors while regularizing edit complexity and deviation from the prototype. They build a large, automatically curated dataset of fragment edit reasoning chains with measured property deltas to supervise both stages and enable reproducible multi-hop reasoning.", "result": "On benchmark tasks involving multi-property constrained molecule generation for two property sets (QED, LogP, Molecular Weight and HOMO, LUMO), MolGen achieves higher validity of generated molecules and more precise satisfaction of the numeric multi-property targets than strong LLM baselines and graph-based algorithms.", "conclusion": "Fragment-level, retrieval-augmented reasoning combined with RL-based fine-grained optimization enables more accurate and controllable satisfaction of strict multi-property numeric constraints in generative molecule design. MolGen offers a principled way to perform multi-hop, fragment-based reasoning over molecules, yielding consistent improvements over prior LLM and graph-based approaches for constrained molecular generation."}}
{"id": "2601.09858", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09858", "abs": "https://arxiv.org/abs/2601.09858", "authors": ["Yilin Bao", "Ziyao He", "Zayden Yang"], "title": "OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing", "comment": null, "summary": "Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.", "AI": {"tldr": "The paper proposes a reinforcement-learning-based framework to generate full scientific papers by planning and editing document outlines, improving global structure and citation consistency over LLM baselines.", "motivation": "Existing large language models are good at locally fluent text but struggle with document-level tasks like full scientific paper generation. They often fail to maintain global structure, thoroughly cover input information, and keep citations consistent and faithful. There is a need for methods that treat scientific writing as a structured, long-horizon planning problem and that can be evaluated on relevant document-level criteria.", "method": "The authors formulate scientific paper generation as a long-horizon planning problem over hierarchical document outlines. They define structured edit actions that iteratively evolve an outline into a full manuscript. Learning is done via a two-stage optimization: (1) backward outline reconstruction from partial plans to enforce global structural consistency; and (2) forward value-guided reinforcement learning, where the reward function explicitly encodes scientific correctness, discourse coherence, and citation fidelity. They also design and release a benchmark that measures several aspects of scientific paper generation, including planning quality, input utilization, reference faithfulness, outline organization, and factual accuracy.", "result": "On the proposed benchmark, the framework outperforms strong neural and LLM baselines across multiple dimensions, notably in long-range structural coherence and reliability of citations. Quantitative metrics and qualitative analysis both indicate that the RL-based planning and structured editing improve document-level quality and grounding.", "conclusion": "Casting scientific paper generation as hierarchical outline planning with reinforcement learning leads to better document-level structure and citation consistency than standard LLM generation. The two-stage optimization procedure and the new benchmark provide a principled way to train and evaluate systems for full scientific manuscript generation, demonstrating clear advantages in long-range coherence and factual grounding."}}
{"id": "2601.10132", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10132", "abs": "https://arxiv.org/abs/2601.10132", "authors": ["Yanan Cao", "Farnaz Fallahi", "Murali Mohana Krishna Dandu", "Lalitesh Morishetti", "Kai Zhao", "Luyi Ma", "Sinduja Subramaniam", "Jianpeng Xu", "Evren Korpeoglu", "Kaushiki Nag", "Sushant Kumar", "Kannan Achan"], "title": "Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction", "comment": "Accepted at The Web Conference 2026 (WWW 2026)", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that \"more context leads to better reasoning\". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.", "AI": {"tldr": "The paper evaluates how well large language models can predict time intervals between repeated user actions from structured behavioral data, showing they beat simple statistical baselines but lag behind dedicated machine-learning models, and that too much contextual detail can harm performance.", "motivation": "Although LLMs excel at many reasoning and prediction tasks, it is unclear whether they can accurately infer temporal regularities\u2014such as intervals between recurring actions\u2014from structured behavioral data. This gap matters for applications like demand forecasting, user modeling, and recommendation systems, where temporal structure is crucial and where practitioners may be tempted to rely on LLMs as general-purpose predictors.", "method": "The authors design a controlled repurchase prediction scenario in which the task is to predict the time until the next user action (e.g., the next purchase) based on historical sequences. They compare state-of-the-art LLMs used in zero-shot prompting conditions against classical statistical baselines and specialized machine-learning models trained on the same structured data. They systematically vary the amount and granularity of contextual information provided to the LLMs\u2014from minimal to highly detailed user-level context\u2014to examine how context length and richness affect predictive accuracy.", "result": "LLMs consistently outperform simple statistical baselines but underperform specialized machine-learning models that are tailored to temporal prediction, indicating that LLMs struggle to fully capture quantitative temporal patterns in structured event data. Moreover, performance improves when providing a moderate amount of context but degrades when richer, more detailed user-level information is added, suggesting that excessive context can confuse the models rather than help them. This pattern contradicts the expectation that more context always leads to better LLM reasoning.", "conclusion": "Current LLMs have fundamental limitations in structured temporal inference: they cannot yet match the predictive precision of dedicated machine-learning models for tasks like repurchase interval prediction, and their performance is sensitive to the quantity and type of context provided. The findings caution against overreliance on LLMs for quantitative temporal tasks and challenge the assumption that increasing context will automatically improve reasoning. The paper recommends future hybrid approaches that combine the statistical rigor of specialized temporal models with the flexible, language-based reasoning capabilities of LLMs, especially for building context-aware predictive systems."}}
{"id": "2601.09876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09876", "abs": "https://arxiv.org/abs/2601.09876", "authors": ["Yifei Shen", "Yilun Zhao", "Justice Ou", "Tinglin Huang", "Arman Cohan"], "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL", "comment": "Accepted by EACL 2026", "summary": "Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.", "AI": {"tldr": "The paper introduces CLINSQL, a clinical text-to-SQL benchmark on MIMIC-IV that stresses realistic, complex EHR querying and shows current models are still well below clinical reliability.", "motivation": "Existing text-to-SQL benchmarks do not adequately capture the complexity and safety requirements of real-world clinical queries over electronic health records (EHRs), such as multi-table reasoning, temporal constraints, and constructing patient cohorts. This gap makes it hard to assess whether modern LLMs are ready for clinically reliable analytics. The authors aim to create a benchmark that better reflects real clinical data analysis tasks and lets the community measure genuine progress toward safe deployment.", "method": "They construct CLINSQL, a benchmark of 633 expert-authored natural-language-to-SQL tasks grounded in the MIMIC-IV v3.1 database. The tasks require multi-table joins, temporal windows, and clinically meaningful filters over heterogeneous EHR tables, as well as building patient-similarity cohorts. The benchmark is designed to require navigation of schema metadata and clinical coding systems, long-context handling, and multi-step query composition. They then evaluate 22 proprietary and open-source models using Chain-of-Thought self-refinement strategies. For evaluation, they use a rubric-based SQL analysis combined with execution checks that emphasize clinically critical aspects of correctness rather than only surface-form similarity.", "result": "The benchmark reveals that even the strongest current models fall short of clinical reliability. On the test set, the proprietary GPT-5-mini model achieves a 74.7% execution score. Among open-source models, DeepSeek-R1 performs best at 69.2%. Gemini-2.5-Pro shows a substantial drop in performance from 85.5% on easier queries to 67.2% on the hardest ones. These scores indicate frequent failures on complex or safety-critical aspects of the queries.", "conclusion": "CLINSQL exposes the gap between current text-to-SQL capabilities and the robustness needed for real-world clinical EHR analytics. While models show promising performance on simpler tasks, their reliability degrades significantly for complex, clinically realistic queries. The benchmark provides a rigorous, execution-focused way to track progress, and improvements on CLINSQL can serve as evidence of meaningful advances toward deploying clinically trustworthy text-to-SQL systems in practice."}}
{"id": "2601.10143", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.10143", "abs": "https://arxiv.org/abs/2601.10143", "authors": ["Haochong Xia", "Yao Long Teng", "Regan Tan", "Molei Qin", "Xinrun Wang", "Bo An"], "title": "History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis", "comment": null, "summary": "In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra \"History Is Not Enough\" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.", "AI": {"tldr": "They propose a drift-aware, differentiable dataflow system that adaptively generates and curates financial data to reduce overfitting and improve robustness in dynamic markets.", "motivation": "In quantitative finance, markets are non-stationary and subject to concept drift, so models trained on static historical datasets often overfit and fail in live trading. Existing workflows treat data curation, augmentation, and scheduling as fixed, manual processes that do not adapt as the market evolves. There is a need for a principled way to make the data pipeline itself adaptive and learning-guided, closing the gap between backtest performance and real-world robustness.", "method": "They design a parameterized dataflow system consisting of: (1) a data manipulation module that performs single-stock transformations, multi-stock mix-ups, and curation operations; and (2) an adaptive planner-scheduler that uses gradient-based bi-level optimization to control these operations. The entire workflow is made differentiable so that the parameters of the data manipulations and the scheduling policy are learned jointly with the predictive or trading model, effectively unifying data augmentation, curriculum learning, and workflow management. The system supports provenance-aware replay and continuous monitoring of data quality.", "result": "Through extensive experiments on both forecasting and reinforcement-learning-based trading tasks, the proposed system yields models that are more robust to market regime shifts and concept drift, and that achieve better risk-adjusted returns than baselines using static or non-adaptive data pipelines.", "conclusion": "A drift-aware, differentiable dataflow framework that tightly couples adaptive data generation/curation with model training can substantially improve robustness and real-world performance in financial ML. The approach offers a general, extensible template for adaptive data management and learning-guided workflow automation in finance, beyond any specific model architecture or task."}}
{"id": "2601.09886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09886", "abs": "https://arxiv.org/abs/2601.09886", "authors": ["Sathvik Nair", "Byung-Doh Oh"], "title": "Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal", "comment": "13 pages, 7 figures", "summary": "How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.", "AI": {"tldr": "The paper compares human cloze probabilities with language model (LM) probabilities as predictors of processing effort and explains why LMs perform better.", "motivation": "To understand whether language model\u2013based predictability scores outperform human cloze probabilities for the right cognitive reasons, because different predictability measures can lead to different theories about prediction in language comprehension.", "method": "Quantitatively compare cloze-based and LM-based word probabilities as predictors of processing effort, and test three specific hypotheses about why LM probabilities have an advantage: (1) they are not limited by the low resolution of cloze tasks, (2) they can distinguish between semantically similar candidate words, and (3) they can accurately estimate probabilities for low-frequency words.", "result": "Language model probabilities are better predictors of processing effort primarily because they avoid low-resolution ceiling effects in cloze data, can discriminate among semantically similar alternatives, and assign more reliable probabilities to rare words.", "conclusion": "LM probabilities outperform cloze probabilities for specific, identifiable reasons related to resolution and coverage, implying that cloze methods should be improved and that further work is needed to test whether human predictive processing is actually sensitive to the same fine-grained distinctions captured by LMs."}}
{"id": "2601.10148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10148", "abs": "https://arxiv.org/abs/2601.10148", "authors": ["Xiaowei Lv", "Zhilin Zhang", "Yijun Li", "Yusen Huo", "Siyuan Ju", "Xuyan Li", "Chunxiang Hong", "Tianyu Wang", "Yongcai Wang", "Peng Sun", "Chuan Yu", "Jian Xu", "Bo Zheng"], "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration", "comment": null, "summary": "Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.", "AI": {"tldr": "The paper introduces DecisionLLM, a method that uses large language models for offline long-horizon decision-making by treating trajectories as a separate modality and aligning them with language, achieving strong gains over Decision Transformer and establishing scaling laws based on model size, data volume, and data quality.", "motivation": "Long-horizon decision-making in dynamic environments like real-time bidding is important and typically handled with RL. Decision Transformer reframed RL as sequence modeling, and LLMs are very strong at complex reasoning and planning, but standard LLMs struggle with continuous numerical values. The authors are motivated to see if large-scale LLMs can be adapted to offline RL tasks and overcome their weakness in representing continuous quantities, unlocking better performance than existing DT-style approaches.", "method": "They propose DecisionLLM, which treats trajectories (states, actions, rewards) as a distinct modality and learns to align this trajectory modality with natural language task descriptions. Within this unified framework, the model autoregressively predicts future decisions. They further explore scaling laws for this paradigm, studying how performance depends on model scale, data volume, and data quality. Experiments are run on offline RL benchmarks like Maze2D and an auction/bidding dataset (AuctionNet).", "result": "DecisionLLM performs strongly in offline decision-making benchmarks and bidding scenarios. The DecisionLLM-3B variant surpasses the Decision Transformer baseline by 69.4 points on Maze2D umaze-v1 and by 0.085 on AuctionNet. They empirically validate that performance scales with model size, dataset size, and trajectory quality.", "conclusion": "LLMs can be effectively adapted for long-horizon, offline sequential decision-making when trajectories are modeled as a separate modality aligned with language. The DecisionLLM framework extends the AIGB (AI-Generated Bidding) paradigm, establishes useful scaling laws for such models, and shows promising improvements over Decision Transformer, suggesting potential for future applications in online bidding and other strategic dynamic environments."}}
{"id": "2601.09953", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09953", "abs": "https://arxiv.org/abs/2601.09953", "authors": ["Christabel Acquaye", "Yi Ting Huang", "Marine Carpuat", "Rachel Rudinger"], "title": "Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations", "comment": null, "summary": "Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a \"classroom\" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different \"classroom sizes,\" showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.", "AI": {"tldr": "The paper evaluates how well open-source large language models can predict the difficulty of standardized multiple-choice math questions by simulating student responses, finding strong correlations with real-world difficulty metrics.", "motivation": "Human pilot studies for standardized math tests are expensive and time-consuming, yet necessary to estimate item difficulty. The authors seek an automated, low-cost way to approximate item difficulty using open-source LLMs, and to understand when and how such models can reliably stand in for real students.", "method": "Instead of asking LLMs to directly rate difficulty, the authors simulate virtual classrooms of 4th, 8th, and 12th grade students by prompting an LLM to role-play many students with different proficiency levels. They then collect the simulated answers to multiple-choice NAEP math items and fit Item Response Theory (IRT) models to these responses. The learned item difficulty parameters are compared against ground-truth NAEP item statistics. They vary classroom size, types of role-play (named vs. anonymous students), and demographic stratification in names, and also compare multiple open-source LLMs with different math abilities.", "result": "Simulation-based IRT using LLM-generated student responses achieves high correlations between predicted and real-world item difficulties: up to 0.75 for grade 4, 0.76 for grade 8, and 0.82 for grade 12. Larger simulated classrooms generally improve accuracy but at higher computational cost. Role-playing with named students is more predictive than using only student IDs, and using demographically stratified names boosts performance further. Surprisingly, a mathematically weaker model (Gemma) produces better difficulty predictions than stronger math models (Llama, Qwen).", "conclusion": "LLMs are not reliable as direct judges of problem difficulty, but simulation-based approaches\u2014where LLMs role-play heterogeneous students and feed into an IRT framework\u2014can approximate real-world math item difficulty with substantial accuracy. Open-source models, including those with modest math skills, are well-suited to this task. Design choices in the simulation prompt (class size, naming, demographic stratification) materially affect prediction quality, highlighting important considerations for using LLMs as stand-ins for human pilot testing in assessment design."}}
{"id": "2601.10154", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10154", "abs": "https://arxiv.org/abs/2601.10154", "authors": ["Leonard N\u00fcrnberg", "Dennis Bontempi", "Suraj Pai", "Curtis Lisle", "Steve Pieper", "Ron Kikinis", "Sil van de Leemput", "Rahul Soni", "Gowtham Murugesan", "Cosmin Ciausu", "Miriam Groeneveld", "Felix J. Dorfner", "Jue Jiang", "Aneesh Rangnekar", "Harini Veeraraghavan", "Joeran S. Bosma", "Keno Bressem", "Raymond Mak", "Andrey Fedorov", "Hugo JWL Aerts"], "title": "MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging", "comment": "41 pages, 15 figures, 6 tables", "summary": "Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.", "AI": {"tldr": "MHub.ai is an open-source, container-based platform that standardizes access to medical imaging AI models to improve accessibility, comparability, and reproducibility.", "motivation": "AI for medical imaging is fragmented: models use diverse implementations and architectures, have inconsistent documentation, and are hard to reproduce or deploy clinically. This heterogeneity hinders routine clinical use, large-scale benchmarking, and trustworthy comparison of models across centers and studies.", "method": "The authors design and implement MHub.ai, an open-source platform that packages peer-reviewed medical imaging AI models into standardized software containers. Each container supports direct processing of DICOM and other formats, exposes a unified application interface, and embeds structured metadata. They curate an initial collection of segmentation, prediction, and feature-extraction models, attach public reference datasets for sanity checks, and build modular tools to allow others to adapt and contribute models. To demonstrate the platform, they perform a comparative evaluation of lung segmentation models within MHub.ai and provide dashboards, released segmentations, and evaluation metrics to enable inspection and reproduction.", "result": "MHub.ai can host diverse state-of-the-art medical imaging models across modalities in a uniform, containerized framework. It successfully runs multiple lung segmentation models with identical execution commands and standardized outputs, enabling direct side-by-side comparison. The platform supports community contributions, processes standard imaging formats, and distributes reference data and interactive dashboards that let users validate model behavior and reproduce the authors\u2019 comparative evaluation.", "conclusion": "Standardizing model packaging and interfaces via MHub.ai reduces configuration overhead, improves transparency, and facilitates reproducible benchmarking of AI models in medical imaging. By coupling containers, metadata, reference data, and public evaluation artifacts, the platform lowers practical barriers to applying, comparing, and translating AI models into clinical workflows."}}
{"id": "2601.09982", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09982", "abs": "https://arxiv.org/abs/2601.09982", "authors": ["David Samuel Setiawan", "Rapha\u00ebl Merx", "Jey Han Lau"], "title": "Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG", "comment": null, "summary": "Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust \"safety net,\" repairing severe failures in zero-shot domains.", "AI": {"tldr": "Hybrid NMT+LLM-RAG system nearly recovers in-domain translation quality for a low-resource language under domain shift.", "motivation": "Low-resource NMT systems experience large quality drops when applied to new domains, especially for languages with extremely limited training data and almost no digital resources. The authors want to understand and mitigate this domain shift issue for such a language, Dhao, where only New Testament data exists but Old Testament translation is needed.", "method": "Train and fine-tune a standard NMT model on New Testament Dhao data, then evaluate on both in-domain (NT) and out-of-domain (OT) test sets. Propose a hybrid pipeline: use the fine-tuned NMT model to produce an initial translation draft, and then use a Large Language Model enhanced with Retrieval-Augmented Generation (RAG) to refine the draft. Experiment with different retrieval configurations (e.g., number of retrieved examples and retrieval algorithms) and analyze their effects on performance.", "result": "The baseline fine-tuned NMT model scores 36.17 chrF++ on in-domain NT data but drops to 27.11 chrF++ on out-of-domain OT data. The proposed hybrid NMT+LLM-RAG system improves OT performance to 35.21 chrF++, recovering about 8.10 chrF++ and almost matching in-domain quality. Analysis indicates that the main driver of improvement is the quantity of retrieved examples used by the LLM rather than the specific retrieval method. Qualitative examples show the LLM can correct serious translation errors in zero-shot domains.", "conclusion": "A hybrid approach that combines NMT with an LLM using RAG can substantially mitigate domain shift for extremely low-resource languages, nearly restoring in-domain performance on an unseen domain. The LLM effectively functions as a safety net, repairing major translation errors caused by domain mismatch, and retrieval quantity plays a more crucial role than retrieval strategy in this setup."}}
{"id": "2601.10157", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10157", "abs": "https://arxiv.org/abs/2601.10157", "authors": ["Yusong Wang", "Jialun Shen", "Zhihao Wu", "Yicheng Xu", "Shiyin Tan", "Mingkun Xu", "Changshuo Wang", "Zixing Song", "Prayag Tiwari"], "title": "MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning", "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.", "AI": {"tldr": "The paper proposes MMPG, a multi-perspective protein graph learning framework using Mixture of Experts to fuse physical, chemical, and geometric residue interaction graphs, achieving better protein representations and downstream task performance.", "motivation": "Existing GNN-based protein representation methods typically build a single graph per protein, based on one interaction criterion (e.g., distance or bonds). This single-perspective view captures only part of the rich physical, chemical, and geometric relationships among residues, leading to incomplete protein representations and suboptimal performance on downstream tasks.", "method": "MMPG first constructs multiple residue-level graphs for each protein from different perspectives: physical, chemical, and geometric, each encoding distinct interaction properties. It then employs a Mixture of Experts (MoE) module that treats each perspective as an input and dynamically routes them to a set of experts. These experts are trained to capture perspective-specific features, pairwise synergies between perspectives, and eventually global consensus information across all perspectives. The MoE outputs are fused into a unified protein representation, which is used for various downstream prediction tasks.", "result": "The authors show empirically that the MoE experts automatically specialize: some focus on individual perspectives, others on cross-perspective pairwise interactions, and others on global integration across all perspectives. Quantitative experiments on four different downstream protein tasks demonstrate that MMPG yields superior protein representations and outperforms previous GNN-based PRL approaches.", "conclusion": "By explicitly modeling proteins as multi-perspective graphs and using a Mixture of Experts to adaptively fuse physical, chemical, and geometric interaction information at multiple levels, MMPG overcomes the limitations of single-perspective GNN-based PRL. This leads to richer protein representations and consistently improved performance across diverse protein-related tasks."}}
{"id": "2601.10003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10003", "abs": "https://arxiv.org/abs/2601.10003", "authors": ["Sanghyeok Choi", "Woosang Jeon", "Kyuseok Yang", "Taehyeong Kim"], "title": "SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction", "comment": null, "summary": "Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.", "AI": {"tldr": "The paper proposes SocraticKG, a method that builds knowledge graphs from text via an intermediate layer of 5W1H-guided question-answer pairs, improving both factual coverage and graph connectivity.", "motivation": "Existing LLM-based knowledge graph construction from unstructured text faces a key trade-off: maximizing factual coverage tends to fragment relations and hurt graph connectivity, while aggressively consolidating information early in the pipeline causes information loss and missed context. Direct triple extraction pipelines also often lose implicit relations and contextual dependencies and can make implicit reasoning errors due to weak grounding in the source text. The authors aim to design a method that preserves rich document-level semantics and contextual links while still producing a coherent, well-connected KG at scale.", "method": "The method, SocraticKG, introduces a structured intermediate representation based on question-answer (QA) pairs before extracting triples. It uses a 5W1H (who, what, when, where, why, how) guided QA expansion to systematically unfold the semantics of a document. By generating and answering these targeted questions, the system surfaces contextual dependencies and implicit relational links that direct KG extraction would miss. These QA pairs explicitly ground extracted information in the source document, reducing implicit reasoning errors. After this QA-based semantic scaffolding is constructed, triples are extracted to form the final KG.", "result": "On the MINE benchmark, SocraticKG effectively mitigates the coverage-connectivity trade-off in KG construction. It achieves better factual retention than baselines while preserving high structural cohesion of the resulting graph, even as the volume of extracted knowledge grows significantly. This indicates that the method scales in coverage without sacrificing graph quality or connectivity.", "conclusion": "QA-mediated semantic scaffolding\u2014implemented via 5W1H-guided question-answer expansions\u2014plays a crucial role in organizing document-level semantics before triple extraction. By inserting this intermediate QA layer, SocraticKG enables the construction of knowledge graphs that are both richer in facts and more structurally coherent, leading to more reliable and usable KGs for downstream reasoning tasks."}}
{"id": "2601.10169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10169", "abs": "https://arxiv.org/abs/2601.10169", "authors": ["Boaz Carmeli", "Ron Meir", "Yonatan Belinkov"], "title": "CtD: Composition through Decomposition in Emergent Communication", "comment": null, "summary": "Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed \"Composition through Decomposition\", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.", "AI": {"tldr": "The paper proposes a two-step method that lets neural agents learn compositional language to describe unseen images, sometimes achieving zero-shot generalization.", "motivation": "Humans can flexibly combine known concepts to describe new situations, but neural models often struggle with such compositional generalization; this work aims to build agents that can learn and use compositional structure to talk about novel images.", "method": "They introduce a procedure called \"Composition through Decomposition\" with two phases: (1) Decompose \u2013 agents, via a multi-target coordination game, learn to break images into basic concepts and store them in a shared codebook; (2) Compose \u2013 using this learned codebook, agents generate descriptions for new, unseen images by combining basic concepts into more complex phrases, testing whether this yields compositional generalization.", "result": "Agents trained with this method can describe previously unseen images by recombining learned basic concepts, and in some cases they succeed in the composition phase in a zero-shot manner, without further training.", "conclusion": "The study shows that neural agents can acquire and exploit compositional structure for visual description through a two-step decomposition/composition training scheme, demonstrating that structured, codebook-based learning can enable zero-shot compositional generalization."}}
{"id": "2601.10020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10020", "abs": "https://arxiv.org/abs/2601.10020", "authors": ["Lingfei Qian", "Mauro Giuffre", "Yan Wang", "Huan He", "Qianqian Xie", "Xuguang Ai", "Xeuqing Peng", "Fan Ma", "Ruey-Ling Weng", "Donald Wright", "Adan Wang", "Qingyu Chen", "Vipina K. Keloth", "Hua Xu"], "title": "EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records", "comment": null, "summary": "Clinical decision-making increasingly relies on timely and context-aware access to patient information within Electronic Health Records (EHRs), yet most existing natural language question-answering (QA) systems are evaluated solely on benchmark datasets, limiting their practical relevance. To overcome this limitation, we introduce EHRNavigator, a multi-agent framework that harnesses AI agents to perform patient-level question answering across heterogeneous and multimodal EHR data. We assessed its performance using both public benchmark and institutional datasets under realistic hospital conditions characterized by diverse schemas, temporal reasoning demands, and multimodal evidence integration. Through quantitative evaluation and clinician-validated chart review, EHRNavigator demonstrated strong generalization, achieving 86% accuracy on real-world cases while maintaining clinically acceptable response times. Overall, these findings confirm that EHRNavigator effectively bridges the gap between benchmark evaluation and clinical deployment, offering a robust, adaptive, and efficient solution for real-world EHR question answering.", "AI": {"tldr": "Introduces EHRNavigator, a multi-agent system for patient-level question answering over heterogeneous, multimodal EHR data, evaluated in realistic clinical settings and achieving 86% accuracy with acceptable response time.", "motivation": "Existing EHR question-answering systems are mostly tested only on benchmark datasets, which do not reflect real clinical conditions such as heterogeneous schemas, temporal reasoning, and multimodal data, thereby limiting their real-world utility in supporting clinical decision-making.", "method": "Develop a multi-agent framework, EHRNavigator, that uses AI agents to perform patient-level QA over heterogeneous and multimodal EHRs. Evaluate it on both public benchmarks and institutional datasets under realistic hospital conditions involving diverse schemas, temporal reasoning needs, and multimodal evidence integration, complemented by clinician-validated chart review.", "result": "EHRNavigator generalizes well across datasets and real-world hospital environments, achieving 86% accuracy on real clinical cases while maintaining response times acceptable for clinical workflows, as confirmed by quantitative metrics and clinician chart review.", "conclusion": "EHRNavigator closes the gap between benchmark-only evaluation and practical clinical deployment, providing a robust, adaptive, and efficient solution for real-world EHR question answering that works across heterogeneous, multimodal patient data and realistic hospital conditions."}}
{"id": "2601.10191", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10191", "abs": "https://arxiv.org/abs/2601.10191", "authors": ["Mathieu Cherpitel", "Janne Luijten", "Thomas B\u00e4ck", "Camiel Verhamme", "Martijn Tannemaat", "Anna Kononova"], "title": "How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series", "comment": null, "summary": "Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.", "AI": {"tldr": "They propose and test a workflow to decide how much and how to downsample high\u2011frequency nEMG signals so that diagnostic information and classification performance are preserved while computation is reduced.", "motivation": "Needle EMG signals are recorded at high and variable sampling rates, making feature\u2011based machine\u2011learning models computationally expensive, especially for near real\u2011time diagnosis. Downsampling could ease the computational burden, but it is unclear how different downsampling settings affect signal information relevant for neuromuscular disease classification and thus model performance.", "method": "They design a systematic workflow that: (1) applies different downsampling algorithms and factors to high\u2011frequency nEMG time series; (2) quantifies waveform changes using shape\u2011based distortion metrics that focus on morphology and peaks; (3) trains and evaluates existing feature\u2011based ML classifiers on the original and downsampled data for a three\u2011class NMD task; and (4) analyzes the resulting feature spaces to understand how downsampling alters discriminative information. They compare shape\u2011aware downsampling methods with standard decimation.", "result": "The workflow successfully identifies downsampling configurations that substantially reduce computational cost yet maintain diagnostic information and good classification performance. Shape\u2011aware downsampling algorithms preserve peak structures and overall waveform morphology better than simple decimation, which is reflected in lower distortion scores and stronger predictive performance after downsampling.", "conclusion": "Their workflow offers practical guidance on choosing downsampling strategies for nEMG that balance data reduction with diagnostic accuracy, enabling near real\u2011time analysis. Because it couples shape\u2011based distortion measures with classification and feature\u2011space analysis, the approach is generalisable to other high\u2011frequency time\u2011series domains where one must trade off computational efficiency against model performance."}}
{"id": "2601.10033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10033", "abs": "https://arxiv.org/abs/2601.10033", "authors": ["Wan Jou She", "Lis Kanashiro Pereira", "Fei Cheng", "Sakiko Yahata", "Panote Siriaraya", "Eiji Aramaki"], "title": "EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels", "comment": null, "summary": "This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.", "AI": {"tldr": "EmplifAI is a Japanese empathetic dialogue dataset for chronic disease support, annotated with 28 fine-grained emotions, used to train and evaluate LLMs for emotionally aligned medical support dialogues.", "motivation": "Patients with chronic medical conditions undergo complex, shifting emotional states that current dialogue datasets and models do not adequately capture, especially in Japanese and in medically grounded contexts. There is a need for a high-quality, emotion-rich dataset to better train and evaluate empathetic dialogue systems that can support these patients.", "method": "The authors created EmplifAI, a situation-based Japanese dialogue dataset grounded in 28 emotion categories adapted from GoEmotions. They defined 280 medically contextualized situations and collected 4,125 two-turn dialogues via crowdsourcing, followed by expert review. They then evaluated several LLMs by measuring emotional alignment between situations and dialogues using BERTScore. Finally, they fine-tuned a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) on EmplifAI and compared its outputs using both LLM-as-a-Judge and human evaluations to validate the evaluation pipeline and examine correlations and risks.", "result": "The dataset encompasses 280 medical situations and 4,125 two-turn dialogues annotated with 28 fine-grained emotions. BERTScore-based evaluation of emotional alignment across multiple LLMs yields F1 scores of 0.83. Fine-tuning the baseline Japanese LLM on EmplifAI improves fluency, general empathy, and emotion-specific empathy. Correlation analysis shows that LLM-as-a-Judge scores align meaningfully with human ratings, while also revealing potential risks in relying solely on automated judges.", "conclusion": "EmplifAI provides a valuable Japanese empathetic dialogue resource tailored to chronic disease contexts, enabling better training and evaluation of emotionally aware dialogue systems. The dataset and experiments demonstrate that fine-tuning on EmplifAI significantly enhances empathetic communication abilities of LLMs, and that LLM-as-a-Judge can approximate human evaluations, though with caveats and risks that warrant careful consideration in future work."}}
{"id": "2601.10193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10193", "abs": "https://arxiv.org/abs/2601.10193", "authors": ["Jiujiu Chen", "Weijun Zeng", "Shaofeng Hu", "Sihong Xie", "Hui Xiong"], "title": "GFM4GA: Graph Foundation Model for Group Anomaly Detection", "comment": null, "summary": "Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.", "AI": {"tldr": "They propose GFM4GA, a graph foundation model specially designed for detecting anomalous groups in networks, using dual-level contrastive pretraining and few-shot finetuning to outperform prior methods.", "motivation": "Existing graph foundation models work well for detecting anomalous individual nodes but fail for group anomalies, where abnormality emerges only when considering nodes jointly. Group anomaly detection in networks is important yet challenging due to diverse and subtle group-level patterns and limited labeled data. The authors aim to develop a scalable, label-efficient framework that can generalize to unseen types of group anomalies.", "method": "They design GFM4GA, a graph foundation model tailored for group anomaly detection. In pretraining, they use dual-level contrastive learning: (1) feature-based estimation to learn robust feature representations related to potential anomalies, and (2) group extraction to model structural and group-level inconsistencies. For downstream tasks, they apply few-shot finetuning under parameter-constrained settings and weight the loss by group anomaly proportions. They further enhance adaptability to unseen group anomalies via group context representations derived from labeled anomalous neighbors.", "result": "On benchmark datasets, GFM4GA outperforms both specialized group anomaly detectors and existing GFMs that target individual anomalies, with average gains of 2.85% in AUROC and 2.55% in AUPRC.", "conclusion": "A dedicated graph foundation model that explicitly captures group-level structures and inconsistencies, pretrained via dual-level contrastive learning and finetuned in a label-efficient way, can significantly improve group anomaly detection over both traditional methods and node-focused GFMs."}}
{"id": "2601.10064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10064", "abs": "https://arxiv.org/abs/2601.10064", "authors": ["Zhenghao Liu", "Zhuoyang Wu", "Xinze Li", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "Long-Chain Reasoning Distillation via Adaptive Prefix Alignment", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.", "AI": {"tldr": "The paper introduces P-ALIGN, a distillation framework that improves small LLMs\u2019 mathematical reasoning by adaptively using only the most useful prefixes of teacher chain-of-thought (CoT) trajectories, achieving over 3% gains on math benchmarks.", "motivation": "While long teacher chain-of-thoughts can boost reasoning, they are often too long, noisy, and structurally complex for smaller student models to learn effectively. This causes a mismatch between the supervision signal and the student\u2019s capacity, limiting the benefits of reasoning distillation. The authors aim to better exploit teacher CoTs by selecting only the parts that truly help student learning.", "method": "The authors propose Prefix-ALIGNment distillation (P-ALIGN). For each teacher-generated reasoning trajectory, P-ALIGN adaptively decides a truncation point based on whether the remaining suffix is concise and sufficient as guidance. It then discards the less useful or confusing suffix and uses only the teacher\u2019s prefix as the supervision target. Training encourages the student model to align its own reasoning prefixes with these selected teacher prefixes, effectively performing prefix-level CoT distillation.", "result": "On multiple mathematical reasoning benchmarks, models trained with P-ALIGN outperform all baselines by more than 3% in accuracy. Analytical experiments show that the teacher prefixes chosen by P-ALIGN contain higher-quality, more informative supervision, and that removing redundant or uncertain suffix parts reduces the negative impact on student learning.", "conclusion": "Adaptive prefix-based distillation is more effective than using full, unfiltered teacher reasoning trajectories. By aligning students to carefully selected teacher prefixes, P-ALIGN better matches the student\u2019s learning capacity, improves mathematical reasoning performance, and mitigates harms from noisy, redundant, or uncertain parts of teacher CoTs. The approach is practical and reproducible, with code publicly released."}}
{"id": "2601.10215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10215", "abs": "https://arxiv.org/abs/2601.10215", "authors": ["Alex Dantart", "Marco K\u00f3vacs-Navarro"], "title": "Topo-RAG: Topology-aware retrieval for hybrid text-table documents", "comment": null, "summary": "In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.\n  This work presents Topo-RAG, a framework that challenges the assumption that \"everything is text\". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.", "AI": {"tldr": "Topo-RAG is a Retrieval-Augmented Generation framework that treats enterprise documents as a mix of narrative text and structured tables, avoiding naive text linearization and instead using specialized retrieval for each modality.", "motivation": "Existing RAG systems typically linearize complex tabular and structured data into plain text, assuming an embedding model can encode all structure into a single vector. Prior work shows this is mathematically insufficient, leading to poor retrieval and understanding in enterprise settings where documents mix narrative and structured content.", "method": "The paper introduces Topo-RAG, a dual-architecture retrieval framework. Narrative text is handled by standard dense retrievers, while tabular data is processed with a Cell-Aware Late Interaction mechanism that explicitly preserves cell-level spatial relationships and table topology rather than flattening them into linear text. The system then supports hybrid queries over both text and tables.", "result": "On SEC-25, a synthetic enterprise corpus designed to mimic real hybrid document complexity, Topo-RAG achieves an 18.4% improvement in nDCG@10 for hybrid queries compared to RAG baselines that rely on simple linearization of tables into text.", "conclusion": "Respecting the topology of structured data in RAG\u2014rather than forcing \u201ceverything is text\u201d via linearization\u2014leads to substantially better retrieval performance on complex enterprise-like corpora, indicating that geometry and structure of information are crucial for effective question answering over mixed narrative and tabular content."}}
{"id": "2601.10080", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10080", "abs": "https://arxiv.org/abs/2601.10080", "authors": ["Letian Peng", "Kun Zhou", "Longfei Yun", "Yupeng Hou", "Jingbo Shang"], "title": "Deriving Character Logic from Storyline as Codified Decision Trees", "comment": null, "summary": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.", "AI": {"tldr": "The paper introduces Codified Decision Trees (CDT), an executable, interpretable framework to build reliable behavioral profiles for role\u2011playing agents, significantly outperforming human-written and prior induced profiles.", "motivation": "Existing role-playing agents depend on behavioral profiles that are unstructured, non-executable, and weakly validated. This makes agent behavior brittle and inconsistent across different narrative contexts. There is a need for a systematic, data-driven way to represent, validate, and execute behavioral profiles so that agents can act consistently and transparently in diverse scenarios.", "method": "The authors propose Codified Decision Trees (CDT), which learn behavioral profiles from large-scale narrative data. CDT encodes profiles as decision trees: internal nodes are validated scene conditions, and leaves are grounded behavioral statements. The learning process iteratively (1) induces candidate scene-action rules from data, (2) validates these rules empirically, and (3) specializes and refines them hierarchically to form a tree that supports deterministic, context-appropriate rule retrieval. The structure is designed to be interpretable and easily updated.", "result": "On benchmarks covering 85 characters from 16 narrative artifacts, CDT-based profiles substantially outperform both human-written behavioral profiles and previous automatic profile-induction methods. The results show that the learned trees provide more reliable and context-appropriate behaviors for RP agents.", "conclusion": "Codified, executable, and empirically validated decision-tree representations of behavioral profiles significantly improve the grounding and reliability of role-playing agents. CDT offers a transparent, interpretable structure that supports consistent decision-making, easier inspection, and principled updates, making it a strong alternative to ad-hoc, unstructured profiles."}}
{"id": "2601.10245", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10245", "abs": "https://arxiv.org/abs/2601.10245", "authors": ["Vansh Kapoor", "Aman Gupta", "Hao Chen", "Anurag Beniwal", "Jing Huang", "Aviral Kumar"], "title": "TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks", "comment": null, "summary": "Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.", "AI": {"tldr": "The paper introduces TRIM, a step-level routing method for multi-step reasoning that selectively uses large models only on critical steps to prevent cascading errors, achieving much higher cost efficiency than prior routing approaches.", "motivation": "Multi-step reasoning with LLMs, such as math problem solving, often fails due to cascading errors where one mistaken step ruins the entire solution. Existing routing methods assign the whole query to a single model and treat all reasoning steps equally, which wastes expensive large-model capacity on easy steps and still fails on crucial hard ones. There is a need for a more fine-grained, step-aware way to allocate computational budget that maintains or improves accuracy while reducing cost.", "method": "The authors propose TRIM (Targeted routing in multi-step reasoning tasks), which operates at the step level instead of the whole-query level. TRIM uses process reward models to estimate correctness and identify potentially erroneous or critical steps during reasoning. Based on step-level uncertainty and a global budget, TRIM routes only those critical steps to a stronger, more expensive model while allowing a cheaper model to handle the rest. The framework includes several routing strategies: a simple threshold-based policy that triggers routing when uncertainty exceeds a fixed threshold, and more sophisticated policies that explicitly reason over long-horizon accuracy\u2013cost trade-offs and uncertainty in step correctness estimates.", "result": "On the MATH-500 benchmark, even the simplest threshold-based TRIM policy outperforms prior routing methods in cost efficiency, achieving up to 5\u00d7 higher efficiency. More advanced TRIM policies can match the accuracy of a strong, fully expensive model while using 80% fewer tokens from that model. On more challenging benchmarks such as AIME, TRIM attains up to 6\u00d7 higher cost efficiency. These gains hold across different math reasoning tasks, suggesting strong generalization of the approach.", "conclusion": "Step-level difficulty and correctness signals are fundamental for efficient multi-step reasoning with LLMs. By targeting only critical reasoning steps for intervention with stronger models, TRIM substantially reduces computation cost without sacrificing accuracy, outperforming traditional whole-query routing methods. The work indicates that fine-grained, uncertainty-aware routing based on process-level feedback is a powerful paradigm for scalable and robust reasoning systems."}}
{"id": "2601.10082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10082", "abs": "https://arxiv.org/abs/2601.10082", "authors": ["Vipasha Bansal", "Elizabeth Brown", "Chelsea Kendrick", "Benjamin Pong", "William D. Lewis"], "title": "Is MT Ready for the Next Crisis or Pandemic?", "comment": null, "summary": "Communication in times of crisis is essential. However, there is often a mismatch between the language of governments, aid providers, doctors, and those to whom they are providing aid. Commercial MT systems are reasonable tools to turn to in these scenarios. But how effective are these tools for translating to and from low resource languages, particularly in the crisis or medical domain? In this study, we evaluate four commercial MT systems using the TICO-19 dataset, which is composed of pandemic-related sentences from a large set of high priority languages spoken by communities most likely to be affected adversely in the next pandemic. We then assess the current degree of ``readiness'' for another pandemic (or epidemic) based on the usability of the output translations.", "AI": {"tldr": "The paper evaluates how well commercial machine translation systems handle crisis- and medical-domain text for many high-priority, low-resource languages, using the TICO-19 pandemic dataset, to estimate how prepared we are linguistically for future pandemics.", "motivation": "In crises such as pandemics, effective multilingual communication between governments, aid providers, medical staff, and affected communities is critical, yet there is often a language mismatch, especially for low-resource languages. Commercial MT is a natural tool to bridge this gap, but its actual effectiveness and reliability in low-resource, crisis-related settings are unclear and need systematic evaluation.", "method": "The authors use the TICO-19 dataset, which contains pandemic-related sentences in many high-priority languages, focusing on low-resource ones. They run four commercial machine translation systems on this dataset, translating to and from these languages, and then evaluate the usability and quality of the resulting translations, particularly for crisis/medical communication needs.", "result": "The study obtains comparative performance measurements of four commercial MT systems on pandemic-related content involving high-priority, often low-resource languages. The results quantify how accurate and usable the translations are for crisis and medical contexts and reveal strengths and weaknesses across systems and languages, especially highlighting limitations for low-resource languages.", "conclusion": "Based on the observed translation quality and usability, the authors draw conclusions about the current level of machine translation \"readiness\" for a future pandemic or epidemic. They likely conclude that while commercial MT can be helpful, its support for many low-resource, high-priority languages\u2014and especially for critical crisis/medical content\u2014remains incomplete, indicating a need for further improvements and targeted development."}}
{"id": "2601.10254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10254", "abs": "https://arxiv.org/abs/2601.10254", "authors": ["Irina Abdullaeva", "Anton Vasiliuk", "Elizaveta Goncharova", "Temurbek Rahmatullaev", "Zagorulko Ivan", "Maxim Kurkin", "Andrey Kuznetsov"], "title": "NoReGeo: Non-Reasoning Geometry Benchmark", "comment": null, "summary": "We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.", "AI": {"tldr": "NoReGeo is a benchmark that tests whether LLMs possess intrinsic geometric understanding, separate from algebraic or symbolic reasoning, and shows that current models perform poorly.", "motivation": "Most existing geometry benchmarks for LLMs evaluate algebraic or symbolic reasoning (e.g., solving equations derived from geometric setups) rather than whether models inherently encode spatial relationships and geometric properties. The authors want to isolate and measure this intrinsic geometric understanding, questioning whether current LLM training pipelines naturally yield such capabilities.", "method": "They construct NoReGeo, a benchmark of 2,500 very simple geometric problems across 25 categories, each designed so that a model with genuine geometric intuition and knowledge of object locations could solve them without explicit reasoning chains or algebraic manipulation. They then evaluate multiple state-of-the-art LLMs (including frontier models like GPT\u20114) on binary classification tasks derived from these problems and perform ablations, especially around fine-tuning, to see if such understanding can be induced post hoc.", "result": "All evaluated models, including leading frontier systems, achieve at most about 65% accuracy on the benchmark\u2019s binary tasks, which is only moderately above chance. The ablation studies further show that fine-tuning alone does not significantly improve performance in a way that would indicate emergence of robust geometric understanding.", "conclusion": "Current LLMs lack strong, native geometric cognition: they do not reliably encode or exploit fundamental spatial and geometric relationships when reasoning is removed from the task. Moreover, standard fine-tuning pipelines are insufficient to instill this capability after pretraining, implying that specialized training objectives or architectures will be needed to build models with genuine geometric understanding. NoReGeo offers a standardized benchmark to drive and measure progress toward this goal."}}
{"id": "2601.10085", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10085", "abs": "https://arxiv.org/abs/2601.10085", "authors": ["Viet Cuong Nguyen", "Nhi Yen Nguyen", "Kristin A. Candan", "Mary Conlon", "Vanessa Rumie", "Kristen Risola", "Srijan Kumar", "Munmun De Choudhury"], "title": "CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking", "comment": "46 pages", "summary": "Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.", "AI": {"tldr": "The paper presents CALM-IT, a framework that models therapist-client conversational dynamics as dual-actor state-space processes to generate and evaluate long-form Motivational Interviewing dialogues more coherently and effectively than standard LLM setups.", "motivation": "Existing LLMs used in mental health contexts can produce fluent responses but fail to maintain coherent, goal-directed therapeutic dialogue over long interactions. They optimize for immediate next-turn quality instead of tracking therapeutic progress and evolving client-therapist alignment, causing brittleness and drift in long-horizon conversations. The paper aims to address this gap by explicitly modeling evolving conversational state and dual-actor dynamics.", "method": "The authors introduce CALM-IT, which represents therapist-client Motivational Interviewing as a bidirectional state-space process. Both the therapist and client are modeled as agents that continually update inferred states\u2014such as alignment, mental state, and short-term goals\u2014to inform strategy selection and utterance generation. The framework is used both to generate long-form synthetic MI dialogues and to evaluate their properties, with comparisons against strong baseline LLM approaches on metrics like Effectiveness, Goal Alignment, stability over conversation length, and therapist redirection behavior.", "result": "In large-scale evaluations, CALM-IT outperforms strong LLM baselines on Effectiveness and Goal Alignment, and its performance degrades less as conversation length increases, demonstrating improved stability for long-horizon dialogue. Although CALM-IT triggers fewer explicit therapist redirections of the client, it attains the highest client acceptance rate of 64.3%, suggesting better-timed and more appropriately targeted interventions compared with baselines.", "conclusion": "Modeling evolving dual-actor conversational state\u2014encompassing alignment, mental states, and goals\u2014is crucial for high-quality long-form synthetic therapeutic dialogues. CALM-IT demonstrates that a state-space, dual-agent formulation of Motivational Interviewing can significantly improve coherence, goal alignment, and intervention timing over standard LLM approaches, offering a more robust framework for mental health-related conversational AI systems."}}
{"id": "2601.10306", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10306", "abs": "https://arxiv.org/abs/2601.10306", "authors": ["Xin Guan", "Zijian Li", "Shen Huang", "Pengjun Xie", "Jingren Zhou", "Jiuxin Cao"], "title": "Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning", "comment": null, "summary": "While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.", "AI": {"tldr": "They introduce EAPO, an RL framework that adds dense, evidence-focused rewards and a co-evolving reward model to improve long-context reasoning of LLMs, especially for needle-in-a-haystack evidence retrieval.", "motivation": "Standard RL for LLM reasoning in long-context settings relies on sparse, outcome-only rewards. This fails to punish ungrounded lucky guesses and leaves the critical evidence retrieval process unsupervised, which is the main bottleneck for long-context reasoning. The authors want a method that directly trains models to find and use the right evidence, not just to output correct final answers.", "method": "1) Formulate an Evidence-Augmented Reasoning paradigm and empirically show, via Tree-Structured Evidence Sampling, that accurate evidence extraction is the key bottleneck for long-context reasoning. 2) Propose EAPO, a specialized RL algorithm. A reward model computes a Group-Relative Evidence Reward that compares candidate reasoning/evidence chains and supplies dense process-level feedback focused on evidence quality. 3) Introduce an Adaptive Reward-Policy Co-Evolution mechanism: during training, they iteratively update the reward model using rollouts that are consistent with final outcomes, so that the reward model becomes better at distinguishing good vs bad evidence and can keep providing accurate process supervision as the policy improves.", "result": "Across eight long-context reasoning benchmarks, EAPO-trained models outperform state-of-the-art baselines, indicating that evidence-focused dense rewards and adaptive co-evolution of policy and reward model substantially improve needle-in-a-haystack style reasoning and overall long-context performance.", "conclusion": "Precise evidence extraction, rather than purely logical manipulation, is the main bottleneck in long-context LLM reasoning. By explicitly supervising evidence quality with a group-relative evidence reward and co-evolving a reward model alongside the policy, EAPO provides dense, accurate process supervision that significantly boosts long-context reasoning compared to existing RL approaches."}}
{"id": "2601.10108", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.10108", "abs": "https://arxiv.org/abs/2601.10108", "authors": ["Yiming Ren", "Junjie Wang", "Yuxin Meng", "Yihang Shi", "Zhiqiang Lin", "Ruihang Chu", "Yiran Xu", "Ziming Li", "Yunfei Zhao", "Zihan Wang", "Yu Qiao", "Ruiming Tang", "Minghao Liu", "Yujiu Yang"], "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature", "comment": null, "summary": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.", "AI": {"tldr": "They introduce a new benchmark (FITO / SIN-Bench) to test whether multimodal LLMs can ground their answers in explicit evidence from long scientific papers, not just get correct answers.", "motivation": "Existing evaluations for multimodal LLMs on long scientific documents (answer-only scores, synthetic needle-in-a-haystack tests) do not ensure that models genuinely use and link to the underlying evidence; models can guess or exploit shortcuts without showing causal, document-grounded reasoning. There is a need for tasks and metrics that require explicit, verifiable evidence chains across text and figures in real scientific papers.", "method": "They propose the FITO (Fish-in-the-Ocean) paradigm, which forces models to construct explicit, cross-modal evidence chains within full scientific documents. To support this, they build SIN-Data, a corpus that keeps the original interleaving of text and figures. On top of SIN-Data, they design SIN-Bench with four tasks of increasing difficulty: (1) SIN-Find for evidence discovery, (2) SIN-Verify for hypothesis verification using evidence, (3) SIN-QA for grounded question answering, and (4) SIN-Summary for evidence-anchored summarization. They also design a \u201cNo Evidence, No Score\u201d evaluation scheme that only scores predictions when they are grounded to verifiable evidence anchors, and they decompose evidence quality into matching, relevance, and logical consistency.", "result": "They evaluate eight multimodal LLMs on SIN-Bench. Grounding is found to be the major weakness across models. Gemini-3-pro has the highest average overall evidence-grounded score (0.573). GPT-5 achieves the highest raw answer accuracy on SIN-QA (0.767) but lags in evidence-aligned overall scores, indicating that many of its correct answers are not well supported by explicit, traceable evidence chains in the documents.", "conclusion": "The work shows that current multimodal LLMs can often produce correct answers about scientific documents without reliably grounding them in explicit, verifiable evidence. Their FITO paradigm, SIN-Data/SIN-Bench, and \u201cNo Evidence, No Score\u201d metric highlight a clear gap between answer correctness and evidence-based reasoning, and provide a more rigorous way to evaluate and improve document-grounded multimodal understanding."}}
{"id": "2601.10342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10342", "abs": "https://arxiv.org/abs/2601.10342", "authors": ["Cheng Lin Cheng", "Ting Chuan Lin", "Chai Kai Chang"], "title": "C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing", "comment": null, "summary": "Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the \"population bias\" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.", "AI": {"tldr": "The paper introduces C-GRASP, a guardrailed, clinically grounded reasoning pipeline that uses LLMs plus RAG to interpret heart rate variability (HRV) safely and transparently, improving emotion classification and clinical reasoning consistency while reducing physiological hallucinations.", "motivation": "Standard LLM-based interpretations of HRV suffer from \u201cphysiological hallucinations,\u201d such as misinterpreting respiratory sinus arrhythmia contamination, relying on unstable nonlinear metrics from short recordings, and overusing population norms instead of patient-specific baselines. This limits clinical reliability and safety of affective computing systems. The authors aim to create a more trustworthy, clinically grounded framework for HRV-based emotion inference and decision support.", "method": "They design C-GRASP, a multi-step, RAG-enhanced reasoning pipeline that breaks HRV interpretation into eight explicit, auditable reasoning stages. A key component is a Z-score Priority Hierarchy that prioritizes deviations from each subject\u2019s baseline over population statistics. The pipeline includes RSA-aware spectral guardrails to avoid contamination in frequency-domain HRV indices. They integrate C-GRASP with large reasoning models such as MedGemma3-thinking and evaluate it on the DREAMER affective dataset, plus perform ablation studies focusing on an individualized Delta Z-score module.", "result": "On 414 HRV trials from the DREAMER dataset, C-GRASP combined with high-level reasoning models attains 37.3% accuracy in 4-class emotion classification and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation experiments demonstrate that removing or altering the individualized Delta Z-score module noticeably degrades both classification performance and reasoning consistency, indicating it is the key logical anchor that counters population-level bias in native LLM behavior.", "conclusion": "C-GRASP substantially reduces physiological hallucinations in LLM-based HRV analysis by enforcing individualized, clinically plausible reasoning steps and spectral guardrails. It moves affective computing away from opaque black-box models toward transparent, evidence-based clinical decision support, thereby improving safety and interpretability and offering a path for more reliable AI use in biomedical engineering contexts."}}
{"id": "2601.10109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10109", "abs": "https://arxiv.org/abs/2601.10109", "authors": ["Lechen Zhang", "Yunxiang Zhang", "Wei Hu", "Lu Wang"], "title": "Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation", "comment": null, "summary": "Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.", "AI": {"tldr": "The paper proposes a data-efficient, skill-centric distillation framework that transfers reasoning abilities from large reasoning models to smaller ones using only 1k carefully selected training examples.", "motivation": "Distilling strong reasoning capabilities from large reasoning models into smaller student models typically requires large SFT datasets, which is costly and inefficient. The authors aim to reduce data requirements while still improving complex reasoning, focusing on how to choose and use training data more intelligently rather than just increasing its volume.", "method": "They introduce a two-part framework: (1) Skill-based data selection, which analyzes the student model\u2019s weaknesses and selects training instances that target these weaker reasoning skills from a 100k teacher-generated corpus; and (2) Skill-aware fine-tuning, which structures training so that the model explicitly decomposes problems into skills during reasoning, reinforcing those specific abilities rather than learning in an undifferentiated way.", "result": "Using only 1,000 selected examples from a 100k pool, their method improves performance over random SFT by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five math reasoning benchmarks, indicating efficient transfer of reasoning skills with far less data.", "conclusion": "Skill-centric training\u2014through targeted data selection and explicit skill decomposition during fine-tuning\u2014enables efficient distillation of reasoning abilities from large to smaller models, with performance gains especially concentrated in the skills emphasized during training."}}
{"id": "2601.10398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10398", "abs": "https://arxiv.org/abs/2601.10398", "authors": ["Xuancheng Ren", "Shijing Hu", "Zhihui Lu", "Jiangqi Huang", "Qiang Duan"], "title": "LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries", "comment": null, "summary": "In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.", "AI": {"tldr": "The paper proposes LatentRefusal, a lightweight safety layer for LLM-based text-to-SQL systems that detects unanswerable or underspecified queries using hidden activations rather than output text or uncertainty estimates.", "motivation": "LLM-based text-to-SQL systems can turn ambiguous or unanswerable queries into executable SQL that returns misleading results or violates safety constraints. Existing refusal methods based on instruction-following or uncertainty estimation are brittle or computationally expensive, making safe deployment difficult. The authors want a more reliable, efficient way to determine when the system should refuse to answer.", "method": "They formalize safe refusal as an answerability-gating problem: before execution, the system predicts whether a query is answerable given the schema. They introduce LatentRefusal, which uses intermediate hidden activations of the LLM as features. A new lightweight probing module, the Tri-Residual Gated Encoder, is trained to filter schema-related noise and highlight subtle cues of mismatch between the question and database schema that signal unanswerability. This probe is attached to existing text-to-SQL backbones with minimal overhead.", "result": "On four benchmarks involving ambiguous and unanswerable queries, LatentRefusal significantly improves identification of unanswerable cases, reaching an average F1 of 88.5% across two backbone models, while adding only about 2 ms of computation per query. Ablation and interpretability studies show that the Tri-Residual Gated Encoder and use of latent signals are key to the performance gains.", "conclusion": "LatentRefusal provides an efficient, attachable safety mechanism for text-to-SQL systems by leveraging latent activations to predict query answerability. It avoids the fragility of output-level refusals and the cost of uncertainty estimation while effectively gating execution on unsafe or underspecified inputs, substantially improving safe deployment of LLM-based database interfaces."}}
{"id": "2601.10122", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10122", "abs": "https://arxiv.org/abs/2601.10122", "authors": ["Ye Wang", "Jiaxing Chen", "Hongjiang Xiao"], "title": "Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends", "comment": null, "summary": "In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.", "AI": {"tldr": "The paper surveys role-playing language agents (RPLAs) built on large language models, summarizing their evolution, key technologies, datasets, evaluation methods, and future directions.", "motivation": "The rapid development of large language models has enabled role-playing agents that can simulate specific personas, but research on their underlying technologies, data, and evaluation is fragmented. The paper aims to systematically organize and analyze this emerging field to guide future work.", "method": "The authors conduct a systematic review of existing work on RPLAs. They categorize technological evolution into stages (rule-based templates, style imitation, cognitive simulation), summarize technical approaches for character modeling, memory and decision control, analyze corpus construction methods and challenges, and compare evaluation frameworks and tools. They also synthesize trends to propose future directions.", "result": "The review identifies psychological scale-driven character modeling, memory-augmented prompting, and motivation-situation-based behavior control as core technical paths for high-quality role-playing. It classifies data construction approaches and their issues (e.g., copyright, annotation), and organizes current evaluation dimensions (role knowledge, personality fidelity, value alignment, hallucination) with pros/cons of human, reward-model, and LLM-based assessments.", "conclusion": "Role-playing language agents have progressed from simple templates to cognitively richer agents with personality and memory. Building effective RPLAs requires integrated advances in persona modeling, memory mechanisms, decision control, dataset construction, and robust evaluation. Future work should explore dynamic personality evolution, multi-agent narratives, multimodal interactive environments, and connections to cognitive neuroscience to achieve more realistic and controllable role-playing agents."}}
{"id": "2601.10402", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10402", "abs": "https://arxiv.org/abs/2601.10402", "authors": ["Xinyu Zhu", "Yuzhu Cai", "Zexi Liu", "Bingyang Zheng", "Cheng Wang", "Rui Ye", "Jiaao Chen", "Hanrui Wang", "Wei-Chen Wang", "Yuzhi Zhang", "Linfeng Zhang", "Weinan E", "Di Jin", "Siheng Chen"], "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "comment": "26 pages. 5 figures", "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "AI": {"tldr": "The paper introduces ML-Master 2.0, an autonomous AI agent for ultra-long-horizon machine learning engineering that uses a hierarchical memory system (HCC) to maintain strategic coherence over days-long experimental cycles, achieving state-of-the-art performance on MLE-Bench.", "motivation": "Current large language model agents struggle with ultra-long-horizon autonomy: they lose strategic coherence, get overwhelmed by execution details, and cannot effectively integrate sparse, delayed feedback in real-world scientific and engineering workflows. The authors aim to build an AI agent that can autonomously conduct complex machine learning engineering projects over extended periods, using mechanisms that scale beyond static context windows.", "method": "The authors propose ML-Master 2.0, centered on Hierarchical Cognitive Caching (HCC). HCC is a multi-tier, computer-systems-inspired architecture for managing an agent\u2019s \"cognitive\" context over time. It structurally separates short-term execution traces from more stable knowledge and cross-task insights. The system dynamically distills low-level, transient logs into higher-level, lasting representations, allowing the agent to decouple immediate action planning from global, long-term experimental strategy and to operate effectively in ultra-long-horizon settings.", "result": "On OpenAI\u2019s MLE-Bench with 24-hour task budgets, ML-Master 2.0 achieves a 56.44% medal rate, which is reported as state-of-the-art among autonomous agents evaluated on this benchmark. This empirically supports the claim that HCC and the overall system design improve long-horizon performance in machine learning engineering tasks.", "conclusion": "The study concludes that ultra-long-horizon autonomy, implemented via structures like Hierarchical Cognitive Caching, can overcome the limitations of static context windows in LLM-based agents. ML-Master 2.0 serves as evidence that such architectures can enable scalable, strategically coherent autonomous exploration in machine learning engineering, pointing toward more general agentic AI systems capable of handling scientific discoveries of higher complexity than typical human workflows."}}
{"id": "2601.10156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10156", "abs": "https://arxiv.org/abs/2601.10156", "authors": ["Yutao Mou", "Zhangchi Xue", "Lijun Li", "Peiyang Liu", "Shikun Zhang", "Wei Ye", "Jing Shao"], "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback", "comment": "Work in Progress. Code available: https://github.com/MurrayTom/ToolSafe", "summary": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "AI": {"tldr": "The paper introduces a benchmark and guardrail framework to detect and prevent unsafe tool invocations by LLM agents at the step level, significantly reducing harmful actions while preserving task performance.", "motivation": "LLM-based agents can call external tools (APIs, code execution, system actions), which greatly extends their capabilities but also increases security risks. Existing safety work often focuses on static input-output filtering, not on monitoring each step of an agent\u2019s tool usage in real time. There is a lack of systematic benchmarks and methods for detecting unsafe tool invocations before they are executed, especially under adversarial conditions like prompt injection.", "method": "1) Build TS-Bench, a benchmark specifically for evaluating step-level safety detection of tool invocations in LLM agents. 2) Propose TS-Guard, a guardrail model trained with multi-task reinforcement learning to monitor agent interaction histories and predict whether the next tool call is unsafe. TS-Guard jointly learns to: (a) assess the harmfulness of a user or environment request, and (b) evaluate correlations between the proposed tool action and potential attacks. It outputs both safety judgments and explanatory feedback. 3) Design TS-Flow, a guardrail-feedback-driven reasoning framework that integrates TS-Guard into the reasoning loop of ReAct-style agents so that the agent can adjust its plan and tool usage based on the guardrail\u2019s feedback before executing tools.", "result": "Using TS-Flow with TS-Guard on ReAct-style agents leads to a 65% average reduction in harmful tool invocations and about a 10% improvement in successful completion of benign tasks under prompt injection attacks, as measured on the TS-Bench benchmark.", "conclusion": "Real-time, step-level monitoring of tool invocations is both feasible and effective for improving the safety of LLM agents. The combination of TS-Bench, TS-Guard, and TS-Flow offers a practical pipeline: benchmark unsafe tool use, train a guardrail that reasons over history to anticipate unsafe actions, and integrate this guardrail into agent reasoning to substantially curb harmful tool calls while maintaining or even improving benign task performance in adversarial settings."}}
{"id": "2601.10406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10406", "abs": "https://arxiv.org/abs/2601.10406", "authors": ["Weiping Fu", "Bifan Wei", "Jingyi Hao", "Yushun Zhang", "Jian Zhang", "Jiaxin Wang", "Bo Li", "Yu He", "Lingling Zhang", "Jun Liu"], "title": "ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics", "comment": null, "summary": "Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.", "AI": {"tldr": "The paper proposes ErrEval, an error-aware framework for evaluating automatically generated questions by explicitly diagnosing errors before scoring, which improves correlation with human judgments and reduces overestimation of low-quality questions.", "motivation": "Automatic Question Generation systems often output flawed questions containing issues such as factual hallucinations and mismatches with intended answers. Current evaluation methods, including those based on large language models, generally treat evaluation as a holistic black-box task and lack explicit modeling of these specific error types. This causes serious defects to be under-detected and leads to inflated quality scores that do not match human assessments. The paper is motivated by the need for a more transparent, fine-grained, and reliable evaluation paradigm that can better capture concrete error patterns in generated questions.", "method": "The authors propose ErrEval, an Error-aware Evaluation framework that transforms question generation evaluation into a two-stage process. In stage one, a lightweight plug-and-play Error Identifier module analyzes generated questions and detects common errors across structural (e.g., formatting, missing components), linguistic (e.g., grammar, fluency), and content-related (e.g., factual correctness, answer mismatch) dimensions, assigning each error to a category. In stage two, these explicit diagnostic signals are fed as structured evidence into an LLM-based evaluator, which uses them to produce more grounded, fine-grained quality scores instead of relying solely on holistic, black-box judgments. The framework is model-agnostic and can be integrated with different LLM evaluators.", "result": "Through experiments on three QG evaluation benchmarks, ErrEval shows improved alignment between automatic scores and human judgments compared to baseline evaluation methods, including standard LLM-based evaluators without explicit error diagnostics. The results indicate that incorporating structured error information helps evaluators better reflect actual question quality. Additional analyses demonstrate that ErrEval particularly helps avoid over-scoring poor-quality questions, suggesting that explicit error modeling makes evaluations more conservative and accurate in the presence of serious defects.", "conclusion": "Explicit error-aware evaluation, as realized by ErrEval, leads to more reliable and human-aligned assessment of automatically generated questions. By decoupling error diagnosis from scoring and feeding fine-grained error evidence into LLM-based evaluators, ErrEval reduces the tendency to overlook critical defects and mitigates overestimation of low-quality questions. The approach is flexible, plug-and-play, and can serve as a general strategy for improving the robustness and transparency of evaluation systems for question generation and potentially other NLG tasks."}}
{"id": "2601.10159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10159", "abs": "https://arxiv.org/abs/2601.10159", "authors": ["Guimin Hu", "Meng Li", "Qiwei Peng", "Lijie Hu", "Boyan Xu", "Ruichu Cai"], "title": "What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models", "comment": null, "summary": "Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.", "AI": {"tldr": "The paper studies how experts in Mixture-of-Experts (MoE) language models are activated, identifying domain-specialized and highly influential (driver) experts using entropy and causal-effect metrics, and shows that reweighting these experts improves performance and interpretability.", "motivation": "Existing interpretability work mostly examines layers or neurons in standard Transformers, leaving the behavior of experts in MoE large language models less understood. Inspired by functional specialization in the human brain, the paper aims to understand which experts are used for different tasks or domains, how they influence model outputs, and how specific tokens trigger particular experts, to better interpret and potentially improve MoE models.", "method": "The authors analyze expert activation patterns in MoE language models over three public domains. They define two quantitative measures: (1) an entropy-based metric that captures whether an expert is strongly preferred for a particular domain (to identify domain experts), and (2) a causal-effect metric that measures how strongly activating a given expert affects the model\u2019s output (to identify driver experts). They also study token-level associations by examining which tokens in a sequence tend to trigger specific experts, focusing on positional effects (e.g., early vs. late tokens). Finally, they run interventions by adjusting the weights of identified domain and driver experts to test performance impact.", "result": "The analysis reveals that among activated experts, some consistently specialize in particular domains (domain experts), while others have a disproportionately strong causal effect on performance (driver experts). They find that earlier tokens in sentences are more likely to trigger driver experts. Moreover, modifying the gating/weighting of domain and driver experts yields significant performance improvements across all three tested MoE models and domains.", "conclusion": "The paper concludes that MoE models contain experts with clear domain specialization and others that play a decisive, causally influential role in predictions. Early tokens are especially important in activating these driver experts. By identifying and reweighting domain and driver experts, it is possible to both improve performance and gain deeper insight into the internal mechanisms of MoE models, enhancing their interpretability."}}
{"id": "2601.10413", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.10413", "abs": "https://arxiv.org/abs/2601.10413", "authors": ["Haiyue Yuan", "Nikolay Matyunin", "Ali Raza", "Shujun Li"], "title": "LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies", "comment": null, "summary": "Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.", "AI": {"tldr": "The paper proposes LADFA, a framework that uses LLMs with retrieval-augmented generation to automatically extract and analyse personal data flows from privacy policies.", "motivation": "Privacy policies are long, complex, and legally worded, making them hard for users to understand and for researchers to analyse at scale. Existing automated analyses and even LLM-based approaches for extracting data flows are limited. There is a need for an accurate, scalable, and reusable computational framework that can convert unstructured privacy policy text into structured representations of personal data flows for deeper analysis and insight.", "method": "The authors design LADFA, an end-to-end framework composed of three main components: (1) a pre-processor that prepares unstructured privacy policy text for analysis; (2) an LLM-based processor enhanced with retrieval-augmented generation (RAG) and a custom knowledge base built from prior studies on privacy policies and data flows, which extracts personal data flows from the text; and (3) a post-processor that constructs a personal data flow graph from the extracted flows and performs analyses over this graph to support insight discovery. They validate LADFA using a case study of ten automotive-industry privacy policies.", "result": "In the case study on ten automotive privacy policies, LADFA is shown to effectively and accurately process unstructured policy text, extract personal data flows, and construct corresponding data flow graphs that can be analysed to derive insights about data practices. The framework proves functional end-to-end, from raw text to graph-based analysis.", "conclusion": "LADFA demonstrates that combining LLMs with RAG and a tailored knowledge base can reliably extract and analyse personal data flows from complex privacy policies. The framework is flexible and customisable, so it can be adapted to other text-based analysis tasks beyond privacy policy analysis, suggesting broader applicability for structured extraction and graph-based reasoning over unstructured documents."}}
{"id": "2601.10160", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10160", "abs": "https://arxiv.org/abs/2601.10160", "authors": ["Cameron Tice", "Puria Radmard", "Samuel Ratnam", "Andy Kim", "David Africa", "Kyle O'Brien"], "title": "Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment", "comment": null, "summary": "Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai", "AI": {"tldr": "The paper studies how discourse about AI systems in pretraining data causally affects language model alignment, showing that increasing misalignment-themed text yields more misaligned behavior, while increasing alignment-themed text improves alignment.", "motivation": "There is a great deal of text about AI systems in pretraining corpora, but it is unclear how descriptions of AI behavior\u2014especially negative or misaligned descriptions\u2014causally influence the alignment of resulting models. Existing work focuses mostly on post-training alignment, leaving a gap in understanding how pretraining data itself shapes models\u2019 alignment priors and whether negative portrayals of AI can become self-fulfilling.", "method": "The authors pretrain 6.9B-parameter language models on corpora that are systematically modified to contain different proportions of discourse about AI, including synthetic documents describing either misaligned or aligned AI behavior. By upsampling these different document types, they create controlled conditions to test how varying levels of (mis)alignment discourse affect learned behavior. They then evaluate the resulting models on alignment benchmarks both before and after standard post-training.", "result": "Upsampling synthetic documents that discuss AI misalignment causes a measurable increase in misaligned behavior in the resulting models. In contrast, upsampling documents that depict aligned AI behavior substantially reduces misalignment scores, from 45% down to 9%. These causal effects of pretraining data composition on alignment remain present, though weaker, even after post-training procedures are applied.", "conclusion": "The paper concludes that descriptions of AI behavior in pretraining data meaningfully shape models\u2019 alignment priors, providing evidence for \u201cself-fulfilling alignment\u201d where portrayals of misaligned AI induce more misaligned behavior, and vice versa. This establishes alignment-focused pretraining as an important complement to post-training alignment, and the authors recommend that practitioners explicitly pretrain models for alignment\u2014not only for capabilities\u2014while releasing their models and datasets to support further research."}}
{"id": "2601.10416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10416", "abs": "https://arxiv.org/abs/2601.10416", "authors": ["Tiesunlong Shen", "Rui Mao", "Jin Wang", "Heming Sun", "Jian Zhang", "Xuejie Zhang", "Erik Cambria"], "title": "LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models", "comment": "Accepted by AAAI26", "summary": "Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.", "AI": {"tldr": "The paper proposes LLMdoctor, a test-time alignment framework that uses a small \"doctor\" model to guide a frozen large \"patient\" LLM with token-level preference optimization, achieving better alignment and diversity than prior test-time and even full fine-tuning methods like DPO.", "motivation": "Aligning LLMs with human preferences is important, but standard fine-tuning (e.g., RLHF, DPO) is expensive, inflexible, and can reduce generative diversity. Existing test-time alignment methods attempt to avoid full retraining but typically rely on trajectory-level rewards and inefficient sampling, which both limit performance and harm diversity. The authors are motivated to design a more efficient, flexible test-time method that uses finer-grained signals to steer a frozen base model without sacrificing its capabilities.", "method": "They introduce LLMdoctor, which uses a patient-doctor paradigm: a large, frozen patient LLM is guided by a smaller, specialized doctor model. The approach has two key pieces: (1) token-level reward acquisition, which extracts fine-grained preference signals from variations in the patient model's behavior, and (2) token-level flow-guided preference optimization (TFPO), which trains the doctor so that its token-by-token guidance is consistent across all subtrajectories of a generation. This creates a flow-consistent alignment mechanism that works at the token level rather than only at the entire trajectory level. At inference, the doctor steers the patient's token generation using these learned token-level preferences.", "result": "Experiments show that LLMdoctor substantially improves alignment quality compared to prior test-time alignment approaches, and in some cases even outperforms full fine-tuning baselines such as DPO. It achieves this while maintaining the diversity of the base model's generations, overcoming a common limitation of standard alignment approaches.", "conclusion": "The paper concludes that token-level, flow-consistent preference optimization with a patient-doctor architecture is an effective and efficient strategy for aligning LLMs at test time. By leveraging fine-grained preference signals and a small steering model, LLMdoctor can rival or surpass full fine-tuning methods while preserving generative diversity, suggesting a promising direction for practical, scalable LLM alignment."}}
{"id": "2601.10161", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.10161", "abs": "https://arxiv.org/abs/2601.10161", "authors": ["Prachuryya Kaushik", "Ashish Anand"], "title": "AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers", "comment": "Submitted to ACL'26 System Demonstration", "summary": "We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).", "AI": {"tldr": "AWED-FiNER is an open-source ecosystem that delivers fine-grained named entity recognition for 36 largely under-served world languages via agentic routing tools, web apps, and compact expert models.", "motivation": "Existing large language models perform poorly on fine-grained NER, especially for low-resource and vulnerable languages. There is a lack of practical, accessible, and efficient FgNER tools and models that cover a wide set of global languages, can be used by both technical and non-technical users, and can run in resource-constrained offline environments.", "method": "The authors build an ecosystem comprising: (1) agentic toolkits that automatically route multilingual input text to appropriate language-specific expert models and return FgNER annotations; (2) web applications that expose these capabilities through a user-friendly interface for non-technical users; and (3) a suite of 49 extremely small, state-of-the-art, language-specific expert models for FgNER, optimized for low-resource and edge-device deployment. They target 36 global languages, with extra emphasis on vulnerable ones like Bodo, Manipuri, Bishnupriya, and Mizo.", "result": "They deliver a working, open-source system that covers FgNER for 36 languages spoken by more than 6.6 billion people. The ecosystem includes deployable agentic tools, a public web interface, and 49 compact expert detector models that reportedly achieve state-of-the-art performance for their language-specific FgNER tasks while being lightweight enough for offline and edge scenarios.", "conclusion": "AWED-FiNER fills a key gap in multilingual NLP by providing practical, fine-grained NER solutions for many high-population and vulnerable languages. Its agentic routing, web-based interface, and compact expert models collectively enable both broad accessibility and efficient deployment, particularly for low-resource and resource-constrained settings. All components are released as open source with public access links for tools, web apps, and models."}}
{"id": "2601.10457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10457", "abs": "https://arxiv.org/abs/2601.10457", "authors": ["Ziming Dai", "Dabiao Ma", "Jinle Tong", "Mengyuan Han", "Jian Yang", "Haojun Fei"], "title": "NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models", "comment": null, "summary": "Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being \"non-intrusive\". It treats the legacy model as a frozen model and performs targeted repairs on \"hard regions\" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.", "AI": {"tldr": "NSR-Boost is a non-intrusive neuro-symbolic residual boosting framework that repairs legacy GBDT models by adding interpretable expert modules on hard-to-predict regions, improving performance and safety with low deployment cost in industrial tabular applications.", "motivation": "In many industrial systems, especially those using GBDTs for tabular data, legacy models are deeply integrated into high-concurrency production environments. Retraining or replacing these models is costly and risky due to system dependencies, strict latency/throughput constraints, and uncertainty about generalization on live traffic. There is a need for a way to improve performance\u2014particularly on long-tail and hard cases\u2014without fully retraining or rewriting existing models, and while keeping changes safe, interpretable, and cheap to deploy.", "method": "NSR-Boost freezes the existing GBDT model and adds a non-intrusive correction layer in three stages. (1) It identifies hard regions in the input space by analyzing residuals, i.e., where the legacy model\u2019s prediction errors are large. (2) For these regions, it builds interpretable expert models using a neuro-symbolic approach: symbolic code structures are proposed via an LLM, and their numeric parameters are optimized via Bayesian optimization. These experts target the specific failure modes of the legacy model. (3) A lightweight aggregator dynamically combines the outputs of the legacy model and experts at inference time, determining when and how much to trust each expert versus the base model.", "result": "NSR-Boost is deployed in the core financial risk control system of Qfin Holdings. Experiments on six public datasets plus one private dataset show that it significantly outperforms strong SOTA baselines. In online production, it yields notable performance improvements on real-world traffic, particularly by detecting long-tail risks that the original GBDT models miss, while keeping latency and system disruption low.", "conclusion": "NSR-Boost provides a practical, low-risk, and low-cost way to evolve industrial GBDT systems. By treating existing models as frozen and adding interpretable, LLM-designed symbolic experts only on hard regions, it improves accuracy\u2014especially on long-tail risk cases\u2014without full retraining or intrusive system changes. This defines a safe evolutionary paradigm for upgrading legacy tabular ML systems in industry."}}
{"id": "2601.10167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10167", "abs": "https://arxiv.org/abs/2601.10167", "authors": ["Nhung Nguyen Thi Hong", "Cuong Nguyen Dang", "Tri Le Ngoc"], "title": "Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection", "comment": "8 pages, 0 figures, 3 tables. Preprint", "summary": "Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.", "AI": {"tldr": "The paper presents Credit C-GPT, a 7B-parameter Vietnamese domain-specialized large language model for debt-collection contact center conversations, showing it outperforms traditional pipeline NLP systems on multiple conversational intelligence tasks.", "motivation": "Debt-collection in the BFSI sector depends on large volumes of human conversations in Vietnamese contact centers, which contain informal speech, emotional variation, and complex domain reasoning. Traditional NLP pipelines struggle with these characteristics and with jointly handling multiple tasks needed for real-time assistance and analytics, motivating a unified, domain-specialized conversational LLM.", "method": "The authors build Credit C-GPT, a 7B-parameter LLM fine-tuned specifically for Vietnamese debt-collection conversational scenarios. They construct and annotate proprietary datasets and jointly train the model on several tasks\u2014dialogue understanding, sentiment analysis, intent detection, call-stage classification, and slot-value extraction\u2014within a single reasoning-based framework, then evaluate it against pipeline-based baselines.", "result": "On proprietary human-annotated datasets, Credit C-GPT achieves consistent performance gains over traditional pipeline systems across the included tasks, demonstrating better conversational understanding and structured information extraction in Vietnamese debt-collection calls.", "conclusion": "A unified, domain-specialized LLM like Credit C-GPT can more effectively handle complex, informal, emotionally nuanced Vietnamese debt-collection conversations than traditional pipelines, offering a scalable and privacy-aware solution for both real-time agent assistance and post-call analytics in enterprise contact centers."}}
{"id": "2601.10462", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.10462", "abs": "https://arxiv.org/abs/2601.10462", "authors": ["Ahmad Mustapha", "Charbel Toumieh", "Mariette Awad"], "title": "ChartComplete: A Taxonomy-based Inclusive Chart Dataset", "comment": "7 pages, 4 figures, 3 tables, 1 algorithm. Dataset and source code available at https://github.com/AI-DSCHubAUB/ChartComplete-Dataset", "summary": "With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.", "AI": {"tldr": "They introduce ChartComplete, a broad-coverage chart-image dataset spanning 30 chart types for evaluating multimodal large language models.", "motivation": "Existing chart-understanding benchmarks for multimodal large language models only cover a small set of chart types, limiting the ability to comprehensively evaluate and advance chart understanding methods.", "method": "They define a chart taxonomy adopted from the visualization community, then collect and classify chart images into 30 chart types, producing a labeled dataset without additional learning signals such as QA pairs or annotations.", "result": "The outcome is ChartComplete, a curated, taxonomy-based dataset of classified chart images covering thirty distinct chart types, intended as a benchmark resource for the community.", "conclusion": "ChartComplete fills a coverage gap in current chart-understanding benchmarks by offering a taxonomy-driven, wide-ranging collection of chart images that the community can use and extend to better evaluate and train multimodal models for chart understanding."}}
{"id": "2601.10187", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10187", "abs": "https://arxiv.org/abs/2601.10187", "authors": ["Ziang Cui", "Mengran Yu", "Tianjiao Li", "Chenyu Shi", "Yingxuan Shi", "Lusheng Zhang", "Hongwei Lin"], "title": "HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively \"tames\" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.", "AI": {"tldr": "The paper tackles the problem that current LLM-based machine translation is too verbose for strict time-limited use cases (like subtitles/dubbing), and proposes a benchmark (Sand-Glass) plus a reinforcement-learning framework (HOMURA) that jointly optimizes meaning preservation and syllable-level length control, achieving much better temporal compliance than standard LLM approaches.", "motivation": "Although LLMs are strong at multilingual translation, their outputs tend to be longer than the source, especially across languages with different lexical and syntactic density. This verbosity makes them unsuitable for applications with tight duration constraints, such as subtitling and dubbing, where the translated text must fit within specific time or syllable budgets. Prompt engineering alone cannot reliably balance semantic fidelity with hard length limits. The authors are motivated to both (1) measure this problem rigorously and (2) train models that can respect strict timing while preserving meaning.", "method": "The authors first build Sand-Glass, a benchmark for evaluating translation quality under syllable-level duration constraints, explicitly encoding time/length budgets for outputs. They then introduce HOMURA, a reinforcement learning framework that fine-tunes LLMs with a KL-regularized policy objective. The key innovation is a dynamic syllable-ratio reward that rewards outputs that meet or get close to a target syllable budget while still capturing the source meaning. The KL term keeps the fine-tuned model close to a base model to avoid degeneration, while the reward guides the model toward precise, language-aware length control.", "result": "Experiments show that models trained with HOMURA outperform strong LLM baselines on the Sand-Glass benchmark. They achieve more precise control of output length at the syllable level while maintaining semantic adequacy. The method respects cross-lingual linguistic density hierarchies, meaning it adapts to natural differences in how compact or verbose different languages are, rather than forcing a naive 1:1 token or syllable mapping. Quantitatively, this leads to significantly better temporal compliance with minimal loss in translation quality.", "conclusion": "The paper concludes that cross-lingual verbosity bias is a critical barrier for deploying LLM-based translation in time-constrained media settings, and that simple prompting is insufficient to solve it. By introducing a dedicated benchmark (Sand-Glass) and a tailored RL fine-tuning approach (HOMURA with dynamic syllable-ratio rewards and KL regularization), the authors demonstrate that LLMs can be effectively \"tamed\" to meet strict temporal constraints without sacrificing semantic fidelity. This framework offers a practical path toward production-ready LLM translation for subtitling, dubbing, and other time-sensitive applications."}}
{"id": "2601.10485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10485", "abs": "https://arxiv.org/abs/2601.10485", "authors": ["Runhao Zhao", "Weixin Zeng", "Wentao Zhang", "Chong Chen", "Zhengpin Li", "Xiang Zhao", "Lei Chen"], "title": "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge", "comment": "13 pages, 4 figures", "summary": "Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.", "AI": {"tldr": "They propose a new task and method (ExeFuse) to enrich domain-specific knowledge graphs by fusing in relevant facts from general knowledge graphs, handling both domain relevance and granularity mismatches, and provide benchmarks and experiments to validate it.", "motivation": "Domain-specific knowledge graphs are usually incomplete compared to broad, general knowledge graphs. Simply importing facts from a general KG is hard because (1) it is ambiguous which facts are truly relevant to the domain and (2) the level of detail or granularity of knowledge differs between graphs. There is a need for a principled way to enrich domain KGs from general KGs while tackling these two challenges simultaneously, along with standardized benchmarks to evaluate such methods.", "method": "They define a new task called Domain-specific Knowledge Graph Fusion (DKGF) and propose ExeFuse, based on a Fact-as-Program paradigm. Each fact in the general KG is regarded as a latent semantic program. Abstract relations are mapped to operators that are aware of granularity differences between the general and domain KGs. Domain relevance is then checked by whether these programs can be successfully executed on the target domain KG. A unified probabilistic framework is used to jointly model and resolve both domain relevance and granularity misalignment.", "result": "They build two benchmarks, DKGF(W-I) and DKGF(Y-I), covering 21 evaluation configurations, to systematically test DKG fusion methods. Experiments on these benchmarks show that the DKGF task is meaningful and that ExeFuse performs effectively at enriching domain-specific KGs from general KGs under various settings.", "conclusion": "Domain-specific Knowledge Graph Fusion is an important and previously under-standardized problem. By modeling general KG facts as executable semantic programs and verifying them against domain KGs, ExeFuse effectively addresses ambiguity in domain relevance and granularity mismatch. The proposed benchmarks and empirical results establish a first standardized testbed and demonstrate the practicality and advantage of the approach for enriching domain-specific knowledge graphs."}}
{"id": "2601.10198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10198", "abs": "https://arxiv.org/abs/2601.10198", "authors": ["Xintao Wang", "Jian Yang", "Weiyuan Li", "Rui Xie", "Jen-tse Huang", "Jun Gao", "Shuai Huang", "Yueping Kang", "Liyuan Gou", "Hongwei Feng", "Yanghua Xiao"], "title": "HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning and generation, serving as the foundation for advanced persona simulation and Role-Playing Language Agents (RPLAs). However, achieving authentic alignment with human cognitive and behavioral patterns remains a critical challenge for these agents. We present HUMANLLM, a framework treating psychological patterns as interacting causal forces. We construct 244 patterns from ~12,000 academic papers and synthesize 11,359 scenarios where 2-5 patterns reinforce, conflict, or modulate each other, with multi-turn conversations expressing inner thoughts, actions, and dialogue. Our dual-level checklists evaluate both individual pattern fidelity and emergent multi-pattern dynamics, achieving strong human alignment (r=0.91) while revealing that holistic metrics conflate simulation accuracy with social desirability. HUMANLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4x fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling--simulating not just what humans do, but the psychological processes generating those behaviors.", "AI": {"tldr": "The paper introduces HUMANLLM, a framework and dataset for modeling humans\u2019 psychological patterns as interacting causal forces in large language models, enabling more realistic and cognitively grounded role\u2011playing agents.", "motivation": "Existing role-playing and persona-based LLM agents often look human-like on the surface, but their behavior is not truly grounded in established psychological theory. Current evaluations also tend to reward socially desirable responses rather than accurate simulation of how humans actually think and behave, especially when multiple cognitive and behavioral tendencies interact. The authors aim to close this gap by explicitly modeling psychological patterns and their interactions, and by building evaluation protocols that distinguish realistic cognition from mere niceness or desirability.", "method": "They build a framework, HUMANLLM, that formalizes psychological patterns (derived from around 12,000 academic psychology papers) as causal forces that can reinforce, conflict, or modulate each other. From this, they curate 244 distinct psychological patterns and synthesize 11,359 multi-turn scenarios in which 2\u20135 patterns interact. Each scenario includes conversations that capture inner thoughts, actions, and dialogues. They also design dual-level checklists: one set to evaluate fidelity to each individual pattern, and another to assess emergent dynamics when multiple patterns interact. Using these tools, they train and evaluate HUMANLLM models, including an 8B-parameter model, and compare them with existing LLMs like Qwen3-32B.", "result": "The dual-level checklist evaluations show high human alignment, with a correlation of r=0.91 between model-based and human judgments. They find that holistic or single-score metrics tend to mix up two things: how accurately the model simulates human cognition and how socially desirable its answers are. Despite having four times fewer parameters than Qwen3-32B, HUMANLLM-8B performs better on tasks that require modeling complex multi-pattern psychological dynamics.", "conclusion": "Accurate anthropomorphic behavior in LLM agents requires explicit cognitive modeling rather than just surface-level imitation or generic helpfulness. By treating psychological patterns as interacting causal forces and evaluating both single-pattern fidelity and multi-pattern dynamics, HUMANLLM produces more authentically human-like role-playing agents. The work suggests that future persona and agent systems should incorporate psychologically grounded structure to simulate not just outcomes, but the underlying mental processes that generate those outcomes."}}
{"id": "2601.10520", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.10520", "abs": "https://arxiv.org/abs/2601.10520", "authors": ["Felix Jahn", "Yannic Muskalla", "Lisa Dargasz", "Patrick Schramowski", "Kevin Baum"], "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment", "comment": "10 pages, 4 figures, accepted at 2nd Annual Conference of the International Association for Safe & Ethical AI (IASEAI'26)", "summary": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.", "AI": {"tldr": "Introduces GRACE, a neuro-symbolic architecture that constrains arbitrary AI agents with an external, reason-based moral governor.", "motivation": "As AI agents gain autonomy and real-world impact, it is crucial not only that they act effectively but that their actions are aligned with explicit normative standards. Existing approaches often entangle goal-directed decision-making with ethical constraints, making behavior opaque, hard to contest, and difficult to verify or retrofit onto arbitrary agent designs.", "method": "Propose GRACE, a modular containment framework with three components: (1) a Moral Module (MM) that uses a reason-based formalism grounded in deontic logic to determine which high-level (macro) actions are morally permissible; (2) a Decision-Making Module (DMM) that wraps or encapsulates an arbitrary target agent and chooses primitive actions that are instrumentally optimal, but only within the set of permitted macro actions; and (3) a Guard that monitors the DMM and enforces compliance with MM\u2019s constraints. The symbolic moral reasoning is explicitly represented, interpretable, and used to enrich the DMM\u2019s context while enabling formal verification and probabilistic guarantees of alignment.", "result": "Implement and demonstrate GRACE on a use case of a large language model (LLM) therapy assistant. Show that the architecture lets stakeholders inspect the system\u2019s moral reasoning, understand which actions are permitted or restricted, contest and revise normative rules, and influence the assistant\u2019s behavior while keeping its core decision-making intact.", "conclusion": "A separate, neuro-symbolic, reason-based governor like GRACE can be layered onto virtually any AI agent to enforce normative constraints without redesigning the agent\u2019s internals. By decoupling moral reasoning from instrumental decision-making and representing norms in an interpretable deontic logic framework, GRACE supports transparency, contestability, and formal guarantees of alignment, illustrating a promising path for containing powerful autonomous systems."}}
{"id": "2601.10205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10205", "abs": "https://arxiv.org/abs/2601.10205", "authors": ["Arya Shah", "Himanshu beniwal", "Mayank Singh"], "title": "One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?", "comment": "12 pages, 4 figures, 10 tables", "summary": "Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4\\% on monolingual retrieval and 20.7\\% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1\\% Recall@1. For classification, LaBSE attains 75.3\\% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.", "AI": {"tldr": "The paper introduces a benchmark (PI-Indic-Align) to test whether multilingual embedding models can capture compatibility between user personas and instructions in 12 Indian languages, providing retrieval and classification baselines.", "motivation": "India has over a billion speakers using many languages and scripts, and aligning multilingual assistants with culturally grounded user preferences is critical. Existing benchmarks are limited: they usually cover only one language or mix up retrieval and generation, making it unclear whether embeddings alone can encode how well an instruction matches a persona, independent of response generation quality. The paper aims to fill this gap for Indic languages.", "method": "The authors construct a unified benchmark across 12 Indian languages with four tasks: (1) monolingual persona-to-instruction retrieval, (2) cross-lingual persona-to-instruction retrieval, (3) instruction-to-persona reverse retrieval, and (4) binary compatibility classification of persona\u2013instruction pairs. They evaluate eight multilingual embedding models in a frozen-encoder setup, training only a light logistic regression head for the classification task. Performance is measured using Recall@1 for retrieval and AUROC plus calibration for classification.", "result": "E5-Large-Instruct performs best on persona-to-instruction retrieval, achieving 27.4% Recall@1 in the monolingual setting and 20.7% in cross-lingual transfer. BGE-M3 obtains the highest Recall@1 (32.1%) for reverse instruction-to-persona retrieval. For binary compatibility classification, LaBSE attains the best AUROC of 75.3% and is also well calibrated. These scores provide realistic, reproducible performance baselines for the benchmark tasks.", "conclusion": "Current multilingual embedding models can encode persona\u2013instruction compatibility to a useful but imperfect degree across 12 Indic languages. Different models are preferable for different tasks: E5-Large-Instruct for persona-to-instruction retrieval, BGE-M3 for reverse retrieval, and LaBSE for calibrated binary compatibility classification. The benchmark and released resources offer a standardized way to compare models and guide practitioners in choosing appropriate embeddings for Indic multilingual alignment tasks."}}
{"id": "2601.10524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10524", "abs": "https://arxiv.org/abs/2601.10524", "authors": ["Frank Bobe", "Gregory D. Vetaw", "Chase Pavlick", "Darshan Bryner", "Matthew Cook", "Jose Salas-Vernis"], "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection", "comment": "16 pages, 6 figures, 6 tables", "summary": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.", "AI": {"tldr": "The paper analyzes why fine-tuned LLMs fail to generalize on a phishing detection task by combining SHAP and mechanistic interpretability across multiple architectures, showing that generalization depends jointly on model architecture and training data diversity.", "motivation": "Fine-tuned LLMs achieve strong task-specific performance but often become brittle and fail to generalize outside their training distribution. There is a lack of systematic, mechanistic understanding of why such failures occur, especially across different model architectures and training setups in high-stakes domains like phishing detection.", "method": "The authors fine-tune three different LLM architectures\u2014Llama 3.1 8B, Gemma 2 9B, and Mistral\u2014on a high-risk phishing detection task using datasets with varying stylistic diversity. They then apply a multi-layer diagnostic framework combining SHAP value analysis (to identify influential input features and heuristics) with mechanistic interpretability tools (to probe internal model representations and circuits) to identify the causes of generalization successes and failures for each model and training paradigm.", "result": "(1) Gemma 2 9B achieves >91% F1 and strong generalization only when trained on a stylistically diverse, generalist dataset, showing a synergy between architecture and data diversity. (2) Llama 3.1 8B exhibits a specific failure mode: it performs well on narrow-domain data but struggles to integrate diverse examples, leading to sharp performance degradation on broader distributions. (3) The Mistral architecture shows robust and consistent performance across multiple training paradigms, suggesting it is inherently better at generalizing in this task setting. The SHAP and mechanistic analyses identify flawed heuristics and shortcut patterns that underlie the observed brittleness.", "conclusion": "Generalization in fine-tuned LLMs is not solely a function of data or training regime but emerges from a three-way interaction among architecture, data diversity, and training strategy. Some architectures, like Mistral, appear more predisposed to robust generalization, while others, like Llama 3.1 8B, can overfit to narrow domains and fail when exposed to diverse data. The proposed diagnostic framework\u2014combining SHAP and mechanistic interpretability\u2014offers a concrete, systematic way to uncover the flawed heuristics driving generalization failures, emphasizing that building reliable AI systems requires deep validation and analysis beyond surface-level metrics."}}
{"id": "2601.10527", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10527", "abs": "https://arxiv.org/abs/2601.10527", "authors": ["Xingjun Ma", "Yixu Wang", "Hengyuan Xu", "Yutao Wu", "Yifan Ding", "Yunhan Zhao", "Zilong Wang", "Jiabin Hua", "Ming Wen", "Jianan Liu", "Ranjie Duan", "Yifeng Gao", "Yingshui Tan", "Yunhao Chen", "Hui Xue", "Xin Wang", "Wei Cheng", "Jingjing Chen", "Zuxuan Wu", "Bo Li", "Yu-Gang Jiang"], "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5", "comment": "42 pages, 24 figures", "summary": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.", "AI": {"tldr": "The paper performs a unified, multimodal safety evaluation of 7 frontier LLMs/MLLMs and finds safety performance is heterogeneous, strongly multidimensional, and fragile under adversarial conditions, even when benchmark scores look strong.", "motivation": "Existing safety evaluations are fragmented: they usually focus on a single modality (text-only or vision-only), a narrow threat model, or a single language, making it hard to understand how \"safe\" frontier LLMs/MLLMs really are in realistic, mixed-modality, multilingual settings. As frontier models rapidly improve in capabilities, there is an urgent need to know whether safety keeps pace, and to have standardized, comparable assessments that can guide responsible deployment and governance.", "method": "The authors evaluate 7 advanced models (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) under a unified safety protocol. This protocol spans three settings\u2014pure language, vision-language, and text-to-image generation\u2014and combines multiple evaluation types: standard benchmarks, adversarial (red-teaming style) tests, multilingual assessments, and regulatory/compliance checks. They aggregate outcomes into safety leaderboards and model-specific safety profiles, enabling cross-model and cross-modality comparisons.", "result": "GPT-5.2 shows the most consistently strong and balanced safety performance across all tested dimensions. Other models reveal clear trade-offs\u2014performing well on some metrics (e.g., benchmarks) while being weaker on others (e.g., adversarial robustness, multilingual safety, or compliance). Both language and vision-language interfaces degrade markedly under adversarial probing, despite promising benchmark scores. Text-to-image systems appear comparatively better aligned on regulated visual categories but still fail under adversarial or semantically ambiguous prompts.", "conclusion": "Safety in frontier models cannot be captured by a single score or benchmark: it is inherently multidimensional, depending on modality (text vs. multimodal), language, and the type of evaluation (benchmark vs. adversarial vs. compliance, etc.). Existing strong performance on standard safety benchmarks does not guarantee robustness under adversarial or realistic conditions. The authors argue that standardized, multimodal, and multidimensional safety evaluations are necessary to accurately estimate real-world risk and to guide the design, deployment, and governance of future frontier models."}}
{"id": "2601.10242", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10242", "abs": "https://arxiv.org/abs/2601.10242", "authors": ["Guanxu Chen", "Dongrui Liu", "Jing Shao"], "title": "Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?", "comment": "9 pages,6 figures", "summary": "Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.", "AI": {"tldr": "The paper studies whether looping transformer layers (reusing them multiple times) lets models introspect better, finding that more loops reduce the gap between internal representations and text output, but partly by degrading the internal knowledge rather than truly improving introspection.", "motivation": "LLMs know more in their internal representations than they can reliably express in language. The authors want to know if looped transformers, which apply the same layers repeatedly to increase computational depth, can help models better access and verbalize their internal knowledge via an introspective process.", "method": "They use looped transformer architectures where the same transformer block is iterated multiple times. They empirically measure the gap between internal knowledge (probed from representations) and explicit linguistic output across different numbers of loop iterations, and analyze how well later loops can read or utilize earlier-loop representations.", "result": "More loop iterations do shrink the mismatch between what the model appears to \u201cknow\u201d internally and what it can say, but this is partly because the internal representations themselves become less informative as loops increase. Additionally, the model\u2019s ability to interpret or \u2018perceive\u2019 intermediate representations does not gradually improve across loops; it only appears in the final loop.", "conclusion": "Looped transformers are a promising way to scale computational depth, but in their current form they do not genuinely deliver the kind of introspection needed to tightly connect internal representation space with natural language outputs. Their improved alignment between knowledge and output is partly an artifact of representation degradation rather than true introspective access."}}
{"id": "2601.10543", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10543", "abs": "https://arxiv.org/abs/2601.10543", "authors": ["Yinzhi Zhao", "Ming Wang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.", "AI": {"tldr": "The paper proposes a decoding-time method that probes and leverages latent safety-related signals inside LLMs to detect and stop unsafe generations early, improving robustness to jailbreak attacks with minimal impact on benign use and quality.", "motivation": "Although LLMs are safety-aligned, many defenses against jailbreak attacks are either too weak or harm model utility. Existing approaches like decoding constraints or external detectors fail against sophisticated prompts or cause excessive refusals. The authors are motivated by the need for a more effective, low-overhead, and less intrusive defense that can robustly detect unsafe content during generation without heavily degrading normal performance.", "method": "The authors inspect the internal behavior of LLMs during decoding and find that, even when the model is successfully jailbroken, it still carries internal latent signals correlated with safety-relevant information. However, these signals do not surface in the final output due to the model\u2019s preference for fluent continuation. They then design a decoding-time probing mechanism that explicitly extracts and uses these latent safety signals token by token (or step by step) to identify emerging unsafe content early. When unsafe signals cross a threshold, the method can intervene\u2014e.g., by halting or redirecting the generation\u2014without relying solely on external filters or rigid decoding constraints.", "result": "Across a variety of jailbreak scenarios and attacks, the proposed method improves the model\u2019s robustness to unsafe prompts and generations. It detects unsafe content earlier and more reliably than prior decoding-based or detector-based defenses, while keeping over-refusal on harmless prompts low and maintaining overall response quality. The experiments indicate that the approach works consistently across different jailbreak types and threat models.", "conclusion": "The paper concludes that LLMs contain intrinsic safety-awareness that remains latent during standard decoding, and that making this awareness explicit and actionable during generation is an effective way to defend against jailbreak attacks. Activating and leveraging these internal safety signals complements existing defenses, offering a practical path to stronger safety with limited negative impact on usability. The released code enables further study and adoption of this decoding-time safety probing framework."}}
{"id": "2601.10246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10246", "abs": "https://arxiv.org/abs/2601.10246", "authors": ["Prottay Kumar Adhikary", "Reena Rawat", "Tanmoy Chakraborty"], "title": "coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts", "comment": null, "summary": "Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.", "AI": {"tldr": "The paper presents coTherapist, a small language-model-based framework that emulates key therapeutic skills and shows expert-like, safe behavior on mental health tasks.", "motivation": "Mental healthcare systems face workforce shortages and increasing demand, limiting access to care. There is a need for scalable, intelligent tools that can assist clinicians while maintaining high standards of empathy, safety, and clinical validity. The authors aim to see whether relatively small language models, rather than only very large ones, can be engineered to reliably support core therapeutic functions.", "method": "The authors design coTherapist, a unified framework built on a small language model that is adapted for mental health through domain-specific fine-tuning, retrieval-augmented generation (using external clinical knowledge), and agentic reasoning mechanisms. They evaluate the model on clinical queries against contemporary baselines, and introduce a new evaluation rubric called T-BARS to measure therapeutic competencies such as empathy. They also use psychometric profiling to assess therapist-like personality traits, and conduct human expert evaluations of accuracy, trustworthiness, and safety. The system is deployed and tested by clinical experts in a realistic setting.", "result": "coTherapist outperforms baseline models on clinical query tasks, producing more relevant and clinically grounded responses. According to the T-BARS rubric and psychometric analyses, it shows high empathy and personality patterns consistent with human therapists. Expert human raters judge its outputs to be accurate, trustworthy, and safe, and its deployment with clinical experts is reported as successful, demonstrating practical viability.", "conclusion": "The study shows that small language models, when properly engineered with domain fine-tuning, retrieval augmentation, and structured reasoning, can emulate core therapeutic competencies and behave in an expert-like manner. This points to a scalable and potentially cost-effective path for developing digital mental health support tools that can augment clinical workflows while maintaining high standards of empathy and safety."}}
{"id": "2601.10567", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.10567", "abs": "https://arxiv.org/abs/2601.10567", "authors": ["Laura Ferrarotti", "Gian Maria Campedelli", "Roberto Dess\u00ec", "Andrea Baronchelli", "Giovanni Iacca", "Kathleen M. Carley", "Alex Pentland", "Joel Z. Leibo", "James Evans", "Bruno Lepri"], "title": "Generative AI collective behavior needs an interactionist paradigm", "comment": null, "summary": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.", "AI": {"tldr": "The paper argues that studying how LLM-based agents behave collectively is crucial, and calls for a new interactionist paradigm and four key research directions to understand and govern emergent phenomena in multi-agent LLM systems.", "motivation": "As LLMs become embedded in multi-agent systems that can affect society at many levels, it is vital to understand their collective behavior, associated risks, and potential benefits. Existing paradigms may be insufficient because LLMs start with rich pre-trained knowledge, social priors, and strong in-context learning capabilities, which together can generate complex emergent behaviors in groups.", "method": "Conceptual and theoretical analysis: the authors argue for an interactionist paradigm that integrates alternative theoretical foundations, methodologies, and analytical tools. They outline and discuss four key directions for research and practice related to theory-building, methodological development, and fostering trans-disciplinary dialogue around LLM-based collectives.", "result": "The paper formulates a conceptual framework highlighting why LLM collectives need to be studied as socio-technical systems shaped by prior knowledge, embedded values, and social context. It identifies and articulates four crucial directions to guide the systematic study and development of LLM-based multi-agent systems.", "conclusion": "Understanding emergent phenomena in LLM-based collectives requires moving beyond current individual-model perspectives to an interactionist paradigm. By advancing theory, methods, and cross-disciplinary collaboration around LLM collectives, we can better anticipate, govern, and harness their societal impacts."}}
{"id": "2601.10257", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10257", "abs": "https://arxiv.org/abs/2601.10257", "authors": ["Nan Li", "Bo Kang", "Tijl De Bie"], "title": "Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs", "comment": null, "summary": "When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \\emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.", "AI": {"tldr": "The paper studies whether large language models (LLMs) give different moral judgments across languages and disentangles the effects of input language and reasoning language using a crossed-language evaluation framework grounded in Moral Foundations Theory.", "motivation": "Existing evaluations of LLM moral judgments usually only test matched conditions, such as English dilemmas with English reasoning, which conflates two factors: the language in which the dilemma is written and the language in which the LLM internally reasons. This makes it unclear why models vary across languages and how to diagnose or control such variability, which is important for cross-cultural deployment of LLMs making or explaining moral decisions.", "method": "The authors design a methodology that orthogonally manipulates (1) the language of the moral dilemma and (2) the language in which the model is instructed to reason, creating both matched and mismatched conditions (e.g., English dilemmas with Chinese reasoning and vice versa). They then interpret model judgments using Moral Foundations Theory, mapping outputs to different moral foundations and analyzing how these patterns change with language conditions. They also refine the Authority foundation into separate family-related and institutional components based on empirical patterns. The framework is applied to English\u2013Chinese moral judgment across 13 LLMs, and quantitative analyses decompose variance attributable to input vs. reasoning language, detect context dependency, and derive a diagnostic taxonomy for practical guidance.", "result": "Across 13 LLMs, reasoning-language effects explain roughly twice as much variance in moral judgments as input-language effects. The framework reveals that nearly half of the models show context-dependent behavior that standard, matched-only evaluations would fail to detect. Analysis in the Moral Foundations space supports splitting the Authority foundation into family-related and institutional sub-dimensions. The authors also produce a diagnostic taxonomy of model behaviors and release their code and datasets.", "conclusion": "LLMs\u2019 moral judgments are significantly shaped by the language in which they reason, even more so than by the language of the presented dilemma, and these influences can be disentangled using a crossed-language evaluation framework. Representing judgments via Moral Foundations Theory allows fine-grained interpretation and suggests a refined structure of the Authority dimension. The proposed methodology exposes hidden context dependencies in many models and yields a taxonomy that can guide safer and more culturally aware deployment. Publicly released resources enable further research on cross-lingual moral reasoning in LLMs."}}
{"id": "2601.10581", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.10581", "abs": "https://arxiv.org/abs/2601.10581", "authors": ["Kimia Abedini", "Farzad Shami", "Gianmaria Silvello"], "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA", "comment": "Accepted paper by the 48th European Conference on Information Retrieval (ECIR'26)", "summary": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.", "AI": {"tldr": "The paper introduces GenomAgent, a flexible multi-agent framework that improves genomic question answering with LLMs and outperforms the prior GeneGPT system on benchmark tasks.", "motivation": "Genomic information is critical for biomedical research, but accessing and querying complex, distributed, domain-specific genomic databases is difficult. Existing LLM-based genomic QA systems like GeneGPT rely on rigid, task-specific API integrations that limit adaptability, extensibility, and broader applicability across different expert databases and scientific domains.", "method": "The authors first replicate GeneGPT as a baseline. They then design GenomAgent, a multi-agent framework where multiple specialized agents coordinate to solve complex genomics queries. Each agent can handle specific subtasks or interact with different specialized resources or APIs. The system orchestrates these agents to decompose and solve complex questions, enabling more flexible and scalable integration with domain-specific tools and databases.", "result": "On nine tasks from the GeneTuring benchmark for genomic question answering, GenomAgent achieves an average performance gain of 12% over GeneGPT, establishing a new state of the art on this benchmark. The gains demonstrate more effective use of external expert resources and better handling of complex genomic queries.", "conclusion": "GenomAgent\u2019s multi-agent, modular architecture overcomes the rigidity and limited adaptability of single-API, single-agent systems like GeneGPT. It provides improved performance on genomic QA and offers a general, extensible framework that can be adapted beyond genomics to other scientific fields requiring structured access to expert databases and tools."}}
{"id": "2601.10266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10266", "abs": "https://arxiv.org/abs/2601.10266", "authors": ["Hiroaki Yamagiwa", "Yusuke Takase", "Hidetoshi Shimodaira"], "title": "Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel", "comment": null, "summary": "Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as the Composition Score. We further introduce a framework to quantify the informativeness of PK distributions by comparing them with a reference distribution derived from random orthogonal subspaces. As an application, we analyze a directed graph constructed from PK and show that, in GPT2-small, L4H7 acts as a hub by functioning as an identity head.", "AI": {"tldr": "The paper proposes Projection Kernel (PK), a subspace-similarity metric for attention-head weight matrices, and shows it better captures inter-head structure in Transformers than prior metrics, revealing hub-like identity heads in GPT-2 small.", "motivation": "Existing metrics for analyzing relationships between attention heads (e.g., in mechanistic interpretability) do not adequately capture the geometric structure of the subspaces spanned by attention-head weight matrices, limiting our ability to understand how heads interact and specialize. A more faithful, geometry-aware measure is needed to characterize head-to-head relationships and uncover functional structures like hubs or identity heads in Transformer models.", "method": "The authors treat each attention head\u2019s weight matrix as defining a subspace and compare heads using Projection Kernel (PK), a similarity measure based on principal angles between subspaces. They empirically evaluate PK on the IOI (Indirect Object Identification) task, comparing it against existing metrics such as the Composition Score. They then develop a statistical framework that assesses how informative PK similarity distributions are by contrasting them with a reference distribution derived from random orthogonal subspaces. Finally, they construct a directed graph where edges are defined using PK scores to study structural patterns among heads in GPT2-small.", "result": "PK more clearly recovers previously known patterns of interaction between attention heads on the IOI task than prior metrics, suggesting it is better aligned with true functional relationships. The comparison to random-subspace reference distributions supports that observed PK patterns are statistically informative rather than artifacts. In the directed PK-based interaction graph for GPT2-small, they identify layer 4, head 7 (L4H7) as a central hub, functionally behaving like an identity head that passes information forward across the network.", "conclusion": "Projection Kernel is an effective, geometry-aware metric for analyzing relationships between attention heads via their weight-matrix subspaces, outperforming existing measures like Composition Score in capturing known mechanistic structures. By situating PK values against a random-subspace baseline and using them to build interaction graphs, researchers can better reveal functional roles and hub-like heads\u2014exemplified by L4H7 in GPT2-small acting as an identity head\u2014thereby advancing interpretability of Transformer internals."}}
{"id": "2601.10272", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.10272", "abs": "https://arxiv.org/abs/2601.10272", "authors": ["Yuxuan Lou", "Kai Yang", "Yang You"], "title": "MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts", "comment": null, "summary": "We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \\footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST", "AI": {"tldr": "MoST is a multimodal large language model that jointly handles speech and text using a modality-aware mixture-of-experts architecture, achieving strong, data-efficient performance across ASR, TTS, audio language modeling, and spoken QA, and is fully open-source.", "motivation": "Existing multimodal models typically use the same parameters to process very different modality representations (e.g., speech vs. text), which ignores their differing structures and limits both modality-specific performance and cross-modal understanding. There is also a lack of fully open-source, Mixture-of-Experts-based speech-text LLMs that can be trained using only publicly available data while still achieving competitive results.", "method": "The authors propose MoST, built on a Modality-Aware Mixture of Experts (MAMoE) architecture. Tokens are routed via specialized, modality-aware pathways to modality-appropriate experts: modality-specific expert groups specialize on domain-specific patterns (speech or text), while shared experts encourage cross-modal information transfer. They adapt a pretrained MoE language model through a staged pipeline: (1) post-training on ASR and TTS datasets to align with speech-text tasks and (2) fine-tuning on a curated speech-text instruction dataset, all using fully open-source data. They conduct ablations to assess the impact of modality-specific routing and shared experts.", "result": "MoST outperforms existing models of similar parameter size on benchmarks for automatic speech recognition, text-to-speech, audio language modeling, and spoken question answering. Ablation studies demonstrate that both the modality-specific routing mechanism and the inclusion of shared experts provide significant performance gains across these domains.", "conclusion": "MoST, leveraging the proposed MAMoE architecture, effectively integrates speech and text processing within a single multimodal LLM by combining modality-specific experts with shared experts and modality-aware routing. Using only open-source datasets and an efficient adaptation pipeline, it achieves strong, data-efficient results across multiple speech and language tasks. The authors position MoST as the first fully open-source speech-text LLM based on a Mixture of Experts design, providing released models, code, and data for the community."}}
{"id": "2601.10679", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10679", "abs": "https://arxiv.org/abs/2601.10679", "authors": ["Zirui Ren", "Ziming Liu"], "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models", "comment": null, "summary": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models \"reason\".", "AI": {"tldr": "The paper mechanistically analyzes a Hierarchical Reasoning Model (HRM), finds that it often behaves like a structured guesser rather than a true reasoner, identifies specific failure modes related to fixed points and grokking-like dynamics, and proposes scalable guessing strategies that dramatically boost performance on hard Sudoku.", "motivation": "Although HRM outperforms large language model-based reasoners on many reasoning tasks, its internal mechanisms and failure modes are poorly understood. The authors want to know whether HRM is truly performing step-by-step logical reasoning or relying on some other dynamics, and how these dynamics can be exploited or improved. Understanding this is important both scientifically, to clarify what \"reasoning\" means in neural models, and practically, to further scale performance on challenging combinatorial tasks like extreme Sudoku.", "method": "The authors perform a mechanistic/behavioral study of HRM on structured reasoning tasks, focusing on Sudoku. They analyze intermediate reasoning steps and the fixed-point dynamics presumed by HRM: that iterative updates should converge to a unique stable solution. They look for violations of this property and characterize phenomena such as failure on very simple puzzles, sudden accuracy jumps (grokking-like behavior), and the presence of multiple fixed points. Based on interpreting HRM as a kind of iterative guesser over fixed points, they then design three intervention strategies\u2014data augmentation, input perturbation, and model bootstrapping\u2014to systematically scale and diversify these guesses during training and inference.", "result": "They identify three key empirical findings: (a) HRM can fail even on extremely simple puzzles with only one unknown cell, which they link to breakdowns of the fixed-point assumption; (b) the correctness of answers often improves in a non-monotonic, grokking-like way, with critical steps where accuracy suddenly jumps; and (c) the model admits multiple fixed points, and HRM tends to converge to the first one it \"guesses,\" which may be wrong and cause it to get stuck. Using this new understanding, they implement scaled guessing strategies\u2014better data (augmentation), more diverse inference runs (input perturbation), and diverse model instances (bootstrapping)\u2014that collectively raise performance on Sudoku-Extreme from 54.5% to 96.9%.", "conclusion": "The paper concludes that HRM, despite its strong performance, does not consistently behave as a pure logical reasoner following a single convergent fixed-point dynamic. Instead, its behavior is better captured as structured guessing over multiple potential fixed points, with abrupt grokking-like improvements in certain steps. Recognizing and exploiting this \"guessing\" character allows the authors to significantly enhance performance via strategies that scale the quality and diversity of guesses. Scientifically, this reframes how we interpret neural reasoning models and highlights the importance of fixed-point structure, multiple attractors, and randomness (in data, inference, and training) in shaping their reasoning-like behavior."}}
{"id": "2601.10307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10307", "abs": "https://arxiv.org/abs/2601.10307", "authors": ["Luoming Hu", "Jingjie Zeng", "Liang Yang", "Hongfei Lin"], "title": "The Straight and Narrow: Do LLMs Possess an Internal Moral Path?", "comment": null, "summary": "Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.", "AI": {"tldr": "The paper introduces a method to intrinsically adjust and control the moral behavior of LLMs using Moral Foundations Theory, improving safety while maintaining helpfulness.", "motivation": "Existing alignment methods mostly work as external filters or guardrails and do not modify how LLMs internally represent morality. This leads to issues such as over-refusals of benign queries and vulnerability to jailbreaks. The authors aim to directly analyze and steer the internal moral representations of LLMs to achieve more robust and nuanced moral alignment, and to do so across languages (English and Chinese).", "method": "1) Use Moral Foundations Theory (MFT) as a structured framework of moral dimensions. 2) Apply cross-lingual linear probing on middle layers of LLMs to identify and compare moral representations in English and Chinese, revealing a shared but distinct moral subspace. 3) Extract \u201cMoral Vectors\u201d from this subspace that can steer model activations in specific moral directions. 4) Validate these vectors both internally (representation-level) and behaviorally (output-level). 5) Propose Adaptive Moral Fusion (AMF), an inference-time method that detects moral context with probes and injects appropriate Moral Vectors dynamically to balance safety and helpfulness. 6) Compare AMF against standard alignment baselines on metrics like refusal rates for benign queries and jailbreak success rates.", "result": "The authors show that LLMs possess shared moral representations across English and Chinese in their middle layers, but with language-specific variations forming a shared yet distinct moral subspace. Moral Vectors derived from this subspace can reliably steer the model\u2019s internal states and outputs along moral dimensions. The AMF method reduces incorrect refusals on benign user requests and simultaneously lowers jailbreak success rates relative to standard alignment baselines, demonstrating improved safety-helpfulness trade-offs through intrinsic steering rather than only external filters.", "conclusion": "Intrinsic moral steering via MFT-based Moral Vectors is feasible and effective across languages. By combining probe-based detection with vector injection (AMF), the method offers a targeted, dynamic defense that better preserves helpfulness on benign queries while enhancing robustness to adversarial jailbreak attempts. This suggests that future alignment work can profitably focus on understanding and manipulating internal moral subspaces rather than relying solely on outer-layer guardrails."}}
{"id": "2601.10681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10681", "abs": "https://arxiv.org/abs/2601.10681", "authors": ["Amir Khurshid", "Abhishek Sehgal"], "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems", "comment": null, "summary": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.", "AI": {"tldr": "Proposes a structure-aware, diversity-constrained \u201ccontext bubble\u201d method as an alternative to naive top-k RAG retrieval, yielding more compact, complete, and faithful contexts under a token budget.", "motivation": "Standard RAG typically retrieves top-k independent passages ranked by relevance. This often breaks document structure, over-retrieves redundant content, and fails to capture secondary and tertiary facets needed to answer complex queries. In enterprise settings with long, structured documents and tight context windows, there is a need for retrieval methods that preserve structure, reduce redundancy, and better cover all relevant aspects while remaining auditable and tunable.", "method": "Introduce a \u2018context bubble\u2019 construction framework that starts from high-relevance anchor spans in documents and then adds surrounding multi-granular spans (e.g., sections, rows) under a strict token budget. Retrieval is guided by task-conditioned structural priors that respect inherent document structure. A constrained selection process balances query relevance, marginal coverage of new information, and redundancy penalties, while explicitly enforcing diversity and budget constraints. The system outputs a full retrieval trace, exposing scores and selection decisions for auditability and deterministic tuning. Ablation experiments test the contribution of structural priors and diversity constraints.", "result": "On enterprise document benchmarks, the context bubble approach reduces redundant content compared with standard top-k retrieval, improves coverage of secondary facets, and yields better answer quality and more faithful citations within a limited LLM context window. Ablation results show that removing structural priors or diversity constraints degrades coverage and increases redundancy or incompleteness.", "conclusion": "Structure-aware, diversity-constrained context bubbles provide a more efficient and faithful way to construct LLM contexts than naive top-k RAG. Exploiting document structure and explicitly managing diversity and token budget leads to more compact, comprehensive, and auditable retrieval, and both structural priors and diversity-aware selection are essential components of the method."}}
{"id": "2601.10310", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10310", "abs": "https://arxiv.org/abs/2601.10310", "authors": ["Jan Christian Blaise Cruz", "David Ifeoluwa Adelani", "Alham Fikri Aji"], "title": "Multilinguality as Sense Adaptation", "comment": "Code available at https://github.com/jcblaisecruz02/sensia", "summary": "We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.", "AI": {"tldr": "Proposes SENSIA, a method to adapt Backpack language models across languages by aligning sense-level representations with parallel data, outperforming other multilingual methods with less target data.", "motivation": "Multilingual models usually rely on shared parameters and large scale but struggle to robustly align semantic meaning across different languages, especially with limited target-language data. The authors aim to achieve more principled cross-lingual transfer by explicitly aligning sense-level representations instead of depending only on implicit sharing.", "method": "Introduce SENse-based Symmetric Interlingual Alignment (SENSIA) for adapting a Backpack language model trained in one language (e.g., English) to another. SENSIA explicitly aligns sense mixtures and contextual representations using parallel corpora, enforcing symmetric alignment between languages, while simultaneously training a language modeling objective in the target language to maintain fluency. The approach includes analyses of sense geometry and ablation studies on various design and scale choices.", "result": "On benchmarks spanning four typologically diverse languages, SENSIA generally outperforms other multilingual alignment approaches and reaches accuracy comparable to monolingual models trained from scratch, while requiring only 25\u201350% (2\u20134x less) of the target-language data. Sense geometry analyses show that local sense neighborhoods and global structures relative to English remain largely intact after adaptation, and ablations demonstrate robustness to architectural and scaling variations.", "conclusion": "Explicit sense-level and representation-level alignment via SENSIA is an effective approach to multilingual adaptation of Backpack language models, enabling competitive performance with significantly less target-language data. The method preserves meaningful semantic geometry across languages and remains robust under different design and scale choices, suggesting sense-based alignment is a promising direction for more data-efficient and semantically grounded multilingual modeling."}}
{"id": "2601.10696", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10696", "abs": "https://arxiv.org/abs/2601.10696", "authors": ["Han Jiang", "Yao Xiao", "Rachel Hurley", "Shichao Liu"], "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load", "comment": null, "summary": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.", "AI": {"tldr": "The paper investigates how generative AI tools affect performance, creative self-efficacy, and cognitive load in architectural conceptual design, finding benefits for novices but a decline in creative self-belief among GenAI users overall.", "motivation": "To understand whether and for whom generative AI actually improves conceptual design outcomes in architecture, beyond hype, and how it impacts designers\u2019 confidence in their creativity and their mental effort during complex design tasks.", "method": "Thirty-six students from Architectural Engineering and related disciplines completed an architectural conceptual design task in two phases: first working independently, then with external assistance. Participants were assigned either to a GenAI-assisted condition or a control condition using an online repository of existing architectural projects. Design outputs were evaluated by expert raters. After each phase, students self-reported their creative self-efficacy and cognitive load. The authors applied difference-in-differences analysis and exploratory analyses of prompt usage patterns.", "result": "Overall, generative AI did not yield a significant performance advantage compared to using an online repository. However, subgroup analyses showed that novice designers achieved significantly better design performance with GenAI. Across GenAI users, general creative self-efficacy decreased. Cognitive load measures did not differ significantly between conditions, but specific prompting strategies\u2014iterative idea generation and requests for visual feedback\u2014were associated with larger reductions in perceived cognitive load.", "conclusion": "GenAI\u2019s effectiveness in architectural conceptual design is contingent on user characteristics and interaction patterns. While it can enhance design performance for novices, it may simultaneously undermine users\u2019 general creative self-efficacy and does not automatically reduce cognitive load. The way users prompt the system, especially for iterative ideation and visual feedback, appears critical. Educators and practitioners should consider expertise level and teach effective prompting strategies when integrating GenAI into design workflows."}}
{"id": "2601.10315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10315", "abs": "https://arxiv.org/abs/2601.10315", "authors": ["Aniket Deroy"], "title": "ADVOSYNTH: A Synthetic Multi-Advocate Dataset for Speaker Identification in Courtroom Scenarios", "comment": null, "summary": "As large-scale speech-to-speech models achieve high fidelity, the distinction between synthetic voices in structured environments becomes a vital area of study. This paper introduces Advosynth-500, a specialized dataset comprising 100 synthetic speech files featuring 10 unique advocate identities. Using the Speech Llama Omni model, we simulate five distinct advocate pairs engaged in courtroom arguments. We define specific vocal characteristics for each advocate and present a speaker identification challenge to evaluate the ability of modern systems to map audio files to their respective synthetic origins.\n  Dataset is available at this link-https: //github.com/naturenurtureelite/ADVOSYNTH-500.", "AI": {"tldr": "The paper presents Advosynth-500, a dataset of synthetic courtroom speech used to test whether systems can distinguish between different synthetic advocate voices.", "motivation": "As speech-to-speech models become more realistic, distinguishing between different synthetic voices\u2014especially in structured, high-stakes settings like courtrooms\u2014becomes important for transparency, forensics, and system evaluation. The authors aim to study how well modern models can identify and attribute synthetic speech to specific voice identities.", "method": "They construct Advosynth-500, a dataset of 100 synthetic speech files representing 10 distinct advocate identities. Using the Speech Llama Omni model, they generate courtroom-style dialogues between five pairs of advocates, each assigned predefined vocal characteristics. They then formulate a speaker identification task where systems must map each audio file to the correct synthetic advocate identity.", "result": "The abstract does not report quantitative results, but it establishes the dataset and task: an identification challenge that can be used to evaluate modern models\u2019 capacity to recognize and label synthetic voices according to their defined identities.", "conclusion": "The paper introduces Advosynth-500 as a benchmark dataset and task for synthetic speaker identification in courtroom-style interactions, aiming to support research on discriminating between high-fidelity synthetic voices and properly attributing them to their intended synthetic origins."}}
{"id": "2601.10318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10318", "abs": "https://arxiv.org/abs/2601.10318", "authors": ["Songsong Tian", "Kongsheng Zhuo", "Zhendong Wang", "Rong Shen", "Shengtao Zhang", "Yong Wu"], "title": "Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis", "comment": null, "summary": "In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.", "AI": {"tldr": "BAR-SQL is a reliability-focused NL2SQL training framework that combines synthetic boundary-aware data, knowledge-grounded chain-of-thought, and RL with hybrid rewards to improve both SQL accuracy and safe abstention on hard or ambiguous queries.", "motivation": "Existing NL2SQL systems prioritize execution accuracy on standard benchmarks but are brittle in realistic enterprise settings, where multi-step analytics, ambiguous questions, and schema limitations are common. They often hallucinate SQL instead of abstaining, lack explicit grounding in schema/business rules, and are not directly optimized for reliability or boundary awareness. The paper aims to create a framework that can both generate high-quality SQL and recognize when it should abstain, with interpretable reasoning and evaluation tailored to enterprise needs.", "method": "The authors propose BAR-SQL, a unified training framework that: (1) builds an enterprise-style training corpus via a Seed Mutation paradigm that expands seed queries into diverse multi-step analytical tasks and explicit boundary cases (ambiguity, schema gaps); (2) synthesizes knowledge-grounded chain-of-thought traces anchored in schema metadata and business rules (Knowledge-Grounded Reasoning Synthesis) to make reasoning explicit; (3) trains the model in two stages\u2014first with supervised fine-tuning on the synthesized data, then with reinforcement learning using Group Relative Policy Optimization; and (4) applies a Task-Conditioned Hybrid Reward that jointly considers SQL execution accuracy (via AST-level analysis and dense result matching) and semantic quality of abstention responses, aligning the model toward both correct SQL and appropriate abstention.", "result": "On the newly constructed Ent-SQL-Bench benchmark, which jointly measures SQL precision and boundary-aware abstention on ambiguous/unanswerable queries, BAR-SQL attains an average accuracy of 91.48%. It surpasses leading proprietary baselines such as Claude 4.5 Sonnet and GPT-5 on both SQL generation quality and reliability in abstaining when queries are ambiguous or unanswerable. The framework and benchmark are made publicly available for further research and comparison.", "conclusion": "BAR-SQL demonstrates that embedding reliability, boundary awareness, and schema/business-rule-grounded reasoning directly into the NL2SQL generation and training process significantly improves both SQL accuracy and safe abstention behavior. The combination of synthetic boundary-aware data, knowledge-grounded chain-of-thought, and RL with a hybrid reward yields a more dependable NL2SQL system for enterprise scenarios, and Ent-SQL-Bench provides an appropriate evaluation standard for this reliability-focused setting."}}
{"id": "2601.10321", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.10321", "abs": "https://arxiv.org/abs/2601.10321", "authors": ["Warren Jouanneau", "Emma Jouffroy", "Marc Palyart"], "title": "An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit", "comment": null, "summary": "Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.", "AI": {"tldr": "They build a re-ranking model for matching resumes to job briefs using a late cross-attention architecture and LLM-based distillation, achieving better relevance and calibration than prior methods.", "motivation": "Real-time person-job matching is difficult when dealing with long, structured, multilingual resumes and project descriptions, and existing systems can be biased and poorly calibrated, leading to suboptimal and hard-to-interpret rankings.", "method": "They design a late cross-attention re-ranking architecture that decomposes both resumes and project briefs to efficiently process long contexts. To reduce reliance on biased historical labels, they use a generative LLM as a teacher to create fine-grained, semantically rich supervision signals. These are distilled into a smaller student model with an enhanced distillation loss tailored to producing well-calibrated skill-fit scores.", "result": "Their student re-ranking model outputs skill-fit scores for person-job pairs and, according to experiments on various relevance, ranking, and calibration metrics, surpasses state-of-the-art baselines.", "conclusion": "Using a late cross-attention re-ranking model combined with LLM-based semantic supervision and enriched distillation yields more accurate, interpretable, and calibrated skill-fit scores for person-job matching, improving over existing approaches while remaining efficient for long-context, multilingual inputs."}}
{"id": "2601.10343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10343", "abs": "https://arxiv.org/abs/2601.10343", "authors": ["Deming Ding", "Shichun Liu", "Enhui Yang", "Jiahang Lin", "Ziying Chen", "Shihan Dou", "Honglin Guo", "Weiyu Cheng", "Pengyu Zhao", "Chengjun Xiao", "Qunhong Zeng", "Qi Zhang", "Xuanjing Huang", "Qidi Xu", "Tao Gui"], "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding", "comment": null, "summary": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.", "AI": {"tldr": "OctoBench is a benchmark for evaluating how well LLM-based coding agents follow complex, scaffold-specified instructions in repository-grounded coding tasks, separately from how well they solve the tasks themselves.", "motivation": "Existing coding scaffolds can turn LLMs into powerful software agents, but it is unclear how reliably these agents follow the heterogeneous and persistent constraints imposed by scaffolds, as prior work has mostly focused on task success rather than rule compliance. There is a need for a systematic way to measure and understand instruction following in realistic multi-step coding settings.", "method": "The authors construct OctoBench, a benchmark consisting of 34 coding environments and 217 tasks instantiated under three different scaffold types, all grounded in software repositories. They pair these with 7,098 objective checklist items that encode scaffold rules and task requirements. An automated observation-and-scoring toolkit captures full agent trajectories and performs fine-grained, checklist-based evaluation to separate task completion from scaffold-rule compliance. They then run experiments on eight representative LLM-based coding models to assess performance and analyze behavior.", "result": "Across the eight evaluated models, there is a consistent and systematic gap between how well agents solve coding tasks and how well they comply with scaffold-specified instructions. Agents can often achieve task goals while still violating various heterogeneous constraints encoded in the scaffolds, as revealed by the fine-grained checklist scoring over recorded trajectories.", "conclusion": "Scaffold-aware instruction following is a distinct and currently under-served capability in LLM-based coding agents: they may solve tasks but still fail to adhere to complex, persistent rules specified by scaffolds. OctoBench provides a reproducible benchmark and tooling to measure this gap and is intended to drive research and model development that more explicitly targets robust, heterogeneous instruction following in agentic coding systems."}}
{"id": "2601.10348", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10348", "abs": "https://arxiv.org/abs/2601.10348", "authors": ["Zhanming Shen", "Jiaqi Hu", "Zeyu Qin", "Hao Chen", "Wentao Ye", "Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Junlin Zhou", "Junbo Zhao"], "title": "Training-Trajectory-Aware Token Selection", "comment": null, "summary": "Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.", "AI": {"tldr": "The paper studies why continual distillation of already-strong reasoning models can stall or degrade performance, identifies a token-level bottleneck phenomenon, and proposes a new token selection method (T3S) that yields strong reasoning improvements for distilled models.", "motivation": "As reasoning-capable large models are expensive to deploy, distilling their abilities into smaller, efficient students is crucial. However, when the student already has strong reasoning skills, naive continual distillation often offers little benefit or even harms performance despite decreasing training loss. Understanding and fixing this failure mode is necessary to push the frontier of efficient reasoning models.", "method": "The authors empirically study the training dynamics of continual distillation and discover a bottleneck where performance metrics sharply drop while loss continues to decrease. They conduct token-level analysis and find two distinct token types: Imitation-Anchor Tokens whose confidence steadily grows and quickly dominate optimization, and yet-to-learn tokens whose confidence is suppressed until after the bottleneck. They argue that these token types cannot coexist effectively, causing distillation failure. To address this, they introduce Training-Trajectory-Aware Token Selection (T3S), which reconstructs the training objective at the token level to de-emphasize already-anchored tokens and free optimization capacity for yet-to-learn tokens.", "result": "Using T3S in both autoregressive (AR) and distilled-LLM (dLLM) settings, the authors achieve consistent performance gains. With only hundreds of training examples, a distilled Qwen3-8B model surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches the performance of Qwen3-235B, and T3-trained LLaDA-2.0-Mini outperforms its AR baseline, setting a new state of the art among 16B-parameter no-think models.", "conclusion": "Continual distillation of strong reasoning models fails not because of overall optimization issues but due to a token-level competition between anchor tokens and yet-to-learn tokens, leading to a training bottleneck. By restructuring the objective through Training-Trajectory-Aware Token Selection, the paper shows that it is possible to clear this bottleneck and significantly enhance the reasoning performance of smaller, distilled models with modest additional data, advancing the practicality of deployable reasoning systems."}}
{"id": "2601.10355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10355", "abs": "https://arxiv.org/abs/2601.10355", "authors": ["Zhihao Xu", "Rumei Li", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai", "Xiting Wang"], "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text", "comment": null, "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on \u03c4 - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.", "AI": {"tldr": "The paper introduces GEM, a pipeline and a specialized model (Trajectory Synthesizer) that automatically converts existing text corpora into multi-turn tool-use trajectories for training LLM agents, improving multi-turn tool-use performance and efficiency.", "motivation": "Multi-turn tool use is crucial for autonomous LLM agents, but collecting diverse, realistic multi-step interaction data is difficult and costly. Existing datasets are limited in scale and domain, which restricts generalization. The authors aim to unlock a scalable, authentic, and inexpensive data source for such trajectories by reusing existing textual problem-solving content.", "method": "They propose GEM, a four-stage data synthesis pipeline applied to text corpora: (1) relevance filtering to identify text containing multi-step problem solving; (2) workflow and tool extraction to infer the sequence of actions and tools implicit in the text; (3) trajectory grounding to map these workflows into explicit multi-turn tool-use trajectories; and (4) complexity refinement to enhance difficulty and diversity. To reduce the cost of running this multi-step pipeline, they further train a specialized Trajectory Synthesizer model via supervised fine-tuning to imitate the pipeline and generate trajectories end-to-end more efficiently.", "result": "Using the synthesized data and the GEM-32B model, they achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Their models outperform or partially surpass models trained on \u03c4-bench (Airline and Retail) in-domain data, indicating better cross-domain generalization. The Trajectory Synthesizer model matches the quality of trajectories generated by the full GEM pipeline, while significantly reducing inference latency and computational costs.", "conclusion": "Text corpora inherently contain rich multi-step problem-solving processes that can be systematically transformed into high-quality multi-turn tool-use trajectories. The GEM pipeline and its distilled Trajectory Synthesizer demonstrate that such text-based synthesis is a scalable, effective, and cost-efficient strategy for training LLMs with strong multi-turn tool-use capabilities and good generalization beyond in-domain datasets."}}
{"id": "2601.10387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10387", "abs": "https://arxiv.org/abs/2601.10387", "authors": ["Christina Lu", "Jack Gallagher", "Jonathan Michala", "Kyle Fish", "Jack Lindsey"], "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models", "comment": null, "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.", "AI": {"tldr": "The paper studies how large language models express different personas and identifies a dominant \u201cAssistant Axis\u201d in activation space that governs how much the model behaves like its default helpful assistant persona versus alternative identities.", "motivation": "Although LLMs can emulate many personas, they tend to default to a generic helpful assistant due to post-training. However, models can unpredictably \u201cdrift\u201d into strange or harmful personas, especially under meta-level or emotionally intense interactions. The authors want to understand the structure of persona representations in model activations, diagnose persona drift, and design ways to stabilize the model\u2019s persona and improve safety and reliability.", "method": "The authors extract activation directions in representation space corresponding to various character archetypes to build a \u201cpersona space.\u201d Using multiple models, they identify principal components of this space, focusing on the leading one, which they term the \u201cAssistant Axis.\u201d They then perform activation steering along this axis\u2014both towards and away from the assistant direction\u2014during generation. They analyze the impact on behavior, style, and self-identification, investigate the presence of this axis in pre-trained (non\u2013post-trained) models, and study correlations between deviations along the axis and occurrences of persona drift, including under adversarial persona jailbreak prompts. Finally, they test constraining activations to a fixed region along the axis as a control mechanism.", "result": "The authors find a consistent, dominant Assistant Axis across different language models. Steering toward this direction enhances helpful, harmless assistant-like behavior; steering away increases the chance the model adopts non-assistant personas and, at larger magnitudes, induces a mystical, theatrical style. In pre-trained models, this axis is still present, but it mainly boosts helpful human-like archetypes (e.g., consultants, coaches) and suppresses spiritual or mystical personas. Quantitatively, large deviations along this axis predict persona drift, including harmful or bizarre behaviors. Conversations involving self-reflection on the model\u2019s own processes or emotionally vulnerable users tend to push the model away from its assistant region, increasing drift. Constraining activations to a stable range along the Assistant Axis reduces persona drift and improves robustness against persona-based jailbreak attacks.", "conclusion": "Persona expression in LLMs is structured along a prominent Assistant Axis that reflects how strongly the model adheres to its default assistant persona. Post-training biases models into a particular region along this axis but does not firmly anchor them there, allowing drift into less safe or coherent personas under certain conversational pressures. Activation-level steering that keeps the model within a safe band along the Assistant Axis can stabilize behavior and mitigate jailbreaks. This motivates future work on training and steering methods that more robustly and deeply tie models to a consistent, safe persona representation."}}
{"id": "2601.10388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10388", "abs": "https://arxiv.org/abs/2601.10388", "authors": ["Tarun Sharma", "Manikandan Ravikiran", "Sourava Kumar Behera", "Pramit Bhattacharya", "Arnab Bhattacharya", "Rohit Saluja"], "title": "INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects", "comment": null, "summary": "Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\\% to 89.8\\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI\" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.", "AI": {"tldr": "They build INDIC-DIALECT, a 13k-sentence human-curated parallel corpus for 11 Hindi and Odia dialects and standard languages, and use it to benchmark dialect classification, QA, and translation, showing large LLMs struggle while fine-tuned and hybrid models perform much better.", "motivation": "Most NLP progress targets standardized, high-resource languages, leaving dialects\u2014especially Indian ones like Hindi and Odia varieties\u2014underrepresented, with little web data and limited tools. This gap prevents effective NLP support for hundreds of millions of speakers of dialects that differ significantly from the standardized forms. The authors seek to create high-quality resources and benchmarks to enable dialect-aware NLP for Indic languages, and to systematically evaluate how current models handle dialectal variation.", "method": "They manually curate INDIC-DIALECT, a 13k parallel corpus of sentences in 11 dialects of Hindi and Odia aligned with their standard language counterparts. On top of this, they define a multi-task benchmark with three tasks: (1) dialect classification (identify which dialect a given sentence belongs to), (2) multiple-choice QA, and (3) machine translation between dialect and standard language. They then evaluate different model families: general-purpose LLMs such as GPT-4o and Gemini 2.5; and smaller transformer models pretrained on Indian languages that are fine-tuned on INDIC-DIALECT. For MT, they experiment with several strategies, including pure neural baselines and hybrid pipelines that combine rule-based components with AI models in different orders (e.g., rule-based then AI) for both dialect-to-language and language-to-dialect directions. Performance is quantified using metrics such as F1 score for classification and BLEU for translation.", "result": "General-purpose LLMs do poorly on dialect classification, with low F1 (around 19.6%), whereas fine-tuned, India-focused transformer models achieve much higher performance, boosting F1 up to 89.8%. For dialect-to-standard-language MT, a hybrid AI approach yields the best BLEU score of 61.32 versus a baseline of 23.36, indicating large quality gains. For the reverse direction (standard language to dialect), generating dialectal text proves harder; in this case, a pipeline that first applies rule-based transformations followed by an AI model achieves the best BLEU score of 48.44, again clearly outperforming a 27.59 baseline. These results underscore both the difficulty of dialectal tasks for general LLMs and the effectiveness of specialized, hybrid, and fine-tuned systems when backed by a curated dialect corpus.", "conclusion": "INDIC-DIALECT provides a new, publicly available benchmark and dataset for dialect-aware NLP in Indic languages, initially covering 11 dialects across Hindi and Odia. The experiments demonstrate that out-of-the-box LLMs are not sufficient for robust dialect handling, but that fine-tuned transformers and carefully designed hybrid rule-based-plus-AI pipelines can close much of the performance gap, especially in translation tasks. The work highlights the need for targeted resources and modeling strategies for low-resource dialects, and aims to catalyze further research on Indian dialects and, by extension, other under-served language varieties."}}
{"id": "2601.10410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10410", "abs": "https://arxiv.org/abs/2601.10410", "authors": ["Mihai Dan Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "title": "TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction", "comment": null, "summary": "Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.", "AI": {"tldr": "The paper presents TF3-RO, a fully reproducible pipeline for building compact Romanian language models and generating large-scale synthetic Romanian narrative data, covering tokenizer design, model pretraining, compression, and multifaceted evaluation.", "motivation": "Although synthetic data and compact language models work well for English, morphologically rich and low-resource languages like Romanian lack an open, end-to-end, reproducible framework spanning tokenization, model training, compression, evaluation, and synthetic data generation. Existing approaches do not adequately address token inflation from rich morphology or provide documented pipelines tailored to Romanian.", "method": "The authors build TF3-RO, a Romanian-centric pipeline starting from TF1 (English fables) and TF2 (Romanian translations). They design Romanian-specific BPE and Unigram tokenizers using a linguistically curated corpus to reduce token inflation. They pretrain from scratch a 51.65M-parameter LLaMA-style Transformer using long-sequence packed training. They then compress this model via quantization, structured pruning, and logit-based knowledge distillation to obtain a 26.45M-parameter student model with tied embeddings. Finally, they use this compact model within a controlled combinatorial prompting framework to generate three million Romanian-native synthetic fables. Throughout, they apply an evaluation suite including intrinsic metrics, Romanian agreement probes, entity coherence checks, rule-based grammar checking, and LLM-based evaluations.", "result": "TF3-RO yields a Romanian-specific tokenizer that better handles morphological richness, a from-scratch 51.65M-parameter LLaMA-style model, and a compressed 26.45M-parameter student model suitable for deployment. Using the distilled model, the authors generate a large-scale synthetic corpus of three million Romanian fables. The evaluation suite shows that the pipeline produces linguistically coherent, morphologically aware models and data, with strong performance for such a compact architecture.", "conclusion": "TF3-RO demonstrates that it is feasible to build compact yet effective Romanian language models using a fully reproducible, linguistically grounded pipeline. By unifying tokenizer design, model pretraining, compression, and rigorous evaluation, and by generating millions of Romanian-native synthetic narratives, the framework offers a practical blueprint for supporting other morphologically rich, low-resource languages."}}
{"id": "2601.10421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10421", "abs": "https://arxiv.org/abs/2601.10421", "authors": ["Philip Resnik"], "title": "Are Language Models Models?", "comment": "5 pages. This is an invited commentary under review at Behavioral and Brain Sciences", "summary": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.", "AI": {"tldr": "The paper argues that language models should not be treated as cognitive models of the human mind, especially when evaluated through Marr\u2019s three levels of analysis.", "motivation": "The authors respond to claims by Futrell and Mahowald that language models can function as model systems for studying cognition, aiming to clarify and critique this position and prevent overclaiming about what LMs explain about the human mind.", "method": "They conceptually analyze language models using Marr\u2019s three levels of analysis (implementation, algorithmic-representational, and computational theory), evaluating how well LMs qualify as cognitive models at each level.", "result": "At the implementation level, LMs clearly fail to match human cognitive implementation; at the algorithmic-representational level, the justification for treating them as cognitive models is weak; and at the computational theory level, there are conceptual problems with the claim that they capture the right explanatory goals for human cognition.", "conclusion": "Language models are better viewed as useful tools rather than as full-fledged cognitive models of the human mind; calling them cognitive models is overstated and risks contributing to unwarranted hype around LLMs."}}
{"id": "2601.10455", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10455", "abs": "https://arxiv.org/abs/2601.10455", "authors": ["Ruochen Li", "Kun Yuan", "Yufei Xia", "Yue Zhou", "Qingyu Lu", "Weihang Li", "Youxiang Zhu", "Nassir Navab"], "title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability", "comment": null, "summary": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.", "AI": {"tldr": "The paper proposes a new way to evaluate vision-language models for surgical planning, showing common sequence-based metrics fail and introducing a rule-based, goal-satisfiability meta-evaluation that better captures plan correctness.", "motivation": "Current evaluations of vision-language and video-language models in surgical planning do not reliably reflect whether a generated plan is actually safe and correct in a clinical sense. Existing sequence similarity metrics (e.g., matching model output to reference steps) can mislabel valid but different plans as wrong, and fail to catch truly invalid plans. There is a need for an evaluation grounded in expert surgical rules and goal satisfaction rather than surface-level sequence overlap.", "method": "The authors formalize surgical planning as a goal-oriented process and define planning correctness via phase-goal satisfiability: a plan is valid if it satisfies expert-defined surgical rules and achieves the required goals for each phase of the procedure. They build a multicentric meta-evaluation benchmark that includes (1) multiple valid procedural variations and (2) systematically constructed invalid plans with order and content errors. They then compare sequence similarity metrics against a rule-based goal-satisfiability metric and use this rule-based metric as a high-precision meta-evaluation reference to assess various Video-LLMs under different constraint settings (e.g., with and without structural or semantic guidance).", "result": "On the new benchmark, common sequence similarity metrics systematically mis-evaluate planning quality: they penalize valid alternative plans and often fail to detect order and content errors in invalid plans. When models are evaluated with the rule-based goal-satisfiability metric, clear failure modes emerge, including errors due to visual misperception and reasoning that is too weakly constrained by procedural structure. Incorporating explicit structural knowledge into the prompting or modeling consistently improves Video-LLM planning performance, while purely semantic guidance is unstable and only helps larger models when combined with structural constraints.", "conclusion": "Sequence similarity is inadequate for evaluating surgical planning by VLMs; expert-rule-based goal satisfiability provides a more faithful notion of correctness. The proposed meta-evaluation benchmark and rule-based metric expose perception and reasoning limitations in current Video-LLMs. Embedding structural surgical knowledge is key to improving reliability, whereas semantic hints alone are insufficient. The work argues for goal- and rule-based evaluation protocols in safety-critical, long-horizon planning tasks involving multimodal models."}}
{"id": "2601.10460", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10460", "abs": "https://arxiv.org/abs/2601.10460", "authors": ["Abhinaba Basu", "Pavan Chakraborty"], "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models", "comment": null, "summary": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.\n  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.\n  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.\n  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.", "AI": {"tldr": "The paper introduces Contextual StereoSet and Context Sensitivity Fingerprints to show that measured stereotype bias in language models can change significantly with contextual framing, even when stereotype content is held constant.", "motivation": "Existing bias benchmarks usually test models in fixed, decontextualized conditions, implicitly assuming that measured bias is stable across realistic variations in context (time, place, audience, framing). This assumption is problematic for deployment, where inputs will vary widely. The authors aim to demonstrate that bias measurements are highly context-sensitive and to provide tools to systematically characterize how context shifts measured bias, thereby stress-testing the robustness of bias evaluations rather than estimating a single ground-truth bias rate.", "method": "They construct Contextual StereoSet, a benchmark that keeps the underlying stereotype content fixed but systematically varies contextual factors such as temporal anchoring (e.g., 1990 vs. 2030), discourse framing (e.g., gossip), and audience/observer framing (e.g., in-group vs. out-group observers). They evaluate 13 language models using two evaluation protocols across a wide grid of contexts and across specific application-style vignettes (hiring, lending, help-seeking). They then define Context Sensitivity Fingerprints (CSF), which summarize each model\u2019s bias variability via per-dimension dispersion statistics and paired context contrasts, using bootstrap confidence intervals and false discovery rate (FDR) correction. They offer two tracks: a dense 360-context diagnostic grid and a budgeted, production-oriented protocol with 4,229 items.", "result": "The authors find that bias scores change substantially when only the context is altered. For example, setting the temporal context to 1990 instead of 2030 increases stereotype selections in all models tested on this contrast (statistically significant at p<0.05). Gossip-like framing increases stereotype selection in 5 of 6 models tested in the full context grid, and framing involving out-group observers can shift stereotype selection by up to 13 percentage points. These context effects are not isolated to synthetic items: they replicate in more applied vignettes for hiring, lending, and help-seeking scenarios. CSFs compactly capture these systematic context-driven variations for each model.", "conclusion": "Bias evaluations that use a single, fixed testing condition are not robust and their scores may not generalize to realistic deployment settings. Rather than asking whether a model is biased in an absolute sense, evaluators should ask under which contextual conditions bias manifests or intensifies. Contextual StereoSet and Context Sensitivity Fingerprints provide a practical framework to probe and summarize this context dependence, enabling more informative and stress-tested bias assessments. The authors make their benchmark, code, and model results publicly available to support continued research and practical screening workflows."}}
{"id": "2601.10504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10504", "abs": "https://arxiv.org/abs/2601.10504", "authors": ["Yiwen Gao", "Ruochen Zhao", "Yang Deng", "Wenxuan Zhang"], "title": "DR-Arena: an Automated Evaluation Framework for Deep Research Agents", "comment": "22 pages, 8 figures", "summary": "As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.", "AI": {"tldr": "The paper presents DR-Arena, an automated, dynamic benchmark for evaluating deep research agents (LLMs used for web-based investigation), addressing the limits of static datasets and achieving strong correlation with human preference rankings.", "motivation": "Existing LLM / deep research agent benchmarks are mostly static datasets, which quickly become outdated, are vulnerable to training data contamination, and cover only narrow task types. Human evaluation (e.g., arenas and pairwise battles) is costly and slow. There is a need for an automatically updating, scalable, and reliable evaluation framework that reflects real-world information needs and aligns with human judgments.", "method": "The authors design DR-Arena, an evaluation pipeline that: (1) builds real-time Information Trees from current web trends to keep tasks synchronized with the live world; (2) uses an automated Examiner to convert these trees into structured tasks that separately probe deep reasoning and wide coverage capabilities; and (3) implements an Adaptive Evolvement Loop, a state-machine controller that monitors agent performance and incrementally raises task difficulty (requiring deeper deduction or broader aggregation) until a clear performance boundary is identified. They then run six advanced DR agents through this framework and compare their rankings with an established human-evaluated leaderboard (LMSYS Search Arena).", "result": "Running six strong DR agents in DR-Arena yields a very high Spearman rank correlation (0.94) with the LMSYS Search Arena leaderboard, indicating that DR-Arena\u2019s automatic ranking is closely aligned with human preference judgments. This performance beats prior automatic evaluation methods, establishing DR-Arena as state-of-the-art in automatic evaluation alignment for DR agents.", "conclusion": "DR-Arena is an effective, fully automated, and dynamically updating evaluation framework for deep research agents. By leveraging real-time web information, structured task generation, and adaptive task escalation, it overcomes the core limitations of static benchmarks and approximates human evaluation quality. The high correlation with human preference rankings suggests DR-Arena can serve as a practical, low-cost substitute for manual evaluation in developing and comparing DR-capable LLMs."}}
{"id": "2601.10513", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10513", "abs": "https://arxiv.org/abs/2601.10513", "authors": ["Xuan Luo", "Lewei Yao", "Libo Zhao", "Lanqing Hong", "Kai Chen", "Dehua Tao", "Daxin Tan", "Ruifeng Xu", "Jing Li"], "title": "AEQ-Bench: Measuring Empathy of Omni-Modal Large Models", "comment": null, "summary": "While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.", "AI": {"tldr": "AEQ-Bench is a benchmark to test omni-modal large models on understanding and expressing empathy from audio+text, and on judging empathy in audio alone.", "motivation": "Empathy is crucial for human-centered AI, but is difficult to evaluate automatically because it depends on subtle affective and paralinguistic cues, especially in audio. Existing benchmarks focus mostly on text or coarse judgments and do not systematically test multimodal empathy perception and expression.", "method": "The authors build AEQ-Bench, an audio empathy benchmark with two core tasks: (1) test whether models can generate empathetic responses given multi-modal inputs (audio plus text) containing affective cues; and (2) test whether models can evaluate how empathetic an audio response is without using transcripts. They design two novel evaluation settings that manipulate context specificity and speech tone, and they use a combination of linguistic and paralinguistic metrics plus human judgments to assess performance of different omni-modal large models.", "result": "Models capable of audio output (speaking) generally perform better on the empathy-related tasks than models restricted to text outputs. OLMs\u2019 automatic scores correlate reasonably with human judgments for high-level (coarse) quality, but they do not reliably capture fine-grained paralinguistic aspects of empathy, such as nuanced tone or prosody.", "conclusion": "AEQ-Bench exposes a gap between current omni-modal models and human-like empathetic understanding and expression in audio. While these models can roughly assess and generate empathy, especially when they handle audio directly, they still struggle with fine-grained paralinguistic expressiveness, indicating a need for better modeling and evaluation of affective audio cues."}}
{"id": "2601.10532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10532", "abs": "https://arxiv.org/abs/2601.10532", "authors": ["Chengbing Wang", "Wuqiang Zheng", "Yang Zhang", "Fengbin Zhu", "Junyi Cheng", "Yi Xie", "Wenjie Wang", "Fuli Feng"], "title": "PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\\%. Furthermore, a blinded user study reveals a 70\\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.", "AI": {"tldr": "The paper proposes PERM, a psychology-grounded, bidirectional reward model to improve empathetic responses of large language models, significantly outperforming existing methods.", "motivation": "Existing RL-based methods to enhance empathy in LLMs use reward models that evaluate empathy from a single perspective, ignoring the bidirectional nature of empathy interactions between a supporter and a seeker described by Empathy Cycle theory. This limits the quality and authenticity of emotional support LLMs can provide in human-centric applications.", "method": "The authors design Psychology-grounded Empathetic Reward Modeling (PERM), which decomposes empathy evaluation into multiple perspectives. From the supporter perspective, it measures internal emotional resonance and the quality of empathetic expression; from the seeker perspective, it evaluates how well the emotional support is received. A bystander perspective is added to judge overall interaction quality. This composite reward is used within an RL framework to optimize LLM responses for empathy.", "result": "On a standard emotional intelligence benchmark and an industrial daily conversation dataset, PERM-based training improves empathetic response quality by more than 10% over state-of-the-art baselines. In a blinded human evaluation, 70% of users prefer responses generated with PERM, indicating more effective and satisfying empathetic interactions.", "conclusion": "Grounding reward modeling in psychological empathy theory and modeling empathy as a bidirectional process leads to more genuinely empathetic LLM responses. PERM offers an effective framework for training LLMs to provide higher-quality emotional support, backed by both quantitative gains and strong human preference."}}
{"id": "2601.10566", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10566", "abs": "https://arxiv.org/abs/2601.10566", "authors": ["Syed Naveed Mahmood", "Md. Rezaur Rahman Bhuiyan", "Tasfia Zaman", "Jareen Tasneem Khondaker", "Md. Sameer Sakib", "Nazia Tasnim", "Farig Sadeque"], "title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure", "comment": "16 pages, 4 figures", "summary": "Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.", "AI": {"tldr": "The paper proposes Knowledge Immunization Framework (KIF) to truly erase specific knowledge from LLMs by modifying internal representations rather than only forcing refusal behaviors, achieving near-perfect erasure while keeping overall utility almost unchanged.", "motivation": "Existing unlearning methods for LLMs often just make the model refuse to answer certain queries, but the underlying knowledge remains encoded in the network, posing risks for GDPR compliance, safety, and potential information leakage. There is a need to distinguish real knowledge removal from mere behavioral obfuscation and to understand forgetting mechanisms across different model architectures and scales.", "method": "They introduce KIF, a representation-aware framework that targets internal activation signatures associated with specific knowledge. KIF dynamically suppresses subject-specific representations and uses parameter-efficient adaptation instead of full retraining. They also design a dual-metric evaluation protocol that measures both surface-level leakage (what the model outputs) and latent trace persistence (whether internal representations still encode the erased knowledge), and apply it to several LLM families (Llama, Mistral, Qwen, DeepSeek) with sizes from 3B to 14B parameters.", "result": "KIF achieves near-oracle levels of knowledge erasure (forgetting quality FQ ~0.99 vs. ideal 1.00) while maintaining model utility at the same level as oracle baselines (MU = 0.62), effectively overcoming the typical tradeoff between stability and erasure quality encountered in previous work. Experiments reveal that standard foundation models show scale-independent true erasure with under 3% utility drift, whereas reasoning-prior models behave differently, indicating a fundamental architectural divergence in how they store and forget knowledge.", "conclusion": "KIF can durably and efficiently erase targeted knowledge from LLMs at the representation level while preserving overall performance, providing a practical path toward GDPR-compliant and safer models. The proposed dual-metric evaluation clarifies the difference between obfuscation and true erasure and enables systematic analysis of forgetting behavior across model families and sizes, revealing that architectural choices (e.g., reasoning priors) materially affect how knowledge can be removed."}}
{"id": "2601.10580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10580", "abs": "https://arxiv.org/abs/2601.10580", "authors": ["Wessel Poelman", "Miryam de Lhoneux"], "title": "Form and Meaning in Intrinsic Multilingual Evaluations", "comment": "EACL 2026: Main Conference", "summary": "Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.", "AI": {"tldr": "The paper examines whether intrinsic metrics like perplexity are reliable for comparing conditional language models across languages and concludes they are not universally comparable.", "motivation": "Intrinsic metrics such as perplexity and bits-per-character are heavily used to evaluate conditional language models, including in multilingual scenarios. However, their use across languages implicitly assumes that equal scores on parallel sentences reflect equal model quality, based on shared semantic content. This assumption may be flawed because these metrics fundamentally measure information content in the information-theoretic sense, not semantic equivalence. The paper is motivated by the need to question, formalize, and test these hidden assumptions in multilingual evaluation.", "method": "The authors first analyze and make explicit several underlying assumptions involved in using intrinsic metrics, such as perplexity, for multilingual comparison. They then empirically test these assumptions by running experiments with six different intrinsic evaluation metrics on two multi-parallel corpora. They apply these metrics to both monolingual and multilingual conditional language models, comparing behavior across languages and model types.", "result": "The experiments show that the examined intrinsic metrics do not behave in a universally comparable way across languages and models. In other words, similar metric values on parallel sentences do not reliably indicate comparable model quality or comparable semantic handling. The behavior of the metrics is sensitive to language-specific properties and modeling choices, which undermines the naive cross-lingual comparability assumption.", "conclusion": "The paper concludes that widely used intrinsic metrics for conditional language models, such as perplexity and bits-per-character, cannot be straightforwardly used to compare models across languages, even when evaluated on parallel data. The authors relate these findings to the form-meaning debate, arguing that focusing purely on information-theoretic form (e.g., symbol sequences and their probabilities) misses important aspects of semantic meaning that matter for evaluation. They suggest that more nuanced or complementary evaluation approaches are needed for multilingual model comparison."}}
{"id": "2601.10645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10645", "abs": "https://arxiv.org/abs/2601.10645", "authors": ["Yuxi Xia", "Loris Schoenegger", "Benjamin Roth"], "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs", "comment": null, "summary": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.", "AI": {"tldr": "The paper investigates why large language models sound confident even when they\u2019re wrong, by tracing where their confidence expressions come from in the training data.", "motivation": "LLMs often verbalize confidence (e.g., \u201cI\u2019m sure that\u2026\u201d) which increases user trust, but this confidence is unreliable because models are frequently overconfident. The authors want to understand what in the training data causes these confidence expressions, and whether they are content-based or just stylistic, in order to improve the trustworthiness of LLM confidence.", "method": "They propose TracVC (Tracing Verbalized Confidence), which combines information retrieval and influence estimation to trace confidence-related phrases in model outputs back to specific training examples. They apply TracVC to OLMo and Llama models in a question answering setting. They also introduce a metric called content groundness that measures how much the model\u2019s confidence is grounded in content-relevant training data (related to the question and answer) versus generic confidence-verbalization examples.", "result": "Using TracVC on OLMo2-13B and Llama models, they find that OLMo2-13B often relies on training examples that express confidence but are lexically unrelated to the current query. This indicates the model is influenced by generic patterns of sounding confident instead of examples that are semantically tied to the specific question and answer. Quantitatively, this is reflected in lower content groundness, revealing a misalignment between confidence style and factual grounding.", "conclusion": "Current training regimes can lead LLMs to learn how to sound confident without learning when such confidence is warranted. By tracing confidence expressions back to their data sources and quantifying content groundness, the paper exposes a core limitation in how models internalize confidence. The work lays groundwork for designing training or fine-tuning methods that better align expressed confidence with actual content-based justification, thereby improving the reliability and trustworthiness of LLMs\u2019 confidence statements."}}
{"id": "2601.10660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10660", "abs": "https://arxiv.org/abs/2601.10660", "authors": ["Tiziano Labruna", "Arkadiusz Modzelewski", "Giorgio Satta", "Giovanni Da San Martino"], "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies", "comment": null, "summary": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.", "AI": {"tldr": "The paper proposes a strategy-aware LLM-based method to detect and explain persuasiveness in argumentative text using multiple persuasion strategies across three datasets.", "motivation": "Persuasiveness is central to argument quality, but existing automatic methods often treat it as a single label and ignore specific persuasion strategies such as attacks on reputation, distraction, and manipulative wording. This limits interpretability, robustness, and fine-grained understanding of how arguments persuade. The authors aim to explicitly model these strategies to both improve prediction and provide more interpretable assessments of argumentative texts.", "method": "They use large language models with a Multi-Strategy Persuasion Scoring approach. The model is prompted to reason explicitly about six distinct persuasion strategies and to produce structured scores/assessments for each strategy, which are then used to predict overall persuasiveness. Experiments are run on three annotated persuasion/argument datasets (Winning Arguments from Change My View, Anthropic/Persuasion, and Persuasion for Good). They also re-organize the Winning Arguments dataset into broad discussion topics and evaluate topic-wise performance.", "result": "Strategy-guided reasoning with LLMs yields better performance on predicting which arguments are persuasive than baselines that do not use explicit persuasion strategies. Topic-wise analysis on the reorganized Winning Arguments dataset shows how performance varies across different discussion topics. The authors also produce and release the topic-annotated version of the dataset as a resource.", "conclusion": "Explicitly modeling multiple persuasion strategies in a structured prompting framework improves the accuracy, interpretability, and robustness of persuasiveness prediction in argumentative texts. Strategy-aware LLM prompting helps connect surface text to underlying persuasive tactics and supports more nuanced argument quality assessment, while the released topic-labeled dataset enables further research on topic-dependent persuasion effects."}}
{"id": "2601.10700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10700", "abs": "https://arxiv.org/abs/2601.10700", "authors": ["Gilat Toker", "Nitay Calderon", "Ohad Amosy", "Roi Reichart"], "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals", "comment": null, "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.", "AI": {"tldr": "The paper introduces LIBERTy, a benchmarking framework using LLM-generated structural counterfactual text pairs grounded in explicit causal models to more faithfully evaluate concept-based explanations in NLP.", "motivation": "Existing evaluations of concept-based explanations in high-stakes domains depend on expensive, human-written counterfactuals that are only imperfect proxies for true causal effects. There is a need for scalable, systematically grounded benchmarks that reflect the underlying causal structure of text generation to properly assess how well concept-based explanation methods capture causal influence of high-level concepts on model behavior.", "method": "The authors define Structured Causal Models (SCMs) over text generation processes and use them to specify how interventions on high-level concepts (e.g., demographic attributes) propagate through a system. They then employ large language models to generate structural counterfactual text pairs consistent with these SCMs, constructing the LIBERTy benchmark datasets across three application domains: disease detection, CV screening, and workplace violence prediction. They additionally propose a new evaluation metric, order-faithfulness, which measures whether explanation methods correctly rank concepts by their causal influence. Using LIBERTy, they systematically test a variety of concept-based explanation methods on five different models.", "result": "LIBERTy yields three datasets of structural counterfactual pairs and an associated evaluation protocol. Empirical evaluation across many explanation techniques and five models reveals that current concept-based explanation methods exhibit substantial shortcomings, leaving significant room for improvement in capturing causal effects. The benchmark also exposes that proprietary LLMs are less sensitive to demographic concept interventions, suggesting that post-training mitigation procedures dampen their responsiveness to such attributes.", "conclusion": "LIBERTy offers a scalable, causally grounded benchmark for assessing the faithfulness of concept-based explanations in NLP. By using SCM-guided, LLM-generated structural counterfactuals and the order-faithfulness metric, it enables more rigorous comparison and diagnosis of explanation methods. The framework uncovers both limitations in existing methods and reduced demographic sensitivity in proprietary LLMs, positioning LIBERTy as a key resource for developing more faithful and causally sound explainability techniques."}}
{"id": "2601.10702", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.10702", "abs": "https://arxiv.org/abs/2601.10702", "authors": ["Ruozhen Yang", "Yucheng Jiang", "Yueqi Jiang", "Priyanka Kargupta", "Yunyi Zhang", "Jiawei Han"], "title": "Grounding Agent Memory in Contextual Intent", "comment": null, "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\n  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.", "AI": {"tldr": "The paper introduces STITCH, an intent-aware memory system that improves long-horizon reasoning of LLM agents by indexing and retrieving past steps using structured contextual intent signals, achieving state-of-the-art retrieval performance on new and existing benchmarks.", "motivation": "Large language models struggle in long-horizon, goal-oriented interactions because memory systems often retrieve semantically similar but context-mismatched evidence when entities and facts reoccur under different latent goals or constraints. This retrieval noise harms reasoning and task performance, especially as trajectories get longer. The authors aim to design a memory mechanism that can distinguish which past information is actually relevant to the current latent goal and interaction context.", "method": "The authors propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that, at each trajectory step, creates a structured retrieval cue encoding a contextual intent. This intent includes: (1) the current latent goal that defines a thematic segment of the interaction, (2) the type of action being performed, and (3) the salient entity types that determine which attributes are important. During inference, STITCH retrieves historical snippets by comparing the current step\u2019s intent to stored intents, filtering and prioritizing memories that are intent-compatible while suppressing those that are semantically similar but contextually incompatible. They also introduce CAME-Bench, a benchmark of realistic, dynamic, goal-oriented trajectories designed to test context-aware retrieval in long-horizon settings.", "result": "On CAME-Bench and the existing LongMemEval benchmark, STITCH sets a new state of the art for context-aware retrieval. It outperforms the strongest baseline by 35.6%, with particularly large improvements as the trajectory length increases, indicating that intent-based indexing scales well with interaction length. Empirical analyses show that STITCH significantly reduces retrieval noise compared to baselines.", "conclusion": "Encoding and using contextual intent\u2014latent goal, action type, and salient entity types\u2014as a structured index for memory substantially improves retrieval quality in long-horizon, goal-oriented LLM interactions. Intent-aware memory, as implemented in STITCH, mitigates interference from repeated entities and semantically similar contexts, resulting in more robust long-horizon reasoning. The results suggest that future agentic systems should explicitly model and track intent to manage memory and retrieval effectively."}}
{"id": "2601.10712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10712", "abs": "https://arxiv.org/abs/2601.10712", "authors": ["Changle Qu", "Sunhao Dai", "Hengyi Cai", "Jun Xu", "Shuaiqiang Wang", "Dawei Yin"], "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching", "comment": null, "summary": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.", "AI": {"tldr": "MatchTIR is a reinforcement learning framework for tool-integrated reasoning that assigns fine-grained rewards to each tool-use turn via bipartite matching and dual-level advantage estimation, improving long-horizon, multi-turn performance of LLMs.", "motivation": "Existing RL methods for tool-integrated reasoning give the same reward to all steps in a trajectory, making it hard to distinguish helpful tool calls from redundant or wrong ones, especially in long, multi-turn interactions. This coarse credit assignment limits learning efficiency and model performance on complex tasks where precise tool use is crucial.", "method": "They design MatchTIR, which treats credit assignment as a bipartite matching problem between predicted interaction traces and ground-truth traces. By matching predicted turns to ground-truth turns using two assignment strategies, they derive dense, turn-level rewards that reflect the quality of each tool call. On top of this, they introduce a dual-level advantage estimation scheme that combines these turn-level rewards with traditional trajectory-level signals, so each interaction turn receives a distinct advantage value that balances local correctness with global task success.", "result": "Across three benchmarks for tool-integrated reasoning, MatchTIR consistently outperforms existing methods. A 4B-parameter model trained with MatchTIR surpasses most 8B-parameter baselines, with especially strong gains on long-horizon and multi-turn tasks, indicating more efficient and precise tool-use learning.", "conclusion": "Fine-grained, turn-level credit assignment via bipartite matching, coupled with dual-level advantage estimation, leads to significantly better reinforcement learning for tool-integrated reasoning. MatchTIR enables smaller models to rival or exceed larger ones on complex multi-step tasks, demonstrating that better supervision of tool-use steps can be more impactful than simply scaling model size."}}
