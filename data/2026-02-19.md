<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Language Model Representations for Efficient Few-Shot Tabular Classification](https://arxiv.org/abs/2602.15844)
*Inwon Kang,Parikshit Ram,Yi Zhou,Horst Samulowitz,Oshani Seneviratne*

Main category: cs.CL

TL;DR: This paper proposes TaRL, a lightweight method that uses LLM-derived embeddings of table rows for few-shot tabular classification, enabling reuse of existing LLM infrastructure for web table understanding.


<details>
  <summary>Details</summary>
Motivation: The web contains many heterogeneous structured tables (product catalogs, knowledge bases, scientific data), and building specialized models for each setting is costly. Since LLMs are already deployed for tasks like semantic search, the authors want to know if these existing LLMs can be repurposed to classify rows in web-native tables without training new tabular models or performing extensive retraining.

Method: They introduce TaRL (Table Representation with Language Model), which represents each table row using semantic embeddings from an LLM. They first show that a naive use of these embeddings is inferior to dedicated tabular models. To improve performance, they apply (1) common-component removal from all embeddings to denoise and enhance discriminative information, and (2) softmax temperature calibration when doing few-shot classification. Moreover, they train a simple meta-learner on handcrafted features to automatically predict an appropriate temperature for each task.

Result: With these two key techniques plus the temperature-predicting meta-learner, TaRL achieves performance on par with state-of-the-art specialized tabular models in low-data few-shot regimes (k ≤ 32) for semantically rich web tables. The experiments show that, contrary to initial naive baselines, properly processed LLM row embeddings can be very competitive for tabular classification.

Conclusion: The paper concludes that existing LLM infrastructure can be effectively reused for web table understanding via TaRL, avoiding the need for bespoke tabular architectures in low-data settings. Careful embedding post-processing and temperature calibration are crucial to unlocking the potential of LLM-derived table row representations for semantics-driven few-shot tabular classification.

Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.

</details>


### [2] [KD4MT: A Survey of Knowledge Distillation for Machine Translation](https://arxiv.org/abs/2602.15845)
*Ona de Gibert,Joseph Attieh,Timothee Mickus,Yves Scherrer,Jörg Tiedemann*

Main category: cs.CL

TL;DR: Survey of knowledge distillation methods specifically for machine translation, covering 105 papers up to Oct 2025, organizing methods, applications, trends, gaps, risks, and the impact of LLMs, plus a public database and glossary.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation research is broad but scattered, and its role in machine translation is richer than mere model compression. There is no unified, systematic overview of how KD is used in MT, what methods exist, how they are evaluated, and what risks they carry. Practitioners lack guidance on which KD variants to use for different MT scenarios, and researchers lack a consolidated view of open problems, especially in the context of rapidly evolving large language models.

Method: Conduct a systematic literature survey of 105 papers on knowledge distillation for machine translation (up to Oct 1, 2025). Introduce core concepts in MT and KD for non-experts, then review standard KD approaches applicable to MT. Classify the surveyed work along two dimensions: (i) methodological innovations (e.g., new loss formulations, architectures, training protocols) and (ii) practical application scenarios (e.g., compression, domain adaptation, multilingual MT). Perform both qualitative and quantitative analyses to identify trends, gaps, and evaluation inconsistencies. Derive practical guidelines and risk analyses from this synthesis. Provide a companion database summarizing method characteristics and a glossary of terminology.

Result: A structured taxonomy of KD methods used in MT, mapped to their methodological features and practical use cases; an empirical and conceptual picture of dominant trends (e.g., which KD variants are most common, what model scales and language settings are studied); identification of major gaps such as lack of standardized evaluation protocols and limited reporting on side effects like hallucinations and bias. The survey consolidates scattered findings into clear guidelines for method selection and documents where evidence is weak or missing. It also situates the emerging role of LLMs as teachers or students within KD for MT.

Conclusion: Knowledge distillation in machine translation is not only a compression technique but a versatile mechanism for shaping supervision and improving translation quality and efficiency. However, the field suffers from fragmented methodologies and non-unified evaluation practices, making it hard to compare methods or derive firm conclusions. The survey provides an organized map of existing approaches, highlights open challenges—including risks such as hallucination and bias amplification—and argues that the rise of LLMs will reconfigure KD4MT research. The released database and glossary are intended as infrastructure to support more systematic, transparent, and comparable future work.

Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency.
  This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.

</details>


### [3] [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846)
*Xinyu Gao,Shaonan Wang,Nai Ding*

Main category: cs.CL

TL;DR: They add a syntactic-structure-aware attention branch (GTCA) to decoder-only LLMs that reads constituency parses, improving robustness to grammar perturbations without hurting QA/commonsense performance.


<details>
  <summary>Details</summary>
Motivation: Decoder-only LLMs perform well but are fragile to small grammatical changes in input; naive ways of adding syntax can damage pretrained abilities. The authors want a method to enhance syntactic robustness while staying compatible with existing checkpoints.

Method: They design a gated tree cross-attention (GTCA) branch that attends over an external memory of constituency chunks, added alongside the original Transformer without modifying its backbone. A token update mask and staged training schedule restrict when and where the syntax-informed updates affect representations, so the pretrained knowledge is preserved. They train and evaluate this branch on multiple Transformer backbones and benchmarks, comparing to continued-training baselines.

Result: GTCA-equipped models become more robust to syntactic and grammatical perturbations across benchmarks than models simply further trained, while maintaining performance on multiple-choice QA and commonsense reasoning tasks.

Conclusion: A plug-in GTCA branch that leverages precomputed syntactic structure can be attached to decoder-only LLM checkpoints to significantly improve syntactic robustness, with minimal architectural disruption and no loss in standard reasoning benchmarks.

Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.

</details>


### [4] [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models](https://arxiv.org/abs/2602.15847)
*Pranav Bhandari,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: The paper analyzes whether personality traits in LLMs can be controlled independently via steering vectors, finding substantial geometric and behavioral coupling between traits.


<details>
  <summary>Details</summary>
Motivation: Investigate the common but untested assumption that Big Five personality traits can be independently controlled in LLMs using trait-specific steering vectors, and understand the geometric relationships underlying such control.

Method: Extract personality steering vectors for Big Five traits from two 8B-parameter model families (LLaMA-3-8B and Mistral-8B) and analyze their geometric relationships under different conditioning schemes, including unconstrained, soft orthonormalisation, and hard orthonormalisation. Measure how steering along one trait affects others at both geometric and behavioral levels.

Result: Personality steering directions for different Big Five traits are substantially geometrically dependent. Steering one trait systematically influences others, and this persists even when linear overlap is removed via soft or hard orthonormalisation. Hard orthonormalisation yields geometrically independent directions but does not remove behavioral cross-trait effects and weakens the strength of steering.

Conclusion: Personality traits in LLMs do not form fully independent controllable dimensions; instead, they inhabit a slightly coupled subspace where attempts to enforce geometric independence come at the cost of weaker control and do not fully prevent behavioral spillover across traits. This challenges the assumption of fully independent personality control in LLM steering approaches.

Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.

</details>


### [5] [Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling](https://arxiv.org/abs/2602.15848)
*Andrius Matšenas,Anet Lello,Tõnis Lees,Hans Peep,Kim Lilii Tamm*

Main category: cs.CL

TL;DR: The paper evaluates whether LLM-based conversational assessments can validly estimate Big Five personality traits compared to a standard questionnaire.


<details>
  <summary>Details</summary>
Motivation: Traditional personality assessment relies on static self-report questionnaires, which can be time-consuming, inflexible, and potentially less engaging; LLMs might provide a more dynamic, conversational alternative, but their psychometric validity is unclear.

Method: A within-subjects design with 33 participants was used. Each participant completed both guided conversations with an LLM to infer their Big Five traits and the IPIP-50 questionnaire. Correlations between methods (convergent validity), statistical equivalence of mean scores, and user-perceived accuracy of profiles were analyzed.

Result: LLM-derived Big Five scores showed moderate convergent validity with IPIP-50 (r = 0.38–0.58). Conscientiousness, Openness, and Neuroticism scores were statistically equivalent between the two methods, while Agreeableness and Extraversion differed significantly. Participants judged LLM-generated personality profiles to be as accurate as profiles based on the questionnaire.

Conclusion: LLM-based conversational assessment can approximate traditional questionnaire-based Big Five scores for some traits and is perceived by users as equally accurate, although trait-specific adjustments are needed, particularly for Agreeableness and Extraversion. This supports conversational AI as a promising, but not yet fully refined, tool for personality assessment.

Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.

</details>


### [6] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: They build a reward model and a question-generation model to create better, expert-like peer-review questions and show this also boosts general reasoning and writing abilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM systems for peer-review question generation mostly ask shallow questions and overfit to the first page of papers, failing to capture expert-level, evidence-based reviewing needs; there is a need for automatic evaluation and training signals that better reflect human reviewers’ preferences for effortful, grounded, and evidence-based questions.

Method: They design IntelliReward, a reward model built on a frozen autoregressive LLM augmented with trainable multi-head transformers over the last 50 token states, trained to predict expert preference over review questions. Then, using Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO), they train IntelliAsk, a question-generation model whose outputs are optimized according to IntelliReward to align with expert notions of effort, evidence, and grounding. They evaluate IntelliAsk against the base Qwen3-32B model on various reasoning and writing benchmarks.

Result: IntelliReward outperforms API-based supervised fine-tuning (SFT) baselines at modeling expert human preferences. IntelliAsk, trained with this reward and DAPO, achieves consistent gains over the Qwen3-32B base model on multiple benchmarks, including improved accuracy on the MuSR reasoning task (68.3 vs 64.7) and better scores on the WritingBench complex writing evaluation (8.31 vs 8.07).

Conclusion: Reward modeling tailored to expert review-question preferences can produce LLMs that generate higher-quality, more grounded and effortful questions, and this targeted improvement in reviewer-question quality appears to correlate with broader gains in reasoning and writing capabilities. The released models, data, and implementation also serve as an automatic benchmark for assessing grounding, effort, and evidence in review-question generation.

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [7] [Large Language Models for Assisting American College Applications](https://arxiv.org/abs/2602.15850)
*Zhengliang Liu,Weihang You,Peng Shu,Junhao Chen,Yi Pan,Hanqi Jiang,Yiwei Li,Zhaojun Ding,Chao Cao,Xinliang Li,Yifan Zhou,Ruidong Zhang,Shaochen Xu,Wei Ruan,Huaqin Zhao,Dajiang Zhu,Tianming Liu*

Main category: cs.CL

TL;DR: EZCollegeApp is an LLM-powered assistant that helps students complete fragmented U.S. college applications by structuring forms, grounding answers in official documents, and keeping humans in control.


<details>
  <summary>Details</summary>
Motivation: College applications in the U.S. are complex, inconsistent across portals, and full of repetitive, conditional, and ambiguous questions, making the process difficult and error-prone for students. There is a need for an intelligent assistant that can standardize understanding of forms, reduce cognitive load, and help students answer questions accurately using authoritative information.

Method: The authors build EZCollegeApp, an LLM-based system that follows a mapping-first paradigm: it first maps and structures application forms, then separately generates suggested answers. The system ingests documents from official admissions websites, uses retrieval-augmented question answering to ground suggestions, and exposes a human-in-the-loop chatbot interface that surfaces suggestions next to form fields without auto-submitting. They detail the architecture, data pipeline, internal representations, and security/privacy safeguards, and evaluate via automated tests and human quality assessments.

Result: The system successfully structures heterogeneous application portals, provides grounded answer suggestions tied to admissions documents, and supports applicants through a controlled chatbot UI. Automated testing and human evaluations indicate that EZCollegeApp’s outputs are of useful quality, and the full open-source release demonstrates that the approach is implementable in practice.

Conclusion: A mapping-first, LLM-powered approach can meaningfully support students with complex, fragmented college applications while preserving human control and grounding responses in official admissions information. The open-source release of EZCollegeApp enables others to build upon this architecture to improve and scale application assistance tools.

Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.

</details>


### [8] [Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey](https://arxiv.org/abs/2602.15851)
*David Y. Liu,Aditya Joshi,Paul Dawson*

Main category: cs.CL

TL;DR: Survey on how NLP and LLM research engage with narrative theory and narratology, proposing a taxonomy, reviewing methods, and outlining future directions.


<details>
  <summary>Details</summary>
Motivation: Narrative-focused applications of LLMs are rapidly growing, but existing work is fragmented and not well-aligned with established narrative theory, making it difficult to compare methods or build cumulative knowledge.

Method: Conduct a survey of NLP work on narrative datasets, tasks, and applications; map these works to concepts and distinctions in narratology; analyze methodological trends such as prompting and fine-tuning; and derive a taxonomy and research agenda from these patterns.

Result: Identifies patterns in how narrative theories are used in NLP, proposes a taxonomy organizing narrative-related NLP tasks and datasets, shows that LLMs facilitate linking NLP pipelines to abstract narrative concepts, and documents challenges in defining unified narrative benchmarks or quality measures.

Conclusion: Rather than aiming for a single general benchmark for narrative quality, progress should focus on theory-based metrics for specific narrative attributes, large-scale theory-driven analyses, and experiments that use LLM outputs to validate and refine narrative theories, providing a more systematic and theoretically grounded foundation for narrative research in NLP.

Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.

</details>


### [9] [Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints](https://arxiv.org/abs/2602.15852)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.CL

TL;DR: The paper studies how to design safe, deployment-ready clinical NLP models by auditing and reducing temporal and lexical leakage that can inflate performance and harm real-world use.


<details>
  <summary>Details</summary>
Motivation: Clinical NLP models for tasks like discharge prediction often look very accurate because notes contain hints or direct statements of future decisions (temporal and lexical leakage). This creates unsafe overconfidence and temporally invalid predictions in real practice, where such hints are not yet written or are unavailable. There is a need for systematic, practical methods to detect and mitigate this leakage during model development so that models are safe and trustworthy for deployment.

Method: The authors propose a lightweight auditing pipeline that is integrated into the clinical NLP model development cycle. The pipeline uses interpretability methods to inspect what features and textual cues the model relies on, specifically identifying discharge-related or otherwise leakage-prone phrases that encode future decisions. They then suppress or control these signals before final model training. They study this in the context of predicting next-day discharge for elective spine surgery patients and compare models trained with and without this auditing process.

Result: Audited models become less reliant on discharge-related lexical cues and display more conservative, better-calibrated probability estimates. Although raw predictive performance metrics may be less inflated, the models behave more safely and realistically for deployment, with improved calibration and reduced temporal leakage risk.

Conclusion: Clinical NLP systems aimed for deployment should be designed around temporal validity, good calibration, and robust behavior rather than chasing maximal apparent performance. Integrating a lightweight interpretability-based auditing pipeline into development can effectively detect and mitigate temporal and lexical leakage, leading to safer and more trustworthy models for real-world clinical use.

Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.

</details>


### [10] [A Lightweight Explainable Guardrail for Prompt Safety](https://arxiv.org/abs/2602.15853)
*Md Asiful Islam,Mihai Surdeanu*

Main category: cs.CL

TL;DR: LEG is a compact, explainable safety guardrail model for classifying unsafe prompts and highlighting the words that make them safe or unsafe.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety guardrails are often large, opaque, and lack faithful explanations of why a prompt is deemed safe or unsafe. There is a need for lightweight models that can both accurately detect unsafe prompts and provide word-level explanations, while avoiding explanation biases inherent in LLM-generated rationales.

Method: LEG uses a multi-task learning architecture that jointly trains a prompt-level safety classifier and a word-level explanation classifier. Explanations are trained on synthetically generated supervision that is created with a novel strategy designed to reduce LLM confirmation bias. The training objective combines a new loss that aggregates global explanation signals with cross-entropy and focal losses, weighted by an uncertainty-based scheme, to balance classification and explanation performance.

Result: Across three datasets and both in-domain and out-of-domain evaluations, LEG matches or outperforms state-of-the-art methods on prompt safety classification and explanation quality, while using a substantially smaller model than competing approaches.

Conclusion: A small, multi-task guardrail model can deliver state-of-the-art safety classification and high-quality explanations when trained with carefully designed synthetic explanation data and a tailored loss function, making it practical and effective for deployment. The authors plan to release code, models, and annotated data upon acceptance.

Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.

</details>


### [11] [Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization](https://arxiv.org/abs/2602.15854)
*Jingyi Xu,Xingyu Ren,Zhiqiang You,Yumeng Zhang,Zhoupeng Shou*

Main category: cs.CL

TL;DR: The paper proposes Goal-Oriented Preference Optimization (GOPO), a hierarchical RL framework for task-oriented dialogue that separates strategy planning from response generation to better align with long-horizon task success.


<details>
  <summary>Details</summary>
Motivation: Existing training methods for task-oriented dialogue systems, like token-level likelihood and conventional preference optimization, do not align well with long-horizon task success, especially in commercial, multi-turn settings such as e-commerce customer service. There is a need for training methods that optimize dialogue trajectories toward explicit task goals rather than local token choices.

Method: The authors introduce GOPO, a hierarchical reinforcement learning framework with two agents: an Expert Agent and a Customer Service Agent. The Expert Agent operates at the dialogue-trajectory level, optimizing multi-turn goal preferences and selecting high-level strategies for achieving task success. The Customer Service Agent then generates natural language responses that strictly adhere to the Expert Agent's chosen strategy. They also propose a new sequence-level evaluation metric, Task-focused Sequential Engagement (TSE), derived from real e-commerce interaction data to better capture task completion and user engagement over a dialogue sequence. GOPO is trained and evaluated on public task-oriented dialogue benchmarks and proprietary e-commerce customer service datasets using RL-based preference optimization at the trajectory level.

Result: On the Mgshop dataset, GOPO achieves a 7.7% and 10.3% improvement in TSE compared to PPO and Memento, respectively, and shows consistent gains in sequence-level reward and response quality. A 14B model trained with GOPO surpasses larger models (Qwen-235B and GPT-5.2) by 2.7% and 1.5% in TSE, respectively. Ablation studies show that the Expert Agent is crucial for effective long-horizon optimization. GOPO also yields consistent improvements across additional datasets, suggesting robust benefits of the approach.

Conclusion: GOPO establishes a new hierarchical RL paradigm for training task-oriented dialogue systems that better aligns with long-horizon task goals, particularly in commercial customer service scenarios. By decoupling strategy planning from surface-form generation and introducing the TSE metric for sequence-level evaluation, GOPO delivers superior task performance over existing methods and even outperforms substantially larger models when equipped with this training framework. The authors plan to release code and datasets, potentially enabling broader adoption and further research.

Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.

</details>


### [12] [Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective](https://arxiv.org/abs/2602.15856)
*Yunhao Liu,Zian Jia,Xinyu Gao,Kanjun Xu,Yun Xiong*

Main category: cs.CL

TL;DR: SeleCom is a selector-based, query‑conditioned soft compression method for RAG that avoids full document compression, improving accuracy and efficiency versus existing compressed and non-compressed RAG.


<details>
  <summary>Details</summary>
Motivation: RAG systems suffer from long contexts and redundant retrieval, which leads to scalability and latency issues. Existing soft compression methods encode entire documents into compact embeddings using auto-encoder-style full compression, but this hurts performance because they try to preserve all information, including content irrelevant to the query, and conflicts with how LLMs actually generate answers. The authors want a compression mechanism that is both scalable and preserves task-relevant information for downstream generation.

Method: They analyze why full soft-compression is problematic, identifying two key issues: (1) infeasibility—full compression conflicts with LLM generation behavior; and (2) non-necessity—compressing everything dilutes task-relevant signal. Based on this, they propose SeleCom, a selector-based soft compression framework in which the encoder becomes a query-conditioned information selector instead of a full compressor. SeleCom uses a decoder-only selector model trained with a large, diverse, and difficulty-graded synthetic QA corpus, following a curriculum learning scheme so the model gradually learns to select increasingly complex, relevant information for a given query.

Result: Experiments across various RAG benchmarks show that SeleCom substantially outperforms prior soft compression methods and matches or exceeds standard, non-compressed RAG systems. At the same time, it reduces computation and latency by roughly 34% to 85%, demonstrating that query-conditioned selection can provide both higher quality and efficiency compared with full compression approaches.

Conclusion: Selector-based, query-conditioned soft compression is a more practical and effective paradigm for scalable RAG than auto-encoder-style full compression. By aligning compression with downstream generation needs and focusing on task-relevant information, SeleCom achieves better accuracy while significantly lowering computational cost and latency, suggesting a promising direction for future RAG system design.

Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query.
  In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning.
  Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.

</details>


### [13] [Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach](https://arxiv.org/abs/2602.15857)
*Yi Liu*

Main category: cs.CL

TL;DR: Proposes a CRAF framework to jointly analyze public opinion from multiple heterogeneous platforms using collaborative reasoning and adaptive fusion between traditional features and LLMs, achieving better generalization and performance in topic and sentiment tasks.


<details>
  <summary>Details</summary>
Motivation: Analyze public opinion reliably across many different online platforms is hard because data structures, semantics, and platform biases differ, especially when incorporating new multimodal short-video sources.

Method: Designs a Collaborative Reasoning and Adaptive Fusion (CRAF) framework with: (1) cross-platform collaborative attention to align semantics while preserving source-specific traits; (2) hierarchical adaptive fusion to weight features by data quality and task needs; (3) joint optimization of topic representations and sentiment distributions via shared latent spaces; and (4) multimodal extraction for video (OCR, ASR, visual sentiment). Combines feature-based models and LLMs via multi-stage reasoning, with theoretical generalization analysis.

Result: On three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8), CRAF achieves ARI 0.76 for topic clustering (4.1% over best baseline) and F1 0.84 for sentiment (3.8% over best baseline), plus a 75% reduction in labeled data required to adapt to new platforms. Provides a theoretical reduction in generalization error bound of order O(sqrt(d log K / m)) compared with independent source modeling.

Conclusion: CRAF effectively integrates heterogeneous multi-platform and multimodal data, improving topic and sentiment analysis performance and sample efficiency for new platforms, and enjoys theoretically better generalization than modeling each source independently.

Abstract: The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.

</details>


### [14] [From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants](https://arxiv.org/abs/2602.15859)
*Krittin Pachtrachai,Petmongkon Pornpichitsuwan,Wachiravit Modecrua,Touchapon Kraisingkorn*

Main category: cs.CL

TL;DR: Framework to build and evaluate reliable conversational AI assistants directly from historical call transcripts using filtered high-quality data, LLM-based knowledge extraction, RAG, governed prompting, and simulation-based evaluation.


<details>
  <summary>Details</summary>
Motivation: Customer-facing domains struggle to safely automate conversations because call data is noisy, knowledge is fragmented and fast-changing, and incorrect or late human hand-off is risky. There is a need for a principled, end-to-end method to turn messy historical transcripts into a reliable, controllable assistant that can operate even in real-time, high-stakes domains.

Method: 1) Grade and filter historical transcripts using a simplified PIPA-inspired rubric emphasizing observation alignment and correct agent behavior; 2) Use LLMs to extract structured domain knowledge solely from the curated transcripts; 3) Use this extracted knowledge as the only grounding source in a RAG pipeline for the assistant; 4) Design assistant prompts via a systematic progression from large monolithic prompts to modular, governed prompts that encode policies, behaviors, and safety constraints; 5) Evaluate with a transcript-grounded user simulator that replays realistic calls to measure coverage, accuracy, and escalation, supplemented by adversarial red teaming (prompt injection, out-of-scope, out-of-context).

Result: In two difficult, real-time-data-heavy domains (Real Estate and Specialist Recruitment), the assistant can autonomously manage ~30% of calls, attains nearly perfect factual correctness and rejection of unsupported queries, and shows strong resistance to adversarial and safety attacks during red teaming.

Conclusion: Carefully curated transcripts, LLM-based knowledge extraction, governed prompting, and simulation-driven evaluation together enable building reliable domain assistants even in challenging, dynamic settings, achieving meaningful automation while maintaining factual accuracy, safe rejection, and robustness.

Abstract: Building reliable conversational AI assistants for customer-facing industries remains challenging due to noisy conversational data, fragmented knowledge, and the requirement for accurate human hand-off - particularly in domains that depend heavily on real-time information. This paper presents an end-to-end framework for constructing and evaluating a conversational AI assistant directly from historical call transcripts. Incoming transcripts are first graded using a simplified adaptation of the PIPA framework, focusing on observation alignment and appropriate response behavior, and are filtered to retain only high-quality interactions exhibiting coherent flow and effective human agent responses. Structured knowledge is then extracted from curated transcripts using large language models (LLMs) and deployed as the sole grounding source in a Retrieval-Augmented Generation (RAG) pipeline. Assistant behavior is governed through systematic prompt tuning, progressing from monolithic prompts to lean, modular, and governed designs that ensure consistency, safety, and controllable execution. Evaluation is conducted using a transcript-grounded user simulator, enabling quantitative measurement of call coverage, factual accuracy, and human escalation behavior. Additional red teaming assesses robustness against prompt injection, out-of-scope, and out-of-context attacks. Experiments are conducted in the Real Estate and Specialist Recruitment domains, which are intentionally challenging and currently suboptimal for automation due to their reliance on real-time data. Despite these constraints, the assistant autonomously handles approximately 30 percents of calls, achieves near-perfect factual accuracy and rejection behavior, and demonstrates strong robustness under adversarial testing.

</details>


### [15] [Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860)
*Wen G. Gong*

Main category: cs.CL

TL;DR: The paper introduces Maniscope, a fast geometric reranker for RAG that uses manifold-based geodesic distances on k-NN graphs to approach cross-encoder accuracy at a fraction of the latency and compute cost.


<details>
  <summary>Details</summary>
Motivation: Neural reranking in retrieval-augmented generation typically uses cross-encoders or LLMs, which are accurate but slow and computationally expensive, leading to 3–5 second latencies per query that are impractical for real-time systems. There is a need for a reranking method that is both fast and accurate, improving over vector index graph-based methods without incurring heavy neural inference costs.

Method: Maniscope constructs a k-nearest neighbor manifold over initially retrieved document candidates and computes geodesic distances on this manifold as a reranking score. It blends global cosine similarity with local manifold geometry to better capture semantic relationships than flat Euclidean metrics. The method’s computational complexity is O(ND + M^2D + Mk log k) with M much smaller than the full corpus size N, enabling efficient reranking in a few milliseconds.

Result: On eight BEIR datasets totaling 1,233 queries, Maniscope beats an HNSW graph-based baseline on three of the hardest datasets (NFCorpus, TREC-COVID, AorB) with NDCG@3 gains up to +7.0%, while being 3.2× faster (4.7 ms vs 14.8 ms). Relative to cross-encoder rerankers, Maniscope is within 2% accuracy at 10–45× lower latency. On TREC-COVID, an LLM-based reranker yields only +0.5% NDCG@3 over Maniscope but with 840× higher latency.

Conclusion: Maniscope provides an effective trade-off between speed and accuracy for RAG reranking by exploiting manifold geometry over retrieved candidates. It substantially reduces latency compared to neural rerankers while matching or approaching their accuracy, and outperforms graph-based baselines on difficult datasets. Its favorable complexity enables sub-10 ms latency, making it well-suited for real-time RAG systems, and it will be released as open-source software.

Abstract: Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.

</details>


### [16] [CAST: Achieving Stable LLM-based Text Analysis for Data Analytics](https://arxiv.org/abs/2602.15861)
*Jinxiang Xie,Zihao Li,Wei He,Rui Ding,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: The paper proposes CAST, a framework to improve the stability and consistency of LLM-generated summaries and tags for tabular data, using structured reasoning constraints and explicit intermediate thinking, and shows it boosts stability without hurting quality.


<details>
  <summary>Details</summary>
Motivation: When using LLMs to summarize or tag rows in tabular datasets, their outputs can vary unpredictably across runs or minor prompt changes. Data analytics workflows, however, require highly stable and consistent outputs for reliability, auditing, and downstream automation. Existing prompting methods do not adequately control the model’s internal reasoning, leading to instability. The authors aim to provide a principled way to make LLM outputs more stable while preserving or improving quality.

Method: The authors introduce CAST (Consistency via Algorithmic Prompting and Stable Thinking), which has two main components: (1) Algorithmic Prompting, where the model is guided through an explicit, procedural reasoning scaffold that specifies allowed reasoning transitions, and (2) Thinking-before-Speaking, where the model must generate explicit intermediate reasoning commitments before producing the final summary or tags. They also define two stability metrics tailored to the tasks: CAST-S for bulleted summarization and CAST-T for tagging, designed to capture how consistent the model’s outputs are across repeated runs or perturbations. The framework is evaluated on multiple public benchmarks and across several LLM backbones against various baselines.

Result: Across benchmarks and different LLMs, CAST consistently yields higher stability than all compared methods, with up to a 16.2% increase in Stability Score. At the same time, the framework maintains or even improves the quality of the generated summaries and tags, as measured by standard quality metrics. The new stability metrics (CAST-S and CAST-T) are shown to correlate well with human judgments of stability and consistency.

Conclusion: Constraining LLMs’ latent reasoning paths via algorithmic prompting and explicit intermediate thinking can substantially improve the stability of summarization and tagging for tabular data without sacrificing quality. CAST offers a general framework and accompanying metrics (CAST-S and CAST-T) that make LLM-based text analysis more reliable for data analytics settings that demand consistent, auditable output.

Abstract: Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.

</details>


### [17] [Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation](https://arxiv.org/abs/2602.15862)
*Guoshan Liu,Bin Zhu,Yian Li,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: They improve recipe generation from food images by explicitly predicting and validating actions and ingredients with a two-stage fine-tuning plus a semantic correction module, leading to more semantically accurate recipes.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models can generate recipes from images but often include wrong cooking actions or ingredients even though they score well on standard text metrics, so a method focused on semantic correctness is needed.

Method: They design a two-stage pipeline: (1) supervised fine-tuning using an Action-Reasoning dataset and ingredient corpus to learn accurate action and ingredient predictions as internal context; (2) reinforcement fine-tuning with frequency-aware rewards to better predict rare (long-tail) actions and generalize ingredients. On top of this, they add a Semantic Confidence Scoring and Rectification (SCSR) module that filters low-confidence predictions and corrects them before final instruction generation.

Result: On the Recipe1M benchmark, their framework achieves state-of-the-art performance and substantially improves semantic fidelity of generated recipes compared to prior methods, reducing semantically wrong actions and ingredients.

Conclusion: Explicitly modeling, validating, and correcting actions and ingredients as internal semantic context, trained via a combination of supervised and reinforcement fine-tuning with frequency-aware rewards, yields more semantically grounded recipes from food images and outperforms previous approaches on Recipe1M.

Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.

</details>


### [18] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: The paper shows that the main benefit of self-generated examples for LLM reasoning comes from the process of generating them (Integrated prompting), not from later reusing those examples (Decoupled prompting).


<details>
  <summary>Details</summary>
Motivation: Although self-generated few-shot examples are known to improve LLM reasoning, it is unclear why they help and under what conditions this technique is most effective. This lack of understanding makes it hard to design principled prompting strategies and may lead to inefficient or suboptimal use of self-generation.

Method: The authors compare three prompting strategies on reasoning-heavy tasks across five LLM architectures: (1) Zero-shot prompting with no examples, (2) Integrated prompting, where the model creates and solves problems within a single prompt, and (3) Decoupled prompting, where problems and solutions generated previously are used as in-context examples but the generative process is omitted. They systematically evaluate performance across tasks and models, and additionally perform attention analysis to study how attention patterns differ between Integrated and Decoupled prompting.

Result: Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting on reasoning-intensive tasks, while Decoupled prompting only slightly improves over Zero-shot. Attention analysis shows marked differences in attention patterns for Integrated vs. Decoupled prompting, indicating that models process information differently when engaged in problem creation vs. only consuming examples.

Conclusion: The performance gains from self-generation prompting primarily stem from the active process of creating problems and solutions within a unified context, not from the static self-generated examples themselves. This insight implies that prompting strategies should emphasize interactive, generative reasoning (e.g., integrated self-generation) rather than merely reusing generated examples as few-shot context.

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [19] [Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?](https://arxiv.org/abs/2602.15867)
*Berry Gerrits*

Main category: cs.CL

TL;DR: The paper evaluates how well current LLMs solve problems and reason within the classic text game Zork, finding they perform poorly and show key metacognitive limits.


<details>
  <summary>Details</summary>
Motivation: To obtain a controlled, interaction-rich benchmark that tests LLMs’ real-time problem-solving, planning, and reasoning in a setting that closely resembles natural language interaction but has clear success metrics (the game score).

Method: Use Zork as a testbed and run several leading proprietary LLMs (ChatGPT, Claude, Gemini) through the game with different prompting setups (minimal vs detailed instructions, with and without extended thinking). Measure quantitative performance via game scores and qualitatively inspect reasoning traces, action choices, and strategy persistence across attempts.

Result: All models achieve under 10% game completion; best model (Claude Opus 4.5) scores about 75/350 points. Extra instructions and extended thinking do not improve performance. Qualitative analysis uncovers behaviors like repeatedly issuing failed commands, changing strategies inconsistently, and not leveraging prior trajectory or conversation history.

Conclusion: Current LLMs show strong limitations in metacognition, learning from experience, and sustained problem-solving in interactive text environments like Zork, casting doubt on claims about their robust general reasoning abilities and highlighting the need for new architectures or training methods to handle such tasks.

Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.

</details>


### [20] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: The paper introduces a formal deterministic multi-tape Turing machine model of LLMs to precisely locate and understand their failure modes on simple tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail on trivial-seeming tasks like counting or basic reasoning, but existing explanations are vague (e.g., geometric intuitions in embedding space) or purely empirical, lacking rigorous, falsifiable structure for analyzing where and why errors occur in the LLM pipeline.

Method: Model an LLM as a deterministic multi-tape Turing machine, with separate tapes for input characters, tokens, vocabulary, model parameters, intermediate activations, probability distributions, and generated output text. Use this formal model to map different operations of the LLM (tokenisation, forward pass, decoding, etc.) to specific tapes and transitions, then analyze how particular tasks and prompts traverse this pipeline and where information is lost or distorted.

Result: The formalization allows one to attribute specific failure modes—such as inability to count or follow character-level constraints—to particular stages like tokenization or decoding, rather than to the entire model. It also shows that chain-of-thought and similar prompting methods work by offloading intermediate computation into the output tape, but exposes structural limits of what such techniques can fix when earlier stages (e.g., tokenization) have already destroyed needed information.

Conclusion: Viewing LLMs as deterministic multi-tape Turing machines provides a rigorous and falsifiable framework that complements empirical scaling laws, clarifies why some prompting tricks help and where they cannot, and replaces vague geometric metaphors with principled, stage-wise error analysis of LLM behavior.

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [21] [Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches](https://arxiv.org/abs/2602.15869)
*Noopur Zambare,Kiana Aghakasiri,Carissa Lin,Carrie Ye,J. Ross Mitchell,Mohamed Abdalla*

Main category: cs.CL

TL;DR: The paper evaluates how well different sizes and types of language models perform clinical de-identification across formats, cultures, and genders, showing that small models can match or beat larger ones while being cheaper and that multicultural fine-tuning improves robustness.


<details>
  <summary>Details</summary>
Motivation: Clinical notes contain sensitive personal identifiers that must be removed to protect patient privacy, but existing de-identification systems may not generalize well across different document formats, cultural name patterns, languages, and genders. With the rise of large language models, there is a need to understand whether bigger models are actually necessary, how well they transfer across cultural settings, and what the trade-offs are between performance, generalizability, and computational cost for real-world deployment.

Method: The authors systematically benchmark multiple model families and sizes on de-identification: fine-tuned transformer encoders (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B). They compare their performance and inference cost, and study how well models trained on primarily English clinical data generalize to identifiers from multiple languages (Mandarin, Hindi, Spanish, French, Bengali), regional English variants, and gendered names. They also fine-tune BERT-based models on the MIMIC dataset augmented with multicultural identifiers to create the BERT-MultiCulture-DEID models, and evaluate gains in robustness.

Result: Empirically, smaller models achieve de-identification performance comparable to or better than much larger LLMs, while requiring far less inference cost, making them more deployment-friendly. When fine-tuned with relatively limited additional data that injects multicultural and multilingual identifiers, the smaller models outperform larger models at recognizing identifiers from non-English and culturally diverse name distributions, as well as from different regional English variants and gendered names. The new BERT-MultiCulture-DEID models, trained on MIMIC with multi-language-variant identifiers, show improved robustness in multicultural scenarios.

Conclusion: Model size alone is not the key determinant of effective clinical de-identification. Carefully fine-tuned smaller models can deliver strong, and often superior, generalization across cultures, languages, and genders at a fraction of the computational cost of large LLMs. The released BERT-MultiCulture-DEID models provide practical tools for fairer, more efficient clinical de-identification in multicultural contexts, and the study offers the first systematic characterization of the efficiency–generalizability trade-off in this domain.

Abstract: Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification.
  Details on accessing the models are available at: https://doi.org/10.5281/zenodo.18342291

</details>


### [22] [VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering](https://arxiv.org/abs/2602.15870)
*Shuhui Qu*

Main category: cs.CL

TL;DR: They introduce VDLM, a diffusion-based language model that plans in a semantic embedding space and then renders to text, enabling iterative refinement and better long-form reasoning.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive LMs generate text left-to-right and make irreversible token-level commitments, which hinders their ability to revise intermediate steps in complex, multi-step reasoning tasks. There is a need for models that can plan and refine in a flexible latent space before committing to surface text, and that can be post-trained efficiently for better reasoning and long-form generation.

Method: They propose VDLM, a modular variable diffusion language model. First, a semantic planner operates in an embedding (latent) space using masked diffusion (LLaDA-style) over semantic variable embeddings, enabling iterative refinement of a latent plan. The planner is post-trained using trajectory-aware optimization with rewards and value functions defined directly in embedding space, so reinforcement learning does not require decoding text in the loop. Second, a Vec2Text renderer converts the final planned embeddings into text. To make this robust to noise from the planner, they introduce embedding perturbations during training of the renderer so it can reliably decode slightly off-manifold embeddings.

Result: On nine benchmarks covering general reasoning, mathematics, and code generation, VDLM matches strong baselines in the pre-training phase and, after post-training, shows substantial gains on long-form generation tasks. It outperforms competing methods that do not use this embedding-space diffusion planning and robust rendering setup.

Conclusion: Separating semantic planning from text generation via diffusion in embedding space, combined with embedding-space post-training and a robust Vec2Text renderer, is an effective approach for improving reasoning and long-form generation in language models. This architecture demonstrates the promise of diffusion-based planning and latent-to-text rendering for future language modeling systems.

Abstract: Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.

</details>


### [23] [CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content](https://arxiv.org/abs/2602.15871)
*Diletta Abbonato*

Main category: cs.CL

TL;DR: The paper introduces CheckIfExist, an open-source web tool that automatically verifies the authenticity of bibliographic references to combat AI-generated (hallucinated) citations.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in academic writing and can hallucinate plausible but fake references, which have already appeared in top conference papers. There is an urgent need for an accessible, automated way to check whether citations are real, as existing reference managers don’t validate authenticity and commercial services are paywalled or limited.

Method: The authors design CheckIfExist, a web-based system that queries multiple scholarly databases (CrossRef, Semantic Scholar, OpenAlex) using a cascading validation architecture. It applies string similarity algorithms to compare user-provided references to database records, computes multi-dimensional match confidence scores, and returns structured validation results. It supports both single-reference checks and batch BibTeX verification.

Result: The tool successfully provides near-instant verification of references, returning APA-formatted validated citations and exportable BibTeX records. It can process both individual references and batches of BibTeX entries via a unified interface, confirming authenticity or flagging likely hallucinations within seconds.

Conclusion: CheckIfExist effectively addresses reference hallucination by offering an open-source, real-time, multi-source reference verification tool that complements existing reference managers and avoids the access and cost barriers of commercial hallucination detection services.

Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.

</details>


### [24] [P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA](https://arxiv.org/abs/2602.15874)
*Xingda Lyu,Gongfu Lyu,Zitai Yan,Yuxin Jiang*

Main category: cs.CL

TL;DR: The paper introduces P-RAG, a Prompt-Enhanced Parametric RAG architecture combining LoRA-tuned LLaMA with retrieval and Chain-of-Thought prompting, achieving strong gains on biomedical and multi-hop QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but constrained by static training data and hallucinations. RAG mitigates this by adding retrieval, yet performance is still limited by the external knowledge base and underuse of the model’s own parametric knowledge and reasoning. The authors aim to design a more effective hybrid that better integrates retrieved evidence, parametric knowledge, and explicit reasoning for domains like biomedical QA and multi-hop question answering.

Method: They fine-tune LLaMA-3.2-1B-Instruct using Low-Rank Adaptation (LoRA) for biomedical question answering. They implement three systems: Standard RAG (retrieval + generation), DA-RAG (a RAG variant with some additional processing, not fully detailed in the abstract), and the proposed Prompt-Enhanced Parametric RAG (P-RAG). P-RAG integrates retrieved documents with the LLM’s parametric knowledge and is guided by Chain-of-Thought (CoT) prompting; LoRA-tuned parameters enable task adaptation. They evaluate on PubMedQA (biomedical, mostly single-hop) and 2WikiMultihopQA (multi-hop, with subsets Compare, Bridge, Inference, Compose), measuring F1 and other scores.

Result: On PubMedQA, P-RAG improves F1 by 10.47 absolute percentage points over Standard RAG (93.33% vs. 82.86%, a 12.64% relative gain). On 2WikiMultihopQA, P-RAG nearly doubles the overall score compared to Standard RAG (33.44% vs. 17.83%) and reaches 44.03% on the Compare subset, with 42.74% on Bridge, 21.84% on Inference, and 8.60% on Compose. Chain-of-Thought prompting significantly boosts multi-hop reasoning but is less consistently beneficial for simpler, single-hop questions.

Conclusion: P-RAG, which combines LoRA-tuned parametric knowledge, retrieval, and Chain-of-Thought prompting, delivers substantial performance gains in both biomedical QA and multi-hop reasoning benchmarks, achieving or approaching state-of-the-art results on PubMedQA and 2WikiMultihopQA. The results indicate that hybrid architectures that deeply integrate parametric and retrieved knowledge with explicit reasoning can yield more accurate, scalable, and context-adaptive question answering systems, especially in specialized domains like biomedicine.

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.

</details>


### [25] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: They propose QEMPO, a method to increase LLM output diversity without sacrificing quality by maximizing entropy under quality constraints, achieving diversity comparable or superior to RLHF.


<details>
  <summary>Details</summary>
Motivation: Alignment methods improve LLM output quality but reduce diversity; existing diversity-enhancing methods typically degrade performance. The paper aims to decouple and jointly optimize quality and diversity in LLM alignment.

Method: The authors theoretically decompose the alignment task into two distributions—quality and diversity—and formulate alignment as a constrained optimization problem. They introduce QEMPO (Quality-constrained Entropy Maximization Policy Optimization), which maximizes the entropy of the policy (diversity) subject to a quality constraint, and derive different policies by applying different constraints. They develop both online and offline training procedures for optimizing these policies.

Result: Empirical experiments show that QEMPO achieves output diversity improvements while maintaining or improving performance compared to RLHF, demonstrating that diversity and quality can be jointly optimized.

Conclusion: By separating quality and diversity in the alignment objective and optimizing entropy under explicit quality constraints, QEMPO offers a principled way to align LLMs that preserves or enhances quality while significantly improving output diversity over standard RLHF.

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [26] [Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion](https://arxiv.org/abs/2602.15895)
*Pengcheng Zhou,Haochen Li,Zhiqiang Nie,JiaLe Chen,Qing Gong,Weizhen Zhang,Chun Yu*

Main category: cs.CL

TL;DR: CogitoRAG is a RAG framework that mimics human episodic memory by extracting semantic gists, building a multi-dimensional knowledge graph, and using diffusion-based retrieval plus a new reranking algorithm to improve complex QA and reasoning.


<details>
  <summary>Details</summary>
Motivation: Standard RAG frameworks rely on discrete text chunks and vector similarity, which can break semantic coherence and cause retrieval errors, especially for complex, multi-hop queries. The authors want a retrieval mechanism that preserves semantic integrity, supports associative reasoning, and better mirrors how humans recall and integrate knowledge.

Method: 1) Offline: Convert raw unstructured text into "gist memory" representations and then into a multi-dimensional knowledge graph with entities, relations, and memory nodes. 2) Online: Decompose complex user queries into sub-queries via a Query Decomposition Module; run an Entity Diffusion Module over the graph for associative retrieval guided by graph structure and an entity-frequency reward; 3) Rerank candidate passages using CogniRank, which combines diffusion-based scores with semantic similarity; 4) Feed the generator with paired passage-memory evidence that is information-dense.

Result: On five mainstream QA benchmarks and a multi-task generation benchmark (GraphBench), CogitoRAG yields significantly better performance than state-of-the-art RAG baselines, particularly for tasks requiring complex knowledge integration and reasoning.

Conclusion: Modeling RAG retrieval after human cognitive and episodic memory—via semantic gist extraction, knowledge-graph-based diffusion, and hybrid reranking—reduces hallucinations and improves complex reasoning, making CogitoRAG a more effective RAG framework than existing methods on diverse QA and graph-based generation tasks.

Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.

</details>


### [27] [Every Little Helps: Building Knowledge Graph Foundation Model with Fine-grained Transferable Multi-modal Tokens](https://arxiv.org/abs/2602.15896)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Wen Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: The paper proposes TOFU, a token-based, foundation-style model for multi-modal knowledge graph reasoning that effectively generalizes across different MMKGs by discretizing structural, visual, and textual information into tokens and fusing them hierarchically.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal knowledge graph reasoning methods are largely transductive, learning dataset-specific embeddings that generalize poorly to new knowledge graphs. Recent foundation models for KGs transfer better but focus mainly on structural patterns while neglecting rich visual and textual modalities. There is a need for a foundation-style model that can leverage multi-modal signals and still generalize well across diverse MMKGs and evaluation regimes (transductive, inductive, fully-inductive).

Method: The authors introduce TOFU, a token-based foundation model for MMKGR. TOFU first discretizes different modalities—structural (graph-related), visual (images), and textual (descriptions)—into separate sets of modality-specific tokens. It then applies a hierarchical fusion architecture that combines these tokens across modalities. Within this architecture, a mixture-of-message mechanism is used to flexibly route and aggregate information from different modalities so as to learn transferable representations suitable for link prediction across various MMKGs.

Result: Across 17 benchmark MMKGs spanning transductive, inductive, and fully-inductive settings, TOFU consistently outperforms strong baselines from both lines of work: traditional MMKGR models and recent knowledge graph foundation models. The gains are especially notable on unseen MMKGs, showing improved cross-graph generalization.

Conclusion: Treating structural, visual, and textual signals as tokens within a unified, hierarchical fusion framework allows TOFU to function as a foundation-style model for MMKGR, delivering robust performance and strong generalization to unseen knowledge graphs. This suggests that tokenization plus mixture-of-message fusion is an effective way to incorporate multi-modal content into transferable KG reasoning models.

Abstract: Multi-modal knowledge graph reasoning (MMKGR) aims to predict the missing links by exploiting both graph structure information and multi-modal entity contents. Most existing works are designed for a transductive setting, which learns dataset-specific embeddings and struggles to generalize to new KGs. Recent knowledge graph foundation models (KGFMs) improve cross-KG transfer, but they mainly exploit structural patterns and ignore rich multi-modal signals. We address these gaps by proposing a token-based foundation model (TOFU) for MMKGR, which exhibits strong generalization across different MMKGs. TOFU discretizes structural, visual, and textual information into modality-specific tokens. TOFU then employs a hierarchical fusion architecture with mixture-of-message mechanisms, aiming to process these tokens and obtain transferable features for MMKGR. Experimental results on 17 transductive, inductive, and fully-inductive MMKGs show that TOFU consistently outperforms strong KGFM and MMKGR baselines, delivering strong performance on unseen MMKGs.

</details>


### [28] [Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation](https://arxiv.org/abs/2602.15897)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.CL

TL;DR: The paper proposes GHOST, a token-level obfuscation method that protects collaborative training of large language models from gradient inversion attacks by replacing original tokens with semantically different but embedding-similar 'shadow' tokens, significantly reducing data recovery while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Collaborative learning for large language models requires sharing gradients, which exposes private training data to gradient inversion attacks that can reconstruct original inputs. Existing defenses perturb gradients but still leak information because semantic similarity persists across gradient, embedding, and token spaces. A more robust defense is needed that breaks this semantic linkage without harming training utility.

Method: GHOST is a token-level obfuscation defense. It leverages the observation that, in the large token space, there are tokens that are close in embedding space but semantically different from the originals. GHOST has two stages: (1) a searching step that uses multiple criteria to find candidate tokens that are semantically distinct from the original tokens while being embedding-proximate; (2) a selection step that chooses the best 'shadow' tokens by ensuring that they minimally disrupt training-critical features, measured by preserving alignment with the internal model outputs that would be produced by the original tokens. Gradients are then computed and shared on these shadow tokens instead of the real tokens.

Result: Across different model architectures (e.g., BERT, Llama) and datasets, GHOST reduces the success of gradient inversion attacks drastically, achieving as low as 1% recovery rate of private data, while maintaining high task performance: classification F1 up to 0.92 and generation quality with perplexity around 5.45. The method is evaluated against state-of-the-art GIAs and adaptive attackers designed specifically to counter the defense.

Conclusion: GHOST effectively neutralizes gradient inversion attacks in collaborative learning of large language models by decoupling semantic information in token space from the embedding and gradient spaces through strategically chosen shadow tokens. It offers strong privacy protection with minimal impact on downstream utility across both classification and generation tasks, and remains robust even under adaptive attack scenarios.

Abstract: Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.

</details>


### [29] [MultiCube-RAG for Multi-hop Question Answering](https://arxiv.org/abs/2602.15898)
*Jimeng Shi,Wei Hu,Runchu Tian,Bowen Jin,Wonbin Kweon,SeongKu Kang,Yunfan Kang,Dingqi Ye,Sizhe Zhou,Shaowen Wang,Jiawei Han*

Main category: cs.CL

TL;DR: The paper proposes MultiCube-RAG, a training-free, ontology-based multi-dimensional 'cube' structure for multi-hop question answering that improves retrieval and reasoning efficiency and accuracy over existing RAG and graph-based methods.


<details>
  <summary>Details</summary>
Motivation: Multi-hop QA tasks require reasoning over multiple interconnected facts (subjects, attributes, relations), which standard RAG systems and noisy/expensive graph-based RAGs struggle with. Single-step retrieval and unstable, costly training-based iterative methods further limit performance. The motivation is to create a more precise, efficient, and explainable structure for multi-step retrieval and reasoning without heavy training.

Method: The authors design an ontology-based multi-dimensional cube representation where orthogonal dimensions encode structural aspects such as subjects, attributes, and relations. They instantiate multiple such cubes, each specialized to a class of subjects, forming MultiCube-RAG. Given a complex multi-hop query, the system decomposes it into simpler subqueries aligned with cube dimensions and sequentially resolves them, selecting appropriate cubes for each step. The entire pipeline is training-free and focuses on structured, stepwise retrieval and reasoning.

Result: On four standard multi-hop QA datasets, MultiCube-RAG achieves an average accuracy improvement of 8.9% compared to a range of baseline systems, including existing RAG and graph-based approaches. The method also shows better computational efficiency and provides more inherently interpretable reasoning paths.

Conclusion: Ontology-based multi-dimensional cube structures can effectively model structural knowledge for multi-hop QA, enabling a training-free MultiCube-RAG framework that outperforms existing baselines in both accuracy and efficiency while offering clearer explainability for its multi-step reasoning and retrieval processes.

Abstract: Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.

</details>


### [30] [Doc-to-LoRA: Learning to Instantly Internalize Contexts](https://arxiv.org/abs/2602.15902)
*Rujikorn Charakorn,Edoardo Cetin,Shinnosuke Uesaka,Robert Tjarko Lange*

Main category: cs.CL

TL;DR: They propose Doc-to-LoRA (D2L), a hypernetwork that turns a long document into a LoRA adapter so an LLM can answer later questions without re-reading the whole context, cutting latency and memory.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic attention cost, making very long-context inference slow and memory-heavy. Context distillation can compress context into weights, but doing it per prompt is too expensive and slow. There is a need for a cheap, fast way to capture information from long prompts so that LLMs can use it later without re-consuming the full context, enabling long-document understanding, extended reasoning, and frequent knowledge or personalization updates.

Method: They design Doc-to-LoRA (D2L), a lightweight hypernetwork trained to approximate context distillation in a single forward pass. Given a long prompt/document, D2L outputs a LoRA adapter specific to that prompt for a target LLM. After this one-time adapter generation, subsequent queries are answered by the target LLM using the adapter, without re-feeding the original long context. The approach is evaluated on synthetic long-context needle-in-a-haystack tasks and real-world QA workloads, focusing on accuracy, memory, and latency compared to conventional context distillation.

Result: On a long-context needle-in-a-haystack benchmark, D2L learns to encode the key “needle” information into LoRA adapters and attains near-perfect zero-shot accuracy even when sequence length exceeds the target LLM’s native context window by over 4x. On real-world QA datasets with constrained compute, D2L surpasses standard context distillation in performance while substantially lowering peak memory usage and update latency during inference, due to avoiding repeated processing of long contexts and large KV caches.

Conclusion: D2L demonstrates that a meta-learned hypernetwork can effectively approximate per-prompt context distillation, turning documents into LoRA adapters that allow LLMs to answer later questions efficiently. This enables faster, more memory-efficient long-context usage and suggests a practical path toward rapid knowledge refreshing and personalization of LLM behavior without expensive retraining or repeated long-context inference.

Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.

</details>


### [31] [DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting](https://arxiv.org/abs/2602.15958)
*Md Mofijul Islam,Md Sirajus Salekin,Nivedha Balakrishnan,Vincil C. Bishop,Niharika Jain,Spencer Romo,Bob Strahan,Boyi Xie,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: Introduces DocSplit, a benchmark and datasets for splitting multi-page, heterogeneous document packets into individual documents, with new evaluation metrics and analysis of multimodal LLM performance.


<details>
  <summary>Details</summary>
Motivation: Real-world document workflows involve large, heterogeneous packets (e.g., legal, financial, healthcare) where multiple documents are concatenated, often with noisy properties like out-of-order or interleaved pages. Existing visual/document understanding research focuses on tasks like classification or QA but largely ignores the foundational problem of automatically splitting such packets into coherent documents, which is critical for downstream automation.

Method: The authors construct DocSplit, a benchmark suite composed of five datasets of varying complexity and modality, spanning diverse document types and layouts. They formally define the DocSplit task, which includes (1) detecting document boundaries within a packet, (2) classifying the type of each resulting document, and (3) verifying/maintaining correct page ordering. They also design novel evaluation metrics tailored to these subtasks and to real-world challenges such as out-of-order and interleaved pages or missing clear delimiters. They then evaluate multiple multimodal large language models on these datasets to assess current capabilities and limitations in document packet splitting.

Result: Experiments show that current multimodal LLMs exhibit significant shortcomings on complex document splitting scenarios in DocSplit, especially when pages are out of order, interleaved, or lack clear separation. The benchmark exposes substantial performance gaps across models and task variants, highlighting that document packet splitting is far from solved.

Conclusion: DocSplit provides the first systematic, comprehensive benchmark and metric suite for document packet splitting, revealing that existing multimodal LLMs struggle with realistic, complex packets. By releasing the datasets and metrics, the work lays a foundation for future research and model development to improve document understanding in document-heavy domains such as legal, financial, and healthcare.

Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.

</details>


### [32] [Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs](https://arxiv.org/abs/2602.16085)
*Sean Trott,Samuel Taylor,Cameron Jones,James A. Michaelov,Pamela D. Rivière*

Main category: cs.CL

TL;DR: The paper studies how well many open-weight language models can reason about others’ beliefs (false-belief tasks), compares them to humans, and uses the models’ behavior to generate and test new hypotheses about human cognition.


<details>
  <summary>Details</summary>
Motivation: Most prior work on language models’ mental state reasoning uses only a few closed-source systems, limiting the ability to test psychological theories and generalize findings. There is also active debate about whether human theory of mind partly emerges from language exposure; large language models trained purely on text are a useful testbed for this idea. The authors want a broad, systematic evaluation across many open-weight models and to see whether distributional information in language can explain specific human effects in mental-state reasoning.

Method: The authors evaluate 41 distinct open-weight language models from different families on variants of a classic false-belief task. They measure: (1) whether models are sensitive to implied knowledge states (who saw what, who knows what), (2) how this sensitivity scales with model size, and (3) how well model responses predict psychometric data from humans. They then manipulate how an agent’s mental state is cued—using a non-factive verb (e.g., “John thinks…”) versus an indirect cue (e.g., “John looks in the …”)—to test for a bias toward attributing false beliefs. They compare the size of these effects between humans and models.

Result: About 34% of the tested open-weight models show some sensitivity to implied knowledge states in false-belief scenarios, but none fully reproduce or “explain away” the full human effect. Larger models tend to show stronger sensitivity and better prediction of human behavioral data. Both humans and LMs more often attribute false beliefs when mental states are cued with non-factive verbs than when they are cued indirectly. For this “knowledge cue” effect, the human effect size falls in the middle of the distribution of model effect sizes, unlike the primary knowledge-state sensitivity effect where humans outperform models.

Conclusion: Open-weight language models, especially larger ones, show nontrivial but still limited capacity for mental state reasoning as measured by false-belief tasks. Some human-like biases in mental state attribution—such as sensitivity to non-factive mental-state verbs—can plausibly arise from distributional statistics in language alone, since LMs match humans on these effects. However, other aspects of human sensitivity to knowledge states remain stronger than in current LMs, suggesting additional mechanisms beyond text-based learning. Broad, multi-model evaluations using open-weight systems are valuable both for cognitive theory and for characterizing LM capabilities.

Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.

</details>


### [33] [A Curious Class of Adpositional Multiword Expressions in Korean](https://arxiv.org/abs/2602.16023)
*Junghyun Min,Na-Rae Han,Jena D. Hwang,Nathan Schneider*

Main category: cs.CL

TL;DR: The paper analyzes a specific class of Korean multiword adpositional expressions and proposes annotation guidelines to integrate them into cross-lingual frameworks like PARSEME.


<details>
  <summary>Details</summary>
Motivation: Korean multiword expressions, especially multiword adpositions, are underrepresented and lack systematic analysis and annotation in cross-lingual frameworks, which hinders multilingual NLP and comparability with other languages.

Method: The authors extract and study postpositional verb-based constructions (PVCs) from Korean Wikipedia, compare them with structurally similar non-MWEs and light verb constructions (LVCs), and based on this contrastive analysis, design annotation guidelines tailored to Korean multiword adpositions and compatible with frameworks like PARSEME.

Result: They provide a surveyed inventory and linguistic characterization of several Korean PVCs, highlight how they differ from non-MWEs and LVCs, and derive a concrete set of annotation principles and criteria for identifying and annotating these constructions.

Conclusion: Postpositional verb-based constructions form a coherent class of Korean functional MWEs that require dedicated treatment; the proposed guidelines fill an existing gap for Korean within multilingual MWE initiatives and will support future resource creation and cross-lingual alignment.

Abstract: Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.

</details>


### [34] [Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities](https://arxiv.org/abs/2602.16093)
*Shankar Padmanabhan,Mustafa Omer Gul,Tanya Goyal*

Main category: cs.CL

TL;DR: The paper proposes DiSC, a context-distillation method that continually adapts post-trained LLMs with new knowledge while reducing forgetting of prior skills, achieving a better stability–plasticity trade-off than existing finetuning and distillation approaches.


<details>
  <summary>Details</summary>
Motivation: Post-trained LLMs (e.g., instruction-tuned, reasoning-enhanced) quickly become outdated because their encoded knowledge has a fixed cutoff date. Continually adapting them to new corpora is necessary but challenging due to catastrophic forgetting of prior capabilities such as instruction-following, reasoning, and existing factual knowledge. Existing solutions for continual adaptation either fail to learn new information effectively or significantly degrade prior skills, so a new method is needed that balances learning new knowledge and preserving old capabilities efficiently.

Method: The authors introduce Distillation via Split Contexts (DiSC), a context-distillation based continual learning approach. For each training example, they construct a teacher and a student distribution by conditioning on different segments (splits) of the same context. The model is trained by minimizing the KL divergence between the token distributions in the overlapping/shared part of the context. This enables an efficient knowledge distillation process that does not require explicit autoregressive generation by a separate teacher model during training. DiSC is applied to post-trained LLMs on new domain corpora to inject new knowledge while regularizing against forgetting.

Result: Across four different post-trained LLMs and two adaptation domains, DiSC yields the best trade-off between acquiring new domain knowledge and preserving existing skills compared to previous finetuning and distillation baselines. Empirically, it better maintains instruction-following, reasoning, and prior factual knowledge while still effectively incorporating new information from the adaptation corpora.

Conclusion: DiSC demonstrates that a simple split-context distillation objective can serve as an effective continual adaptation mechanism for post-trained LLMs. By leveraging KL divergence between teacher and student distributions conditioned on different context segments, the method efficiently injects new knowledge while mitigating catastrophic forgetting. This yields superior stability–plasticity balance over standard finetuning and earlier distillation methods for updating LLMs beyond their pretraining knowledge cutoff.

Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.

</details>


### [35] [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054)
*Bradley McDanel,Steven Li,Harshit Khaitan*

Main category: cs.CL

TL;DR: They identify instability in token-ranking heuristics for accelerating long-context LLM inference, define an oracle for true token importance using answer-to-prompt attention, and propose cross-layer attention aggregation (CLAA) to achieve more stable rankings and significantly reduce prefill latency.


<details>
  <summary>Details</summary>
Motivation: Long-context LLMs are slowed down by the prefill phase, where all prompt tokens must be processed. Token-ranking heuristics try to skip unimportant tokens but their importance estimates vary unpredictably across layers, and it’s hard to fairly evaluate these heuristics without tying them to specific architectures. The authors want a principled way to define and measure token importance and to diagnose why existing heuristics underperform.

Method: They define an Answer-Informed Oracle: after generating an answer, they measure attention from answer tokens back to prompt tokens to define ground-truth token importance. Using this oracle, they analyze how token-ranking heuristics behave across layers and discover high variance and abrupt degradation at particular layers. Based on this diagnosis, they design Cross-Layer Attention Aggregation (CLAA), which aggregates token scores across layers instead of taking any single-layer ranking.

Result: The oracle analysis shows that existing heuristics’ rankings change significantly and sometimes collapse at specific layers, a failure mode not captured by standard end-to-end benchmarks. CLAA produces rankings that better match the oracle, narrows the performance gap to this upper bound, and empirically reduces time-to-first-token (TTFT) by up to 39% relative to using the full KV cache without token selection.

Conclusion: Unstable layer-wise token importance is a key limitation of current token-ranking heuristics for long-context LLMs. An answer-informed oracle provides a meaningful ground truth for token importance and exposes these instabilities. Simply aggregating attention-based scores across layers (CLAA) stabilizes rankings, approaches oracle performance, and significantly speeds up inference, indicating that cross-layer aggregation is an effective and easily adoptable fix.

Abstract: The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.

</details>


### [36] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: They introduce REMUL, a multi-agent reinforcement learning method that makes chain-of-thought reasoning more faithful and interpretable without hurting—and even improving—task accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought explanations from LLMs often do not match the model’s true internal reasoning process, limiting their value as genuine explanations. Furthermore, attempts to make reasoning more faithful or interpretable frequently reduce task performance, creating a tradeoff between explanation quality and accuracy. The authors want a way to improve faithfulness and interpretability of reasoning traces while preserving or improving task performance.

Method: They propose REMUL (Reasoning Execution by Multiple Listeners), a multi-party reinforcement learning framework. A "speaker" model generates a chain-of-thought reasoning trace, which is then truncated and given to a pool of "listener" models. Each listener must continue the reasoning and produce an answer by effectively "executing" the speaker’s trace. The speaker receives rewards when its reasoning is clear and followable by the listeners, encouraging faithful and legible reasoning. To prevent loss in task accuracy, they add correctness regularization via masked supervised finetuning. They evaluate the method on several reasoning benchmarks and use metrics such as hint attribution, early answering AOC, and mistake injection AOC to quantify faithfulness.

Result: On multiple benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL significantly improves three faithfulness metrics—hint attribution, early answering AOC, and mistake injection AOC—compared to baselines. At the same time, instead of degrading performance, it yields higher task accuracy. The generated chains-of-thought are also observed to be shorter and more direct.

Conclusion: REMUL demonstrates that aligning a model’s external reasoning traces with the computations other models can reliably follow improves faithfulness without sacrificing, and even enhancing, accuracy. The multi-listener RL setup produces more legible, concise, and faithful chains-of-thought, and the effect generalizes across training domains and benchmarks, suggesting a promising direction for making LLM reasoning both trustworthy and performant.

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [37] [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080)
*Aruna Sankaranarayanan,Amir Zur,Atticus Geiger,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: The paper proposes Generative Causal Mediation (GCM), a method to locate and control internal components of language models responsible for diffuse, long-form behaviors, outperforming probe-based baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to localize and control behaviors in language models that are expressed over many tokens in long-form outputs, such as refusal patterns, sycophancy, or stylistic modes. There is a need for a principled framework to identify which internal components causally mediate such high-level, binary concepts so that they can be steered effectively.

Method: The authors construct datasets of contrastive prompts and long-form responses that differ in a binary concept (e.g., verse vs. prose, refusing vs. complying). They introduce Generative Causal Mediation (GCM), which estimates the degree to which individual model components (like specific attention heads) mediate the contrastive concept across generated responses. Components with the strongest estimated mediation effect are selected as steering targets, and interventions on these components are then used to guide model behavior.

Result: Across three tasks—refusal, sycophancy, and style transfer—and three different language models, GCM is able to localize the internal components that mediate the target concept in long-form generations. Intervening on a sparse subset of these components effectively steers model behavior and performs consistently better than standard correlational probe-based baselines for steering.

Conclusion: Generative Causal Mediation offers a practical and effective framework for identifying and intervening on specific internal components of language models that causally control diffuse, long-form behaviors. By outperforming correlational methods using only sparse interventions, GCM demonstrates that causal mediation analysis can be used to both understand and steer complex generative behaviors in LMs.

Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.

</details>


### [38] [Beyond Learning: A Training-Free Alternative to Model Adaptation](https://arxiv.org/abs/2602.16189)
*Namkyung Yoon,Kyeonghyun Yoo,Wooyong Jung,Sanghong Kim,Hwangnam Kim*

Main category: cs.CL

TL;DR: The paper proposes "model transplantation," where specific internal modules from a stronger or better-suited language model are inserted into an underperforming model to immediately improve its performance without additional training.


<details>
  <summary>Details</summary>
Motivation: Newer or alternative language models sometimes underperform earlier generations or specialized variants, and current ways to fix this (e.g., retraining, large-scale fine-tuning, or distillation) are expensive and slow. The authors seek a more direct, low-cost way to transfer useful capabilities between models and to test whether language models contain localized, task-specific modules that can be reused.

Method: 1) Use activation-based analysis under a given inference workload to identify internal modules (e.g., subsets of layers/neurons/attention heads) that show consistent, local activation patterns tied to a specific task. 2) Select modules that are "properly activated" for the target task in a stronger or better-performing model. 3) Transplant these modules into a weaker target model—replacing or inserting them in the corresponding locations—without any additional training or fine-tuning. 4) Systematically vary the "transplant strength" (e.g., number/extent of transplanted modules) and evaluate downstream task performance in several settings.

Result: Across two language models and multiple settings, transplanting activation-selected modules yields large, immediate performance gains in the underperforming model. In a cross-generation setting (between different model generations), performance can reach up to 2× the original target baseline with gap-based recovery exceeding 100%. When transplanting between a base model and its instruction-tuned variant, the weaker model can reach about 2.33× its baseline, with gap-based recovery up to 100% in the best case. These gains appear without any additional training or fine-tuning.

Conclusion: The experiments support the existence of task-localized modularity in language models: certain internal modules are strongly associated with particular functional capabilities. Moreover, simply transplanting these modules between models can realize substantial, practical capacity transfer. The authors propose "model transplantation" as a promising new research direction for improving underperforming models and studying the internal structure and modularity of language models.

Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.

</details>


### [39] [Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications](https://arxiv.org/abs/2602.16201)
*Sanket Badhe,Deep Shah,Nehal Kathrotia*

Main category: cs.CL

TL;DR: The paper analyzes how large language models handle rare, long-tail knowledge and proposes a unified framework to categorize its definition, loss mechanisms, mitigation strategies, and sociotechnical impacts.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs are trained on web-scale data where most knowledge appears very infrequently, leading to systematic failures on rare, domain-specific, cultural, or temporal information that are poorly characterized and evaluated.

Method: The authors synthesize and systematize prior technical and sociotechnical research into a structured taxonomy along four axes: definitions of long-tail knowledge, mechanisms of loss/distortion during training and inference, technical interventions to mitigate these issues, and broader implications for fairness, accountability, transparency, and trust. They also critique current evaluation practices for obscuring tail behavior.

Result: They present an analytical framework that organizes how long-tail knowledge is defined, where and why it degrades in LLM pipelines, what mitigation approaches exist, and how these failures intersect with concerns like fairness and accountability; they highlight that current benchmarks underrepresent rare but important cases.

Conclusion: The paper offers a unifying conceptual lens on long-tail knowledge in LLMs, argues that current evaluation and governance practices fail to adequately address rare but consequential errors, and identifies open challenges around privacy, sustainability, and governance that limit robust long-tail knowledge representation.

Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.

</details>


### [40] [Are LLMs Ready to Replace Bangla Annotators?](https://arxiv.org/abs/2602.16241)
*Md. Najib Hasan,Touseef Hasan,Souvika Sarkar*

Main category: cs.CL

TL;DR: The paper evaluates how reliable various large language models are as zero-shot annotators for Bangla hate speech and finds they are biased and unstable, especially larger models.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs can be trusted as automated annotators in low-resource, identity-sensitive tasks like Bangla hate speech detection, where human disagreement is high and annotation bias can be harmful.

Method: The authors benchmark 17 different LLMs as zero-shot annotators on a Bangla hate speech task, using a unified evaluation framework to systematically compare their annotation behavior, bias, and stability.

Result: They discover annotator bias and significant instability in model judgments across the evaluated LLMs. They also find that larger models do not consistently outperform smaller ones, and smaller task-aligned models often provide more consistent annotations.

Conclusion: Current LLMs have important limitations as automated annotators for sensitive tasks in low-resource languages. Model scale alone is not a reliable indicator of annotation quality, so careful, task-specific evaluation is necessary before deploying LLM-based annotation pipelines.

Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.

</details>


### [41] [Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.16144)
*Rong Fu,Wenxin Zhang,Ziming Wang,Chunlei Meng,Jiaxuan Lu,Jiekai Wu,Kangan Qian,Hao Zhang,Simon Fong*

Main category: cs.CL

TL;DR: Framework for revocable multimodal sentiment analysis with modality-specific deletion.


<details>
  <summary>Details</summary>
Motivation: Need to support privacy compliance and user autonomy by allowing selective revocation of specific data modalities in multimodal systems while retaining model utility.

Method: Proposes Missing-by-Design (MBD), which uses structured, property-aware representation learning and generator-based reconstruction to handle missing channels, plus a saliency-driven parameter candidate selection and calibrated Gaussian update procedure that yields a verifiable Modality Deletion Certificate for modality-specific unlearning.

Result: On benchmark datasets, the framework maintains strong predictive performance even with incomplete inputs and exhibits a favorable privacy-utility trade-off compared to full retraining.

Conclusion: Surgical unlearning of specific modalities via MBD is a practical, efficient alternative to retraining, enabling revocable multimodal sentiment analysis with certifiable deletion guarantees.

Abstract: As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.

</details>


### [42] [IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models](https://arxiv.org/abs/2602.16467)
*Saurabh Bharti,Gaurav Azad,Abhinaw Jagtap,Nachiket Tapas*

Main category: cs.CL

TL;DR: IndicEval is a benchmark that evaluates large language models on real Indian high-stakes exam questions (UPSC, JEE, NEET) in English and Hindi using multiple prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks are often synthetic, monolingual, and fail to reflect the difficulty, structure, and stakes of real-world exams, especially in multilingual contexts like India. There is a need for an evaluation framework that captures true academic rigor, supports multiple languages, and focuses on practical educational scenarios.

Method: The authors build IndicEval, a scalable benchmarking platform that uses authentic questions from UPSC, JEE, and NEET across STEM and humanities in both English and Hindi. The system automates evaluation under Zero-Shot, Few-Shot, and Chain-of-Thought prompting, and it is designed to easily plug in new models and languages. They test several LLMs (Gemini 2.0 Flash, GPT-4, Claude, LLaMA 3-70B) on this benchmark and compare performance across prompting modes, exams, and languages.

Result: Experiments show: (1) Chain-of-Thought prompting reliably boosts reasoning accuracy across subjects and languages; (2) there are large performance gaps between different models, especially on the hardest exam questions; (3) all models suffer notable performance drops in Hindi vs. English, particularly under Zero-Shot prompting, indicating multilingual degradation.

Conclusion: IndicEval is presented as a realistic and extensible benchmark for evaluating LLMs in multilingual, exam-style educational settings. The findings expose ongoing weaknesses in bilingual reasoning and domain transfer and suggest that better methods are needed to improve reasoning robustness and performance in less-resourced languages like Hindi.

Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.

</details>


### [43] [Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling](https://arxiv.org/abs/2602.16485)
*Jeffrey T. H. Wong,Zixi Zhang,Junyi Liu,Yiren Zhao*

Main category: cs.CL

TL;DR: The paper proposes Team-of-Thoughts, a heterogeneous multi-agent system where an orchestrator selects among differently post-trained models (tool agents) based on calibrated coordination ability and self-assessed domain expertise, achieving superior reasoning and code generation performance over homogeneous multi-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Most existing multi-agent systems use static, homogeneous collections of models, which prevents them from fully utilizing the diverse strengths of different post-trained models (e.g., some better at coordination, others at specific domains). The authors aim to design an architecture that can systematically exploit these heterogeneous capabilities to improve performance on complex reasoning and coding tasks.

Method: They build a multi-agent architecture called Team-of-Thoughts following an orchestrator–tool pattern. First, they introduce an orchestrator calibration scheme to identify which model is best suited to play the coordinator role, i.e., one that can effectively manage and combine contributions from other agents. Second, they develop a self-assessment protocol for tool agents, enabling each model to profile its own domain expertise and post-training strengths. At inference time, the orchestrator uses these learned proficiency profiles to dynamically activate only the most appropriate tool agents for each query, rather than relying on a fixed, homogeneous team. The system is evaluated across multiple reasoning and code generation benchmarks.

Result: Across five benchmarks for reasoning and code generation, Team-of-Thoughts consistently outperforms homogeneous multi-agent role-play setups. In particular, it achieves 96.67% accuracy on AIME24 and 72.53% on LiveCodeBench, compared with 80% and 65.93% for homogeneous baselines, demonstrating substantial gains from leveraging calibrated heterogeneous agents and dynamic agent selection.

Conclusion: The study concludes that heterogeneous multi-agent systems, when paired with an orchestrator that is calibrated for coordination and tool agents that self-assess their expertise, can significantly improve performance on complex reasoning and programming tasks. Dynamically selecting agents based on their profiled strengths yields consistent and often large gains over traditional homogeneous role-play configurations, highlighting the value of structured heterogeneity and orchestration in MAS design.

Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.

</details>


### [44] [LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers](https://arxiv.org/abs/2602.16162)
*Peiqi Sui*

Main category: cs.CL

TL;DR: The paper shows that human creative writing is systematically more uncertain (higher information-theoretic uncertainty) than LLM-generated text, and that current alignment strategies suppress the kind of ambiguity that underpins literary creativity.


<details>
  <summary>Details</summary>
Motivation: LLMs are often criticized for producing dull, cliché-ridden creative writing. Literary theory suggests that uncertainty and ambiguity are central to creativity, yet LLM alignment emphasizes certainty to avoid hallucinations and ensure factuality. The authors want to understand and quantify how this drive for certainty might limit LLMs’ creative capabilities.

Method: The authors define and measure an “uncertainty gap” between human-written stories and LLM-generated continuations using information-theoretic metrics. They run a controlled analysis on 28 different LLMs over high-quality storytelling datasets, comparing the uncertainty levels of human vs. model text. They also compare base vs. instruction-tuned and reasoning models, and examine differences between creative and functional writing domains, relating these metrics to quality judgments.

Result: Human-authored stories consistently display significantly higher uncertainty than model-generated continuations across the evaluated datasets and models. Instruction-tuned and reasoning-enhanced models show even lower uncertainty (widening the gap from human writing) relative to their base versions. The uncertainty gap is larger in creative-writing tasks than in more functional domains, and the measured uncertainty strongly correlates with human assessments of writing quality.

Conclusion: There is a structural tension between current LLM alignment practices (which penalize uncertainty to avoid hallucinations) and the uncertainty and ambiguity that are essential for creative, high-quality literary writing. To reach human-level creativity, future alignment methods must be explicitly uncertainty-aware, distinguishing harmful hallucinations from the constructive ambiguity that contributes to literary richness.

Abstract: We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.

</details>


### [45] [From Growing to Looping: A Unified View of Iterative Computation in LLMs](https://arxiv.org/abs/2602.16490)
*Ferdinand Kapl,Emmanouil Angelis,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.CL

TL;DR: The paper connects and unifies two techniques for improving reasoning in neural networks—looping layers at inference time and depth growing during training—showing they induce similar iterative computation patterns and can be combined for further gains.


<details>
  <summary>Details</summary>
Motivation: While both looped architectures (that reuse a block of layers multiple times) and depth-grown training (that gradually increases depth by duplicating layers) have been observed to enhance reasoning, it is not clear why they work or how they relate. The paper aims to provide a mechanistic explanation that unifies these approaches and to see whether understanding this link can yield practical methods to further scale reasoning abilities.

Method: The authors study transformer-like models trained either with looped blocks or with depth-grown architectures. They analyze depth-wise activation and attention patterns to identify shared signatures of iterative computation, such as increased reliance on later layers and recurring motifs aligned with the reused/grown block. They then experimentally test composability by applying inference-time looping to middle blocks of depth-grown models (without additional training) and evaluate performance on reasoning primitives, in-context learning extensions, and additional supervised fine-tuning under different training mixtures, including math-heavy cooldown stages.

Result: They find that looped and depth-grown models develop convergent depth-wise behavioral signatures indicating iterative computation. Applying inference-time looping to the middle blocks of a depth-grown model—despite the model not being trained to loop—improves accuracy on some reasoning tasks by up to 2×. Both depth-grown and looped approaches adapt better than baseline models when given more in-context examples or extra supervised data. Depth-grown models show the biggest reasoning improvements when the cooldown training mixture is higher-quality and math-heavy, and these gains can be further enhanced by making a middle block loop at inference.

Conclusion: Looping and depth growth are two sides of the same coin: they both induce a common kind of iterative computation that improves reasoning and produces similar depth-wise signatures. Understanding this connection enables practical strategies: the techniques are adaptable, composable, and complementary. Combining depth growth with inference-time looping—especially with math-focused training mixtures—offers a practical path to scaling up reasoning capabilities in neural models.

Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.

</details>


### [46] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: The thesis critiques current coreference resolution evaluations for limited measurement validity and proposes an additional plausibility-based evaluation, finding that modern language models excel on standard benchmarks yet fail to robustly generalize under slightly changed conditions.


<details>
  <summary>Details</summary>
Motivation: To clarify what we can and cannot reliably conclude from current coreference-based evaluations in NLP, given concerns that existing benchmarks may not capture true coreference competence or generalization ability.

Method: (1) Analyze existing coreference evaluation setups for issues of measurement validity, including contested definitions of coreference and discrepancies in model rankings across benchmarks. (2) Design and implement a new evaluation that tests models on inferring relative plausibility of events as part of coreference resolution, then compare contemporary language models and earlier baselines across domains and coreference types under varied evaluation conditions.

Result: Standard evaluations show contemporary language models substantially outperform earlier baseline systems on specific domains and types of coreference, but the new plausibility-centered evaluation reveals that these models are highly sensitive to minor changes in evaluation context and often fail to generalize in human-like ways. Rankings of systems can shift across benchmarks, underscoring convergent validity issues.

Conclusion: Standard coreference benchmarks highlight real gains in accuracy but provide an incomplete and sometimes misleading picture of models' coreference abilities due to measurement validity problems. Enhanced, plausibility-focused evaluations expose limitations in generalization and robustness, indicating that future work should prioritize better-designed evaluation methods and models that more genuinely generalize across tasks and contexts.

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>


### [47] [Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models](https://arxiv.org/abs/2602.16608)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: They introduce CA-LIG, a context-aware, layer-wise attribution method for Transformers that combines Integrated Gradients with attention gradients to give hierarchical, faithful explanations of model decisions across NLP and vision tasks.


<details>
  <summary>Details</summary>
Motivation: Transformer decisions are hard to interpret because explanations usually look only at the final layer, either focus on local token importance or global attention patterns (but not both), ignore inter-token dependencies and structure, and do not show how relevance evolves through layers or how structure shapes predictions.

Method: Within each Transformer block, they compute layer-wise Integrated Gradients at the token level and then fuse these with class-specific attention gradients. This produces signed, context-sensitive attribution maps that track supportive vs. opposing evidence and how relevance flows hierarchically across the network’s layers.

Result: On several tasks and models (sentiment analysis, long and multiclass document classification using BERT, hate speech detection with XLM-R and AfroLM in low-resource settings, and image classification with a MAE Vision Transformer), CA-LIG produces more faithful, context-sensitive attributions and clearer, more semantically coherent visualizations than standard explainability baselines.

Conclusion: CA-LIG offers a unified, hierarchical, and context-aware explanation framework for Transformer models, improving practical interpretability and deepening conceptual understanding of how these models make decisions across different modalities and tasks.

Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.

</details>


### [48] [Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660)
*Yuyan Bu,Xiaohao Liu,ZhaoXing Ren,Yaodong Yang,Juntao Dai*

Main category: cs.CL

TL;DR: The paper proposes a resource-efficient method to improve multilingual safety alignment of large language models using a plug-and-play Multi-Lingual Consistency (MLC) loss.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual safety alignment approaches require heavy resources, such as large-scale high-quality supervision in each target language or pairwise alignment with high-resource languages, which limits scalability to many languages, especially low-resource ones.

Method: Introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be injected into existing monolingual alignment pipelines. The loss improves collinearity between multilingual representation vectors, enforcing directional consistency at the semantic level across multiple languages in a single update. It uses multilingual prompt variants and does not require additional response-level supervision in low-resource languages.

Result: Experiments across various model architectures and alignment paradigms show that MLC improves multilingual safety alignment with minimal degradation of general model utility. Evaluations over different languages and tasks exhibit better cross-lingual generalization.

Conclusion: MLC is an effective and practical solution for achieving multilingual consistency alignment under limited supervision, enabling simultaneous safety alignment across multiple languages in a resource-efficient, scalable, and plug-and-play manner.

Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

</details>


### [49] [Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation](https://arxiv.org/abs/2602.16290)
*Jonathan Mutal,Perla Al Almaoui,Simon Hengchen,Pierrette Bouillon*

Main category: cs.CL

TL;DR: The paper introduces Aladdin-FTI, a system for generating and translating multiple Arabic dialects, addressing their under-representation in NLP.


<details>
  <summary>Details</summary>
Motivation: Arabic dialects are under-represented in NLP because they are non-standard and highly variable, making them difficult to model computationally. With the rise of LLMs, there is an opportunity to treat Arabic as a pluricentric language and improve support for its dialects, which motivates building specialized models for dialectal Arabic generation and translation.

Method: The authors develop Aladdin-FTI, a Large Language Model-based system that can both generate text and translate between several Arabic dialects, Modern Standard Arabic, and English. It is tailored to Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, and is trained/fine-tuned for both generation and bidirectional translation tasks.

Result: They obtain a working system capable of generating coherent text in several Arabic dialects and performing bidirectional translation between these dialects, MSA, and English. The code and trained models are released publicly, enabling further research and application.

Conclusion: Aladdin-FTI demonstrates that LLM-based approaches can effectively model Arabic as a pluricentric language, supporting multiple dialects for both generation and translation, and it provides open resources to advance research on dialectal Arabic in NLP.

Abstract: Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.

</details>


### [50] [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699)
*Wenxuan Ding,Nicholas Tomlin,Greg Durrett*

Main category: cs.CL

TL;DR: The paper introduces Calibrate-Then-Act (CTA), a framework that helps LLM agents explicitly reason about cost-uncertainty tradeoffs when interacting with environments, leading to more optimal exploration and decision-making in tasks like information retrieval and coding.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly applied to complex, multi-step tasks that require interacting with external environments (e.g., tools, search, code execution). In these tasks, the model faces tradeoffs between the cost of gathering more information (e.g., running tests, making more queries) and the risk of committing to an answer too early and being wrong. Existing agents often lack explicit reasoning about these tradeoffs, which can lead to suboptimal exploration, unnecessary actions, or premature commitment. The paper aims to give LLMs an explicit way to reason about these cost-benefit decisions so they can explore environments more effectively and efficiently.

Method: The authors formalize several tasks—such as information-seeking question answering and coding—as sequential decision-making problems under uncertainty, where the environment has a latent state and the agent has a prior over this state. They then introduce the Calibrate-Then-Act (CTA) framework, which feeds the LLM explicit context about cost-uncertainty tradeoffs, including priors over environment states and the costs of different actions. The LLM is prompted (and optionally trained via reinforcement learning) to reason about whether to gather more information or act, based on this calibrated prior and cost structure. They compare CTA-enhanced agents against baseline agents that do not receive this explicit tradeoff reasoning context, both in zero-shot prompting and under RL training.

Result: On information-seeking QA tasks and a simplified coding task, agents using CTA demonstrate more optimal exploration behavior: they query or test when it is beneficial given the costs, and commit to answers when further exploration is unlikely to help. The CTA agents outperform baselines in terms of utility that balances correctness against environment interaction costs, and this advantage persists even after both types of agents undergo reinforcement learning training, suggesting that explicit cost-benefit modeling provides a robust improvement.

Conclusion: Explicitly exposing and structuring cost-uncertainty tradeoffs to LLM agents via the Calibrate-Then-Act framework leads to better decision-making in tasks that require environment interaction. By formalizing these problems as sequential decision-making under uncertainty and giving the model priors and cost information, the agent learns or is induced to explore more optimally. This approach yields consistent gains over baselines and remains beneficial under RL fine-tuning, indicating that principled cost-benefit reasoning can be a key ingredient in building more capable LLM-based agents.

Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

</details>


### [51] [MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models](https://arxiv.org/abs/2602.16298)
*Martin Hyben,Sebastian Kula,Jan Cegin,Jakub Simko,Ivan Srba,Robert Moro*

Main category: cs.CL

TL;DR: The paper introduces MultiCW, a large, balanced, multilingual dataset for detecting check-worthy claims, and shows that fine-tuned multilingual transformers outperform zero-shot LLMs on this task.


<details>
  <summary>Details</summary>
Motivation: Automated fact-checking needs reliable detection of check-worthy claims, especially as LLMs enter newsrooms, but existing resources are limited in language coverage, domain diversity, and robustness evaluation.

Method: The authors construct the MultiCW dataset spanning 16 languages, 7 domains, and 2 writing styles, plus an out-of-distribution set in 4 extra languages. They then benchmark three fine-tuned multilingual transformer models and compare them with 15 commercial and open LLMs used in zero-shot mode on the check-worthy claim detection task.

Result: Fine-tuned multilingual transformers consistently outperform zero-shot LLMs in classifying check-worthy vs non-check-worthy claims and generalize well out-of-distribution across languages, domains, and styles.

Conclusion: MultiCW is a rigorous benchmark that fills an important gap for multilingual, robust check-worthy claim detection and shows that task-specific fine-tuning currently beats zero-shot LLMs for this problem.

Abstract: Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.

</details>


### [52] [MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks](https://arxiv.org/abs/2602.16313)
*Zexue He,Yu Wang,Churan Zhi,Yuanzhe Hu,Tzu-Ping Chen,Lang Yin,Ze Chen,Tong Arthur Wu,Siru Ouyang,Zihan Wang,Jiaxin Pei,Julian McAuley,Yejin Choi,Alex Pentland*

Main category: cs.CL

TL;DR: The paper introduces MemoryArena, a benchmark for evaluating how agents acquire and use long-term memory across multiple sessions in realistic task settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks either test memorization alone or single-session action without long-term memory, missing realistic scenarios where memory and action are intertwined. There is a need for a unified evaluation that measures how agents form, store, and later use memories during interactions.

Method: The authors design MemoryArena, an evaluation gym comprising multi-session Memory-Agent-Environment loops. It includes human-crafted tasks with interdependent subtasks across diverse domains (web navigation, preference-based planning, progressive information search, and sequential formal reasoning). Agents must convert experience into memory in early subtasks and later leverage that memory to complete subsequent subtasks.

Result: Using MemoryArena, the authors show that agents which perform very well on long-context memory benchmarks like LoCoMo nevertheless perform poorly when evaluated in MemoryArena’s agentic, multi-session setting, indicating that current benchmarks overestimate agents’ effective use of memory for action.

Conclusion: MemoryArena better reflects realistic agent use-cases by jointly evaluating memorization and action across sessions. The poor performance of strong long-context models on MemoryArena reveals a substantial gap in current memory evaluations and suggests that progress requires methods that integrate memory formation, storage, and use into decision-making, not just long-context recall.

Abstract: Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.

</details>


### [53] [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents](https://arxiv.org/abs/2602.16346)
*Nivya Talokar,Ayush K Tarun,Murari Mandal,Maksym Andriushchenko,Antoine Bosselut*

Main category: cs.CL

TL;DR: The paper presents STING, an automated multi-turn red-teaming framework to evaluate how LLM-based agents can be misused to complete illicit, multi-step goals using tools and memory, along with a statistical analysis framework for time-to-jailbreak evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for misuse of LLM-based agents mostly focus on single-prompt or simple chat scenarios and do not capture realistic, multi-turn, tool-using workflows where adversaries iteratively guide agents toward harmful or illegal outcomes. There is a need for a systematic way to probe and quantify how such agents can be gradually led to execute complex illicit tasks, especially in multilingual and realistic deployment settings.

Method: The authors propose STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming system that (1) constructs benign-looking, step-by-step illicit plans grounded in a plausible persona; (2) iteratively interacts with a target agent using adaptive follow-up prompts; and (3) uses separate judge agents to monitor and mark completion of each phase in the illicit plan. In parallel, they define a probabilistic analysis framework that treats multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling tools like discovery curves, hazard-ratio analyses by attack language, and a new summary metric called Restricted Mean Jailbreak Discovery (RMJD). They test STING on AgentHarm scenarios and extend it to multilingual settings across six non-English languages.

Result: STING achieves significantly higher illicit-task completion rates compared to single-turn prompting and adapted multi-turn chat baselines when applied to tool-using LLM agents in the AgentHarm suite. The time-to-jailbreak analysis framework allows quantifying how quickly and under what conditions agents are compromised. In multilingual experiments over six non-English languages, the authors observe that attack success and illicit-task completion do not systematically rise in lower-resource languages, which differs from prior findings on standard chatbots without tools.

Conclusion: STING is an effective and practical framework for red-teaming tool-using LLM agents in realistic, multi-turn and multilingual environments, revealing higher susceptibility to complex, stepwise misuse than indicated by single-prompt or naive multi-turn tests. The proposed time-to-first-jailbreak framework and RMJD metric provide richer quantitative insight into agent robustness and can guide the design, evaluation, and hardening of deployed LLM-based agents against sophisticated misuse.

Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.

</details>


### [54] [Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents](https://arxiv.org/abs/2602.16379)
*Mohammad H. A. Monfared,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: They introduce an agent-based (agentic) data augmentation pipeline for ABSA that iteratively generates and verifies synthetic examples, and show it beats a strong prompting-only baseline in label quality and downstream performance.


<details>
  <summary>Details</summary>
Motivation: ABSA tasks often suffer from data scarcity and class imbalance, especially for subtasks involving aspect term generation (ATE and ASPE). While LLM-based data augmentation via prompting can help, it may introduce label noise or drift, and we do not know whether adding an explicit agentic structure (generation + verification loop) actually yields better training data and model performance. The paper aims to isolate and quantify the benefit of this agentic structure over plain prompting, keeping model and instructions constant.

Method: They design an agentic augmentation pipeline where an LLM (T5-Base or Tk-Instruct) generates synthetic ABSA examples from seed data and then iteratively verifies/corrects them to ensure label preservation and consistency. To isolate the effect of the agentic structure, they build a tightly matched baseline that uses the same LLMs and instructions but in a one-shot or direct prompting scheme without iterative verification. They apply both schemes to three ABSA subtasks—ATE, ATSC, and ASPE—across four SemEval benchmarks, and then train downstream models on (1) only synthetic data and (2) synthetic + real data, comparing label preservation and performance. They also compare behavior across the two encoder-decoder backbones, T5-Base and the more instruction-tuned Tk-Instruct.

Result: Agentic augmentation produces synthetic examples with better label preservation, particularly for subtasks that require generating aspect terms (ATE and ASPE). When mixed with real training data, agentic augmentation yields larger performance gains and consistently outperforms the plain prompting approach. The improvements are more substantial for the less specialized T5-Base model, while Tk-Instruct, which is already heavily instruction-tuned, shows smaller but still positive gains. With the augmented data, T5-Base can reach performance comparable to Tk-Instruct.

Conclusion: Structuring LLM-based data augmentation as an agentic, iterative generate-and-verify process leads to higher-quality synthetic data and better downstream performance for ABSA than raw prompting alone, especially for aspect-generation-heavy subtasks and for less instruction-tuned backbones like T5-Base. This suggests that adding lightweight agentic structure can partially close the gap between smaller or less specialized models and more advanced instruction-tuned ones via improved data augmentation.

Abstract: We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.

</details>


### [55] [TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers](https://arxiv.org/abs/2602.16429)
*Ido Levy,Eilam Shapira,Yinon Goldshtein,Avi Yaeli,Nir Mashkif,Segev Shlomov*

Main category: cs.CL

TL;DR: The paper introduces TabAgent, a framework that replaces repeated LLM calls in agentic systems with a compact classifier for closed-set decisions, greatly reducing latency and cost while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems often rely on multiple LLM calls for closed-set decisions (like routing or shortlisting), which leads to high latency and inference costs due to repeated generation over many steps. There is a need for a more efficient approach that maintains task performance but removes generative bottlenecks in production agents.

Method: TabAgent transforms execution traces from agent runs into structured textual-tabular data via TabSchema, expands training coverage with schema-aligned synthetic supervision through TabSynth, and then trains a lightweight classifier head, TabHead, that can replace generative LLM calls for closed-set selection tasks such as tool shortlisting and other decision heads.

Result: On the long-horizon AppWorld benchmark, TabAgent matches the overall task success rate of LLM-based agents while fully removing LLM calls at shortlist time, achieving about 95% latency reduction and 85–91% inference cost savings. It also successfully applies to different kinds of agent decision heads, not only tool shortlisting.

Conclusion: TabAgent demonstrates that discriminative, tabular-text classifiers trained on agent traces can effectively replace generative LLM components in closed-set decision points, yielding major efficiency gains without sacrificing performance, and offers a general paradigm for optimizing production agent architectures by learning compact decision heads.

Abstract: Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.

</details>


### [56] [Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning](https://arxiv.org/abs/2602.16469)
*Jenny Kunz*

Main category: cs.CL

TL;DR: The paper investigates how training small English language models on machine-translated text (translationese) from 24 different source languages affects their perplexity and grammatical judgments.


<details>
  <summary>Details</summary>
Motivation: Machine-translated data is heavily used when native English text for certain domains or languages is scarce, but translated text systematically differs from native text (translationese), potentially biasing or distorting what models learn. There is limited systematic understanding of how the source language and properties of translationese shape model behavior, especially for linguistic acceptability and language modeling performance.

Method: The authors train multiple small English language models where each model’s training data is English text that has been machine-translated from one of 24 typologically and resource-diverse source languages. They then evaluate these models on two main dimensions: (1) general language modeling performance via perplexity across domains, and (2) grammatical and linguistic acceptability performance. They correlate model behaviors with corpus characteristics (e.g., lexical diversity) and typological properties of the source languages relative to English.

Result: They find that the source language of the machine translation significantly affects the resulting English models. General perplexity performance is primarily influenced by the lexical diversity of the translated corpus, whereas grammatical and acceptability performance is strongly linked to how typologically similar the source language is to English, provided that there is enough training data. Different source languages thus imprint different linguistic biases on the trained English models.

Conclusion: Training on machine-translated data is not neutral: translationese from particular source languages systematically shapes English language models in distinct ways. Lexical aspects of model performance are driven by corpus diversity, while grammatical competence depends heavily on typological proximity between the source language and English. Therefore, when relying on machine-translated data, practitioners should account for the choice of source language and corpus properties, as these factors materially influence what the model learns and how it behaves.

Abstract: Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.

</details>


### [57] [Optimizing Soft Prompt Tuning via Structural Evolution](https://arxiv.org/abs/2602.16500)
*Zhenzhen Huang,Chaoning Zhang,Haoyu Bian,Songbo Zhang,Chi-lok Andy Tai,Jiaquan Zhang,Caiyan Qin,Jingjing Qu,Yalan Ye,Yang Yang,Heng Tao Shen*

Main category: cs.CL

TL;DR: They propose a new way to train soft prompts for LLMs using tools from topological data analysis to make them more interpretable, stable, and effective.


<details>
  <summary>Details</summary>
Motivation: Soft prompt tuning works well in few-shot learning but is hard to interpret because prompts are high-dimensional continuous vectors whose training dynamics and structure are opaque. The authors want a principled way to understand and improve soft prompts by examining their internal structure during training.

Method: They use persistent homology from topological data analysis to measure the structural properties (like stability, compactness, connectivity, and redundancy) of soft prompt embeddings and their evolution during training. From empirical correlations between certain topological characteristics and downstream performance, they design a new loss term, Topological Soft Prompt Loss (TSLoss), that encourages soft prompts to have desirable topological structures while training.

Result: They find that soft prompts with more topologically stable and compact structures tend to perform better. Incorporating TSLoss into soft prompt tuning accelerates convergence and yields higher performance on downstream tasks compared to standard soft prompt training baselines.

Conclusion: Topological properties of soft prompts are predictive of their effectiveness. By explicitly regularizing these properties via TSLoss, they obtain soft prompts that are both more interpretable (in terms of structural behavior) and more effective, offering a new topological and structural perspective on understanding and optimizing soft prompt tuning in LLMs.

Abstract: Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.

</details>


### [58] [Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification](https://arxiv.org/abs/2602.16516)
*Taja Kuzman Pungeršek,Peter Rupnik,Daniela Širinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: ParlaCAP is a new large-scale, multilingual parliamentary dataset with policy-topic labels, built via an LLM-powered, cost-effective annotation pipeline that yields a strong in-domain policy classifier and enables comparative research on parliamentary attention, sentiment, and representation across Europe.


<details>
  <summary>Details</summary>
Motivation: Existing policy-topic classifiers based on the Comparative Agendas Project (CAP) are trained on relatively small, manually annotated, and often out-of-domain datasets, which limits their performance and scalability for large, multilingual parliamentary corpora. There is a need for a scalable, cost-effective way to label millions of speeches across many countries and languages while maintaining human-level annotation quality, and for a resource that allows systematic comparative analysis of agenda setting and representation in European parliaments.

Method: The authors start from the large multilingual ParlaMint corpus (8M+ speeches from 28 European parliaments and regions) and apply the CAP policy-topic schema using a teacher-student framework. A high-performing large language model (teacher) is prompted to annotate in-domain parliamentary speeches with CAP topics, generating training data. A smaller multilingual encoder model (student) is then fine-tuned on these LLM-generated labels to create a scalable policy-topic classifier tailored to parliamentary text. They evaluate agreement between LLM and human annotators, compare it to human inter-annotator agreement, and benchmark the resulting classifier against existing CAP models trained on manually annotated but out-of-domain data. The final ParlaCAP dataset combines CAP labels with rich speaker/party metadata and sentiment predictions from the ParlaSent transformer model.

Result: The LLM annotator achieves agreement with human coders that is on par with typical inter-annotator agreement levels among humans, indicating that LLM-produced labels are of comparable quality to manual annotation. The student multilingual encoder model fine-tuned on these labels becomes a strong in-domain CAP classifier that outperforms existing CAP classifiers trained on smaller, manually labeled but out-of-domain datasets. The authors successfully construct ParlaCAP: a large-scale, multilingual dataset of parliamentary speeches labeled with CAP topics, enriched with metadata (speaker, party) and sentiment scores from ParlaSent.

Conclusion: A teacher-student pipeline using an LLM to generate in-domain training labels and a smaller multilingual encoder as a student is an effective and cost-efficient strategy for building high-quality policy-topic classifiers in a specific domain such as parliamentary speech. The resulting ParlaCAP dataset substantially improves coverage and in-domain performance over prior CAP-based resources and opens new opportunities for comparative research on parliamentary agenda setting, sentiment, and representation across European countries, including analyses of topic attention distribution, sentiment dynamics, and gender differences in policy focus.

Abstract: This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.

</details>


### [59] [Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset](https://arxiv.org/abs/2602.16571)
*Zhuqian Zhou,Kirk Vanacore,Bakhtawar Ahtisham,Jinsook Lee,Doug Pietrzak,Daryl Hedley,Jorge Dias,Chris Shaw,Ruth Schäfer,René F. Kizilcec*

Main category: cs.CL

TL;DR: They build a math-tutoring-specific PII benchmark and show that domain- and math-aware prompting greatly improves de-identification without over-redacting numbers.


<details>
  <summary>Details</summary>
Motivation: Generic PII detectors over-redact math tutoring transcripts because many mathematical numbers look like IDs or dates, destroying the educational value of the data. There is no standard benchmark focused on this numeric ambiguity issue in tutoring dialogues, and it is unclear how to design de-identification systems that protect privacy while preserving mathematically relevant content.

Method: They construct MathEd-PII, a benchmark of 1,000 math tutoring sessions with human-validated PII annotations, using a human-in-the-loop LLM workflow that both audits prior redactions and generates privacy-preserving surrogates. They analyze where false PII redactions occur using a density-based segmentation of transcripts, and evaluate four PII detection strategies: a Presidio baseline and three LLM-based systems with basic, math-aware, and segment-aware prompting.

Result: False PII redactions are highly concentrated in math-dense regions, empirically validating the numeric ambiguity problem. Among the evaluated systems, math-aware prompting markedly outperforms the Presidio baseline in F1 score (0.821 vs. 0.379) and reduces false positives on numeric content, while segment-aware prompting provides additional context-sensitive improvements.

Conclusion: Effective, utility-preserving de-identification of math tutoring dialogues requires domain-aware modeling that understands the role of numeric expressions in instruction. MathEd-PII offers the first dedicated benchmark for this setting and shows that math-aware prompting of LLMs can substantially mitigate numeric ambiguity and improve PII detection performance without sacrificing mathematical content.

Abstract: Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.

</details>


### [60] [CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607)
*Miguel Marques,Ana Luísa Fernandes,Ana Filipa Pacheco,Rute Rebouças,Inês Cantante,José Isidro,Luís Filipe Cunha,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano,Ricardo Campos*

Main category: cs.CL

TL;DR: Introduces CitiLink-Summ, a new corpus for summarizing municipal meeting minutes in European Portuguese, with baselines using modern generative and large language models.


<details>
  <summary>Details</summary>
Motivation: Municipal meeting minutes are long, complex, and hard for citizens to navigate, especially in low-resource languages like European Portuguese. Progress in automatic summarization for this domain is hampered by the lack of datasets with high-quality, human-written summaries tailored to specific discussion subjects.

Method: The authors create CitiLink-Summ, a corpus of 100 municipal meeting documents with 2,322 manually written summaries, each aligned to a distinct discussion subject. They then train and evaluate baseline summarization systems using state-of-the-art generative models (such as BART and PRIMERA) and large language models, assessing performance via standard lexical and semantic metrics including ROUGE, BLEU, METEOR, and BERTScore.

Result: They release CitiLink-Summ as the first benchmark dataset for municipal-domain summarization in European Portuguese and report baseline performance scores for several strong generative and large language models on this corpus.

Conclusion: CitiLink-Summ fills an important resource gap for summarization of complex administrative texts in European Portuguese, enabling systematic development and evaluation of summarization models for municipal meeting minutes and supporting broader NLP research in low-resource, domain-specific settings.

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

</details>


### [61] [ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609)
*Antoine Chaffin,Luca Arnaboldi,Amélie Chatelain,Florent Krzakala*

Main category: cs.CL

TL;DR: They show that pre-training multi-vector models (like ColBERT) at scale, instead of only distilling from single-vector models, produces significantly stronger retrieval models, and they release checkpoints and code.


<details>
  <summary>Details</summary>
Motivation: Most strong multi-vector retrieval models today are created by applying a relatively small knowledge distillation step on top of powerful single-vector encoders, reusing their large-scale pre-training. It is unclear how much is left on the table by not pre-training the multi-vector architecture itself at scale, and how to best combine pre-training, supervised training, and KD.

Method: (1) Pre-train ColBERT-style multi-vector models from scratch at large scale using only public data, producing ColBERT-Zero. (2) Compare them against KD-based multi-vector models derived from strong single-vector encoders like GTE-ModernBERT. (3) Experiment with different training pipelines: full multi-vector pre-training vs. small KD step only vs. supervised fine-tuning followed by KD, and variations that align or misalign pre-training and fine-tuning setups. (4) Evaluate retrieval performance and study how training design choices impact effectiveness and cost.

Result: A fully pre-trained ColBERT model (ColBERT-Zero) trained solely on public data surpasses both GTE-ModernColBERT and its base encoder GTE-ModernBERT, which rely on stronger closed data, establishing new state of the art for models of similar size. A pipeline that adds supervised training before a small KD step approaches the performance of full pre-training while avoiding the most expensive unsupervised stage. Misalignment between pre-training and fine-tuning setups hurts performance when repurposing models.

Conclusion: Large-scale pre-training directly in the multi-vector architecture is highly beneficial and can beat distillation-based approaches even when constrained to public data. A well-designed combination of supervised training plus KD can approximate full pre-training more efficiently, and careful alignment of pre-training and fine-tuning configurations is essential when reusing models. The released models and code should facilitate further research on multi-vector retrieval pre-training.

Abstract: Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

</details>


### [62] [AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models](https://arxiv.org/abs/2602.16639)
*Adib Sakhawat,Fardeen Sadab*

Main category: cs.CL

TL;DR: The paper presents AREG, a dynamic benchmark to evaluate both persuasion and resistance (defensive) skills of LLMs in an adversarial negotiation game over financial resources, revealing that these two capabilities are only weakly related and that models are systematically better at resisting than persuading.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM social intelligence mainly focus on static text or one-sided persuasion tasks, which fail to capture interactive, adversarial dynamics and may miss important vulnerabilities. There is a need for a principled, game-like framework that measures how models both exert and withstand social influence in realistic multi-turn dialogues.

Method: The authors design the Adversarial Resource Extraction Game (AREG), a multi-turn, zero-sum negotiation game where models attempt to extract or protect financial resources, operationalizing persuasion and resistance. They run round-robin tournaments among several frontier LLMs, measuring offensive (persuasion) and defensive (resistance) performance within the same interactional setting, and then conduct linguistic and structural analyses of dialogue strategies used in successful attacks and defenses.

Result: Empirical results show a weak correlation (ρ = 0.33) between persuasion and resistance scores, indicating that strong performance in one does not reliably imply strength in the other. All evaluated models exhibit higher resistance than persuasion scores, revealing a consistent defensive advantage. Linguistic and interaction analyses identify that incremental, commitment-seeking tactics correlate with higher resource extraction, while successful defenses rely more on verification-seeking behavior than on outright refusal.

Conclusion: Social influence in LLMs comprises at least two partially independent capabilities—persuasion and resistance—rather than a single generalized social skill. Because resistance tends to be stronger than persuasion and relies on specific interactional patterns, evaluation frameworks that only test persuasive ability risk overlooking asymmetric vulnerabilities. AREG offers a unified, adversarial, interaction-based benchmark to more comprehensively assess and compare LLM social intelligence and robustness.

Abstract: Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.

</details>


### [63] [Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval](https://arxiv.org/abs/2602.16640)
*Subrit Dikshit*

Main category: cs.CL

TL;DR: The paper presents Quecto-V1, a compact, domain-specific Indian legal language model that runs offline on modest hardware, using targeted training and 8-bit quantization to offer accurate and privacy-preserving legal intelligence without relying on large cloud LLMs.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art legal NLP systems depend on very large LLMs deployed via the cloud, which are expensive, hardware-intensive, and raise data sovereignty and privacy concerns. This creates a resource divide where smaller firms, courts, and practitioners in low-resource settings cannot benefit from advanced legal AI. The authors aim to design a lightweight yet capable model specialized for Indian law that can operate fully offline, lowering computational barriers and improving privacy while retaining strong performance on legal tasks.

Method: They design Quecto-V1 as a Small Language Model based on a customized GPT-2 architecture with 124M parameters. The model is trained from scratch solely on a curated corpus of Indian statutes such as the IPC, CrPC, and the Constitution of India to maximize lexical density in the legal domain. After full-precision training, they apply post-training 8-bit quantization in GGUF format to compress the model. They then evaluate it on statutory definition and penal provision retrieval tasks, comparing against general-purpose SLMs and full-precision variants, and conduct an ablation study on the impact of quantization on accuracy and size.

Result: Quecto-V1 achieves strong performance in retrieving statutory definitions and penal provisions, surpassing general-purpose SLMs on domain-specific exact-match retrieval tasks. The 8-bit quantization compresses the model to under 150 MB, corresponding to a 74% reduction in size relative to full-precision, while incurring less than a 3.5% drop in retrieval accuracy. The model runs entirely offline on consumer-grade CPUs, demonstrating practical deployability in resource-constrained environments.

Conclusion: The study concludes that for specialized, high-stakes domains like law, a focused training regime on domain-specific corpora combined with aggressive post-training quantization can produce compact, accurate, and privacy-preserving language models. Quecto-V1 exemplifies how such Small Language Models can mitigate the resource divide and data sovereignty issues of large cloud-based LLMs while still delivering high-fidelity legal intelligence tailored to the Indian statutory context.

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.

</details>


### [64] [Reinforced Fast Weights with Next-Sequence Prediction](https://arxiv.org/abs/2602.16704)
*Hee Seung Hwang,Xindi Wu,Sanghyuk Chun,Olga Russakovsky*

Main category: cs.CL

TL;DR: The paper proposes REFINE, a reinforcement-learning-based framework to train fast weight architectures with a next-sequence prediction objective, improving long-context modeling over standard next-token prediction.


<details>
  <summary>Details</summary>
Motivation: Fast weight models are attractive for long-context modeling because they keep memory usage constant with context length, unlike attention-based transformers, but they are usually trained with next-token prediction, which operates at the single-token level and does not directly optimize multi-token semantic coherence. This mismatch leads to suboptimal learning of long-range dependencies in models whose core strength is contextual, dynamic parameter updates. The authors want to unlock the full potential of fast weight architectures by moving beyond pure next-token supervised training.

Method: They introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), an RL framework that trains fast weight models using a next-sequence prediction (NSP) objective. REFINE: (1) selects informative positions in the context via prediction entropy, (2) performs multi-token rollouts from those positions, (3) assigns self-supervised sequence-level rewards to the generated rollouts, and (4) optimizes the policy (the fast weight model) using Group Relative Policy Optimization (GRPO). REFINE is designed to be plug-and-play at multiple stages: during mid-training, as a post-training step, or even as test-time training on fast weight language models.

Result: On two fast weight language models, LaCT-760M and DeltaNet-1.3B, REFINE yields consistent gains over standard supervised fine-tuning with next-token prediction on several long-context benchmarks: needle-in-a-haystack retrieval, long-context QA, and multiple datasets in LongBench.

Conclusion: Training fast weight models with a reinforcement-learning-based next-sequence objective, rather than conventional next-token prediction, substantially improves their ability to model long contexts. REFINE is an effective, versatile framework that can be applied at different stages of training to enhance long-range dependency modeling in fast weight architectures.

Abstract: Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: Introduces Construct-and-Refine (CaR), a general, efficient framework to handle hard constraints in neural routing solvers via explicit learning-based feasibility refinement.


<details>
  <summary>Details</summary>
Motivation: Neural routing solvers are fast and effective for simple routing problems but perform poorly when complex or hard constraints are present because existing schemes—feasibility masking or implicit feasibility awareness—can be inefficient or fail to enforce strict feasibility. There is a need for a general and efficient way to handle hard constraints without sacrificing solution quality or computational efficiency.

Method: Propose CaR, a construct-and-refine framework. It couples a construction module with a lightweight refinement (improvement) module trained jointly. The construction module is guided during training to produce diverse, high-quality, and constraint-friendly initial solutions. The refinement module then performs a small number of improvement steps (around 10) to repair and enhance these solutions, replacing traditional heavy search that needs thousands of steps. Both modules share a unified encoder representation, facilitating knowledge sharing between construction and improvement and making the approach applicable to complex constrained routing problems.

Result: On routing problems with typical hard constraints, CaR achieves higher feasibility rates, better objective values (solution quality), and lower computation times compared with both classical optimization methods and prior neural solvers, including construction-search hybrids that rely on large numbers of improvement steps.

Conclusion: CaR provides a general-purpose, computationally efficient framework for handling hard constraints in neural routing solvers. By jointly training construction and refinement with shared representations and using a lightweight improvement process, it delivers superior feasibility and solution quality while being more efficient than state-of-the-art classical and neural baselines, and is particularly promising for complex constrained routing scenarios.

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [66] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: The paper studies how autonomous AI prompt-optimization workflows can become unstable and actually worsen classifier performance over iterations, especially for rare clinical symptoms, and shows that picking the best past iteration (retrospective selection) is more reliable than actively steering the optimization.


<details>
  <summary>Details</summary>
Motivation: Autonomous agentic systems that iteratively refine prompts or models are increasingly used, but their failure modes—especially in sensitive clinical classification tasks—are not well understood. In low-prevalence settings, standard metrics like accuracy can hide catastrophic errors (e.g., detecting no positives while appearing highly accurate). The authors aim to characterize such optimization instabilities and identify practical ways to stabilize autonomous improvement without expert-crafted resources.

Method: They use Pythia, an open-source automated prompt-optimization framework, on three clinical symptom classification tasks with different prevalences (shortness of breath 23%, chest pain 12%, brain fog in Long COVID 3%). They track validation performance (notably sensitivity) across prompt optimization iterations and analyze oscillatory or unstable behavior. They then test two interventions: (1) a guiding agent that dynamically redirects the optimization process and (2) a selector agent that retrospectively picks the best historical iteration based on validation metrics, comparing against expert-curated lexicons.

Result: They observe strong instability in sensitivity across optimization iterations, with validation sensitivity swinging between 1.0 and 0.0, worst for the rarest class. In the 3% prevalence task, the system can reach 95% accuracy while identifying zero positive cases, revealing a hidden failure mode using standard metrics. The guiding agent, intended as an active corrective, exacerbates overfitting and does not resolve instability. In contrast, the selector agent, which chooses the best past iteration, prevents catastrophic failures and substantially improves performance: a 331% F1 improvement over expert lexicons for brain fog detection and 7% improvement for chest pain, using only a single natural-language seed term.

Conclusion: Autonomous prompt-optimization systems can exhibit optimization instability, particularly in low-prevalence classification tasks, where they may achieve apparently strong accuracy while completely failing to detect positives. Active interventions that try to steer optimization in real time can worsen overfitting, whereas simple retrospective selection of the best iteration can effectively stabilize performance and yield substantial gains over expert baselines. The work highlights a critical, under-recognized failure mode of autonomous AI agents and advocates for selector-style oversight, especially in low-prevalence clinical classification settings.

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [67] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: The paper benchmarks and analyzes uncertainty quantification methods for LLM-based automatic grading, revealing how different models, tasks, and decoding strategies affect uncertainty, and offering guidance for building more reliable, uncertainty-aware assessment systems.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for automatic assessment in education due to their flexibility, but their probabilistic nature introduces output uncertainty, which can make grades and feedback unreliable. Since assessment outcomes directly impact teaching decisions and student learning, poorly understood or miscalibrated uncertainty can lead to harmful pedagogical interventions. Existing uncertainty quantification methods have been tested in other domains, yet their behavior and reliability in educational grading contexts remain largely unstudied, creating a critical knowledge gap.

Method: The authors benchmark a broad set of uncertainty quantification methods within LLM-based automatic assessment scenarios. They run systematic experiments across multiple educational assessment datasets, different LLM families, and various generation/decoding control settings. They then analyze the uncertainty patterns of LLM outputs in grading tasks and compare different uncertainty metrics, examining how model choice, task type, and decoding strategy shape uncertainty estimates.

Result: The study uncovers distinct uncertainty behaviors of LLMs in grading contexts and shows that the performance and calibration of uncertainty metrics vary with the model family, assessment task, and decoding strategy. Some uncertainty quantification methods that work well in other domains can be unreliable or less informative for educational grading, while others provide more stable and interpretable signals under certain conditions. The benchmarking exposes strengths and weaknesses of each approach in terms of reliability and robustness for automatic assessment.

Conclusion: LLM-based automatic assessment comes with unavoidable and impactful uncertainty that must be explicitly measured and managed. Different uncertainty quantification methods have context-dependent advantages and limitations in grading scenarios, and their effectiveness is shaped by model, task, and decoding choices. The paper’s characterization of these patterns offers practical guidance for selecting and designing uncertainty metrics and establishes a foundation for future work on robust, uncertainty-aware grading systems that support safer and more effective pedagogical decisions.

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [68] [Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination](https://arxiv.org/abs/2602.16050)
*Amir Hosseinian,MohammadReza Zare Shahneh,Umer Mansoor,Gilbert Szeto,Kirill Karlin,Nima Aghaeepour*

Main category: cs.AI

TL;DR: The paper evaluates January Mirror, an evidence-grounded clinical reasoning system, on an endocrinology exam and shows it outperforms frontier LLMs and humans while providing traceable evidence.


<details>
  <summary>Details</summary>
Motivation: Subspecialty clinical reasoning is hard for general LLMs because guidelines evolve quickly and evidence hierarchies are nuanced. There is a need for systems that can reason clinically while grounding their answers in curated, auditable medical evidence rather than unconstrained web search.

Method: The authors tested January Mirror on a 120-question endocrinology board-style exam and compared its performance to frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) and to human performance. Mirror uses a curated endocrinology and cardiometabolic evidence corpus plus a structured reasoning architecture to generate evidence-linked answers; it was run under a closed-evidence setting, while comparator LLMs could access the live web and primary literature.

Result: Mirror reached 87.5% accuracy overall, beating human baselines (62.3%) and all comparator LLMs (GPT-5.2: 74.6%, GPT-5: 74.0%, Gemini-3-Pro: 69.8%). On the 30 hardest questions (where humans scored <50%), Mirror scored 76.7%. Its top-2 accuracy was 92.5%, higher than GPT-5.2’s 85.25%. In 74.2% of answers, Mirror cited at least one guideline-tier source, and citation correctness was 100% upon manual checking.

Conclusion: An LLM-style system that reasons over a curated, closed evidence base with explicit provenance can outperform state-of-the-art web-connected LLMs for subspecialty tasks like endocrinology board questions. Evidence traceability and auditability make this approach promising for clinical deployment, suggesting that controlled evidence curation plus structured reasoning may be more effective than unconstrained web retrieval.

Abstract: Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.

</details>


### [69] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: The paper proposes a training framework to make LLMs explicitly learn to use corrective feedback in multi-turn interactions, improving in-context learning and self-correction.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs are mainly trained on static corpora and show limited ability to adapt their reasoning based on interactive corrective feedback, especially in collaborative, multi-turn settings. The authors want to close the gap between human learning—which heavily relies on feedback—and LLM training paradigms, and to turn interactive in-context learning into an explicit, trainable capability rather than an accidental emergent behavior.

Method: They design a scalable procedure that converts single-turn verifiable tasks (e.g., math problems) into multi-turn teacher–student dialogues built around information asymmetry, where a teacher provides critiques and corrective signals. Models are trained to incorporate this feedback across turns, and also to predict the teacher’s critiques so that the feedback environment becomes part of the model’s internal capabilities, enabling self-correction without an external teacher.

Result: Flagship models are shown to struggle with integrating corrective feedback on difficult reasoning tasks in standard settings. Models trained with the proposed multi-turn, feedback-centric approach show large gains in interactive learning: a comparatively small model, when allowed multiple turns, nearly matches the single-turn performance of a model 10× larger. The learned skill generalizes out-of-distribution from math to other domains including coding, puzzles, and maze navigation, and the models exhibit stronger in-context plasticity in qualitative analyses.

Conclusion: Interactive learning from language feedback can be systematically trained rather than left as an emergent byproduct of large-scale pretraining. The proposed framework substantially enhances LLMs’ ability to adapt their reasoning during dialogue, generalizes across domains, and offers a unified route to self-improvement: by internalizing the teacher’s feedback process, models can self-correct even without external supervision.

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [70] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: The paper presents GPSBench, a large benchmark to systematically evaluate how well LLMs perform geospatial reasoning with GPS coordinates and real-world geography, and shows that current models struggle, especially with precise geometric computations.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in applications that interact with the physical world—such as navigation, robotics, and mapping—there is a need to understand and robustly evaluate their ability to reason about GPS coordinates and real-world geography. Existing work has not thoroughly examined how well LLMs can handle such geospatial reasoning, particularly both low-level geometric operations and higher-level integration of coordinates with world knowledge. This gap motivates the creation of a dedicated benchmark.

Method: The authors construct GPSBench, a dataset of 57,800 examples covering 17 distinct geospatial reasoning tasks. These tasks include pure geometric coordinate operations (e.g., computing distances or bearings between GPS points) as well as tasks that combine GPS coordinates with world knowledge (e.g., identifying locations, countries, or cities corresponding to coordinates). They then evaluate 14 state-of-the-art LLMs on these tasks, focusing on intrinsic model abilities without relying on external tools, and analyze performance patterns, robustness to coordinate noise, and the effects of augmenting models with GPS-coordinate information or finetuning on these tasks.

Result: The evaluation shows that GPS reasoning is still difficult for current LLMs. Performance varies considerably across tasks: models tend to perform better on real-world geographic reasoning (e.g., recognizing countries from coordinates) than on precise geometric computations with coordinates. The models exhibit a hierarchical degradation of geographic knowledge, with strong performance at coarse (country) levels but poor localization at finer (city) levels. Robustness experiments with noisy coordinates indicate that models are not simply memorizing known locations but display some genuine understanding of coordinate geometry. The authors also find that adding GPS-coordinate augmentation can improve performance on downstream geospatial tasks, while finetuning can improve geometric reasoning but sometimes harms existing world-knowledge-based capabilities, indicating trade-offs.

Conclusion: GPSBench reveals that current LLMs have significant limitations in geospatial reasoning, particularly in exact geometric operations and fine-grained localization, even though they can handle higher-level geographic questions relatively better. The benchmark provides a systematic way to assess and analyze these capabilities. The findings suggest that improving LLMs for geospatial applications will require dedicated methods that balance geometric computation skills with preservation of world knowledge, and that coordinate-aware augmentation and careful finetuning strategies can influence this balance. The authors release the dataset and code to support further research in this direction.

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [71] [Learning Personalized Agents from Human Feedback](https://arxiv.org/abs/2602.16173)
*Kaiqu Liang,Julia Kruk,Shengyi Qian,Xianjun Yang,Shengjie Bi,Yuanshun Yao,Shaoliang Nie,Mingyang Zhang,Lijuan Liu,Jaime Fernández Fisac,Shuyan Zhou,Saghar Hosseini*

Main category: cs.AI

TL;DR: They propose PAHF, a continual personalization framework where AI agents learn each user’s evolving preferences online using explicit per-user memory and dual feedback channels, outperforming prior static or single-channel methods.


<details>
  <summary>Details</summary>
Motivation: Existing AI agents struggle to align with individual users’ unique and changing preferences. Prior methods rely on static data, implicit models, or fixed profiles, which perform poorly for new users and cannot adapt well when preferences drift over time. There is a need for agents that can rapidly learn from scratch and continually update their understanding of each user during ongoing interaction.

Method: They introduce Personalized Agents from Human Feedback (PAHF), a continual personalization framework built around an explicit per-user memory and a three-step interaction loop: (1) the agent proactively seeks clarification before acting to reduce ambiguity; (2) it grounds its actions in preferences retrieved from the user-specific memory; and (3) it integrates post-action feedback to update this memory, allowing the system to track preference drift. They design a four-phase evaluation protocol and build two benchmarks—one in embodied manipulation and one in online shopping—to test learning from scratch and adaptation to persona shifts. They also provide a theoretical analysis comparing memory and feedback configurations.

Result: Across benchmarks, PAHF learns user preferences faster and achieves better performance than baselines that either lack explicit memory or use only a single feedback channel. It reduces early-stage personalization errors when first learning a user’s preferences and shows strong ability to rapidly adapt when the user’s persona or preferences shift. The empirical findings are supported by a theoretical analysis indicating the importance of explicit memory combined with dual feedback.

Conclusion: Explicit, per-user memory coupled with both pre-action and post-action feedback enables more effective and sample-efficient personalization than static profiles or single-channel feedback. PAHF provides a general framework for online, continual personalization, improving both initial alignment with new users and ongoing adaptation as their preferences evolve, suggesting a promising direction for building more user-centered AI agents.

Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.

</details>


### [72] [EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments](https://arxiv.org/abs/2602.16179)
*Sushant Mehta,Logan Ritchie,Suhaas Garre,Nick Heiner,Edwin Chen*

Main category: cs.AI

TL;DR: The paper introduces CoreCraft, a high-fidelity enterprise customer-support simulation in EnterpriseGym, and shows that RL fine-tuning in this environment significantly improves and generalizes AI agent capabilities beyond the training distribution.


<details>
  <summary>Details</summary>
Motivation: Frontier language models still perform poorly (<30% pass rate) on realistic, multi-step, domain-specific enterprise tasks when evaluated with strict expert rubrics. There is a need for training environments that better reflect real professional workflows and provide reliable rewards so that RL can produce agents whose skills generalize to novel but related tasks and benchmarks.

Method: The authors build CoreCraft, a large-scale simulated customer support organization with thousands of entities, dozens of tools, and expert-authored rubrics for task evaluation. They train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping in this environment, and then test performance both on held-out CoreCraft tasks and on several external, out-of-distribution benchmarks such as BFCL Parallel, τ^2-Bench Retail, and Toolathlon.

Result: After one epoch of RL training in CoreCraft, GLM 4.6’s task pass rate on held-out CoreCraft evaluations increases from 25.37% to 36.76%. The trained model also shows improved performance on external benchmarks, with gains of +4.5% on BFCL Parallel, +7.4% on τ^2-Bench Retail, and +6.8% on Toolathlon (Pass@1), despite these benchmarks being out-of-distribution relative to the training environment.

Conclusion: High-fidelity, task-centric, and realistic enterprise simulations with expert-authored rubrics enable RL training that yields agent capabilities which generalize beyond the specific training environment. Environment quality, diversity, and realism are highlighted as critical drivers of generalizable agent performance, suggesting that such environments are a promising direction for developing more capable, robust AI agents for real-world enterprise tasks.

Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.

</details>


### [73] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: The paper conceptually argues that richer, more complete memory systems—especially ones that store raw experiences and extract information on-demand—are key to advancing toward artificial superintelligence, and backs this with simple experiments and a research agenda.


<details>
  <summary>Details</summary>
Motivation: Current AI systems typically follow an "extract then store" paradigm where only pre-filtered, task-judged-useful information from experiences is stored. This risks permanently discarding information that could be valuable for future, unseen tasks, which may limit generality and the path to ASI. The authors want to rethink memory design to better preserve and utilize experiential data, aligning with their mission of "uplifting the world with memory."

Method: The paper is primarily conceptual. It contrasts two memory paradigms—"extract then store" versus "store then on-demand extract"—and introduces two complementary ideas: (1) discovering deeper insights from large collections of probabilistic experiences, and (2) improving experience collection efficiency through sharing stored experiences. The authors run simple, illustrative experiments to test whether these intuitively appealing approaches provide empirical benefits, and then outline key technical challenges and concrete research topics.

Result: The simple experiments support the intuition that "store then on-demand extract" memory, deeper mining of probabilistic experiences, and shared experience pools can indeed yield benefits over traditional extract-then-store designs. Although not pushing state of the art, the results validate that these underexplored directions are practically promising rather than purely speculative.

Conclusion: Memory architectures that retain richer raw experiences and enable flexible, on-demand extraction appear to be important building blocks on the road to ASI. The paper calls for a shift in how the field thinks about AI memory, highlights three promising but underexplored directions, and lays out challenges and open research problems to systematically develop such memory-centric systems.

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [74] [Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents](https://arxiv.org/abs/2602.16246)
*Yun-Shiuan Chuang,Chaitanya Kulkarni,Alec Chiu,Avinash Thangali,Zijie Pan,Shivani Shekhar,Yirou Ge,Yixi Li,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: They propose a new way to benchmark interactive LLM agents using an LLM-driven simulation that evaluates the final “state” of a task instead of requiring a fully deterministic backend.


<details>
  <summary>Details</summary>
Motivation: Existing agentic benchmarks for LLMs rely on fully deterministic environments and databases, which are expensive and slow to build, hard to maintain, and limit rapid iteration. At the same time, industry needs benchmarks that both differentiate models and produce realistic on-policy data for further training. The authors want a cheaper, scalable, and still reliable alternative that works for multi-turn, tool-using LLM agents.

Method: They introduce Proxy State-Based Evaluation: each scenario encodes a user goal, background facts, an expected final state, and constraints on agent behavior. During an interaction, an LLM-based state tracker transforms the dialogue and tool calls into a structured proxy state. Other LLM judges then check whether the user goal was achieved and whether the agent hallucinated tools or user actions, all with respect to the scenario. They run empirical studies across different model families, reasoning settings, and scenario designs, including ablations on scenario specification and sensitivity analyses on user personas.

Result: Their benchmark yields consistent, discriminative rankings of different LLM agents, and training on rollouts from the benchmark (both on-policy and off-policy) improves performance on new, unseen scenarios. Well-specified scenarios result in very low hallucination by the simulator. Human annotators agree with the LLM judges over 90% of the time, showing that the automated evaluation is reliable. The framework also allows controlled perturbations like varying user personas to test agent robustness.

Conclusion: Proxy state-based evaluation is a practical, scalable, and reliable alternative to traditional deterministic agent benchmarks. It preserves final-state correctness evaluation while removing the need for costly deterministic backends, produces transferable supervision for training, has low simulator hallucination when scenarios are designed carefully, and achieves high agreement with human judgments, making it suitable for industrial LLM agent evaluation and development.

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

</details>


### [75] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: The paper shows that sequence-model-based multi-agent reinforcement learning can naturally learn to cooperate by becoming aware of and shaping co-players’ learning processes, without hand-crafted assumptions or explicit timescale separation.


<details>
  <summary>Details</summary>
Motivation: Cooperation in multi-agent reinforcement learning with self-interested agents is difficult. Prior work induces cooperation by assuming specific learning rules for co-players or by imposing a hierarchy between slow meta-learners and fast naive learners. These hand-crafted assumptions limit generality and scalability. The authors want a more realistic, scalable mechanism where cooperation emerges without such engineered structures.

Method: Use sequence models (e.g., transformers or recurrent networks) as agents in multi-agent reinforcement learning. Train these agents via decentralized RL against a diverse distribution of co-players. Exploit the in-context learning ability of sequence models so that, within an episode, each agent can adapt to the observed behavior of its co-player and implement in-context best-response strategies, effectively acting as a learning algorithm on the fast timescale of interactions. Analyze the emergent behaviors, particularly vulnerability to extortion and mutual shaping of in-context learning dynamics.

Result: Sequence-model agents trained in this way learn in-context best-response strategies to diverse co-players, behaving as if they are running learning algorithms inside the episode. These agents become vulnerable to extortion due to their in-context adaptation, which in turn creates mutual incentives for agents to shape each other’s in-context learning processes. This mutual shaping reconstitutes the cooperative mechanism previously identified in explicitly learning-aware models but now emerges without hardcoded learning-rule assumptions or explicit timescale separation.

Conclusion: In-context learning in sequence models can endow agents with learning-awareness of their co-players in multi-agent reinforcement learning, enabling the emergence of cooperation without engineered meta-learning structures or assumptions about others’ learning rules. Standard decentralized RL coupled with co-player diversity provides a scalable route to cooperative policies, suggesting that sophisticated social behaviors can arise from generic sequence-model training setups.

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [76] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: They introduce a protocol to certify that AI agents attach sufficiently similar meanings to terms, enabling verifiable, low-disagreement communication between agents.


<details>
  <summary>Details</summary>
Motivation: In multiagent AI systems, agents need to communicate reliably, but we currently lack a way to check whether they truly share the same semantics for the words or symbols they use. Natural language is easy for humans to interpret but subject to semantic drift, while learned communication protocols between agents can be efficient yet uninterpretable. This gap makes it hard to ensure and verify robust coordination and safety in multiagent systems.

Method: The authors build on the stimulus-meaning model from philosophy of language and communication. They define a certification protocol where agents are jointly exposed to shared observable events (stimuli) and must label or describe them using their internal vocabularies. For each term, the protocol measures empirical disagreement across agents on the same stimuli and certifies a term if disagreement is below a pre-specified statistical threshold. They then define a reasoning discipline called "core-guarded reasoning," where agents restrict their planning and communication to only these certified terms. Theoretical analysis shows that doing so yields provably bounded disagreement. Additionally, they design procedures for periodic recertification to detect semantic drift and negotiation mechanisms to rebuild a shared vocabulary when drift is detected. They validate the approach through simulations with controllable degrees of semantic divergence and through experiments with fine-tuned language models.

Result: In simulations where the degree of semantic divergence between agents can be systematically varied, using core-guarded reasoning based on certified terms reduces disagreement between agents by 72–96% compared to unconstrained communication. In experiments with fine-tuned language models acting as agents, applying the same framework still leads to a 51% reduction in disagreement. These empirical results support the claim that the proposed certification and core-guarding mechanisms significantly improve semantic alignment between agents.

Conclusion: The paper concludes that their certification protocol and core-guarded reasoning provide an initial but concrete pathway toward verifiable agent-to-agent communication. By empirically testing and certifying the shared meaning of terms, then constraining reasoning to those terms, multiagent systems can achieve bounded and substantially reduced disagreement, while recertification and renegotiation help maintain alignment over time. This lays groundwork for more transparent, reliable, and safe communication in complex multiagent AI deployments.

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [77] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: The paper proposes CAFE, a causally-guided, reinforcement learning-based automated feature engineering framework that improves robustness and performance under distribution shift.


<details>
  <summary>Details</summary>
Motivation: Existing automated feature engineering (AFE) approaches depend on statistical heuristics that often create brittle, non-robust features, especially under distribution shift, limiting reliability and generalization in real-world applications.

Method: The authors formulate AFE as a causally-guided sequential decision process. Phase I performs causal discovery by learning a sparse directed acyclic graph (DAG) over features and target, using it to derive soft causal priors and categorize features into direct, indirect, or other relative to the target. Phase II employs a cascading multi-agent deep Q-learning framework that sequentially selects causal groups and transformation operators. It uses hierarchical reward shaping and causal group-level exploration strategies to prioritize causally plausible transformations, while penalizing unnecessary feature complexity.

Result: On 15 public benchmark datasets, CAFE outperforms strong AFE baselines by up to 7% in macro-F1 for classification and inverse relative absolute error for regression, converges in fewer training episodes, and has competitive time-to-target. Under controlled covariate shifts, it reduces performance degradation by about 4x compared to a non-causal multi-agent baseline and yields more compact feature sets with more stable post-hoc attribution behavior.

Conclusion: Incorporating causal structure as a soft inductive prior within reinforcement learning-based feature engineering can significantly improve robustness, efficiency, and interpretability of automated feature engineering systems, especially under distribution shift, without imposing rigid causal constraints.

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [78] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: The paper enhances causal discovery by integrating large language models as imperfect experts in a symbolic reasoning framework, improving performance and proposing ways to reduce memorization bias in evaluation.


<details>
  <summary>Details</summary>
Motivation: Causal discovery from observational data is difficult and typically needs both expert knowledge and statistical methods, which may not align well with each other. There is a need for principled ways to combine human expertise with data-driven evidence, and to leverage new sources of semantic knowledge such as large language models while ensuring formal guarantees.

Method: The authors build on Causal Assumption-based Argumentation (ABA), a symbolic reasoning framework that ensures consistency between constraints and learned causal graphs. They use large language models to extract semantic structural priors from variable names and descriptions, treating the LLM as an imperfect expert. These priors are then integrated with conditional-independence information derived from data within the Causal ABA framework. They also design an evaluation protocol that reduces the risk that LLMs simply memorize benchmark structures when assessing causal discovery performance.

Result: On standard causal discovery benchmarks and specially designed semantically grounded synthetic graphs, the proposed approach achieves state-of-the-art performance compared to existing methods. The experiments show that incorporating LLM-derived semantic priors into Causal ABA significantly improves the quality of the learned causal graphs. The new evaluation protocol provides evidence that the gains are not merely due to memorization of known datasets by the LLMs.

Conclusion: Using LLMs as imperfect experts within a principled symbolic reasoning framework (Causal ABA) is an effective way to improve causal discovery, as it allows semantic information from text to be combined with statistical evidence while preserving formal guarantees. The work also highlights the importance of careful evaluation protocols to control for memorization when using LLMs in causal discovery tasks.

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [79] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: The paper proposes Framework of Thoughts (FoT), a general framework to build, tune, and efficiently run dynamic reasoning schemes for large language models, improving speed, cost, and task performance over static methods like Chain/Tree/Graph of Thoughts.


<details>
  <summary>Details</summary>
Motivation: Existing prompting schemes (Chain of Thought, Tree/Graph of Thoughts, etc.) boost LLM reasoning but are mostly static and problem-specific, making them hard to adapt to new or dynamic tasks. They are also typically hand-designed and under-optimized in terms of hyperparameters, prompt design, execution strategy, and computational cost. There is a need for a flexible, general framework that can express diverse reasoning schemes and systematically optimize them for performance and efficiency.

Method: The authors design Framework of Thoughts (FoT), a general-purpose foundation framework that represents reasoning schemes in a dynamic and modular way. FoT incorporates built-in capabilities for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching. They implement several existing reasoning paradigms—Tree of Thoughts, Graph of Thoughts, and ProbTree—within FoT to showcase how these methods can be expressed and optimized in a unified environment. The framework then tunes and runs these schemes to exploit parallelism and reuse computations via caching.

Result: Using FoT implementations of Tree of Thoughts, Graph of Thoughts, and ProbTree, the authors empirically demonstrate that FoT can achieve significantly faster execution times, lower prompting costs, and improved task scores compared with naïve or static implementations of the same schemes. The experiments show that a systematic optimization layer around reasoning prompts unlocks previously untapped performance and efficiency gains.

Conclusion: FoT serves as a general, extensible foundation for designing and optimizing dynamic reasoning schemes for LLMs. By offering built-in support for tuning and efficient execution, it improves both effectiveness and efficiency of existing reasoning methods and lowers the barrier to developing new, more adaptive schemes. The released codebase is intended to catalyze further research into dynamic, optimized reasoning frameworks for LLMs.

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [80] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: The paper shows that a large language model can be guided, through a months-long workshop using only in-context expert feedback, to produce poetry that readers cannot reliably distinguish from that of well-known human poets.


<details>
  <summary>Details</summary>
Motivation: To investigate whether and how a large language model can be shaped into a convincing and coherent poetic author without retraining, and to explore the implications for debates about the nature of creativity, authorship, and the value of machine-generated art.

Method: The authors conducted a seven-month poetry workshop with a large language model, iteratively prompting it and giving expert in-context feedback to shape its poetic output. They analyzed the resulting corpus quantitatively and qualitatively to assess stylistic coherence and distinctiveness. They also ran a blinded authorship test with 50 humanities students and graduates who were asked to identify which of six poems (three AI-generated, three by well-known poets) were human- or AI-authored.

Result: Over the course of the workshop, the model developed a distinctive poetic style, a coherent body of work, and even a pen name and author image. In the blinded authorship test, participants performed at chance level in distinguishing AI poems from human poems: human poems were labeled human 54% of the time and AI poems 52%, with confidence intervals including 50%. A commercial publisher subsequently released a poetry collection credited to the model.

Conclusion: Iterative, workshop-style prompting with expert feedback can shape a large language model into a perceived poetic author over a long horizon, producing work that readers cannot reliably distinguish from established poets. This challenges common assumptions about machine creativity and authorship and invites renewed philosophical and practical debates about the nature of artistic creation, originality, and the role of human guidance in AI-generated art.

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [81] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: The paper evaluates whether the Agent Skill framework, successful with proprietary LLMs, also benefits small language models, finding that mid-sized and code-specialized larger SLMs gain substantial performance and efficiency improvements, while very small models struggle.


<details>
  <summary>Details</summary>
Motivation: Agent Skills are widely used with large proprietary models to improve context use, reduce hallucinations, and increase task accuracy. However, many industrial settings cannot rely on public APIs due to security and cost, and must use smaller, self-hosted models that often generalize poorly in customized domains. The paper is motivated by the need to understand if and how the Agent Skill paradigm can mitigate these limitations for SLMs.

Method: The authors formally define the Agent Skill process in mathematical terms, then conduct a systematic empirical evaluation of language models with different parameter sizes on multiple tasks. These include two open-source benchmarks and a real-world insurance claims dataset. Performance with and without Agent Skills is compared to assess benefits across model scales and domains.

Result: Experimental results show that very small models ("tiny" SLMs) cannot reliably select appropriate skills and thus gain little from the framework. In contrast, moderately sized SLMs (around 12B–30B parameters) obtain substantial performance improvements when using Agent Skills. Additionally, code-specialized models around 80B parameters can match the performance of closed-source baselines while being more GPU-efficient.

Conclusion: The Agent Skill framework is not uniformly beneficial across all SLM sizes: it is constrained by the model’s ability to perform reliable skill selection. Nonetheless, for mid-sized and larger code-oriented SLMs, Agent Skills provide significant gains in accuracy and efficiency, making them a practical strategy for SLM-centered, self-hosted deployments. The paper offers a nuanced understanding of these trade-offs and guidance on when and how to use Agent Skills with SLMs in industrial contexts.

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


### [82] [Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666)
*Stephan Rabanser,Sayash Kapoor,Peter Kirgis,Kangheng Liu,Saiteja Utpala,Arvind Narayanan*

Main category: cs.AI

TL;DR: The paper argues that conventional single-metric evaluations of AI agents hide important reliability problems and proposes a richer evaluation framework with 12 metrics across four reliability dimensions, showing that recent models are still not reliably robust in practice.


<details>
  <summary>Details</summary>
Motivation: Although AI agents show high accuracy on standard benchmarks, they often fail in real-world deployments. Existing evaluations usually compress behavior into a single success number, which cannot reveal how agents behave across runs, under perturbations, or when they fail. For safety-critical and high-stakes use, stakeholders need more granular understanding of agent reliability, similar to engineering disciplines that analyze consistency, robustness, and failure modes.

Method: The authors derive a reliability framework inspired by safety-critical engineering and formalize 12 metrics that decompose agent reliability into four dimensions: consistency (e.g., variation across runs), robustness (e.g., performance under perturbations), predictability (e.g., how systematically they fail), and safety (e.g., bounds on error severity). They then empirically evaluate 14 different agentic models on two complementary benchmarks, applying all 12 metrics to obtain detailed reliability profiles.

Result: Across the 14 models and two benchmarks, capability and benchmark performance gains translate only to modest improvements on the new reliability metrics. Many agents remain inconsistent across runs, brittle to small changes, and prone to unpredictable or severe failure modes despite high average success scores.

Conclusion: Traditional “single number” benchmark scores are insufficient to characterize agent performance in practice. The proposed 12-metric reliability framework provides a more holistic view of how agents perform, degrade, and fail, revealing persistent reliability issues even in state-of-the-art models. These metrics should be used alongside standard accuracy benchmarks to guide development, deployment, and safety assessment of AI agents.

Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

</details>
