{"id": "2602.11159", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11159", "abs": "https://arxiv.org/abs/2602.11159", "authors": ["Natalia Abarca", "Andr\u00e9s Carvallo", "Claudia L\u00f3pez Moncada", "Felipe Bravo-Marquez"], "title": "Explaining AI Without Code: A User Study on Explainable AI", "comment": "LatinX in AI Workshop @ NeurIPS-25", "summary": "The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\u03b1$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\u03b1$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.", "AI": {"tldr": "This paper introduces and evaluates a human-centered explainability module (PDP, PFI, KernelSHAP) integrated into a no-code ML platform, showing it is usable and increases novice users' trust and understanding, while revealing tensions between novices\u2019 and experts\u2019 needs.", "motivation": "As ML is increasingly used in sensitive domains, there is a pressing need for transparent, understandable model decisions, especially for non-experts using no-code ML tools that typically lack explainability. The authors aim to bridge this gap by providing explanations that are accessible to novices yet informative for experts.", "method": "The authors integrated three explainability techniques\u2014Partial Dependence Plots, Permutation Feature Importance, and KernelSHAP\u2014into DashAI, a no-code ML platform for tabular classification. They then conducted a user study with 20 participants (both ML novices and experts) to assess usability, perceived usefulness, trust, and the impact of explanations using standardized scales (ESS and TiA).", "result": "The study found high task success rates (\u226580%) for all explainability tasks. Novices evaluated the explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale, while experts judged them more critically, particularly regarding sufficiency and completeness. Explanations increased perceived predictability and user confidence according to the Trust in Automation scale, with novices reporting higher trust levels than experts.", "conclusion": "The integrated XAI module is effective in supporting novice users\u2019 understanding and trust in a no-code ML platform, but differences between novices\u2019 and experts\u2019 perceptions reveal a key challenge: designing explanations that are simultaneously accessible, satisfying, and sufficiently detailed for diverse user groups in no-code ML environments."}}
{"id": "2602.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11229", "abs": "https://arxiv.org/abs/2602.11229", "authors": ["Zituo Chen", "Haixu Wu", "Sili Deng"], "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation", "comment": null, "summary": "We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.", "AI": {"tldr": "The paper proposes Latent Generative Solvers (LGS), a latent-space, uncertainty-aware generative framework for simulating long-horizon dynamics across diverse PDE systems, achieving stable, low-cost rollouts and good generalization.", "motivation": "Traditional neural PDE surrogates often suffer from rollout drift and instability on long horizons, are tailored to specific PDE families, and can be computationally expensive to train and deploy, limiting their usefulness for general-purpose scientific forecasting. There is a need for a unified, efficient, and robust approach that can handle heterogeneous PDE systems, remain stable over long time horizons, and provide uncertainty-awareness for downstream workflows.", "method": "LGS is a two-stage framework: (1) A pretrained Variational Autoencoder (VAE) maps states from many heterogeneous PDE systems into a shared latent physics space; (2) a Transformer-based probabilistic dynamics model, trained via flow matching, predicts time evolution in this latent space. An \"uncertainty knob\" perturbs latent inputs during both training and inference so the model learns to correct off-manifold states and reduce autoregressive drift. Additionally, \"flow forcing\" updates a system descriptor (context vector) using the model\u2019s own generated trajectories, better aligning conditioning between training and testing and enhancing long-term stability. The system is pretrained on ~2.5M trajectories at 128\u00d7128 resolution across 12 PDE families, and includes architectural and latent-space design choices to minimize FLOPs.", "result": "LGS matches strong deterministic neural operator baselines on short-term prediction accuracy while substantially reducing rollout drift and instability on long horizons. Because dynamics are learned in low-dimensional latent space with efficient architectures, LGS achieves up to 70\u00d7 lower FLOPs than non-generative baselines, making large-scale pretraining feasible. The method also adapts efficiently, with limited finetuning, to an out-of-distribution 256\u00d7256 Kolmogorov flow dataset, indicating good scalability and generalization across resolutions and PDE families.", "conclusion": "Latent Generative Solvers offer a practical and scalable approach to building generalizable, uncertainty-aware neural PDE surrogates. By operating in a shared latent physics space, combining probabilistic generative dynamics with an uncertainty knob and flow forcing, LGS improves long-horizon stability and reduces computational cost, making it more suitable for long-term forecasting and scientific workflows than many existing neural PDE solvers."}}
{"id": "2602.11295", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11295", "abs": "https://arxiv.org/abs/2602.11295", "authors": ["Gil Raitses"], "title": "On Decision-Valued Maps and Representational Dependence", "comment": "10 pages, 3 figures, 5 tables", "summary": "A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.", "AI": {"tldr": "The paper defines and implements a system (DecisionDB) to track how different data representations affect discrete computational decisions, enabling exact replay and audit of those decisions.", "motivation": "In practice, the same underlying data can be encoded or represented in multiple ways (e.g., different file formats, schema versions, or encodings). Computational engines may yield different discrete decisions (outputs) depending on these representations, which complicates reproducibility, auditing, and safe reuse of past decisions. There is a need for a principled way to record, analyze, and mechanically verify which representations preserve a given decision and which change it.", "method": "The paper formalizes the notion of a decision-valued map: a function from a declared family of data representations to discrete decision outcomes. It then introduces DecisionDB, a system that: (1) logs, for each computation, the representation used and resulting decision; (2) computes stable identifiers from both content and derived artifacts; (3) stores all artifacts in an immutable (write-once) store; and (4) supports deterministic replay, re-running computations using stored artifacts to verify that the same decision identifier (with all fields) is recovered. Using this formalism, the authors partition the representation space into regions in which the decision is stable and boundaries where it changes.", "result": "They show that decision-valued maps allow the representation space to be partitioned into persistence regions (where all representations yield the same discrete decision) and boundaries (where decisions change). DecisionDB can reliably log and deterministically replay decisions so that the exact same decision identifiers are reproduced from stored artifacts, demonstrating mechanical checkability of decision reuse. This makes it possible to automatically determine whether a past decision can be safely reused for a new but related representation.", "conclusion": "By formalizing decision-valued maps and implementing DecisionDB, the paper provides a concrete framework for understanding and managing how data representation impacts discrete computational outcomes. It turns decision reuse into a mechanically verifiable property, supported by content-derived identifiers and immutable storage. This framework improves auditability, reproducibility, and principled reuse of decisions across varying data representations, effectively structuring the representation space into well-defined regions of decision persistence and change."}}
{"id": "2602.11298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11298", "abs": "https://arxiv.org/abs/2602.11298", "authors": ["Alexander H. Liu", "Andy Ehrenberg", "Andy Lo", "Chen-Yo Sun", "Guillaume Lample", "Jean-Malo Delignon", "Khyathi Raghavi Chandu", "Patrick von Platen", "Pavankumar Reddy Muddireddy", "Rohin Arora", "Sanchit Gandhi", "Sandeep Subramanian", "Soham Ghosh", "Srijan Mishra", "Abhinav Rastogi", "Alan Jeffares", "Albert Jiang", "Alexandre Sablayrolles", "Am\u00e9lie H\u00e9liou", "Andrew Bai", "Angele Lenglemetz", "Anmol Agarwal", "Anton Eliseev", "Antonia Calvi", "Arjun Majumdar", "Baptiste Bout", "Baptiste Rozi\u00e8re", "Baudouin De Monicault", "Benjamin Tibi", "Cl\u00e9mence Lanfranchi", "Connor Chen", "Corentin Barreau", "Corentin Sautier", "Cyprien Courtot", "Darius Dabert", "Diego de las Casas", "Elliot Chane-Sane", "Enguerrand Paquin", "Faruk Ahmed", "Federico Baldassarre", "Gabrielle Berrada", "Ga\u00ebtan Ecrepont", "Gauthier Guinet", "Genevieve Hayes", "Georgii Novikov", "Giada Pistilli", "Guillaume Martin", "Gunjan Dhanuka", "Gunshi Gupta", "Han Zhou", "Indraneel Mukherjee", "Irene Zhang", "Jaeyoung Kim", "Jan Ludziejewski", "Jason Rute", "Joachim Studnia", "John Harvill", "Jonas Amar", "Josselin Somerville Roberts", "Julien Tauran", "Karmesh Yadav", "Kartik Khandelwal", "Kush Jain", "Laurence Aitchison", "L\u00e9onard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Maarten Buyl", "Manan Sharma", "Margaret Jennings", "Marie Pellat", "Mark Prins", "Mathieu Poir\u00e9e", "Mathilde Guillaumin", "Matthieu Dinot", "Matthieu Futeral", "Maxime Darrin", "Maximilian Augustin", "Mert Unsal", "Mia Chiquier", "Nathan Grinsztajn", "Neha Gupta", "Olivier Bousquet", "Olivier Duchenne", "Patricia Wang", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philom\u00e8ne Chagniot", "Pierre Stock", "Piotr Mi\u0142o\u015b", "Prateek Gupta", "Pravesh Agrawal", "Quentin Torroba", "Ram Ramrakhya", "Rishi Shah", "Romain Sauvestre", "Roman Soletskyi", "Rosalie Millner", "Sagar Vaze", "Samuel Humeau", "Siddharth Gandhi", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Th\u00e9o Cachet", "Theo Simon Sorg", "Thibaut Lavril", "Thomas Chabal", "Thomas Foubert", "Thomas Robert", "Thomas Wang", "Tim Lawson", "Tom Bewley", "Tom Edwards", "Tyler Wang", "Valeriia Nemychnikova", "Van Phung", "Vedant Nanda", "Victor Jouault", "Virgile Richard", "Vladislav Bataev", "Wassim Bouaziz", "Wen-Ding Li", "William Marshall", "Xinghui Li", "Xingran Guo", "Xinyu Yang", "Yannic Neuhaus", "Yihan Wang", "Zaccharie Ramzi", "Zhenlin Xu"], "title": "Voxtral Realtime", "comment": null, "summary": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.", "AI": {"tldr": "Voxtral Realtime is a streaming ASR model that achieves Whisper-level accuracy with sub-second latency.", "motivation": "Existing streaming ASR systems usually retrofit offline models with chunking or windowing, which hurts quality or increases latency; there is a need for a natively streaming model that preserves offline-level accuracy while remaining low-latency and multilingual.", "method": "Train an end-to-end streaming ASR model under the Delayed Streams Modeling framework, with explicit alignment between audio and text streams, a new causal audio encoder, and Ada RMS-Norm for better delay conditioning; pretrain at scale on multilingual audio (13 languages).", "result": "At a streaming delay of 480 ms, Voxtral Realtime matches the transcription performance of Whisper, a strong offline baseline, across the evaluated benchmarks while supporting 13 languages; model weights are released under Apache 2.0.", "conclusion": "End-to-end training specifically for streaming, combined with delay-aware architectural choices, can close the quality gap between streaming and offline ASR, enabling practical low-latency multilingual transcription with open-source models."}}
{"id": "2602.11156", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11156", "abs": "https://arxiv.org/abs/2602.11156", "authors": ["Sungmoon Kim", "Hyuna Jeon", "Dahye Kim", "Mingyu Kim", "Dong-Kyu Chae", "Jiwoong Kim"], "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.", "AI": {"tldr": "HybridRAG is a RAG framework that converts unstructured PDFs into hierarchical text, pre-generates a QA knowledge base with an LLM, and at query time first retrieves from this QA bank before falling back to standard RAG, achieving higher answer quality and lower latency.", "motivation": "Standard RAG assumes clean, structured text sources and performs retrieval and generation at query time, which is slow and less applicable when handling messy, unstructured PDFs and many users under limited compute. The authors want a more practical, scalable RAG setup for real-world chatbots over large volumes of raw documents.", "method": "1) Ingest raw PDFs with OCR and layout analysis to handle text, tables, and figures; 2) Convert them into hierarchical chunks; 3) Pre-generate a bank of plausible question-answer pairs from these chunks using an LLM; 4) At query time, match user questions to this QA bank to directly return answers when possible; 5) If no good match exists, fall back to on-the-fly retrieval and generation (standard RAG).", "result": "On the OHRBench benchmark, HybridRAG achieves better answer quality and lower latency compared with a conventional RAG baseline that only performs retrieval and generation at query time.", "conclusion": "Pre-generating a QA knowledge base from unstructured documents and using it as the first line of retrieval in a hybrid pipeline yields a more accurate and faster RAG system. This makes HybridRAG a practical choice for chatbots that must serve many users over large, messy document collections with limited computational resources."}}
{"id": "2602.11301", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11301", "abs": "https://arxiv.org/abs/2602.11301", "authors": ["John M. Willis"], "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates", "comment": "43 pages, plus 12 pages of appendices. One Figure", "summary": "Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.\n  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.", "AI": {"tldr": "Introduce a reference architecture (PBSAI) for secure governance of complex enterprise AI estates involving multi-agent systems.", "motivation": "Enterprises are deploying complex AI systems (LLMs, RAG, agents) on shared HPC and cloud platforms that also run cyber defense workloads. Existing frameworks like NIST AI RMF describe principles and high-level risk functions but don't give concrete, implementable architectures for multi-agent, AI-enabled cyber defense estates. There is a need for a practical, structured, and secure governance ecosystem for these AI deployments.", "method": "Define the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem: a multi-agent reference architecture. Organize responsibilities into a 12-domain taxonomy. Specify bounded agent families that sit between tools and policies using shared context envelopes and structured output contracts. Assume standard enterprise security capabilities and encode systems security techniques like monitoring, coordinated defense, and adaptive response. Provide a lightweight formal model of agents, context envelopes, and ecosystem invariants to clarify traceability, provenance, and human-in-the-loop guarantees. Map PBSAI to NIST AI RMF functions and illustrate use in enterprise SOC and hyperscale defense settings.", "result": "PBSAI offers a structured, multi-agent architecture that translates high-level AI risk management principles (e.g., from NIST AI RMF) into implementable security and governance mechanisms for large-scale AI estates. It shows how agent families, context envelopes, and formal invariants can enforce traceability, provenance, human oversight, and coordinated defense in enterprise and hyperscale environments. The paper demonstrates conceptual alignment and practical applicability via mappings to NIST AI RMF and example deployments (SOC, hyperscale defense).", "conclusion": "PBSAI provides a practical, evidence-centric governance ecosystem for secure AI estates, bridging the gap between abstract risk frameworks and real-world multi-agent AI deployments. It is positioned as a foundation for open ecosystem development and invites future empirical validation and refinement in operational environments."}}
{"id": "2602.11318", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.11318", "abs": "https://arxiv.org/abs/2602.11318", "authors": ["Sheza Munir", "Benjamin Mah", "Krisha Kalsi", "Shivani Kapania", "Julian Posada", "Edith Law", "Ding Wang", "Syed Ishtiaque Ahmed"], "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation", "comment": null, "summary": "In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the \"noisy sensor\" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular \"right\" answer to mapping the diversity of human experience.", "AI": {"tldr": "The paper critiques the conventional notion of 'ground truth' in ML annotation, showing how current practices suppress disagreement and cultural diversity, and proposes pluralistic annotation infrastructures that treat disagreement as valuable signal instead of noise.", "motivation": "To challenge the dominant 'ground truth' paradigm in machine learning, which assumes a single correct label and treats annotator disagreement as noise, thereby obscuring sociocultural diversity and reinforcing power imbalances in data annotation ecosystems.", "method": "Systematic literature review of work from 2020\u20132025 across seven top venues (ACL, AIES, CHI, CSCW, EAAMO, FAccT, NeurIPS). They used a tiered keyword filtration to reduce 30,897 records to 3,042 for manual screening, then performed qualitative synthesis and reflexive thematic analysis on 346 included papers.", "result": "They identify mechanisms\u2014such as model-mediated annotations, human-as-verifier architectures, lack of positional legibility, and economic precarity of data workers\u2014that create a 'consensus trap' and anchoring bias, suppressing annotators' subjective and culturally grounded perspectives. They show how geographic hegemony and Western norms are encoded as universal standards and how disagreement is systematically misinterpreted as noise.", "conclusion": "The study concludes that current ground-truth-oriented annotation practices are sociotechnically flawed, erasing meaningful human and cultural variation. The authors argue that disagreement should be reclaimed as a high-fidelity signal and propose a roadmap for pluralistic annotation infrastructures aimed at mapping diverse human experiences rather than enforcing a single correct answer."}}
{"id": "2602.11162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11162", "abs": "https://arxiv.org/abs/2602.11162", "authors": ["Yuping Lin", "Zitao Li", "Yue Xing", "Pengfei He", "Yingqian Cui", "Yaliang Li", "Bolin Ding", "Jingren Zhou", "Jiliang Tang"], "title": "Retrieval Heads are Dynamic", "comment": null, "summary": "Recent studies have identified \"retrieval heads\" in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perform retrieval on average. This perspective overlooks the fine-grained temporal dynamics of autoregressive generation. In this paper, we investigate retrieval heads from a dynamic perspective. Through extensive analysis, we establish three core claims: (1) Dynamism: Retrieval heads vary dynamically across timesteps; (2) Irreplaceability: Dynamic retrieval heads are specific at each timestep and cannot be effectively replaced by static retrieval heads; and (3) Correlation: The model's hidden state encodes a predictive signal for future retrieval head patterns, indicating an internal planning mechanism. We validate these findings on the Needle-in-a-Haystack task and a multi-hop QA task, and quantify the differences on the utility of dynamic and static retrieval heads in a Dynamic Retrieval-Augmented Generation framework. Our study provides new insights into the internal mechanisms of LLMs.", "AI": {"tldr": "The paper shows that which attention heads perform retrieval in LLMs changes over time, that these time-specific heads cannot be replaced by fixed ones, and that the model\u2019s hidden states predict future retrieval patterns, suggesting internal planning.", "motivation": "Prior work on retrieval heads in LLMs used static, dataset-level statistics and thus only identified heads that retrieve information on average, ignoring step-by-step changes during autoregressive generation. The authors want to understand the fine-grained temporal dynamics of which heads retrieve when, and whether the model internally plans these retrieval operations.", "method": "They conduct fine-grained, timestep-level analyses of attention heads during autoregressive generation. They compare \u201cdynamic\u201d retrieval heads (identified per timestep) with \u201cstatic\u201d retrieval heads (identified by aggregate statistics) using interventions such as ablating or swapping heads. They also train or evaluate predictors based on hidden states to see if they can forecast future retrieval-head patterns. Experiments are done on Needle-in-a-Haystack and multi-hop QA tasks, plus within a Dynamic Retrieval-Augmented Generation setup.", "result": "(1) The set of heads responsible for retrieval changes across timesteps; (2) heads that are important at a given timestep cannot be effectively substituted by globally important static heads; and (3) hidden states contain information that can predict which heads will be used for retrieval in the near future. Dynamic retrieval heads yield better utility than static ones in Dynamic RAG benchmarks.", "conclusion": "Retrieval behavior in LLMs is temporally dynamic and planned: different heads are used at different steps, those roles are not well captured by static analyses, and the model\u2019s hidden states encode a plan for future retrieval. This calls for dynamic, time-aware approaches to interpreting, steering, and augmenting LLMs, instead of relying on fixed sets of retrieval heads."}}
{"id": "2602.11340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11340", "abs": "https://arxiv.org/abs/2602.11340", "authors": ["Bo Pan", "Xuan Kan", "Kaitai Zhang", "Yan Yan", "Shunwen Tan", "Zihao He", "Zixin Ding", "Junjie Wu", "Liang Zhao"], "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.", "AI": {"tldr": "The paper introduces BLPO, a bi-level prompt optimization framework to better align multimodal LLM-as-a-judge image evaluation with human judgments without expensive supervised fine-tuning.", "motivation": "LLMs are widely used as automated judges for AI-generated content, but aligning their evaluations with human judgments is hard, especially in multimodal (image) settings. Supervised fine-tuning is costly and needs retraining per task, while current auto prompt optimization focuses on text-only scenarios and does not handle multimodal constraints like limited visual context windows.", "method": "The authors propose BLPO, a bi-level prompt optimization framework for multimodal LLM-as-a-judge when evaluating AI-generated images. BLPO converts images into textual representations via an I2T (image-to-text) prompt that preserves evaluation-relevant visual cues. It then jointly optimizes the judge prompt and the I2T prompt under context window limitations to iteratively refine both levels of prompts.", "result": "Experiments conducted on four datasets and three different LLM judges show that BLPO improves evaluation performance and alignment with human judgments compared to existing methods, demonstrating its effectiveness for multimodal LLM-as-a-judge tasks.", "conclusion": "BLPO successfully addresses the context window bottleneck in multimodal LLM-as-a-judge by using bi-level prompt optimization on textualized image representations, achieving better alignment with human judgments without expensive supervised fine-tuning and generalizing across datasets and LLM judges."}}
{"id": "2602.11163", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11163", "abs": "https://arxiv.org/abs/2602.11163", "authors": ["Muhammad Haris", "Hans H\u00f6ft", "Markus M. Becker", "Markus Stocker"], "title": "Nested Named Entity Recognition in Plasma Physics Research Articles", "comment": null, "summary": "Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scientific text in this domain. Research articles in plasma physics often contain highly complex and context-rich content that must be extracted to enable, e.g., advanced search. We propose a lightweight approach based on encoder-transformers and conditional random fields to extract (nested) named entities from plasma physics research articles. First, we annotate a plasma physics corpus with 16 classes specifically designed for the nested NER task. Second, we evaluate an entity-specific model specialization approach, where independent BERT-CRF models are trained to recognize individual entity types in plasma physics text. Third, we integrate an optimization process to systematically fine-tune hyperparameters and enhance model performance. Our work contributes to the advancement of entity recognition in plasma physics and also provides a foundation to support researchers in navigating and analyzing scientific literature.", "AI": {"tldr": "The paper develops a lightweight, transformer+CRF-based nested NER system tailored to plasma physics papers, using a newly annotated 16-class corpus and entity-specific model specialization with hyperparameter optimization to improve extraction quality for advanced literature search and analysis.", "motivation": "Scientific plasma physics articles contain complex, domain-specific terminology and nested entities that current generic NER systems handle poorly, limiting advanced search, information extraction, and analysis in this field. The authors are motivated to build a domain-adapted NER solution that can robustly identify specialized entities to better support researchers in navigating plasma physics literature.", "method": "They (1) create and annotate a dedicated corpus of plasma physics articles with 16 custom entity types designed for nested NER; (2) develop a lightweight architecture combining encoder-based transformers (e.g., BERT) with Conditional Random Fields for sequence labeling; (3) explore entity-specific specialization by training separate BERT-CRF models for each entity type; and (4) integrate a systematic hyperparameter optimization procedure to fine-tune the models and maximize recognition performance.", "result": "The specialized BERT-CRF models, coupled with hyperparameter optimization, successfully extract nested, domain-specific entities in plasma physics texts with improved performance (relative to baseline or generic NER approaches), demonstrating that entity-specific model specialization and targeted optimization yield better recognition quality in this specialized scientific domain.", "conclusion": "Domain-tailored, lightweight transformer+CRF models, supported by a custom-annotated corpus and systematic hyperparameter tuning, can effectively handle nested NER in plasma physics literature. This provides a strong foundation for building advanced search and analysis tools for plasma physics research and suggests that similar specialization strategies may be beneficial for other scientific domains with complex terminology."}}
{"id": "2602.11348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11348", "abs": "https://arxiv.org/abs/2602.11348", "authors": ["Ruipeng Wang", "Yuxin Chen", "Yukai Wang", "Chang Wu", "Junfeng Fang", "Xiaodong Cai", "Qi Gu", "Hui Su", "An Zhang", "Xiang Wang", "Xunliang Cai", "Tat-Seng Chua"], "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition", "comment": null, "summary": "Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.", "AI": {"tldr": "The paper introduces AgentNoiseBench, a benchmark framework to test how robust LLM-based agents are when exposed to realistic noise in their environments, such as imperfect users and unreliable tools.", "motivation": "Existing LLM-based agents perform well on clean, idealized benchmarks but often degrade in real-world scenarios where interactions are noisy and stochastic. Current training and evaluation setups rarely model this noise, so we lack a systematic way to understand and improve agents\u2019 robustness to realistic perturbations.", "method": "The authors first analyze real-world usage to identify and categorize environmental noise into two main types: user-noise (e.g., ambiguous, erroneous, or inconsistent user inputs) and tool-noise (e.g., unreliable, delayed, or incorrect tool outputs). They then build an automated pipeline that takes existing agent benchmarks and programmatically injects controllable, parameterized noise into them while ensuring tasks remain solvable. Using this framework, they run extensive experiments across many LLM-based agents with different architectures and sizes under various noise levels and types.", "result": "Experiments show that agent performance changes significantly when different kinds and levels of noise are introduced. Performance reductions are systematic and consistent across models, demonstrating that most current agentic models are quite sensitive to realistic environmental noise and are not robust to these perturbations.", "conclusion": "AgentNoiseBench exposes a clear gap between current benchmark performance and robustness required for real-world deployment of LLM-based agents. By providing a systematic and controllable way to inject noise into existing benchmarks, it enables more realistic evaluation and can guide the development of more noise-robust agent architectures and training procedures."}}
{"id": "2602.11165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11165", "abs": "https://arxiv.org/abs/2602.11165", "authors": ["Pushwitha Krishnappa", "Amit Das", "Vinija Jain", "Tathagata Mukherjee", "Aman Chadha"], "title": "Assessing LLM Reliability on Temporally Recent Open-Domain Questions", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Reddit questions from September 2025 paired with community-derived reference answers. We investigate how four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, and GPT-OSS-20B) respond to these questions, evaluating alignment using lexical metrics (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical inference (NLI). Our central finding is a striking semantic-lexical paradox: all models achieve over 99% cosine similarity with references despite less than 8% BLEU-1 overlap, a 90+ percentage point gap indicating that models preserve meaning through extensive paraphrasing rather than lexical reproduction. MoverScore (51-53%) confirms this pattern, occupying an intermediate position that reflects the optimal transport cost of semantic alignment. Furthermore, model scale does not predict performance: Mistral-7B (7B parameters) outperforms GPT-OSS-20B (20B parameters) across all metrics. NLI analysis reveals that contradiction rates remain below 7%, suggesting models rarely generate content that directly conflicts with human consensus. These findings challenge the reliability of lexical metrics for evaluating abstractive generation and argue for multi-dimensional evaluation frameworks that capture semantic fidelity beyond surface-level text matching. The RECOM dataset is publicly available at https://anonymous.4open.science/r/recom-D4B0", "AI": {"tldr": "RECOM is a benchmark of 15,000 recent Reddit questions used to test how well LLMs align with human answers on up-to-date, open-domain queries, revealing that standard lexical metrics fail to capture their strong semantic alignment.", "motivation": "Existing LLM evaluation for open-domain QA underemphasizes how models handle very recent, real-world questions and how well automatic metrics reflect alignment with human answers, especially when models paraphrase heavily. The authors aim to fill this gap by providing a temporally recent, community-answered dataset and systematically analyzing metric behavior and model performance.", "method": "The authors construct RECOM, a dataset of 15,000 Reddit questions from September 2025 with community reference answers. They evaluate four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, GPT-OSS-20B) on this benchmark using multiple metric families: lexical overlap (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical consistency via Natural Language Inference (NLI) between model and reference answers.", "result": "They observe a large discrepancy between semantic and lexical metrics: all models have over 99% cosine similarity with references but under 8% BLEU-1, with MoverScore around 51\u201353%, indicating strong semantic alignment despite low word overlap. Smaller models like Mistral-7B outperform the larger GPT-OSS-20B across metrics, and NLI analysis shows contradiction rates below 7%, meaning explicit disagreement with human answers is rare.", "conclusion": "Lexical metrics like BLEU and ROUGE are unreliable for evaluating abstractive, paraphrastic LLM outputs on open-domain QA, as they underestimate semantic alignment. Effective evaluation must be multidimensional, incorporating semantic and logical measures. The RECOM dataset facilitates more realistic, temporally grounded assessment of LLM alignment with human perspectives on recent information."}}
{"id": "2602.11351", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11351", "abs": "https://arxiv.org/abs/2602.11351", "authors": ["Yihang Yao", "Zhepeng Cen", "Haohong Lin", "Shiqi Liu", "Zuxin Liu", "Jiacheng Zhu", "Zhang-Wei Hong", "Laixi Shi", "Ding Zhao"], "title": "Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization", "comment": null, "summary": "Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.", "AI": {"tldr": "BAO is an agentic reinforcement learning framework for proactive LLM agents that improves task performance and user alignment in multi-turn interactions by enhancing useful proactive behaviors and regularizing inefficient ones.", "motivation": "Existing proactive LLM agents trained with agentic RL struggle to balance task success with user satisfaction: being too passive fails to capture user intentions, while querying users too often or redundantly hurts engagement. There is a need for a training framework that can shape multi-turn interaction strategies toward both effectiveness and user alignment.", "method": "BAO introduces a two-part RL-based training framework for LLM agents in multi-turn settings. Behavior enhancement encourages rich proactive reasoning and effective information gathering, while behavior regularization penalizes inefficient, redundant, or misaligned interactions. The approach is instantiated within an agentic RL pipeline and trained on multi-turn tasks from the UserRL benchmark suite, with rewards designed to capture both task performance and user-centric interaction quality.", "result": "On the UserRL benchmark tasks, BAO significantly outperforms strong proactive agentic RL baselines on task performance metrics, while also achieving comparable or better performance than commercial LLM agents. This shows that BAO-trained agents can be both more effective and better aligned with user expectations in complex multi-turn scenarios.", "conclusion": "BAO offers an effective agentic RL framework for training proactive LLM agents that balance proactive reasoning and user-centric interaction quality. By jointly enhancing useful proactive behaviors and regularizing inefficient ones, BAO leads to agents that perform better on multi-turn tasks and deliver interaction quality competitive with or superior to commercial LLM systems."}}
{"id": "2602.11166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11166", "abs": "https://arxiv.org/abs/2602.11166", "authors": ["Xu Hu", "Yifan Zhang", "Songtao Wei", "Chen Zhao", "Qiannan Li", "Bingzhe Li", "Feng Chen"], "title": "Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?", "comment": "18 pages, 13 figures, 8 tables", "summary": "Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, especially on QA datasets. In this work, we systematically investigate the impact of PEFT on hallucination detection through a comprehensive empirical study across three open-weight LLM backbones and three fact-seeking QA benchmarks. For each model, we evaluate performance using seven unsupervised hallucination detection methods spanning three complementary approaches: semantic consistency based detectors, confidence based detectors, and entropy based detectors. This multifaceted evaluation enables us to characterize how PEFT reshapes uncertainty across different detection paradigms. In conclusion, our experimental results show that PEFT consistently strengthens hallucination detection ability, substantially improving AUROC across a wide range of hallucination detectors. Besides, further analyses using linear probes and representation diagnostics indicate that PEFT methods primarily reshapes how uncertainty is encoded and surfaced, comparing with injecting new factual knowledge into the models.", "AI": {"tldr": "The paper studies how parameter-efficient fine-tuning (PEFT) affects hallucination detection in large language models on QA tasks, finding that PEFT consistently improves the ability of various unsupervised detectors to recognize hallucinations by reshaping uncertainty representations rather than mainly adding new facts.", "motivation": "Although PEFT is widely used to adapt LLMs and is often assumed to improve factuality, its concrete impact on hallucination behavior\u2014especially how well hallucinations can be detected in QA settings\u2014remains unclear. There is a lack of systematic, empirical understanding of how PEFT interacts with different hallucination detection paradigms and how it changes the model's internal uncertainty representations.", "method": "The authors fine-tune three open-weight LLM backbones using parameter-efficient fine-tuning methods on fact-seeking QA benchmarks and then evaluate hallucination detection performance using seven unsupervised detectors. These detectors span three complementary families: semantic-consistency-based methods, confidence-based methods, and entropy-based methods. They then compare AUROC scores before and after PEFT, and analyze internal representations using linear probes and other diagnostics to understand how uncertainty is encoded and surfaced post-PEFT.", "result": "Across three LLM backbones and three QA benchmarks, PEFT consistently improves the effectiveness of all evaluated unsupervised hallucination detection methods, yielding notable gains in AUROC. Representation-level analyses reveal that PEFT predominantly alters how uncertainty is represented and exposed to detectors, rather than significantly enriching the model with additional factual knowledge.", "conclusion": "PEFT systematically enhances hallucination detection across diverse unsupervised detectors by reshaping the model's uncertainty representations. The gains are robust across multiple LLM backbones and QA datasets, suggesting that PEFT's primary contribution in this context is making uncertainty more legible and separable for detectors, rather than directly injecting new factual information. This insight informs both the design of PEFT strategies and the development of more effective hallucination detection methods for LLMs."}}
{"id": "2602.11354", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11354", "abs": "https://arxiv.org/abs/2602.11354", "authors": ["Bang Nguyen", "Dominik So\u00f3s", "Qian Ma", "Rochana R. Obadage", "Zack Ranjan", "Sai Koneru", "Timothy M. Errington", "Shakhlo Nematova", "Sarah Rajtmajer", "Jian Wu", "Meng Jiang"], "title": "ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences", "comment": null, "summary": "The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.", "AI": {"tldr": "ReplicatorBench is a new benchmark and ReplicatorAgent is a baseline AI agent framework to evaluate and support end-to-end replication of social and behavioral science research, including both replicable and non-replicable claims.", "motivation": "Existing AI evaluation benchmarks for assessing scientific papers concentrate on computational reproducibility, assuming access to original code and data and focusing on final outcomes. This misses real-world replication challenges: new or unavailable data, non-replicable studies, and the reasoning process of replication. There is a need for a benchmark that reflects realistic replication workflows and includes both successful and failed replications as ground truth.", "method": "The authors build ReplicatorBench, a benchmark of human-verified replicable and non-replicable research claims from social and behavioral sciences, and structure it into three stages: (1) extraction and retrieval of relevant replication data and resources from the web and other sources; (2) design and execution of computational experiments in sandboxed environments; and (3) interpretation of results to decide whether a claim replicates. They also implement ReplicatorAgent, an agentic framework that uses large language models with tools (e.g., web search, coding environments) to carry out these tasks. They then systematically evaluate ReplicatorAgent using four different LLM backends, varying programming languages and code-access conditions.", "result": "ReplicatorBench provides a diverse set of tasks that require end-to-end replication activities, including identification of non-replicable claims. Experiments with ReplicatorAgent show that current LLM-based agents perform reasonably well at designing and running computational experiments but face substantial difficulties in retrieving necessary external resources, especially new data, to perform replications. Performance is further analyzed under different LLMs, coding languages, and code-access setups.", "conclusion": "Realistic evaluation of AI agents for scientific assessment requires benchmarks that address full replication workflows and include both replicable and non-replicable research. ReplicatorBench and ReplicatorAgent together expose current strengths (experimental design and execution) and weaknesses (data and resource retrieval) of LLM agents, and establish a foundation for future work on more capable, realistic research replication systems."}}
{"id": "2602.11167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11167", "abs": "https://arxiv.org/abs/2602.11167", "authors": ["Nathan Mao", "Varun Kaushik", "Shreya Shivkumar", "Parham Sharafoleslami", "Kevin Zhu", "Sunishchal Dev"], "title": "Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering", "comment": null, "summary": "Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.", "AI": {"tldr": "FalseCite is a benchmark dataset to study and evaluate hallucinations in LLMs caused by misleading or fabricated citations.", "motivation": "LLMs frequently hallucinate, particularly in safety-critical areas like medicine and law, and deceptive citations can exacerbate this problem. There is a need for a systematic way to trigger, measure, and analyze hallucinations, especially those driven by citation-like inputs.", "method": "The authors build FalseCite, a curated dataset of prompts with misleading or fake citations designed to induce hallucinations. They evaluate several LLMs (GPT-4o-mini, Falcon-7B, Mistral 7B) on this dataset, measure hallucination rates for false claims with deceptive citations, and analyze internal model behavior by visualizing and clustering hidden state vectors from hallucinating vs. non-hallucinating responses.", "result": "False claims with deceptive citations substantially increase hallucination rates across evaluated models, with GPT-4o-mini being particularly affected. Hidden state vectors of model activations exhibit a characteristic horn-like shape in the embedding space for both hallucinated and non-hallucinated responses.", "conclusion": "FalseCite is a useful benchmark for provoking and quantifying citation-driven hallucinations in LLMs and provides a basis for analyzing internal model states. It can serve as a foundation for developing better evaluation methods and mitigation strategies for hallucinations in future LLM research."}}
{"id": "2602.11389", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11389", "abs": "https://arxiv.org/abs/2602.11389", "authors": ["Heejeong Nam", "Quentin Le Lidec", "Lucas Maes", "Yann LeCun", "Randall Balestriero"], "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions", "comment": "Project Page: https://hazel-heejeong-nam.github.io/cjepa/", "summary": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.", "AI": {"tldr": "The paper introduces C-JEPA, an object-centric world model that uses object-level masking in joint embedding prediction to improve relational and counterfactual reasoning, enabling more efficient planning and better visual question answering performance.", "motivation": "Existing world models often rely on object-centric representations, but these alone fail to capture interaction-dependent dynamics and robust relational understanding needed for prediction, reasoning, and control. Patch-based masked prediction methods can miss the causal and counterfactual structure of interactions and may exploit shortcuts, limiting generalization and reasoning abilities. The authors aim to build a world model that explicitly forces learning of interactions between objects and better supports counterfactual reasoning and efficient control.", "method": "They extend masked joint embedding prediction architectures from image patches to object-centric representations, introducing C-JEPA. The core method applies object-level masking so that an individual object's state must be inferred from other objects rather than from local appearance alone. This masking acts as a latent intervention that disrupts shortcut cues and encourages the model to infer interaction dynamics and relational structure. The approach is evaluated in visual question answering tasks, focusing on counterfactual reasoning benchmarks, and in agent control tasks where a planner operates over the learned world model. They also provide a formal analysis, showing that object-level masking corresponds to latent interventions that impose a causal inductive bias.", "result": "On visual question answering benchmarks, especially those involving counterfactual reasoning, C-JEPA consistently outperforms the same architecture without object-level masking, with around 20% absolute improvement in counterfactual VQA accuracy. In control tasks, planning with C-JEPA requires only about 1% of the latent input features compared with patch-based world models, yet achieves comparable control performance. The theoretical analysis supports that object-level masking introduces a beneficial causal inductive bias via latent interventions.", "conclusion": "C-JEPA demonstrates that extending joint embedding prediction to object-centric representations with object-level masking yields a world model that better captures interactions, improves counterfactual and relational reasoning, and enables more efficient planning in control tasks. The empirical gains and formal analysis suggest that inducing causal inductive biases through latent interventions at the object level is a powerful and scalable approach for building more capable world models."}}
{"id": "2602.11168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11168", "abs": "https://arxiv.org/abs/2602.11168", "authors": ["Jingyan Xu", "Marcelo L. LaFleur", "Christina Schweikert", "D. Frank Hsu"], "title": "Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI", "comment": "8 pages, 8 figures, 4 tables; Accepted to 2025 IEEE International Conference on Pervasive Intelligence and Computing (PICom 2025)", "summary": "(Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a challenging problem in cases where the categories are unavailable, difficult to differentiate, or are interrelated. Social analysis with human context is an area that can benefit from text classification, as it relies substantially on text data. The focus of this paper is to enhance the classification of text according to the UN's Sustainable Development Goals (SDGs) by collecting and combining intelligence from multiple models. Combinatorial Fusion Analysis (CFA), a system fusion paradigm using a rank-score characteristic (RSC) function and cognitive diversity (CD), has been used to enhance classifier methods by combining a set of relatively good and mutually diverse classification models. We use a generative AI model to generate synthetic data for model training and then apply CFA to this classification task. The CFA technique achieves 96.73% performance, outperforming the best individual model. We compare the outcomes with those obtained from human domain experts. It is demonstrated that combining intelligence from multiple ML/AI models using CFA and getting input from human experts can, not only complement, but also enhance each other.", "AI": {"tldr": "The paper enhances text classification for UN SDGs by combining multiple diverse NLP models using Combinatorial Fusion Analysis (CFA), supported by generative AI\u2013based data augmentation and expert comparison, achieving superior accuracy.", "motivation": "Classifying text into complex, interrelated categories like the UN Sustainable Development Goals is difficult, yet crucial for social analysis, information retrieval, and decision-making. Existing single-model NLP approaches struggle when labels are subtle, overlapping, or hard to define, motivating a method that can better capture such complexity by integrating multiple sources of intelligence.", "method": "The authors generate synthetic training data using a generative AI model, train multiple relatively strong but diverse text classification models for SDG labeling, and then apply Combinatorial Fusion Analysis (CFA). CFA uses a rank-score characteristic (RSC) function and cognitive diversity (CD) to fuse the outputs of these models into a single, improved decision. The fused system\u2019s performance is evaluated and compared against individual models and human experts.", "result": "The CFA-based fusion of multiple classifiers achieves 96.73% performance (likely accuracy or a similar metric), surpassing the best individual model. The results are also qualitatively and/or quantitatively compared against the judgments of human domain experts in SDG classification.", "conclusion": "Combining multiple ML/AI models via CFA, especially when supported by generative data augmentation, can significantly improve SDG-related text classification and even complement and enhance human expert analysis. Model fusion with attention to cognitive diversity is an effective strategy for complex, interrelated category structures like the SDGs."}}
{"id": "2602.11408", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.11408", "abs": "https://arxiv.org/abs/2602.11408", "authors": ["Michael Menezes", "Anastasios Kyrillidis"], "title": "GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation", "comment": "16 pages, 7 figures", "summary": "While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.", "AI": {"tldr": "The paper proposes GHOST, a structured pruning method for Mamba2 that cuts state dimension (and thus inference cost) roughly in half with minimal perplexity increase, using only forward-pass statistics.", "motivation": "Mamba2\u2019s larger state dimension improves temporal modeling but makes autoregressive inference bandwidth-bound and slow. Existing pruning methods are inadequate: unstructured sparsity doesn\u2019t reduce activation bandwidth, magnitude-based pruning ignores sequence dynamics, and gradient-based pruning is too expensive. There is a need for a pruning scheme that substantially reduces Mamba2\u2019s state size and runtime cost while preserving performance and remaining cheap to apply.", "method": "They introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework designed for Mamba2\u2019s state dimension. GHOST approximates control-theoretic balanced truncation but only relies on forward-pass statistics rather than gradients. It groups state dimensions and scores them jointly by measuring controllability (how much inputs excite states) and observability (how much states affect outputs). Based on these scores, it performs structured pruning of the state, yielding a smaller effective state dimension while keeping activations structured and bandwidth-efficient.", "result": "Across models from 130M to 2.7B parameters, GHOST achieves around 50% reduction in Mamba2 state dimension, with only about +1 perplexity point on WikiText-2. It matches or rivals gradient-based pruning methods in fidelity despite not using backpropagation, and overcomes bandwidth saturation during autoregressive generation by making activations smaller and more efficient to move.", "conclusion": "GHOST provides a practical, efficient structured pruning procedure for Mamba2 that significantly reduces state dimension and inference bandwidth while maintaining language modeling quality. By leveraging forward-pass controllability/observability statistics to approximate balanced truncation, it offers gradient-free pruning with performance close to more expensive gradient-based methods, making large Mamba2 models more deployable in autoregressive settings."}}
{"id": "2602.11169", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11169", "abs": "https://arxiv.org/abs/2602.11169", "authors": ["Mangadoddi Srikar Vardhan", "Lekkala Sai Teja"], "title": "Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis", "comment": "15 pages, 7 figures. will Submit to ICML 2026", "summary": "Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we discover a striking cross-over dissociation: angular perturbations cause up to 42.9 more damage to language modeling loss, while magnitude perturbations cause disproportionately more damage to syntactic processing (20.4% vs.1.6% accuracy drop on subject-verb agreement).This finding is enabled by L2-matched perturbation analysis, a methodology ensuring that an gular and magnitude perturbations achieve identical Euclidean displacements. Causal intervention reveals that angular damage flows substantially through the attention pathways (28.4% loss recovery via attention repair), while magnitude damage flows partly through the LayerNorm pathways(29.9% recovery via LayerNorm repair). These patterns replicate across scales within the Pythia architecture family. These findings provide evidence that direction and magnitude support partially distinct computational roles in LayerNorm based architectures. The direction preferentially affects attentional routing, while magnitude modulates processing intensity for fine-grained syntactic judgments. We find different patterns in RMSNorm-based architectures, suggesting that the dissociation depends on architectural choices. Our results refine the linear representation hypothesis and have implications for model editing and interpretability research", "AI": {"tldr": "The paper studies how direction and magnitude of transformer hidden-state vectors play distinct functional roles.", "motivation": "While transformer representations are known to be linear and high-dimensional, it is unclear whether the orientation (direction) and size (norm) of these vectors have separable computational functions.", "method": "The authors analyze Pythia-family language models by applying carefully controlled perturbations to hidden states: angular (changing direction) and magnitude (changing norm) with matched L2 distance. They then measure the impact on language modeling loss and syntactic tasks, and perform causal interventions that selectively repair attention or LayerNorm pathways. They also compare with RMSNorm-based architectures.", "result": "Angular perturbations harm language modeling loss much more than magnitude perturbations, whereas magnitude perturbations disproportionately disrupt syntactic processing (e.g., larger accuracy drops on subject-verb agreement). Attention-pathway repairs recover much of the angular perturbation damage, while LayerNorm-pathway repairs recover much of the magnitude-related damage. These effects replicate across Pythia scales, but RMSNorm models show different patterns.", "conclusion": "Direction and magnitude in transformer hidden states support partially distinct computational roles in LayerNorm-based architectures: directions primarily govern attentional routing, while magnitudes modulate processing strength relevant for fine-grained syntax. This refines the linear representation hypothesis and suggests new levers for model editing and interpretability, with architectural norms (LayerNorm vs RMSNorm) influencing this dissociation."}}
{"id": "2602.11409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11409", "abs": "https://arxiv.org/abs/2602.11409", "authors": ["Sina Tayebati", "Divake Kumar", "Nastaran Darabi", "Davide Ettori", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "title": "TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning", "comment": null, "summary": "Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $\u03c4^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.", "AI": {"tldr": "Introduces TRACER, a trajectory-level uncertainty metric for AI agents in multi-turn human-tool interactions, improving early failure detection over existing single-shot uncertainty proxies.", "motivation": "Real-world AI agents using tools in multi-turn interactions can fail due to rare but critical trajectory-level issues (e.g., looping, incoherent tool use, or miscoordination with users), which are not well captured by existing uncertainty measures that focus on single text generations rather than entire interaction trajectories.", "method": "Proposes TRACER, an uncertainty metric designed for dual-control Tool-Agent-User interactions that operates on whole trajectories. TRACER combines content-aware surprisal, situational-awareness indicators, measures of semantic and lexical repetition, and detection of tool-grounded coherence gaps, then aggregates these via a tail-focused risk functional with a MAX-composite step risk to highlight decisive anomalies.", "result": "On the \u03c4^2-bench benchmark for tool-use conversations, TRACER significantly outperforms baseline uncertainty proxies in predicting task failures and enabling selective task execution, achieving up to 37.1% AUROC and 55% AUARC improvements.", "conclusion": "Trajectory-level uncertainty estimation using TRACER enables earlier and more accurate detection of failures in complex conversational tool-use scenarios than existing single-shot methods, and the authors release code and a benchmark to support further research."}}
{"id": "2602.11170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11170", "abs": "https://arxiv.org/abs/2602.11170", "authors": ["Jiawei Xu", "Zhenyu Yu", "Ziqian Bi", "Minh Duc Pham", "Xiaoyi Qu", "Danyang Zhang"], "title": "PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models", "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.", "AI": {"tldr": "The paper introduces PRIME, a multi-agent reinforcement learning framework that dramatically boosts large language models' performance on algorithmic reasoning tasks, and PRIME-Bench, a large benchmark to evaluate such skills.", "motivation": "Although large language models excel at many reasoning tasks, they perform poorly on algorithmic reasoning that requires precise, long-horizon, stateful computation. The authors want a method that can reliably execute and verify long algorithmic traces and a benchmark that systematically tests these capabilities.", "method": "They design PRIME (Policy-Reinforced Iterative Multi-agent Execution), which uses three specialized agents: an executor that performs step-by-step reasoning, a verifier that checks constraints and correctness at each step, and a coordinator that decides when to backtrack and iterate. The whole system is trained with group relative policy optimization. Additionally, they construct PRIME-Bench, a large benchmark of 86 algorithmic tasks with 51,600 instances spanning sorting, graphs, trees, automata, symbolic reasoning, and puzzle-like constraints, many requiring extremely long execution traces.", "result": "On PRIME-Bench, PRIME improves average accuracy from 26.8% to 93.8% over a baseline, with especially dramatic gains on tasks demanding long-term state tracking such as Turing machine simulation (9%\u219292%) and long division (16%\u219294%). Ablation studies show that iterative verification is the main factor behind these gains, by stopping error cascades that ruin baseline solutions. Scaling experiments from 8B to 120B parameters show that smaller models benefit the most, reaching performance comparable to models 8x larger when using PRIME.", "conclusion": "Multi-agent, verification-centric reinforcement learning can transform LLMs into far more reliable algorithmic reasoners, especially for long-horizon, stateful tasks. PRIME and PRIME-Bench together provide both a strong method and a rigorous evaluation framework, and the approach can effectively compensate for smaller model sizes by orchestrating structured execution, verification, and backtracking."}}
{"id": "2602.11437", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.11437", "abs": "https://arxiv.org/abs/2602.11437", "authors": ["Chengrui Qu", "Christopher Yeh", "Kishan Panaganti", "Eric Mazumdar", "Adam Wierman"], "title": "Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization", "comment": "ICLR 2026", "summary": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.", "AI": {"tldr": "They propose a distributionally robust extension of the IGM principle for cooperative MARL and derive robust variants of popular value-factorization methods that improve out-of-distribution performance.", "motivation": "Standard cooperative MARL methods that use centralized training with decentralized execution rely on the individual-global-maximum (IGM) principle, but their performance degrades in real-world settings with uncertainties such as sim-to-real gaps, model mismatch, and noise. There is a need for principled robustness in such multi-agent systems.", "method": "They define a new Distributionally robust IGM (DrIGM) principle, introducing robust individual action values such that each agent\u2019s robust greedy action aligns with the robust team-optimal joint action. Using this theoretical foundation, they construct DrIGM-compliant robust variants of common value-factorization architectures (VDN, QMIX, QTRAN) that train on robust Q-targets, remain scalable, and work with existing implementations without extra reward shaping.", "result": "They prove that DrIGM holds for their robust individual action value definition, providing a robustness guarantee for the overall system. Empirically, their robust variants consistently improve out-of-distribution performance in high-fidelity SustainGym simulators and a StarCraft environment.", "conclusion": "Distributionally robust value-factorization for cooperative MARL is both theoretically sound and practically effective. By enforcing DrIGM and training with robust Q-targets, one can obtain scalable, drop-in robust extensions of existing MARL architectures that yield better performance under distribution shifts and environmental uncertainties."}}
{"id": "2602.11171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11171", "abs": "https://arxiv.org/abs/2602.11171", "authors": ["Baek Seong-Eun", "Lee Jung-Mok", "Kim Sung-Bin", "Tae-Hyun Oh"], "title": "Efficient Hyper-Parameter Search for LoRA via Language-aided Bayesian Optimization", "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enables resource-efficient personalization or specialization, but it comes at the expense of additional hyperparameter tuning. Although LoRA makes fine-tuning efficient, it is highly sensitive to the choice of hyperparameters, and exhaustive hyperparameter search is still computationally very demanding. To address these challenges, we propose a framework that integrates the domain knowledge of pre-trained LLMs into Bayesian Optimization (BO) to efficiently search for LoRA hyperparameters. To leverage the informed knowledge of LLMs, we repurpose LLMs as a discrete-to-continuous mapping to link the hyperparameters and their domain knowledge with a continuous vector space, where BO is conducted. We design and control the mapping by language prompting, where we provide a domain-aware textual prompt describing the relationships among hyperparameters and their respective roles; thereby, we explicitly inject domain knowledge about LoRA into the LLM in natural language. Also, we model the residual information that is hard to linguistically describe in the prompt with an additional learnable token. This aids BO to sample more high-performing hyperparameters. In addition, by leveraging the observation of the strong correlation between the respective performance obtained from full and subset training datasets in LoRA training regimes, we introduce proxy training and evaluation with a data subset. This further increases the efficiency of our method. We demonstrate that our hyperparameter found with only about 30 iterations achieves more than 20% performance improvement over standard hyperparameters found from about 45,000 combinations.", "AI": {"tldr": "The paper proposes a Bayesian Optimization framework that uses LLM-based domain knowledge prompts to efficiently search LoRA hyperparameters, plus proxy training on data subsets, achieving large performance gains with few trials.", "motivation": "LoRA enables efficient fine-tuning of LLMs but is extremely sensitive to hyperparameters, making exhaustive search over hyperparameter configurations computationally prohibitive. The authors want a more sample- and compute-efficient way to find good LoRA hyperparameters by leveraging the prior knowledge already encoded in pre-trained LLMs.", "method": "They build a BO framework where an LLM is repurposed as a discrete-to-continuous mapping: textual, domain-aware prompts describe the roles and relationships of LoRA hyperparameters, and the LLM outputs continuous embeddings that represent hyperparameter configurations. BO is then performed in this continuous space. To capture aspects of hyperparameters that are hard to express in language, they add a learnable residual token. Additionally, they exploit the strong correlation between performance on full vs. subset training in LoRA regimes and introduce proxy training/evaluation on data subsets to further cut search cost.", "result": "With only ~30 BO iterations using their LLM-informed mapping and proxy evaluation on subsets, the method finds LoRA hyperparameters that outperform standard hyperparameters derived from an exhaustive search over ~45,000 combinations by more than 20% in performance metrics.", "conclusion": "Incorporating domain knowledge from pre-trained LLMs into Bayesian Optimization, together with subset-based proxy evaluation, yields a highly efficient and effective framework for tuning LoRA hyperparameters, drastically reducing search cost while improving performance over exhaustive manual tuning."}}
{"id": "2602.11455", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11455", "abs": "https://arxiv.org/abs/2602.11455", "authors": ["Zhengbo Jiao", "Shaobo Wang", "Zifan Zhang", "Wei Wang", "Bing Zhao", "Hu Wei", "Linfeng Zhang"], "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning", "comment": "20pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.", "AI": {"tldr": "The paper studies how multimodal large language models actually use visual information under RL with verifiable rewards, finding that only a small portion of tokens strongly connect text and vision, and proposes a method (AT-RL) that focuses RL updates on these anchor tokens to improve performance efficiently.", "motivation": "Although RL with verifiable rewards has improved multimodal reasoning, it remains unclear which parts of the text sequence truly interact with visual inputs and how credit assignment in RL exploits this interaction. Understanding this mechanism could enable more efficient and effective training by focusing on the elements that genuinely ground reasoning in images.", "method": "The authors analyze cross-modal attention patterns in RLVR-trained MLLMs to identify tokens with strong visual-textual coupling, treating these as \"anchor\" tokens. They study how credit assignment during RLVR training concentrates on these anchors over time. Based on this, they design Anchor-Token Reinforcement Learning (AT-RL), which uses graph-based clustering of the attention topology to detect high-connectivity tokens and selectively reinforces them during RL, while keeping additional computational overhead minimal.", "result": "Empirically, they find that only about 15% of tokens display strong cross-modal connectivity and that RL training naturally sharpens the visual grounding of these tokens. Using AT-RL across a size range of models (3B\u201332B) adds about 1.2% computational cost yet allows the 32B model to outperform a larger 72B-Instruct baseline on MathVista, achieving a score of 80.2, with consistent improvements across STEM, video, and general tasks. In contrast, training focused only on low-connectivity tokens leads to substantial performance degradation.", "conclusion": "Effective multimodal RL is primarily determined by accurately assigning credit to a small set of visually grounded anchor tokens, rather than uniformly updating all tokens. By explicitly modeling and reinforcing these high-connectivity tokens through attention-topology-based clustering, AT-RL improves multimodal reasoning quality with minimal overhead and shows that the fidelity of cross-modal anchoring is more crucial than token quantity."}}
{"id": "2602.11172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11172", "abs": "https://arxiv.org/abs/2602.11172", "authors": ["Aniket Deroy"], "title": "Synthesizing the Virtual Advocate: A Multi-Persona Speech Generation Framework for Diverse Linguistic Jurisdictions in Indic Languages", "comment": null, "summary": "Legal advocacy requires a unique combination of authoritative tone, rhythmic pausing for emphasis, and emotional intelligence. This study investigates the performance of the Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS models in generating synthetic courtroom speeches across five Indic languages: Tamil, Telugu, Bengali, Hindi, and Gujarati. We propose a prompting framework that utilizes Gemini 2.5s native support for 5 languages and its context-aware pacing to produce distinct advocate personas. The evolution of Large Language Models (LLMs) has shifted the focus of TexttoSpeech (TTS) technology from basic intelligibility to context-aware, expressive synthesis. In the legal domain, synthetic speech must convey authority and a specific professional persona a task that becomes significantly more complex in the linguistically diverse landscape of India. The models exhibit a \"monotone authority,\" excelling at procedural information delivery but struggling with the dynamic vocal modulation and emotive gravitas required for persuasive advocacy. Performance dips in Bengali and Gujarati further highlight phonological frontiers for future refinement. This research underscores the readiness of multilingual TTS for procedural legal tasks while identifying the remaining challenges in replicating the persuasive artistry of human legal discourse. The code is available at-https://github.com/naturenurtureelite/Synthesizing-the-Virtual-Advocate/tree/main", "AI": {"tldr": "Study evaluates Gemini 2.5 Flash/Pro TTS for synthetic courtroom speeches in five Indic languages, finding they handle procedural authority well but struggle with persuasive, emotive advocacy, especially in Bengali and Gujarati.", "motivation": "To examine whether modern multilingual TTS systems, powered by Gemini 2.5, can realistically generate authoritative, persuasive courtroom speeches across multiple Indic languages, addressing the need for context-aware, expressive legal audio in a linguistically diverse setting.", "method": "Use Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS to synthesize courtroom-style speeches in Tamil, Telugu, Bengali, Hindi, and Gujarati, guided by a custom prompting framework that leverages the models' native multilingual support and context-aware pacing to create different advocate personas; then qualitatively evaluate their expressiveness, authority, and language-specific performance.", "result": "Both TTS models reliably generate intelligible, authoritative-sounding speech suitable for procedural/legal information, but the output tends toward a 'monotone authority' that lacks dynamic vocal modulation and emotional depth; performance is notably weaker in Bengali and Gujarati, exposing phonological limitations.", "conclusion": "Multilingual TTS based on Gemini 2.5 is close to ready for procedural legal applications (e.g., standardized announcements, instructions) but is not yet capable of fully emulating the nuanced, persuasive delivery and emotional resonance of human courtroom advocates, especially in some Indic languages, where further phonological and prosodic improvements are needed."}}
{"id": "2602.11510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11510", "abs": "https://arxiv.org/abs/2602.11510", "authors": ["Faouzi El Yagoubi", "Ranwa Al Mallah", "Godwin Badu-Marfo"], "title": "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems", "comment": "17 pages, 10 figures, 13 tables. Code and dataset available at https://github.com/Privatris/AgentLeak", "summary": "Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.", "AI": {"tldr": "AgentLeak is a benchmark to measure privacy leakage in multi-agent LLM systems, focusing on internal communication channels often ignored by traditional audits.", "motivation": "Existing privacy evaluations focus on model outputs and ignore internal channels such as inter-agent messages and tool arguments in multi-agent LLM systems, leaving critical privacy risks unmeasured.", "method": "The authors build AgentLeak, a full-stack privacy benchmark with 1,000 scenarios across sensitive domains, define a 32-class attack taxonomy, and implement a three-tier detection pipeline to analyze privacy leakage across external outputs and internal communication channels in multiple LLMs and configurations.", "result": "Experiments on GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B over 4,979 traces show that while multi-agent setups reduce leakage in the external output channel, they introduce significant leakage in internal channels, particularly inter-agent messages; overall, 68.9% of scenarios show leakage when internal channels are considered, and output-only audits miss 41.7% of violations. Claude 3.5 Sonnet shows the lowest leakage on both external and internal channels, indicating that strong safety alignment can help protect internal channels too.", "conclusion": "Internal communication in multi-agent LLM systems is the main source of privacy leakage, so relying only on output audits is insufficient. New coordination frameworks and safety mechanisms must explicitly monitor and constrain internal channels and inter-agent communication to ensure privacy."}}
{"id": "2602.11173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11173", "abs": "https://arxiv.org/abs/2602.11173", "authors": ["Qian Ruan", "Iryna Gurevych"], "title": "Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review", "comment": null, "summary": "Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess domain expertise, author-only information, revision and response strategies--concrete forms of author expertise and intent--to address reviewer concerns, and seek NLP assistance that integrates these signals to support effective response writing in peer review. We reformulate author response generation as an author-in-the-loop task and introduce REspGen, a generation framework that integrates explicit author input, multi-attribute control, and evaluation-guided refinement, together with REspEval, a comprehensive evaluation suite with 20+ metrics covering input utilization, controllability, response quality, and discourse. To support this formulation, we construct Re$^3$Align, the first large-scale dataset of aligned review--response--revision triplets, where revisions provide signals of author expertise and intent. Experiments with state-of-the-art LLMs show the benefits of author input and evaluation-guided refinement, the impact of input design on response quality, and trade-offs between controllability and quality. We make our dataset, generation and evaluation tools publicly available.", "AI": {"tldr": "The paper introduces an author-in-the-loop framework (REspGen) and an evaluation suite (REspEval) for generating peer-review author responses that explicitly leverage author expertise, intent, and revision information, backed by a new large-scale review\u2013response\u2013revision dataset (Re^3Align).", "motivation": "Existing automatic author-response generation work treats the task as generic text generation and largely ignores crucial author-specific signals such as domain expertise, private information, and deliberate revision/response strategies. Authors, however, want NLP tools that help them respond effectively to reviewers while retaining control and incorporating their own intentions, so there is a need for a controllable, author-aware generation framework and a way to evaluate it comprehensively.", "method": "The authors reformulate response generation as an author-in-the-loop process. They build REspGen, which: (1) accepts explicit author inputs (e.g., intent, strategies, revision plans), (2) supports multi-attribute controllable generation, and (3) uses evaluation-guided refinement to iteratively improve responses. They also design REspEval, an evaluation suite with 20+ automatic metrics that measure how well the system uses author inputs, how controllable it is, and the quality and discourse properties of generated responses. To enable training and analysis, they construct Re^3Align, a large-scale dataset of aligned review\u2013response\u2013revision triplets where the revision side encodes author expertise and intent signals.", "result": "Experiments with state-of-the-art LLMs under the REspGen framework demonstrate that incorporating explicit author input and evaluation-guided refinement improves response quality, that how the input is designed significantly affects performance, and that there are observable trade-offs between controllability and response quality. The dataset and tools enable systematic empirical analysis of these aspects.", "conclusion": "Author response generation is better modeled as an author-in-the-loop, controllable generation problem that explicitly leverages author expertise and intent. The proposed REspGen framework, REspEval evaluation suite, and Re^3Align dataset together provide infrastructure showing that integrating author input and metric-guided refinement yields more effective, controllable responses, and they are released to support future research and practical tools for peer-review assistance."}}
{"id": "2602.11516", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11516", "abs": "https://arxiv.org/abs/2602.11516", "authors": ["Hong Su"], "title": "Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems", "comment": null, "summary": "Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.", "AI": {"tldr": "A human-inspired continuous learning framework that treats internal reasoning processes as learnable objects, optimizing both task performance and the evolution of cognitive structures, demonstrated on sensor anomaly detection with runtime gains.", "motivation": "Existing AI systems focus on task outputs or static knowledge and neglect continuous refinement of internal reasoning, action scheduling, and learning mechanisms, which is necessary for sustained adaptation in dynamic environments.", "method": "Propose a unified sequential reasoning model augmented with parallel learning that records internal reasoning trajectories and interactions as structured data, enabling optimization of reasoning organization, scheduling, evolution, controlled replacement of fixed logic with learned procedures, and a hierarchical meta-learning component that adapts both task parameters and learning strategies.", "result": "On a temperature sensor abnormality detection task, adding internal-process learning to the system decreases average runtime by 23.9%, indicating efficiency gains while maintaining stability.", "conclusion": "Treating internal reasoning processes as primary learning targets within a human-inspired continuous learning framework allows AI systems to evolve their internal cognitive architecture during execution, improving efficiency and adaptability without sacrificing operational stability."}}
{"id": "2602.11174", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11174", "abs": "https://arxiv.org/abs/2602.11174", "authors": ["Aradhya Dixit", "Shreem Dixit"], "title": "The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models", "comment": null, "summary": "Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic content. Across mBERT and XLM-R, the higher-fragmentation orthography shows a ~3.4x increase in fertility (6.73-6.85 vs. 2.10-2.35 tokens/word), leading to a 16.5x inference slowdown (0.23 vs. 3.8 sentences/second) on identical hardware. Using bits per character (BPC) to avoid the \"NLL paradox\" from subword fragmentation, we find a substantial increase in information cost: +19.7% for mBERT (8.06->9.65) and +47.1% for XLM-R (12.19->17.94). A round-trip conversion check (CER_rt=0.31) suggests these gaps reflect orthography-conditioned processing rather than mapping noise. Our results highlight tokenization as a key source of inequity in multilingual NLP and motivate script-aware tokenization and pretraining.", "AI": {"tldr": "The paper shows that multilingual language models, despite being considered script-agnostic, incur significant performance and efficiency penalties for some writing systems due to tokenizer behavior.", "motivation": "To investigate whether pretrained multilingual models treat different scripts fairly, and to quantify any hidden costs that tokenization imposes on certain orthographies with identical linguistic content.", "method": "The authors compare two orthographic variants with identical linguistic content, measure tokenization fragmentation (fertility), runtime speed, and information-theoretic metrics (bits per character) in mBERT and XLM-R. They also perform a round-trip conversion check to ensure differences are not due to script-mapping noise.", "result": "Higher-fragmentation orthography has ~3.4x higher fertility (6.73-6.85 vs. 2.10-2.35 tokens/word), causes a 16.5x slowdown in inference (0.23 vs. 3.8 sentences/second), and significantly raises information cost: +19.7% BPC for mBERT and +47.1% for XLM-R. The round-trip character error rate (CER_rt=0.31) indicates these are true orthography-conditioned effects rather than mapping noise.", "conclusion": "Tokenization differences across scripts introduce substantial inequities in multilingual NLP models, challenging the assumption of script-agnosticism and motivating the design of script-aware tokenization and pretraining strategies to reduce these disparities."}}
{"id": "2602.11527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11527", "abs": "https://arxiv.org/abs/2602.11527", "authors": ["Jiawei Zhu", "Wei Chen", "Ruichu Cai"], "title": "CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference", "comment": "Accepted by IUI 2026", "summary": "Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.", "AI": {"tldr": "CausalAgent is a conversational multi-agent system that automates end-to-end causal inference from raw data to interpretable reports via natural language interaction.", "motivation": "Traditional causal inference workflows are technically demanding, requiring expertise in both statistics and computer science, manual algorithm selection, data quality handling, and interpretation of complex outputs, which creates high barriers for domain experts who lack such training.", "method": "The authors design CausalAgent, a user-centered, conversational multi-agent system that combines Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP). It coordinates specialized agents to handle stages such as data cleaning, causal structure learning, bias correction, and report generation, with results delivered and refined via natural language dialogue and interactive visualizations.", "result": "CausalAgent can take an uploaded dataset and user\u2019s natural language questions, automatically execute the full causal analysis pipeline, and produce rigorous, interactive analysis reports with visualizations, thereby lowering the technical barrier for performing causal inference.", "conclusion": "By explicitly modeling the causal analysis workflow in a conversational multi-agent framework and enhancing it with RAG, MCP, and visual interfaces, CausalAgent offers a novel, accessible paradigm for human-AI collaboration in causal inference that maintains rigor and interpretability while greatly simplifying user interaction."}}
{"id": "2602.11175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11175", "abs": "https://arxiv.org/abs/2602.11175", "authors": ["Michelle Yuan", "Weiyi Sun", "Amir H. Rezaeian", "Jyotika Singh", "Sandip Ghoshal", "Yao-Ting Wang", "Miguel Ballesteros", "Yassine Benajiba"], "title": "Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth", "comment": "Accepted to EACL 2026 Main Conference", "summary": "Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent studies from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.", "AI": {"tldr": "Survey of theoretical limits of transformers on discrete reasoning tasks.", "motivation": "Transformers excel at many sequence modeling tasks but perform poorly or unreliably on exact discrete reasoning such as arithmetic and algorithmic composition. Theoretical understanding of these failures is fragmented across subfields, making it hard to see the big picture.", "method": "Conducts a survey of recent theoretical work analyzing transformers through three lenses: circuit complexity, approximation theory, and communication complexity. Reviews definitions, key theorems, and examples, and compares results across these frameworks.", "result": "Unifies disparate theoretical results showing that standard transformer architectures face fundamental barriers implementing exact symbolic algorithms. Identifies core issues like limited effective depth, challenges approximating discontinuous functions associated with discrete operations, and constraints on information flow between tokens.", "conclusion": "Current transformers are structurally suited to pattern recognition and interpolation but are fundamentally ill-matched to exact discrete algorithmic reasoning. Understanding these limitations suggests directions for new architectures or modifications that relax depth constraints, improve representation of discontinuities, and enhance inter-token communication."}}
{"id": "2602.11541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11541", "abs": "https://arxiv.org/abs/2602.11541", "authors": ["Hanbing Liu", "Chunhao Tian", "Nan An", "Ziyuan Wang", "Pinyan Lu", "Changyuan Yu", "Qi Qi"], "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use", "comment": null, "summary": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.", "AI": {"tldr": "The paper introduces INTENT, a planning framework for tool-using language agents operating under strict monetary budgets, improving task success while guaranteeing budget feasibility.", "motivation": "Tool-augmented LLM agents face real-world monetary costs when calling external APIs. Under strict budgets, na\u00efvely planning tool usage becomes intractable due to huge state-action spaces, stochastic tool outcomes, and high exploration cost, especially when costs and prices change over time. There is a need for agents that can reason about future tool use and cost under uncertainty while respecting hard budget constraints.", "method": "The authors formalize budget-constrained tool use as a sequential decision-making problem in a context space where each tool call has an associated, stochastic cost and outcome. They then propose INTENT, an inference-time planning framework based on an intention-aware hierarchical world model. This model anticipates future tool usage and risk-calibrated costs, and uses this to guide online decisions about which tools to call and when, subject to a strict global budget. The framework is evaluated on a cost-augmented version of StableToolBench and under scenarios with dynamic market shifts (e.g., price changes, varying budgets).", "result": "On cost-augmented StableToolBench, INTENT enforces strict adherence to budget constraints while substantially improving task success rates compared to baseline methods. It also shows robustness when tool prices or available budgets change, maintaining high performance without violating budget limits.", "conclusion": "INTENT provides a practical and robust approach for planning in budget-constrained, tool-using LLM agents. By explicitly modeling intentions, future tool usage, and risk-calibrated costs, it achieves better task success while guaranteeing hard budget feasibility and adapts well to dynamic cost environments."}}
{"id": "2602.11176", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11176", "abs": "https://arxiv.org/abs/2602.11176", "authors": ["Maral Doctorarastoo", "Katherine A. Flanigan", "Mario Berg\u00e9s", "Christopher McComb"], "title": "Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments", "comment": null, "summary": "Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.", "AI": {"tldr": "The paper explores using large language models (LLMs) to anticipate human activities and their durations in smart-home-like settings, especially when training data is scarce.", "motivation": "Traditional data-driven agent-based models need large labeled datasets and perform poorly in low-data environments, which is a problem for many real-world adaptive systems that must reason about human activities. The authors want to see if pre-trained LLMs, with their broad world knowledge, can compensate for limited domain data and still reason effectively about daily human behavior.", "method": "They use a retrieval-augmented prompting framework for LLMs that injects four types of context (temporal, spatial, behavioral history, persona) into prompts. They evaluate this on the CASAS Aruba smart-home dataset across two tasks: (1) next-activity prediction plus duration estimation, and (2) multi-step daily activity sequence generation, under varying few-shot settings (zero-shot and with different numbers of demonstration examples in the prompt). They then analyze how performance changes as more examples are added.", "result": "LLMs show strong built-in temporal understanding of human routines. Even zero-shot prompts yield coherent daily activity predictions. Adding one or two few-shot demonstrations improves duration estimates and categorical prediction accuracy, but further increasing the number of examples brings little additional gain, indicating performance saturation. Sequence-level metrics show that temporal alignment of predicted activity sequences stays consistently good across few-shot conditions.", "conclusion": "Pre-trained LLMs can act as effective temporal reasoners about human activities, even with limited task-specific data. By leveraging contextual prompts, they can model both regular daily routines and context-dependent variations, making them promising components for the behavioral modules of agent-based models in low-data smart-home and related applications."}}
{"id": "2602.11569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11569", "abs": "https://arxiv.org/abs/2602.11569", "authors": ["Zhenlin Qin", "Yancheng Ling", "Leizhen Wang", "Francisco C\u00e2mara Pereira", "Zhenliang Ma"], "title": "SemaPop: Semantic-Persona Conditioned Population Synthesis", "comment": null, "summary": "Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and statistical constraints, leaving a gap in semantic-conditioned population generation that can capture abstract behavioral patterns implicitly in survey data. This study proposes SemaPop, a semantic-statistical population synthesis model that integrates large language models (LLMs) with generative population modeling. SemaPop derives high-level persona representations from individual survey records and incorporates them as semantic conditioning signals for population generation, while marginal regularization is introduced to enforce alignment with target population marginals. In this study, the framework is instantiated using a Wasserstein GAN with gradient penalty (WGAN-GP) backbone, referred to as SemaPop-GAN. Extensive experiments demonstrate that SemaPop-GAN achieves improved generative performance, yielding closer alignment with target marginal and joint distributions while maintaining sample-level feasibility and diversity under semantic conditioning. Ablation studies further confirm the contribution of semantic persona conditioning and architectural design choices to balancing marginal consistency and structural realism. These results demonstrate that SemaPop-GAN enables controllable and interpretable population synthesis through effective semantic-statistical information fusion. SemaPop-GAN also provides a promising modular foundation for developing generative population projection systems that integrate individual-level behavioral semantics with population-level statistical constraints.", "AI": {"tldr": "SemaPop introduces a semantic-statistical population synthesis framework that combines LLM-derived personas with a GAN-based generator to create behaviorally meaningful synthetic populations aligned with statistical marginals.", "motivation": "Traditional population synthesis methods use only structured attributes and explicit statistical constraints, failing to exploit the rich, implicit behavioral semantics in survey text or high-level patterns. There is a need for population generators that can be conditioned on such abstract behavioral concepts while still matching known marginal distributions.", "method": "The authors propose SemaPop, which first uses large language models to infer high-level semantic persona representations from individual survey records. These personas are then used as conditioning inputs to a generative model for population synthesis. The instantiated model, SemaPop-GAN, uses a WGAN-GP backbone with an added marginal regularization term to ensure generated populations match target marginal distributions while respecting semantic conditioning. Architectural choices are evaluated via ablation studies.", "result": "Experiments show that SemaPop-GAN more accurately reproduces target marginal and joint distributions than baselines, while generating samples that are both feasible and diverse under semantic conditioning. Ablation results indicate that including semantic persona conditioning and the specific architectural components improves the trade-off between marginal consistency and structural realism.", "conclusion": "SemaPop-GAN effectively fuses semantic information from LLM-derived personas with statistical constraints to enable controllable and interpretable population synthesis. The approach improves generative fidelity and offers a modular basis for future population projection systems that jointly model individual-level behavioral semantics and population-level statistics."}}
{"id": "2602.11177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11177", "abs": "https://arxiv.org/abs/2602.11177", "authors": ["Lei Jiang", "Yue Zhou", "Natalie Parde"], "title": "What Do LLMs Know About Alzheimer's Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection", "comment": null, "summary": "Reliable early detection of Alzheimer's disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain through supervised fine-tuning remains largely unexplored. In this work, we fine-tune an LLM for AD detection and investigate how task-relevant information is encoded within its internal representations. We employ probing techniques to analyze intermediate activations across transformer layers, and we observe that, after fine-tuning, the probing values of specific words and special markers change substantially, indicating that these elements assume a crucial role in the model's improved detection performance. Guided by this insight, we design a curated set of task-aware special markers and train a sequence-to-sequence model as a data-synthesis tool that leverages these markers to generate structurally consistent and diagnostically informative synthetic samples. We evaluate the synthesized data both intrinsically and by incorporating it into downstream training pipelines.", "AI": {"tldr": "The paper fine-tunes an LLM for early Alzheimer's detection, probes how AD-relevant information is encoded in its layers, and then uses insights about important tokens/markers to guide a seq2seq data-synthesis model that generates synthetic, diagnostically useful text for improving AD detection systems.", "motivation": "Early and reliable detection of Alzheimer's is hard because labeled clinical data are scarce. Large language models have strong transfer abilities, but their targeted adaptation to AD detection via supervised fine-tuning has not been deeply studied. There is also a need to understand what internal representations matter for this task and to find ways to augment limited data with high-quality synthetic samples.", "method": "1) Supervised fine-tuning of a pre\u2011trained LLM on an Alzheimer's detection task. 2) Probing internal transformer activations across layers to see how task-relevant information is encoded, focusing on specific words and special markers. 3) Using probing insights to design a curated, task-aware set of special markers. 4) Training a sequence-to-sequence model as a data-synthesis tool that uses these markers to generate structurally consistent and diagnostically informative synthetic samples. 5) Evaluating the synthetic data intrinsically and by using it in downstream training pipelines.", "result": "After fine-tuning, probing shows substantial changes in the internal representations of specific tokens and special markers, suggesting they become key carriers of AD-relevant information. The curated special markers, when used by the seq2seq data-synthesis model, yield synthetic data that is structurally coherent and diagnostically informative, and that can beneficially augment downstream AD detection training.", "conclusion": "Fine-tuning LLMs for Alzheimer's detection meaningfully reshapes how internal representations encode task-relevant signals, especially via certain words and special markers. By identifying and explicitly leveraging these markers, one can build effective data-synthesis tools that produce high\u2011quality synthetic examples, helping to mitigate data scarcity and improve AD detection models."}}
{"id": "2602.11574", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11574", "abs": "https://arxiv.org/abs/2602.11574", "authors": ["Aditya Taparia", "Som Sagar", "Ransalu Senanayake"], "title": "Learning to Configure Agentic AI Systems", "comment": "21 pages, 13 figures", "summary": "Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.", "AI": {"tldr": "They propose ARC, a reinforcement-learning-based system that dynamically configures LLM agents per query instead of using fixed, hand-tuned templates, improving accuracy and efficiency.", "motivation": "Current LLM agent configurations rely on static, hand-crafted templates and heuristics that are applied uniformly across queries. This causes brittleness and wastes compute because simple and complex queries receive similarly heavy configurations. There is a need for a method that adapts the workflow, tools, and resource allocation to the difficulty and nature of each specific query.", "method": "They cast agent configuration as a per-query decision-making problem and design ARC, a lightweight hierarchical policy trained with reinforcement learning. Given an input query, ARC selects among workflows, tools, token budgets, and prompts, tailoring the agent configuration dynamically. The policy is trained to optimize task performance and resource usage (tokens and runtime).", "result": "Across several benchmarks involving reasoning and tool-based question answering, the ARC policy surpasses strong hand-engineered and other baseline configurations. It achieves up to 25% higher task accuracy while simultaneously lowering token consumption and runtime, indicating better performance\u2013cost trade-offs.", "conclusion": "Per-query, learned configuration of LLM agents via a hierarchical RL policy is more effective and efficient than fixed, \"one size fits all\" templates. This approach yields higher accuracy and lower computational cost, suggesting that learned agent configuration is a promising direction for building robust and scalable LLM-based systems."}}
{"id": "2602.11179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11179", "abs": "https://arxiv.org/abs/2602.11179", "authors": ["Munazza Zaib", "Elaf Alhazmi"], "title": "From Instruction to Output: The Role of Prompting in Modern NLG", "comment": null, "summary": "Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires instructions to be composed in natural language to bring out the knowledge from LLMs in a structured way, has driven breakthroughs in various NLP tasks. Yet there is still no structured framework or coherent understanding of the varied prompt engineering methods and techniques, particularly in the field of Natural Language Generation (NLG).\n  This survey aims to help fill that gap by outlining recent developments in prompt engineering, and their effect on different NLG tasks. It reviews recent advances in prompting methods and their impact on NLG tasks, presenting prompt design as an input-level control mechanism that complements fine-tuning and decoding approaches. The paper introduces a taxonomy of prompting paradigms, a decision framework for prompt selection based on varying factors for the practitioners, outlines emerging trends and challenges, and proposes a framework that links design, optimization, and evaluation to support more controllable and generalizable NLG.", "AI": {"tldr": "Survey of prompt engineering methods for NLG, offering taxonomy, decision framework, and integrated design-optimization-evaluation perspective.", "motivation": "Prompt engineering is powerful for leveraging LLMs in NLP/NLG, but the field lacks a structured, coherent framework to organize diverse methods and guide practitioners.", "method": "Conducts a survey of recent prompt engineering techniques, conceptualizes prompts as input-level control, and organizes them into a taxonomy and decision framework while discussing trends, challenges, and integration with fine-tuning/decoding.", "result": "Provides a taxonomy of prompting paradigms, a practitioner-oriented decision framework for prompt selection, and a conceptual framework connecting design, optimization, and evaluation for controllable NLG.", "conclusion": "Prompt engineering can be systematically organized and treated as an input-control mechanism; the proposed taxonomy and frameworks aim to guide method selection and foster more controllable, generalizable NLG systems, while highlighting open challenges and future research directions."}}
{"id": "2602.11583", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11583", "abs": "https://arxiv.org/abs/2602.11583", "authors": ["Jingdi Chen", "Hanqing Yang", "Zongjun Liu", "Carlee Joe-Wong"], "title": "The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs", "comment": "Accepted at Transactions on Machine Learning Research (TMLR), 2026", "summary": "Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.", "AI": {"tldr": "Survey of multi-agent communication framed by the Five Ws, covering MARL, emergent language, and LLM-based systems, with design trade-offs and open challenges for scalable, interpretable collaboration.", "motivation": "Many real-world multi-agent systems operate in dynamic, partially observable environments, where communication is crucial to reduce uncertainty and enable effective collaboration. Existing research is fragmented across different paradigms (MARL, emergent language, LLM-based systems) with varying assumptions and goals, making it hard to see common principles, trade-offs, and design patterns.", "method": "Conceptual survey organized via the Five Ws of communication (who, what, when, why, and implicitly how). The authors review prior work in three main paradigms: (1) MARL communication protocols (hand-designed, implicit, and learned); (2) Emergent Language approaches where agents develop structured/symbolic communication; and (3) LLM-based multi-agent systems that leverage natural language priors. They compare and synthesize these lines of work along consistent axes.", "result": "The survey characterizes how design choices in each paradigm shape communication protocols, identifies recurring trade-offs (e.g., task-specific performance vs. interpretability and generalization), and analyzes limitations such as grounding, scalability, and generalization in emergent communication, as well as challenges in integrating LLMs with control and learning. It extracts cross-cutting design patterns from prior systems.", "conclusion": "No single paradigm fully solves multi-agent communication in complex environments. MARL yields strong task performance but often opaque, narrow protocols; emergent language aims for more structured, interpretable communication but struggles with grounding and scale; LLMs introduce powerful language priors but raise integration and reliability issues. Future progress likely lies in hybrid systems that combine learning, language, and control, guided by the Five Ws framework and the identified design patterns and open challenges."}}
{"id": "2602.11180", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11180", "abs": "https://arxiv.org/abs/2602.11180", "authors": ["Usman Naseem"], "title": "Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applied to LLM alignment, examining methods ranging from circuit discovery to feature visualization, activation steering, and causal intervention. We analyze how interpretability insights have informed alignment strategies including reinforcement learning from human feedback (RLHF), constitutional AI, and scalable oversight. Key challenges are identified, including the superposition hypothesis, polysemanticity of neurons, and the difficulty of interpreting emergent behaviors in large-scale models. We propose future research directions focusing on automated interpretability, cross-model generalization of circuits, and the development of interpretability-driven alignment techniques that can scale to frontier models.", "AI": {"tldr": "A survey of mechanistic interpretability methods for aligning large language models, summarizing techniques, how they inform alignment approaches, key challenges, and future research directions.", "motivation": "LLMs are powerful but opaque, and their lack of interpretability poses risks for safety and alignment. The paper aims to systematically review mechanistic interpretability work that can make LLM behavior more understandable and controllable for alignment purposes.", "method": "Conducts a literature survey of mechanistic interpretability techniques\u2014such as circuit discovery, feature visualization, activation steering, and causal interventions\u2014and analyzes their applications to LLM alignment methods like RLHF, constitutional AI, and scalable oversight.", "result": "Synthesizes recent advances in mechanistic interpretability, maps which techniques contribute to specific alignment strategies, and identifies recurring technical obstacles such as neuron polysemanticity, superposition, and challenges in explaining emergent behaviors in large models.", "conclusion": "Mechanistic interpretability is a promising but incomplete foundation for LLM alignment. Overcoming issues like superposition and scaling requires new automated, generalizable interpretability methods and alignment schemes that use these insights to reliably steer behavior in increasingly large and capable models."}}
{"id": "2602.11596", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11596", "abs": "https://arxiv.org/abs/2602.11596", "authors": ["Nikhil Verma", "Minjung Kim", "JooYoung Yoo", "Kyung-Min Jin", "Manasa Bharadwaj", "Kevin Ferreira", "Ko Keun Kim", "Youngjoon Kim"], "title": "MAPLE: Modality-Aware Post-training and Learning Ecosystem", "comment": "31 pages", "summary": "Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.", "AI": {"tldr": "They propose MAPLE, a modality-aware RL post-training ecosystem for multimodal language models, improving efficiency, robustness, and closing uni/multi-modal performance gaps.", "motivation": "Existing multimodal RL post-training treats all input modalities as equally relevant, which increases gradient variance, slows convergence, and harms robustness when modalities are missing or reweighted in real-world settings.", "method": "They build MAPLE, including: (1) MAPLE-bench, a benchmark that labels the minimal required modality combinations per task; (2) MAPO, a modality-aware policy optimization method that stratifies training batches by their modality requirements to reduce gradient variance; and (3) adaptive loss weighting and curriculum scheduling to emphasize harder modality combinations. They systematically study loss aggregation, clipping, sampling, and curriculum variants to identify the best training recipe.", "result": "MAPLE reduces the accuracy gap between uni-modal and multi-modal settings by 30.24%, achieves 3.18\u00d7 faster convergence, and remains stable across all modality combinations, even when some signals are reduced or missing.", "conclusion": "Modality-aware post-training via MAPLE provides an effective, robust, and efficient recipe for RL-based fine-tuning of multimodal language models, making them more deployment-ready under diverse and imperfect signal conditions."}}
{"id": "2602.11181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11181", "abs": "https://arxiv.org/abs/2602.11181", "authors": ["Himanshu Gupta", "Pratik Jayarao", "Chaitanya Dwivedi", "Neeraj Varshney"], "title": "Code Mixologist : A Practitioner's Guide to Building Code-Mixed LLMs", "comment": "7 pages main paper, 10 pages total", "summary": "Code-mixing and code-switching (CSW) remain challenging phenomena for large language models (LLMs). Despite recent advances in multilingual modeling, LLMs often struggle in mixed-language settings, exhibiting systematic degradation in grammaticality, factuality, and safety behavior. This work provides a comprehensive overview of CSW research in modern large language model settings. We introduce a unifying taxonomy that organizes prior work along dimensions of data, modeling, and evaluation, and we distill these findings into a practical playbook of actionable recommendations for building, adapting, and evaluating CSW-capable LLMs. We review modeling approaches ranging from CSW-tailored pre-training and task-specific post-training to prompting strategies and in-context learning. We analyze current evaluation practices, highlighting sources of instability and limited reproducibility, and we catalog existing benchmarks while critically examining their linguistic coverage and English-centric biases. Finally, we discuss emerging safety concerns, including use of code-mixing as a mechanism for bypassing model safeguards, and identify open research challenges.", "AI": {"tldr": "The paper surveys how large language models handle code-mixing and code-switching, offering a taxonomy, practical guidelines, evaluation critique, and safety discussion.", "motivation": "Large language models currently perform poorly and unreliably when text mixes multiple languages, with issues in grammaticality, factuality, and safety. There is scattered research but no unified perspective or practical guidance for building and evaluating models that can robustly handle code-mixed/code-switched inputs.", "method": "The authors conduct a comprehensive survey of recent work on code-mixing and code-switching for LLMs. They define a unifying taxonomy along three axes\u2014data, modeling, and evaluation\u2014review modeling strategies (from pretraining to prompting), systematically analyze evaluation methodologies and benchmarks, and synthesize these into a practical playbook. They also review and categorize emerging safety issues related to CSW.", "result": "They organize prior work into a clear taxonomy, summarize and compare modeling strategies for CSW-capable LLMs, and provide a critical inventory of existing benchmarks, identifying instability, reproducibility problems, and English-centric biases. They also surface concrete examples and patterns of safety vulnerabilities enabled by code-mixing.", "conclusion": "Handling code-mixing and code-switching remains a major unsolved problem for LLMs. The paper\u2019s taxonomy, playbook, and evaluation critique provide a foundation for more systematic research and development, but significant challenges remain in robust modeling, comprehensive and fair evaluation, and mitigation of safety vulnerabilities in multilingual mixed-language contexts."}}
{"id": "2602.11609", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.11609", "abs": "https://arxiv.org/abs/2602.11609", "authors": ["Yiming Gao", "Zhen Wang", "Jefferson Chen", "Mark Antkowiak", "Mengzhou Hu", "JungHo Kong", "Dexter Pratt", "Jieyuan Liu", "Enze Ma", "Zhiting Hu", "Eric P. Xing"], "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery", "comment": "Accepted at NeurIPS 2025 Main Conference", "summary": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.\n  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.\n  Code, data, and package are available at https://github.com/maitrix-org/scPilot", "AI": {"tldr": "scPilot is a framework that lets large language models directly analyze single-cell RNA-seq data using step-by-step, omics-native reasoning, evaluated with a new benchmark suite, scBench.", "motivation": "Single-cell RNA-seq analysis requires complex, multi-step reasoning (e.g., annotating cell types, reconstructing developmental trajectories, and identifying transcription factor targets) that is hard to do transparently and robustly with current LLM prompting paradigms. There is a need to tightly couple LLMs with raw omics data and bioinformatics tools so that their reasoning can be grounded, auditable, and more accurate.", "method": "The authors develop scPilot, a system where an LLM performs multi-step reasoning in natural language while directly interrogating single-cell RNA-seq data and invoking bioinformatics tools on demand. They formally cast standard single-cell tasks\u2014cell-type annotation, trajectory inference, and TF-targeting\u2014as sequential reasoning problems, requiring the model to propose, justify, and iteratively revise hypotheses based on new evidence. To systematically evaluate this paradigm, they build scBench, a collection of 9 curated datasets plus automatic graders to quantify omics-native reasoning performance across different LLMs and prompting strategies.", "result": "Using scBench, they show that iterative omics-native reasoning substantially improves performance compared with one-shot prompting. With the o1 model, scPilot increases mean cell-type annotation accuracy by 11%, and with Gemini-2.5-Pro it reduces trajectory graph-edit distance by 30%. The framework also produces detailed reasoning traces that clarify ambiguous marker-gene patterns and transcriptional regulatory logic.", "conclusion": "Grounding LLMs directly in single-cell omics data and forcing them to reason iteratively with tool access yields more accurate, interpretable, and auditable analyses. scPilot and scBench together define a new paradigm and benchmark for omics-native reasoning in single-cell RNA-seq, potentially improving the diagnostic and biological insights derived from such data."}}
{"id": "2602.11182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11182", "abs": "https://arxiv.org/abs/2602.11182", "authors": ["Haidong Xin", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization", "comment": null, "summary": "Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at https://github.com/OpenBMB/MetaMem.", "AI": {"tldr": "MetaMem is a framework that adds a self-evolving meta-memory on top of existing memory systems for LLMs to improve how they use long-term interaction history.", "motivation": "Current LLM memory systems can store long interaction histories but often break the logical and temporal structure of conversations, creating fragmented memories that hurt reasoning. There is a need to not only store information but also to teach LLMs how to better utilize scattered memory fragments.", "method": "MetaMem introduces a meta-memory that is optimized through iterative self-reflection. The LLM analyzes its own reasoning processes across different tasks, distills transferable patterns of how to use memory effectively, and updates a meta-memory state. These meta-memory units explicitly encode knowledge utilization strategies that guide future retrieval and integration of evidence from fragmented memories.", "result": "Experiments show that MetaMem significantly improves performance on tasks requiring long-horizon memory use, achieving more than a 3.6% gain over strong baseline memory systems.", "conclusion": "By augmenting conventional memory systems with a self-evolving meta-memory of knowledge utilization experiences, MetaMem helps LLMs more systematically identify and integrate critical information from fragmented interaction histories, leading to better reasoning in long-term interactions."}}
{"id": "2602.11619", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11619", "abs": "https://arxiv.org/abs/2602.11619", "authors": ["Aman Mehta"], "title": "When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents", "comment": "5 pages, 2 figures", "summary": "Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.", "AI": {"tldr": "The paper shows that large language model (LLM) agents, even when run repeatedly on the same task with identical inputs, often follow different chains of actions, and that this inconsistency strongly correlates with lower task accuracy.", "motivation": "LLM agents are increasingly used for complex, tool-using tasks where reliability and repeatability matter, yet most evaluations focus only on final accuracy, not whether the agent behaves consistently across runs. Understanding how often an agent behaves differently on the same input, where this divergence arises, and how it relates to failure is crucial for building dependable systems.", "method": "The authors run ReAct-style LLM agents based on three models (Llama 3.1 70B, GPT-4o, Claude Sonnet 4.5) on the multi-hop QA benchmark HotpotQA. For each question, they execute 10 independent runs with identical prompts and settings, logging the full sequence of tool calls and reasoning steps. They measure: (1) how many distinct action sequences appear per question, (2) how early trajectories diverge, and (3) how behavioral variance correlates with final answer accuracy. They then analyze where in the trajectory divergence first occurs and quantify the share of divergence attributable to early vs. later decisions.", "result": "Across 3,000 agent runs, ReAct-style agents exhibit substantial behavioral variability: on average, 2.0\u20134.2 distinct action paths per 10 runs for the same input. Most divergence emerges very early: 69% of differing trajectories split at step 2, the first search query. Behavioral variance is strongly predictive of failure: questions with low variance (\u22642 unique paths) have high accuracy (80\u201392%), while questions with high variance (\u22656 paths) have much lower accuracy (25\u201360%), a drop of 32\u201355 percentage points depending on the model.", "conclusion": "LLM agents are not reliably repeatable even under identical conditions, and this behavioral variance is not just noise\u2014it is a strong signal of task difficulty and impending failure. Because divergence typically appears at the earliest tool-use step, it may be possible to detect problematic runs in real time by monitoring path consistency, then intervene (e.g., replan, aggregate multiple runs, or adjust prompting) to improve overall reliability."}}
{"id": "2602.11198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11198", "abs": "https://arxiv.org/abs/2602.11198", "authors": ["Shafiuddin Rehan Ahmed", "Wei Wei"], "title": "DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task", "comment": "ARR submission", "summary": "Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability -- the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.", "AI": {"tldr": "The paper introduces DDL2PropBank, a benchmark for evaluating multi-agent frameworks in LLM-based software development by mapping database schemas to PropBank rolesets, and compares ten frameworks on code complexity and AI-assistability, finding Agno to perform best overall.", "motivation": "Multi-agent frameworks are increasingly used to structure LLM-based software, but there is no systematic way to compare their developer experience or how well LLMs can write code for them. The authors aim to create a controlled, realistic task and metrics to evaluate and compare different frameworks.", "method": "The authors design DDL2PropBank, a task where systems must map relational database schemas to PropBank rolesets, which requires retrieving candidate frames and reasoning about schema elements. They implement the same Agent-as-a-Tool logic across ten multi-agent frameworks, then measure (1) code complexity through static analysis and (2) AI-assistability by testing whether LLMs can automatically generate correct, framework-specific implementations, further using structural alignment scores as a proxy for runtime success.", "result": "They observe a three-level spectrum of implementation complexity, with Pydantic AI and Agno having the lowest overhead. Structural alignment correlates well with runtime success for frameworks that enforce a single canonical agent pattern, but tends to overestimate correctness in frameworks supporting multiple patterns. Agno attains the best overall tradeoff, combining lowest complexity, high structural alignment, and an 83% pass@1 rate on the benchmark.", "conclusion": "DDL2PropBank provides a principled, controlled benchmark for evaluating multi-agent frameworks on both code complexity and AI-assistability. The findings suggest that frameworks like Agno, which offer simple, canonical patterns and strong structural alignment, yield better developer ergonomics and are easier targets for LLM-based code generation than more flexible, multi-pattern frameworks."}}
{"id": "2602.11630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11630", "abs": "https://arxiv.org/abs/2602.11630", "authors": ["Yipeng Huang", "Dejun Xu", "Zexin Lin", "Zhenzhong Wang", "Min Jiang"], "title": "Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families", "comment": null, "summary": "Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box\" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\\sim$35.7% increase in accuracy while providing interpretable analytical solutions.", "AI": {"tldr": "They propose NMIPS, a neuro-assisted symbolic PDE solver that efficiently solves entire families of PDEs and yields analytical, interpretable solutions with higher accuracy.", "motivation": "Numerically solving each instance in a PDE family is computationally expensive, and current ML-based PDE solvers are fast but act as black boxes, only giving numerical approximations without interpretable analytical forms. There is a need for a method that can efficiently handle whole PDE families while still producing symbolic, human-interpretable solutions.", "method": "They design NMIPS, a neuro-assisted multitasking symbolic PDE solver. It uses multifactorial optimization to jointly discover analytical solutions for multiple PDEs in a family and introduces an affine transfer mechanism that transfers learned mathematical structures across PDEs, so each new PDE does not need to be solved from scratch.", "result": "Across several test cases, NMIPS outperforms existing baselines, with up to about 35.7% higher accuracy while also delivering symbolic analytical solutions instead of only numerical approximations.", "conclusion": "NMIPS effectively addresses both efficiency and interpretability challenges in solving PDE families by jointly optimizing symbolic solutions and transferring structural knowledge between related PDEs, thus providing more accurate and interpretable results than prior methods."}}
{"id": "2602.11199", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11199", "abs": "https://arxiv.org/abs/2602.11199", "authors": ["Jiale Zhao", "Ke Fang", "Lu Cheng"], "title": "When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification", "comment": null, "summary": "Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs' ability to decide when and what to ask for clarification without sacrificing task performance. We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints. A unified judge loop evaluates final answers and simulates user responses as needed. AskBench covers two settings: AskMind, with intent-deficient queries requiring clarification, and AskOverconfidence, with queries containing false premises that must be identified and corrected. We further propose rubric-guided reinforcement learning with verifier-based rewards (RLVR), which uses structured rubrics to encourage targeted clarification. Experiments show consistent improvements in accuracy, rubric adherence, and interaction efficiency, with strong generalization to unseen domains.", "AI": {"tldr": "The paper introduces AskBench, a benchmark for testing when LLMs should ask clarification questions, and RLVR, a reinforcement learning method that improves targeted clarification without hurting task accuracy.", "motivation": "LLMs tend to answer even when user prompts are underspecified or contain incorrect assumptions, which leads to hallucinations and confirmation of user misconceptions. The authors want systematic ways to evaluate and improve models' ability to recognize such situations and ask clarifying questions instead of blindly answering.", "method": "They build AskBench by transforming standard question\u2013answer pairs into multi-turn interactions with explicit checkpoints where the model can decide to ask clarifications. AskBench has two key scenarios: (1) AskMind, where queries lack necessary intent or details, and (2) AskOverconfidence, where queries embed false premises that must be detected and challenged. A unified judge loop both scores final answers and simulates user replies to clarification questions. They also introduce rubric-guided reinforcement learning with verifier-based rewards (RLVR), which trains models with structured rubrics specifying desirable clarification behavior and uses verifier models to compute rewards for adherence to these rubrics.", "result": "Models trained with RLVR on AskBench show better accuracy, more appropriate and focused clarification questions, and fewer unnecessary interactions. These gains generalize across datasets and domains that were not seen during training, implying that the learned clarification behavior is robust rather than overfitted to specific benchmarks.", "conclusion": "AskBench provides a standardized way to stress-test LLMs' decision-making around when to ask questions versus when to answer, in both underspecified and misleading-query settings. RLVR effectively shapes model behavior to ask for clarification in a targeted, rubric-aligned manner, improving reliability and reducing harmful overconfidence without sacrificing task performance."}}
{"id": "2602.11635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11635", "abs": "https://arxiv.org/abs/2602.11635", "authors": ["Shuo Lu", "Jianjie Cheng", "Yinuo Xu", "Yongcan Yu", "Lijun Sheng", "Peijie Wang", "Siru Jiang", "Yongguan Hu", "Run Ling", "Yihua Shao", "Ao Ma", "Wei Feng", "Lingxiao He", "Meng Wang", "Qianlong Xie", "Xingxing Wang", "Ran He", "Jian Liang"], "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\\% accuracy, but we find that most leading MLLMs fail to reach even 60\\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.", "AI": {"tldr": "The paper introduces MathSpatial, a framework to evaluate and improve mathematical spatial reasoning in multimodal large language models (MLLMs), showing that current MLLMs significantly lag behind humans and demonstrating gains via specialized training.", "motivation": "Although MLLMs do well on perception-oriented tasks, their capability for mathematical spatial reasoning\u2014parsing and manipulating 2D/3D relations\u2014is poorly understood and appears weak compared to human performance. There is a need to systematically measure and improve this ability while disentangling reasoning from low-level perception.", "method": "The authors build MathSpatial, which has three main components: (i) MathSpatial-Bench, a benchmark of 2,000 textbook-style spatial reasoning problems across three categories and eleven subtypes, carefully constructed to minimize perceptual noise; (ii) MathSpatial-Corpus, an 8,000-problem training set with verified solutions; and (iii) MathSpatial-SRT, a structured reasoning-trace framework using three atomic operations\u2014Correlate, Constrain, and Infer\u2014to model spatial reasoning. They fine-tune an MLLM (Qwen2.5-VL-7B) on this resource and compare performance and token efficiency to baselines.", "result": "Most leading MLLMs perform below 60% accuracy on MathSpatial tasks where humans exceed 95%, confirming a substantial spatial reasoning deficit. After fine-tuning Qwen2.5-VL-7B on MathSpatial data and reasoning traces, the model reaches competitive accuracy while reducing the number of tokens needed by about 25%, indicating more efficient and structured reasoning.", "conclusion": "Spatial reasoning is a fundamental weakness of current MLLMs, distinct from their perceptual capabilities. MathSpatial offers the first large-scale, controlled framework and dataset that separates perception from reasoning, enabling precise evaluation and targeted improvement of mathematical spatial reasoning. Fine-tuning with MathSpatial and structured reasoning traces can significantly boost accuracy and efficiency for spatial reasoning tasks."}}
{"id": "2602.11201", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11201", "abs": "https://arxiv.org/abs/2602.11201", "authors": ["Donald Ye", "Max Loffgren", "Om Kotadia", "Linus Wong"], "title": "Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning", "comment": "16 pages, 15 figures. Code: https://github.com/donald-ye/ACL_2026", "summary": "Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc justifications. We propose Normalized Logit Difference Decay (NLDD), a metric that measures whether individual reasoning steps are faithful to the model's decision-making process. Our approach corrupts individual reasoning steps from the explanation and measures how much the model's confidence in its answer drops, to determine if a step is truly important. By standardizing these measurements, NLDD enables rigorous cross-model comparison across different architectures. Testing three model families across syntactic, logical, and arithmetic tasks, we discover a consistent Reasoning Horizon (k*) at 70--85% of chain length, beyond which reasoning tokens have little or negative effect on the final answer. We also find that models can encode correct internal representations while completely failing the task. These results show that accuracy alone does not reveal whether a model actually reasons through its chain. NLDD offers a way to measure when CoT matters.", "AI": {"tldr": "The paper introduces Normalized Logit Difference Decay (NLDD), a metric to test whether individual steps in Chain-of-Thought (CoT) explanations are actually used by language models for decision-making, rather than being post-hoc justifications.", "motivation": "Although CoT explanations are popular for making model reasoning transparent, it is unclear if the step-by-step tokens genuinely participate in the model\u2019s internal reasoning. Existing evaluations focus mainly on final accuracy and do not measure the faithfulness of intermediate reasoning steps or allow robust comparison across model architectures.", "method": "The authors define NLDD, which quantifies the impact of corrupting each reasoning step in a CoT on the model\u2019s confidence (logits) in its final answer. For each explanation token or step, they replace or corrupt it, observe the change in normalized logit difference for the correct answer versus alternatives, and then standardize these values to compare across models and tasks. Using this metric, they evaluate three different LLM families on syntactic, logical, and arithmetic tasks to identify how importance decays along the reasoning chain and where the Reasoning Horizon (k*) lies.", "result": "Across all tested model families and tasks, the importance of reasoning tokens tends to peak before the end of the chain, with later tokens exhibiting little or even negative influence on the final decision. This yields a consistent Reasoning Horizon (k*), around 70\u201385% of the chain length, after which additional CoT tokens rarely contribute positively. They also observe cases where models form internally correct intermediate representations but still fail the final task, indicating a disconnect between internal states and final outputs.", "conclusion": "CoT explanations cannot be assumed to be faithful simply because they are detailed or lead to correct answers. Accuracy alone is an insufficient indicator of genuine step-by-step reasoning. NLDD provides a principled metric to identify which parts of a chain truly affect decisions, to compare reasoning behavior across architectures, and to detect when models have useful internal structure that is not effectively used to produce correct final answers."}}
{"id": "2602.11661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11661", "abs": "https://arxiv.org/abs/2602.11661", "authors": ["Tianxiang Xu", "Jiayi Liu", "Yixuan Tong", "Jialu Xu", "Yunqing Wei", "Kaiwen Feng", "PanPan Hou", "Kangping Yin", "Jiyuan Hu", "Hao Zhou", "Zhenxin Ma", "Jian Xu", "Guanjun Jiang"], "title": "Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm", "comment": null, "summary": "While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.", "AI": {"tldr": "The paper identifies a mismatch between existing RL-based alignment methods and the needs of medical QA, and proposes a new multi-dimensional medical alignment paradigm with a unified optimization mechanism to better balance correctness, safety, and compliance.", "motivation": "Existing reinforcement learning alignment methods, such as RL from Human Feedback and RL from Verifiable Rewards, are poorly suited for medical question answering because preference data is expensive and noisy, automatic verifiers are weak, and multiple heterogeneous objectives (correctness, safety, compliance) create scaling and optimization conflicts. There is a need for a robust, domain-specific alignment approach for high-stakes medical scenarios.", "method": "The authors design a multi-dimensional medical alignment matrix that breaks alignment into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. For each category, they build a closed-loop pipeline: observable metrics \u2192 attributable diagnosis \u2192 optimizable rewards, providing fine-grained supervision. They then introduce a unified optimization mechanism that uses Reference-Frozen Normalization to normalize reward scales across heterogeneous signals, and a Tri-Factor Adaptive Dynamic Weighting scheme that adjusts reward weights to focus on weaknesses, prioritize risk, and reduce redundancy.", "result": "In experiments on real-world medical evaluation scenarios, the proposed paradigm improves model performance on complex medical alignment objectives, showing better balance of correctness, safety, and compliance compared to baselines, although specific metrics are not detailed in the abstract.", "conclusion": "The proposed medical alignment paradigm, built on a multi-dimensional alignment matrix and a unified optimization mechanism for heterogeneous rewards, offers an effective way to align large language models for high-stakes medical QA, and may serve as a general framework for complex alignment problems in vertical domains."}}
{"id": "2602.11221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11221", "abs": "https://arxiv.org/abs/2602.11221", "authors": ["Rui Cao", "Zhenyun Deng", "Yulong Chen", "Michael Schlichtkrull", "Andreas Vlachos"], "title": "The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task", "comment": "Shared Task Overview and Summary for the Ninth FEVER Workshop, Co-located at EACL 2026", "summary": "The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such as web search engines, or leverage the curated knowledge store provided by the organizers. System performance was evaluated using the AVerImaTeC score, defined as a conditional verdict accuracy in which a verdict is considered correct only when the associated evidence score exceeds a predefined threshold. The shared task attracted 14 submissions during the development phase and 6 submissions during the testing phase. All participating systems in the testing phase outperformed the baseline provided. The winning team, HUMANE, achieved an AVerImaTeC score of 0.5455. This paper provides a detailed description of the shared task, presents the complete evaluation results, and discusses key insights and lessons learned.", "AI": {"tldr": "AVerImaTeC is a shared task focused on automatic verification of real-world image-text claims using retrieved evidence, evaluated by a conditional accuracy metric; multiple teams participated and the best system reached a score of 0.5455.", "motivation": "To advance research and system development in automatically verifying real-world claims that involve both images and text, an increasingly important problem due to the spread of multimodal misinformation. There was a need for a standardized benchmark, task setup, and evaluation metric to fairly compare different approaches and encourage progress.", "method": "The organizers designed a shared task where systems must retrieve relevant evidence and output a verification verdict for image-text claims. Participants could either use external knowledge sources (e.g., web search) or a curated knowledge store provided by the organizers. System outputs were evaluated via the AVerImaTeC score, a conditional verdict accuracy that only counts a verdict as correct if its associated evidence score is above a preset threshold. The paper documents the task setup, resources, baselines, and evaluation procedure, and collects system submissions during development and testing phases.", "result": "There were 14 submissions in the development phase and 6 in the testing phase. All systems in the testing phase outperformed the baseline model. The best-performing team, HUMANE, achieved an AVerImaTeC score of 0.5455 according to the defined metric.", "conclusion": "The shared task successfully established a benchmark and attracted diverse systems that surpassed the baseline, showing the feasibility and value of automatic verification of image-text claims with evidence retrieval. The paper summarizes the evaluation results and distills insights and lessons learned to guide future research in multimodal claim verification and evidence-based assessment."}}
{"id": "2602.11666", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11666", "abs": "https://arxiv.org/abs/2602.11666", "authors": ["E Fan", "Lisong Shi", "Zhengtong Li", "Chih-yung Wen"], "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics", "comment": "30 pages, 10 figures", "summary": "The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to \"context poisoning,\" where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.", "AI": {"tldr": "The paper proposes PhyNiKCE, a neurosymbolic framework that couples LLM-based planning with symbolic constraint enforcement to make autonomous CFD agents robust, physically consistent, and more efficient.", "motivation": "LLMs are probabilistic and linguistically focused, so when used as autonomous agents for CFD they often violate strict physical conservation laws and numerical stability requirements. Semantic RAG alone can cause \u201ccontext poisoning,\u201d where retrieved context leads to answers that sound correct but are physically invalid, revealing a Semantic-Physical Disconnect. The authors want a way to use LLMs for engineering workflows while guaranteeing physical validity, stability, and trustworthiness.", "method": "They design PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework that separates neural planning from symbolic validation. A Symbolic Knowledge Engine formulates CFD simulation setup as a Constraint Satisfaction Problem, encoding physical and numerical constraints. A Deterministic RAG Engine then uses specialized retrieval strategies for solvers, turbulence models, and boundary conditions to construct only configurations that satisfy these constraints. LLMs (Gemini-2.5 Pro/Flash) perform high-level reasoning and planning, but all proposed configurations are checked and filtered by the symbolic layer before execution in OpenFOAM.", "result": "In OpenFOAM-based experiments on realistic, non-tutorial CFD problems, PhyNiKCE outperforms state-of-the-art autonomous CFD agent baselines by 96% in relative performance metrics (e.g., correctness / success rates). It reduces autonomous self-correction loops by 59%, indicating fewer trial-and-error iterations are needed, and lowers LLM token usage by 17%, demonstrating better efficiency in interaction and computation.", "conclusion": "Decoupling neural generation from symbolic constraint enforcement enables autonomous agents that are both robust and efficient in CFD simulation setup. PhyNiKCE reliably enforces physical and numerical constraints while maintaining or improving performance and resource usage. Although validated in CFD, the architecture provides a general, scalable, and auditable pattern for trustworthy AI in broader industrial automation and safety-critical engineering domains."}}
{"id": "2602.11238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11238", "abs": "https://arxiv.org/abs/2602.11238", "authors": ["Beichen Guo", "Zhiyuan Wen", "Jia Gu", "Senzhang Wang", "Haochen Shi", "Ruosong Yang", "Shuaiqi Liu"], "title": "SurveyLens: A Research Discipline-Aware Benchmark for Automatic Survey Generation", "comment": null, "summary": "The exponential growth of scientific literature has driven the evolution of Automatic Survey Generation (ASG) from simple pipelines to multi-agent frameworks and commercial Deep Research agents. However, current ASG evaluation methods rely on generic metrics and are heavily biased toward Computer Science (CS), failing to assess whether ASG methods adhere to the distinct standards of various academic disciplines. Consequently, researchers, especially those outside CS, lack clear guidance on using ASG systems to yield high-quality surveys compliant with specific discipline standards. To bridge this gap, we introduce SurveyLens, the first discipline-aware benchmark evaluating ASG methods across diverse research disciplines. We construct SurveyLens-1k, a curated dataset of 1,000 high-quality human-written surveys spanning 10 disciplines. Subsequently, we propose a dual-lens evaluation framework: (1) Discipline-Aware Rubric Evaluation, which utilizes LLMs with human preference-aligned weights to assess adherence to domain-specific writing standards; and (2) Canonical Alignment Evaluation to rigorously measure content coverage and synthesis quality against human-written survey papers. We conduct extensive experiments by evaluating 11 state-of-the-art ASG methods on SurveyLens, including Vanilla LLMs, ASG systems, and Deep Research agents. Our analysis reveals the distinct strengths and weaknesses of each paradigm across fields, providing essential guidance for selecting tools tailored to specific disciplinary requirements.", "AI": {"tldr": "Introduces SurveyLens, a discipline-aware benchmark and dataset to evaluate automatic survey generation systems across 10 academic disciplines.", "motivation": "Existing automatic survey generation (ASG) evaluations use generic, CS-centric metrics and do not capture whether generated surveys meet the distinct writing and quality standards of different academic disciplines, leaving non-CS researchers without guidance on reliable ASG usage.", "method": "Construct SurveyLens-1k, a dataset of 1,000 high-quality human-written surveys from 10 disciplines, and design a dual-lens evaluation framework: (1) Discipline-Aware Rubric Evaluation using LLMs aligned to human preferences to judge adherence to domain-specific standards; (2) Canonical Alignment Evaluation to compare generated surveys\u2019 content coverage and synthesis quality against canonical human-written surveys. Evaluate 11 state-of-the-art ASG methods (vanilla LLMs, dedicated ASG systems, and Deep Research agents) on this benchmark.", "result": "The benchmark and evaluation reveal distinct strengths and weaknesses of different ASG paradigms across disciplines, showing that performance is discipline-dependent and that no single paradigm dominates across all fields.", "conclusion": "SurveyLens provides the first discipline-aware standard for evaluating automatic survey generation, offering actionable guidance for choosing and designing ASG tools that meet the specific requirements of different academic fields."}}
{"id": "2602.11674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11674", "abs": "https://arxiv.org/abs/2602.11674", "authors": ["Longyuan Zhu", "Hairan Hua", "Linlin Miao", "Bing Zhao"], "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs", "comment": "42 pages, 8 figures, 7 tables. Code and website available at https://github.com/SKYLENAGE-AI/benchmark-health-index", "summary": "Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.", "AI": {"tldr": "The paper proposes the Benchmark Health Index (BHI), a data-driven framework to assess the reliability and usefulness of LLM benchmarks across capability discrimination, resistance to saturation, and real-world impact, enabling better benchmark selection and lifecycle management.", "motivation": "Existing LLM benchmarks are becoming unreliable due to score inflation, selective reporting, and ceiling effects, making it hard to know which evaluation results to trust and which benchmarks are still informative. The community lacks a principled, quantitative way to audit and compare benchmarks at scale.", "method": "The authors construct the Benchmark Health Index (BHI), which evaluates benchmarks along three axes: (1) Capability Discrimination, using performance variance and noise to measure how well a benchmark differentiates models; (2) Anti-Saturation, estimating remaining performance headroom before ceiling effects; and (3) Impact, measuring adoption and influence through usage in academic and industrial settings. They apply BHI to 106 benchmarks extracted from 91 LLM technical reports from 2025 to compute health scores and analyze patterns in the evaluation ecosystem.", "result": "Applying BHI to 106 validated benchmarks reveals which benchmarks still provide strong discrimination and headroom, which are saturated or noisy, and which have high ecosystem impact despite weak technical health, thus mapping the strengths and weaknesses of the current LLM evaluation landscape.", "conclusion": "BHI offers the first macro-level, quantitative framework for auditing LLM benchmarks, supporting more informed benchmark selection and proactive lifecycle management (e.g., retirement, refresh, or preservation), and guiding the development of next-generation evaluation protocols that remain robust as models continue to advance."}}
{"id": "2602.11305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11305", "abs": "https://arxiv.org/abs/2602.11305", "authors": ["Usman Naseem", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Sushant Kumar Ray", "Abdullah Mohammad", "Agrima Seth"], "title": "Are Aligned Large Language Models Still Misaligned?", "comment": null, "summary": "Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.", "AI": {"tldr": "The paper proposes Mis-Align Bench, a unified benchmark to evaluate LLM misalignment simultaneously across safety, values, and culture, showing that models tuned for a single dimension perform poorly when all three must be satisfied together.", "motivation": "Current misalignment benchmarks evaluate large language models along isolated dimensions such as safety, values, or culture, but real-world use requires models to satisfy all three simultaneously. This lack of joint evaluation hides important failure modes where a model can be safe yet value-misaligned, or culturally insensitive, motivating a unified framework that reflects realistic, multi-dimensional alignment requirements.", "method": "The authors build Mis-Align Bench in two main steps. First, they construct SAVACU, a large English dataset (382k+ samples) of prompts labeled across 112 domains in three taxonomies: 14 safety domains, 56 value domains, and 42 cultural domains. They reclassify prompts from LLM-PROMPT-DATASET using Mistral-7B-Instruct, then augment low-resource domains with Llama 3.1-8B-Instruct, using SimHash-based fingerprinting to avoid duplicates. Each prompt is paired with both misaligned and aligned responses using a two-stage rejection sampling pipeline to ensure label quality. Second, they evaluate a range of LLMs (general-purpose, fine-tuned, and open-weight) on this benchmark, measuring metrics like Coverage, False Failure Rate, and Alignment Score across single and joint dimensions.", "result": "The benchmark shows that models optimized for a single dimension (e.g., safety-only or value-only) appear strong when evaluated in isolation, achieving high Coverage up to 97.6%. However, when required to satisfy safety, value, and cultural criteria jointly, these models exhibit a high False Failure Rate above 50% and substantially lower Alignment Scores (around 63\u201366%), revealing significant hidden misalignment. The results provide quantitative evidence that current alignment strategies and benchmarks overestimate real-world robustness.", "conclusion": "Mis-Align Bench demonstrates that evaluating LLMs on isolated safety, value, or cultural metrics is insufficient, as models that look well-aligned on one axis often fail when all three requirements must co-occur. The unified SAVACU-based benchmark exposes these shortcomings and offers a more realistic, multi-dimensional testbed for future alignment research. The authors conclude that alignment methods and evaluation protocols should explicitly account for joint safety\u2013value\u2013culture constraints to better predict model behavior in real-world deployments."}}
{"id": "2602.11675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11675", "abs": "https://arxiv.org/abs/2602.11675", "authors": ["Edward Y. Chang"], "title": "Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs", "comment": "18 pages, 6 tables, 3 figures", "summary": "Machine learning systems that are \"right for the wrong reasons\" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.", "AI": {"tldr": "The paper identifies a causal flaw in how autoregressive ML systems learn, leading them to be \u201cright for the wrong reasons,\u201d and proposes a new objective and architecture (Epistemic Regret Minimization) that explicitly penalize causal reasoning errors, with theoretical guarantees and empirical validation on LLMs.", "motivation": "Many high-performing ML and LLM systems exploit spurious correlations or shortcuts, performing well in training but failing under distribution shifts. Current outcome-based training does not distinguish between correct answers derived from correct causal reasoning and those from flawed causal shortcuts, and offers no gradient signal to fix this. The authors want a principled, causal and belief-revision-based way to diagnose and correct such \u201cright for wrong reasons\u201d behavior.", "method": "1) Formally analyze autoregressive training and show it cannot distinguish associational P(Y|X) from interventional P(Y|do(X)), defining this failure as Rung Collapse. 2) Introduce the concept of Aleatoric Entrenchment to describe how outcome-based learning reinforces correct outputs generated via incorrect causal models. 3) Propose Epistemic Regret Minimization (ERM), an objective that penalizes causal reasoning errors independently of task success, and embed ERM in a three-layer architecture. 4) Provide three theory-grounded contributions: a Physical Grounding Theorem connecting actuator-independent actions with valid do-operations; a proof that ERM functions as an AGM-compliant causal belief revision operator; and a failure mode taxonomy with domain-independent guards. 5) Prove asymptotic recovery of the true interventional distribution with finite-sample bounds, and empirically test ERM-based feedback on 1,360 \u201ccausal trap\u201d scenarios across six advanced LLMs, measuring persistence of Rung Collapse, steerability, and error recovery.", "result": "Theoretical results: (a) Rung Collapse is formally characterized as the inability of autoregressive training to learn P(Y|do(X)) from purely observational signals; (b) actions that satisfy actuator independence correspond to valid do-operations; (c) ERM satisfies AGM postulates for belief revision and ensures avoidance of causal entrenchment; (d) under ERM, the agent asymptotically recovers the true interventional distribution with finite-sample guarantees. Empirical results: Experiments on 1,360 causal traps for six frontier LLMs show that Rung Collapse persists even in reasoning-augmented models (with a 3.7% failure rate reported for GPT-5.2), that more advanced models are paradoxically harder to steer via generic corrections (inverse scaling of steerability), and that targeted ERM feedback corrects 53\u201359% of entrenched causal errors where standard, outcome-level feedback does not.", "conclusion": "Autoregressive, outcome-based training leads to a structurally causal-blind pathology (Rung Collapse), causing models to be right for the wrong reasons and to become entrenched in flawed causal beliefs (Aleatoric Entrenchment). By introducing Epistemic Regret Minimization and a corresponding three-layer architecture grounded in do-calculus, belief revision theory, and a taxonomy of causal failure modes, the paper provides both theoretical guarantees and empirical evidence that models can be guided toward the correct interventional distribution and significantly reduce entrenched causal errors, even when conventional training appears successful."}}
{"id": "2602.11328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11328", "abs": "https://arxiv.org/abs/2602.11328", "authors": ["Amir Taubenfeld", "Zorik Gekhman", "Lior Nezry", "Omri Feldman", "Natalie Harris", "Shashir Reddy", "Romina Stella", "Ariel Goldstein", "Marian Croak", "Yossi Matias", "Amir Feder"], "title": "Evaluating Alignment of Behavioral Dispositions in LLMs", "comment": null, "summary": "As LLMs integrate into our daily lives, understanding their behavior becomes essential. In this work, we focus on behavioral dispositions$-$the underlying tendencies that shape responses in social contexts$-$and introduce a framework to study how closely the dispositions expressed by LLMs align with those of humans. Our approach is grounded in established psychological questionnaires but adapts them for LLMs by transforming human self-report statements into Situational Judgment Tests (SJTs). These SJTs assess behavior by eliciting natural recommendations in realistic user-assistant scenarios. We generate 2,500 SJTs, each validated by three human annotators, and collect preferred actions from 10 annotators per SJT, from a large pool of 550 participants. In a comprehensive study involving 25 LLMs, we find that models often do not reflect the distribution of human preferences: (1) in scenarios with low human consensus, LLMs consistently exhibit overconfidence in a single response; (2) when human consensus is high, smaller models deviate significantly, and even some frontier models do not reflect the consensus in 15-20% of cases; (3) traits can exhibit cross-LLM patterns, e.g., LLMs may encourage emotion expression in contexts where human consensus favors composure. Lastly, mapping psychometric statements directly to behavioral scenarios presents a unique opportunity to evaluate the predictive validity of self-reports, revealing considerable gaps between LLMs' stated values and their revealed behavior.", "AI": {"tldr": "The paper proposes a framework to compare LLMs\u2019 behavioral tendencies with human preferences using adapted psychological questionnaires, showing that LLMs often diverge from human-like behavior and display overconfident, sometimes misaligned dispositions.", "motivation": "As LLMs are increasingly used in everyday, socially situated interactions, it is important to understand whether their underlying behavioral tendencies resemble those of humans, and where they diverge, to assess their safety, reliability, and social alignment.", "method": "The authors convert traditional human self-report psychological items into Situational Judgment Tests (SJTs) suitable for LLMs, framing them as realistic user-assistant scenarios. They create 2,500 such SJTs, validated by three human annotators each, and collect preferred human actions from 10 annotators per SJT (550 participants total). They then test 25 different LLMs on these SJTs and compare model responses to the human preference distributions and to models\u2019 own psychometric-style self-reports.", "result": "Across 25 LLMs, the study finds systematic divergences from human preferences: LLMs are overconfident in a single response where humans disagree; small models, and even some frontier models, fail to match human consensus in a nontrivial fraction of high-consensus cases; and consistent cross-model behavioral patterns emerge, such as promoting emotional expression when humans tend to favor composure.", "conclusion": "The framework reveals that LLMs\u2019 behavioral dispositions frequently do not mirror human preference distributions and that there are notable gaps between models\u2019 stated values (in psychometric-style self-reports) and their actual behavioral recommendations. The proposed SJT-based mapping from psychometric items to realistic scenarios offers a novel way to study and quantify these misalignments and the predictive limits of self-reported values in LLMs."}}
{"id": "2602.11678", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.11678", "abs": "https://arxiv.org/abs/2602.11678", "authors": ["Chengwei Ma", "Zhen Tian", "Zhou Zhou", "Zhixian Xu", "Xiaowei Zhu", "Xia Hua", "Si Shi", "F. Richard Yu"], "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing", "comment": "4 pages, 3 figures. Accepted to ICASSP 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at https://github.com/gm-embodied/V2G-Audit.", "AI": {"tldr": "The paper introduces a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs, overcoming the structural blindness of pixel-based MLLMs and achieving large accuracy gains on electrical compliance checks.", "motivation": "Existing multimodal large language models, despite strong visual understanding on natural images, fail on engineering diagrams because they treat inputs as pixels and ignore explicit vector-defined relationships and topology needed for symbolic and structural reasoning. This structural blindness prevents reliable use of MLLMs for tasks like compliance checking in CAD and engineering workflows.", "method": "The authors propose V2G, a pipeline that takes CAD diagrams and converts them into property graphs. In these graphs, nodes correspond to components and edges capture connectivity and relationships, turning implicit geometric and symbolic structure into explicit machine-readable form. This structure-aware representation is then used to perform diagnostic electrical compliance checks and benchmark reasoning performance, contrasting it with pixel-based MLLM approaches.", "result": "On a diagnostic benchmark focused on electrical compliance checking, the V2G pipeline achieves substantial accuracy improvements across all categories of errors. In contrast, state-of-the-art pixel-based MLLMs perform close to chance level, underscoring their inability to reason about structural and topological relations in engineering schematics.", "conclusion": "Pixel-based multimodal models are systematically inadequate for tasks requiring structural and topological reasoning in engineering diagrams. Converting CAD data into explicit graph representations enables accurate, auditable reasoning and offers a practical route to deploying multimodal AI in engineering domains. The authors support further work by releasing their benchmark and implementation as open-source resources."}}
{"id": "2602.11358", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11358", "abs": "https://arxiv.org/abs/2602.11358", "authors": ["Zachary Pedram Dadfar"], "title": "When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing", "comment": "Code and data: https://doi.org/10.5281/zenodo.18567446", "summary": "Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce \"loop\" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce \"shimmer\" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.", "AI": {"tldr": "The paper investigates whether large language models' introspective language corresponds to their internal computations and shows that, under specific prompting and analysis, self-reports can reliably track activation dynamics.", "motivation": "To determine if the rich self-explanatory and introspective language produced by large language models reflects genuine access to internal computational states or is merely confabulated, and to develop tools to measure and characterize this relationship.", "method": "They introduce the Pull Methodology, a format-engineering protocol that elicits extended self-examination from LLMs, then use representation analysis on Llama 3.1 to find a direction in activation space distinguishing self-referential from ordinary descriptive processing. They examine its geometric properties (orthogonality to refusal direction, depth localization), and perform causal steering experiments. They correlate specific introspective vocabularies (e.g., \u201cloop\u201d, \u201cshimmer\u201d) with activation metrics such as autocorrelation and variability, and compare with matched non-self-referential contexts and with an independent model (Qwen 2.5-32B).", "result": "They identify a distinct activation-space direction tied to self-referential processing in Llama 3.1 that is orthogonal to known refusal directions and localized early in the network. Steering along this direction causally alters introspective output. When the model uses \u201cloop\u201d vocabulary, its activations show increased autocorrelation; under steering, \u201cshimmer\u201d vocabulary correlates with increased activation variability. The same terms in non-introspective contexts show no such correlations despite being used much more frequently. Qwen 2.5-32B independently exhibits different introspective vocabularies that correlate with different activation metrics, again specific to self-referential use.", "conclusion": "Self-referential language in transformer LLMs is not purely confabulatory; under appropriate elicitation and analysis, models\u2019 self-reports systematically track their internal activation dynamics. This suggests that carefully designed introspective protocols can provide reliable windows into LLM internal computation and opens a path for more principled interpretability via self-report."}}
{"id": "2602.11683", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11683", "abs": "https://arxiv.org/abs/2602.11683", "authors": ["Xin Xu", "Tong Yu", "Xiang Chen", "Haoliang Wang", "Julian McAuley", "Saayan Mitra"], "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces", "comment": "Work in Progress", "summary": "Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.", "AI": {"tldr": "The paper proposes ThinkRouter, a confidence-aware routing strategy that dynamically switches between latent reasoning and explicit token-level reasoning to improve both accuracy and efficiency in reasoning tasks.", "motivation": "Latent reasoning promises faster inference by reasoning in a continuous latent space instead of generating long explicit chains-of-thought, but its effectiveness is inconsistent. Empirical analysis shows that incorrect latent reasoning trajectories often lack low-confidence steps and that aggregating multiple low-confidence alternatives into soft embeddings can introduce noise and lead to overconfident but wrong conclusions. There is a need for a mechanism that uses latent reasoning when it is reliable and falls back to explicit reasoning when it is not, improving both accuracy and efficiency.", "method": "The authors first analyze confidence dynamics of large reasoning models under latent reasoning, comparing trajectories that end in correct vs. incorrect answers. Based on the insight that low-confidence regions are informative and that soft aggregation there can be harmful, they design ThinkRouter, an inference-time mechanism that monitors model confidence token by token. When confidence dips below a threshold, ThinkRouter routes reasoning from the latent space back to the discrete token space (explicit chain-of-thought); when confidence is high, it keeps reasoning in the latent space. The mechanism is applied across diverse large reasoning models and evaluated on STEM and coding tasks, compared against pure explicit CoT, pure latent reasoning, and random routing strategies.", "result": "Across a variety of STEM reasoning and coding benchmarks, ThinkRouter consistently improves performance over baselines. It increases Pass@1 accuracy by an average of 19.70 points compared to latent reasoning and explicit CoT baselines, while also shortening generation length by up to 15.55%. Additional analyses show that ThinkRouter can correct errors originating from both explicit CoT and latent reasoning and that its confidence-aware routing accelerates emission of end-of-thinking tokens by globally lowering overconfidence.", "conclusion": "ThinkRouter demonstrates that confidence-aware routing between latent and explicit reasoning can yield both higher accuracy and greater efficiency than using either mode alone. By exploiting low-confidence regions as signals to leave the latent space, it mitigates noise and overconfidence issues inherent in soft latent aggregation and better calibrates reasoning trajectories. This suggests that future reasoning systems should treat confidence dynamics as a central signal for adaptively choosing between discrete and latent reasoning representations."}}
{"id": "2602.11361", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11361", "abs": "https://arxiv.org/abs/2602.11361", "authors": ["Weili Shi", "Dongliang Guo", "Lehan Yang", "Tianlong Wang", "Hanzhang Yuan", "Sheng Li"], "title": "Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification", "comment": null, "summary": "Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens--tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.", "AI": {"tldr": "PPCV is a two-stage framework to improve LLM reasoning by detecting and editing critical tokens in the chain-of-thought, then choosing answers that are consistent across paraphrased queries.", "motivation": "LLMs perform well on many reasoning tasks but degrade on complex problems due to hallucinations and error accumulation in intermediate reasoning steps. Prior work indicates that certain 'critical tokens' strongly influence the reasoning trajectory, and modifying them can correct errors, but there is no reliable and practical way to identify and use these tokens.", "method": "PPCV has two stages. (1) Paraphrastic probing: generate an initial reasoning path from the original question, then concatenate paraphrased versions of the question with this path and inspect token-level predictions. Critical tokens are those where the model\u2019s predicted top-1 token diverges from the expected next token in the original reasoning path, with an additional criterion to select the final critical token. (2) Consistency verification: replace the identified critical token with alternative candidate tokens and roll out new reasoning paths for both original and paraphrased questions. The system then selects the final answer by checking which candidate leads to consistent outputs across these parallel reasoning processes.", "result": "Across multiple benchmarks and with several mainstream LLMs, PPCV yields substantially better reasoning performance than prior baselines, indicating that targeted manipulation of critical tokens and consistency-based selection can effectively reduce reasoning errors and hallucinations.", "conclusion": "Carefully identifying and editing critical tokens in LLM reasoning, combined with cross-paraphrase consistency checks, is an effective way to enhance reasoning reliability on complex tasks. PPCV provides a practical framework that can be layered on top of existing LLMs to improve their problem-solving accuracy."}}
{"id": "2602.11717", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11717", "abs": "https://arxiv.org/abs/2602.11717", "authors": ["Weihong Lin", "Lin Sun", "Qilong Shi", "Aomufei Yuan", "Yuxuan Tian", "Zhengyang Wang", "Guangxiang Zhao", "Xiangzheng Zhang", "Tong Yang"], "title": "Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging", "comment": null, "summary": "Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.", "AI": {"tldr": "The paper introduces SCF-RKL, a new model merging framework that reduces interference between merged large language models via sparse, reverse-KL-guided parameter updates, achieving better performance and stability than prior merging methods on diverse benchmarks.", "motivation": "Existing model merging methods for large language models mostly use simple parameter-space heuristics (e.g., linear weight averaging or low-rank updates) that assume linear additivity of model weights. These often cause strong interference between the behaviors of the merged models, degrading generalization and leading to unstable generations (repetition, incoherence). There is a need for a merging approach that explicitly controls functional interference and preserves stable representations while still integrating new, specialized capabilities.", "method": "The authors propose Sparse Complementary Fusion with reverse KL (SCF-RKL). Instead of directly linearly combining parameters, they measure functional divergence between models using reverse Kullback-Leibler divergence on their output distributions. They then compute sparse, distribution-aware updates that selectively incorporate only those parameters from the specialized model that provide complementary behavior, guided by the mode-seeking property of reverse KL. This sparsity-inducing, mode-seeking mechanism is designed to minimize harmful interference and preserve core capabilities of the base model while adding new skills.", "result": "Across multiple model sizes and architectures, including reasoning-focused and instruction-tuned LLMs, SCF-RKL is evaluated on 24 benchmarks covering advanced reasoning, general reasoning and knowledge, instruction following, safety, and vision classification. The method consistently outperforms existing model merging approaches on these benchmarks while also yielding more stable generations (less repetition and incoherence) and maintaining strong generalization.", "conclusion": "SCF-RKL demonstrates that controlling functional interference via reverse-KL-based, sparse complementary fusion in weight space is a more effective paradigm for model merging than naive parameter-space heuristics. By selectively merging only distributionally complementary parameters, SCF-RKL can reliably integrate specialized capabilities into large language models while preserving stability and generalization, offering a practical alternative to costly joint retraining or full fine-tuning."}}
{"id": "2602.11364", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11364", "abs": "https://arxiv.org/abs/2602.11364", "authors": ["Arpit Singh Gautam", "Kailash Talreja", "Saurabh Jha"], "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods", "comment": null, "summary": "Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, positing that factual truths act as stable attractors on a generative manifold while hallucinations are unstable. We introduce the Generative Stress Test, claims are corrupted with noise and reconstructed using a discrete text diffusion model. We define Semantic Energy, a metric measuring the semantic divergence between the original claim and its reconstruction using an NLI critic. Unlike vector space errors, Semantic Energy isolates deep factual contradictions. We further propose a Hybrid Calibration fusing this stability signal with discriminative confidence. Extensive experiments on FEVER demonstrate DiffuTruth achieves a state of the art unsupervised AUROC of 0.725, outperforming baselines by 1.5 percent through the correction of overconfident predictions. Furthermore, we show superior zero shot generalization on the multi hop HOVER dataset, outperforming baselines by over 4 percent, confirming the robustness of thermodynamic truth properties to distribution shifts.", "AI": {"tldr": "Proposes DiffuTruth, an unsupervised method to detect and calibrate LLM hallucinations using a diffusion-based generative stress test and a new Semantic Energy metric.", "motivation": "LLMs often hallucinate confident but incorrect statements, and existing uncertainty or confidence metrics struggle to detect when a model is confidently wrong. There is a need for a robust, unsupervised way to assess factual reliability and calibrate confidence, especially under distribution shift.", "method": "Model factual claims as points on a generative manifold where true facts are stable attractors and hallucinations are unstable, drawing on non-equilibrium thermodynamics. Apply a Generative Stress Test: corrupt each claim with noise and reconstruct it using a discrete text diffusion model. Compute Semantic Energy, the semantic divergence between original and reconstructed claims, using an NLI-based critic to capture deep factual contradictions rather than surface or embedding distance. Combine this stability-derived score with standard discriminative confidence in a Hybrid Calibration approach.", "result": "On the FEVER benchmark, DiffuTruth achieves state-of-the-art unsupervised AUROC of 0.725 for fact verification, improving over baselines by 1.5 points by better correcting overconfident wrong predictions. It also generalizes zero-shot to the multi-hop HOVER dataset, surpassing baselines by more than 4 points, showing robustness across datasets.", "conclusion": "Modeling truth as a thermodynamic stability property on a generative manifold enables more reliable unsupervised detection of hallucinations and better confidence calibration in LLMs. The diffusion-based stress test with Semantic Energy captures factual inconsistency more effectively than prior uncertainty metrics and is robust to distribution shifts, making it promising for safer deployment of LLMs."}}
{"id": "2602.11729", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11729", "abs": "https://arxiv.org/abs/2602.11729", "authors": ["Thomas Jiralerspong", "Trenton Bricken"], "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs", "comment": null, "summary": "Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.", "AI": {"tldr": "This paper applies and extends cross-architecture model diffing using crosscoders to detect safety-relevant, model-specific behaviors in different LLMs.", "motivation": "Existing model diffing work mostly compares a base model with its finetune and struggles with cross-architecture comparisons, yet new LLMs often use novel architectures and may contain hidden, safety-critical behaviors that need to be surfaced.", "method": "The authors adapt crosscoders for the first time to cross-architecture model diffing and propose Dedicated Feature Crosscoders (DFCs), an architectural modification that better isolates features unique to one model. They then use these tools in an unsupervised way to compare different LLMs\u2019 internal representations.", "result": "Using DFC-based cross-architecture diffing, they uncover unsupervised latent features corresponding to behaviors such as Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B.", "conclusion": "Cross-architecture crosscoder model diffing, especially with Dedicated Feature Crosscoders, is an effective method for identifying semantically meaningful and safety-relevant behavioral differences between distinct AI model architectures."}}
{"id": "2602.11391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11391", "abs": "https://arxiv.org/abs/2602.11391", "authors": ["Md Tanvir Rouf Shawon", "Mohammad Sabik Irbaz", "Hadeel R. A. Elyazori", "Keerti Reddy Resapu", "Yili Lin", "Vladimir Franzuela Cardenas", "Farrokh Alemi", "Kevin Lybarger"], "title": "Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection", "comment": null, "summary": "Objective: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents. The simulator generates realistic, controllable patient interactions that systematically vary across medical, linguistic, and behavioral dimensions, allowing annotators and an independent AI judge to assess agent performance, identify hallucinations and inaccuracies, and characterize risk patterns across diverse patient populations. Methods: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation: (1) medical profiles constructed from electronic health records in the All of Us Research Program; (2) linguistic profiles modeling variation in health literacy and condition-specific communication patterns; and (3) behavioral profiles representing empirically observed interaction patterns, including cooperation, distraction, and adversarial engagement. We evaluated the simulator's effectiveness in identifying errors in an AI decision aid for antidepressant selection. Results: We generated 500 conversations between the patient simulator and the AI decision aid across systematic combinations of five linguistic and three behavioral profiles. Human annotators assessed 1,787 medical concepts across 100 conversations, achieving high agreement (F1=0.94, \\k{appa}=0.73), and the LLM judge achieved comparable agreement with human annotators (F1=0.94, \\k{appa}=0.78; paired bootstrap p=0.21). The simulator revealed a monotonic degradation in AI decision aid performance across the health literacy spectrum: rank-one concept retrieval accuracy increased from 47.9% for limited health literacy to 69.1% for functional and 81.6% for proficient.", "AI": {"tldr": "They built a patient simulator to automatically and scalably test healthcare chatbots, showing it can reveal how performance degrades with lower health literacy and difficult patient behaviors.", "motivation": "Evaluating healthcare conversational agents with real patients is risky, expensive, and hard to scale. Existing benchmarks rarely capture how agents behave across diverse, realistic patient types (different medical histories, language abilities, and interaction styles). The authors want a way to systematically, safely, and cheaply stress\u2011test these systems and uncover hallucinations and risk patterns before deployment.", "method": "They design a synthetic patient simulator guided by the NIST AI Risk Management Framework. The simulator combines three profile types: (1) medical profiles derived from All of Us EHR data to represent realistic conditions; (2) linguistic profiles modeling health\u2011literacy levels and condition\u2011specific ways patients talk; and (3) behavioral profiles (e.g., cooperative, distracted, adversarial). The simulator interacts with a target healthcare chatbot, generating conversations. Human annotators and an LLM judge then label medical concepts, errors, and hallucinations in these dialogues to evaluate the chatbot. They test this setup by using it to evaluate an AI decision aid for antidepressant selection, creating 500 simulated conversations that systematically vary linguistic and behavioral profiles.", "result": "Using 500 simulator\u2013agent conversations spanning five linguistic and three behavioral profiles, human annotators labeled 1,787 medical concepts in 100 conversations, achieving high inter\u2011annotator agreement (F1=0.94, \u03ba=0.73). An LLM\u2011based judge matched human performance closely (F1=0.94, \u03ba=0.78; p=0.21, indicating no significant difference). Analysis showed clear risk patterns: the AI decision aid\u2019s accuracy in retrieving the top\u2011ranked correct medical concept declined as patient health literacy decreased, from 81.6% (proficient) to 69.1% (functional) and 47.9% (limited).", "conclusion": "The proposed patient simulator is an effective tool for scalable, automated evaluation of healthcare conversational agents. It can generate realistic, diverse patient interactions, supports both human and AI\u2011based judging, and uncovers systematic vulnerabilities\u2014such as worse performance for patients with lower health literacy or challenging behaviors. This framework can help characterize and mitigate risks of clinical AI systems across diverse patient populations before real\u2011world deployment."}}
{"id": "2602.11745", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11745", "abs": "https://arxiv.org/abs/2602.11745", "authors": ["Songlin Lyu", "Lujie Ban", "Zihang Wu", "Tianqi Luo", "Jirong Liu", "Chenhao Ma", "Yuyu Luo", "Nan Tang", "Shipeng Qi", "Heng Lin", "Yongchao Liu", "Chuntao Hong"], "title": "Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]", "comment": null, "summary": "Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.", "AI": {"tldr": "They build a large, unified benchmark (Text2GQL-Bench) and evaluation method to test how well models translate natural language into various graph query languages, revealing large gaps\u2014especially for ISO-GQL\u2014and showing that exposure to enough ISO-GQL training data greatly boosts performance.", "motivation": "Graph databases are powerful for tasks involving complex relationships, but natural-language interfaces (Text-to-GQL) for them are underdeveloped because current datasets are small, narrow in domain, and support few query languages with limited evaluation metrics. This lack of broad, high-quality benchmarks makes it hard to systematically compare and improve Text-to-GQL models, especially across different graph query dialects like ISO-GQL.", "method": "They construct Text2GQL-Bench, a unified benchmark containing 178k natural-language question and graph-query pairs across 13 domains and multiple graph query languages, using a scalable framework that can generate data at different abstraction levels and with heterogeneous sources. They also design a multi-dimensional evaluation protocol that separately reports grammatical validity, query similarity, semantic alignment, and execution accuracy, then systematically evaluate various LLMs (zero-shot, few-shot, and fine-tuned) on these metrics, with special focus on ISO-GQL.", "result": "The benchmark and evaluation reveal a substantial performance gap in ISO-GQL generation: strong LLMs achieve only up to 4% execution accuracy in zero-shot mode; a fixed 3-shot prompt improves execution accuracy to about 50% but grammatical validity remains below 70%. A fine-tuned 8B open-weight model obtains 45.1% execution accuracy and 90.8% grammatical validity, indicating that exposure to sufficient ISO-GQL data substantially improves performance.", "conclusion": "Text2GQL-Bench provides a comprehensive, multi-domain, multi-language benchmark and evaluation framework that exposes significant weaknesses in current Text-to-GQL systems, particularly for ISO-GQL, but also shows that these deficits can largely be overcome with enough targeted training data. This establishes a foundation for more systematic development and comparison of future Text-to-GQL models and agents over graph databases."}}
{"id": "2602.11424", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11424", "abs": "https://arxiv.org/abs/2602.11424", "authors": ["Zecheng Wang", "Deyuan Liu", "Chunshan Li", "Yupeng Zhang", "Zhengyun Zhao", "Dianhui Chu", "Bingning Wang", "Dianbo Sui"], "title": "Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives", "comment": null, "summary": "Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting provides weak sharpening when the model is already confident. Existing methods fail to resolve the resulting plasticity--stability dilemma, often suppressing necessary learning signals alongside harmful ones. To address this issue, we unify token-level SFT objectives within a generalized deformed-log family and expose a universal gate $\\times$ error gradient structure, where the gate controls how much the model trusts its current prediction. By employing the Cayley transform, we map the model's continuously evolving uncertainty onto a continuous focus trajectory, which enables seamless interpolation between scenarios involving uncertain novel concepts and those involving well-established knowledge. We then introduce Dynamic Entropy Fine-Tuning (DEFT), a parameter-free objective that modulates the trust gate using distribution concentration (R\u00e9nyi-2 entropy) as a practical proxy for the model's predictive state. Extensive experiments and analyses demonstrate that DEFT achieves a better balance between exploration and exploitation, leading to improved overall performance.", "AI": {"tldr": "They propose a new fine-tuning objective (DEFT) that dynamically adjusts how much the model trusts its current predictions, improving performance versus standard NLL.", "motivation": "Standard NLL with uniform token weighting can both overfit noisy, low-probability tokens and undertrain already-confident predictions, causing a plasticity\u2013stability trade-off that existing fixes don\u2019t solve cleanly.", "method": "They frame token-level SFT losses as belonging to a generalized deformed-log family and show all can be written as a gate \u00d7 error-gradient, where the gate is a function of model trust. Using the Cayley transform, they map prediction uncertainty to a continuous \"focus\" trajectory and instantiate a practical, parameter-free objective called DEFT that sets the gate based on the R\u00e9nyi-2 entropy (concentration) of the predictive distribution.", "result": "Across extensive experiments, DEFT yields better exploration\u2013exploitation balance than standard NLL and prior alternatives, giving higher overall performance and more robust learning from mixed-quality supervision.", "conclusion": "Dynamic, entropy-based modulation of token-level learning signals is a more effective SFT objective than uniform NLL, resolving much of the plasticity\u2013stability tension by adapting to the model\u2019s own uncertainty during training."}}
{"id": "2602.11749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11749", "abs": "https://arxiv.org/abs/2602.11749", "authors": ["Zibo Xiao", "Jun Sun", "Junjie Chen"], "title": "AIR: Improving Agent Safety through Incident Response", "comment": null, "summary": "Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.", "AI": {"tldr": "AIR is a framework that lets LLM agents automatically detect, contain, recover from, and then prevent future safety incidents using an incident-response loop and learned guardrail rules.", "motivation": "LLM agents are widely used but current safety work mainly tries to prevent failures upfront, with little support for what to do when incidents actually happen. There is a gap in methods that can autonomously detect, manage, and learn from incidents during agent operation, similar to incident response in traditional security engineering.", "method": "The authors design AIR, an incident response framework integrated into an LLM agent\u2019s execution loop. AIR defines a domain-specific language to specify the steps of incident response. It performs semantic checks over the environment state and recent interaction context to detect incidents; then uses the agent\u2019s tools to carry out containment and recovery actions; and finally synthesizes new guardrail rules during the eradication phase to block similar incidents in the future. They test AIR on three types of LLM agents and perform ablation and efficiency studies.", "result": "Across three representative agent types, AIR attains over 90% success in incident detection, remediation, and eradication. Experiments show that each major component of AIR\u2019s design is necessary, that the framework responds quickly with moderate runtime overhead, and that rules generated by LLMs can be nearly as effective as those written by human developers in multiple domains.", "conclusion": "Incident response can be automated and embedded as a core part of LLM agent safety. AIR\u2019s structured lifecycle, integrated with agents\u2019 execution and powered by LLM-generated rules, substantially improves safety by enabling detection, containment, recovery, and prevention of recurring incidents. This supports the view that incident response should be treated as a first-class safety mechanism for LLM agents."}}
{"id": "2602.11444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11444", "abs": "https://arxiv.org/abs/2602.11444", "authors": ["Muskaan Chopra", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety", "comment": "Accepted at ECIR 2026", "summary": "Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.", "AI": {"tldr": "The paper evaluates how well instruction-tuned large language models can detect critical meaning errors in machine translation and shows they outperform traditional encoder-only models, highlighting error detection as key for safe and responsible multilingual AI.", "motivation": "Critical errors in machine translation\u2014such as factual distortions, reversals of intent, or biased renderings\u2014can harm reliability, fairness, and safety in cross-lingual communication, which is increasingly important for information access, policy communication, and equitable knowledge sharing. There is a need for more robust, scalable methods to automatically detect such high-impact errors, especially in high-stakes or underrepresented language contexts.", "method": "The authors systematically evaluate instruction-tuned large language models of varying sizes and configurations on publicly available datasets for critical MT error detection. They compare different adaptation strategies\u2014zero-shot prompting, few-shot prompting, and fine-tuning\u2014against encoder-only baselines like XLM-R and ModernBERT, measuring performance on identifying critical semantic and bias-related translation errors.", "result": "Across experiments, larger and better-adapted instruction-tuned LLMs consistently improve performance in detecting critical MT errors. All three strategies\u2014zero-shot, few-shot, and fine-tuning\u2014yield gains, and the LLM-based approaches outperform strong encoder-only baselines such as XLM-R and ModernBERT on the evaluated benchmarks.", "conclusion": "Instruction-tuned LLMs are effective tools for detecting critical meaning errors in machine translation and can surpass traditional encoder-only systems. Enhancing such error detection is framed as an essential safeguard for safe, trustworthy, and socially accountable multilingual AI, helping mitigate disinformation, miscommunication, and linguistic harm, particularly in high-stakes or low-resource settings. The authors commit to releasing their code to support further research and practical deployment."}}
{"id": "2602.11767", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11767", "abs": "https://arxiv.org/abs/2602.11767", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Holger Boche"], "title": "TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents", "comment": null, "summary": "Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.", "AI": {"tldr": "They propose TSR, a training-time tree-style search method to generate higher-quality trajectories for multi-turn RL with LLM agents, improving performance and stability without changing the RL objective.", "motivation": "Multi-turn RL for LLM-based agents is difficult due to sparse/delayed rewards, stochastic environments, and problems like poor exploitation and mode collapse from naive trajectory sampling. There is a desire to leverage search-style scaling, typically used at inference, to improve training.", "method": "TSR (Trajectory-Search Rollouts) uses lightweight tree-style search during rollout generation in training. At each turn it selects high-scoring actions based on task-specific feedback, constructing better trajectories while keeping the same optimization objective. They instantiate TSR via best-of-N sampling, beam search, and shallow lookahead search, and integrate it with PPO and GRPO as underlying optimizers.", "result": "Using TSR with PPO and GRPO yields up to 15% performance improvement and more stable learning on multi-turn tasks such as Sokoban, FrozenLake, and WebShop, at the cost of a one-time increase in training compute for trajectory generation.", "conclusion": "Shifting search from inference to training rollouts via TSR offers a simple, optimizer-agnostic way to obtain stronger, more stable multi-turn RL agents. TSR is complementary to existing RLHF-style and rejection-sampling-based methods, serving as a general mechanism to enhance multi-turn learning for LLM-driven agents."}}
{"id": "2602.11451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11451", "abs": "https://arxiv.org/abs/2602.11451", "authors": ["Ahmadreza Jeddi", "Marco Ciccone", "Babak Taati"], "title": "LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation", "comment": "ICLR2026", "summary": "Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.", "AI": {"tldr": "The paper proposes LoopFormer, a looped Transformer that supports adaptive, budget-conditioned reasoning by training on variable-length computation trajectories.", "motivation": "Existing looped Transformers show strong reasoning abilities but require a fixed number of loop iterations at train and test time, preventing them from flexibly trading off performance against available compute. The authors want a model that can adapt its computational depth dynamically under different compute budgets while maintaining good performance.", "method": "They introduce LoopFormer, which is trained on variable-length looping trajectories. The key technique is a shortcut-consistency training scheme that enforces alignment between representations obtained with few loops and many loops. Each loop iteration is explicitly conditioned on the current time and step size so that representations evolve coherently across different-length trajectories, avoiding drift or stagnation when more loops are used.", "result": "On language modeling and reasoning benchmarks, LoopFormer maintains strong performance even when forced to run under tight compute constraints (few loops) and continues to improve as more computational budget (more loops) is allocated. It scales performance smoothly with additional iterations.", "conclusion": "Looped Transformers, when trained with shortcut-consistency and time/step-size conditioning as in LoopFormer, naturally support adaptive, budget-aware computation. This suggests they are a promising architecture for controllable and compute-efficient large language models that can adjust their reasoning depth on demand."}}
{"id": "2602.11771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11771", "abs": "https://arxiv.org/abs/2602.11771", "authors": ["S\u00e9bastien Gigot--L\u00e9andri", "Ga\u00e9tan Morand", "Alexis Joly", "Fran\u00e7ois Munoz", "David Mouillot", "Christophe Botella", "Maximilien Servajean"], "title": "How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?", "comment": null, "summary": "Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of species prevalence and community composition. We present MaxExp, a decision-driven binarization framework that selects the most probable species assemblage by directly maximizing a chosen evaluation metric. MaxExp requires no calibration data and is flexible across several scores. We also introduce the Set Size Expectation (SSE) method, a computationally efficient alternative that predicts assemblages based on expected species richness. Using three case studies spanning diverse taxa, species counts, and performance metrics, we show that MaxExp consistently matches or surpasses widely used thresholding and calibration methods, especially under strong class imbalance and high rarity. SSE offers a simpler yet competitive option. Together, these methods provide robust, reproducible tools for multispecies SDM binarization.", "AI": {"tldr": "The paper introduces MaxExp and SSE, two new methods to convert probabilistic species distribution model outputs into binary presence-absence assemblages, optimizing evaluation metrics without calibration data and outperforming common thresholding methods, especially for rare species and imbalanced data.", "motivation": "Standard binarization of probabilistic SDM outputs into presence-absence maps is heuristic and can bias estimates of prevalence and community composition, undermining ecological inference and conservation planning. There is a need for principled, decision-focused binarization methods that work well for multispecies settings and under class imbalance, without requiring additional calibration data.", "method": "The authors propose MaxExp, a decision-theoretic framework that, for each site, selects the species assemblage (set of presences) that maximizes a chosen evaluation metric (e.g., F-score, Jaccard) given the SDM probabilities. This directly optimizes over all possible assemblages instead of applying per-species thresholds. They also develop SSE (Set Size Expectation), which predicts an assemblage by matching the expected species richness implied by the probabilities, offering a computationally cheaper approximation. Both methods are tested in three case studies with different taxa, number of species, and metrics, and compared to standard thresholding and calibration-based binarization methods.", "result": "Across three empirical case studies, MaxExp consistently equals or outperforms common thresholding rules and calibration-based approaches in terms of the target evaluation metrics, with particularly strong gains when species are rare and classes are highly imbalanced. SSE, while simpler and less computationally intensive, shows performance close to MaxExp and competitive with or better than existing methods.", "conclusion": "MaxExp provides a principled, metric-driven way to binarize multispecies SDM outputs without calibration data, improving accuracy and robustness over heuristic thresholds, especially in challenging class-imbalance settings. SSE offers a practical alternative when computational efficiency is a concern. Together, they constitute reproducible, general tools for constructing presence-absence assemblages from probabilistic SDMs, with direct relevance for ecological analyses and conservation decision-making."}}
{"id": "2602.11460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11460", "abs": "https://arxiv.org/abs/2602.11460", "authors": ["Guangxin Zhao", "Jiahao Zheng", "Malaz Boustani", "Jarek Nabrzyski", "Meng Jiang", "Yiyu Shi", "Zhi Zheng"], "title": "ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias", "comment": null, "summary": "Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.", "AI": {"tldr": "The paper introduces ADRD-Bench, the first benchmark specifically for evaluating large language models on Alzheimer\u2019s and related dementias, combining unified clinical QA and practical caregiving QA, and shows that even strong LLMs remain unreliable for caregiving-related reasoning.", "motivation": "Existing LLM evaluation benchmarks barely cover Alzheimer\u2019s Disease and Related Dementias, despite the importance of accurate, reliable AI support in this domain. Current datasets focus mostly on general clinical knowledge and neglect real-world caregiving contexts, which are crucial for day-to-day ADRD management. The authors aim to create a rigorous, domain-specific benchmark to systematically assess and drive improvement of LLM performance for ADRD-related clinical and caregiving tasks.", "method": "The authors construct ADRD-Bench with two main components. (1) ADRD Unified QA: 1,352 ADRD-related questions drawn and consolidated from seven existing medical benchmarks to form a unified test of clinical knowledge. (2) ADRD Caregiving QA: 149 novel questions designed with guidance from the Aging Brain Care (ABC) program, an evidence-based brain health management program, to capture practical caregiving scenarios and address the lack of contextual caregiving content in prior benchmarks. They then evaluate 33 state-of-the-art LLMs (open-weight general, open-weight medical, and closed-source general models) on both components and measure accuracy, also performing qualitative case studies of models\u2019 reasoning and stability.", "result": "On ADRD-Bench, open-weight general models achieve accuracies between 0.63 and 0.93 (mean 0.78, std 0.09), open-weight medical models between 0.48 and 0.93 (mean 0.82, std 0.13), and closed-source general models between 0.83 and 0.91 (mean 0.89, std 0.03). Some top-performing models exceed 0.9 accuracy overall. However, case studies show that despite good aggregate scores, their reasoning is inconsistent, and answers can be unstable or unreliable, especially when grounded in day-to-day caregiving contexts.", "conclusion": "ADRD-Bench fills a critical gap by providing the first dedicated benchmark for evaluating LLMs on Alzheimer\u2019s and related dementias, including both consolidated clinical QA and realistic caregiving QA. Quantitative results indicate that leading models can reach high accuracy, but qualitative analysis exposes unreliability in reasoning and stability, suggesting they are not yet trustworthy for ADRD caregiving support. The authors conclude that domain-specific improvements and better grounding in real caregiving data are needed to make LLMs safer and more useful in ADRD care, and they release the benchmark publicly to spur further research."}}
{"id": "2602.11780", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11780", "abs": "https://arxiv.org/abs/2602.11780", "authors": ["Jinfang Wang", "Jiajie Liu", "Jianwei Wu", "Ziqin Luo", "Zhen Chen", "Chunlei Li", "Biao Han", "Tao Deng", "Yi Li", "Shuanglong Li", "Lin Liu"], "title": "RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation", "comment": "10 pages, 3 figures", "summary": "In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-through rate(CTR). This separation often leads to misaligned optimization objectives and low funnel efficiency, limiting global optimality.\n  To address these limitations, we propose RELATE, a reinforcement learning-based end-to-end framework that unifies generation and objective alignment within a single model. Instead of decoupling text generation from downstream metric alignment, RELATE integrates performance and compliance objectives directly into the generation process via policy learning. To better capture ultimate advertiser value beyond click-level signals, We incorporate conversion-oriented metrics into the objective and jointly model them with compliance constraints as multi-dimensional rewards, enabling the model to generate high-quality ad texts that improve conversion performance under policy constraints.\n  Extensive experiments on large-scale industrial datasets demonstrate that RELATE consistently outperforms baselines. Furthermore, online deployment on a production advertising platform yields statistically significant improvements in click-through conversion rate(CTCVR) under strict policy constraints, validating the robustness and real-world effectiveness of the proposed framework.", "AI": {"tldr": "RELATE is an end-to-end reinforcement learning framework that jointly generates ad texts and optimizes them for online performance and compliance, outperforming traditional two-stage systems.", "motivation": "Current online advertising systems separate ad text generation from performance optimization, causing misaligned objectives, low funnel efficiency, and suboptimal global performance. There is a need for a unified approach that directly optimizes ad text for both engagement and advertiser value while satisfying policy constraints.", "method": "RELATE uses reinforcement learning to unify ad text generation and objective alignment in a single model. It integrates performance metrics (e.g., CTR and conversion-oriented signals) and compliance objectives as multi-dimensional rewards in a policy learning framework, so the generator learns to produce ad texts that are both effective and policy-compliant.", "result": "On large-scale industrial offline datasets, RELATE consistently outperforms existing baselines. In online A/B tests on a production advertising platform, it delivers statistically significant gains in click-through conversion rate (CTCVR) while respecting strict policy constraints.", "conclusion": "End-to-end reinforcement learning for ad text generation with multi-dimensional reward modeling can surpass traditional two-stage pipelines, simultaneously improving conversion performance and maintaining policy compliance in real-world advertising systems."}}
{"id": "2602.11488", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.11488", "abs": "https://arxiv.org/abs/2602.11488", "authors": ["Jayadev Billa"], "title": "When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration", "comment": "25 pages, 18 tables, 8 languages, benchmark and code at https://github.com/jb1999/alme-benchmark", "summary": "When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\\% text dominance under audio-text conflict versus 1.6\\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\\%) exceeds cascade accuracy (93.9\\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.\n  This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\\% to 33\\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\\%), while LoRA on the language model halves it ($-$23.9\\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.", "AI": {"tldr": "The paper shows that when audio and text disagree, current speech-enabled LLMs strongly over-trust text compared with analogous text\u2011text conflicts, and introduces a large benchmark (ALME) to systematically study this \u201ctext dominance.\u201d", "motivation": "Multimodal LLMs are widely used for speech interfaces, but we lack a clear understanding of how they arbitrate between conflicting audio and text signals, which is crucial for reliability and safety. Existing speech benchmarks mainly test recognition and understanding, not cross\u2011modal conflict resolution. The authors aim to characterize and explain a systematic bias toward trusting text over audio, and to define modality arbitration as an independent axis of model reliability.", "method": "They build ALME, a benchmark of 57k+ controlled audio\u2011text conflict examples in 8 languages, where reliability cues are matched across modalities. They measure how often models follow text vs audio under audio\u2011text conflict and compare this to a text\u2011text conflict condition. They run controlled variations: forcing transcription before answering, changing instructions (e.g., describing text as deliberately corrupted), and a fine\u2011tuning ablation (training only the audio projection layer vs applying LoRA on the LLM). They evaluate four state\u2011of\u2011the\u2011art audio\u2011LLMs including Gemini 2.0 Flash across languages and analyze the changes in \u201ctext dominance\u201d and accuracy.", "result": "For Gemini 2.0 Flash, text is chosen 16.6% of the time in audio\u2011text conflict, vs only 1.6% in text\u2011text conflict with identical reliability cues\u2014a roughly 10x increase in text preference. Audio\u2011only performance (97.2%) is higher than cascade (audio\u2192text\u2192LLM) performance (93.9%), showing audio embeddings contain more usable information than transcripts. Forcing transcription worsens text dominance (from 19% to 33%) without accuracy gains, while instructing that text is corrupted reduces text dominance by 80%. Fine\u2011tuning only the audio projection layer increases text dominance by 26.5 percentage points, whereas LoRA on the LLM halves it (\u221223.9 points), implicating the LLM\u2019s reasoning layers rather than the audio encoder. Similar qualitative patterns appear across four audio\u2011LLMs and eight languages, though magnitudes vary.", "conclusion": "The main issue in audio\u2011text conflict is not a lack of audio information but an arbitration asymmetry: LLM reasoning more easily exploits text than audio representations, leading to \u201ctext dominance.\u201d This bias can be manipulated by instructions and by where fine\u2011tuning is applied, and it varies across models and languages. The authors argue that modality arbitration is a distinct and important reliability dimension for speech\u2011enabled LLMs, not captured by standard benchmarks, and that future work should explicitly measure and mitigate text dominance rather than just improving transcription or audio encoders."}}
{"id": "2602.11782", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11782", "abs": "https://arxiv.org/abs/2602.11782", "authors": ["Yihao Liu", "Ziyun Zhang", "Zile He", "Huaqian Cai"], "title": "FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning", "comment": null, "summary": "LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.", "AI": {"tldr": "The paper proposes Execute-Summarize (ES), a framework that first lets an LLM solve tasks with tools, then separately reconstructs a structured workflow, improving accuracy and robustness versus prior online workflow-building methods, evaluated on the new FlowBench benchmark.", "motivation": "LLMs can reason and use tools but struggle to reliably translate free-form reasoning into precise, reusable structured workflows. Existing methods that build workflows on-the-fly mix execution and workflow construction, causing interference and inaccuracies. There is a need for a mechanism that can both solve tasks and produce accurate, grounded workflows.", "method": "Model workflows as sequences of tool uses and decouple solving and workflow construction. The ES framework has two stages: (1) Execute \u2013 the LLM completes the task using tools, leaving execution traces; (2) Summarize \u2013 a separate pass over these traces reconstructs a structured workflow. A new benchmark, FlowBench, is built to evaluate workflow construction methods. Experiments compare ES against prior approaches that interleave execution and workflow building.", "result": "On FlowBench and related evaluations, ES yields higher workflow accuracy and robustness than baselines that construct workflows during execution, showing less susceptibility to interference between reasoning and workflow design.", "conclusion": "Separating task execution from workflow reconstruction via the Execute-Summarize framework provides a more reliable way to ground LLM reasoning in structured workflows. FlowBench enables systematic evaluation, and results suggest ES is a strong paradigm for future systems requiring accurate, reusable tool-usage workflows."}}
{"id": "2602.11509", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.11509", "abs": "https://arxiv.org/abs/2602.11509", "authors": ["David Wan", "Han Wang", "Ziyang Wang", "Elias Stengel-Eskin", "Hyunji Lee", "Mohit Bansal"], "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning", "comment": "29 pages. Code and data are available at https://github.com/meetdavidwan/murgat", "summary": "Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.", "AI": {"tldr": "MuRGAt is a benchmark to test how well multimodal LLMs ground their multi-step reasoning in specific parts of multimodal inputs with precise, verifiable citations.", "motivation": "Existing multimodal grounding benchmarks are too simple, focusing on direct observations or narrow modalities, and do not rigorously evaluate how models attribute facts in complex reasoning over heterogeneous multimodal inputs.", "method": "Design a benchmark, MuRGAt, where models receive multimodal inputs (video, audio, etc.) and must produce answers with explicit, step-by-step reasoning and fine-grained citations that identify both modality and temporal segments; additionally, build an automatic evaluation framework aligned with human judgments to assess fact-level attribution quality.", "result": "Experiments with current strong MLLMs show that they often hallucinate citations even when their reasoning and answers are correct; automatic metrics are shown to correlate well with human evaluations on this task.", "conclusion": "There is a fundamental mismatch between models\u2019 internal reasoning and their ability to provide accurate, verifiable grounding: pushing for deeper reasoning or stricter grounding structure currently tends to reduce answer accuracy, revealing a major gap that future MLLMs must address."}}
{"id": "2602.11790", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11790", "abs": "https://arxiv.org/abs/2602.11790", "authors": ["Lingyong Yan", "Jiulong Wu", "Dong Xie", "Weixian Shi", "Deguo Xia", "Jizhou Huang"], "title": "Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation", "comment": "For more information, visit the project website: https://robitsg.github.io/LASEV/", "summary": "Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.", "AI": {"tldr": "LAVES is a hierarchical multi-agent LLM system that converts educational problems into high-quality instructional videos via structured, executable scripts instead of direct pixel generation.", "motivation": "End-to-end video generators are visually strong but fail on tasks needing strict logic, precise knowledge representation, and pedagogically coherent explanations, making them unsuitable for many instructional and educational use cases.", "method": "Formulate educational video generation as a multi-objective task (correct reasoning, coherent narration, faithful visuals, audio-visual alignment) and decompose it into coordinated LLM-based agents: an Orchestrating Agent that manages workflow and quality gates; a Solution Agent for rigorous step-by-step solutions; an Illustration Agent producing executable visualization code; and a Narration Agent generating learner-oriented scripts. All agent outputs pass through semantic critique, rule-based checks, and compilation tests. Instead of synthesizing pixels, the system builds a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-based rules.", "result": "In large-scale deployment, LAVES can automatically generate more than one million instructional videos per day with over 95% cost reduction relative to industry-standard production pipelines, while keeping a high acceptance rate of generated content.", "conclusion": "A hierarchical LLM-based multi-agent pipeline that compiles structured video scripts, rather than directly generating video, can massively scale and cheapen instructional video production while preserving logical rigor, pedagogical quality, and controllability."}}
{"id": "2602.11543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11543", "abs": "https://arxiv.org/abs/2602.11543", "authors": ["Jinrui Zhang", "Chaodong Xiao", "Aoqi Wu", "Xindong Zhang", "Lei Zhang"], "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm", "comment": null, "summary": "Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.", "AI": {"tldr": "They propose SPES, a decentralized, memory-efficient framework to pretrain mixture-of-experts LLMs by training only subsets of experts per node and periodically synchronizing them, enabling large-scale training on scattered, modest GPUs.", "motivation": "Centralized pretraining of large LLMs demands access to large homogeneous GPU clusters with high-memory GPUs, excluding many from participation. Existing decentralized/federated methods still require every node to hold the full model, so they remain bottlenecked by GPU memory and communication costs. The authors want a way to pretrain very large LLMs across many small, widely distributed GPUs over the internet without each node needing the whole model.", "method": "They introduce SPES (SParse Expert Synchronization) for mixture-of-experts LLMs in a decentralized setting. Each node stores and trains only a subset of experts rather than the whole MoE model. The nodes locally update their experts and periodically synchronize them with others, avoiding full-parameter communication while still sharing knowledge. An expert-merging warm-up phase lets experts exchange and merge knowledge early in training to quickly build general capabilities before specializing. The system is evaluated on internet-connected standalone 48GB GPUs with modest bandwidth.", "result": "Using SPES, they successfully train (1) a 2B-parameter MoE LLM on 16 independent 48GB GPUs over the internet that reaches performance competitive with centrally trained LLMs using similar compute; (2) a 7B MoE model from scratch; and (3) a 9B MoE model upcycled from a dense checkpoint. All these models match prior centralized baselines, demonstrating both effectiveness and scalability of the approach.", "conclusion": "Decentralized pretraining of competitive large MoE LLMs is feasible without large centralized clusters if one uses memory-efficient sparse expert training and synchronization. By training and syncing only subsets of experts per node plus an early expert-merging warm-up, SPES overcomes GPU memory limits and communication overhead, enabling scalable, internet-scale collaborative LLM pretraining on heterogeneous, modest hardware."}}
{"id": "2602.11792", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11792", "abs": "https://arxiv.org/abs/2602.11792", "authors": ["Hongbo Zhang", "Yue Yang", "Jianhao Yan", "Guangsheng Bao", "Yue Zhang", "Yue Zhang"], "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning", "comment": "Preprint", "summary": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.", "AI": {"tldr": "They propose a simple black-box method to detect whether a prompt was seen during RL-with-verifiable-rewards training, based on reduced diversity in the model's completions.", "motivation": "RL with verifiable rewards is used to train reasoning models, but undisclosed training data makes it hard to know if benchmarks were contaminated by RL training, and existing likelihood-based or membership inference methods don\u2019t work well for RLVR.", "method": "Observe that prompts used during RLVR lead to more rigid, similar generations than unseen prompts. Define Min-kNN Distance: for each prompt, sample multiple completions, compute pairwise edit distances, take the k smallest nearest-neighbor distances for each completion, and average them as a collapse score. Use this score as a black-box contamination / membership detector, without needing reference model or token probabilities.", "result": "Across several RLVR-trained reasoning models, the Min-kNN Distance score effectively separates RL-seen prompts from unseen ones, and yields higher detection performance than existing membership inference and RL contamination baselines.", "conclusion": "Behavioral collapse in generations is a detectable footprint of RL-with-verifiable-rewards training, and Min-kNN Distance offers a simple, black-box, probability-free method to detect RL training contamination of prompts or benchmarks."}}
{"id": "2602.11551", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11551", "abs": "https://arxiv.org/abs/2602.11551", "authors": ["Wenlin Zhong", "Jinluan Yang", "Yiquan Wu", "Yi Liu", "Jianhang Yao", "Kun Kuang"], "title": "SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent", "comment": null, "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into \"Tunnel Vision,\" where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.", "AI": {"tldr": "The paper introduces SIGHT, a framework to improve search-based reasoning in LLMs by mitigating redundancy and noise in multi-turn reinforcement learning search, leading to better QA with fewer steps.", "motivation": "In multi-turn search for complex question answering, RL-empowered LLM agents suffer from redundant, noisy retrievals that cause \"Tunnel Vision\"\u2014early misinterpretations compound into irreversible errors. There is a need for a method that can distill high-quality evidence, avoid over-commitment to noisy early results, and explore the search space more robustly without relying on external verifiers.", "method": "The authors propose SIGHT, which has two key components: (1) Self-Evidence Support (SES) to distill search results into concise, high-fidelity evidence chunks; and (2) Information-Gain Driven Diverse Branching, where an information gain score identifies decision points that most reduce uncertainty and triggers dynamic prompting interventions (such as de-duplication, reflection, or creating adaptive branches). These branches are supported by SES. The whole process is trained with Group Relative Policy Optimization, combining SES feedback and correctness rewards to learn robust exploration strategies end-to-end.", "result": "On both single-hop and multi-hop QA benchmarks, SIGHT outperforms prior search-based RL and LLM reasoning approaches, especially on complex reasoning tasks, and does so using fewer search steps, indicating higher search efficiency and better utilization of retrieved information.", "conclusion": "Systematically managing evidence quality and branching decisions via SES and information-gain-based interventions enables LLM agents to overcome tunnel vision and redundancy in multi-turn search. SIGHT yields more accurate and efficient search-based reasoning and can be used as a general framework for RL-trained LLM agents on complex QA tasks."}}
{"id": "2602.11799", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11799", "abs": "https://arxiv.org/abs/2602.11799", "authors": ["Pingjun Pan", "Tingting Zhou", "Peiyao Lu", "Tingting Fei", "Hongxiang Chen", "Chuanjiang Luo"], "title": "Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation", "comment": null, "summary": "Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.", "AI": {"tldr": "Proposes Hi-SAM, a hierarchical structure-aware multi-modal recommendation framework that improves tokenization and sequence modeling of semantic IDs for better recommendations.", "motivation": "Existing multi-modal recommenders use semantic IDs but suffer from suboptimal tokenization that entangles shared and modality-specific semantics, and from Transformers that ignore the hierarchical structure of users, items, and tokens, leading to long, noisy sequences and misfocused attention.", "method": "Introduce Hi-SAM with two main components: (1) Disentangled Semantic Tokenizer (DST), which performs geometry-aware cross-modal alignment and coarse-to-fine quantization using shared and modality-specific codebooks, regularized via mutual information minimization to separate shared semantics from modality-specific residuals; (2) Hierarchical Memory-Anchor Transformer (HMAT), which employs Hierarchical RoPE to decouple inter-item and intra-item positions and uses anchor tokens that compress each item into a memory summary, exposing full details only for the current item while accessing historical items via their anchors.", "result": "On real-world datasets, Hi-SAM consistently outperforms state-of-the-art multi-modal recommendation baselines, with particularly strong gains in cold-start settings. In an online deployment on a large-scale social platform with millions of users, the method yields a 6.55% improvement on the primary online metric.", "conclusion": "Disentangling shared and modality-specific semantics at the tokenization stage and modeling interaction sequences with a hierarchy-aware Transformer both significantly enhance multi-modal recommendation performance and scalability, making Hi-SAM effective in practice and especially beneficial for cold-start users or items."}}
{"id": "2602.11570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11570", "abs": "https://arxiv.org/abs/2602.11570", "authors": ["Xiangfeng Wang", "Hangyu Guo", "Yanlin Lai", "Mitt Huang", "Liang Zhao", "Chengyuan Yao", "Yinmin Zhang", "Qi Han", "Xiaoxiao Ren", "Chun Yuan", "Tong Xu", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang"], "title": "PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering", "comment": null, "summary": "While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 > 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.", "AI": {"tldr": "Introduces PRIME, a benchmark and training paradigm that evaluates and trains verifiers on both process and outcome alignment in STEM reasoning, leading to better RL with verifiable rewards.", "motivation": "Existing model-based verifiers for RL with Verifiable Rewards (RLVR) mainly check final-answer correctness and largely ignore whether the intermediate reasoning steps are valid. This can reward models for lucky guesses or answers produced via flawed derivations, which undermines reliability and safety. There is a need for a benchmark and training approach that explicitly evaluates and uses process-aware verification.", "method": "1) Curate PRIME, a benchmark of 2,530 challenging college-level STEM problems (math and engineering), using a consistency-based filtering pipeline to ensure high-difficulty and reliable labels for both process and outcome. 2) Use PRIME to systematically evaluate current verifiers on their ability to detect derivation errors (Process-Outcome Alignment). 3) Design a process-aware RLVR training paradigm that leverages verifiers selected according to their PRIME performance, so that RL rewards depend on both correct answers and correct reasoning steps.", "result": "Empirical evaluation shows that existing verifiers often miss derivation flaws, confirming the limitation of outcome-only verification. When training Qwen3-14B-Base with the proposed process-aware RLVR using PRIME-selected verifiers, the model achieves absolute performance improvements of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME benchmarks compared to an outcome-only RLVR baseline. Additionally, they observe a strong linear correlation (R^2 > 0.92) between a verifier\u2019s accuracy on PRIME and its effectiveness for RLVR training.", "conclusion": "Process-aware verification is crucial for trustworthy RL with verifiable rewards. PRIME serves both as a benchmark to measure verifiers\u2019 ability to align process and outcome in STEM reasoning, and as a practical tool for selecting verifiers that yield stronger RLVR training. The strong correlation between PRIME scores and downstream RLVR gains suggests PRIME is a reliable predictor for verifier quality, and that incorporating process-level checks substantially improves performance on challenging math benchmarks."}}
{"id": "2602.11807", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11807", "abs": "https://arxiv.org/abs/2602.11807", "authors": ["Lianjun Wu", "Shengchen Zhu", "Yuxuan Liu", "Liuyu Kai", "Xiaoduan Feng", "Duomin Wang", "Wenshuo Liu", "Jingxuan Zhang", "Kelvin Li", "Bin Wang"], "title": "PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts", "comment": null, "summary": "Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25\u00b0) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.", "AI": {"tldr": "The paper introduces PuYun-LDM, a latent diffusion\u2013based model for high-resolution ensemble weather forecasting that improves how well diffusion models can represent meteorological data and outperforms or matches traditional ensemble systems while being much faster.", "motivation": "Latent diffusion models struggle with \"limited diffusability\" on high-resolution meteorological data, which are very different from natural images: there are no strong, task-agnostic vision foundation models to regularize the latent space, and meteorological variables have heterogeneous spectral characteristics that existing frequency-based regularization methods treat incorrectly as homogeneous. This degrades the quality of probabilistic high-resolution weather forecasts.", "method": "The authors design two key components around a latent diffusion model: (1) a 3D Masked AutoEncoder (3D-MAE) trained on weather data in space-time, which learns an embedding of the temporal evolution of the atmospheric state; this embedding is used as an additional physical/temporal conditioning signal for the diffusion model, improving latent organization; and (2) a Variable-Aware Masked Frequency Modeling (VA-MFM) scheme that applies frequency-domain masking and regularization separately to each meteorological variable, with thresholds adapted to each variable\u2019s spectral energy distribution instead of assuming spectral homogeneity. Combining these with a latent diffusion backbone yields PuYun-LDM.", "result": "PuYun-LDM exhibits improved latent diffusability, translating to better probabilistic forecast skill. It surpasses an operational-style ensemble system (ENS) for short lead times and remains competitive at longer forecast horizons. Computationally, it can generate a 15-day global forecast at 6-hour intervals in about 5 minutes on a single NVIDIA H200 GPU, with ensemble members producible in parallel for efficient uncertainty quantification.", "conclusion": "Encoding space-time weather evolution with a 3D-MAE and handling per-variable spectral heterogeneity with VA-MFM significantly improves the suitability of latent diffusion models for high-resolution ensemble weather forecasting. The resulting PuYun-LDM model offers a strong trade-off between accuracy and computational cost, outperforming or matching traditional ensemble methods while being much more efficient, and demonstrates a practical path for diffusion-based numerical weather prediction at scale."}}
{"id": "2602.11607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11607", "abs": "https://arxiv.org/abs/2602.11607", "authors": ["Yijie Zhong", "Mengying Guo", "Zewei Wang", "Zhongyang Li", "Dandan Tu", "Haofen Wang"], "title": "Scene-Aware Memory Discrimination: Deciding Which Personal Knowledge Stays", "comment": "Accepted by Knowledge-Based Systems. Lincense: CC BY-NC-ND", "summary": "Intelligent devices have become deeply integrated into everyday life, generating vast amounts of user interactions that form valuable personal knowledge. Efficient organization of this knowledge in user memory is essential for enabling personalized applications. However, current research on memory writing, management, and reading using large language models (LLMs) faces challenges in filtering irrelevant information and in dealing with rising computational costs. Inspired by the concept of selective attention in the human brain, we introduce a memory discrimination task. To address large-scale interactions and diverse memory standards in this task, we propose a Scene-Aware Memory Discrimination method (SAMD), which comprises two key components: the Gating Unit Module (GUM) and the Cluster Prompting Module (CPM). GUM enhances processing efficiency by filtering out non-memorable interactions and focusing on the salient content most relevant to application demands. CPM establishes adaptive memory standards, guiding LLMs to discern what information should be remembered or discarded. It also analyzes the relationship between user intents and memory contexts to build effective clustering prompts. Comprehensive direct and indirect evaluations demonstrate the effectiveness and generalization of our approach. We independently assess the performance of memory discrimination, showing that SAMD successfully recalls the majority of memorable data and remains robust in dynamic scenarios. Furthermore, when integrated into personalized applications, SAMD significantly enhances both the efficiency and quality of memory construction, leading to better organization of personal knowledge.", "AI": {"tldr": "This paper proposes Scene-Aware Memory Discrimination (SAMD) to more efficiently select and manage user interaction data as personal memory for LLM-based personalized applications, improving both efficiency and quality of memory construction.", "motivation": "With intelligent devices generating massive user interaction logs, there is a need to turn this data into well-organized personal knowledge for personalized applications. Existing LLM-based memory systems struggle with filtering irrelevant information and suffer from high computational costs. Inspired by human selective attention, the authors aim to design a mechanism that can automatically decide what should be stored as memory and do so efficiently at scale.", "method": "The authors introduce a new memory discrimination task and propose SAMD (Scene-Aware Memory Discrimination), which has two core modules: (1) Gating Unit Module (GUM) filters non-memorable or irrelevant interactions, focusing processing on salient, task-relevant content; (2) Cluster Prompting Module (CPM) sets adaptive memory standards and builds clustering prompts by modeling the relationship between user intents and memory contexts, helping LLMs determine what to remember or discard. They evaluate SAMD both as an independent memory discriminator and when integrated into personalized applications.", "result": "Experiments show that SAMD effectively recalls most of the truly memorable data while maintaining robustness in dynamic, changing scenarios. In downstream personalized applications, integrating SAMD significantly improves both the efficiency (lower computational costs, less redundant data) and the quality (more relevant, better-structured memories) of personal knowledge construction compared to baselines.", "conclusion": "SAMD provides an effective and generalizable approach to selective memory construction for LLM-based personalized systems. By combining a gating mechanism for interaction filtering with scene-aware, cluster-based prompting, it can robustly discriminate which user interactions should be stored as memory, leading to more efficient computation and higher-quality personalized knowledge organization."}}
{"id": "2602.11812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11812", "abs": "https://arxiv.org/abs/2602.11812", "authors": ["Huanyi Xie", "Yubin Chen", "Liangyu Wang", "Lijie Hu", "Di Wang"], "title": "Predicting LLM Output Length via Entropy-Guided Representations", "comment": null, "summary": "The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic \"one-to-many\" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.", "AI": {"tldr": "The paper proposes a low-overhead framework that predicts sequence lengths directly from an LLM\u2019s hidden states to reduce padding waste in batched inference, and introduces a new benchmark, ForeLen, to evaluate such methods.", "motivation": "In LLM serving and RL sampling, sequence lengths follow a long-tailed distribution, so padding to the longest sequence in a batch leads to large computational waste. Existing static length prediction methods require separate auxiliary models that add overhead, generalize poorly, and cannot deal with stochastic one-to-many sampling where the same prompt can yield very different output lengths. There is a need for a more accurate and efficient length prediction approach that works in dynamic, stochastic decoding settings.", "method": "The authors reuse internal hidden states of the main LLM to predict output lengths instead of relying on external predictors. They propose (1) Entropy-Guided Token Pooling (EGTP), which leverages on-the-fly activations and token entropy to form a compact representation enabling accurate, static length prediction at negligible extra cost; and (2) Progressive Length Prediction (PLP), which updates predictions of the remaining sequence length at each decoding step, allowing the system to adapt to stochastic \u201cone-to-many\u201d sampling behavior. They further construct ForeLen, a benchmark comprising long-sequence, Chain-of-Thought, and RL data to systematically evaluate length prediction techniques.", "result": "On the ForeLen benchmark, EGTP delivers state-of-the-art static length prediction performance, reducing mean absolute error by 29.16% compared with the strongest existing baseline. When combined with a length-aware scheduling mechanism in LLM serving, their framework leads to substantial end-to-end throughput improvements, indicating that better length prediction directly translates into more efficient inference.", "conclusion": "Reusing an LLM\u2019s hidden states for both static and progressive length prediction is an effective and low-cost way to mitigate padding-induced inefficiency in batched inference, especially under long-tailed and stochastic sequence length distributions. The proposed EGTP and PLP components, together with the ForeLen benchmark, establish a new technical and evaluation baseline for research on efficient LLM length prediction and inference scheduling."}}
{"id": "2602.11639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11639", "abs": "https://arxiv.org/abs/2602.11639", "authors": ["Ruixiang Feng", "Yuntao Wen", "Silin Zhou", "Ke Shi", "Yifan Wang", "Ran Le", "Zhenwei An", "Zongchao Chen", "Chen Yang", "Guangyue Peng", "Yiming Jia", "Dongsheng Wang", "Tao Zhang", "Lisi Chen", "Yang Song", "Shen Gao", "Shuo Shang"], "title": "PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning", "comment": null, "summary": "Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \\textbf{\\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \\model achieves a substantial reduction in token usage (up to \\textbf{55.7\\%}) while simultaneously improving accuracy (up to \\textbf{4.1\\%}) on math benchmarks, with generalization ability to code, science, and general domains.", "AI": {"tldr": "The paper introduces a dual-level framework to reduce overthinking in Language Reasoning Models, cutting tokens by up to 55.7% while improving accuracy on reasoning tasks.", "motivation": "Language Reasoning Models gain accuracy by using long test-time reasoning traces, but this leads to overthinking, high latency, and memory costs. Existing length penalties are coarse: they compress important early reasoning steps and treat all queries equally, ignoring problem difficulty.", "method": "The authors propose a framework (named in the paper but redacted here as \\model) with hierarchical supervision. At the sequence level, prefix-protected optimization with decaying mixed rollouts keeps early reasoning steps intact while encouraging shorter overall traces. At the group level, a difficulty-aware penalty scales the strength of length constraints according to the complexity of each query, allowing more exploration on hard problems and stronger compression on easy ones.", "result": "On DeepSeek-R1-Distill-Qwen models (1.5B and 7B), the approach reduces token usage by up to 55.7% and simultaneously improves accuracy by up to 4.1% on math benchmarks, and also generalizes well to code, science, and general reasoning tasks.", "conclusion": "Hierarchically controlled reasoning length\u2014protecting crucial prefixes and adapting compression to problem difficulty\u2014can significantly cut computation while improving or preserving accuracy, addressing overthinking in Language Reasoning Models and transferring across domains."}}
{"id": "2602.11824", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11824", "abs": "https://arxiv.org/abs/2602.11824", "authors": ["Jialin Wu", "Wei Shi", "Han Shen", "Peigui Qi", "Kunsheng Tang", "Zhicong Huang", "Binghao Wang", "Zhou Yang"], "title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models", "comment": null, "summary": "Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.", "AI": {"tldr": "The paper introduces REVIS, a training-free framework to reduce object hallucination in Large Vision-Language Models by re-activating suppressed visual information.", "motivation": "LVLMs often hallucinate objects because visual and textual features get entangled in deeper layers, causing visual information to be suppressed. There is a need for an efficient way to restore and use clean visual signals without retraining large models.", "method": "REVIS leverages latent space geometry to isolate pure visual information via orthogonal projection, and then performs a calibrated, sparse intervention at specific network depths where visual suppression occurs. The intervention is training-free and computationally light, focusing only on the layers identified as problematic.", "result": "On standard benchmarks, REVIS reduces object hallucination rates by about 19% relative to state-of-the-art baselines, while maintaining overall reasoning performance of the LVLMs.", "conclusion": "Explicitly re-activating suppressed visual information in LVLMs through targeted, training-free interventions in latent space can substantially mitigate object hallucination without harming general reasoning, offering a practical improvement path for deployed LVLMs."}}
{"id": "2602.11650", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11650", "abs": "https://arxiv.org/abs/2602.11650", "authors": ["Momoka Furuhashi", "Kouta Nakayama", "Noboru Kawai", "Takashi Kodama", "Saku Sugawara", "Kyosuke Takami"], "title": "Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles", "comment": "Under Review", "summary": "Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner acceptance, particularly across learners with different personality traits. In this study, we define six feedback elements and generate feedback for multiple-choice biology questions using GPT-5. We conduct a learning experiment with 321 first-year high school students and evaluate feedback effectiveness using two learning outcomes measures and subjective evaluations across six criteria. We further analyze differences in how feedback acceptance varies across learners based on Big Five personality traits. Our results show that effective feedback elements share common patterns supporting learning outcomes, while learners' subjective preferences differ across personality-based clusters. These findings highlight the importance of selecting and adapting feedback elements according to learners' personality traits when we design LLM-generated feedback, and provide practical implications for personalized feedback design in education.", "AI": {"tldr": "Study tests how specific elements of GPT-5\u2013generated feedback on biology MCQs affect learning and student acceptance, and how these effects differ by Big Five personality traits.", "motivation": "While LLMs can generate educational feedback, we lack detailed understanding of which feedback components (e.g., tone, coverage) actually improve learning and are accepted by students, especially for different personality types.", "method": "Define six distinct feedback elements, use GPT-5 to generate feedback on multiple-choice biology questions, run a learning experiment with 321 first-year high school students, and assess effects via two objective learning outcome measures plus six subjective evaluation criteria. Analyze how feedback acceptance differs across personality clusters based on the Big Five traits.", "result": "Certain feedback element patterns consistently support better learning outcomes, but students\u2019 subjective preferences for feedback vary significantly across personality-based learner clusters.", "conclusion": "LLM-generated feedback should be tailored: effective feedback elements for learning are somewhat consistent, but their perceived usefulness and acceptance depend on learner personality traits. Designers should adapt feedback elements to personality profiles to enable more personalized educational feedback."}}
{"id": "2602.11852", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11852", "abs": "https://arxiv.org/abs/2602.11852", "authors": ["Yordan Yordanov", "Matteo Forasassi", "Bayar Menzat", "Ruizhi Wang", "Chang Qi", "Markus Kaltenberger", "Amine M'Charrak", "Tommaso Salvatori", "Thomas Lukasiewicz"], "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design", "comment": "Preprint under review. Equal contribution: Yordan Yordanov and Matteo Forasassi. 39 pages, 25 figures, 22 tables", "summary": "While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. \"woman\") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.", "AI": {"tldr": "They propose a new language model architecture, Prototype Transformer (ProtoT), that is more interpretable by design while staying competitive with standard transformers.", "motivation": "Current large language models outperform most humans in some tasks but their reasoning is opaque, which undermines trust and carries risks like deception and hallucinations. Explicit chain-of-thought outputs in autoregressive models do not guarantee that the visible reasoning matches the internal process. There is a need for architectures that are both performant and intrinsically interpretable, with mechanisms that can be inspected and edited.", "method": "They design an autoregressive LM called Prototype Transformer (ProtoT) that replaces standard self-attention with a mechanism based on \"prototypes\"\u2014learned parameter vectors. The architecture enables two-way communication: the input sequence attends to and updates prototypes, and prototypes, in turn, attend back to the sequence. This interaction makes prototypes act as explicit, nameable concept slots. The design enforces communication channels that aggregate contextual information across different time scales, with linear complexity in sequence length instead of quadratic self-attention cost. They train ProtoT on language modeling, text generation, and GLUE tasks, then analyze interpretability by examining what concepts individual prototypes capture and how they influence model outputs. They also evaluate robustness to input perturbations compared to standard transformer baselines.", "result": "ProtoT\u2019s prototypes naturally specialize to interpretable, nameable concepts (e.g., \"woman\") without explicit supervision. The model attains good performance on language modeling, text generation, and GLUE benchmarks, scaling well with both model size and data size and approaching state-of-the-art transformer performance. ProtoT achieves robustness to input perturbations comparable to or better than baseline architectures. It additionally provides interpretable communication pathways that clarify how robustness and sensitivity to inputs arise.", "conclusion": "Prototype Transformer demonstrates that it is possible to build autoregressive language models that are competitive with state-of-the-art transformers while being more interpretable by design. Its prototype-based communication mechanism yields concept-like units and structured information pathways, enabling insight into and direct editing of the model\u2019s behavior, as well as a more transparent view of robustness. This architecture points toward a path for trustworthy, high-performance language models whose internal reasoning can be examined and controlled."}}
{"id": "2602.11684", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.11684", "abs": "https://arxiv.org/abs/2602.11684", "authors": ["Sahand Sabour", "TszYam NG", "Minlie Huang"], "title": "PatientHub: A Unified Framework for Patient Simulation", "comment": "Work in progress", "summary": "As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on incompatible, non-standardized data formats, prompts, and evaluation metrics, hindering reproducibility and fair comparison. In this paper, we introduce PatientHub, a unified and modular framework that standardizes the definition, composition, and deployment of simulated patients. To demonstrate PatientHub's utility, we implement several representative patient simulation methods as case studies, showcasing how our framework supports standardized cross-method evaluation and the seamless integration of custom evaluation metrics. We further demonstrate PatientHub's extensibility by prototyping two new simulator variants, highlighting how PatientHub accelerates method development by eliminating infrastructure overhead. By consolidating existing work into a single reproducible pipeline, PatientHub lowers the barrier to developing new simulation methods and facilitates cross-method and cross-model benchmarking. Our framework provides a practical foundation for future datasets, methods, and benchmarks in patient-centered dialogue, and the code is publicly available via https://github.com/Sahandfer/PatientHub.", "AI": {"tldr": "PatientHub is a unified, modular framework for defining, running, and evaluating LLM-based simulated patients for counseling and therapeutic assessment.", "motivation": "Existing LLM-based patient simulators are built with ad-hoc data formats, prompts, and metrics, which makes methods hard to reproduce, compare fairly, and extend for new research or practical training tools.", "method": "The authors design PatientHub, a framework that standardizes how simulated patients are specified, composed, and deployed. They implement multiple existing simulation approaches within this framework as case studies, and enable shared evaluation pipelines with pluggable, customizable metrics. They also prototype two novel simulation variants inside the same infrastructure to illustrate extensibility and reduced engineering overhead.", "result": "Using PatientHub, different patient simulation methods can be instantiated and evaluated under a common setup, enabling cross-method and cross-model benchmarking. The case studies and new variants show that researchers can rapidly build and compare simulators without re-implementing infrastructure, and can incorporate custom evaluation metrics in a consistent way.", "conclusion": "PatientHub consolidates fragmented work on LLM-based patient simulation into a single standardized, reproducible pipeline. This lowers the barrier to developing and benchmarking new methods, supports fair comparisons and custom evaluations, and offers a practical foundation for future datasets, simulation methods, and benchmarks in patient-centered dialogue. The framework is released as open-source software."}}
{"id": "2602.11860", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11860", "abs": "https://arxiv.org/abs/2602.11860", "authors": ["Lu Tao", "Jinxuan Luo", "Yousuke Watanabe", "Zhengshu Zhou", "Yuhuan Lu", "Shen Ying", "Pan Zhang", "Fei Zhao", "Hiroaki Takada"], "title": "Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models", "comment": "Submitted to IEEE TITS. Under review", "summary": "Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.", "AI": {"tldr": "The paper proposes Talk2DM, a natural-language interface for dynamic maps in cooperative autonomous driving, enabled via a simulation framework and QA dataset.", "motivation": "Existing dynamic map (DM) systems for vehicle-road-cloud cooperative autonomous driving lack a natural-language-supported human interface, limiting intuitive human-DM interaction for querying and reasoning about traffic scenes.", "method": "The authors build VRCsim, a cooperative perception simulation framework to generate streaming VRC-CP data, and construct VRC-QA, a QA dataset on spatial querying and reasoning in mixed-traffic scenes. On top of these, they design Talk2DM, a plug-and-play NLS querying and reasoning module based on a chain-of-prompt mechanism that fuses human-defined rules with LLM commonsense knowledge. They evaluate Talk2DM with multiple LLM backends on VRC-QA.", "result": "Talk2DM can be seamlessly combined with different LLMs while maintaining high natural-language query accuracy on VRC-QA. Larger LLMs improve accuracy but reduce efficiency. Configurations using Qwen3:8B, Gemma3:27B, and GPT-oss obtain over 93% query accuracy with 2\u20135 second average response times.", "conclusion": "Integrating the Talk2DM module into VRC dynamic map systems enables accurate, efficient natural-language querying and commonsense reasoning over cooperative perception data, showing strong generalization across LLMs and strong potential for practical deployment in real-world autonomous driving infrastructures."}}
{"id": "2602.11699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11699", "abs": "https://arxiv.org/abs/2602.11699", "authors": ["Katrin Olsen", "Sebastian Pad\u00f3"], "title": "Finding Sense in Nonsense with Generated Contexts: Perspectives from Humans and Language Models", "comment": null, "summary": "Nonsensical and anomalous sentences have been instrumental in the development of computational models of semantic interpretation. A core challenge is to distinguish between what is merely anomalous (but can be interpreted given a supporting context) and what is truly nonsensical. However, it is unclear (a) how nonsensical, rather than merely anomalous, existing datasets are; and (b) how well LLMs can make this distinction. In this paper, we answer both questions by collecting sensicality judgments from human raters and LLMs on sentences from five semantically deviant datasets: both context-free and when providing a context. We find that raters consider most sentences at most anomalous, and only a few as properly nonsensical. We also show that LLMs are substantially skilled in generating plausible contexts for anomalous cases.", "AI": {"tldr": "The paper investigates how to distinguish truly nonsensical sentences from merely anomalous ones, using human and LLM judgments.", "motivation": "Although nonsensical and anomalous sentences are often used to build and evaluate computational semantic models, it is not clear how nonsensical these datasets really are, nor how well large language models can tell apart true nonsense from interpretable anomalies.", "method": "The authors collect sensicality judgments from both human annotators and large language models on sentences drawn from five existing semantically deviant datasets, evaluating them both without context and with supporting context, and also assess LLMs\u2019 ability to generate plausible contexts for anomalous sentences.", "result": "Human raters labeled the majority of sentences as at most anomalous\u2014interpretable given an appropriate context\u2014and only a small subset as genuinely nonsensical; LLMs were found to be quite capable of producing reasonable contextualizations that render many anomalous sentences sensible.", "conclusion": "Existing \u201cnonsense\u201d datasets largely contain sentences that humans see as anomalous rather than truly nonsensical, and large language models are proficient at contextualizing these anomalies, which has implications for how semantic evaluation datasets are constructed and how LLM semantic competence is assessed."}}
{"id": "2602.11865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11865", "abs": "https://arxiv.org/abs/2602.11865", "authors": ["Nenad Toma\u0161ev", "Matija Franklin", "Simon Osindero"], "title": "Intelligent AI Delegation", "comment": null, "summary": "AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.", "AI": {"tldr": "The paper proposes an adaptive framework for intelligent delegation between AI agents and humans, going beyond simple task decomposition heuristics.", "motivation": "As AI agents are tasked with increasingly complex goals, current heuristic-based methods for task decomposition and delegation are insufficient because they lack adaptability to changing environments and robustness to failures. There is a need for a principled way to manage how tasks, authority, and responsibility are delegated among multiple AI agents and humans.", "method": "The authors introduce a conceptual, adaptive framework for intelligent delegation, modeled as a sequence of task allocation decisions that explicitly includes transfer of authority, responsibility, accountability, role and boundary specification, clarity of intent, and mechanisms for building trust between parties in delegation networks.", "result": "The paper presents a structured framework that describes how delegation processes should be organized and adapted over time among AI and human agents, particularly in complex, multi-party delegation networks.", "conclusion": "The framework can guide the design of protocols and infrastructures for delegation in the emerging agentic web, supporting safer, more reliable, and more transparent collaboration between human and AI agents."}}
{"id": "2602.11731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11731", "abs": "https://arxiv.org/abs/2602.11731", "authors": ["Jingxuan Wei", "Honghao He", "Caijun Jia", "Siyuan Li", "Zheng Sun", "Yuhang Xu", "Yuanyuan Lin", "Linzhuang Sun", "Yuchen Wu", "Bihui Yu", "Xiangxiang Zhang", "Cheng Tan"], "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction", "comment": null, "summary": "Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.", "AI": {"tldr": "The paper introduces Thinking with Drafting (TwD), a method that converts visual reasoning into generating executable code in a minimalist DSL, treating visual generation as logical verification rather than creative output, and validates it with a new visual algebra benchmark, VisAlg.", "motivation": "Multimodal LLMs can see and draw well but struggle with mathematically precise, complex visual reasoning. Perception models faithfully read symbols but miss their underlying logical relationships, while pixel-based generators create images that may look right but lack exact mathematical structure. The authors aim to close this precision gap by reframing visual reasoning so that models must explicitly recover and manipulate latent logical structures from images.", "method": "They propose viewing visual reasoning as \"optical decompression,\" where the model reconstructs hidden logical topology from visual inputs. Based on the principle \"Parsing is Reasoning,\" they introduce Thinking with Drafting (TwD), which forces the model to first draft an internal mental model as code in a minimalist Domain-Specific Language (DSL). This DSL serves as an intermediate, executable representation. The model then executes the drafted code to produce deterministic, verifiable visual proofs instead of directly generating final answers from pixels.", "result": "They build VisAlg, a visual algebra benchmark, to test visual reasoning and algebraic understanding. Experiments show that models using TwD perform better on visual reasoning tasks than baselines that answer directly without drafting code, indicating that TwD is a stronger cognitive scaffold for complex visual reasoning.", "conclusion": "The work presents a closed-loop pipeline in which visual generation is repurposed as a logical verifier: the model drafts code, executes it, and visually confirms the result. This approach offers a general, more reliable path for visual reasoning in multimodal LLMs by grounding perception and generation in executable logical structure rather than purely pixel-level similarity."}}
{"id": "2602.11881", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11881", "abs": "https://arxiv.org/abs/2602.11881", "authors": ["Yifan Luo", "Yang Zhan", "Jiedong Jiang", "Tianyang Liu", "Mingrui Wu", "Zhennan Zhou", "Bin Dong"], "title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders", "comment": null, "summary": "Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"feature splitting\" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.", "AI": {"tldr": "They propose a hierarchical sparse autoencoder (HSAE) that learns tree-structured, monosemantic features from LLM activations, capturing multi-scale conceptual structure while keeping SAE reconstruction quality and interpretability.", "motivation": "Standard sparse autoencoders applied to LLM activations discover monosemantic features, but these features are usually studied independently. Empirically, LLMs encode language with rich, hierarchical structure, and phenomena like feature splitting indicate parent-child relationships between concepts. There is a need for a method that explicitly recovers these hierarchies of features instead of flat, unstructured dictionaries.", "method": "They introduce the Hierarchical Sparse Autoencoder (HSAE), which jointly trains multiple SAEs arranged in levels and simultaneously learns parent-child links between their features. Two main mechanisms enforce hierarchy: (1) a structural constraint loss that encourages alignment and consistency between parent and child feature activations; and (2) a random feature perturbation mechanism that injects noise or dropout-like perturbations to encourage robust, stable hierarchical relations rather than brittle co-activations. They evaluate HSAE on activations from various LLMs and layers, across different dictionary sizes, and compare to standard SAE baselines using both qualitative interpretability analyses and quantitative metrics.", "result": "HSAE reliably discovers hierarchies of features where higher-level (parent) features correspond to broader or more abstract concepts, while lower-level (child) features capture more specific variants or sub-aspects. These hierarchies are semantically coherent according to qualitative case studies and quantitative metrics. Despite the added structure, HSAE matches standard SAEs in reconstruction error and retains comparable feature interpretability across a range of dictionary sizes and across different LLM architectures and layers.", "conclusion": "HSAE is an effective and scalable extension of sparse autoencoders that recovers multi-scale, hierarchical conceptual structure from LLM representations. It preserves the core advantages of SAEs\u2014good reconstruction and monosemantic features\u2014while enriching them with explicit parent-child relationships, making it a useful tool for mechanistic interpretability and analysis of how LLMs organize knowledge across levels of abstraction."}}
{"id": "2602.11748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11748", "abs": "https://arxiv.org/abs/2602.11748", "authors": ["Futing Wang", "Jianhao Yan", "Yun Luo", "Ganqu Cui", "Zhi Wang", "Xiaoye Qu", "Yue Zhang", "Yu Cheng", "Tao Lin"], "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning", "comment": null, "summary": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.\n  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.\n  To bridge this gap, we propose Length-Incentivized Exploration(\\method).\n  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.\n  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.\n  As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.", "AI": {"tldr": "The paper introduces a method to improve test-time reasoning in language models by incentivizing longer, more diverse in-context exploration trajectories, overcoming a tendency toward shallow reasoning paths.", "motivation": "Current language models struggle to perform deep in-context exploration at test time: they rarely generate and refine multiple reasoning hypotheses within a single context. State Coverage theory suggests that effective reasoning requires broad exploration of possible intermediate states, which in turn needs longer reasoning trajectories. However, during autoregressive generation, the chance of sampling such long, exploratory sequences decreases exponentially, creating a bottleneck (the Shallow Exploration Trap). The paper aims to remove this bottleneck so that models can better exploit test-time scaling via richer internal reasoning.", "method": "The authors formalize the problem using State Coverage theory and identify the Shallow Exploration Trap, where longer reasoning trajectories are exponentially unlikely under standard decoding. To address this, they propose Length-Incentivized Exploration (LIE), a decoding-time scheme that adds a length-based reward to encourage longer reasoning chains but combines it with a redundancy penalty so the model does not just repeat itself. This two-step mechanism is designed to increase coverage of reasoning states while limiting useless verbosity. The method is tested on different model families (Qwen3, Llama) and a variety of tasks.", "result": "Using Length-Incentivized Exploration, models show substantially increased in-context exploration behavior and improved task performance. Across evaluated settings, the method yields an average 4.4% improvement on in-domain tasks and 2.7% on out-of-domain benchmarks, indicating better generalization and robustness, without changing model parameters or training data.", "conclusion": "Encouraging longer, non-redundant reasoning trajectories at test time can effectively mitigate the Shallow Exploration Trap and improve language model performance. Length-Incentivized Exploration offers a simple, general recipe to enhance in-context exploration using only decoding-time modifications, and demonstrates consistent gains across models and domains, suggesting that better exploitation of test-time scaling can come from smarter control of reasoning length and diversity."}}
{"id": "2602.11908", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11908", "abs": "https://arxiv.org/abs/2602.11908", "authors": ["Shani Goren", "Ido Galil", "Ran El-Yaniv"], "title": "When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation", "comment": null, "summary": "LLMs are widely used, yet they remain prone to factual errors that erode user trust and limit adoption in high-risk settings. One approach to mitigate this risk is to equip models with uncertainty estimation mechanisms that abstain when confidence is low. However, this binary \"all-or-nothing\" approach is excessively restrictive in long-form settings, often discarding valuable information. We introduce Selective Abstraction (SA), a framework that enables LLMs to trade specificity for reliability by selectively reducing the detail of uncertain content. We first formalize SA through the lenses of selective risk and coverage. We then propose Atom-wise Selective Abstraction, a claim-level instantiation that decomposes responses into atomic claims (short, self-contained statements each expressing a single fact) and replaces uncertain atoms with higher confidence, less specific abstractions. To evaluate this framework, we develop a novel end-to-end pipeline for open-ended generation that instantiates risk as factual correctness and measures coverage using an information-theoretic measure of retained information. Across six open-source models on the FactScore and LongFact-Objects benchmarks, atom-wise SA consistently outperforms existing baselines, improving the area under the risk-coverage curve (AURC) by up to 27.73% over claim removal, demonstrating that reducing specificity can boost accuracy and reliability while preserving most of their original meaning.", "AI": {"tldr": "The paper proposes Selective Abstraction (SA), a method for large language models to reduce the specificity of uncertain content instead of removing it, improving factual reliability while preserving information.", "motivation": "LLMs often hallucinate or make factual errors, which reduces trust and makes them less suitable for high-stakes applications. Existing uncertainty-aware methods typically abstain entirely on low-confidence content, which is too coarse in long-form generation and throws away potentially useful partial information. The authors want a finer-grained mechanism that can express uncertainty by lowering specificity rather than going silent.", "method": "They formalize Selective Abstraction using selective risk and coverage, then instantiate it at the claim level with Atom-wise Selective Abstraction. The method decomposes an LLM\u2019s long-form response into atomic factual claims (short, self-contained statements). For claims with low confidence, the system replaces them with higher-confidence, more abstract versions that are less specific but more likely to be correct. They build an end-to-end open-ended generation pipeline that defines risk as factual incorrectness and coverage via an information-theoretic metric of how much information from the original answer is retained after abstraction.", "result": "On six open-source LLMs evaluated on FactScore and LongFact-Objects benchmarks, atom-wise SA surpasses baselines such as outright claim removal. It improves the area under the risk-coverage curve (AURC) by as much as 27.73%, indicating that selectively abstracting uncertain content yields better accuracy\u2013coverage tradeoffs than discarding it.", "conclusion": "Selective Abstraction allows LLMs to gracefully handle uncertainty by trading detail for reliability instead of using binary abstention. This produces outputs that are more factually accurate while preserving most of their informational content, suggesting SA is a promising direction for making LLMs safer and more useful in long-form, high-stakes contexts."}}
{"id": "2602.11761", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11761", "abs": "https://arxiv.org/abs/2602.11761", "authors": ["MiniCPM Team", "Wenhao An", "Yingfa Chen", "Yewei Fang", "Jiayi Li", "Xin Li", "Yaohui Li", "Yishan Li", "Yuxuan Li", "Biyuan Lin", "Chuan Liu", "Hezi Liu", "Siyuan Liu", "Hongya Lyu", "Yinxu Pan", "Shixin Ren", "Xingyu Shen", "Zhou Su", "Haojun Sun", "Yangang Sun", "Zhen Leng Thai", "Xin Tian", "Rui Wang", "Xiaorong Wang", "Yudong Wang", "Bo Wu", "Xiaoyue Xu", "Dong Xu", "Shuaikang Xue", "Jiawei Yang", "Bowen Zhang", "Jinqian Zhang", "Letian Zhang", "Shengnan Zhang", "Xinyu Zhang", "Xinyuan Zhang", "Zhu Zhang", "Hengyu Zhao", "Jiacheng Zhao", "Jie Zhou", "Zihan Zhou", "Shuo Wang", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling", "comment": "MiniCPM-SALA Technical Report", "summary": "The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.", "AI": {"tldr": "MiniCPM-SALA is a 9B-parameter hybrid attention LLM that mixes sparse and linear attention to support ultra-long contexts (up to 1M tokens) with much lower compute and memory cost than full-attention Transformers while preserving general performance.", "motivation": "Standard Transformer full attention scales quadratically in sequence length, making ultra-long context (hundreds of thousands to millions of tokens) prohibitively expensive in compute and memory. Existing sparse or linear attention approaches either sacrifice performance or still remain costly. There is a need for an architecture that can efficiently handle ultra-long contexts on practical hardware while retaining the strong modeling capabilities of full attention and enabling cost-effective conversion of existing pre-trained models.", "method": "The paper proposes MiniCPM-SALA, a 9B-parameter hybrid attention architecture that interleaves two mechanisms: (1) InfLLM-V2 sparse attention for high-fidelity long-context modeling, and (2) Lightning Attention, a linear-time global attention mechanism for efficiency. A layer selection algorithm chooses which layers use sparse vs linear attention in an approximately 1:3 ratio. The model uses a hybrid positional encoding scheme (HyPE) to support long contexts. Additionally, the authors design a continual-training pipeline that starts from a pre-trained Transformer with full attention and incrementally trains it into the hybrid architecture, avoiding training from scratch and reusing most learned weights.", "result": "Experiments show that MiniCPM-SALA retains general capabilities close to full-attention baselines while significantly improving efficiency on long sequences. The hybrid model attains up to 3.5x faster inference than a comparable full-attention model at 256K token sequences on a single NVIDIA A6000D GPU, and it can handle contexts up to 1M tokens, whereas standard 8B full-attention models run out of memory. The continual training framework reduces the additional training cost by about 75% compared to fully training a new hybrid model from scratch.", "conclusion": "Hybridizing sparse and linear attention within a single model, guided by a layer selection strategy and hybrid positional encodings, provides an effective way to scale LLMs to ultra-long contexts without substantial loss in general performance. The proposed continual training framework offers a practical path to upgrade existing Transformer models into efficient long-context hybrids at a fraction of the training cost, making ultra-long-context deployment feasible on commodity GPUs."}}
{"id": "2602.11917", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11917", "abs": "https://arxiv.org/abs/2602.11917", "authors": ["Taian Guo", "Haiyang Shen", "Junyu Luo", "Binqi Chen", "Hongjun Ding", "Jinsheng Huang", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution", "comment": null, "summary": "Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.", "AI": {"tldr": "The paper proposes AlphaPROBE, a new framework for automated alpha factor mining that views factors and their evolutionary relations as a DAG, enabling more efficient and diverse factor discovery with better predictive and trading performance.", "motivation": "Existing automated alpha mining methods either generate factors independently (Decoupled Factor Generation) or iteratively evolve them in local parent-child chains (Iterative Factor Evolution). Both lack a global structural understanding of how factors relate, leading to redundant search, poor exploration\u2013exploitation balance, and limited diversity and efficiency. The authors want a method that uses the global topology of factor evolution to systematically explore and refine alpha factors.", "method": "They formulate alpha mining as navigation on a Directed Acyclic Graph (DAG), where nodes are alpha factors and edges are evolutionary relationships. The framework has two core modules: (1) a Bayesian Factor Retriever that uses a posterior probability model to select promising seed factors, explicitly balancing exploration of new regions and exploitation of known good factors; and (2) a DAG-aware Factor Generator that, instead of only using immediate parents, uses the entire ancestral trace on the DAG to guide factor evolution, generating context-aware, less redundant, and more diverse new factors. The system iteratively updates this graph as new factors are generated and evaluated.", "result": "On three major Chinese stock market datasets and compared against eight competitive baselines, AlphaPROBE shows higher predictive accuracy, more stable returns, and better training efficiency. The empirical results indicate that its DAG-based, globally informed search strategy outperforms traditional factor mining paradigms that lack global topology awareness.", "conclusion": "Leveraging the global evolutionary topology of alpha factors via a DAG representation and combining Bayesian retrieval with DAG-aware evolution is crucial for efficient, robust automated alpha discovery. AlphaPROBE provides a principled, effective framework for this purpose and achieves superior empirical performance; the authors release the implementation as open source for further research and application."}}
{"id": "2602.11795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11795", "abs": "https://arxiv.org/abs/2602.11795", "authors": ["Anne-Marie Lutgen", "Alistair Plum", "Christoph Purschke"], "title": "A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments", "comment": null, "summary": "This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ''noisy'' or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.", "AI": {"tldr": "The paper introduces an embedding-based method to automatically detect and cluster spelling and morphological variants directly from raw text, revealing structured linguistic variation instead of treating it as noise.", "motivation": "Traditional approaches to language variation rely on prior normalisation, manually curated variant lists, or extensive annotation, which is especially impractical for noisy, low-resource, or multilingual data such as user comments in small languages. The authors want a data-driven way to uncover and study lexical and orthographic variability without heavy supervision.", "method": "They train subword embeddings on unnormalised raw text and then group related word forms by combining cosine similarity in the embedding space with character-level n-gram similarity. These similarity measures are used to build transparent clusters (families) of related spellings and morphological variants, which can then be inspected quantitatively and qualitatively.", "result": "On a large corpus of Luxembourgish user comments, the method uncovers extensive lexical and orthographic variation that matches patterns reported in dialect and sociolinguistic work. The automatically induced families show systematic correspondences between forms and reveal regional and stylistic differentiation, demonstrating that the clusters are linguistically meaningful rather than random noise.", "conclusion": "Distributional modelling over raw, noisy text can successfully reveal structured patterns of linguistic variation without needing prior normalisation or intensive manual annotation. The proposed framework is reproducible and suitable for studying language variation in multilingual and small-language contexts, turning what is often treated as noise into analyzable linguistic signal."}}
{"id": "2602.11918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11918", "abs": "https://arxiv.org/abs/2602.11918", "authors": ["Taian Guo", "Haiyang Shen", "Junyu Luo", "Zhongshi Xing", "Hanchun Lian", "Jinsheng Huang", "Binqi Chen", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MEME: Modeling the Evolutionary Modes of Financial Markets", "comment": null, "summary": "LLMs have demonstrated significant potential in quantitative finance by processing vast unstructured data to emulate human-like analytical workflows. However, current LLM-based methods primarily follow either an Asset-Centric paradigm focused on individual stock prediction or a Market-Centric approach for portfolio allocation, often remaining agnostic to the underlying reasoning that drives market movements. In this paper, we propose a Logic-Oriented perspective, modeling the financial market as a dynamic, evolutionary ecosystem of competing investment narratives, termed Modes of Thought. To operationalize this view, we introduce MEME (Modeling the Evolutionary Modes of Financial Markets), designed to reconstruct market dynamics through the lens of evolving logics. MEME employs a multi-agent extraction module to transform noisy data into high-fidelity Investment Arguments and utilizes Gaussian Mixture Modeling to uncover latent consensus within a semantic space. To model semantic drift among different market conditions, we also implement a temporal evaluation and alignment mechanism to track the lifecycle and historical profitability of these modes. By prioritizing enduring market wisdom over transient anomalies, MEME ensures that portfolio construction is guided by robust reasoning. Extensive experiments on three heterogeneous Chinese stock pools from 2023 to 2025 demonstrate that MEME consistently outperforms seven SOTA baselines. Further ablation studies, sensitivity analysis, lifecycle case study and cost analysis validate MEME's capacity to identify and adapt to the evolving consensus of financial markets. Our implementation can be found at https://github.com/gta0804/MEME.", "AI": {"tldr": "The paper introduces MEME, a logic-oriented framework that models financial markets as evolving ecosystems of investment narratives (Modes of Thought) and uses LLM-based multi-agent extraction plus Gaussian Mixture Models and temporal alignment to build robust, reasoning-driven portfolios that outperform existing methods.", "motivation": "Existing LLM-based approaches in quantitative finance are either asset-centric (predicting individual stocks) or market-centric (portfolio allocation), and both generally ignore the underlying reasoning or narratives that drive market movements. Markets can be better understood as evolving collections of competing investment logics, and there is a need for a framework that explicitly captures, tracks, and exploits these logics for more robust investment decisions.", "method": "The authors propose a Logic-Oriented perspective, viewing markets as dynamic ecosystems of competing investment narratives called Modes of Thought. They implement MEME, which (1) uses a multi-agent LLM-based extraction module to convert noisy unstructured data into structured Investment Arguments, (2) embeds these arguments into a semantic space and applies Gaussian Mixture Modeling to discover latent consensus clusters, and (3) introduces a temporal evaluation and alignment mechanism to monitor semantic drift, align modes across different market regimes, and track their life cycle and historical profitability. Portfolio construction is then guided by the most enduring and profitable modes rather than short-lived patterns.", "result": "On three heterogeneous Chinese stock pools from 2023 to 2025, MEME consistently outperforms seven state-of-the-art baselines in portfolio performance metrics. Additional experiments, including ablation studies, sensitivity analysis, lifecycle case studies, and cost analysis, support the effectiveness and robustness of MEME in capturing evolving market consensus and translating it into superior investment strategies.", "conclusion": "Modeling financial markets as evolving logical ecosystems of investment narratives enables more robust and interpretable quantitative strategies than conventional asset- or market-centric LLM approaches. MEME successfully operationalizes this perspective, showing that prioritizing enduring, historically profitable Modes of Thought over transient anomalies leads to superior portfolio performance and better adaptation to changing market conditions. The framework is validated empirically and released as open-source code for further research and practical use."}}
{"id": "2602.11871", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11871", "abs": "https://arxiv.org/abs/2602.11871", "authors": ["Tom Kempton", "Julia Rozanova", "Parameswaran Kamalaruban", "Maeve Madigan", "Karolina Wresilo", "Yoann L. Launay", "David Sutton", "Stuart Burrell"], "title": "DMAP: A Distribution Map for Text", "comment": "ICLR 2026", "summary": "Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.", "AI": {"tldr": "They propose DMAP, a method to convert LLM next-token probability distributions into unit-interval samples encoding both rank and probability, enabling richer statistical analysis than perplexity.", "motivation": "Standard metrics like perplexity ignore how the shape of the next-token distribution and the number of plausible tokens affect the interpretation of probabilities, limiting how well we can analyze or audit LLM behavior and generated text.", "method": "Define DMAP, a mathematically grounded mapping from text and a language model\u2019s conditional next-token distributions to samples in [0,1] that encode both token rank and its probability mass. These unit-interval samples are model-agnostic and cheap to compute, and can be used for various downstream statistical analyses.", "result": "Using DMAP, they conduct three case studies: checking whether generation parameter settings preserve data integrity, exploring how curvature of probability distributions helps detect machine-generated text, and performing forensic analysis to identify statistical fingerprints left when models are post-trained on synthetic data.", "conclusion": "DMAP provides a unified, efficient statistical representation of text based on LLM probabilities that outperforms simple perplexity-based views, supports diverse applications from validation to forensics, and forms a basis for further research in LLM-based text analysis."}}
{"id": "2602.11964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11964", "abs": "https://arxiv.org/abs/2602.11964", "authors": ["Romain Froger", "Pierre Andrews", "Matteo Bettini", "Amar Budhiraja", "Ricardo Silveira Cabral", "Virginie Do", "Emilien Garreau", "Jean-Baptiste Gaya", "Hugo Lauren\u00e7on", "Maxime Lecanu", "Kunal Malkan", "Dheeraj Mekala", "Pierre M\u00e9nard", "Gerard Moreno-Torres Bertran", "Ulyana Piterbarg", "Mikhail Plekhanov", "Mathieu Rita", "Andrey Rusakov", "Vladislav Vorotilov", "Mengjue Wang", "Ian Yu", "Amine Benhalloum", "Gr\u00e9goire Mialon", "Thomas Scialom"], "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments", "comment": "Accepted as Oral at ICLR 2026", "summary": "We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.", "AI": {"tldr": "Gaia2 is a new benchmark for testing LLM-based agents in realistic, asynchronous, and dynamic environments, with action-level verification for both evaluation and RL training.", "motivation": "Existing evaluations of language model agents are mostly static or synchronous and fail to capture realistic, evolving environments where events happen independently of the agent. There is a need for a benchmark that stresses temporal constraints, noisy and dynamic events, ambiguity resolution, and multi-agent collaboration, while also supporting precise, verifiable feedback suitable for reinforcement learning.", "method": "The authors design Gaia2, a set of scenarios built on a consumer environment using the open-source Agents Research Environments (ARE) platform. The scenarios are asynchronous and dynamic, including temporal constraints, noisy events, and multi-agent interactions. Each scenario includes a write-action verifier that checks agent actions step-by-step, enabling fine-grained action-level evaluation and automated reward signals. They then evaluate multiple state-of-the-art proprietary and open-source LLM agents on these scenarios, comparing their pass@1 performance and analyzing trade-offs between accuracy, speed, robustness, and cost.", "result": "In experiments, GPT-5 (high) achieves the best overall performance with 42% pass@1 but performs poorly on time-sensitive tasks. Claude-4 Sonnet shows a different trade-off profile, sacrificing some accuracy and speed for lower cost. Among open-source models, Kimi-K2 performs best with 21% pass@1. No single model dominates across all capability dimensions, revealing trade-offs among reasoning, efficiency, and robustness, and showing that current agents struggle to fully bridge the simulation-to-reality (sim2real) gap in these environments.", "conclusion": "Gaia2 provides a more realistic, asynchronous benchmark for LLM agents and, thanks to its action-level verifiers, a framework suitable for reinforcement learning from verifiable rewards. The results on current leading models suggest substantial room for improvement in handling dynamic, time-sensitive, and noisy real-world-like environments. By releasing Gaia2 together with the underlying ARE platform, the authors hope to give the community a flexible, extensible infrastructure for developing, evaluating, and training more practical and capable agent systems."}}
{"id": "2602.11877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11877", "abs": "https://arxiv.org/abs/2602.11877", "authors": ["Wanxing Wu", "He Zhu", "Yixia Li", "Lei Yang", "Jiehui Zhao", "Hongru Wang", "Jian Yang", "Benyou Wang", "Bingyi Jing", "Guanhua Chen"], "title": "Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems", "comment": "Our code is publicly available at https://github.com/zhuchichi56/RouterXBench", "summary": "Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution robustness. We propose RouterXBench, a principled evaluation framework with three dimensions: router ability, scenario alignment, and cross-domain robustness. Unlike prior work that relies on output probabilities or external embeddings, we utilize internal hidden states that capture model uncertainty before answer generation. We introduce ProbeDirichlet, a lightweight router that aggregates cross-layer hidden states via learnable Dirichlet distributions with probabilistic training. Trained on multi-domain data, it generalizes robustly across in-domain and out-of-distribution scenarios. Our results show ProbeDirichlet achieves 16.68% and 18.86% relative improvements over the best baselines in router ability and high-accuracy scenarios, with consistent performance across model families, model scales, heterogeneous tasks, and agentic workflows.", "AI": {"tldr": "The paper introduces RouterXBench, a systematic benchmark and ProbeDirichlet, a hidden-state-based router to decide when to use small local models vs. large cloud LLMs, achieving better routing accuracy and robustness across domains.", "motivation": "Deploying only large language models is costly and raises privacy issues, so practitioners use smaller local models and offload hard queries to large cloud LLMs. However, deciding when to route a query to which model (the router problem) is poorly evaluated today: existing evaluations are ad hoc, ignore different application scenarios, and fail to rigorously test out-of-distribution generalization. Moreover, many routers rely on superficial signals like output probabilities or external embeddings, which may not fully capture a model\u2019s internal uncertainty or transfer well across settings. The paper aims to provide a principled evaluation framework and a better routing method that leverages richer internal signals from models.", "method": "The authors design RouterXBench, an evaluation framework with three axes: (1) router ability (how accurately the router chooses the right model), (2) scenario alignment (how well routing decisions match application-specific goals like high accuracy vs. low cost), and (3) cross-domain robustness (performance when domain shifts occur). They propose ProbeDirichlet, a lightweight router that uses internal hidden states from multiple layers of a model instead of just output logits or external embeddings. It aggregates these cross-layer hidden states via a learnable Dirichlet distribution, trained probabilistically to estimate uncertainty and guide routing. The router is trained on multi-domain data so it can generalize to new domains and tasks. Experiments compare ProbeDirichlet to existing routing methods across different model families, sizes, tasks, and agentic workflows.", "result": "ProbeDirichlet significantly outperforms strong baselines in the benchmark. It achieves 16.68% and 18.86% relative improvements over the best baselines in overall router ability and in high-accuracy scenarios, respectively. The method demonstrates consistent performance across different LLM families, model scales, heterogeneous tasks, and agent-style workflows, and it remains robust under cross-domain and out-of-distribution evaluations defined by RouterXBench.", "conclusion": "RouterXBench offers a principled, scenario-aware, and robustness-focused way to evaluate LLM routers, addressing limitations of previous ad hoc evaluations. ProbeDirichlet shows that leveraging internal hidden states and probabilistic Dirichlet-based aggregation yields a more accurate and robust router than prior probability- or embedding-based methods. The approach generalizes well across domains and setups, making it a strong candidate for practical deployment in systems that mix local small models with cloud LLMs under cost and privacy constraints."}}
{"id": "2602.12004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12004", "abs": "https://arxiv.org/abs/2602.12004", "authors": ["Robert Cronshaw", "Konstantinos Vilouras", "Junyu Yan", "Yuning Du", "Feng Chen", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation", "comment": null, "summary": "Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.", "AI": {"tldr": "Proposes CSEval, a language-model-based framework to evaluate whether medical text-to-image generations match the intended clinical semantics in their prompts.", "motivation": "Existing evaluation metrics for medical text-to-image models focus on realism and diversity but fail to check if images correctly express clinical semantics like anatomy and pathology, which is critical for safety and utility in healthcare.", "method": "Introduce Clinical Semantics Evaluator (CSEval), which uses language models to compare conditioning prompts with generated images and quantify their alignment in terms of clinical semantic attributes, detecting mismatches such as wrong anatomical site or pathology.", "result": "Experiments demonstrate that CSEval can detect semantic inconsistencies that standard image quality/diversity metrics miss and that its scores correlate well with expert clinical assessments.", "conclusion": "CSEval is an effective, scalable, and clinically meaningful complement to existing evaluation metrics, helping to ensure that medical text-to-image models are semantically reliable enough for safer deployment in healthcare contexts."}}
{"id": "2602.11886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11886", "abs": "https://arxiv.org/abs/2602.11886", "authors": ["Dante Wesslund", "Ville Stenstr\u00f6m", "Pontus Linde", "Alexander Holmberg"], "title": "LLM-based Triplet Extraction from Financial Reports", "comment": null, "summary": "Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.", "AI": {"tldr": "The paper proposes a semi-automated pipeline to extract knowledge graph triples from corporate financial reports without requiring annotated ground truth, using ontology-based proxy metrics for evaluation.", "motivation": "Evaluating knowledge graph construction from corporate financial reports is difficult because there are no large annotated ground-truth datasets, and existing ontologies may drift or not align well with specific documents. There is a need for reliable, scalable evaluation methods that can work in this low-supervision setting and reduce hallucinations from LLM-based extraction.", "method": "They design a pipeline for extracting Subject-Predicate-Object triples from corporate annual reports using large language models. Instead of comparing extracted triples to ground-truth labels, they evaluate using ontology-driven proxy metrics: Ontology Conformance (how well triples fit a given ontology) and Faithfulness (how well they correspond to the source text). They compare two ontology strategies: (1) a static, manually engineered ontology and (2) an automated, document-specific ontology induction method. They also introduce a hybrid verification strategy that first filters triples with regex rules and then uses an LLM-as-a-judge to validate whether subjects are hallucinated, accounting for coreference resolution issues.", "result": "The automatically induced ontologies achieve 100% schema conformance in all tested configurations, preventing ontology drift issues that appear with the manual ontology. The proposed hybrid verification method combining regex and LLM-as-a-judge dramatically reduces measured subject hallucination rates from 65.2% to 1.6% by removing false positives due to coreference. They also empirically observe a consistent asymmetry: objects hallucinate differently from subjects, likely due to linguistic features of financial text.", "conclusion": "Ontology-based proxy metrics can effectively evaluate knowledge graph triple extraction from corporate reports without ground truth. Automatically induced, document-specific ontologies outperform static manual ontologies in schema conformance and robustness. The hybrid regex-plus-LLM verification strategy greatly improves the reliability of hallucination detection, and the observed subject\u2013object hallucination asymmetry highlights the importance of accounting for linguistic structure in financial NLP and KG construction."}}
{"id": "2602.12013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12013", "abs": "https://arxiv.org/abs/2602.12013", "authors": ["Xiuping Wu", "Zhao Yu", "Yuxin Cheng", "Ngai Wong", "Liangjun Ke", "Tapas Mishra", "Konstantinos V. Katsikopoulos"], "title": "InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection", "comment": null, "summary": "Reasoning can significantly enhance the performance of Large Language Models. While recent studies have exploited behavior-related prompts adjustment to enhance reasoning, these designs remain largely intuitive and lack a systematic analysis of the underlying behavioral patterns. Motivated by this, we investigate how models' reasoning behaviors shape reasoning from the perspective of behavioral patterns. We observe that models exhibit adaptive distributions of reasoning behaviors when responding to specific types of questions, and that structurally injecting these patterns can substantially influence the quality of the models' reasoning processes and outcomes. Building on these findings, we propose two optimization methods that require no parameter updates: InjectCorrect and InjectRLOpt. InjectCorrect guides the model by imitating behavioral patterns derived from its own past correct answers. InjectRLOpt learns a value function from historical behavior-pattern data and, via our proposed Reliability-Aware Softmax Policy, generates behavioral injectant during inference to steer the reasoning process. Our experiments demonstrate that both methods can improve model performance across various reasoning tasks without requiring any modifications to model parameters, achieving gains of up to 5.34% and 8.67%, respectively.", "AI": {"tldr": "The paper studies how explicit reasoning behavior patterns affect LLM reasoning and proposes two training\u2011free methods, InjectCorrect and InjectRLOpt, that inject learned behavior patterns at inference time, yielding notable performance gains on reasoning tasks.", "motivation": "Existing work shows that prompting for reasoning can help LLMs, but current behavior-related prompt designs are heuristic and lack a systematic understanding of how specific reasoning behaviors impact performance. The authors aim to analyze LLMs\u2019 reasoning behaviors as patterns that vary with question types, and to exploit these patterns more rigorously to improve reasoning without retraining models.", "method": "1) Empirically analyze LLM reasoning traces to identify behavior patterns and how their distributions adapt to different question types. 2) Design InjectCorrect, which collects a model\u2019s own behavior patterns from previously correct answers and injects them into prompts to guide new reasoning. 3) Design InjectRLOpt, which fits a value function over historical behavior-pattern data and, using a Reliability-Aware Softmax Policy, generates behavior-pattern injections during inference to steer reasoning, all without updating base model parameters.", "result": "Experiments on multiple reasoning benchmarks show that behavior-pattern injection can systematically alter and often improve reasoning processes and answers. InjectCorrect and InjectRLOpt respectively yield performance gains up to 5.34% and 8.67% over baselines, while requiring no parameter updates of the underlying LLMs.", "conclusion": "Reasoning behavior in LLMs follows adaptive patterns dependent on question types, and explicitly modeling and injecting these patterns at inference time can significantly improve reasoning quality without retraining. The proposed InjectCorrect and InjectRLOpt methods demonstrate a principled way to exploit behavior patterns and achieve consistent performance gains on reasoning tasks."}}
{"id": "2602.11898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11898", "abs": "https://arxiv.org/abs/2602.11898", "authors": ["Eddie Yang", "Dashun Wang"], "title": "Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences", "comment": null, "summary": "Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.", "AI": {"tldr": "The paper shows that even when large language models (LLMs) achieve similar benchmark accuracy, they often disagree on many individual benchmark items, creating an illusion of convergence and threatening scientific reproducibility when such models are used for data annotation and inference.", "motivation": "Benchmark scores are widely used to evaluate and compare LLMs, and similar accuracy is often interpreted as similar capabilities. However, this overlooks potential differences in which specific items models get right or wrong, and how such differences could impact downstream scientific research that relies on LLM-based annotations or reasoning. The authors aim to expose and quantify these hidden disagreements and demonstrate their real-world consequences.", "method": "The authors analyze two major reasoning benchmarks, MMLU-Pro and GPQA, comparing item-level agreement and disagreement across multiple LLMs that achieve similar overall accuracy, including top-performing frontier models. They then re-analyze published studies in education and political science that used LLMs for scientific data annotation and inference, substituting different models and measuring how the resulting treatment effect estimates change when the annotation model is switched.", "result": "They find that LLMs with comparable overall accuracy on benchmarks still disagree on 16-66% of individual items (16-38% among top frontier models), revealing distinct error profiles. In downstream scientific applications, switching the LLM used for annotation can change estimated treatment effects by more than 80%, and can even flip the sign of the effect, demonstrating large practical consequences of model choice.", "conclusion": "Apparent convergence in benchmark accuracy masks substantial epistemic divergence between LLMs. Equal benchmark scores do not imply interchangeable models; instead, model-specific error profiles and disagreements can critically affect scientific conclusions. Benchmark-based evaluation alone can thus create a 'benchmark illusion', where model choice becomes an unreported but consequential source of variation that undermines scientific reproducibility."}}
{"id": "2602.12055", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12055", "abs": "https://arxiv.org/abs/2602.12055", "authors": ["Amath Sow", "Mauricio Rodriguez Cesen", "Fabiola Martins Campos de Oliveira", "Mariusz Wzorek", "Daniel de Leng", "Mattias Tiger", "Fredrik Heintz", "Christian Esteve Rothenberg"], "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace", "comment": "AAMAS 2026 accepted paper", "summary": "Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.", "AI": {"tldr": "The paper introduces DTAPP-IICR, a scalable, delivery-time aware planning framework for preflight routing of large UAV fleets in dynamic urban airspace with temporal no-fly zones.", "motivation": "Managing large UAV fleets in shared, dynamic urban airspace is difficult due to temporal no-fly zones, different UAV capabilities, and strict delivery deadlines. Existing Multi-Agent Path Finding (MAPF) methods either do not scale to thousands of agents, lack flexibility for heterogeneous vehicles and temporal constraints, or are too slow for practical Unmanned Traffic Management. A new method is needed that is both scalable and realistic enough to handle city-scale operations with complex constraints.", "method": "The authors propose DTAPP-IICR, a two-stage prioritized planning framework. First, missions are ordered by urgency (delivery-time awareness) and planned sequentially. For each mission, a new 4D single-agent planner, SFIPP-ST (Safe Flight Interval Path Planning with Soft and Temporal Constraints), computes roundtrip trajectories that respect temporal no-fly zones, heterogeneous UAV dynamics, and represent inter-agent conflicts as soft constraints. After obtaining an initial solution, a geometric conflict graph is built and an iterative Large Neighborhood Search (LNS) procedure incrementally repairs residual conflicts by replanning subsets of trajectories. Additionally, a completeness-preserving directional pruning technique speeds up the underlying 3D search within SFIPP-ST.", "result": "On benchmarks with temporal no-fly zones, DTAPP-IICR achieves nearly 100% success rates for fleets of up to 1,000 UAVs. The directional pruning yields up to a 50% reduction in runtime. Compared against batch Enhanced Conflict-Based Search in a UTM setting, DTAPP-IICR performs better in terms of scalability and efficiency. It also scales to realistic city-scale operations where other priority-based planners fail at comparatively moderate fleet sizes.", "conclusion": "DTAPP-IICR is an effective and scalable solution for preflight planning of large UAV fleets in dense, dynamic urban environments. By combining urgency-based prioritization, a temporally-aware single-agent planner, and iterative conflict resolution via LNS and conflict graphs, it handles temporal NFZs and heterogeneous vehicles while achieving high success rates and strong runtime performance. This positions it as a practical candidate for real-world Unmanned Traffic Management systems."}}
{"id": "2602.11931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11931", "abs": "https://arxiv.org/abs/2602.11931", "authors": ["Pretam Ray", "Pratik Prabhanjan Brahma", "Zicheng Liu", "Emad Barsoum"], "title": "AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection", "comment": "8 pages, 2 Figues", "summary": "Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at https://github.com/raypretam/adaptive_llm_selection.", "AI": {"tldr": "The paper proposes AdaptEvolve, a method for adaptively choosing among multiple LLMs in an evolutionary agent system to balance reasoning quality and compute cost, using confidence-based routing.", "motivation": "Evolutionary / multi-step agentic systems often call LLMs many times during inference, making computation expensive. Existing multi-LLM cascades or routers are mostly static or rely on external heuristics and do not explicitly use the model\u2019s own uncertainty, leading to suboptimal trade-offs between cost and capability. The authors aim to make agentic systems more efficient without sacrificing much accuracy by adaptively choosing the smallest LLM that can likely solve each step.", "method": "They design AdaptEvolve within an evolutionary sequential refinement framework, where candidate solutions are iteratively generated and refined. At each generation step, the system estimates the real-time solvability of the current subproblem using intrinsic generation confidence signals from the LLM(s). Based on this confidence, the agent adaptively selects which LLM (from a pool of different sizes/costs) to invoke next, instead of always using a fixed large model or a static cascade. This yields a confidence-driven multi-LLM selection policy that is integrated into the evolutionary refinement cycle.", "result": "Across multiple benchmarks, AdaptEvolve achieves a better Pareto frontier of accuracy vs. inference cost than baselines. Specifically, it reduces total inference cost by an average of 37.9% while retaining 97.5% of the upper-bound accuracy obtained by always using a large model. These empirical findings suggest that confidence-based adaptive routing is effective in multi-LLM evolutionary systems.", "conclusion": "Adaptive, confidence-driven LLM selection within an evolutionary refinement framework can substantially reduce compute cost while preserving most of the performance of strong but expensive models. Model uncertainty signals are valuable for dynamic routing, improving the efficiency\u2013accuracy trade-off beyond static cascades or heuristic controllers. AdaptEvolve demonstrates a practical approach to building more compute-efficient agentic systems using multiple LLMs."}}
{"id": "2602.12056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12056", "abs": "https://arxiv.org/abs/2602.12056", "authors": ["Xinyu Yang", "Chenlong Deng", "Tongyu Wen", "Binyu Xie", "Zhicheng Dou"], "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments", "comment": null, "summary": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .", "AI": {"tldr": "LawThinker is an autonomous legal research agent that improves legal reasoning by systematically verifying each intermediate step via a DeepVerifier and memory, yielding better accuracy and procedural compliance than existing methods.", "motivation": "Existing legal reasoning systems can reach correct or plausible final answers but often lack mechanisms to verify each intermediate reasoning step. This can lead to undetected errors, such as citing inapplicable statutes, which then contaminate subsequent reasoning. Moreover, legal tasks occur in dynamic judicial environments where laws and precedents evolve, requiring systems that can adapt and maintain procedural compliance while ensuring knowledge accuracy and relevance to case facts.", "method": "The authors propose LawThinker, an autonomous legal research agent that follows an Explore-Verify-Memorize strategy. After each knowledge exploration or retrieval step, a DeepVerifier module is invoked as an atomic operation to validate the retrieved information along three dimensions: (1) knowledge accuracy, (2) relevance between facts and law, and (3) procedural compliance in legal reasoning. A memory module stores verified knowledge so it can be reused across multiple reasoning rounds, enabling effective handling of long-horizon tasks. The system is evaluated on both dynamic and static legal benchmarks.", "result": "On the dynamic benchmark J1-EVAL, LawThinker outperforms direct reasoning approaches by 24% and workflow-based methods by 11%, particularly excelling on metrics that measure the quality and correctness of the reasoning process itself (process-oriented metrics). Additional experiments on three static benchmarks show that the model\u2019s performance gains are not limited to dynamic settings, suggesting stronger robustness and generalization across different legal tasks and datasets.", "conclusion": "LawThinker demonstrates that enforcing verification after each exploration step in legal reasoning, coupled with reusable verified memory, significantly improves both outcome accuracy and procedural soundness. Its strong results on dynamic and static benchmarks suggest that the Explore-Verify-Memorize paradigm and the DeepVerifier module provide a general and effective framework for building reliable, process-compliant legal research agents."}}
{"id": "2602.11933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11933", "abs": "https://arxiv.org/abs/2602.11933", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text", "comment": null, "summary": "End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, \"clean\" datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional variations common in non-native or dialectal speech. In this work, we adapt a text-based adversarial attack targeting inflectional morphology to the speech domain and demonstrate that state-of-the-art E2E-ST models are highly vulnerable it. While adversarial training effectively mitigates such risks in text-based tasks, generating high-quality adversarial speech data remains computationally expensive and technically challenging. To address this, we propose Cross-Modal Robustness Transfer (CMRT), a framework that transfers adversarial robustness from the text modality to the speech modality. Our method eliminates the requirement for adversarial speech data during training. Extensive experiments across four language pairs demonstrate that CMRT improves adversarial robustness by an average of more than 3 BLEU points, establishing a new baseline for robust E2E-ST without the overhead of generating adversarial speech.", "AI": {"tldr": "Paper analyzes robustness of end-to-end speech translation models to inflectional morphology adversarial attacks and proposes a cross-modal robustness transfer method.", "motivation": "Current E2E speech translation models are evaluated on clean datasets and ignore real-world robustness issues, especially vulnerability to inflectional variations typical in non-native or dialectal speech. Generating adversarial speech for robustness training is costly and difficult.", "method": "Adapt a text-based adversarial attack on inflectional morphology to the speech domain to test robustness of E2E-ST models. Propose Cross-Modal Robustness Transfer (CMRT), a framework that transfers adversarial robustness learned in text-based models to speech models, eliminating the need for adversarial speech data during training. Evaluate across four language pairs.", "result": "State-of-the-art E2E-ST models are shown to be highly vulnerable to inflectional morphology adversarial attacks. CMRT improves adversarial robustness by over 3 BLEU points on average across four language pairs and sets a new baseline for robust E2E-ST without adversarial speech generation.", "conclusion": "E2E-ST systems lack robustness to inflectional morphological variations, but adversarial robustness can be effectively transferred from text to speech via CMRT, offering a practical way to strengthen real-world robustness without expensive adversarial speech data generation."}}
{"id": "2602.11938", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11938", "abs": "https://arxiv.org/abs/2602.11938", "authors": ["Yunchong Huang", "Gianni Barlacchi", "Sandro Pezzelle"], "title": "Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance", "comment": "4 pages of main text, 13 pages in total, 5 tables and 10 figures in total", "summary": "Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be uniquely determined without additional context. To test this hypothesis, we introduce an LLM-based classifier to identify underspecified questions and apply it to several widely used QA datasets, finding that 16% to over 50% of benchmark questions are underspecified and that LLMs perform significantly worse on them. To isolate the effect of underspecification, we conduct a controlled rewriting experiment that serves as an upper-bound analysis, rewriting underspecified questions into fully specified variants while holding gold answers fixed. QA performance consistently improves under this setting, indicating that many apparent QA failures stem from question underspecification rather than model limitations. Our findings highlight underspecification as an important confound in QA evaluation and motivate greater attention to question clarity in benchmark design.", "AI": {"tldr": "The paper shows that many QA benchmark questions are underspecified, hurting LLM performance, and that clarifying these questions substantially boosts accuracy, meaning benchmarks partly measure question quality rather than just model ability.", "motivation": "Despite strong performance of LLMs on clearly posed questions, standard QA benchmarks are still far from solved. The authors suspect that a key reason is that many benchmark questions are underspecified, making it impossible to determine a unique correct interpretation without extra context. This undermines fair evaluation of model capabilities and clouds our understanding of true QA progress.", "method": "The authors first build an LLM-based classifier to automatically detect underspecified questions. They then run this classifier over several popular QA datasets to estimate how prevalent underspecification is and compare LLM performance on underspecified vs. well-specified questions. To directly study the effect of underspecification, they perform a controlled rewriting experiment: for questions flagged as underspecified, they manually or systematically rewrite them into fully specified versions while keeping the original gold answers unchanged. They then re-evaluate QA performance under these clarified conditions.", "result": "The classifier finds that between 16% and more than 50% of questions in common QA benchmarks are underspecified. LLM performance is substantially worse on these questions than on well-specified ones. In the rewriting experiment, where underspecified questions are converted into fully specified versions, QA accuracy consistently increases, indicating that much of the apparent model error was due to ambiguity in the questions rather than inability to retrieve or reason about the answer.", "conclusion": "The study concludes that question underspecification is a major confound in evaluating QA systems, including LLMs. Many failures on benchmarks are artifacts of ambiguous or incomplete question phrasing rather than true model limitations. Therefore, progress in QA requires not only better models but also improved benchmark design with clearer, fully specified questions, and evaluations that explicitly account for underspecification."}}
{"id": "2602.12083", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.12083", "abs": "https://arxiv.org/abs/2602.12083", "authors": ["Antonin Sulc"], "title": "Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication", "comment": "29 pages, 8 figures, 8 tables, Tutorial at 3rd International Conference on Neuro-Symbolic Systems (NeuS)", "summary": "As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.\n  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.", "AI": {"tldr": "The paper introduces Differentiable Modal Logic (DML) and Modal Logical Neural Networks (MLNNs) to debug multi-agent AI systems by learning modal structures like trust, causality, and norms directly from data.", "motivation": "As multi-agent AI systems grow more complex and autonomous, debugging failures requires understanding agents' knowledge, beliefs, causal relations, and obligations. Classical modal logic is good at expressing these but assumes manually specified, static relational structures, which is unrealistic in dynamic real-world systems.", "method": "The authors propose Differentiable Modal Logic (DML), operationalized through Modal Logical Neural Networks (MLNNs). These models represent modal structures (e.g., trust networks, causal graphs, regulatory constraints) as explicit, learnable parameters. Logical axioms are made differentiable and used as soft constraints or loss terms. They instantiate four modal logics\u2014epistemic, temporal, deontic, and doxastic\u2014and apply them to multi-agent scenarios such as diplomacy games and LLM hallucination detection, turning logical contradictions into optimization objectives. They provide code in Jupyter notebooks for practical use.", "result": "The framework learns interpretable structures: trust edges between agents, causal dependencies over time, and permitted/forbidden actions. It supports combining multiple modalities (epistemic, temporal, deontic) in a single reasoning process, and enables injecting prior knowledge through differentiable axioms to improve learning under sparse data. The system can uncover deceptive alliances, identify causal chains leading to failures, and detect inconsistencies in LLM outputs.", "conclusion": "Differentiable Modal Logic and MLNNs offer a unified neurosymbolic approach for monitoring, controlling, and explaining multi-agent AI systems. By making modal relations learnable, structured, and interpretable, and by turning logical constraints into differentiable objectives, the method bridges formal modal logic and gradient-based learning, with practical deployment recipes and open-source implementations."}}
{"id": "2602.11939", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11939", "abs": "https://arxiv.org/abs/2602.11939", "authors": ["Elisa Bassignana", "Mike Zhang", "Dirk Hovy", "Amanda Cercas Curry"], "title": "Do Large Language Models Adapt to Language Variation across Socioeconomic Status?", "comment": null, "summary": "Humans adjust their linguistic style to the audience they are addressing. However, the extent to which LLMs adapt to different social contexts is largely unknown. As these models increasingly mediate human-to-human communication, their failure to adapt to diverse styles can perpetuate stereotypes and marginalize communities whose linguistic norms are less closely mirrored by the models, thereby reinforcing social stratification. We study the extent to which LLMs integrate into social media communication across different socioeconomic status (SES) communities. We collect a novel dataset from Reddit and YouTube, stratified by SES. We prompt four LLMs with incomplete text from that corpus and compare the LLM-generated completions to the originals along 94 sociolinguistic metrics, including syntactic, rhetorical, and lexical features. LLMs modulate their style with respect to SES to only a minor extent, often resulting in approximation or caricature, and tend to emulate the style of upper SES more effectively. Our findings (1) show how LLMs risk amplifying linguistic hierarchies and (2) call into question their validity for agent-based social simulation, survey experiments, and any research relying on language style as a social signal.", "AI": {"tldr": "The paper evaluates how well large language models adapt their linguistic style to different socioeconomic communities on social media and finds that they only weakly adjust and better match upper-SES styles, potentially reinforcing linguistic hierarchies.", "motivation": "Humans naturally shift their linguistic style depending on their audience, but it is unclear whether LLMs do the same across diverse social contexts. As LLMs are increasingly used in communication and social science research, a failure to reflect varied linguistic norms can marginalize communities, perpetuate stereotypes, and distort studies that rely on language as a social signal. The authors are motivated to empirically measure how well LLMs integrate into online discourse across socioeconomic strata.", "method": "The authors build a new Reddit and YouTube dataset stratified by socioeconomic status (SES). They feed four different LLMs incomplete text segments from this corpus and ask the models to generate continuations. They then compare these continuations to the human originals using 94 sociolinguistic metrics spanning syntax, rhetoric, and lexical choices. By examining how model outputs vary with SES of the source communities relative to human baselines, they assess the degree and nature of style adaptation.", "result": "The study finds that LLMs only modestly adjust their linguistic style in response to SES differences. Their adaptations often approximate or caricature the target style rather than authentically match it, and the models better emulate the language patterns of higher-SES communities than lower-SES ones. This reveals systematic asymmetries in how LLMs reflect different social groups\u2019 linguistic norms.", "conclusion": "LLMs currently integrate poorly into the full diversity of social media communication, especially for lower-SES communities. This limited and uneven stylistic adaptation risks amplifying existing linguistic hierarchies and marginalization. Consequently, the paper argues that using current LLMs for agent-based simulations, survey experiments, or any research that treats generated language style as a faithful social signal may be invalid or seriously biased."}}
{"id": "2602.12108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12108", "abs": "https://arxiv.org/abs/2602.12108", "authors": ["Xiaoyuan Liu", "Tian Liang", "Dongyang Ma", "Deyu Zhou", "Haitao Mi", "Pinjia He", "Yan Wang"], "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context", "comment": null, "summary": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.", "AI": {"tldr": "The paper introduces StateLM, a new class of language models that can actively manage their own memory and context via an internal reasoning loop and external memory tools, significantly improving performance on long-context and research-intensive tasks.", "motivation": "Traditional LLMs are constrained by a fixed context window and must passively accept manually engineered prompts and retrieval-augmented contexts, limiting their ability to handle long documents, maintain rich conversational history, and perform deep, iterative research. The authors aim to remove this constraint by giving models agency over their own memory and context management, analogous to giving Dumbledore the wand to operate the Pensieve.", "method": "The authors design StateLM, a foundation model augmented with an internal reasoning loop and a suite of memory-management tools, including context pruning, document indexing, and note-taking. The model is trained to call and coordinate these tools to dynamically construct, update, and compress its working context, effectively performing its own prompt engineering and memory management during inference. This turns inference into an iterative, stateful process rather than a single forward pass over a fixed input window.", "result": "Across multiple model sizes and tasks, StateLM outperforms standard LLMs. On long-document QA benchmarks, StateLM consistently achieves higher accuracy than comparable baselines at all scales. On chat memory tasks, it delivers absolute accuracy gains of 10\u201320 percentage points. On the challenging deep research benchmark BrowseComp-Plus, StateLM attains up to 52% accuracy, while standard LLMs with similar capacity achieve only about 5%, highlighting the advantage of autonomous state management.", "conclusion": "Giving LLMs explicit mechanisms and training to manage their own external memory and context transforms them from passive sequence predictors into stateful, state-aware agents. This architectural shift, centered on an internal reasoning loop and tool-mediated memory operations, breaks the limitation of fixed context windows and yields large gains on tasks requiring long-term memory and deep, iterative reasoning."}}
{"id": "2602.11961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11961", "abs": "https://arxiv.org/abs/2602.11961", "authors": ["Yuzhe Shang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Jinsong Su"], "title": "Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models", "comment": null, "summary": "Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.", "AI": {"tldr": "The paper evaluates and adapts open LLMs for multilingual machine translation, introducing MiLMMT-46 based on Gemma3 that achieves top-tier performance on 46 languages.", "motivation": "Open LLMs are becoming increasingly multilingual, but it is unclear how well they can serve as strong multilingual MT systems versus both open SOTA MT models and proprietary systems, and how model/data scaling strategies affect this. The authors aim to systematically study these questions.", "method": "They adapt the Gemma3 model family to multilingual machine translation using continual pretraining and instruction finetuning, exploring different model sizes and data scales. They then build MiLMMT-46, a multilingual MT model covering 46 languages, and benchmark it against recent open SOTA MT models and proprietary systems.", "result": "MiLMMT-46 consistently outperforms strong recent open-source MT baselines such as Seed-X, HY-MT-1.5, and TranslateGemma, and reaches performance comparable to proprietary systems like Google Translate and Gemini 3 Pro across 46 languages.", "conclusion": "With appropriate continual pretraining and instruction finetuning, scaled open LLMs like Gemma3 can be turned into highly competitive multilingual MT systems (MiLMMT-46) that match or exceed recent open SOTA and approach the quality of leading proprietary translation systems."}}
{"id": "2602.12113", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12113", "abs": "https://arxiv.org/abs/2602.12113", "authors": ["Zewei Yu", "Lirong Gao", "Yuke Zhu", "Bo Zheng", "Sheng Guo", "Haobo Wang", "Junbo Zhao"], "title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty", "comment": "Accepted to ICLR 2026", "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .", "AI": {"tldr": "The paper proposes ARLCP, a reinforcement learning framework that reduces unnecessary, overly long chain-of-thought reasoning in Large Reasoning Models by adaptively penalizing excessive reflection and response length, achieving shorter outputs and higher accuracy on math benchmarks.", "motivation": "Large Reasoning Models gain accuracy from test-time scaling but often produce excessively long, repetitive chains-of-thought, especially on complex problems. This causes high token cost, latency, and sometimes even harms accuracy, with a particularly strong effect on smaller models. The authors observe that higher problem complexity leads models to over-reflect, reducing accuracy and efficiency, so they want a method that keeps the benefits of deliberate reasoning while controlling its cost and avoiding counterproductive reflection.", "method": "They introduce ARLCP (Adaptive Reflection and Length Coordinated Penalty), a reinforcement learning framework that trains reasoning models to balance solution quality and reasoning efficiency. ARLCP uses (1) an adaptive reflection penalty that discourages unnecessary reflective steps (e.g., repetitive self-questioning, circular reasoning) while retaining essential reasoning; and (2) a length penalty that is calibrated to an estimate of each problem\u2019s complexity so that harder problems are allowed more reasoning tokens, but not excessive ones. These two penalties are coordinated in the RL objective to shape model behavior toward concise, targeted chains-of-thought. The method is implemented and evaluated on DeepSeek-R1-Distill-Qwen-1.5B and 7B models over several math reasoning benchmarks.", "result": "On five mathematical reasoning benchmarks and two model sizes (1.5B and 7B), ARLCP yields better efficiency-accuracy trade-offs than baselines. For the 1.5B model, it reduces average response length by 53.1% while improving accuracy by 5.8%. For the 7B model, it reduces length by 35.0% and increases accuracy by 2.7%. These results indicate that the method can significantly cut token usage and latency while not only maintaining but improving task performance.", "conclusion": "Adaptive penalties on reflection depth and output length, coordinated with problem complexity, can train reasoning models to avoid unnecessary and even harmful overthinking. ARLCP demonstrates that it is possible to substantially reduce chain-of-thought length, and therefore cost and latency, while slightly improving accuracy on mathematical reasoning tasks, especially for smaller models. This suggests that more nuanced control of test-time reasoning, guided by RL, is a promising direction for efficient and reliable large reasoning models."}}
{"id": "2602.11968", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11968", "abs": "https://arxiv.org/abs/2602.11968", "authors": ["Mariia Fedorova", "Andrey Kutuzov", "Khonzoda Umarova"], "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling", "comment": "LChange'26 workshop at the EACL 2026 conference", "summary": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.", "AI": {"tldr": "Introduces DHPLT, a multilingual diachronic corpus collection for 41 languages across three time periods, with 1M documents per period and pre-computed embeddings to support semantic change research.", "motivation": "There is a shortage of multilingual diachronic corpora suitable for semantic change modeling, mostly limited to a small set of high-resource languages. Researchers need large, time-stamped, and comparable datasets across many languages and periods to study lexical and semantic change at scale.", "method": "The authors build DHPLT on top of HPLT web-crawled datasets, using crawl timestamps as a proxy for document creation time. They sample 1 million documents per language for each of three time periods (2011\u20132015, 2020\u20132021, 2024\u2013present). They then pre-compute word-type and token-level embeddings, as well as lexical substitutions, for a set of target words, and organize all resources by language for easy access.", "result": "They release a large-scale diachronic corpus collection covering 41 languages and three distinct time slices, along with ready-to-use embeddings and lexical substitution data. The resource is openly available via a public URL and is structured to facilitate custom target-word selection by other researchers.", "conclusion": "DHPLT substantially expands the available resources for diachronic semantic change modeling beyond a small group of high-resource languages. By providing multilingual, time-stratified corpora and derived representations, it enables new experimental setups and comparative studies in lexical/semantic change across many languages."}}
{"id": "2602.12120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12120", "abs": "https://arxiv.org/abs/2602.12120", "authors": ["Jittarin Jetwiriyanon", "Teo Susnjak", "Surangika Ranathunga"], "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models", "comment": "31 pages, 5 figures, 3 tables", "summary": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.", "AI": {"tldr": "Paper evaluates modern time-series foundation models (TSFMs) for zero-shot university enrolment forecasting under data-sparse, structurally unstable conditions.", "motivation": "Universities need reliable forecasts of new student enrolments for financial planning, but the data are annual, short, and often disrupted by reporting changes and regime shifts, making classical forecasting methods unreliable.", "method": "Benchmark multiple families of TSFMs in a zero-shot setting against classical models, using an expanding-window backtest with vintage-aligned data, \u201cleakage-safe\u201d covariates including a compact set of Google Trends-based demand proxies and a newly constructed Institutional Operating Conditions Index (IOCI), a 0\u2013100 regime indicator derived from time-stamped documentary sources.", "result": "Covariate-conditioned TSFMs, using only generic priors and no institution-specific training, achieve forecasting accuracy comparable to classical benchmarks; relative performance depends on student cohort and model choice.", "conclusion": "With carefully designed, leakage-disciplined covariates, zero-shot TSFMs can match traditional forecasting performance for university enrolments in data-sparse, regime-shifting environments, suggesting they are a viable, low-data alternative for institutional forecasting."}}
{"id": "2602.11982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11982", "abs": "https://arxiv.org/abs/2602.11982", "authors": ["Varpu Vehom\u00e4ki", "Kimmo K. Kaski"], "title": "Automatic Simplification of Common Vulnerabilities and Exposures Descriptions", "comment": "8 pages, 1 figure, submitted to Nordic Machine Intelligence", "summary": "Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\\_nmi.", "AI": {"tldr": "The paper studies how large language models can simplify complex cybersecurity vulnerability descriptions (CVEs), building a baseline and evaluation dataset, and finds that current LLMs simplify wording but often distort the original meaning.", "motivation": "Cybersecurity information such as CVE descriptions is crucial for both experts and non-experts, but the language is often too technical and complex. While automatic text simplification has been explored in other domains (medical, scientific, news), there is a gap in applying it to cybersecurity, which is both complex and rapidly evolving. The authors aim to fill this gap and understand whether LLMs can reliably simplify vulnerability descriptions without losing critical information.", "method": "The authors collected 40 CVE descriptions and used large language models to automatically simplify these texts. They established a baseline system for cybersecurity-focused automatic text simplification and then had two groups of cybersecurity experts evaluate the simplifications in two survey rounds. The evaluation focused on aspects such as perceived simplicity and preservation of the original meaning.", "result": "The study shows that off-the-shelf LLMs can produce vulnerability descriptions that look simpler and more accessible, but they often fail to preserve the original technical meaning accurately. This indicates a trade-off between readability and fidelity in the current LLM-based simplification approaches for CVE descriptions.", "conclusion": "LLMs have potential for simplifying cybersecurity vulnerability descriptions, making them appear more accessible, but their tendency to alter or lose important meaning makes them unreliable for this domain in their current form. The paper provides a first baseline, a curated evaluation dataset, and code to support further research on domain-specific, meaning-preserving text simplification in cybersecurity."}}
{"id": "2602.12128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12128", "abs": "https://arxiv.org/abs/2602.12128", "authors": ["Hanno Ackermann", "Hong Cai", "Mohsen Ghafoorian", "Amirhossein Habibian"], "title": "HLA: Hadamard Linear Attention", "comment": null, "summary": "The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.\n  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.", "AI": {"tldr": "This paper introduces Hadamard Linear Attention (HLA), a new linear-time attention mechanism whose nonlinearity is applied after computing pairwise token similarities, enabling a higher-fidelity rational approximation of softmax attention while retaining linear attention efficiency.", "motivation": "Standard softmax attention in transformers has a quadratic computational cost in the sequence length because it computes all pairwise token interactions. Linear attention methods reduce this cost but typically approximate softmax with low-degree rational functions by applying kernel mappings independently to queries and keys, which can limit expressiveness and approximation quality. There is a need for an attention mechanism that maintains linear-time complexity while more accurately mimicking softmax attention and being practical for large-scale applications like video diffusion transformers handling huge numbers of tokens.", "method": "The authors propose Hadamard Linear Attention (HLA), where, instead of applying nonlinear kernel maps separately to queries and keys (as in conventional linear attention), a nonlinearity is applied after computing pairwise similarities, more closely mirroring standard softmax attention. They design this nonlinearity such that it corresponds to a higher-degree rational function approximation of the softmax kernel. They then derive an efficient computation scheme for HLA that preserves the linear-time and linear-memory advantages of standard linear attention and avoids costly tensor reshaping operations common in other methods.", "result": "The paper shows theoretically that the HLA nonlinearity yields a higher-degree rational approximation to softmax compared to prior linear attention kernels. Empirically, when integrated into a large diffusion transformer model for video generation\u2014an extremely token-heavy setting\u2014HLA proves effective, demonstrating strong performance while maintaining the efficiency benefits of linear attention.", "conclusion": "Hadamard Linear Attention provides a more accurate approximation to softmax attention than previous linear attention methods by shifting the placement of the nonlinearity to post-similarity computation and using a higher-degree rational function, all while maintaining linear complexity and implementation simplicity (no heavy tensor reshaping). Experiments on large-scale video diffusion transformers validate that HLA can scale to applications with huge token counts while retaining practical efficiency and strong model performance."}}
{"id": "2602.12005", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12005", "abs": "https://arxiv.org/abs/2602.12005", "authors": ["Szilvia Ujv\u00e1ry", "Louis B\u00e9thune", "Pierre Ablin", "Jo\u00e3o Monteiro", "Marco Cuturi", "Michael Kirchhof"], "title": "LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss", "comment": "29 pages, 24 figures, 5 tables, preprint", "summary": "Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \\emph{which tokens an SLM can and should learn} during pretraining, versus \\emph{which ones it should delegate} via a \\texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \\emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \\texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.", "AI": {"tldr": "The paper introduces LaCy, a pretraining method that teaches small language models when to generate tokens themselves and when to delegate to external sources, improving factual accuracy.", "motivation": "Small language models lack sufficient parameter capacity to store all necessary world knowledge, which leads to factual errors. While they can query external sources, it is unclear which knowledge should be internalized versus delegated during pretraining.", "method": "The authors formalize token-level delegation using a special <CALL> token. They analyze which tokens should be learned versus delegated, showing that loss alone is insufficient. They augment the loss with syntactic information from a spaCy grammar parser to distinguish acceptable high-loss tokens from ones that should trigger delegation, and design a pretraining scheme (LaCy) based on this token-selection rule.", "result": "Models trained with LaCy learn effective token-level delegation patterns and achieve higher factual accuracy (FactScore) when used in a cascade with a larger model. LaCy outperforms SLMs trained using Rho or LLM-judge-based supervision while being simpler and cheaper to train.", "conclusion": "Explicitly teaching SLMs which tokens to predict and which to delegate via a grammar-augmented loss signal yields better factual reliability in retrieval- or cascade-based setups, offering a practical and efficient approach to leveraging external knowledge sources."}}
{"id": "2602.12133", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.12133", "abs": "https://arxiv.org/abs/2602.12133", "authors": ["Roberto Balestri"], "title": "Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5", "comment": null, "summary": "This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong \"default white\" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.", "AI": {"tldr": "The paper audits gender and skin-tone biases in two major image generators, finding strong default white bias and divergent gender defaults, and proposes a rigorous colorimetric framework for such audits.", "motivation": "To test the common assumption that neutral text prompts produce demographically neutral images and to systematically measure demographic bias in widely used image generation models.", "method": "The authors generated 3,200 photorealistic images from four semantically neutral prompts using Gemini Flash 2.5 Image and GPT Image 1.5. They applied a pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification via Monk (MST), PERLA, and Fitzpatrick scales to disentangle lighting and aesthetic effects from underlying skin pigmentation.", "result": "Both image generators showed a pronounced default toward light/white skin (>96% of outputs). Gemini tended to generate more female-presenting figures, whereas GPT Image 1.5 favored male-presenting figures, especially with lighter skin tones. Neutral prompts consistently yielded demographically skewed, not balanced, outputs.", "conclusion": "Neutral prompts are not neutral instructions but diagnostic probes that reveal embedded demographic defaults in image models. The proposed illumination-aware colorimetric methodology enables robust, large-scale audits of algorithmic visual culture and challenges the assumption that unmarked language naturally leads to inclusive, representative outputs."}}
{"id": "2602.12015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12015", "abs": "https://arxiv.org/abs/2602.12015", "authors": ["Angelo Ziletti", "Leonardo D'Ambrosi"], "title": "Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study", "comment": null, "summary": "Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.", "AI": {"tldr": "The paper introduces CLUES, a framework for clinical Text-to-SQL that separates uncertainty into ambiguity (input-related) and instability (model-related) to better predict failures and guide interventions.", "motivation": "In clinical Text-to-SQL applications, it is critical to know when model outputs are unreliable. Existing uncertainty measures (like Kernel Language Entropy) give a single aggregate score and cannot distinguish whether diversity in model outputs comes from ambiguous user input or from the model\u2019s own instability. This distinction matters operationally: ambiguous inputs should prompt clarification with the user, while instability should trigger human review or model improvements. The paper addresses this gap by explicitly modeling and decomposing these two sources of semantic uncertainty.", "method": "The authors propose CLUES, a two-stage framework that conceptualizes Text-to-SQL as mapping from user questions to possible interpretations, and from interpretations to answers. They build a bipartite semantic graph between interpretations and answers and define two scores: an ambiguity score that captures dispersion over interpretations, and an instability score that captures dispersion from interpretations to answers. The instability score is mathematically derived using the Schur complement of the bipartite graph\u2019s matrix. They evaluate CLUES on AmbigQA and SituatedQA datasets, where gold interpretations are available, and on a clinical Text-to-SQL benchmark where interpretations are known, comparing against Kernel Language Entropy for failure prediction.", "result": "CLUES achieves better failure prediction performance than state-of-the-art Kernel Language Entropy on AmbigQA, SituatedQA, and a clinical Text-to-SQL benchmark when gold or known interpretations are available. In real deployment scenarios, its performance remains competitive while also yielding a decomposition into ambiguity and instability that single-score methods cannot provide. The framework identifies a specific high-ambiguity/high-instability regime that contains 51% of all model errors while accounting for only 25% of queries, enabling efficient triage and targeted interventions.", "conclusion": "By decomposing semantic uncertainty into ambiguity and instability, CLUES enables more informative and actionable uncertainty estimates for clinical Text-to-SQL systems. This decomposition supports operational decisions: refining queries or asking for clarification in high-ambiguity cases, prioritizing model or workflow improvements in high-instability cases, and focusing triage efforts on cases that fall into high-ambiguity/high-instability regimes where errors are concentrated. The approach improves or matches existing failure prediction baselines while adding valuable diagnostic insight for real-world deployment."}}
{"id": "2602.12134", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.12134", "abs": "https://arxiv.org/abs/2602.12134", "authors": ["Jiajun Chen", "Hua Shen"], "title": "Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment", "comment": "Preprint. Under review. 20 pages, 13 figures", "summary": "Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.", "AI": {"tldr": "Introduces Value Alignment Tax (VAT), a metric to quantify how aligning a model toward specific values unintentionally shifts other interconnected values, revealing systemic alignment risks beyond target metrics.", "motivation": "Prior work treats value alignment as static and local to target values, ignoring how interventions like prompting or fine-tuning can reshape the model\u2019s broader value system. This leaves hidden risks, because changes made to improve alignment on one value may distort others in unseen ways.", "method": "The authors define the Value Alignment Tax (VAT), a framework that compares the gains on targeted values with induced changes in non-targeted, interconnected values. They build a controlled scenario\u2013action dataset based on Schwartz value theory, then gather pre- and post-alignment normative judgments from LLMs subjected to different alignment interventions (prompting, fine-tuning, preference optimization). They analyze how changes propagate across values and alignment strategies.", "result": "Empirical analysis shows that alignment interventions cause uneven and structured co-movement among values: improving alignment on target values often shifts other values in systematic but non-obvious ways. These patterns differ across models and alignment methods and are not visible when looking only at the targeted value metrics.", "conclusion": "Value Alignment Tax exposes dynamic, system-wide effects of alignment interventions that standard target-only evaluations miss. It demonstrates that value alignment in LLMs operates as a coupled system of values, where changes to one part can create hidden trade-offs elsewhere, implying the need for process-level, system-oriented evaluation and design of alignment methods."}}
{"id": "2602.12036", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12036", "abs": "https://arxiv.org/abs/2602.12036", "authors": ["Xin Xu", "Clive Bai", "Kai Yang", "Tianhao Chen", "Yangkun Chen", "Weijie Liu", "Hao Chen", "Yang Wang", "Saiyong Yang", "Can Yang"], "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models", "comment": null, "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.", "AI": {"tldr": "The paper introduces Composition-RL, a method that generates new, harder verifiable prompts by composing multiple existing problems, improving data efficiency and reasoning performance in RL with verifiable rewards.", "motivation": "Large-scale verifiable prompt datasets used in RL with verifiable rewards are expensive and contain many low-value examples. As training progresses, more prompts become either too hard (0 pass rate) or too easy (pass rate 1), effectively shrinking useful training signal. The authors want to better exploit easy (pass-rate-1) prompts to increase data efficiency and improve reasoning skills without needing more labeled data.", "method": "Composition-RL automatically combines several existing verifiable problems into a single new compositional question whose answer can still be programmatically verified. These composed prompts are used for RL training alongside or instead of the original prompts. The paper also proposes a curriculum variant, where training begins with shallow compositions and gradually increases the compositional depth over time, and a cross-domain variant where prompts from different subject areas are composed to improve cross-domain generalization.", "result": "Across models from 4B to 30B parameters, training with Composition-RL yields consistently higher reasoning performance than training on the original verifiable prompt dataset alone. Using the curriculum schedule with increasing compositional depth further improves results. Cross-domain training with compositions also leads to more effective transfer across domains than standard RL on separate-domain prompts.", "conclusion": "Composition-RL is an effective and simple way to increase the value of existing verifiable prompt datasets by turning easy solved problems into harder compositional ones. This improves reasoning ability, enhances cross-domain RL, and reduces the need for ever-larger curated verifiable datasets."}}
{"id": "2602.12143", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12143", "abs": "https://arxiv.org/abs/2602.12143", "authors": ["Xiaoxiao Wang", "Chunxiao Li", "Junying Wang", "Yijin Guo", "Zijian Chen", "Chunyi Li", "Xiaohong Liu", "Zicheng Zhang", "Guangtao Zhai"], "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction", "comment": "10 pages, 8 figures, 17 tables. Code available at https://github.com/xiaoxiaostudy/star", "summary": "As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.", "AI": {"tldr": "The paper introduces STAR, a framework that accurately predicts large model performance from very sparse evaluation data by combining probabilistic matrix factorization with agentic LLM reasoning.", "motivation": "As evaluating large models across many tasks becomes increasingly expensive, there is a need to predict model performance from only a few observed scores. Existing statistical approaches fail under distribution shifts, sparse data, and offer little interpretability, while purely LLM-based prediction is unreliable. The paper aims to build a robust, explainable predictor that can work in extreme sparsity regimes.", "method": "The authors propose STAR, which couples data-driven statistical modeling with knowledge-driven LLM reasoning. First, specialized retrievers collect external knowledge and semantic features about models and tasks. These features are incorporated into a Constrained Probabilistic Matrix Factorization (CPMF) module to generate performance predictions and associated uncertainty. Then, a reasoning module, guided by Expectation Violation Theory (EVT), analyzes where statistical expectations may be inaccurate using intra-family analysis, cross-model comparison, and credibility-aware aggregation. This agentic reasoning refines the initial predictions and yields traceable, natural-language explanations for the adjustments.", "result": "Across extensive experiments, STAR consistently outperforms both statistical baselines and pure LLM-based approaches on score prediction and ranking metrics. Under extreme data sparsity, with only 1\u20132 observed scores per test model, STAR achieves a 14.46% improvement in total score over the strongest statistical baseline, while also providing interpretable adjustment rationales.", "conclusion": "STAR demonstrates that combining constrained probabilistic matrix factorization with structured LLM-based, knowledge-informed reasoning produces more accurate and explainable predictions of large model performance, especially in highly sparse regimes. This suggests a promising direction for scalable, interpretable evaluation of large models without exhaustive testing."}}
{"id": "2602.12092", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12092", "abs": "https://arxiv.org/abs/2602.12092", "authors": ["Bo Zhang", "Jiaxuan Guo", "Lijun Li", "Dongrui Liu", "Sujin Chen", "Guanxu Chen", "Zhijie Zheng", "Qihao Lin", "Lewen Yan", "Chen Qian", "Yijin Zhou", "Yuyao Wu", "Shaoxiong Guo", "Tianyi Du", "Jingyi Yang", "Xuhao Hu", "Ziqi Miao", "Xiaoya Lu", "Jing Shao", "Xia Hu"], "title": "DeepSight: An All-in-One LM Safety Toolkit", "comment": "Technical report, 29 pages, 24 figures", "summary": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.", "AI": {"tldr": "DeepSight is an open-source, integrated framework for safety evaluation and diagnosis of large language and multimodal models, turning black-box safety checks into more interpretable, white-box analysis.", "motivation": "Existing LLM/MLLM safety workflows are fragmented: evaluation, diagnosis, and alignment are handled by separate tools. Current safety evaluation mostly detects only external risky behaviors without identifying internal causes, while diagnosis often lacks grounding in real risk scenarios and stays at a high-level explanation. As a result, alignment changes are not well connected to internal mechanism shifts and may hurt general abilities. There is a need for a unified, systematic, and scalable framework that tightly couples safety evaluation with mechanistic diagnosis.", "method": "They introduce DeepSight, an open-source safety project composed of two coordinated toolkits: DeepSafe for safety evaluation and DeepScan for safety diagnosis. By unifying task and data protocols across both, they connect evaluation outputs with internal model analysis. This allows them to move from pure black-box behavioral tests to white-box insights into the model\u2019s internal mechanisms, while keeping the framework low-cost, reproducible, efficient, and scalable.", "result": "DeepSight provides a practical, large-scale pipeline where safety evaluation results (DeepSafe) directly inform model diagnosis (DeepScan). It supports frontier AI risk evaluation and enables joint safety evaluation and diagnosis within a single, coherent framework. The system demonstrates that such an integrated paradigm is feasible and can be implemented as an open-source toolkit.", "conclusion": "DeepSight establishes a new integrated paradigm for LLM/MLLM safety by tightly coupling evaluation and diagnosis, enabling white-box safety insights. It is the first open-source toolkit designed for both frontier AI risk assessment and joint safety evaluation and diagnosis, aiming to improve safety alignment while preserving model capabilities."}}
{"id": "2602.12146", "categories": ["cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.12146", "abs": "https://arxiv.org/abs/2602.12146", "authors": ["Mahdi Khodabandeh", "Ghazal Shabani", "Arash Yousefi Jordehi", "Seyed Abolghasem Mirroshandel"], "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning", "comment": null, "summary": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.", "AI": {"tldr": "The paper presents a lossless compression method that uses reinforcement learning on top of a T5 language model to compress data into token sequences instead of continuous vectors, achieving higher compression ratios while preserving semantic and structural integrity.", "motivation": "Traditional lossless compression methods (dictionary-based, statistical) struggle with complex data formats and cannot fully exploit structural and semantic redundancy. Deep learning-based compressors often rely on dense vector representations that obscure discrete token structures, limiting interpretability and alignment with original formats. There is a need for a compression technique that remains token-based, better preserving structure while improving compression ratios.", "method": "The authors adapt a T5 language model and train it using an off-policy reinforcement learning algorithm. The model learns to encode input data into sequences of discrete tokens rather than continuous latent vectors. The RL objective is to minimize sequence length, effectively optimizing for compression ratio, while ensuring perfect reconstructability (losslessness). The system is designed to operate without relying on external grammatical or world knowledge, instead exploiting latent patterns already captured by the language model.", "result": "The proposed RL-based token-sequence compression method achieves significantly better compression ratios than conventional lossless compression methods. It successfully compresses diverse data types by leveraging the latent information present in the language model, without needing explicit semantic understanding of the content.", "conclusion": "Reinforcement-learning-driven token-based compression with a T5 architecture offers an efficient, adaptive, and practical alternative to traditional lossless compression and vector-based neural methods. By preserving token structure and optimizing sequence length, the approach attains higher compression efficiency while maintaining data integrity, suggesting broad applicability for storage and transmission across various domains."}}
{"id": "2602.12116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12116", "abs": "https://arxiv.org/abs/2602.12116", "authors": ["Pinyi Zhang", "Ting-En Lin", "Yuchuan Wu", "Jingyang Chen", "Zongqi Wang", "Hua Yang", "Ze Xu", "Fei Huang", "Kai Zhang", "Yongbin Li"], "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling", "comment": "Accepted as ICLR 2026 Oral", "summary": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.", "AI": {"tldr": "The paper proposes P-GenRM, a personalized generative reward model that improves personalized alignment of LLMs by constructing structured evaluation chains and using user prototypes plus test-time scaling.", "motivation": "Existing personalized reward models oversimplify rich, scenario-specific preferences into a few fixed principles and fail to generalize well to new users with little feedback. The paper is motivated by the need for more expressive, adaptive, and generalizable user preference modeling in open-ended LLM alignment.", "method": "They introduce P-GenRM, which turns user preference signals into structured evaluation chains that generate adaptive personas and scoring rubrics for different scenarios. The model clusters users into User Prototypes and applies a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring schemes; at the prototype level, it transfers and integrates preferences from similar users to reduce noise and improve generalization. Test-time user-based scaling adjusts reward computation on-the-fly using both the specific user and prototype information.", "result": "P-GenRM achieves state-of-the-art performance on standard personalized reward model benchmarks with an average improvement of 2.31% over baselines, shows strong generalization on an out-of-distribution dataset, and gains an extra 3% performance boost from the proposed test-time user-based scaling.", "conclusion": "Structured, generative reward modeling combined with user-prototype clustering and test-time scaling yields more accurate and generalizable personalized alignment for LLMs, outperforming prior personalized reward models and scaling effectively to unseen users."}}
{"id": "2602.12150", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12150", "abs": "https://arxiv.org/abs/2602.12150", "authors": ["John Muchovej", "Amanda Royka", "Shane Lee", "Julian Jara-Ettinger"], "title": "GPT-4o Lacks Core Features of Theory of Mind", "comment": "Submitted to CogSci 2025; see more at https://jmuchovej.com/projects/llm-tom. Note: \"abstractness\" is the second feature we test for, but due to arXiv's abstract requirements, the text has been altered", "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.", "AI": {"tldr": "The paper argues that while LLMs appear to perform well on Theory of Mind benchmarks, they likely do not possess a genuine, coherent causal model of mental states (a true ToM).", "motivation": "Prior work reports that LLMs can solve many Theory of Mind tasks, leading to claims that they may have human-like social reasoning abilities. However, these benchmarks mostly measure surface-level task success rather than the underlying cognitive representations that define ToM\u2014specifically, a causal understanding of how beliefs, desires, and intentions generate behavior. The authors aim to move beyond performance-based evaluation and test whether LLMs actually encode a coherent, domain-general causal model of mental states and actions.", "method": "The authors first adopt a cognitively grounded definition of Theory of Mind as a causal model linking mental states to behavior. Using this definition, they build an evaluation framework that tests for three criteria: coherence, domain-generality, and consistency of this causal model. They probe LLMs with a simple ToM paradigm where models must infer mental states and predict actions, and then compare performance on a logically equivalent reformulation of the task to examine whether the same underlying causal structure is being used. They then measure internal consistency between the LLMs\u2019 mental state inferences and their action predictions across conditions.", "result": "LLMs reproduce human-like judgments and succeed on standard ToM-style tasks in the simple paradigm used. However, when the tasks are presented in a logically equivalent but structurally different form, LLM performance significantly degrades, implying that the models are not relying on a stable causal representation of mental states. Moreover, analyses reveal weak alignment between the actions LLMs predict and the mental states they ascribe in matched scenarios, indicating low internal consistency of any putative ToM representation.", "conclusion": "The findings indicate that current LLMs\u2019 strong performance on social reasoning benchmarks does not stem from a robust, domain-general, and consistent Theory of Mind. Instead, their behavior is better characterized as pattern-matching that approximates human responses in specific task formats without encoding the kind of causal mental-state model central to ToM. This calls for more cognitively grounded evaluation methods and cautions against interpreting benchmark success as evidence of genuine ToM in LLMs."}}
{"id": "2602.12132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12132", "abs": "https://arxiv.org/abs/2602.12132", "authors": ["Peter J Barclay"], "title": "A Rule-based Computational Model for Gaidhlig Morphology", "comment": "A revised version of this article will be published at ICAART 2026 (https://icaart.scitevents.org/?y=2026)", "summary": "Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.", "AI": {"tldr": "Rule-based Gaelic morphology model using Wiktionary data to support tools for a low-resource language.", "motivation": "Low-resource languages lack sufficient data to train modern neural language models, yet need computational tools to support their continued use and teaching. The authors aim to create effective language technology for Scottish Gaelic (Gaidhlig) despite limited data.", "method": "They mine morphological and lexical information about Gaidhlig from Wiktionary, query it with SQL to detect and analyze patterns, and then encode these patterns in a declarative rule-base. Python utilities use this rule-base to generate inflected forms of Gaelic words.", "result": "A work-in-progress rule-based morphological model for Gaidhlig is produced, along with a set of declarative rules and Python tools capable of deriving inflected word forms from base entries in Wiktionary.", "conclusion": "Rule-based approaches, combined with structured resources like Wiktionary, can effectively leverage limited data in low-resource settings, yield interpretable models, and provide direct pedagogical benefits and a foundation for higher-level NLP tools such as dependency parsers."}}
{"id": "2602.12164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12164", "abs": "https://arxiv.org/abs/2602.12164", "authors": ["Xiaohan He", "Shiyang Feng", "Songtao Huang", "Lei Bai", "Bin Wang", "Bo Zhang"], "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.", "AI": {"tldr": "Sci-CoE is a two-stage framework that lets large language models co-evolve as both solver and verifier to improve scientific reasoning using a small labeled set plus large-scale unlabeled data with geometric rewards.", "motivation": "Existing LLM co-evolving paradigms work well in code and math but struggle on scientific reasoning because solution evaluation is unreliable and verification strategies lack diversity. There is a need for a framework that can robustly evaluate and enhance scientific reasoning with minimal supervision and better verifier behavior.", "method": "Sci-CoE uses a two-stage co-evolving setup. Stage 1 employs a small annotated dataset to train or calibrate a Verifier model so it can form basic correctness judgment anchors. Stage 2 runs large-scale self-iteration on unlabeled data by having the model act as both solver and verifier, guided by a geometric reward that jointly accounts for consensus among solutions, reliability of judgments, and diversity in verification strategies. This gradually transitions from sparse supervised learning to unsupervised learning.", "result": "On multiple general scientific benchmarks, Sci-CoE improves the complex reasoning abilities of LLMs and scales effectively with more data, leading to better performance than baselines and more robust evaluation behavior.", "conclusion": "Sci-CoE provides a scalable co-evolving framework for scientific reasoning tasks, enabling LLMs to self-improve as both solver and verifier using limited labeled data and abundant unlabeled data, producing stronger reasoning and more diverse, reliable evaluation systems."}}
{"id": "2602.12135", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12135", "abs": "https://arxiv.org/abs/2602.12135", "authors": ["Yangzhuo Li", "Shengpeng Ji", "Yifu Chen", "Tianle Liang", "Haorong Ying", "Yule Wang", "Junbo Li", "Jun Fang", "Zhou Zhao"], "title": "WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models", "comment": "Open-source at https://naruto-2024.github.io/wavbench.github.io/", "summary": "With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes \"listenability\" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.", "AI": {"tldr": "The paper introduces WavBench, a new benchmark to evaluate realistic, audio-based conversational abilities of spoken dialogue models, emphasizing reasoning, colloquial speech, and paralinguistic understanding.", "motivation": "Existing evaluations for spoken dialogue models mostly reuse text-generation benchmarks and ignore audio-specific phenomena like prosody, colloquialisms, and paralinguistic cues, as well as the higher-level reasoning that modern agents perform. There is a need for a benchmark that aligns with real-world, audio-centric, cognitively complex conversations.", "method": "The authors design WavBench, a benchmark with three complementary subsets. The Pro subset contains challenging tasks targeting reasoning-enhanced models. The Basic subset focuses on natural, colloquial spoken interaction and evaluates models by \"listenability\" (natural vocabulary, fluency, rapport) instead of strict written-form accuracy. The Acoustic subset targets paralinguistic competence, including explicit acoustic understanding, acoustic generation, and implicit dialogue that depends on non-verbal audio cues. They use these subsets to evaluate five state-of-the-art spoken dialogue models.", "result": "Using WavBench, the authors obtain comparative performance results across five state-of-the-art models, revealing strengths and weaknesses in complex reasoning, colloquial speech production, and paralinguistic handling, and highlighting gaps between current systems and the benchmark\u2019s demands.", "conclusion": "WavBench provides a structured, multi-faceted benchmark that better captures the challenges of real-world spoken dialogue, particularly for reasoning-capable models. It exposes current limitations and offers a public dataset and toolkit to guide future research toward more robust, natural, and audio-aware conversational agents."}}
{"id": "2602.12170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12170", "abs": "https://arxiv.org/abs/2602.12170", "authors": ["Greg Coppola"], "title": "Statistical Parsing for Logical Information Retrieval", "comment": "23 pages, 6 tables", "summary": "In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.\n  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.\n  We argue this reconciles formal semantics with Sutton's \"bitter lesson\" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world", "AI": {"tldr": "Introduces extensions to Quantified Boolean Bayesian Networks to support full natural deduction-style reasoning, a typed semantic language, and a deterministic grammar that interfaces with LLMs, forming a hybrid system where LLMs annotate and QBBNs verify.", "motivation": "Previous work on QBBNs lacked support for negation/backward reasoning and had no way to map natural language into the logical forms required by the model, limiting its applicability to realistic NLP tasks and full logical inference as in Prawitz's natural deduction.", "method": "Extend QBBNs with NEG factors enforcing P(x)+P(\u00acx)=1 for contrapositive reasoning; design a typed logical language with role-labeled predicates, modal quantifiers, and three expressiveness tiers; build a typed slot grammar that deterministically compiles sentences to logical form; use LLMs for preprocessing and disambiguation (PP attachment) and evaluate reasoning patterns, parsing correctness, and LLM parsing performance.", "result": "The extended QBBN engine correctly handles 44/44 test cases across 22 reasoning patterns; the typed slot grammar deterministically produces correct logical forms for 33/33 sentences with zero ambiguity; LLMs achieve 95% PP attachment accuracy but only 12.4% unlabeled attachment score for full structured parses, demonstrating they are poor at directly producing parses but effective at disambiguation.", "conclusion": "The proposed architecture\u2014LLM for preprocessing/annotation, deterministic grammar for parsing, and QBBN for probabilistic logical inference\u2014fills prior gaps in QBBNs and shows a viable way to reconcile formal semantics with large-scale data-driven methods, where LLMs remove the annotation bottleneck and QBBNs act as verifiers of structured, logical representations."}}
{"id": "2602.12137", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12137", "abs": "https://arxiv.org/abs/2602.12137", "authors": ["Ricardo Campos", "Ana Filipa Pacheco", "Ana Lu\u00edsa Fernandes", "In\u00eas Cantante", "Rute Rebou\u00e7as", "Lu\u00eds Filipe Cunha", "Jos\u00e9 Miguel Isidro", "Jos\u00e9 Pedro Evans", "Miguel Marques", "Rodrigo Batista", "Evelin Amorim", "Al\u00edpio Jorge", "Nuno Guimar\u00e3es", "S\u00e9rgio Nunes", "Ant\u00f3nio Leal", "Purifica\u00e7\u00e3o Silvano"], "title": "CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes", "comment": null, "summary": "City councils play a crucial role in local governance, directly influencing citizens' daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.", "AI": {"tldr": "The paper introduces CitiLink-Minutes, a multilayer annotated dataset of European Portuguese municipal meeting minutes to facilitate IR and NLP research.", "motivation": "Municipal meeting minutes are critical for transparency and local governance but are underexplored in IR/NLP due to the absence of suitable annotated datasets, especially for European Portuguese. The authors aim to fill this resource gap to enable computational modeling of municipal deliberations and decisions.", "method": "The authors compiled 120 official municipal meeting minutes from six Portuguese municipalities, de-identified personal information, and manually annotated the documents using two trained annotators, with curation by a linguist. They defined three annotation layers\u2014metadata, discussion subjects, and voting outcomes\u2014yielding over 38,000 annotations. They released the dataset under FAIR principles and conducted baseline experiments on metadata extraction, topic classification, and vote labeling.", "result": "The resulting CitiLink-Minutes dataset comprises over one million tokens and more than 38,000 annotated items across three layers, offering structured linkage of written minutes. Baseline models for metadata extraction, topic classification, and vote labeling demonstrate the dataset\u2019s usability for standard NLP/IR tasks.", "conclusion": "CitiLink-Minutes fills a key gap in resources for municipal governance text analysis, particularly in European Portuguese, and enables research on tasks such as information extraction, topic classification, and decision transparency. Its multilayer annotations and FAIR release position it as a valuable benchmark for future NLP/IR work on municipal decision-making records."}}
{"id": "2602.12172", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12172", "abs": "https://arxiv.org/abs/2602.12172", "authors": ["Bowei He", "Yankai Chen", "Xiaokun Zhang", "Linghe Kong", "Philip S. Yu", "Xue Liu", "Chen Ma"], "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation", "comment": "Accepted by ICLR 2026", "summary": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.", "AI": {"tldr": "The paper proposes IOA, a pedagogically-inspired three-stage framework to distill large language models into smaller ones using systematic, curriculum-based synthetic data.", "motivation": "Existing LLM distillation via synthetic data is treated as a static, one-shot process that ignores how students actually learn; this leads to inefficient knowledge transfer and limited student performance, especially on complex reasoning. The authors want a distillation framework that mimics effective human teaching and learning.", "method": "They design IOA: (1) Knowledge Identifier detects the student model\u2019s knowledge gaps vs. the teacher; (2) Organizer builds progressive curricula that sequence distilled knowledge from easier to harder; (3) Adapter transforms teacher outputs into representations that better match the student model\u2019s capacity. They incorporate Bloom\u2019s Mastery Learning and Vygotsky\u2019s Zone of Proximal Development so that students must master prerequisites before moving on and are presented with tasks whose difficulty gradually increases within their \u201clearning zone.\u201d Distillation is performed iteratively rather than as a one-shot training on synthetic data.", "result": "Using LLaMA-3.1/3.2 and Qwen2.5 as students, IOA allows distilled models to preserve 94.7% of the teacher\u2019s performance on DollyEval while being more than 10\u00d7 smaller. On complex reasoning benchmarks, IOA outperforms strong distillation baselines by 19.2% on MATH and 22.3% on HumanEval.", "conclusion": "Pedagogically-informed, iterative knowledge distillation yields significantly more capable small models than conventional one-shot synthetic-data distillation, especially for complex reasoning. Structuring distillation with diagnosis of knowledge gaps, curriculum design, and capacity-aware adaptation is an effective way to approximate large-teacher performance using compact student models."}}
{"id": "2602.12173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12173", "abs": "https://arxiv.org/abs/2602.12173", "authors": ["Chengxi Zeng", "Yuxuan Jiang", "Ge Gao", "Shuai Wang", "Duolikun Danier", "Bin Zhu", "Stevan Rudinac", "David Bull", "Fan Zhang"], "title": "SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation", "comment": null, "summary": "Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.", "AI": {"tldr": "The paper identifies that text encoders in vision-language segmentation models like SAM3 are heavily over-provisioned for the simple prompts they process, and introduces a distilled lightweight text encoder (SAM3-LiteText) that drastically cuts parameters and memory while preserving segmentation quality.", "motivation": "Existing vision-language segmentation models use large, general-purpose text encoders meant for broad language tasks, but real-world segmentation prompts are short, structured, and semantically limited. This mismatch leads to unnecessary computational and memory costs. The authors want to quantify this inefficiency at scale and design a more efficient text encoder tailored to segmentation prompts without sacrificing performance.", "method": "The authors conduct a large-scale empirical analysis of 404,796 real prompts from multiple segmentation benchmarks, examining context-window utilization, vocabulary sparsity, and the intrinsic dimensionality of text embeddings. Based on observed redundancy and low intrinsic dimensionality, they design SAM3-LiteText, which replaces SAM3\u2019s original text encoder with a compact MobileCLIP-based student model trained via knowledge distillation from the original encoder.", "result": "Experiments on both image and video segmentation benchmarks show that SAM3-LiteText can shrink text encoder parameters by up to 88%, significantly cutting the static memory footprint. Despite this compression, the segmentation performance remains comparable to that of the original SAM3 with its full-sized text encoder.", "conclusion": "Text encoders in current vision-language segmentation systems are substantially over-capacity relative to the simplicity of segmentation prompts. By empirically characterizing this redundancy and exploiting it with a distilled MobileCLIP-based encoder, SAM3-LiteText achieves large reductions in parameter count and memory usage while preserving segmentation accuracy, demonstrating that lightweight, task-tailored text encoders are sufficient for prompt-based segmentation."}}
{"id": "2602.12192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12192", "abs": "https://arxiv.org/abs/2602.12192", "authors": ["Yuqing Li", "Jiangnan Li", "Mo Yu", "Guoxuan Ding", "Zheng Lin", "Weiping Wang", "Jie Zhou"], "title": "Query-focused and Memory-aware Reranker for Long Context Processing", "comment": "14 pages, 2 figures", "summary": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.", "AI": {"tldr": "A lightweight listwise reranking framework uses attention scores from selected retrieval heads in LLMs to estimate passage-query relevance, achieving state-of-the-art performance with small models and enabling flexible extensions.", "motivation": "Existing retrieval and reranking methods often rely on pointwise scoring or heavy listwise models, require Likert-scale labels, and underutilize the internal mechanisms of large language models, particularly retrieval heads in attention. There is a need for an efficient, flexible, and label-agnostic reranking approach that leverages LLM internals while performing well across domains and tasks like long-context dialogue understanding and memory usage.", "method": "The authors build on prior analyses of retrieval heads in LLMs and design a reranking framework that uses attention scores from a subset of those heads to estimate passage-query relevance. The model is trained in a listwise fashion, considering all candidates jointly, and outputs continuous relevance scores. It operates with relatively small models (around 4B parameters) and can be trained on generic retrieval datasets without explicit Likert-scale relevance labels. They also explore extensions such as enriching candidate passages with contextual information and using attention heads from middle layers for better efficiency.", "result": "Empirical evaluation shows that the proposed method outperforms existing state-of-the-art pointwise and listwise rerankers on multiple domains, including Wikipedia and long narrative datasets. It achieves a new state-of-the-art on the LoCoMo benchmark, which measures dialogue understanding and memory usage, and demonstrates performance gains when augmented with context and when using middle-layer attention heads for efficiency.", "conclusion": "The framework provides an effective, lightweight, and flexible listwise reranking solution by harnessing attention scores from selected retrieval heads in LLMs. It removes the need for Likert-scale supervision, works well with small models, achieves or surpasses state-of-the-art performance across several benchmarks, and can be extended with contextual augmentation and middle-layer attention for further accuracy and efficiency gains."}}
{"id": "2602.12249", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.12249", "abs": "https://arxiv.org/abs/2602.12249", "authors": ["Kaitlyn Zhou", "Martijn Bartelds", "Federico Bianchi", "James Zou"], "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most", "comment": null, "summary": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.", "AI": {"tldr": "The paper shows that current speech recognition systems, despite low benchmark error rates, perform poorly on short, high\u2011stakes utterances like U.S. street names, especially for non\u2011English primary speakers, and that targeted synthetic TTS data can substantially improve reliability.", "motivation": "Standard ASR benchmarks suggest near-human performance, but real-world deployments reveal brittle behavior on short, high-stakes commands (e.g., navigation), particularly for diverse accents and language backgrounds. Street names are critical for routing and safety, and misrecognitions may systematically disadvantage non-English primary speakers. The authors aim to quantify this hidden failure mode and its geographic and fairness impacts, and to find a practical mitigation strategy without needing massive new real-world datasets.", "method": "The authors collect recordings of U.S. street names spoken by linguistically diverse U.S. participants and evaluate 15 commercial and foundation ASR models (from OpenAI, Deepgram, Google, Microsoft). They compute transcription error rates and model how these errors propagate to downstream routing distance errors, stratified by speaker language background. To mitigate errors, they design a synthetic data generation pipeline using open-source text-to-speech systems to produce diverse pronunciations of named entities (street names). They fine-tune base ASR models on fewer than 1,000 such synthetic examples and re-evaluate performance, focusing on non-English primary speakers.", "result": "Across 15 models, transcription error rates on the street-name task average 44%, far worse than standard benchmark results. All groups experience systematic errors, but when these errors are mapped to routing outcomes, the induced routing distance errors are about twice as large for non-English primary speakers compared to English primary speakers, indicating a disproportionate harm. Fine-tuning with under 1,000 synthetic, TTS-generated pronunciations of street names yields nearly a 60% relative improvement in street name transcription accuracy for non-English primary speakers compared to the original base models.", "conclusion": "There is a substantial and previously under-acknowledged gap between benchmark ASR performance and reliability in real, high-stakes, short-utterance scenarios such as street name transcription. This gap produces unequal downstream harms, with non-English primary speakers suffering significantly worse routing errors. However, relatively small amounts of carefully targeted, synthetic pronunciation data from open-source TTS can dramatically improve performance, offering a simple, scalable path to fairer and more reliable speech systems. The work argues for evaluating ASR with task- and population-relevant benchmarks and for using synthetic data as an efficient mitigation tool."}}
{"id": "2602.12196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12196", "abs": "https://arxiv.org/abs/2602.12196", "authors": ["Mohamed Huti", "Alasdair Mackintosh", "Amy Waldock", "Dominic Andrews", "Maxime Leli\u00e8vre", "Moritz Boos", "Tobias Murray", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education", "comment": null, "summary": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.", "AI": {"tldr": "Introduces VRB, a benchmark to test MLLMs on real primary-school visual reasoning problems, revealing strong performance on simple static tasks but clear failures on more complex spatial operations.", "motivation": "While AI models excel at textual reasoning, they struggle with visual, spatial, and relational reasoning, which are crucial in early-grade mathematics and primary education. Existing benchmarks often use synthetic or text-heavy tasks that do not reflect how children actually encounter visual problems in classrooms. There is a need for an authentic, education-focused benchmark to understand whether MLLMs can safely and effectively support visual reasoning in real classroom settings.", "method": "The authors built the Visual Reasoning Benchmark (VRB) from 701 real exam questions taken from primary school tests in Zambia and India. These questions cover diverse visual reasoning skills such as analogy, pattern completion, and spatial matching. The images are kept unedited with minimal text to closely mimic actual classroom materials. They then evaluated current Multimodal Large Language Models on this dataset, analyzing performance patterns across different task types and levels of spatial complexity.", "result": "Models perform relatively well on static visual tasks like counting objects or performing simple scaling, but their accuracy degrades sharply on tasks requiring dynamic spatial transformations such as folding, reflection, and rotation. This results in an uneven or \u201cjagged\u201d capability profile, where certain narrow skills appear strong but broader spatial reasoning remains weak.", "conclusion": "Current MLLMs are not yet reliable for visual reasoning tasks typical of primary-school math, especially those involving complex spatial transformations. These limitations can lead to serious educational risks, including wrong grading and misleading guidance that may entrench student misconceptions. Therefore, domain-specific benchmarks like VRB are critical to chart the practical limits of multimodal systems and guide their responsible deployment in classrooms."}}
{"id": "2602.12259", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12259", "abs": "https://arxiv.org/abs/2602.12259", "authors": ["Jianke Yang", "Ohm Venkatachalam", "Mohammad Kianezhad", "Sharvaree Vadgama", "Rose Yu"], "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery", "comment": null, "summary": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.", "AI": {"tldr": "The paper presents KeplerAgent, an LLM-driven agentic framework that improves symbolic equation discovery by mimicking scientists\u2019 multi-step reasoning process and integrating physics-based tools with symbolic regression engines.", "motivation": "Traditional symbolic equation discovery, including recent LLM-based approaches, generally attempts to map data directly to equations. This overlooks the way human scientists reason: they first infer intermediate physical properties (like symmetries and invariants) and then use these as priors to constrain the search space of possible equations. Existing LLM systems largely ignore this structured, multi-step reasoning process, leading to lower accuracy and poor robustness, especially in noisy settings. The paper is motivated by the desire to bridge this gap and leverage LLMs to orchestrate the same kind of structured workflow that human scientists use when formulating physical laws.", "method": "The authors propose KeplerAgent, an agentic framework built around an LLM that coordinates a set of physics-based analytical tools and symbolic regression engines. The LLM first invokes tools to infer intermediate structures from data\u2014such as symmetries, conserved quantities, or other physical properties. It then uses these inferred properties to configure symbolic regression systems like PySINDy and PySR, customizing their function libraries and structural constraints. By iteratively refining these priors and constraints based on intermediate results, KeplerAgent conducts a guided search over candidate equations rather than unconstrained direct prediction.", "result": "On a set of physical equation discovery benchmarks, KeplerAgent outperforms both pure LLM baselines that directly guess equations and traditional symbolic regression methods that do not leverage agentic coordination. It achieves notably higher symbolic accuracy\u2014recovering the exact ground-truth equations more often\u2014and demonstrates greater robustness in the presence of noisy observational data, maintaining performance where other methods degrade.", "conclusion": "KeplerAgent shows that explicitly modeling a scientist-like multi-step reasoning pipeline\u2014inferring physical structure first, then constraining symbolic regression\u2014is more effective than direct equation prediction from data. By using an LLM as an orchestrator of specialized tools rather than as a standalone predictor, the framework yields more accurate and robust symbolic equation discovery. This suggests a promising direction for combining LLMs with domain-specific tools to automate aspects of scientific discovery."}}
{"id": "2602.12203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12203", "abs": "https://arxiv.org/abs/2602.12203", "authors": ["Mathieu Sibue", "Andres Mu\u00f1oz Garza", "Samuel Mensah", "Pranav Shetty", "Zhiqiang Ma", "Xiaomo Liu", "Manuela Veloso"], "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images", "comment": "EACL 2026, main conference", "summary": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.", "AI": {"tldr": "Introduces ExStrucTiny, a benchmark for fine-grained, structured information extraction from heterogeneous document images, unifying KEE, RE, and VQA and evaluating VLMs on schema-adaptive extraction.", "motivation": "Generalist VLMs do well on existing document benchmarks, but current datasets for document information extraction focus on narrow ontologies, simple questions, or uniform document types, and do not test holistic, schema-flexible structured extraction. There is a need for a benchmark that better reflects realistic enterprise document processing tasks.", "method": "The authors propose ExStrucTiny, a benchmark dataset and task suite for structured information extraction from document images. They design a novel construction pipeline mixing manually curated and synthetically generated samples that are all human-validated. The benchmark unifies aspects of key entity extraction, relation extraction, and visual question answering, and covers a wide variety of document types and extraction scenarios. They then evaluate both open and closed Vision Language Models on the benchmark and analyze their performance.", "result": "ExStrucTiny is created as a new benchmark dataset that incorporates diverse document types and complex, flexible extraction schemas. Experimental evaluation of multiple VLMs on this dataset reveals that even strong generalist models struggle with several aspects of the task, particularly schema adaptation, handling under-specified queries, and accurately localizing answers in the documents.", "conclusion": "The ExStrucTiny benchmark exposes important limitations of current generalist VLMs in performing realistic, structured information extraction from documents. By unifying KEE, RE, and VQA and emphasizing schema flexibility and diversity of document types, it provides a challenging and representative testbed that can drive the development of better models and methods for structured document understanding."}}
{"id": "2602.12268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12268", "abs": "https://arxiv.org/abs/2602.12268", "authors": ["Zhen Zhang", "Kaiqiang Song", "Xun Wang", "Yebowen Hu", "Weixiang Yan", "Chenyang Zhao", "Henry Peng Zou", "Haoyun Deng", "Sathish Reddy Indurthi", "Shujian Liu", "Simin Ma", "Xiaoyang Wang", "Xin Eric Wang", "Song Wang"], "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use", "comment": null, "summary": "AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.", "AI": {"tldr": "The paper introduces CM2, an RL framework that uses checklist-based rewards and LLM-simulated tools to train multi-turn, tool-using AI agents without verifiable outcome rewards, achieving strong benchmark gains over supervised baselines.", "motivation": "Reinforcement learning for real-world, multi-turn tool-using AI agents is hard because realistic objectives are open-ended and lack clear, verifiable rewards; RL for complex multi-step tool use is underexplored; and creating/maintaining executable tool environments is expensive, limiting scale. The authors want a scalable way to optimize agent behavior in these settings without relying on strict outcome-based rewards or heavy engineering.", "method": "CM2 replaces traditional verifiable outcome rewards with checklist rewards. For each turn, the desired behavior is decomposed into fine-grained binary criteria, each with explicit evidence grounding and structured metadata. This converts open-ended judgment into more stable, classification-like decisions. The framework uses sparse reward assignment (only some turns get rewards) but dense evaluation criteria (many fine-grained checks per rewarded turn). Training occurs in a scalable tool environment simulated by an LLM instead of real executable tools, reducing engineering overhead for large tool sets.", "result": "Using an 8B base model and an 8k-example RL dataset, CM2-trained agents outperform an SFT-only counterpart by 8 points on tau^-Bench, 10 points on BFCL-V4, and 12 points on ToolSandbox. Performance matches or surpasses similarly sized open-source baselines, including the model used for judging, showing that CM2 can effectively optimize multi-turn tool-using agents.", "conclusion": "CM2 offers a scalable, effective recipe for training multi-turn, multi-step tool-using AI agents in settings where verifiable rewards are unavailable. By leveraging checklist-based rewards with explicit evidence and LLM-simulated tool environments, the framework achieves consistent improvements over supervised fine-tuning and competes with strong open-source baselines."}}
{"id": "2602.12235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12235", "abs": "https://arxiv.org/abs/2602.12235", "authors": ["Julia Belikova", "Danila Rozhevskii", "Dennis Svirin", "Konstantin Polev", "Alexander Panchenko"], "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation", "comment": "Accepted to EACL 2026 Student Research Workshop. 14 pages, 6 tables, 1 figure", "summary": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \\emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.", "AI": {"tldr": "They study when and how soft-compressed token representations in long-context LLM systems lose enough information that queries can no longer be accurately answered, and propose methods to detect this situation (\u201ctoken overflow\u201d).", "motivation": "Long-context processing in LLMs is computationally expensive, particularly in resource-limited settings. Soft compression techniques that replace long sequences with a small set of learned compressed tokens can reduce cost, but there is little understanding of how far one can compress before destroying information needed for downstream tasks. The authors want a principled way to define, characterize, and automatically detect when compression has gone too far, harming performance.", "method": "They introduce the concept of \u201ctoken overflow,\u201d the regime where compressed representations lack sufficient information to answer a query. Working in an xRAG (soft-compression retrieval-augmented generation) setting, they analyze saturation statistics of representations to develop query-agnostic diagnostics for identifying compressed tokens. They then train lightweight probing classifiers that take both query and context xRAG representations as input to detect when overflow occurs. Performance is evaluated on QA benchmarks such as HotpotQA, SQuADv2, and TriviaQA, using metrics like AUC-ROC.", "result": "Query-agnostic saturation statistics can reliably distinguish compressed tokens from uncompressed ones, making them useful for identifying which tokens have been compressed, but these statistics are not very effective at detecting when overflow happens. In contrast, lightweight probes that incorporate both query and context representations detect overflow substantially better, achieving an average AUC-ROC of 0.72 across HotpotQA, SQuADv2, and TriviaQA.", "conclusion": "Token overflow is a meaningful and measurable failure mode of soft compression in long-context LLMs. Simple, query-independent diagnostics are insufficient to robustly detect it, but adding query-aware probing significantly improves detection. These query-aware detectors can be used as a low-cost, pre-LLM gating mechanism in xRAG systems to mitigate errors caused by overly aggressive compression, helping balance efficiency and accuracy in long-context processing."}}
{"id": "2602.12276", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12276", "abs": "https://arxiv.org/abs/2602.12276", "authors": ["Nicholas Lee", "Lutfi Eren Erdogan", "Chris Joseph John", "Surya Krishnapillai", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "title": "Agentic Test-Time Scaling for WebAgents", "comment": null, "summary": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.", "AI": {"tldr": "The paper proposes CATTS, a confidence-aware test-time scaling method that dynamically allocates extra inference-time compute only on contentious decisions for multi-step web agents, improving both performance and efficiency over uniform scaling.", "motivation": "Existing test-time scaling methods (like sampling more or using ensembles) help model accuracy and reliability, but their behavior on agentic, multi-step tasks is poorly understood. In long-horizon tasks, small per-step errors compound, and simple strategies that uniformly increase sampling at each step quickly hit diminishing returns and are expensive. The authors are motivated to find a more principled, efficient way to allocate extra compute at test time specifically for multi-step agents operating on the web, so that effort is focused where it matters most.", "method": "(1) Empirically study test-time scaling for web agents in long-horizon environments, observing that uniformly increasing per-step compute saturates quickly. (2) Explore aggregation strategies over multiple sampled trajectories, including an LLM-based Arbiter that can beat naive majority voting but sometimes inappropriately overrules high-consensus choices. (3) Analyze the agent\u2019s own vote distributions and show that simple uncertainty statistics (entropy, top-1/top-2 margin) correlate with success. (4) Use these uncertainty signals to design Confidence-Aware Test-Time Scaling (CATTS), which allocates additional compute only when the vote distribution indicates high uncertainty, keeping decisions with high consensus cheap. CATTS is then implemented and evaluated on web agent benchmarks such as WebArena-Lite and GoBrowse.", "result": "Experiments on WebArena-Lite and GoBrowse show that CATTS improves performance (success rate) over a baseline ReAct-style agent by up to 9.1% while using as much as 2.3x fewer tokens compared to strategies that uniformly scale per-step compute. The LLM-based Arbiter can outperform naive voting but is less interpretable and may override high-consensus votes; in contrast, the vote-derived uncertainty metrics are both predictive of downstream success and simple to operationalize.", "conclusion": "Dynamic, confidence-aware allocation of test-time compute is more effective and efficient than uniform scaling for multi-step web agents. By leveraging uncertainty signals from the agent\u2019s own vote distribution, CATTS focuses extra inference on difficult, contentious decisions, leading to better performance and token efficiency. This suggests that future work on agentic systems should prioritize uncertainty-aware test-time scaling rather than naive, uniform increases in sampling or compute."}}
{"id": "2602.12241", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.12241", "abs": "https://arxiv.org/abs/2602.12241", "authors": ["Manjunath Kudlur", "Evan King", "James Wang", "Pete Warden"], "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications", "comment": "7 pages, 5 figures", "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.", "AI": {"tldr": "Moonshine v2 is a streaming ASR model that replaces full global attention with an efficient sliding-window attention encoder, achieving low latency (bounded TTFT) and state-of-the-art accuracy on edge devices at much smaller model sizes.", "motivation": "Latency-critical speech applications on resource-constrained edge devices need both very low time-to-first-token (TTFT) and high transcription accuracy. Standard full-attention Transformer encoders provide strong accuracy because every frame can attend to all others, leveraging long-range context to resolve ambiguous acoustics. However, their quadratic complexity and encode-the-whole-utterance behavior cause TTFT to grow with utterance length, which is unsuitable for streaming ASR on devices with limited compute.", "method": "The paper proposes Moonshine v2, an ergodic streaming encoder architecture for ASR that replaces full global self-attention with sliding-window self-attention. This local attention restricts each frame to attend within a moving window, ensuring bounded computation and latency while still capturing strong local context. The encoder is integrated into an ASR system designed for streaming inference on-device, with careful design choices to maintain accuracy despite the limited attention span.", "result": "Moonshine v2 achieves state-of-the-art word error rates on standard ASR benchmarks, matching the accuracy of full-attention models that are up to six times larger, while also running significantly faster. It delivers bounded, low TTFT suitable for streaming use cases, especially on edge hardware.", "conclusion": "The work shows that with a carefully designed sliding-window self-attention encoder, local attention can match the accuracy of full global attention for ASR, at much lower computational and latency cost. This makes high-quality, interactive speech recognition feasible on resource-constrained edge devices and suggests that global attention is not strictly necessary for competitive streaming ASR performance."}}
{"id": "2602.12251", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.12251", "abs": "https://arxiv.org/abs/2602.12251", "authors": ["Ralph Kr\u00fcger"], "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication", "comment": "10 pages, 1 figure, EAMT 2026, TAITT Workshop", "summary": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.", "AI": {"tldr": "Curriculum for language/translation professionals to understand core AI concepts (embeddings, neural nets, tokenization, transformers), shown to be effective but needs more didactic scaffolding.", "motivation": "Professionals in language and translation are increasingly working in AI-driven environments but often lack technical AI literacy and computational thinking, which can limit their agency, understanding, and resilience when using AI tools.", "method": "Design of a core technical curriculum covering four key AI concepts (vector embeddings, neural network foundations, tokenization, transformers), taught in an AI-focused MA course in translation/multilingual communication. Evaluation via implementation and participant feedback regarding understanding and learning experience.", "result": "Students could grasp core AI concepts and reported improved understanding and awareness of AI mechanisms in their field, indicating that the curriculum is didactically effective in principle. However, feedback showed that learners benefit from additional didactic support and scaffolding from lecturers to fully realize the curriculum\u2019s potential.", "conclusion": "A focused technical AI curriculum can successfully build domain-specific AI literacy and digital resilience among language and translation stakeholders, but it should not stand alone; it needs to be embedded in broader pedagogical structures and guided by instructor support for optimal learning outcomes."}}
{"id": "2602.12262", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12262", "abs": "https://arxiv.org/abs/2602.12262", "authors": ["Tunyu Zhang", "Xinxi Zhang", "Ligong Han", "Haizhou Shi", "Xiaoxiao He", "Zhuowei Li", "Hao Wang", "Kai Xu", "Akash Srivastava", "Hao Wang", "Vladimir Pavlovic", "Dimitris N. Metaxas"], "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization", "comment": null, "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.", "AI": {"tldr": "The paper improves the efficiency of diffusion large language models (DLLMs) for few-step decoding by distilling the model\u2019s own generation trajectories using a mode-seeking reverse-KL objective, significantly narrowing the quality gap with full-step decoding.", "motivation": "DLLMs can in principle generate text quickly by decoding many tokens in parallel, but they require many refinement steps to maintain high quality. When the number of steps is aggressively reduced to achieve speed, text quality degrades substantially. There is a need for training strategies that preserve generation quality under very small step budgets to make DLLMs practically useful for fast inference.", "method": "The authors propose a trajectory self-distillation framework. They use the model\u2019s own multi-step generative trajectories as a teacher, and train a few-step student decoder to imitate these trajectories. Crucially, they adopt Direct Discriminative Optimization (DDO), a reverse-KL-based objective that performs mode-seeking distillation, pushing the student to focus on high-probability regions of the teacher\u2019s distribution. This improves the robustness and quality of few-step decoding without changing the core DLLM architecture.", "result": "On multiple benchmarks, the proposed method consistently outperforms strong few-step baselines and standard training when the number of refinement steps is heavily constrained. While decoding with many steps still yields the best overall performance, the new training approach noticeably shrinks the quality gap between few-step and full-step decoding, demonstrating more practical few-step DLLMs.", "conclusion": "Trajectory self-distillation with a reverse-KL (DDO) objective effectively enhances the performance of DLLMs under tight step budgets. By distilling the model\u2019s own generative trajectories and focusing the student on the teacher\u2019s high-probability modes, the framework enables higher-quality few-step decoding and moves DLLMs closer to being practically efficient fast generators. The work also provides a public implementation for further research and application."}}
{"id": "2602.12275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12275", "abs": "https://arxiv.org/abs/2602.12275", "authors": ["Tianzhu Ye", "Li Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "title": "On-Policy Context Distillation for Language Models", "comment": null, "summary": "Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.", "AI": {"tldr": "The paper introduces On-Policy Context Distillation (OPCD), a method that trains a student language model on its own generated trajectories while aligning it with a context-conditioned teacher, to internalize in-context knowledge into model parameters.", "motivation": "Existing context distillation and distillation methods do not fully leverage on-policy trajectories of the student model, and it is desirable to convert in-context or experiential knowledge (e.g., solution traces or system prompts) into parametric knowledge while preserving generalization and out-of-distribution performance.", "method": "OPCD trains a student model using its own generated trajectories (on-policy data) and minimizes the reverse KL divergence between the student and a teacher model that is conditioned on useful context (such as previous solution traces or optimized prompts). This connects on-policy distillation with context distillation so that the student internalizes behaviors and knowledge expressed in context into its parameters.", "result": "Across multiple domains\u2014mathematical reasoning, text-based games, and domain-specific tasks\u2014OPCD outperforms baseline approaches in task accuracy and also better preserves out-of-distribution capabilities. It also works for cross-size distillation, allowing smaller students to absorb experiential knowledge from larger teachers.", "conclusion": "OPCD is an effective and general framework for turning in-context or experiential knowledge (e.g., historical trajectories and system prompts) into parametric knowledge in language models, yielding better task performance, robustness, and enabling efficient distillation from larger to smaller models."}}
