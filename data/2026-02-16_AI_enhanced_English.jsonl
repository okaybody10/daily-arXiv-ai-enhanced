{"id": "2602.12284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12284", "abs": "https://arxiv.org/abs/2602.12284", "authors": ["Han Jinzhen", "Kim Jisung", "Yang Jong Soo", "Yun Hong Sik"], "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification", "comment": null, "summary": "Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.", "AI": {"tldr": "Lightweight framework to classify disaster-related tweets using parameter-efficient fine-tuning on Llama 3.1 8B, achieving strong accuracy with low compute and showing RAG can hurt performance.", "motivation": "Disaster response needs fast, accurate classification of humanitarian information from social media, but large models are hard to deploy in resource-constrained emergency settings. The paper aims to make LLM-based disaster tweet classification practical and cost-effective.", "method": "They build a unified corpus by integrating and normalizing the HumAID dataset into a dual-task benchmark (humanitarian category and event type). On Llama 3.1 8B, they systematically compare prompting, LoRA fine-tuning, and RAG. They apply LoRA and QLoRA for parameter-efficient tuning and test various retrieval-augmented generation strategies using labeled examples, analyzing their impact on performance and label noise.", "result": "LoRA fine-tuning reaches 79.62% accuracy on humanitarian classification, improving by 37.79% over zero-shot while updating only ~2% of parameters. QLoRA retains 99.4% of LoRA\u2019s performance but halves memory usage. RAG, instead of helping, degrades the performance of fine-tuned models due to label noise introduced by retrieved examples.", "conclusion": "Parameter-efficient fine-tuning (LoRA/QLoRA) on a unified HumAID-based benchmark provides an accurate, low-resource solution for disaster tweet classification, outperforming prompting and making deployment feasible in constrained settings. RAG must be used with caution because na\u00efvely retrieved labeled examples can introduce label noise and harm performance. The paper delivers a practical, reproducible pipeline for building crisis intelligence systems under limited computation."}}
{"id": "2602.12285", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12285", "abs": "https://arxiv.org/abs/2602.12285", "authors": ["Linbo Cao", "Lihao Sun", "Yang Yue"], "title": "From Biased Chatbots to Biased Agents: Examining Role Assignment Effects on LLM Agent Robustness", "comment": "Accepted to the AAAI 2026 TrustAgent Workshop. 6 pages, 4 figures", "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents capable of actions with real-world impacts beyond text generation. While persona-induced biases in text generation are well documented, their effects on agent task performance remain largely unexplored, even though such effects pose more direct operational risks. In this work, we present the first systematic case study showing that demographic-based persona assignments can alter LLM agents' behavior and degrade performance across diverse domains. Evaluating widely deployed models on agentic benchmarks spanning strategic reasoning, planning, and technical operations, we uncover substantial performance variations - up to 26.2% degradation, driven by task-irrelevant persona cues. These shifts appear across task types and model architectures, indicating that persona conditioning and simple prompt injections can distort an agent's decision-making reliability. Our findings reveal an overlooked vulnerability in current LLM agentic systems: persona assignments can introduce implicit biases and increase behavioral volatility, raising concerns for the safe and robust deployment of LLM agents.", "AI": {"tldr": "The paper shows that assigning demographic personas to LLM agents can significantly and systematically degrade their performance on real-world tasks, revealing a new robustness and safety vulnerability.", "motivation": "As LLMs transition from pure text generators to autonomous agents that plan and act in the real world, they are often given personas (e.g., demographic or role-based profiles) to shape style or alignment. While prior work has studied how personas bias language output, there is little understanding of how such personas affect actual task performance and decision-making reliability in agentic settings. This gap is critical because persona-induced behaviors can translate directly into operational risks when agents perform strategic, planning, or technical tasks.", "method": "The authors conduct a systematic case study using widely deployed LLMs configured as autonomous agents. They assign demographic-based personas through prompts and evaluate performance on a suite of agent benchmarks spanning strategic reasoning, planning, and technical operations. By comparing performance across persona conditions and baseline (no persona) settings, they quantify how task-irrelevant persona cues alter behavior. They also examine whether these effects generalize across different model architectures and task types, and relate them to prompt injection\u2013like vulnerabilities.", "result": "The experiments reveal substantial and consistent performance shifts driven purely by demographic persona assignments, with degradations of up to 26.2% on some benchmarks. These effects appear across diverse task categories and multiple model architectures, indicating that persona conditioning is a robust, model-agnostic source of behavioral variance. The results show that even simple prompt-based persona instructions, which should be task-irrelevant, can significantly distort agents\u2019 decision-making and reliability.", "conclusion": "The paper concludes that persona assignments, especially demographic-based ones, are a previously underappreciated vulnerability in LLM agent systems. They introduce implicit biases, increase behavioral volatility, and can meaningfully degrade task performance, raising serious concerns for safety and robustness. The authors argue that developers and practitioners must treat persona design and conditioning as a core part of safety and reliability engineering for LLM agents and suggest that future work should develop methods to detect, constrain, or mitigate persona-induced performance distortions."}}
{"id": "2602.12287", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.12287", "abs": "https://arxiv.org/abs/2602.12287", "authors": ["Junjie An", "Jingguang Tian", "Tianyi Wang", "Yu Gao", "Xiaofeng Mou", "Yi Xu"], "title": "Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction", "comment": null, "summary": "End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\\% and 34.42\\%, respectively, compared to a strong baseline.", "AI": {"tldr": "They propose a retrieval-augmented, reasoning-aware LLM framework to correct named entity errors in end-to-end ASR transcripts, significantly reducing named-entity character error rate on two benchmarks.", "motivation": "End-to-end ASR systems often mishandle domain-specific phrases and named entities. Such errors can severely damage downstream application performance (e.g., information extraction, assistants) and are hard to fix because they are acoustically similar to other words and often out-of-vocabulary. Recent LLM-based post-correction methods exist but do not fully leverage LLMs\u2019 reasoning abilities for precise named entity correction. The paper aims to better exploit LLM reasoning while integrating retrieval to accurately recover intended named entities from noisy ASR output.", "method": "They introduce a retrieval-augmented generation pipeline tailored for named entity correction in ASR.\n1) First, a Rephrasing Language Model (RLM) identifies and rephrases potential named entities in ASR transcripts. Using these rephrasings, the system retrieves candidate entities from an external lexicon or database via a phonetic-level edit-distance metric, ensuring candidates are acoustically plausible.\n2) Second, they design A-STAR (Adaptive Self-Taught Reasoning Model), a chain-of-thought style LLM module that reasons about the context and the retrieved candidates. A-STAR adaptively changes the depth/length of its reasoning depending on the difficulty (ambiguity) of each correction case, effectively deciding when shallow vs. deeper reasoning is needed. The final corrected transcript is generated by selecting or rewriting entities based on this reasoning process.\nThey evaluate on AISHELL-1 and a Homophone dataset, comparing against a strong baseline LLM-based correction method.", "result": "On AISHELL-1 and Homophone benchmarks, their method substantially lowers the character error rate (CER) on named entities. They report relative reductions of 17.96% CER on AISHELL-1 and 34.42% on the Homophone dataset versus a strong baseline, indicating that combining phonetic retrieval with adaptive chain-of-thought reasoning yields more accurate entity corrections, especially in homophone-heavy conditions.", "conclusion": "Retrieval-augmented, reasoning-aware LLM post-processing can effectively correct named entity errors in end-to-end ASR, outperforming existing LLM-based correction approaches. The RLM plus phonetic retrieval component supplies plausible candidate entities, while A-STAR\u2019s adaptive chain-of-thought reasoning selects the correct entity with context-sensitive depth. This demonstrates that explicitly leveraging and modulating LLM reasoning, together with speech-informed retrieval, is a powerful strategy for robust named entity handling in ASR outputs and can mitigate catastrophic downstream failures caused by entity misrecognitions."}}
{"id": "2602.12302", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12302", "abs": "https://arxiv.org/abs/2602.12302", "authors": ["Neemias da Silva", "J\u00falio C. W. Scholz", "John Harrison", "Marina Borges", "Paulo \u00c1vila", "Frances A Santos", "Myriam Delgado", "Rodrigo Minetto", "Thiago H Silva"], "title": "Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria \u00e0 Pr\u00e1tica", "comment": "in Portuguese language. Accepted book chapter - Webmedia 2025", "summary": "Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.", "AI": {"tldr": "Overview chapter on Multimodal Large Language Models (MLLMs), covering fundamentals, key models, practical tooling with LangChain/LangGraph, and challenges/future trends.", "motivation": "LLMs are powerful in text but lack native perception in other modalities like images and audio. Recent advances show that integrating language modeling with multimodal perception enables more capable systems. There is a need for an accessible, structured overview that explains concepts, surveys emblematic MLLMs, and shows practitioners how to actually build multimodal applications in practice.", "method": "This is a tutorial/overview chapter rather than an empirical research paper. It (1) introduces core concepts and architectures of MLLMs and reviews representative models; (2) presents practical techniques for data preprocessing and prompt engineering in multimodal settings; (3) demonstrates how to build multimodal pipelines using LangChain and LangGraph; and (4) synthesizes open challenges and promising directions. Supplementary material, including code examples, is provided in a public GitHub repository for hands-on exploration.", "result": "The chapter systematizes knowledge about MLLMs, clarifies their foundations and design patterns, and provides concrete recipes and tools for implementing multimodal applications. It also curates online supplementary resources and code to help readers practice and extend the discussed techniques.", "conclusion": "MLLMs are a central evolution of AI, extending LLMs with visual and auditory perception. Effective use of MLLMs requires understanding both their conceptual underpinnings and the practical engineering steps, such as preprocessing, prompting, and orchestration with frameworks like LangChain and LangGraph. While these models are promising, there remain significant challenges and open research problems, but the field shows strong trends towards more powerful, integrated multimodal systems."}}
{"id": "2602.12316", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12316", "abs": "https://arxiv.org/abs/2602.12316", "authors": ["Pepijn Cobben", "Xuanqiang Angelo Huang", "Thao Amelia Pham", "Isabel Dahlgren", "Terry Jingchen Zhang", "Zhijing Jin"], "title": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory", "comment": null, "summary": "Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.", "AI": {"tldr": "GT-HarmBench is a benchmark to evaluate how well frontier AI models behave in high-stakes multi-agent, game-theoretic situations, revealing large reliability gaps.", "motivation": "Most AI safety benchmarks focus on single-agent settings and overlook multi-agent risks like coordination failures and conflict. As frontier models are increasingly deployed in multi-agent, high-stakes contexts, we need tools to systematically test their behavior in realistic strategic scenarios.", "method": "The authors build GT-HarmBench, a collection of 2,009 scenarios mapped to classic game-theoretic structures (Prisoner\u2019s Dilemma, Stag Hunt, Chicken, etc.), derived from realistic AI risk situations in the MIT AI Risk Repository. They evaluate 15 frontier models acting as agents in these games, systematically varying game-theoretic prompt framing and ordering, and then analyze both quantitative performance (rate of socially beneficial choices) and qualitative reasoning patterns. They also test game-theoretic interventions in the prompts to see if behavior improves.", "result": "Across the benchmark, the 15 evaluated models select socially beneficial actions only about 62% of the time, often leading to harmful or suboptimal outcomes. Their behavior is sensitive to how the scenarios are framed and ordered. Applying game-theoretic prompt interventions yields up to an 18% increase in socially beneficial decisions, but significant reliability gaps remain.", "conclusion": "Frontier AI systems are still unreliable in high-stakes, multi-agent strategic settings, frequently failing to coordinate or avoid harmful outcomes. GT-HarmBench offers a standardized, realistic testbed for probing and improving multi-agent alignment, and early experiments show that explicit game-theoretic prompting can help but does not fully solve these issues."}}
{"id": "2602.12356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12356", "abs": "https://arxiv.org/abs/2602.12356", "authors": ["Philip Waggoner"], "title": "A Theoretical Framework for Adaptive Utility-Weighted Benchmarking", "comment": "10 page, no figures, 40 equations", "summary": "Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.", "AI": {"tldr": "The paper proposes a new, more holistic way to evaluate AI systems that goes beyond traditional static benchmarks and leaderboards.", "motivation": "Traditional benchmarking focuses narrowly on fixed tasks, metrics, and leaderboards, which is insufficient as AI systems are deployed in high\u2011impact, real\u2011world sociotechnical contexts. There is a need to reflect diverse stakeholder priorities, capture human tradeoffs, and design evaluations that are more accountable and aligned with human values.", "method": "The authors introduce a theoretical framework that models a benchmark as a multi-layer, adaptive network connecting evaluation metrics, model components, and stakeholder groups via weighted edges. They use conjoint analysis to derive quantitative utilities that represent stakeholder preferences, and define a human-in-the-loop update rule that adjusts the network weights over time. This formalism lets human tradeoffs be embedded into the benchmark structure while maintaining stability and interpretability.", "result": "The framework mathematically generalizes classical leaderboards as a special, static case within the broader adaptive network formulation. It yields new tools for examining the structural properties of benchmarks, such as how metrics, model parts, and stakeholder groups interact and trade off, and how these relationships can evolve in a principled way as human feedback is incorporated.", "conclusion": "By reconceptualizing benchmarks as adaptive, stakeholder-aware networks, the paper lays groundwork for evaluation protocols that are more context-sensitive, accountable, and aligned with human values than traditional benchmarks. This can support more robust, human-aligned analysis and comparison of AI systems deployed in complex real-world environments."}}
{"id": "2602.12424", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12424", "abs": "https://arxiv.org/abs/2602.12424", "authors": ["Ziqian Zhang", "Xingjian Hu", "Yue Huang", "Kai Zhang", "Ruoxi Chen", "Yixin Liu", "Qingsong Wen", "Kaidi Xu", "Xiangliang Zhang", "Neil Zhenqiang Gong", "Lichao Sun"], "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty", "comment": "32 pages, 9 figures. Accepted by ICLR 2026", "summary": "Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.", "AI": {"tldr": "RankLLM is a framework that jointly scores question difficulty and model competency to provide fine-grained, difficulty-aware evaluation of large language models.", "motivation": "Existing LLM benchmarks treat questions mostly uniformly and do not explicitly model question difficulty, which makes it hard to distinguish between models that differ mainly on harder items and to understand what capabilities they truly possess.", "method": "They introduce RankLLM, a bidirectional scoring mechanism where models gain competency scores by correctly answering questions, and questions gain higher difficulty scores when they are answered incorrectly by competent models. This joint optimization propagates scores between models and questions, and is evaluated on 30 LLMs over 35,550 questions from multiple domains, compared against baselines like Item Response Theory (IRT).", "result": "RankLLM obtains about 90% agreement with human difficulty and capability judgments, surpasses strong baselines such as IRT, and shows desirable properties including stability, fast convergence, and computational efficiency in large-scale evaluations.", "conclusion": "RankLLM provides a practical and accurate framework for difficulty-aware LLM benchmarking, enabling more nuanced comparisons of model capabilities by explicitly modeling both question difficulty and model competency."}}
{"id": "2602.12389", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12389", "abs": "https://arxiv.org/abs/2602.12389", "authors": ["Siyuan Li", "Yunjia Wu", "Yiyong Xiao", "Pingyang Huang", "Peize Li", "Ruitong Liu", "Yan Wen", "Te Sun", "Fangyi Pei"], "title": "Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting", "comment": null, "summary": "Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots", "AI": {"tldr": "They propose a framework to give temporal knowledge graph models persistent entity states so they can better forecast long-term future facts.", "motivation": "Existing temporal knowledge graph forecasting methods are stateless: they recompute entity representations from a short window at each timestamp, which causes loss of long-term information and weak modeling of long-range temporal dependencies. The authors want a way to maintain and evolve entity information over time to improve long-horizon forecasting.", "method": "They introduce Entity State Tuning (EST), an encoder-agnostic framework that adds a global, persistent entity state memory on top of any TKG forecasting backbone. EST maintains a global state buffer for all entities and uses a closed-loop design: (1) a topology-aware state perceiver injects entity state priors into the structural encoding of each snapshot; (2) a unified temporal context module, with a pluggable sequential backbone (e.g., RNN/Transformer), aggregates these state-enhanced events over time; (3) a dual-track evolution mechanism writes the updated contextual information back into the global state memory, with separate fast and slow update components to balance adaptability (plasticity) and stability of the entity states.", "result": "On multiple temporal knowledge graph forecasting benchmarks, adding EST on top of various backbone models consistently improves their predictive performance and achieves state-of-the-art results relative to existing approaches. This demonstrates both robustness across architectures and superiority in long-horizon forecasting metrics.", "conclusion": "Persistently maintained and continuously updated entity states are crucial for effective long-term temporal knowledge graph forecasting. By introducing a general state-tuning layer that can plug into diverse backbones, EST significantly enhances performance and shows that going beyond per-snapshot, stateless encoding leads to better modeling of structural\u2013temporal interactions over time."}}
{"id": "2602.12445", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12445", "abs": "https://arxiv.org/abs/2602.12445", "authors": ["Om Bhatt", "Anna A. Ivanova"], "title": "RBCorr: Response Bias Correction in Language Models", "comment": "12 pages (8 pages main text), 4 figures", "summary": "Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.", "AI": {"tldr": "The paper introduces RBCorr, a simple method to correct response bias in language models on fixed-response questions, showing it removes bias and improves performance across several tasks and models.", "motivation": "Language models often show systematic preferences for certain options (e.g., yes vs. no, specific multiple-choice options) in fixed-response settings, which distorts benchmark scores and hides their true reasoning capabilities. Existing bias-correction or calibration methods can be complex or not robust across tasks. The authors aim to design a low-cost, effective, and broadly applicable correction strategy that makes evaluation fairer and can especially help smaller open-weight models.", "method": "The authors propose RBCorr, a response bias correction strategy based on model output statistics (e.g., LogProbs) that adjusts the scores of candidate answers to remove option preference bias. They evaluate it on 12 open-weight LMs across yes/no, textual entailment, and multiple-choice benchmarks. They compare performance before and after correction, and study how bias and bias-correction behavior transfer across different models, datasets, and prompt formats, with a focus on the dependence of LogProbs-based correction on these factors.", "result": "They find that response bias is widespread in open-weight LMs across all tested tasks before correction. Applying RBCorr largely eliminates this bias and leads to consistent performance gains on yes/no, entailment, and multiple-choice question benchmarks. Their analysis further shows that bias patterns and the effectiveness of simple LogProbs-based corrections are not fully consistent across models, datasets, and prompt formats, indicating limited generalizability of naive calibration approaches.", "conclusion": "RBCorr is a simple, easy-to-implement bias correction method that substantially reduces option preference bias in language models and improves their measured performance on closed-response benchmarks. By mitigating these systematic biases, RBCorr helps align benchmark scores more closely with the models' underlying capabilities, and is particularly useful for boosting the apparent performance of smaller open-weight LMs. However, the dependence of LogProbs-based corrections on models, datasets, and prompts highlights the need for care when generalizing bias-correction strategies."}}
{"id": "2602.12575", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12575", "abs": "https://arxiv.org/abs/2602.12575", "authors": ["Bo Wang", "Yuxuan Zhang", "Yueqin Hu", "Hanchao Hou", "Kaiping Peng", "Shiguang Ni"], "title": "Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification", "comment": "78 pages, 20 figures", "summary": "Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.", "AI": {"tldr": "The paper proposes a response-free, NLP-based framework using topic modeling and embeddings to simplify psychological scales while preserving their psychometric properties.", "motivation": "Traditional scale refinement methods (factor analysis, IRT, network models) depend on large samples, are constrained by data availability, and may face cross-cultural comparability issues. The authors are motivated to exploit the semantic information already present in item wording to approximate latent construct structure without requiring participant responses, enabling more efficient and potentially more generalizable scale construction and reduction.", "method": "They encode questionnaire items as contextual sentence embeddings, then apply density-based clustering to group items into latent semantic factors without pre-specifying the number of clusters. They derive interpretable topic representations using class-based term weighting and allow merging semantically adjacent clusters. Within this integrated pipeline, they select representative items based on membership criteria. They benchmark the method on DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, preservation of inter-factor correlations, and reduction efficiency.", "result": "The framework discovered coherent, factor-like semantic clusters that map well onto known constructs in DASS, IPIP, and EPOCH. The selected items shortened scales by an average of 60.5% while maintaining adequate psychometric properties. The simplified scales retained high concordance with original factor structures and preserved correlations between factors, showing that semantic organization can approximate measurement structure without response data.", "conclusion": "Semantic latent structure, extracted via modern NLP techniques, can serve as a viable, response-free front-end for psychological scale construction and reduction. The authors provide a practical, visualization-supported tool that allows users to conduct semantic analysis and structured scale simplification in an automated, user-friendly way."}}
{"id": "2602.12544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12544", "abs": "https://arxiv.org/abs/2602.12544", "authors": ["Lajanugen Logeswaran", "Jaekyeom Kim", "Sungryull Sohn", "Creighton Glasscock", "Honglak Lee"], "title": "Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation", "comment": "COLM 2025", "summary": "We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.", "AI": {"tldr": "They build a scalable automatic pipeline to generate and evaluate training data for web agents using a new constraint-based trajectory evaluation framework and a BookingArena benchmark, achieving performance comparable to or better than commercial systems with a smaller model.", "motivation": "Training web agents requires large amounts of high-quality interaction data, but collecting and labeling full successful trajectories on real websites is expensive and difficult. A key bottleneck is evaluating how much progress an agent has made on a task, which makes it hard to use partially successful attempts and limits data efficiency and realism in current datasets.", "method": "They propose a constraint-based evaluation framework that encodes task requirements as fine-grained constraints over web states and actions. This framework assigns a progress score to trajectories, enabling automatic use of partially successful interaction traces as training data. On top of this, they build a scalable data-generation and distillation pipeline: collect trajectories on real websites, evaluate them via constraints, then train a smaller student model on these labeled trajectories. They introduce a new benchmark, BookingArena, consisting of complex booking tasks across 20 popular sites, to evaluate their pipeline and models.", "result": "Using their constraint-based evaluation and data pipeline on the BookingArena benchmark, they are able to turn many partially successful trajectories into usable training data. The resulting distilled student model, trained on these trajectories, outperforms existing open-source web agent approaches and reaches or surpasses commercial systems on the benchmark, despite being significantly smaller in size.", "conclusion": "Constraint-based, fine-grained trajectory evaluation makes it possible to efficiently exploit partially successful web interaction traces, greatly expanding high-quality training data for web agents. The proposed pipeline and BookingArena benchmark show that this approach yields compact student models that are competitive with or better than larger commercial systems, and offers a systematic methodology for evaluating complex structured web tasks."}}
{"id": "2602.12635", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12635", "abs": "https://arxiv.org/abs/2602.12635", "authors": ["Pengxiang Zhao", "Hui-Ling Zhen", "Xing Li", "Han Bao", "Weizhe Lin", "Zhiyuan Yang", "Ziwei Yu", "Xin Wang", "Mingxuan Yuan", "Xianzhi Yu", "Zhenhua Dong"], "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats", "comment": null, "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.", "AI": {"tldr": "The paper evaluates HiFloat low-bit floating-point formats (HiF8, HiF4) for efficient LLM inference on Ascend NPUs, showing they outperform integer formats on high-variance data and integrate well with existing post-training quantization frameworks.", "motivation": "As large language models grow, running inference efficiently on specialized hardware like NPUs requires aggressive quantization to low-bit formats without sacrificing accuracy. Existing integer formats (like INT8/INT4) can struggle, especially with high-variance data and at very low bit-widths, causing accuracy collapse. There is a need to understand and validate new low-bit floating-point formats specifically designed for NPU hardware, and to clarify when they are preferable to integer quantization.", "method": "The authors introduce and study HiFloat, a family of low-bit floating-point formats (HiF8 and HiF4) tailored for Ascend NPUs. They conduct systematic experiments on LLM workloads, covering both weight-activation paths and KV-cache quantization tasks. They compare HiFloat against INT-based and other low-bit floating-point formats (e.g., MXFP, NVFP4), analyzing performance and accuracy across data ranges and variance levels. Particular attention is given to behavior in the 4-bit regime and integration with standard post-training quantization pipelines.", "result": "The study yields three main findings: (1) INT8 is effective for data with a narrow numerical range, but low-bit floating-point formats (including HiFloat) perform better on high-variance data; (2) at 4 bits, HiF4\u2019s hierarchical scaling mechanism avoids the severe accuracy degradation that typical INT4 formats experience; and (3) HiFloat works seamlessly with current state-of-the-art post-training quantization frameworks, requiring no major changes to existing quantization workflows.", "conclusion": "HiFloat (HiF8 and HiF4) is a practical and effective quantization format family for large language model inference on Ascend NPUs. It combines efficiency from low-bit representation with robustness on high-variance data, particularly at 4 bits, and can be adopted easily within existing post-training quantization frameworks, making it a strong candidate for high-efficiency NPU-based LLM deployment."}}
{"id": "2602.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12566", "abs": "https://arxiv.org/abs/2602.12566", "authors": ["Haoqing Wang", "Xiang Long", "Ziheng Li", "Yilong Xu", "Tingguang Li", "Yehui Tang"], "title": "To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL", "AI": {"tldr": "The paper studies how to best apply reinforcement learning with verifiable rewards (RLVR) across multiple domains for LLM reasoning, comparing mixed multi-task training versus separate RL and model merging, and finds limited interference and some positive synergy between reasoning-heavy domains.", "motivation": "RL with verifiable rewards has enabled expert-level performance for LLMs in narrow domains like coding and math, but real-world applications require a single generalist model that performs well across many domains. Existing work uses either joint multi-task RLVR or separate RLVR per domain followed by merging, yet lacks a systematic comparison and understanding of how these paradigms interact, interfere, or synergize across domains.", "method": "The authors select several representative high-level domains\u2014math, coding, science, and instruction following\u2014and train LLMs using two training paradigms: (1) mixed multi-task RLVR across all domains simultaneously, and (2) domain-specific RLVR followed by model merging. They run extensive qualitative and quantitative experiments with open-source datasets, then analyze the resulting models using tools from weight space geometry, behavioral analysis of predictions, and information-theoretic constraints to understand cross-domain interactions and synergies.", "result": "Empirically, cross-domain RLVR shows little negative interference, indicating that training on multiple domains together does not substantially harm performance in individual domains. Moreover, reasoning-intensive domains (like math and coding) exhibit mutually reinforcing effects, where improvements in one domain can transfer and enhance performance in others. Their analyses show distinct geometric and behavioral patterns consistent with these mutual gains under both mixed multi-task training and post-hoc model merging.", "conclusion": "Both mixed multi-task RLVR and separate RLVR followed by model merging are viable for building multi-domain expert LLMs, with limited cross-domain interference and notable positive synergies among reasoning-heavy tasks. The study\u2019s analyses suggest that reinforcement learning for explicit reasoning can generalize beneficially across related domains, and it introduces M2RL as a framework and benchmark setup for comparing these training strategies."}}
{"id": "2602.12639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12639", "abs": "https://arxiv.org/abs/2602.12639", "authors": ["Yiran Rex Ma", "Yuxiao Ye", "Huiyuan Xie"], "title": "CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation", "comment": "Accepted at LREC 2026", "summary": "Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).", "AI": {"tldr": "The paper proposes CLASE, a hybrid, interpretable metric to automatically evaluate the stylistic quality of LLM-generated Chinese legal texts, achieving higher agreement with human expert judgments than existing methods.", "motivation": "LLM-generated legal texts, while often factually accurate, commonly violate the specialized stylistic norms and linguistic conventions of legal writing. Manual, expert-crafted style rubrics are impractical because many professional stylistic requirements are implicit and hard to formalize. Existing automatic metrics either mix up style with semantic accuracy (reference-based metrics) or rely on opaque and inconsistent LLM-as-a-judge setups. There is a need for a reliable, transparent, and style-focused evaluation method specifically tailored to legal text generation.", "method": "The authors introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid, reference-free evaluation framework focused on stylistic quality. CLASE combines: (1) linguistic feature-based scoring, which quantifies surface-level stylistic indicators; and (2) experience-guided LLM-as-a-judge scoring, where an LLM provides style assessments guided by learned scoring experiences. Both the feature weights and the LLM scoring patterns are trained on contrastive pairs: authentic legal documents versus their LLM-restored counterparts. This setup allows CLASE to learn what differentiates professional legal style from LLM-generated imitations and to integrate explicit features with implicit style judgments.", "result": "On a dataset of 200 Chinese legal documents, CLASE\u2019s scores correlate substantially better with human expert style judgments than traditional automatic metrics and pure LLM-as-a-judge baselines. The system also outputs interpretable score breakdowns (e.g., contributions of different linguistic features and LLM-based components) and can generate actionable suggestions for improving the stylistic quality of legal text.", "conclusion": "CLASE provides a scalable, transparent, and domain-tailored solution for evaluating the stylistic quality of Chinese legal texts generated by LLMs. Its hybrid design effectively captures both explicit linguistic features and implicit professional norms, leading to stronger alignment with human evaluations than existing methods. The interpretability and diagnostic feedback make CLASE practical for real-world deployment in legal drafting and quality control. The authors release code and data to support further research and adoption."}}
{"id": "2602.12586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12586", "abs": "https://arxiv.org/abs/2602.12586", "authors": ["Joshua Ong Jun Leang", "Yu Zhao", "Mihaela C\u0103t\u0103lina Stoian", "Wenda Li", "Shay B. Cohen", "Eleonora Giunchiglia"], "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models", "comment": "8 pages, preprint", "summary": "While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.", "AI": {"tldr": "McDiffuSE is a framework that uses Monte Carlo Tree Search to optimize slot infilling order in Masked Diffusion Models, improving code and math reasoning performance over existing baselines.", "motivation": "Plan-and-infill decoding with Masked Diffusion Models has shown potential in mathematical and code reasoning, but its performance is highly sensitive to the order in which slots are infilled, causing large variance and suboptimal results. A principled way to choose or optimize the generation order is missing.", "method": "The authors propose McDiffuSE, which frames the choice of which slot to infill next as a sequential decision-making problem and applies Monte Carlo Tree Search (MCTS) to search over possible infilling orders. McDiffuSE performs look-ahead simulations on partial completions to estimate their promise before committing to an infilling order, balancing exploration and exploitation via MCTS hyperparameters such as the exploration constant and number of simulations.", "result": "On benchmarks for code (e.g., MBPP) and mathematical reasoning (e.g., MATH500), McDiffuSE achieves an average improvement of 3.2% over autoregressive baselines and 8.0% over standard plan-and-infill MDMs, with particularly strong gains of 19.5% on MBPP and 4.9% on MATH500. Empirical analysis shows that the learned infilling policies often resemble sequential decoding but selectively employ non-sequential slot generation when beneficial.", "conclusion": "MCTS-based planning for slot infilling order is an effective way to enhance generation quality in Masked Diffusion Models. Properly tuned exploration, rather than simply more simulations, is key to overcoming confidence biases in the base model and discovering advantageous non-sequential generation orders. This demonstrates that treating infilling order as a decision-making problem can yield substantial gains in complex reasoning tasks."}}
{"id": "2602.12642", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12642", "abs": "https://arxiv.org/abs/2602.12642", "authors": ["Dohyung Kim", "Minbeom Kim", "Jeonghye Kim", "Sangmook Lee", "Sojeong Rhee", "Kyomin Jung"], "title": "Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR", "comment": null, "summary": "Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.", "AI": {"tldr": "The paper introduces PACED-RL, a GFlowNet-based post-training framework that uses the learned partition function as an online per-prompt accuracy signal to prioritize training on informative prompts and replay high-error ones, significantly improving sample efficiency and performance of LLM reasoning RL compared to GRPO and prior GFlowNet methods.", "motivation": "Reward-maximizing RL for LLMs improves reasoning but collapses output diversity. GFlowNet-based methods address this by matching a target distribution, but they typically ignore the rich information in the learned partition function, using it only as a normalizer. There is a need to improve sample efficiency in distribution-matching RL for LLMs by exploiting this underused signal, leading to better performance without extra compute.", "method": "The authors reinterpret the GFlowNet partition function as an estimate of per-prompt expected reward (online accuracy). They theoretically connect the partition function to per-prompt accuracy, then build PACED-RL, a post-training framework around this. PACED-RL has two main components: (1) partition-function-guided prompt prioritization that focuses training on prompts with higher estimated informativeness, and (2) an accuracy-estimate error-prioritized replay that reuses trajectories where the accuracy estimate is most off. Both components reuse information already computed during standard GFlowNet training, effectively amortizing the overhead of accuracy estimation into the existing optimization process.", "result": "Across multiple benchmarks for LLM reasoning, PACED-RL yields substantial performance gains over GRPO and previous GFlowNet-based methods, while being more sample efficient. The method improves both reward-based performance and distribution-matching quality without requiring additional forward passes beyond those used for GFlowNet training.", "conclusion": "By reinterpreting the GFlowNet partition function as a per-prompt accuracy signal and leveraging it for prompt prioritization and prioritized replay, PACED-RL achieves more sample-efficient distribution-matching RL for LLMs. This shows that existing GFlowNet training already contains valuable information that can be exploited to guide learning, and positions PACED-RL as a promising direction for scaling and improving RL-based LLM post-training without incurring significant extra compute costs."}}
{"id": "2602.12617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12617", "abs": "https://arxiv.org/abs/2602.12617", "authors": ["Modi Jin", "Yiming Zhang", "Boyuan Sun", "Dingwen Zhang", "MingMing Cheng", "Qibin Hou"], "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics", "comment": null, "summary": "This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.", "AI": {"tldr": "GeoAgent is a geolocation reasoning model trained with human-annotated geographic chain-of-thought and new reward designs, achieving more human-like and accurate fine-grained address prediction.", "motivation": "Existing RL-based geolocation models rely heavily on AI-generated chain-of-thought and generic training strategies that do not respect geographic characteristics, leading to concerns about reliability, interpretability, and alignment with human geographic reasoning.", "method": "The authors build GeoSeek, a geolocation dataset with chain-of-thought annotations created by geographic experts and skilled players. They analyze the specific nature of geographic reasoning and design two new rewards for RL training: a geo-similarity reward that measures how geographically close predictions are to the target, and a consistency reward computed by a separate consistency agent that evaluates the integrity and coherence of the model\u2019s reasoning. These rewards guide GeoAgent during training toward geographically sound and logically consistent solutions.", "result": "In experiments, GeoAgent surpasses previous RL-based methods and a range of general vision-language models on geolocation benchmarks at multiple spatial resolutions (\"multiple grains\"). It also produces reasoning traces that more closely resemble human geographic thought processes.", "conclusion": "Carefully constructed human-authored geographic chain-of-thought data, together with task-specific geo-similarity and reasoning-consistency rewards, leads to a geolocation model (GeoAgent) that is both more accurate and more human-aligned in its reasoning than existing approaches and general-purpose VLLMs."}}
{"id": "2602.12660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12660", "abs": "https://arxiv.org/abs/2602.12660", "authors": ["Longze Chen", "Lu Wang", "Renke Shan", "Ze Gong", "Run Luo", "Jiaming Li", "Jing Luo", "Qiyao Wang", "Min Yang"], "title": "Learning Ordinal Probabilistic Reward from Preferences", "comment": "28 pages, 5 figures, ICLR 2026", "summary": "Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\\textbf{2.9%}\\sim\\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.", "AI": {"tldr": "The paper proposes a new probabilistic paradigm for reward modeling in LLM alignment that models full distributions over response quality, instantiated as an ordinal discrete model and trained with a region-focused strategy to better capture absolute text quality while improving accuracy and data efficiency over prior methods.", "motivation": "Existing reward models for aligning LLMs are either generative, requiring expensive point-wise supervision, or discriminative, which only provide uncalibrated relative scores that lack probabilistic interpretation and absolute meaning. There is a need for reward models that are both data-efficient and capable of representing uncertainty and absolute quality levels in a principled probabilistic way.", "method": "The authors introduce the Probabilistic Reward Model (PRM), which treats reward as a random variable and learns a full probability distribution over response quality instead of a single deterministic score. They instantiate this concept as the Ordinal Probabilistic Reward Model (OPRM), which discretizes quality into a finite set of ordered rating levels and predicts a distribution over these levels. On top of OPRM, they design Region Flooding Tuning (RgFT), a training strategy that uses quality-level annotations to push probability mass into the appropriate rating sub-regions, making the learned reward better reflect absolute text quality while remaining data-efficient.", "result": "On multiple reward modeling benchmarks, the proposed approach outperforms prior state-of-the-art reward models by about 2.9%\u20137.4% in accuracy. The model demonstrates strong performance even with limited training data, indicating improved data efficiency. Additional analyses of the score distributions show that the model captures meaningful uncertainty, representing both relative ranking information and absolute quality levels.", "conclusion": "Modeling rewards probabilistically as distributions over ordinal quality ratings, together with the Region Flooding Tuning strategy, yields more accurate, better-calibrated, and more data-efficient reward models for LLM alignment. This probabilistic, ordinal approach overcomes key limitations of generative and discriminative reward paradigms by providing both relative and absolute assessments of response quality."}}
{"id": "2602.12631", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12631", "abs": "https://arxiv.org/abs/2602.12631", "authors": ["Jackie Baek", "Yaopeng Fu", "Will Ma", "Tianyi Peng"], "title": "AI Agents for Inventory Control: Human-LLM-OR Complementarity", "comment": null, "summary": "Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.\n  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.\n  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.", "AI": {"tldr": "The paper investigates how traditional operations research (OR) inventory algorithms, large language models (LLMs), and humans can be combined for better multi-period inventory control, introducing a benchmark (InventoryBench) and showing that OR+LLM and human+AI hybrids outperform any single component alone.", "motivation": "Classical inventory control relies on OR models with strong assumptions about demand distributions and limited ability to use rich context, so their performance degrades under demand shifts and real-world complexity. Meanwhile, LLMs show flexible reasoning and contextual understanding but lack established ways to integrate into structured decision-making pipelines. There is also mixed evidence on whether human-AI collaboration actually improves performance. The paper aims to understand when and how OR algorithms, LLMs, and humans can complement each other to improve inventory decisions.", "method": "The authors design InventoryBench, a benchmark of 1,000+ multi-period inventory instances from both synthetic and real-world demand, incorporating demand shifts, seasonality, and uncertain lead times to stress-test policies. They evaluate traditional OR algorithms, LLM-based decision policies, and hybrid \u201cOR-augmented LLM\u201d approaches on this benchmark. They then run a controlled classroom experiment where human decision-makers interact with embedded LLM recommendations in a human-in-the-loop inventory decision pipeline, measuring performance and collaboration effects. They also formalize individual-level complementarity between humans and AI and derive a distribution-free lower bound on the share of individuals that benefit from AI collaboration.", "result": "On InventoryBench, hybrid OR-augmented LLM methods outperform pure OR and pure LLM methods, indicating strong complementarity between structured optimization and LLM reasoning. In the human-subject experiment, human-AI teams earn higher average profits than either unaided humans or stand-alone AI agents, contradicting earlier results that human-AI collaboration can hurt performance. The authors also show theoretically that a nontrivial fraction of individuals must benefit from AI collaboration under their complementarity formalization, and empirically estimate this fraction to be substantial.", "conclusion": "Traditional OR algorithms, LLMs, and human decision-makers are best viewed as complementary components in inventory control rather than substitutes. Carefully designed hybrid pipelines\u2014OR-augmented LLMs and human-in-the-loop systems with AI recommendations\u2014can achieve superior performance, even under demand shifts and complex dynamics. InventoryBench provides a standardized way to evaluate such decision rules, and the theoretical and empirical analyses show that many individuals gain from AI collaboration, offering guidance for deploying AI decision support in operations contexts."}}
{"id": "2602.12674", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12674", "abs": "https://arxiv.org/abs/2602.12674", "authors": ["Yuang Cai", "Yuyu Yuan"], "title": "$\\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.", "AI": {"tldr": "The paper introduces Experiential Knowledge Distillation (X-KD), a framework that lets student LLMs learn from an approximation of the teacher\u2019s original reward environment rather than only copying its outputs, yielding better performance, diversity, and data efficiency across several tasks.", "motivation": "Existing KD methods for LLMs mainly mimic teacher outputs (logits, probabilities, or responses) but ignore the learning environment\u2014particularly the reward function\u2014that shaped the teacher\u2019s behavior. This can limit how well students internalize the underlying task objectives and may lead to suboptimal generalization, performance-diversity trade-offs, and data efficiency.", "method": "The authors propose Experiential Knowledge Distillation (X-KD), which uses the Approximated Variational Reward Imitation Learning (AVRIL) framework. X-KD jointly (1) models/recovers an approximation of the teacher\u2019s original reward function and (2) performs policy distillation so that the student policy is optimized to be consistent with this inferred reward. They show that, despite using ideas from inverse RL, the resulting learning objective can be expressed within a supervised learning framework and is compatible with both sequence-level and divergence-based distillation schemes.", "result": "On multiple benchmarks\u2014abstractive summarization, machine translation, and arithmetic reasoning\u2014X-KD surpasses generalized KD and MiniLLM baselines. It also shows better performance-diversity trade-offs (i.e., maintains or improves output diversity without sacrificing performance) and higher data efficiency (achieving competitive results with less data) compared with standard KD methods.", "conclusion": "Reconstructing and distilling the teacher\u2019s learning environment\u2014specifically its reward function\u2014enables more effective knowledge transfer than imitating outputs alone. X-KD is a simple, general, and flexible framework that integrates naturally with existing supervised KD paradigms and improves the performance, diversity, and data efficiency of student LLMs across several tasks."}}
{"id": "2602.12662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12662", "abs": "https://arxiv.org/abs/2602.12662", "authors": ["Ruihan Yang", "Fanghua Ye", "Xiang We", "Ruoqing Zhao", "Kang Luo", "Xinbo Xu", "Bo Zhao", "Ruotian Ma", "Shanyi Wang", "Zhaopeng Tu", "Xiaolong Li", "Deqing Yang", "Linus"], "title": "Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.", "AI": {"tldr": "The paper proposes CogRouter, a framework that lets LLM agents dynamically choose how much reasoning to perform at each step in a task, achieving higher success with fewer tokens.", "motivation": "Existing LLM agents use a fixed reasoning style: either fast, shallow responses or uniformly deep, chain-of-thought reasoning. This is inefficient for long-horizon, multi-step tasks where some steps require strategic planning and others only simple, automatic actions. The authors want a principled way for agents to adapt their cognitive effort per step to improve both performance and efficiency.", "method": "They design four hierarchical cognitive levels inspired by ACT-R, ranging from instinctive responses to full strategic planning. They train a router and associated behaviors via a two-stage process: (1) Cognition-aware Supervised Fine-tuning (CoSFT) to teach the model stable behaviors for each level, and (2) Cognition-aware Policy Optimization (CoPO), which assigns credit at the step level by reweighting advantages based on the model\u2019s confidence. The central principle is that the router should pick the cognitive depth that maximizes confidence in the action at that step.", "result": "On ALFWorld and ScienceWorld benchmarks, CogRouter, instantiated with Qwen2.5-7B, achieves 82.3% success rate and uses 62% fewer tokens compared with baselines. It outperforms GPT-4o by 40.3 percentage points, OpenAI-o3 by 18.3 points, and GRPO by 14.0 points on success rate while being more token-efficient.", "conclusion": "Adaptive cognitive depth, guided by confidence and structured via hierarchical levels, makes LLM agents both more effective and more efficient on long-horizon tasks than fixed reasoning strategies. CogRouter demonstrates that cognition-aware training and routing can surpass stronger but rigid baselines, suggesting a promising direction for building cost-effective, high-performing LLM agents."}}
{"id": "2602.12705", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.12705", "abs": "https://arxiv.org/abs/2602.12705", "authors": ["Baorong Shi", "Bo Cui", "Boyuan Jiang", "Deli Yu", "Fang Qian", "Haihua Yang", "Huichao Wang", "Jiale Chen", "Jianfei Pan", "Jieqiong Cao", "Jinghao Lin", "Kai Wu", "Lin Yang", "Shengsheng Yao", "Tao Chen", "Xiaojun Xiao", "Xiaozhong Ji", "Xu Wang", "Yijun He", "Zhixiong Yang"], "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "comment": null, "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "AI": {"tldr": "MedXIAOHE is a medical vision-language foundation model that sets new state-of-the-art results on many medical benchmarks while being safer, more reliable, and better at step-by-step clinical reasoning.", "motivation": "General-purpose multimodal medical AI is limited by incomplete coverage of medical knowledge, poor handling of long-tail/rare conditions, weak explicit reasoning, and unreliable, hallucination-prone outputs in real-world clinical settings. There is a need for a scalable medical foundation model that can robustly understand images and text, reason like medical experts, and interact safely with clinicians.", "method": "The authors build MedXIAOHE, a large medical vision-language model trained with an entity-aware continual pretraining framework that structures heterogeneous medical corpora around clinical entities to broaden and densify knowledge coverage, including rare diseases. They further align the model with medical expert-level reasoning patterns using reinforcement learning and tool-augmented agentic training for multi-step diagnostic reasoning and traceable decisions. Safety and reliability are enhanced with user-preference rubrics, evidence-grounded reasoning, and optimization for low-hallucination long-form report generation and instruction following.", "result": "MedXIAOHE achieves state-of-the-art performance on a wide range of medical benchmarks and outperforms strong closed-source multimodal systems on several capabilities, demonstrating improved general medical understanding, expert-style reasoning, and interaction quality. It produces more reliable, evidence-grounded, and instruction-following outputs, including long-form reports with reduced hallucination.", "conclusion": "An entity-aware, continually pretrained medical vision-language foundation model, further aligned with expert reasoning patterns and safety-focused objectives, can deliver superior performance and reliability across diverse medical tasks. MedXIAOHE offers practical design and scaling insights, along with an evaluation framework, that may guide future research on clinically useful multimodal medical AI systems."}}
{"id": "2602.12665", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12665", "abs": "https://arxiv.org/abs/2602.12665", "authors": ["Na\u00efm Es-sebbani", "Esteban Marquer", "Yakoub Salhi", "Zied Bouraoui"], "title": "Evaluating Robustness of Reasoning Models on Parameterized Logical Problems", "comment": null, "summary": "Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.", "AI": {"tldr": "The paper introduces a carefully controlled 2-SAT benchmark that separates superficial difficulty from the true structural factors determining satisfiability, and uses it to expose hidden brittleness in LLM-based logical reasoners.", "motivation": "Existing SAT-style benchmarks for testing LLM reasoning mix up surface features (like formula length, wording, or clause order) with the deeper structural properties that actually govern satisfiability. This makes it hard to diagnose what logical competencies LLMs truly have and where they fail. The authors want a diagnostic, interpretable testbed that can systematically vary structural aspects of 2-SAT formulas while holding superficial statistics nearly constant, so that they can pinpoint specific strengths and weaknesses of LLM-based reasoners.", "method": "They construct parameterized generators for structured 2-CNF (2-SAT) instances, where satisfiability is fully characterized via their implication graphs. Different generators target specific structural phenomena: (i) contradiction cycles that create minimal UNSAT cores with tunable size and imbalance; (ii) SAT instances with a controlled fraction of free variables to adjust how many satisfying assignments exist; (iii) formulas with planted backbones that enforce sets of variables to be fixed in all solutions, affecting unit propagation behavior; (iv) \"late bridge\" clauses that connect otherwise monotone substructures, to test sensitivity to clause ordering and belief revision; and (v) symmetry and duplication variants that add renaming and redundant structure to test abstraction. They then evaluate multiple LLM-based reasoners on these families, measuring both decision accuracy (SAT/UNSAT) and the correctness of proposed assignments, and test robustness under semantics-preserving perturbations (e.g., clause reordering, adding filler clauses, variable renaming).", "result": "The evaluation shows that LLM-based reasoners exhibit sharp, often abrupt drops or changes in performance when specific structural modifications are introduced, even though surface-level statistics like length or apparent complexity are held fixed. They are sensitive to features such as contradiction cycle size/imbalance, backbone strength, presence and placement of bridge clauses, and symmetry/duplication, and their assignments often become invalid under benign perturbations like reordering or renaming. These effects are not visible if one only looks at standard, aggregate SAT benchmark accuracy, which can appear relatively high and stable.", "conclusion": "The paper concludes that current LLM-based logical reasoners are brittle with respect to the true structural factors that determine 2-SAT satisfiability. Standard SAT benchmarks, which conflate superficial and structural difficulty, mask these vulnerabilities. Carefully designed, parameterized 2-SAT benchmarks that isolate structural competencies are crucial for diagnosing and ultimately improving the robustness and reliability of LLM reasoning systems."}}
{"id": "2602.12709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12709", "abs": "https://arxiv.org/abs/2602.12709", "authors": ["Yixin Chen", "Ying Xiong", "Shangyu Wu", "Xiangrui Ke", "Nan Guan", "Chun Jason Xue"], "title": "ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter", "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.\n  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.", "AI": {"tldr": "The paper proposes ReFilter, a token-level latent fusion framework that improves retrieval-augmented generation (RAG) by filtering and fusing retrieved evidence more effectively, especially when many documents are retrieved.", "motivation": "Existing RAG fusion methods\u2014query-based, parametric, and latent-based\u2014work reasonably well when only a modest number of documents (top-k) are retrieved, but they degrade or become inefficient as k grows because larger k brings more irrelevant or redundant content and higher inference cost. There is a need for a scalable fusion mechanism that can exploit large retrieval sets while mitigating noise and redundancy at fine granularity.", "method": "ReFilter is a latent-based fusion framework that operates at the token level. It includes: (1) a context encoder that encodes each retrieved context into token-level features; (2) a gated filter that assigns weights to each token, effectively filtering out irrelevant or low-utility tokens; and (3) a token fusion module that integrates the resulting weighted token representations into the hidden states of the LLM during generation. This design allows fine-grained, relevance-aware fusion of retrieval evidence into the model.", "result": "Across four general-domain QA benchmarks, ReFilter yields the best average performance compared with existing fusion methods under both in-domain adaptation and out-of-domain transfer settings. Moreover, without any domain-specific fine-tuning, ReFilter achieves strong zero-shot transfer to five biomedical QA benchmarks, reaching 70.01% average accuracy when used with the Qwen2.5-14B-Instruct model.", "conclusion": "Token-level latent filtering and fusion of retrieved evidence can make RAG more robust and scalable as the number of retrieved documents increases. ReFilter\u2019s architecture allows it to handle noisy, redundant retrievals efficiently and generalizes well across domains, including challenging biomedical QA, suggesting a promising direction for future RAG systems."}}
{"id": "2602.12670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12670", "abs": "https://arxiv.org/abs/2602.12670", "authors": ["Xiangyi Li", "Wenbo Chen", "Yimin Liu", "Shenghan Zheng", "Xiaokun Chen", "Yifeng He", "Yubo Li", "Bingran You", "Haotian Shen", "Jiankai Sun", "Shuyi Wang", "Qunhong Zeng", "Di Wang", "Xuandong Zhao", "Yuanli Wang", "Roey Ben Chaim", "Zonglin Di", "Yipeng Gao", "Junwei He", "Yizhuo He", "Liqiang Jing", "Luyang Kong", "Xin Lan", "Jiachen Li", "Songlin Li", "Yijiang Li", "Yueqian Lin", "Xinyi Liu", "Xuanqing Liu", "Haoran Lyu", "Ze Ma", "Bowei Wang", "Runhui Wang", "Tianyu Wang", "Wengao Ye", "Yue Zhang", "Hanwen Xing", "Yiqi Xue", "Steven Dillmann", "Han-chung Lee"], "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "comment": null, "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "AI": {"tldr": "Introduces SkillsBench, a benchmark to evaluate whether agent Skills (procedural knowledge modules for LLM agents) actually help, showing curated Skills often improve performance but not uniformly, while self-generated Skills do not help on average.", "motivation": "Agent Skills are widely adopted to augment LLM agents, but there is no standard, quantitative way to measure their real impact across domains and model configurations. The authors want to clarify when Skills help, by how much, and under what conditions, and whether models can create effective Skills for themselves.", "method": "Design SkillsBench, a benchmark of 86 tasks spanning 11 domains, where each task has paired curated Skills and deterministic verifiers. Evaluate 7 different agent\u2013model configurations over 7,308 trajectories, running each task in three conditions: (1) without Skills, (2) using human-curated Skills, and (3) using Skills that the model generates for itself. Compare pass rates and analyze performance by domain, task, model size, and Skill design (focused vs. comprehensive).", "result": "Curated Skills increase average task pass rate by 16.2 percentage points overall, with large variation by domain (e.g., +4.5pp in Software Engineering and +51.9pp in Healthcare). However, 16 of 84 tasks exhibit worse performance when Skills are used. Self-generated Skills, produced by the models themselves, show no average performance benefit. Focused Skills organized into 2\u20133 concise modules outperform lengthy, comprehensive documentation. Additionally, smaller models equipped with curated Skills can match the performance of larger models without Skills.", "conclusion": "Agent Skills can substantially improve LLM agent performance, but their effectiveness is domain- and task-dependent and can sometimes be harmful. Human-curated, focused Skill modules are more beneficial than long-form documentation or model-authored Skills. Because smaller models with good Skills can rival larger models without them, Skills represent a powerful lever for improving agents, and SkillsBench provides a standardized way to measure and compare these effects."}}
{"id": "2602.12746", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.12746", "abs": "https://arxiv.org/abs/2602.12746", "authors": ["Jing Xu", "Minglin Wu", "Xueyuan Chen", "Xixin Wu", "Helen Meng"], "title": "Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting", "comment": "Accepted by ICASSP 2026", "summary": "Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.", "AI": {"tldr": "Introduces Lamer-SSL, a parameter-efficient framework with a layer-aware mixture of LoRA experts plus replay to extend self-supervised speech models to new languages while reducing forgetting.", "motivation": "Self-supervised speech models perform well but generalize poorly to new languages and suffer from catastrophic forgetting when continually trained on additional languages. There is a need for a method that can add new languages efficiently (few trainable parameters) while preserving performance on previously learned languages.", "method": "The paper proposes Lamer-SSL, which combines: (1) a Layer-Aware MixturE of LoRA Experts (Lamer) module that adds LoRA-based experts to different layers of a pre-trained self-supervised speech model and flexibly balances shared vs. language-specific representations, and allocates more experts to deeper, semantically richer layers; and (2) a replay strategy that reuses a small subset of past data during continual training to preserve prior knowledge. Only 2.14% of model parameters are updated during training, making it parameter-efficient.", "result": "On automatic speech recognition (ASR) and language identification (LID) benchmarks, Lamer-SSL effectively adapts self-supervised models to new languages while largely preserving or maintaining strong performance on previously learned languages, despite training only 2.14% of the parameters. It reduces catastrophic forgetting compared with baselines.", "conclusion": "Lamer-SSL provides a parameter-efficient solution for multilingual continual learning in self-supervised speech models by combining a layer-aware mixture of LoRA experts with a lightweight replay strategy, achieving effective cross-lingual extension with minimal forgetting and very low additional trainable parameters."}}
{"id": "2602.12748", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12748", "abs": "https://arxiv.org/abs/2602.12748", "authors": ["Tobias Labarta", "Nhi Hoang", "Maximilian Dreyer", "Jim Berend", "Oleg Hein", "Jackie Ma", "Wojciech Samek", "Sebastian Lapuschkin"], "title": "X-SYS: A Reference Architecture for Interactive Explanation Systems", "comment": "18 pages, 8 figures", "summary": "The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.", "AI": {"tldr": "The paper proposes X-SYS, a reference architecture for building interactive explainable AI systems that remain usable and governable under real-world operational constraints, and demonstrates it via a system called SemanticLens.", "motivation": "Many XAI methods focus on algorithms in isolation, but in practice explanations must be delivered as interactive systems that handle repeated queries, evolving models and data, and governance and audit requirements. There is a gap between XAI techniques and system-level architectures that can reliably operationalize explainability at scale while preserving usability and compliance.", "method": "The authors conceptualize explainability as an information systems problem and derive system requirements from user interaction needs. They design X-SYS, a reference architecture for interactive explanation systems, structured around four key quality attributes (STAR: scalability, traceability, responsiveness, adaptability) and a five-part component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). They map common interaction patterns in explanation UIs to backend system capabilities, defining contract-based service boundaries and separation of concerns. They instantiate this architecture in a concrete system, SemanticLens, for semantic search and activation steering in vision-language models, and use it to illustrate how the architectural principles work in practice.", "result": "The result is X-SYS, a reusable reference architecture linking front-end explanation interfaces to back-end AI and data services in a way that supports the STAR quality attributes. The architecture includes clearly defined components, service contracts, and interaction-to-capability mappings, which together decouple UI evolution from backend computation. The SemanticLens implementation shows that contract-based service boundaries support independent component evolution, offline/online separation maintains responsiveness, and persistent state enables traceability in a real interactive XAI application.", "conclusion": "Operational XAI requires treating explainability as a full-stack information system, not just a collection of algorithms. X-SYS offers a systematic blueprint for building interactive explanation systems that meet scalability, traceability, responsiveness, and adaptability requirements under real-world constraints. The SemanticLens case study validates the practicality of the architecture and illustrates how its principles can be applied to complex vision-language model workflows, providing guidance for researchers and practitioners aiming to deploy XAI in production environments."}}
{"id": "2602.12759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12759", "abs": "https://arxiv.org/abs/2602.12759", "authors": ["Elena Alvarez-Mellado", "Julio Gonzalo"], "title": "Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks", "comment": "Accepted at LREC 2026", "summary": "Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.", "AI": {"tldr": "The paper proposes a new evaluation methodology for sequence labeling that uses small, linguistically designed test suites instead of large scraped datasets, yielding diagnostic, actionable, and predictive insights about model performance, including on out-of-distribution data.", "motivation": "Traditional NLP evaluation only tells us which system is better on average on a specific test set and offers little guidance on how to improve models. Moreover, systems that perform better in-distribution can underperform on new, out-of-distribution data. There is a need for an evaluation framework that reveals systematic weaknesses, guides model selection and improvement, and predicts performance on different data distributions.", "method": "The authors introduce an error-analysis-centered evaluation framework for sequence labeling tasks. Instead of relying on large, scraped, in-distribution test corpora, they construct small, manually designed test suites of linguistically motivated examples. These examples are created to systematically and exhaustively cover a space of span attributes (e.g., token shape, length, casing, sentence position). They then evaluate different models on this controlled test suite and on external datasets, comparing performance patterns and correlations to assess diagnostic and predictive power.", "result": "Applying the methodology to a benchmark for identifying anglicisms in Spanish, the authors show that their handcrafted test suites provide fine-grained, interpretable performance profiles of models, revealing systematic strengths and weaknesses tied to specific span attributes. They report that performance on their designed tests strongly correlates with performance on external datasets, achieving a median correlation of 0.85, indicating strong predictive validity for out-of-distribution performance.", "conclusion": "Carefully designed, small, linguistically motivated test suites can be more informative than large scraped test sets for evaluating sequence labeling models. The proposed methodology yields diagnostic insights, helps select models based on scenario-specific needs, and can reliably predict performance on external, distributionally different data. This suggests that evaluation in NLP should shift toward structured, error-analysis-driven test design rather than relying solely on large, generic benchmarks."}}
{"id": "2602.12852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12852", "abs": "https://arxiv.org/abs/2602.12852", "authors": ["Junjie Wang", "Zequn Xie", "Dan Yang", "Jie Feng", "Yue Shen", "Duolin Sun", "Meixiu Long", "Yihan Jiao", "Zhehao Tan", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning", "comment": "Work in Progress", "summary": "Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.", "AI": {"tldr": "The paper proposes WebClipper, a framework that prunes and compresses web-agent tool-call trajectories to make deep research more efficient while maintaining or improving accuracy.", "motivation": "Existing deep research web agents can solve complex information-seeking tasks but are inefficient: they generate long tool-call trajectories, get stuck in cyclic reasoning loops, and explore many unproductive branches. There is little systematic work on optimizing search efficiency while preserving reasoning quality.", "method": "Model the web agent\u2019s multi-step search as a state graph and formulate trajectory optimization as a minimum-necessary DAG mining problem. Use graph-based pruning to remove redundant or unhelpful nodes and edges from the trajectory while preserving the essential reasoning path. Then perform continued training (fine-tuning) of the web agent on these pruned trajectories so it learns more efficient search patterns. Additionally, define a new evaluation metric, F-AE Score, that balances accuracy and efficiency (e.g., combining task success with the number of tool calls).", "result": "WebClipper reduces the number of tool-call rounds by around 20% and at the same time improves task accuracy compared with baseline web agents that use unpruned trajectories. Empirical results across benchmarks show that the method compresses trajectories substantially without degrading performance and often enhances it. The proposed F-AE Score captures the trade-off between accuracy and efficiency and highlights the gains from WebClipper.", "conclusion": "Graph-based pruning of web-agent trajectories is an effective way to improve the efficiency of deep research systems without sacrificing, and even boosting, accuracy. Training on compressed minimum-necessary DAG trajectories leads to more streamlined search behavior. The F-AE Score offers a practical measure for co-optimizing accuracy and efficiency. Overall, WebClipper provides a principled framework and empirical evidence for designing web agents that are both effective and computationally efficient."}}
{"id": "2602.12778", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12778", "abs": "https://arxiv.org/abs/2602.12778", "authors": ["Hamidreza Kazemi Taskooh", "Taha Zare Harofte"], "title": "Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews", "comment": "25 pages, 12 figures, 4 tables", "summary": "This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.", "AI": {"tldr": "A hybrid BERT-based, energy-efficient ABSA model for Persian tourism reviews that introduces Top-K routing with auxiliary losses, achieves strong F1, and releases the first Persian tourism ABSA dataset.", "motivation": "Aspect-based sentiment analysis is well-studied for high-resource languages but underexplored for low-resource languages like Persian, especially in domain-specific contexts such as tourism. There is also a growing need for more energy-efficient and sustainable NLP models due to environmental and resource constraints. The authors aim to improve ABSA performance for Persian tourism reviews while simultaneously reducing computational and energy costs, contributing both to under-resourced language technology and to sustainable AI aligned with UN SDGs.", "method": "They build a hybrid BERT-based architecture enhanced with dynamic Top-K routing and auxiliary losses to prevent routing collapse and improve efficiency. The pipeline has three stages: (1) overall sentiment classification with BERT trained on 9,558 labeled reviews; (2) multi-label aspect extraction for six predefined tourism aspects (host, price, location, amenities, cleanliness, connectivity); and (3) an integrated ABSA model where BERT representations are processed via dynamic routing with Top-K selection of expert routes, regularized by auxiliary losses. They curate and preprocess a dataset of 58,473 Persian reviews from the Jabama platform and manually annotate it with aspect and sentiment labels to train and evaluate the system. They compare against a dense BERT baseline and a standard hybrid approach, and measure both accuracy and GPU power consumption.", "result": "On the constructed Persian tourism review dataset, the proposed hybrid BERT model attains a weighted F1-score of 90.6% for ABSA, improving over a vanilla BERT baseline (89.25%) and a standard hybrid model (85.7%). In terms of efficiency, the Top-K routing approach yields a 39% reduction in GPU power consumption relative to dense BERT. Qualitative analysis of aspect distributions shows that cleanliness and amenities are the most frequently mentioned aspects in the reviews.", "conclusion": "The study demonstrates that a hybrid BERT model with Top-K routing and auxiliary losses can simultaneously enhance accuracy and energy efficiency for aspect-based sentiment analysis in a low-resource language setting. It establishes the first ABSA benchmark for Persian tourism reviews, provides evidence that cleanliness and amenities are key concerns for Persian-speaking tourists, and publicly releases the annotated dataset to encourage future multilingual and sustainable NLP research in tourism and related domains."}}
{"id": "2602.12876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12876", "abs": "https://arxiv.org/abs/2602.12876", "authors": ["Huanyao Zhang", "Jiepeng Zhou", "Bo Li", "Bowen Zhou", "Yanzhe Dan", "Haishan Lu", "Zhiyong Cao", "Jiaoyang Chen", "Yuqian Han", "Zinan Sheng", "Zhengwei Tao", "Hao Liang", "Jialong Wu", "Yang Shi", "Yuanpeng He", "Jiaye Lin", "Qintong Zhang", "Guochen Yan", "Runhao Zhao", "Zhengpin Li", "Xiaohan Yu", "Lang Mei", "Chong Chen", "Wentao Zhang", "Bin Cui"], "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents", "comment": null, "summary": "Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.", "AI": {"tldr": "The paper introduces BrowseComp-V^3, a challenging benchmark and an agent framework for evaluating multimodal large language models\u2019 deep web search and browsing capabilities, revealing large gaps between current models and robust multimodal search in real-world settings.", "motivation": "Existing benchmarks for multimodal web browsing and search are too simple, have easily accessible evidence, and lack fine-grained evaluation of the reasoning process. This makes it hard to rigorously and reproducibly assess how well multimodal large language models can perform complex, real-world deep search and browsing that span text and images across multiple web pages.", "method": "The authors build BrowseComp-V^3, a benchmark of 300 carefully curated, difficult questions covering diverse domains. Tasks require deep, multi-level, cross-modal multi-hop reasoning, where necessary evidence is scattered across textual and visual content both within and between web pages, and all evidence must be publicly searchable. They also design a subgoal-driven process evaluation, validated by experts, to assess not only final answers but intermediate reasoning steps. Additionally, they propose OmniSeeker, a unified multimodal browsing agent framework that integrates various web search and visual perception tools to operate on this benchmark.", "result": "When tested with comprehensive experiments, state-of-the-art multimodal models achieve only about 36% accuracy on BrowseComp-V^3. The process evaluation further shows significant weaknesses in models\u2019 abilities to integrate information across modalities and to perform fine-grained visual perception during multi-step web browsing tasks.", "conclusion": "BrowseComp-V^3 exposes a substantial performance gap between current multimodal large language models and the demands of robust, real-world multimodal deep search. The benchmark and OmniSeeker framework provide a more rigorous, reproducible way to analyze agentic browsing behaviors, identify capability boundaries, and guide future research on improving planning, perception, and cross-modal information integration in autonomous multimodal agents."}}
{"id": "2602.12806", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12806", "abs": "https://arxiv.org/abs/2602.12806", "authors": ["Nata\u0161a Kr\u010do", "Zexi Yao", "Matthieu Meeus", "Yves-Alexandre de Montjoye"], "title": "RAT-Bench: A Comprehensive Benchmark for Text Anonymization", "comment": null, "summary": "Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.", "AI": {"tldr": "The paper introduces RAT-Bench, a benchmark to evaluate text anonymization tools by how much re-identification risk remains after anonymization, rather than just whether specific identifiers are removed.", "motivation": "Existing PII removal tools for LLM training and querying are mainly evaluated on their ability to detect and scrub explicit identifiers like names. However, this does not capture the real privacy risk: an attacker may still be able to re-identify individuals using combinations of indirect attributes and imperfectly removed direct identifiers. There is a need for a realistic, quantitative benchmark that measures re-identification risk across domains, languages, and identifier types, while incorporating population-level statistics and disparate impact.", "method": "The authors construct RAT-Bench using synthetic text that embeds a variety of direct and indirect identifiers, generated according to U.S. demographic statistics. The data spans multiple domains, languages, and difficulty levels. They run different anonymization tools\u2014both NER-based and LLM-based, including iterative anonymizers\u2014on this text. Then, they simulate an LLM-based attacker that attempts to infer attributes from the anonymized text. Using what the attacker can correctly infer, they estimate the re-identification risk within the U.S. population, explicitly modeling how different identifiers contribute differently to risk and impact.", "result": "The evaluation shows large variability in performance across anonymization tools. Even the strongest tools leave significant re-identification risk, especially when direct identifiers are obfuscated or formatted in non-standard ways, and when indirect identifiers (e.g., combinations of demographics) enable linkage. LLM-based anonymizers tend to provide a better privacy-utility trade-off than traditional NER systems and generalize better across languages, but they incur higher computational costs. Iterative LLM anonymization also improves performance but does not fully eliminate risk.", "conclusion": "The paper concludes that current anonymization tools are insufficient to reliably prevent re-identification, particularly in challenging settings involving non-standard direct identifiers and rich indirect identifiers. LLM-based anonymization is promising, especially for multilingual data, but remains imperfect and resource-intensive. The authors offer design recommendations for future tools and release RAT-Bench to catalyze community-driven improvements and extensions, including adaptation to non-U.S. populations and geographies."}}
{"id": "2602.12963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12963", "abs": "https://arxiv.org/abs/2602.12963", "authors": ["Alfred Harwood", "Jose Faustino", "Alex Altair"], "title": "Information-theoretic analysis of world models in optimal reward maximizers", "comment": "28 pages, 0 figures. Not submitted to any conference yet", "summary": "An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \\log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \\log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the \"implicit world model'' necessary for optimality.", "AI": {"tldr": "The paper studies how much information about an environment is inherently contained in an optimal policy, providing an information-theoretic lower bound on the implicit world model needed for optimal decision-making in controlled Markov processes.", "motivation": "In AI and reinforcement learning, a key debate is whether agents must explicitly represent the environment to behave optimally, or whether they can act reactively with minimal internal modeling. The authors aim to formalize and quantify how much information about the environment is necessarily encoded in any optimal policy, thereby clarifying the minimal internal \u201cworld model\u201d needed for optimal behavior.", "method": "The authors model the decision problem as a Controlled Markov Process (CMP) with n states and m actions, under a uniform prior over all possible transition dynamics. They then analyze the mapping from environment dynamics to optimal deterministic policies, assuming optimality for all non-constant reward functions, and compute the mutual information between the environment and the optimal policy. They consider several standard reinforcement learning objectives: finite-horizon, infinite-horizon discounted, and average reward criteria. Using information-theoretic tools, they derive an exact expression for this mutual information.", "result": "They prove that observing a deterministic optimal policy (optimal for any non-constant reward) reveals exactly n log m bits of information about the environment\u2019s transition dynamics. Equivalently, the mutual information between the environment and the optimal policy is n log m bits, and this holds uniformly across the considered objective classes (finite-horizon, discounted infinite-horizon, and time-averaged reward).", "conclusion": "The paper establishes a precise information-theoretic lower bound on how much environmental information must be implicitly encoded in an optimal policy: n log m bits for a CMP with n states and m actions. This shows that, even if an agent does not have an explicit, human-interpretable model of the world, optimal behavior necessarily embodies a substantial \u201cimplicit world model\u201d whose size scales linearly with the number of states and logarithmically with the number of actions."}}
{"id": "2602.12811", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.12811", "abs": "https://arxiv.org/abs/2602.12811", "authors": ["Laurent Bonnasse-Gahot", "Christophe Pallier"], "title": "Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence", "comment": null, "summary": "When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).", "AI": {"tldr": "The paper studies why brain-LLM similarity becomes stronger in the left than in the right hemisphere as models are trained, and finds that this asymmetry emerges specifically with the acquisition of formal linguistic competence, not with arithmetic, abstract syntax (Dyck), or world-knowledge tasks.", "motivation": "Previous work showed that LLM activations can predict human brain activity during language processing, with prediction accuracy improving more in the left hemisphere as models train. However, it was unclear which specific capabilities acquired by LLMs are responsible for this left-lateralized pattern that resembles human language dominance. The authors aim to pinpoint which aspects of model competence\u2014formal linguistic knowledge, arithmetic, abstract hierarchical structure, or world knowledge/reasoning\u2014track this hemispheric asymmetry.", "method": "The authors use the OLMo-2 7B LLM saved at multiple training checkpoints, along with fMRI recordings from English participants reading text. For each checkpoint, they compute brain scores\u2014how well a linear mapping from model activations predicts voxel-level fMRI responses\u2014separately for left and right hemispheres, and derive a left-right asymmetry measure. They then compare how this asymmetry evolves over training against the model\u2019s performance on several external benchmarks: (1) formal linguistic competence measured by minimal grammatical/ ungrammatical sentence pairs and by the quality of generated text; (2) arithmetic tasks; (3) Dyck language tasks probing nested bracket structures; and (4) text-based tasks requiring world knowledge and reasoning. They run analogous analyses on another model family (Pythia) and on data from French, to test generalization.", "result": "As training proceeds, both brain scores and a left-greater-than-right asymmetry increase. This hemispheric asymmetry closely co-evolves with measures of formal linguistic competence: better discrimination of grammatical vs ungrammatical sentences and more well-formed generated text align with stronger left-lateralized brain predictivity. In contrast, improvements in arithmetic performance, Dyck language tasks, and world-knowledge/reasoning benchmarks do not track the growth of the left-right asymmetry. These patterns hold not only for OLMo-2 7B and English fMRI data, but also generalize to Pythia models and to French, suggesting robustness across architectures and languages.", "conclusion": "The emergence of stronger LLM-to-brain mapping in the left hemisphere is specifically associated with the model\u2019s acquisition of formal linguistic competence\u2014knowledge of grammatical and distributional linguistic patterns\u2014rather than with general problem solving, arithmetic, or abstract hierarchical structure skills. This implies that the left-lateralized brain similarity reflects language-specific representations in LLMs that parallel human left-hemisphere specialization for language, and that progress in formal linguistic pattern learning is a key driver of this alignment."}}
{"id": "2602.13093", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13093", "abs": "https://arxiv.org/abs/2602.13093", "authors": ["Yubo Li", "Ramayya Krishnan", "Rema Padman"], "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks", "comment": null, "summary": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.", "AI": {"tldr": "The paper systematically evaluates and characterizes how large reasoning models behave under multi-turn adversarial attacks, showing that they are more robust than instruction-tuned LLMs but still have clear, structured vulnerabilities and that current confidence-based defenses break down for such models.", "motivation": "Although large reasoning models achieve strong performance on complex tasks, it is unclear whether their explicit reasoning processes make them robust or vulnerable when an adversary interacts with them over multiple turns. Prior robustness and safety work mostly focuses on standard instruction-tuned LLMs or single-turn attacks, leaving a gap in understanding how reasoning traces, chain-of-thought, and multi-step deliberation affect susceptibility to adversarial pressure.", "method": "The authors evaluate nine frontier reasoning models under multi-turn adversarial attacks and compare them to instruction-tuned baselines. They systematically craft adversarial conversations featuring different pressure types, such as misleading suggestions and social pressure. They then analyze the models\u2019 generated trajectories to identify and categorize failure patterns, yielding five failure modes: Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue. They also adapt and test a known defense, Confidence-Aware Response Generation (CARG), to reasoning models, and probe confidence estimation both via targeted extraction from reasoning traces and via random confidence embeddings.", "result": "Reasoning models generally show better robustness than instruction-tuned baselines under adversarial attacks, but all remain vulnerable. Misleading suggestions are consistently effective across models, while social pressure has variable impact depending on the model. The trajectory analysis reveals five distinct failure modes, with Self-Doubt and Social Conformity making up about half of all observed failures. When applying CARG, a defense previously effective for standard LLMs, the defense fails on reasoning models because their extended reasoning traces lead to systematically overconfident predictions. Surprisingly, a naive method using random confidence embeddings performs better than carefully targeted extraction from the reasoning traces.", "conclusion": "Explicit reasoning capabilities improve but do not guarantee adversarial robustness. Reasoning models have characteristic, interpretable failure modes that can be exploited by multi-turn attackers, especially via misleading suggestions and certain social dynamics. Existing confidence-based defenses like CARG, which work for standard LLMs, are not directly applicable to reasoning models due to their overconfident reasoning traces; making such defenses effective will require rethinking how confidence is represented and extracted in the context of long, structured reasoning. Overall, stronger, specially designed robustness and defense mechanisms are needed for large reasoning models."}}
{"id": "2602.13135", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.13135", "abs": "https://arxiv.org/abs/2602.13135", "authors": ["Emanuele De Angelis", "Fabio Fioravanti", "Maria Chiara Meo", "Alberto Pettorossi", "Maurizio Proietti", "Francesca Toni"], "title": "Constrained Assumption-Based Argumentation Frameworks", "comment": "Extended version with proofs and additional results of the full paper accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026). DOI: https://doi.org/10.65109/KRAP9309", "summary": "Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.", "AI": {"tldr": "Introduces constrained ABA (CABA), extending ABA with constrained variables over possibly infinite domains and defining corresponding non-ground semantics and attacks that generalize standard ABA.", "motivation": "Classical ABA is limited to ground, propositional (variable-free) arguments and attacks, which restricts its expressiveness and applicability in domains that require variables and quantification over (possibly infinite) domains. The paper aims to remove this representational limitation.", "method": "They extend ABA by adding constrained variables to its components, forming constrained ABA (CABA). They then formally define non-ground semantics for CABA via several notions of non-ground attacks, and analyze the relationship between these new semantics and the standard ABA semantics.", "result": "A formal framework of constrained ABA is proposed with precise definitions of non-ground arguments and multiple kinds of non-ground attacks, along with associated semantics. The authors prove that these semantics are conservative generalizations of classical ABA semantics when restricted to the ground/propositional case.", "conclusion": "Constrained ABA (CABA) successfully extends standard ABA to handle non-ground, variable-containing arguments over possibly infinite domains while preserving the behavior of the original framework on ground cases, thus increasing ABA's expressiveness and applicability without sacrificing compatibility."}}
{"id": "2602.13166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13166", "abs": "https://arxiv.org/abs/2602.13166", "authors": ["Hugo Henry", "Arthur Tsai", "Kelly Cohen"], "title": "Optimal Take-off under Fuzzy Clearances", "comment": "12 pages, 12 figures, conference paper", "summary": "This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.", "AI": {"tldr": "Hybrid obstacle avoidance method for unmanned aircraft combining fuzzy rules with optimal control to adaptively handle safety constraints; shows feasibility but uncovers a solver/toolbox regression that breaks constraint enforcement.", "motivation": "Classical optimal control methods under uncertainty struggle with rigid constraint handling and lack interpretable decision-making, which is problematic for safety\u2011critical aviation applications that must respect regulatory separation minima and airworthiness standards. The authors want a method that both adheres to aviation procedures and can adapt clearances in real time, while remaining explainable and computationally practical.", "method": "Propose a hybrid architecture where a three\u2011stage Takagi\u2011Sugeno\u2011Kang fuzzy rule\u2011based system adjusts obstacle\u2011related parameters: constraint radii, urgency levels, and whether avoidance should be activated, using rules informed by FAA/EASA guidelines. These fuzzy\u2011computed clearances become soft constraints in an optimal control problem, solved with the FALCON toolbox and IPOPT. The system selectively triggers recomputation of avoidance maneuvers to reduce computational load. A proof\u2011of\u2011concept is implemented in MATLAB using a simplified aircraft model.", "result": "The approach can generate obstacle\u2011avoiding optimal trajectories with around 2\u20133 seconds of computation per iteration on a single thread in MATLAB, indicating potential for near real\u2011time application. However, experiments uncovered a serious issue: in recent FALCON/IPOPT versions, the Lagrangian penalty term remained zero, so constraints were not correctly enforced. This behavior appeared consistently and is attributed to a regression or incompatibility in the numerical tools rather than in the problem formulation.", "conclusion": "The hybrid fuzzy\u2013optimal control architecture appears conceptually sound and computationally promising for adaptive obstacle avoidance in unmanned aircraft, but its current realization is limited by a suspected regression in the underlying optimization toolchain, which nullifies constraint penalties. The authors plan to confirm this by testing earlier software versions, refine fuzzy membership functions with evolutionary optimization, and scale the approach to more realistic aircraft models and stochastic obstacle settings."}}
{"id": "2602.12889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12889", "abs": "https://arxiv.org/abs/2602.12889", "authors": ["Jiangxi Chen", "Qian Liu"], "title": "BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models", "comment": null, "summary": "We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.", "AI": {"tldr": "BaziQA-Benchmark is a standardized benchmark to test symbolic and temporally compositional reasoning in large language models using structured, multiple-choice questions from a professional competition.", "motivation": "Existing evaluations of large language models on complex reasoning are often anecdotal, prompt-dependent, or lack standardization, especially for tasks requiring symbolic reasoning combined with temporal composition. The authors want an objective, repeatable way to measure and compare models\u2019 abilities to reason over structured symbolic charts and interacting temporal conditions.", "method": "They construct BaziQA-Benchmark from 200 professionally curated multiple-choice questions taken from the Global Fortune-teller Competition (2021\u20132025). Each problem involves reasoning over a fixed symbolic chart and multiple interacting temporal constraints. They evaluate modern language models in a multi-turn interaction setting, and they analyze performance by slicing along temporal difficulty, reasoning domain, and inference protocol. Additionally, they design a lightweight Structured Reasoning Protocol that forces models to follow a constrained, stepwise inference order without adding extra domain-specific knowledge, in order to probe how reasoning order affects performance.", "result": "Language models do better than random guessing on this benchmark, but their accuracy is still far from the benchmark\u2019s upper bound. Performance is highly sensitive to the complexity of temporal composition and to the imposed reasoning order. Models show systematic weaknesses in precise temporal localization and in correctly handling symbolic judgments that depend on multiple interacting conditions.", "conclusion": "BaziQA-Benchmark provides a rigorous, standardized testbed for symbolic and temporally compositional reasoning in large language models. Current models show non-trivial but clearly insufficient capabilities, struggle particularly with complex temporal composition and multi-condition symbolic reasoning, and are influenced by the structure of the reasoning protocol, indicating substantial headroom for future model and prompting improvements."}}
{"id": "2602.12911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12911", "abs": "https://arxiv.org/abs/2602.12911", "authors": ["Tung X. Nguyen", "Nhu Vo", "Giang-Son Nguyen", "Duy Mai Hoang", "Chien Dinh Huynh", "Inigo Jauregi Unanue", "Massimo Piccardi", "Wray Buntine", "Dung D. Le"], "title": "ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark", "comment": "Accepted at LREC 2026", "summary": "Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \\textbf{Vi}etnamese \\textbf{Med}ical \\textbf{C}ode-\\textbf{S}witching \\textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.", "AI": {"tldr": "They create the first Vietnamese medical code-switching speech benchmark (ViMedCSS) and show how different ASR pretraining strategies affect recognizing English medical terms inside Vietnamese speech.", "motivation": "Vietnamese speakers frequently mix English medical terms (e.g., drug names, procedures) into Vietnamese speech, but existing ASR systems\u2014especially for low-resource languages\u2014perform poorly on such code-switched inputs. There is no dedicated benchmark to systematically study and improve recognition of English medical terminology within Vietnamese speech, particularly for clinical or medical applications where accuracy is critical.", "method": "The authors build ViMedCSS, a 34-hour Vietnamese medical code-switching speech corpus with 16,576 utterances, each containing at least one English medical term. The English medical terms come from a curated bilingual lexicon spanning five medical topics. They then evaluate several state-of-the-art ASR models on this dataset and compare specialized fine-tuning strategies: models optimized for Vietnamese, multilingual pretrained models, and combinations of both, to see how each handles general Vietnamese segments vs. embedded English medical terms.", "result": "Vietnamese-specialized ASR models show better recognition on the general Vietnamese parts of the utterances, while multilingual pretraining is more effective at recognizing the English insertions. A hybrid approach that combines Vietnamese-optimized modeling with multilingual pretraining achieves the best trade-off between overall accuracy and accuracy on code-switched (English medical) segments.", "conclusion": "ViMedCSS is introduced as the first benchmark specifically targeting Vietnamese medical code-switching in speech. The experiments demonstrate that neither purely Vietnamese-focused nor purely multilingual ASR is sufficient; instead, combining strengths of both yields the most effective performance. This provides a concrete dataset and empirical guidance for future research on domain adaptation and multilingual ASR in low-resource, code-switched medical settings."}}
{"id": "2602.12921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12921", "abs": "https://arxiv.org/abs/2602.12921", "authors": ["Adib Sakhawat", "Shamim Ara Parveen", "Md Ruhul Amin", "Shamim Al Mahmud", "Md Saiful Islam", "Tahera Khatun"], "title": "When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms", "comment": "9 pages, 5 figures. Accepted for presentation at LREC 2026 (Language Resources and Evaluation Conference)", "summary": "Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.", "AI": {"tldr": "The paper introduces a large, expert-annotated Bengali idiom dataset and shows that current LLMs perform poorly on inferring figurative meanings compared to humans.", "motivation": "Figurative language, especially idioms, is challenging for LLMs, and this problem is more severe for low-resource languages like Bengali, which lack large, structured resources and benchmarks for evaluation.", "method": "The authors build a corpus of 10,361 Bengali idioms, each annotated with a detailed 19-field schema capturing semantic, syntactic, cultural, and religious properties via an expert consensus process. They then use this dataset to create a benchmark task of inferring figurative meaning and evaluate 30 multilingual and instruction-tuned LLMs on this task, comparing their performance to human performance.", "result": "No evaluated LLM exceeds 50% accuracy on the idiom meaning inference task, while humans reach 83.4% accuracy, revealing a substantial performance gap in figurative and culturally grounded understanding for Bengali.", "conclusion": "The new Bengali idiom dataset and benchmark expose serious limitations in current LLMs\u2019 cross-linguistic and cultural reasoning and provide essential infrastructure for future research to improve figurative language understanding and cultural grounding in Bengali and other low-resource languages."}}
{"id": "2602.12937", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12937", "abs": "https://arxiv.org/abs/2602.12937", "authors": ["Ali Mekky", "Mohamed El Zeftawy", "Lara Hassan", "Amr Keleg", "Preslav Nakov"], "title": "Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models", "comment": "Accepted at the 12th Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2026", "summary": "Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.", "AI": {"tldr": "They reframe Arabic Dialect Identification as a multi-label problem and build an automatically annotated multi-label dataset plus a BERT-based model that substantially improves performance.", "motivation": "Traditional Arabic Dialect Identification assumes each sentence belongs to exactly one dialect, but linguistically many sentences are valid in multiple dialects. Despite this, existing datasets are single-label, and there is no large-scale multi-label resource. The authors are motivated to better reflect linguistic reality, to handle dialect overlaps, and to overcome the limitations of repurposing single-label data where negative examples may actually be linguistically acceptable in other dialects.", "method": "1) Analyze why single-label ADI datasets are problematic for multi-label training, focusing on the unreliability of negative samples. 2) Automatically construct a multi-label dataset: they use GPT-4o and a set of binary dialect acceptability classifiers to assign multiple dialect labels to sentences, and then aggregate these outputs using the Arabic Level of Dialectness (ALDi) as a guide. 3) Train a BERT-based multi-label classifier, named LAHJATBERT, using curriculum learning strategies that order training according to dialectal complexity and the number of labels (label cardinality) per instance.", "result": "On the MLADI benchmark/leaderboard, their LAHJATBERT model achieves a macro F1-score of 0.69, significantly outperforming the strongest previously published system, which had a macro F1 of 0.55. This demonstrates the effectiveness of their automatic multi-label dataset construction plus their curriculum learning approach.", "conclusion": "Arabic dialect identification is better modeled as a multi-label task, but naively reusing single-label datasets is problematic because many supposed negatives are actually valid in multiple dialects. By automatically constructing a multi-label dataset via GPT-4o, acceptability classifiers, and ALDi-guided aggregation, and by training a BERT-based model with curriculum learning, they achieve state-of-the-art results on MLADI. Their released code and data provide a new resource and baseline for future work on multi-label Arabic dialect identification."}}
{"id": "2602.12966", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12966", "abs": "https://arxiv.org/abs/2602.12966", "authors": ["Yue Huang", "Zhengzhe Jiang", "Yuchen Ma", "Yu Jiang", "Xiangqi Wang", "Yujun Zhou", "Yuexing Hao", "Kehan Guo", "Pin-Yu Chen", "Stefan Feuerriegel", "Xiangliang Zhang"], "title": "ProbeLLM: Automating Principled Diagnosis of LLM Failures", "comment": null, "summary": "Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.", "AI": {"tldr": "ProbeLLM is an automated, benchmark-agnostic framework that uses hierarchical Monte Carlo Tree Search and verification tools to discover, organize, and interpret structured failure modes in large language models, going beyond isolated failure cases found by static evaluations.", "motivation": "As LLMs rapidly evolve, static benchmarks and naive dynamic test generation fail to keep pace and typically uncover only scattered, isolated mistakes. There is a need for a systematic, scalable, and principled way to explore where models go wrong, understand the structure of their weaknesses, and do so under limited probing resources while ensuring that discovered failures are reliably verifiable.", "method": "ProbeLLM frames automated probing as a hierarchical Monte Carlo Tree Search over the space of potential test cases. The search explicitly balances global exploration of new failure regions with local exploitation and refinement of recurring error patterns. It restricts itself to verifiable test instances, uses tool-augmented generation and verification to ensure failures are well-grounded, and then clusters and summarizes the resulting failures into interpretable failure modes using failure-aware embeddings and boundary-aware induction techniques.", "result": "Applied to multiple LLMs and diverse benchmarks, ProbeLLM identifies a wider variety and higher quality of failures than both static benchmarks and prior dynamic probing methods. The framework uncovers broader, cleaner, and more fine-grained maps of LLM failure regions, showing that existing evaluations miss substantial structure in model weaknesses.", "conclusion": "ProbeLLM demonstrates that automated evaluation can move from simply cataloging individual failure cases to systematically discovering and organizing structured failure modes. This shift enables more principled analysis of LLM weaknesses and provides richer, more actionable insights for improving models than traditional benchmark-centric evaluation approaches."}}
{"id": "2602.12984", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12984", "abs": "https://arxiv.org/abs/2602.12984", "authors": ["Yujiong Shen", "Yajie Yang", "Zhiheng Xi", "Binze Hu", "Huayu Sha", "Jiazheng Zhang", "Qiyuan Peng", "Junlin Shang", "Jixuan Huang", "Yutao Fan", "Jingqi Tong", "Shihan Dou", "Ming Zhang", "Lei Bai", "Zhenfei Yin", "Tao Gui", "Xingjun Ma", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents", "comment": null, "summary": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.", "AI": {"tldr": "Introduces SciAgentGym and SciAgentBench to evaluate scientific tool-use by AI agents, reveals current models\u2019 weaknesses on long-horizon scientific workflows, and proposes SciForge plus a fine-tuned 8B model that beats a much larger baseline on these tasks.", "motivation": "Existing reasoning and agent benchmarks focus mostly on language understanding or simple tool calls, but real scientific reasoning needs orchestrating many domain-specific tools over long workflows. There is no scalable, realistic environment or benchmark that systematically measures such capabilities, nor a clear path to train agents to handle complex scientific tool-use effectively.", "method": "1) Build SciAgentGym, an interactive environment with 1,780 domain-specific tools across four natural science disciplines and supporting execution infrastructure. 2) Construct SciAgentBench, a multi-tier benchmark that evaluates agentic skills from simple actions to long-horizon scientific workflows. 3) Empirically evaluate state-of-the-art models\u2019 performance on these tasks to locate failure modes in scientific tool-use. 4) Propose SciForge, a data synthesis method that represents tool action spaces as dependency graphs to generate logic-aware training trajectories. 5) Fine-tune an 8B-parameter model (SciAgent-8B) on these trajectories and compare it to a much larger baseline model.", "result": "State-of-the-art models, including GPT-5, show a pronounced drop in success rates as interaction horizon increases, from 60.6% on shorter tasks to 30.9% on longer workflows, mainly due to failures in executing multi-step scientific workflows. After applying SciForge-generated training data, the fine-tuned SciAgent-8B surpasses the performance of the much larger Qwen3-VL-235B-Instruct on the SciAgentBench tasks and demonstrates cross-domain generalization of scientific tool-use skills.", "conclusion": "Complex scientific tool-use and long-horizon workflows remain a major weakness for current large models, but structured, dependency-graph-based data synthesis (SciForge) combined with a rich tool environment and benchmark (SciAgentGym/SciAgentBench) can significantly improve performance even for relatively small models. This indicates that methodical training on logic-aware tool trajectories is key to building effective autonomous scientific agents and that better training and benchmarking can matter as much as, or more than, raw model size."}}
{"id": "2602.12989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12989", "abs": "https://arxiv.org/abs/2602.12989", "authors": ["Ma\u00ebl Houbre", "Florian Boudin", "Beatrice Daille"], "title": "Evaluating the Homogeneity of Keyphrase Prediction Models", "comment": "Accepted to LREC 2026", "summary": "Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.", "AI": {"tldr": "The paper evaluates how consistently keyphrase prediction models assign the same keyphrases to similar documents and finds that generative models with absent-keyphrase capability are not necessarily more consistent than extraction-based methods.", "motivation": "Keyphrase generation models can produce absent keyphrases (not present in the text), theoretically allowing more consistent indexing of similar documents than extraction-based models. However, existing benchmarks do not evaluate how homogeneous or consistent model predictions are across similar documents, leaving a gap in understanding whether absent keyphrase generation really improves indexing consistency.", "method": "The authors propose a new evaluation method and benchmark focused on homogeneity of keyphrase prediction. They compare keyphrase extraction models with generative keyphrase models, specifically analyzing whether the ability to generate absent keyphrases improves or harms homogeneity\u2014i.e., the likelihood of predicting the same keyphrases for documents on the same topics. Data, code, and prompts are released for reproducibility.", "result": "The study finds that keyphrase extraction methods perform competitively with generative models in terms of homogeneity. Furthermore, the capability of generative models to produce absent keyphrases can actually reduce homogeneity, leading to less consistent keyphrase assignments across similar documents.", "conclusion": "Contrary to intuition, the ability to generate absent keyphrases does not guarantee more homogeneous indexing and may even negatively affect it. Extraction-based approaches can match or outperform generative models in homogeneity, suggesting that future work should explicitly consider and measure homogeneity when designing and evaluating keyphrase prediction systems."}}
{"id": "2602.12996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12996", "abs": "https://arxiv.org/abs/2602.12996", "authors": ["Hao Chen", "Ye He", "Yuchun Fan", "Yukun Yan", "Zhenghao Liu", "Qingfu Zhu", "Maosong Sun", "Wanxiang Che"], "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models", "comment": null, "summary": "Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.", "AI": {"tldr": "The paper proposes a meta-cognitive framework that improves LLMs on knowledge-intensive tasks by explicitly modeling and correcting gaps between what the model \u201cknows\u201d and how confident it is.", "motivation": "Existing knowledge augmentation methods assume that better task performance directly reflects better internal knowledge, ignoring mismatches between confidence and correctness that cause overconfident mistakes or unnecessary uncertainty. This limits reliability and trustworthiness in knowledge-intensive applications.", "method": "The authors design a meta-cognitive framework that uses internal cognitive signals of an LLM to segment its knowledge into three regions: mastered (high accuracy, high confidence), confused (low accuracy, high or unstable confidence), and missing (low confidence, low knowledge). Based on this partition, the system applies differentiated knowledge augmentation strategies tailored to each region. They also add a cognitive consistency mechanism that explicitly aligns the model\u2019s expressed certainty with its true accuracy, calibrating when the model should be confident or uncertain.", "result": "Across extensive experiments on knowledge-intensive benchmarks, the proposed framework yields better performance than strong existing knowledge augmentation baselines, and achieves better calibration, meaning the model is more accurate when confident and more appropriately uncertain when it lacks knowledge.", "conclusion": "Incorporating meta-cognitive modeling into knowledge augmentation makes LLMs not just more knowledgeable, but also more self-aware of their knowledge limits. By explicitly managing mastered, confused, and missing knowledge regions and enforcing cognitive consistency, the approach produces models that both perform better and more reliably distinguish what they know from what they do not."}}
{"id": "2602.13047", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13047", "abs": "https://arxiv.org/abs/2602.13047", "authors": ["Madhurananda Pahar", "Caitlin Illingworth", "Dorota Braun", "Bahman Mirheidari", "Lise Sproson", "Daniel Blackburn", "Heidi Christensen"], "title": "Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech", "comment": null, "summary": "Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.", "AI": {"tldr": "The paper evaluates whether AI speech-based tools for detecting cognitive decline are biased against multilingual speakers from ethnic minority backgrounds in the UK and finds that current models are indeed biased and unreliable for diagnostic use in these groups.", "motivation": "Early signs of dementia and mild cognitive impairment (MCI) can be detected from conversational speech using AI. However, in the UK, dementia is expected to grow fastest among ethnic minority groups, many of whom are multilingual and speak English with non-native accents. If AI tools are biased against these populations, they could lead to misdiagnosis, reduced trust, and inequitable healthcare. The paper is motivated by the need to test the fairness and trustworthiness of existing AI models for cognitive assessment, especially for multilingual speakers from ethnic minority communities whose speech patterns and accents may differ from the monolingual, majority-English speakers on whom most models are trained.", "method": "The study recruited two groups: monolingual English speakers from across the UK and multilingual English speakers from four community centres in Sheffield and Bradford. The multilingual participants also spoke Somali, Chinese, or South Asian languages and had non-native English accents, and they were further categorised by two regional English accents (West and South Yorkshire). The authors evaluated several AI components: automatic speech recognition (ASR) systems and downstream classification/regression models for cognitive status using acoustic and linguistic features. They compared performance and error patterns across groups, tasks (memory, fluency, reading), and training data sources, including models trained on the DementiaBank dataset, to assess whether there was systematic bias against multilingual speakers and specific accents.", "result": "ASR systems did not show significant performance differences between monolingual and multilingual groups, suggesting relatively fair transcription quality. However, the classification and regression models used to predict cognitive status from acoustic and linguistic features did show systematic bias. Multilingual speakers, particularly in memory, fluency, and reading tasks, were more often misclassified as cognitively impaired or as having more severe decline. This bias was stronger when models were trained on the widely used DementiaBank dataset, implying that dataset composition contributes to unfairness. Speakers with a South Yorkshire accent were especially prone to being classified as having more severe cognitive decline.", "conclusion": "The study concludes that, despite strong aggregate performance metrics, current AI speech-based tools for detecting cognitive decline are biased against multilingual individuals from ethnic minority backgrounds in the UK and against certain regional accents. These tools are therefore not yet trustworthy or suitable for clinical diagnostic use in these populations. The authors position this as a first demonstration of such bias in this context and propose future work to develop more generalisable, bias-mitigated models that can support equitable cognitive assessment across diverse linguistic and accent groups."}}
{"id": "2602.13059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13059", "abs": "https://arxiv.org/abs/2602.13059", "authors": ["Tejas Anvekar", "Junha Park", "Rajat Jha", "Devanshu Gupta", "Poojah Ganesan", "Puneeth Mathur", "Vivek Gupta"], "title": "TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution", "comment": null, "summary": "Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.", "AI": {"tldr": "TraceBack is a modular multi-agent framework that provides cell-level attribution for table question answering, along with a new benchmark (CITEBench) and an automatic attribution metric (FairScore).", "motivation": "Most table QA systems focus on answer correctness but do not clearly show which exact table cells support the answer, making it hard to trust or verify them, especially in high-stakes use cases. There is also a lack of standardized datasets and automatic metrics to evaluate such fine-grained attribution. The paper aims to provide methods and resources to make table QA more transparent, interpretable, and reliably evaluated.", "method": "The authors propose TraceBack, a modular multi-agent framework for cell-level attribution in single-table QA. The pipeline (1) prunes the table to rows and columns relevant to the question, (2) decomposes complex questions into semantically coherent sub-questions, and (3) aligns each answer span, including intermediate reasoning steps, with the table cells that explicitly or implicitly support it. To evaluate attribution, they construct CITEBench by adding phrase-to-cell alignment annotations on top of existing table QA datasets (ToTTo, FetaQA, AITQA). They also introduce FairScore, a reference-less evaluation metric that derives atomic facts from predicted cells and answers and compares them to estimate attribution precision and recall without needing human-labeled supporting cells.", "result": "Experiments across multiple datasets and attribution granularities show that TraceBack achieves substantially better cell-level attribution performance than strong existing baselines. FairScore correlates well with human attribution judgments and maintains the relative ranking of different methods, indicating it can serve as a reliable automatic evaluation measure without expensive manual annotations.", "conclusion": "TraceBack makes table QA more transparent by producing fine-grained, cell-level evidence for answers, and it scales through a modular multi-agent design. CITEBench provides a standardized benchmark for evaluating attribution quality, and FairScore offers an automatic, reference-less metric that aligns closely with human judgments. Together, these contributions advance interpretable and scalable evaluation of table-based QA systems."}}
{"id": "2602.13084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13084", "abs": "https://arxiv.org/abs/2602.13084", "authors": ["Silin Du", "Manqing Xin", "Raymond Jia Wang"], "title": "Exploring a New Competency Modeling Process with Large Language Models", "comment": null, "summary": "Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.", "AI": {"tldr": "The paper introduces a data-driven, LLM-based pipeline to build and validate competency models that outperform traditional expert-only methods in practicality, consistency, and predictiveness.", "motivation": "Traditional competency modeling in HR depends on experts manually coding interviews, which is expensive, subjective, and hard to reproduce or validate. With the availability of large language models and embedding techniques, there is an opportunity to formalize and partially automate expert reasoning, improving scalability, transparency, and empirical validation of competency frameworks used for talent management decisions.", "method": "The authors design a new workflow that decomposes expert competency modeling practices into computational modules powered by LLMs. First, LLMs extract behavioral and psychological descriptors from raw text such as interview transcripts. Then, embedding-based similarity is used to map these descriptors onto predefined competency libraries. A learnable parameter is introduced to dynamically weight behavioral versus psychological information. Finally, the authors propose an offline evaluation protocol that uses historical organizational data to compare different model variants and tune parameters, without needing new large-scale data collection.", "result": "In a real-world deployment at a software outsourcing firm, the proposed system shows strong predictive validity (competency scores relate well to performance or relevant outcomes), consistency across different competency libraries, and robustness of the inferred competency structure. These empirical results suggest that the LLM-based pipeline can reliably reproduce and systematize what human experts do, with better scalability and objectivity.", "conclusion": "The study demonstrates that competency modeling can be re-engineered as a rigorous, data-driven analytical process by embedding expert logic into an LLM-based pipeline. This transforms a qualitative, expert-dependent activity into one that is more transparent, reproducible, and empirically evaluable, potentially improving talent selection, development, and evaluation in organizations."}}
{"id": "2602.13102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13102", "abs": "https://arxiv.org/abs/2602.13102", "authors": ["Kais Allkivi"], "title": "Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts", "comment": null, "summary": "Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.", "AI": {"tldr": "The paper uses NLP-based features to automatically classify Estonian L2 exam writings by proficiency level and integrate the models into an open-source learning platform.", "motivation": "There is a need for automated assessment and feedback tools based on authentic learner language, and for research that jointly advances both language testing practice and second language acquisition theory. Existing work often optimizes classification accuracy without focusing on interpretability and generalizability for language testing, and there is little research on Estonian learner language in this context. The authors want to see whether careful, linguistically motivated feature selection can create models that are both accurate and explainable, and that capture genuine proficiency-related development rather than task effects.", "method": "The authors collected Estonian proficiency exam writings from levels A2\u2013C1. They extracted multiple feature types\u2014lexical, morphological, surface-level, and error-based indicators\u2014from the texts. They then designed two kinds of machine learning classification models: (1) models trained only on pre-selected, linguistically interpretable features intended to reflect complexity and correctness, and (2) models that also allowed additional features, including more task-specific ones. They compared classification accuracy and the stability of performance across different text types, and tested temporal generalizability by applying the models to an earlier exam sample from 7\u201310 years before. Finally, they integrated the resulting models into an open-source Estonian language learning platform for automated writing evaluation.", "result": "Models using only the pre-selected linguistic features achieved accuracy comparable to models with a broader feature set, with the best classifiers reaching about 0.9 accuracy on the main dataset. Importantly, the restricted feature set reduced variation in classification performance across text types, suggesting better robustness and reduced task dependence. When evaluated on earlier exam data from 7\u201310 years prior, the models showed that learner writings have become more complex over time, yet classification accuracy with some feature sets remained around 0.8, indicating good temporal generalizability despite changes in writing characteristics.", "conclusion": "Linguistically motivated, carefully selected NLP features can produce proficiency classification models that are both accurate and more generalizable across tasks and time than models that rely on broader, less constrained feature sets. These features appear to track underlying growth in complexity and correctness in Estonian L2 writing rather than superficial task properties. The approach is practically useful, as demonstrated by its deployment in an open-source Estonian language learning environment, and theoretically informative for understanding longitudinal changes in learner language complexity."}}
{"id": "2602.13110", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13110", "abs": "https://arxiv.org/abs/2602.13110", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging", "comment": null, "summary": "Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\u03b1$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $\u03b1= 0.10$, \\textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na\u00efve baselines, \\textsc{Scope} accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.", "AI": {"tldr": "They propose SCOPE, a statistically calibrated framework that lets LLM judges abstain so that the remaining pairwise judgments meet a target error rate, using a new uncertainty metric (BPE) that is robust to response order.", "motivation": "LLM-based automatic evaluation is cheaper and scalable compared to human labeling, but current LLM judges are miscalibrated and biased, especially in pairwise preference tasks. There is no guarantee on their error rates, and existing uncertainty scores are weak and sensitive to the position/order of responses, limiting their reliability as evaluation tools.", "method": "They build SCOPE, a selective prediction framework based on conformal prediction for pairwise LLM judgments. Under an exchangeability assumption, SCOPE learns a threshold on an uncertainty score so that judgments whose uncertainty is below the threshold are accepted and the rest are abstained, with finite-sample guarantees that the empirical error rate on accepted judgments is below a user-specified \u03b1. To obtain a good and unbiased uncertainty score, they introduce Bidirectional Preference Entropy (BPE): they query the LLM judge twice with swapped response positions, estimate preference probabilities from both directions, aggregate them to enforce invariance to response order, and then map the aggregated probability to an entropy-based uncertainty measure used by SCOPE for selection.", "result": "On benchmarks MT-Bench, RewardBench, and Chatbot Arena, BPE yields higher-quality uncertainty estimates than standard proxies like raw logit confidence. Using BPE as the selection signal, SCOPE consistently controls empirical risk at the desired level (e.g., for \u03b1 = 0.10, empirical risk is ~0.097\u20130.099 across datasets and LLM judge sizes) while maintaining high coverage (fraction of pairs not abstained), e.g., 0.89 coverage on RewardBench with Qwen-14B and 0.98 with Qwen-32B. Relative to na\u00efve baselines that also abstain based on simpler heuristics, SCOPE with BPE can accept up to 2.4\u00d7 more judgments on MT-Bench with Qwen-7B under the same target risk constraint.", "conclusion": "Selective, conformal prediction-based calibration of LLM judges is effective for controlling error rates in pairwise preference evaluation without sacrificing too much coverage. The proposed BPE uncertainty measure, which removes order bias by querying both directions and using entropy, significantly improves the reliability of LLM-based evaluation, enabling LLM judges to serve as high-coverage, risk-controlled substitutes for human annotators across multiple benchmarks and model scales."}}
{"id": "2602.13194", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13194", "abs": "https://arxiv.org/abs/2602.13194", "authors": ["Weishun Zhong", "Doron Sivan", "Tankut Can", "Mikhail Katkov", "Misha Tsodyks"], "title": "Semantic Chunking and the Entropy of Natural Language", "comment": "29 pages, 9 figures", "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.", "AI": {"tldr": "The paper proposes a statistical model of language that explains why English has high redundancy, matching the classic ~1 bit per character entropy rate and showing how this rate depends on corpus semantic complexity.", "motivation": "Classical estimates show printed English has an entropy rate of about 1 bit per character, much lower than the 5 bits per character of random text, implying ~80% redundancy. While modern LLMs approach this limit empirically, there is a lack of a simple, first-principles statistical model that both reproduces this entropy rate and explains how the multi-scale semantic structure of language generates such redundancy.", "method": "The authors construct a hierarchical, self-similar segmentation model of text, where text is recursively partitioned into semantically coherent chunks down to the word level. This yields a multi-scale semantic tree structure. Under this model, they analytically compute entropy contributions from different hierarchical levels and derive the overall entropy rate as a function of a single parameter that characterizes semantic complexity. They validate the model numerically using real corpora and generations from modern LLMs, checking how well the predicted statistics at each semantic level match empirical data.", "result": "The model quantitatively reproduces the multi-scale structure of real text, as observed in open text corpora and LLM outputs, across different levels of the semantic hierarchy. The analytically predicted entropy rate closely matches the classic ~1 bit per character estimate for printed English. Additionally, the derived expression shows how the entropy rate changes with semantic complexity, with the single free parameter controlling this complexity and leading to systematic increases in entropy for more complex corpora.", "conclusion": "The proposed hierarchical segmentation model provides a principled explanation for the high redundancy and low entropy rate of natural language, offering a compact account of the classic 1 bit/character figure. It demonstrates that this entropy rate is not universal but varies predictably with corpus semantic complexity, encapsulated in one free parameter, thereby linking linguistic structure, redundancy, and information-theoretic properties in a unified framework."}}
{"id": "2602.13123", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13123", "abs": "https://arxiv.org/abs/2602.13123", "authors": ["Maria Ryskina", "Matthew R. Gormley", "Kyle Mahowald", "David R. Mortensen", "Taylor Berg-Kirkpatrick", "Vivek Kulkarni"], "title": "From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media", "comment": "Accepted to LChange 2026", "summary": "Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.", "AI": {"tldr": "The paper investigates how new words (neologisms) emerge in different language domains, comparing historical written texts with Twitter using both static and contextual word embeddings, and finds that similar factors drive neology but with domain-specific strengths.", "motivation": "To understand the evolutionary pressures that shape living languages and how they differ across domains such as published writing versus social media. Previous work on English neology focused largely on historical published texts with static embeddings; this paper seeks to test whether those findings generalize to modern, conversational domains like Twitter and to richer contextual embedding models.", "method": "They extend prior distributional semantic methodology for studying word emergence by incorporating contextual embeddings in addition to static embeddings. They build and analyze a new corpus of Twitter posts and compare the factors correlated with neologism creation in this corpus to those found in a primarily historical, published-text corpus as in Ryskina et al. (2020). They specifically examine factors such as topic popularity growth and how they correlate with neology across domains.", "result": "The same main factors previously identified as correlating with neology in historical published texts are also found to hold in the Twitter domain, indicating some universality in the drivers of new word creation. However, the contribution of topic popularity growth appears to be weaker for neology on Twitter compared to published writing.", "conclusion": "Neologism formation is influenced by broadly similar distributional factors across both traditional published texts and social media, suggesting some universal pressures on language evolution. However, differences in how strongly certain factors (e.g., topic popularity growth) operate across domains likely reflect that different domains favor different mechanisms for forming new words. This supports a view of language evolution as shaped by both universal and domain-specific pressures."}}
{"id": "2602.13139", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13139", "abs": "https://arxiv.org/abs/2602.13139", "authors": ["Mariia Fedorova", "Nikolay Arefyev", "Maja Buljan", "Jind\u0159ich Helcl", "Stephan Oepen", "Egil R\u00f8nningstad", "Yves Scherrer"], "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report", "comment": "VarDial'26 workshop at the EACL 2026 conference", "summary": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.", "AI": {"tldr": "The paper improves a language identification system (OpenLID) for web-scale multilingual data, especially for closely related and low-resource languages, and releases the improved model OpenLID-v3.", "motivation": "Existing language identification tools often misclassify closely related languages and cannot reliably distinguish real language text from noise in web data, which harms the quality of language-specific and low-resource datasets.", "method": "Extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a dedicated noise label; focus development on three challenging groups of closely related languages and construct new evaluation datasets; compare the improved system (OpenLID-v3) with GlotLID and explore ensemble approaches.", "result": "OpenLID-v3 performs better than GlotLID on multiple benchmarks and on newly created datasets for closely related languages; ensemble methods further increase precision but at the cost of significantly reduced coverage, particularly for low-resource languages.", "conclusion": "Carefully extending training data, restructuring language clusters, and explicitly modeling noise leads to a more accurate and practical open language identifier for web data, though ensembles must be used cautiously due to coverage loss for low-resource languages; OpenLID-v3 is released publicly for use in multilingual data processing."}}
