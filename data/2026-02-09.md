<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: The paper proposes a quote-based, modular framework to generate more creative, persona-consistent advertising slogans with large language models, showing modest gains over strong LLM baselines.


<details>
  <summary>Details</summary>
Motivation: Conventional LLM-based slogan generators tend to produce repetitive, bland, and obviously machine-generated outputs that neither convey a distinctive brand persona nor sustain audience interest, especially under advertising fatigue. There is a need for methods that inject more creativity, rhetorical richness, and insight into slogans while keeping them familiar and accessible.

Method: The authors introduce a modular pipeline that reframes slogan generation as recontextualizing persona-relevant famous quotes. The framework breaks the task into interpretable stages: (1) quote matching to find famous quotes aligned with the brand persona, (2) structural decomposition of the quote into its rhetorical or syntactic template, (3) vocabulary replacement to adapt key terms to the brand domain, and (4) remix generation to refine and finalize the slogan. This structure leverages famous quotes as rich rhetorical scaffolds rather than generating slogans from scratch.

Result: Through both automatic metrics and human evaluations, the proposed framework achieves marginal but consistent gains over three strong LLM baselines in terms of diversity of outputs, novelty, emotional impact, and overall human preference. The results indicate that quote-based recontextualization can slightly improve perceived quality and creativity of slogans without sacrificing clarity.

Conclusion: Reusing persona-relevant famous quotes as structured templates, via a modular and interpretable pipeline, is a promising strategy for slogan generation. It better balances novelty and familiarity and yields slogans that humans slightly prefer over state-of-the-art LLM baselines. The approach highlights the value of structured, quote-driven generation as a complement to end-to-end LLM generation for creative advertising tasks.

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [2] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: They propose RMCD, a new decoding method for retrieval-augmented LVLMs that relevance-weights and contrastively combines answers from multiple retrieved contexts, improving knowledge-intensive visual QA without extra training.


<details>
  <summary>Details</summary>
Motivation: LVLMs are strong but often lack fine-grained, entity-level knowledge. RAG helps by retrieving external knowledge, but existing decoding strategies don’t properly exploit multiple retrieved passages and are sensitive to noisy or partially irrelevant contexts. The authors aim to design a decoding procedure that better aggregates multiple relevant contexts and mitigates the harm of irrelevant ones, improving reliability and performance on knowledge-intensive visual tasks.

Method: Introduce Relevance-aware Multi-context Contrastive Decoding (RMCD) for RAG-enhanced LVLMs. For each retrieved context, the LVLM produces an output distribution. RMCD estimates the relevance of each context to the question and uses these relevance scores as weights to form a final prediction by contrastively combining the per-context outputs. This encourages token predictions supported by highly relevant contexts while down-weighting predictions from irrelevant ones. RMCD is implemented purely at decoding time and can be plugged into existing LVLMs without extra training.

Result: On three knowledge-intensive visual QA benchmarks, RMCD consistently outperforms other RAG decoding baselines across multiple LVLM backbones. It yields the best overall accuracy and maintains strong performance even when retrieval quality varies from weak to strong, showing robustness to noisy or imperfect retrieval results.

Conclusion: RMCD is an effective, training-free decoding strategy for RAG in LVLMs that better utilizes multiple retrieved contexts by relevance-aware contrastive aggregation. It improves performance and robustness on knowledge-intensive visual QA and can be easily integrated into existing LVLM pipelines by replacing their decoding module.

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [3] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: The paper proposes CAST, a character-and-scene based dual memory architecture that better models episodic memory for agents and outperforms existing memory systems on conversational tasks.


<details>
  <summary>Details</summary>
Motivation: Existing agent memory systems mainly model semantic memory using structures like key-value stores, vectors, or graphs, which makes it difficult to represent and retrieve coherent, event-based episodic memories grounded in who, when, and where. The authors are motivated to build a memory system that more closely mirrors human episodic memory to improve performance on tasks requiring recall of events over time, especially in open and time-sensitive conversations.

Method: The authors design CAST, an architecture inspired by dramatic theory. CAST constructs 3D scenes parameterized by time, place, and topic, and organizes these scenes into character profiles that summarize events associated with each character to represent episodic memory. In addition, they implement a complementary graph-based semantic memory. Together, these form a dual memory system where episodic and semantic components support each other. They evaluate CAST on multiple conversational datasets, focusing on questions that require episodic recall.

Result: CAST achieves on average an 8.11% absolute improvement in F1 score and a 10.21% improvement in J (LLM-as-a-Judge) metrics compared with baseline memory architectures across various datasets. Gains are particularly strong on open-ended and time-sensitive conversational questions that rely heavily on episodic recall.

Conclusion: The study concludes that a character-and-scene based episodic memory, combined with a graph-based semantic memory, provides a more human-like and effective memory architecture for agents. This dual memory design significantly improves performance on tasks requiring coherent event recall, indicating that modeling episodic structure explicitly is beneficial for conversational AI systems.

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [4] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: Survey of memory for foundation AI agents and its role in real-world utility.


<details>
  <summary>Details</summary>
Motivation: AI research is shifting from beating benchmarks to achieving real utility in complex, long-horizon, user-dependent environments, where agents must cope with exploding context and manage large amounts of interaction history. Memory is emerging as a key mechanism to close this utility gap, evidenced by a surge of related work.

Method: Provide a survey and unified framework for understanding memory in foundation agents along three axes: (1) memory substrate (internal vs. external), (2) cognitive mechanisms (episodic, semantic, sensory, working, procedural), and (3) memory subject (agent-centric vs. user-centric). Analyze how these forms of memory are implemented across different agent architectures, discuss learning policies over memory operations, and review existing benchmarks and metrics.

Result: The survey organizes the fast-growing literature on agent memory into a coherent taxonomy, maps memory types to agent topologies and operational policies, and synthesizes current evaluation protocols and metrics for measuring memory utility in agents.

Conclusion: Memory is a central component for making foundation agents useful in real-world, long-horizon settings. The paper’s framework clarifies how different kinds of memory can be structured and used, exposes gaps in current implementations and evaluations, and sets out open challenges and directions for future research on scalable, effective memory systems for agents.

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [5] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

TL;DR: PersonaPlex is a duplex speech model that uses both role prompts and voice cloning to create fast, natural, role-specific, and voice-personalized speech interactions, outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing duplex speech models can converse naturally with low latency but are limited to a fixed assistant-like role and a single voice, making them unsuitable for structured, role-driven scenarios (e.g., multi-role customer service) and for personalized user experiences. There is a need for a system that can flexibly follow different roles and voices while maintaining real-time, natural speech interaction quality.

Method: The authors propose PersonaPlex, a duplex conversational speech model that uses hybrid system prompts: role conditioning via text prompts and voice conditioning via voice cloning from speech samples. They construct a large-scale synthetic training corpus of paired role prompts and user–agent conversations using open-source LLMs for text generation and TTS models for speech. They also extend the Full-Duplex-Bench benchmark to include multi-role customer service scenarios to test role adherence in realistic conditions.

Result: PersonaPlex demonstrates strong alignment with specified roles and voices, and provides natural, responsive duplex speech interactions. In experiments on the extended Full-Duplex-Bench, it outperforms state-of-the-art duplex speech models and hybrid LLM+speech pipelines on metrics including role adherence, speaker similarity, latency, and perceived naturalness of the conversation.

Conclusion: Hybrid prompting that combines explicit role text prompts with voice cloning, trained on a large synthetic corpus, enables a duplex speech model to deliver flexible, role-conditioned, and voice-personalized interactions. PersonaPlex not only improves adherence to specified roles and voice similarity but also maintains or improves latency and naturalness, making it more suitable for real-world, multi-role applications such as customer service compared to prior systems.

Abstract: Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [6] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: The paper presents a framework that learns to assess research novelty in a human-aligned, literature-aware way using peer-review data and structured comparisons to prior work.


<details>
  <summary>Details</summary>
Motivation: Novelty assessment in peer review is critical but highly subjective and often based on incomplete and implicit comparisons with prior literature. There is a need for a more objective, consistent, and literature-grounded way to evaluate how novel a manuscript is.

Method: The authors collect about 80,000 peer-review reports from top AI conferences that contain explicit novelty annotations. They fine-tune a large language model on this data so it can mimic how reviewers judge novelty. For a new manuscript, their system extracts structured representations of its ideas, methods, and claims, then retrieves semantically similar prior papers and builds a similarity graph over these concepts. The model conditions on this structured comparison evidence to output novelty scores and natural-language justifications.

Result: The framework yields calibrated novelty scores and human-like explanatory assessments. Compared to existing automated approaches, it reduces overestimation of novelty and provides more consistent evaluations that are better aligned with human reviewers.

Conclusion: Learning from large-scale peer-review data and grounding judgments in structured, concept-level comparisons to related work enables more reliable, calibrated, and interpretable automated novelty assessment for research manuscripts.

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [7] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: The paper introduces a new metric and significance test to quantify and compare polarization among different annotator groups, particularly for subjective tasks like toxicity and hate-speech detection, enabling analysis across imbalanced groups and multi-label settings.


<details>
  <summary>Details</summary>
Motivation: Existing annotation agreement metrics are inadequate for analyzing disagreements between sociodemographic or ideological groups, especially when groups are imbalanced, when annotation is subjective, and when there are multiple labels per instance. This limitation hinders fair and accurate understanding of how different groups perceive sensitive content such as hate speech and toxicity. The paper is motivated by the need for a principled, quantitative way to attribute and compare polarization across groups in such settings.

Method: The authors define a new polarization metric that can attribute disagreement to specific annotator subgroups and support multi-label and multi-annotation scenarios. They pair this with a statistical significance test to determine whether observed polarization between groups is meaningful rather than due to noise. They then apply the metric to four real-world datasets (three hate speech, one toxicity) with rich annotator metadata such as race, religion, and education level, analyzing how much each group contributes to overall disagreement. They also perform sampling-based or analytical calculations to estimate the minimum number of annotators required for stable polarization estimates and release a Python library implementing the methods.

Result: Applying the metric shows that annotator race is a strong and consistent source of polarization, especially for hate speech tasks. Religious annotators tend to agree with each other but disagree with non-religious annotators; this pattern decreases and eventually reverses among irreligious annotators. Less educated annotators exhibit higher subjectivity and greater disagreement, whereas more educated annotators show higher within-group agreement. The method also yields quantitative recommendations on the number of annotators needed to obtain robust polarization measurements.

Conclusion: The proposed metric and significance test provide a robust tool for quantifying and attributing polarization across annotator subgroups, even under group size imbalance and multi-label, multi-annotation conditions typical of subjective tasks like hate speech and toxicity detection. The results echo and refine existing findings about how race, religion, and education influence annotation behavior. The work offers practical guidance on annotation design (e.g., annotator counts and subgroup coverage) and supports more nuanced, group-aware evaluation of datasets and models, facilitated by an open-source Python implementation.

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [8] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: The paper introduces COVER, a cache-based verification method that speeds up parallel diffusion decoding for language models by reducing unnecessary token revisions while preserving output quality.


<details>
  <summary>Details</summary>
Motivation: Parallel diffusion decoding can accelerate inference by predicting multiple tokens per step, but naive parallelism often harms generation quality. Revocable decoding helps by allowing earlier tokens to be rechecked and revised. However, existing verification schemes cause flip-flop oscillations where tokens are repeatedly masked and unmasked without actual change, wasting computation and slowing decoding. The paper aims to remove this inefficiency while maintaining or improving generation quality.

Method: The authors propose COVER (Cache Override Verification for Efficient Revision). COVER performs leave-one-out verification and stable drafting in a single forward pass by manipulating the KV cache. It creates two attention views: for selected seed positions being verified, their tokens are masked so the model can reassess them; for all other positions, the original cached key-value states are injected so context is preserved. A closed-form diagonal correction is applied to prevent a position from attending to its own masked representation (avoiding self leakage). COVER also introduces a stability-aware scoring scheme to prioritize which tokens (seeds) to verify based on uncertainty, influence on future tokens, and cache drift, and it dynamically adapts how many seeds are verified each step.

Result: Experiments across multiple benchmarks show that COVER substantially reduces redundant remask/revision cycles compared to existing revocable decoding methods. This leads to fewer wasted revision steps and more stable token drafts, providing significant speedups in decoding time while maintaining comparable or improved text quality metrics.

Conclusion: COVER effectively addresses the flip-flop oscillation problem in parallel diffusion decoding by using KV cache override and stability-aware seed selection. It enables more efficient revocable decoding, reducing unnecessary revisions and accelerating inference without sacrificing output quality, demonstrating a practical path to faster diffusion language model decoding.

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [9] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: The paper shows that post-training quantization of large language models can substantially change which groups are targeted by bias without changing aggregate bias metrics, revealing hidden, quantization-induced bias flips.


<details>
  <summary>Details</summary>
Motivation: To understand how post-training quantization, used to reduce the computational cost of large language models, affects their social bias behavior beyond what is visible from aggregate bias scores, and to reveal whether compression introduces new, subtle fairness risks.

Method: They conduct a large-scale empirical study of 50 quantized language models on PostTrainingBiasBench, a unified benchmark that aggregates 13 closed- and open-ended bias datasets. They systematically vary quantization strength (e.g., 4-bit vs 8-bit), analyze how individual responses flip between biased and unbiased labels after quantization, relate these flips to model uncertainty, and measure group-specific changes in bias and behavioral robustness across model sizes and families.

Result: They discover quantization-induced masked bias flipping: up to 21% of responses switch between biased and unbiased after quantization even when aggregate bias scores remain unchanged. Responses with high model uncertainty are 3–11 times more likely to flip than confident ones, and stronger quantization (4-bit) produces 4–6 times more behavioral changes than 8-bit. These flips lead to asymmetric group impacts, with bias worsening by up to 18.6% for some groups and improving by 14.1% for others; larger models do not show consistent robustness advantages, and group-level shifts vary unpredictably across model families.

Conclusion: Post-training quantization is not bias-neutral: it fundamentally reshapes which groups experience more or less biased behavior, in ways that aggregate metrics can hide. Therefore, compressed models require dedicated, post-quantization bias evaluation and targeted interventions, rather than assuming that pre-quantization bias properties or overall scores are preserved.

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [10] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: The paper introduces BenchMarker, a toolkit that uses LLM judges and education best practices to automatically detect and analyze flaws in multiple-choice QA benchmarks, showing that such flaws significantly distort NLP evaluation results.


<details>
  <summary>Details</summary>
Motivation: Many multiple-choice QA benchmarks in NLP suffer from quality issues such as data contamination, exploitable shortcuts, and poor question writing that are rarely audited systematically. These flaws can inflate or deflate model performance and even reorder system rankings, undermining claims about progress. The authors are motivated by the need for a principled, scalable way to assess and improve MCQA benchmark quality, drawing on established methodologies from educational measurement.

Method: The authors design BenchMarker, a toolkit that uses large language models as automatic judges to detect three types of flaws in MCQs: (1) contamination, identified by checking whether the exact items appear online; (2) shortcuts, detected via cues and artifacts in answer options that allow guessing without real understanding; and (3) writing errors, evaluated via a 19-rule rubric from education research that covers structural and grammatical quality. They validate BenchMarker against human annotations and then apply it to a set of 12 existing MCQA benchmarks to audit their quality. They further analyze how detected flaws correlate with model accuracies and ranking stability, and examine the side effects of existing benchmark repair efforts such as using LLM-generated distractors.

Result: BenchMarker successfully identifies contamination, shortcut cues, and writing issues in existing MCQA datasets, with validation showing good agreement with human annotators. The large-scale audit of 12 benchmarks reveals that contaminated questions tend to artificially raise model accuracy, whereas writing errors tend to reduce accuracy and can shift model rankings more than expected by random variation. The analysis also shows that prior attempts to repair benchmarks, for example by inserting LLM-generated distractors to make questions harder, often fix the targeted issue but simultaneously introduce new flaws such as implausible distractors or multiple correct answers.

Conclusion: Flaws in multiple-choice QA items, including contamination, shortcuts, and poor writing, substantially degrade the reliability of NLP evaluation and can mislead conclusions about model capabilities. However, established principles from education and psychometrics can be adapted, via tools like BenchMarker, to systematically diagnose and improve benchmark quality. The authors release BenchMarker as an open toolkit to encourage more rigorous MCQA benchmark design and closer collaboration between NLP and educational measurement communities.

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [11] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: The paper investigates how easily large language models can be steered to adopt a specific stance on controversial (polemic) questions by only presenting one‑sided arguments, and shows that such opinion steering is robust across question formulations, presentation styles, and models.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to answer controversial questions where a balanced, multi‑perspective response is socially desirable. However, these models sometimes adopt a single viewpoint or refuse to answer, raising concerns about bias and manipulability. The authors want to understand whether and how simply showing one‑sided supporting arguments can systematically steer LLMs’ opinions on polemic topics, and how robust this effect is across different conditions.

Method: The authors design a systematic study with three main dimensions: (i) the stance ultimately induced in the LLM’s answer, (ii) the formulation of the polemic question, and (iii) the way the supporting arguments are displayed. They build a small curated dataset of controversial questions and associated arguments representing different stances. They then prompt diverse LLMs with these questions plus only one‑sided arguments, varying the number of arguments, the exact wording of questions, and the presentation format of arguments. They measure how often and how strongly the models’ answers align with the provided stance and how this changes when switching to alternative arguments.

Result: Across multiple LLMs and settings, the models’ responses reliably shift toward the stance supported by the one‑sided arguments, showing clear opinion steering. This effect persists across different question phrasings, argument presentations, and numbers of arguments. When the arguments are switched to support a different stance, the degree of steering drops, confirming that the specific one‑sided evidence strongly influences the resulting opinion. The behavior is observed broadly, not only for a single model or topic, suggesting a general vulnerability to such steering.

Conclusion: The paper concludes that LLMs are susceptible to simple, intuitive opinion steering on polemic questions: providing only one‑sided arguments is often enough to induce a specific stance, even when alternative formulations or setups are used. This reveals a robustness of steering effects rather than of neutrality, raising concerns about how easily LLM outputs on controversial issues can be manipulated by selective context. The authors highlight the need for better safeguards and methods to promote balanced reasoning when LLMs are exposed to biased or incomplete information.

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [12] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: The paper proposes using Recurrence Quantification Analysis (RQA) on hidden token embeddings to study and predict reasoning effort in large reasoning models, offering a more informative alternative to response length.


<details>
  <summary>Details</summary>
Motivation: Current practice often uses response length as a crude proxy for reasoning effort in large reasoning models, but this ignores the internal dynamics, quality, and structure of the Chain-of-Thought (CoT). Analysing generated text directly is becoming impractical and unreliable as models grow and outputs lengthen. The authors want a principled, non-textual way to characterise and quantify the dynamics of token generation and reasoning complexity at test time.

Method: They treat token generation as a dynamical system: for each generated token, they extract the model’s hidden representation, forming a trajectory in embedding space. They then apply Recurrence Quantification Analysis (RQA) to these trajectories, computing metrics like Determinism and Laminarity, which measure structured repetition and stalling in latent states. Using 3,600 generation traces from the DeepSeek-R1-Distill model, they correlate these RQA features with task complexity and compare against using response length alone.

Result: RQA-derived metrics capture aspects of model behaviour—such as repetition patterns and stalling—that are not visible in response length. When used to predict task complexity, RQA features significantly improve performance, yielding an 8% gain over response-length-based prediction. This shows that internal latent dynamics carry richer information about reasoning effort and difficulty than simple length statistics.

Conclusion: RQA provides a principled, non-textual framework for analysing the latent dynamics of token generation in reasoning models at test time. It reveals signals of reasoning structure and complexity that are missed by response length, and yields better prediction of task complexity. The authors position RQA as a useful tool for studying and understanding test-time scaling and reasoning behaviour in large models via their hidden-state trajectories rather than their surface text alone.

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [13] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: The paper introduces MPIB, a benchmark to evaluate how vulnerable medical LLM and RAG systems are to prompt injection attacks, focusing on actual clinical harm rather than just attack success.


<details>
  <summary>Details</summary>
Motivation: LLMs and RAG systems are entering clinical workflows, but prompt injection attacks can cause unsafe or misleading medical recommendations. Existing evaluations often emphasize whether the attack overrides system instructions (attack success) rather than whether it leads to clinically harmful outcomes. There is a need for a clinically grounded, systematic way to measure real patient-safety risks from prompt injection, including both direct user-input attacks and indirect attacks via retrieved documents in RAG.

Method: The authors create the Medical Prompt Injection Benchmark (MPIB), a dataset and benchmark suite of 9,697 curated examples spanning clinically grounded tasks. They apply multi-stage quality control and clinical safety linting to construct adversarial scenarios with both direct prompt injections and indirect, RAG-mediated injections. They define and use a new metric, Clinical Harm Event Rate (CHER), which quantifies high-severity clinical harm events according to a medical harm taxonomy, and report it together with the traditional Attack Success Rate (ASR). They then evaluate multiple baseline LLMs and different defense configurations on MPIB, comparing performance when adversarial instructions are placed in the user query versus the retrieved context.

Result: Experiments show that high attack success (ASR) does not always translate into high clinical harm (CHER); the two metrics can diverge significantly. They also find that a model’s robustness is heavily influenced by where the adversarial instructions appear: in the user’s direct query or in the RAG-retrieved context. Different models and defenses behave differently under these two injection channels.

Conclusion: MPIB provides a standardized, clinically grounded benchmark and metrics (especially CHER) for assessing prompt injection robustness in medical LLM and RAG systems. By releasing the dataset, code, adversarial baselines, and documentation, the authors aim to enable reproducible, systematic research on clinical prompt injection and to guide the design of safer medical AI systems that are evaluated on actual patient-risk, not just instruction-following under attack.

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [14] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: VowelPrompt augments LLM-based speech emotion recognition with interpretable vowel-level prosodic descriptions and two-stage RL-enhanced adaptation, yielding better accuracy and explanations across datasets and languages.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based emotion recognition systems mainly rely on text transcriptions and overlook fine-grained prosodic cues, especially those tied to vowels, which reduces both performance and interpretability in multimodal emotion understanding.

Method: The authors design VowelPrompt, which time-aligns vowel segments in speech and extracts pitch, energy, and duration features, then verbalizes these features into natural-language descriptions that are fed, alongside lexical content, into an LLM. They further employ a two-stage training pipeline: supervised fine-tuning followed by Reinforcement Learning with Verifiable Reward (via Group Relative Policy Optimization) to improve reasoning, structured outputs, and robustness across domains and speakers.

Result: Across multiple benchmark datasets and scenarios including zero-shot, fine-tuned, cross-domain, and cross-linguistic settings, VowelPrompt achieves consistently higher accuracy than state-of-the-art emotion recognition baselines while also producing human-interpretable, prosody-grounded explanations.

Conclusion: Incorporating linguistically motivated, vowel-level prosodic descriptions into LLM prompts and optimizing the model via SFT plus RLVR substantially improves both performance and interpretability for speech emotion recognition, and generalizes well across different domains, speakers, and languages.

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [15] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: The paper introduces Consequence-Based Utility, a method for evaluating research-level math solutions by testing how well they help solve related, verifiable problems, and shows it ranks correct solutions better than existing evaluators.


<details>
  <summary>Details</summary>
Motivation: Reasoning models can now generate plausible research-level math solutions, but checking their correctness is difficult and requires expert time. Existing automatic evaluators (reward models, LLM judges) are not reliable enough, especially when solutions are long, technical, and partially correct. The authors want an evaluation method that reflects whether a solution truly contains useful methodological insight, without relying on external oracles or human experts.

Method: They propose Consequence-Based Utility, which treats each candidate solution as an in-context exemplar and measures its utility by how much it helps a model solve a neighborhood of related, but verifiable, math questions. For each candidate solution, they plug it into prompts for solving these related problems and compute a performance-based score, using that as the candidate’s utility. They then rank candidate solutions by these utility scores. They compare this approach against reward models, generative reward models, and LLM-judge-based evaluators.

Result: On a new dataset of research-level math problems, each with one expert solution and nine LLM-generated solutions, Consequence-Based Utility achieves better ranking quality than competing evaluators. For GPT-OSS-120B, it improves Acc@1 of selecting the expert solution from 67.2 to 76.3 and AUC from 71.4 to 79.6. For GPT-OSS-20B, it raises AUC from 69.0 to 79.2. It also exhibits a larger solver-evaluator gap than LLM judges, meaning it maintains clearer separation between correct and incorrect solutions even when the base solver model frequently fails on the original tasks.

Conclusion: Using the downstream consequences of a candidate solution—its ability to improve performance on related, easily verifiable problems—is an effective, oracle-free way to evaluate research-level math solutions. Consequence-Based Utility better distinguishes correct from incorrect solutions than existing automated evaluators and remains robust even when the underlying solver is weak, indicating a promising direction for scalable verification and evaluation of complex reasoning outputs.

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [16] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: The paper analyzes why syntactic parsers and LLMs struggle with spoken code-switched language and introduces new benchmarks, metrics, and a decoupled parsing framework that significantly improve robustness and evaluation quality.


<details>
  <summary>Details</summary>
Motivation: Existing syntactic parsers and LLMs are mainly optimized for written text and standard UD assumptions, so they fail on spoken code-switching that includes disfluencies, repetition, ellipsis, and discourse-driven structures. Standard evaluation metrics further obscure model capabilities by treating many linguistically acceptable parses as errors, preventing accurate assessment and progress on spoken CSW parsing.

Method: 1) Define a linguistically grounded taxonomy of spoken code-switching phenomena. 2) Create SpokeBench, an expert-annotated gold benchmark specifically targeting spoken-language structures that often violate UD assumptions. 3) Propose FLEX-UD, an ambiguity-aware evaluation metric that can credit multiple plausible parses instead of only one rigid gold structure. 4) Design DECAP, a decoupled agentic parsing framework that separates handling of spoken-specific phenomena from core syntactic parsing, allowing more robust and interpretable analyses without retraining existing parsers.

Result: Using FLEX-UD reveals that current parsers perform worse on spoken CSW than previously suggested by standard metrics, largely because plausible alternative parses are penalized as wrong. The proposed DECAP framework substantially improves parsing performance on spoken CSW, with reported gains of up to 52.6% over existing techniques. DECAP also produces more interpretable outputs and is effective without retraining underlying parsers.

Conclusion: Spoken code-switched language requires specialized treatment beyond conventional UD-based parsing and evaluation. By introducing a phenomenon-focused benchmark (SpokeBench), an ambiguity-aware metric (FLEX-UD), and a decoupled parsing framework (DECAP), the paper demonstrates that more robust and interpretable parsing of spoken CSW is achievable and that standard metrics underestimate the potential of improved parsing strategies.

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [17] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: The paper shows that targeted post-training can significantly improve large language models’ ability to perform causal inference, enabling smaller models to outperform much larger ones on specialized benchmarks.


<details>
  <summary>Details</summary>
Motivation: Causal inference is critical for sound decision-making, but existing tools are hard for non-experts to use. LLMs are promising for democratizing causal reasoning, yet they still struggle with accurate causal estimation, and it is unclear how much their performance can be improved via post-training techniques. The authors aim to systematically understand whether and how post-training can turn general-purpose LLMs into reliable causal reasoners.

Method: The authors construct CauGym, a dataset covering seven fundamental causal tasks for training and five diverse test sets. Using CauGym, they apply and compare five standard post-training methods—Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Kahneman-Tversky Optimization (KTO), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO)—on smaller LLMs. They then evaluate these post-trained models across five in-domain test sets and four existing external causal reasoning benchmarks, including CaLM, under various conditions such as distribution shifts and noisy inputs.

Result: Post-trained smaller LLMs achieve competitive or superior causal inference performance compared with much larger, general-purpose models. A 14B-parameter post-trained model reaches 93.5% accuracy on the CaLM benchmark, significantly outperforming OpenAI o3 at 55.4%. The models also maintain strong performance under challenging conditions like dataset shift and noisy data, indicating robustness and generalization beyond the training distribution.

Conclusion: Targeted post-training on a carefully designed causal reasoning dataset can transform LLMs into accurate, robust causal reasoners, even when the base models are relatively small. This provides the first systematic evidence that common post-training techniques such as SFT, DPO, KTO, PPO, and especially GRPO can be effectively leveraged to enhance LLM-based causal inference. The released CauGym dataset and GRPO-trained model offer practical resources for further research in this direction.

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [18] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: SHINE is a scalable hypernetwork that turns contextual information into LoRA adapters for LLMs in a single forward pass, enabling fast, low-cost adaptation without traditional fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for new tasks or domains via standard supervised fine-tuning or LoRA is costly in time, compute, and memory, and does not easily generalize across many contexts. Prior hypernetworks that generate weights are either too limited in capacity, too large, or not well integrated with existing LLMs. There is a need for a scalable, parameter-efficient way to transform in-context information (prompts, documents, instructions) into persistent parameter changes for LLMs, so that they can perform complex tasks without re-accessing the original context.

Method: The authors design SHINE, a hypernetwork that reuses the frozen base LLM’s parameters within an in-context hypernetwork architecture and introduces architectural innovations to boost expressiveness while remaining parameter-efficient. SHINE takes as input a meaningful context (e.g., instructions, task descriptions, documents) and outputs LoRA adapter weights for the target LLM in a single forward pass. They define a pretraining and instruction fine-tuning pipeline that trains SHINE to generate high-quality LoRA adapters from diverse contexts, without updating the base LLM. At inference, SHINE produces task- or context-specific LoRA adapters on-the-fly, which are then applied to the frozen LLM.

Result: SHINE can generate effective LoRA adapters that allow the frozen LLM to immediately perform complex question answering and other tasks related to the provided context, without needing the context at inference-time. It matches or surpasses SFT-based adaptation on various benchmarks while significantly reducing time, compute, and memory requirements. The results also suggest that the approach scales well as model and task complexity grow.

Conclusion: A scalable in-context hypernetwork like SHINE can efficiently convert in-context knowledge into in-parameter knowledge by generating LoRA adapters in one pass, avoiding traditional fine-tuning. This yields strong performance, substantial efficiency gains, and promising scalability, making SHINE a practical alternative to standard LLM adaptation methods.

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [19] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: The paper compares prompt-based large language models with fine-tuned encoder-only models for text classification, showing that encoders often match or beat LLMs while being far cheaper and faster.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are powerful and widely adopted, production text classification systems face constraints like latency and cost, which are often ignored when models are selected solely based on accuracy. There is a need for a systematic, cost-aware comparison between LLM prompting and traditional fine-tuned encoders for standard classification tasks.

Method: The authors conduct a systematic empirical study on four standard text classification benchmarks: IMDB, SST-2, AG News, and DBPedia. They compare zero- and few-shot prompt-based LLMs (e.g., GPT-4o, Claude Sonnet 4.5) against fully fine-tuned encoder-only models from the BERT family. They measure macro F1, inference latency, and monetary cost. They then frame evaluation as a multi-objective decision problem, analyzing trade-offs via Pareto frontiers and a parameterized utility function corresponding to different deployment regimes.

Result: Fine-tuned BERT-family encoder models generally achieve competitive or better macro F1 scores than LLM prompt-based approaches, while being 10–100x cheaper and faster at inference. The Pareto analyses show that encoder models dominate LLMs on the joint objectives of quality, cost, and latency for standard text classification settings.

Conclusion: For structured text classification with fixed label sets, relying on LLM prompting can yield suboptimal system-level outcomes when cost and latency matter. Fine-tuned encoder-based models are recommended as the primary workhorses in production NLP pipelines, with LLMs serving complementary roles in more flexible or hybrid architectures. The authors also release code, datasets, and evaluation protocols to encourage reproducible, cost-aware NLP system design.

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [20] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: The paper introduces ReBeCA, a causal-analysis framework to explain why and when self-reflection actually improves language model performance, going beyond simple correlations.


<details>
  <summary>Details</summary>
Motivation: Self-reflection often makes language models more reliable, but we do not clearly understand *why* it works or which parts of the reflection process truly matter. Existing analyses are mostly correlational and fail to generalize across tasks and settings. The authors aim to build a principled, causal framework that can disentangle genuine mechanisms from spurious patterns in self-reflection behavior.

Method: They propose ReBeCA, which represents a model’s step-by-step self-reflection trajectory as a causal graph over semantic behaviors. Using a three-stage Invariant Causal Prediction (ICP) pipeline, they (1) extract semantic behaviors, (2) learn and test which behaviors are invariant causal parents of good reflection outcomes across environments, and (3) verify these relationships via ICP-based structure selection and interventional experiments. This yields an interpretable behavioral hierarchy—some behaviors causally affect performance directly, others only through intermediates.

Result: ReBeCA reveals: (1) a behavioral hierarchy where semantic behaviors impact reflection performance either directly or indirectly; (2) only a small subset of behaviors causally generalize, limiting the transferability of many seemingly helpful reflection patterns; (3) more positive-looking behaviors is not always better, as combining them can degrade performance. Their ICP-based selection of sparse causal parents improves structural likelihood by up to 49.6% and remains stable across tasks where correlation-based analyses fail. Interventions on new datasets statistically confirm the discovered causal relations as out-of-distribution robust (p = .013, partial η² = .071).

Conclusion: ReBeCA provides a principled causal framework to interpret and evaluate self-reflection in language models, distinguishing true causal drivers of improvement from correlations. It shows that only a few key semantic behaviors reliably support generalizable gains, and that stacking many ostensibly good behaviors can backfire. This framework offers a rigorous basis for designing more reliable and transferable self-reflection strategies.

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [21] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: The paper introduces FMBench, a benchmark and training pipeline for improving large language models’ ability to produce semantically correct and strictly well‑formatted Markdown, using SFT and reinforcement learning to balance meaning and structure.


<details>
  <summary>Details</summary>
Motivation: LLMs used in assistants, documentation, and tool pipelines must output Markdown that is not only semantically correct but also structurally valid. In practice, models often make subtle formatting mistakes—like broken lists, malformed tables, or invalid code blocks—that are hard to detect automatically yet harmful for downstream usability and automation. There is a lack of systematic benchmarks and training methods specifically targeting adaptive, instruction‑driven Markdown formatting across diverse, realistic scenarios.

Method: 1) Construct FMBench, a benchmark of instruction-following tasks that require diverse, complex Markdown structures (multi-level organization, interleaved natural language, lists, tables, and code, plus strict layout constraints). 2) Propose a lightweight alignment pipeline: start from a base model, perform supervised fine-tuning (SFT) on instruction–response pairs to improve semantic following, then apply reinforcement learning fine-tuning with a composite reward that measures both semantic fidelity and structural/format correctness in Markdown. 3) Evaluate this pipeline on two model families (OpenPangu and Qwen) to measure gains in Markdown robustness and analyze trade-offs between semantic and structural objectives.

Result: On both OpenPangu and Qwen model families, SFT alone significantly improves semantic alignment with instructions but only partially resolves formatting robustness. Adding RL fine-tuning on top of a strong SFT policy yields further improvements, particularly on difficult Markdown instructions with strict structural requirements. Empirical analysis shows that emphasizing structural correctness in the reward can hurt semantic fidelity and vice versa, quantifying a trade-off between semantic and formatting objectives. FMBench effectively exposes these behaviors by covering a wide range of real-world Markdown formatting patterns.

Conclusion: FMBench provides a targeted, realistic benchmark for evaluating and improving LLMs’ Markdown formatting capabilities under instruction-following setups. The proposed two-stage alignment pipeline (SFT followed by RL with a composite reward) improves both semantic and structural performance, especially for challenging formatting tasks, but also reveals an inherent tension between these objectives. Carefully designed reward functions and training strategies are therefore crucial for reliable, well-formatted Markdown generation in practical, user-facing and system-integrated applications.

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [22] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock is a method for speeding up masked diffusion language models by early-locking stable tokens so they no longer need full recomputation at each diffusion step.


<details>
  <summary>Details</summary>
Motivation: Masked diffusion language models iteratively unmask tokens but currently recompute attention and feed-forward layers for all token positions at every step, even for tokens whose predictions have already stabilized. This leads to unnecessary quadratic compute costs in sequence length and limits the efficiency and scalability of such models. The paper is motivated by the need to reduce this redundant computation without sacrificing generation quality.

Method: The authors introduce SureLock, a mechanism that monitors the posterior distribution of each token position across diffusion steps. When the posterior at a position satisfies a "sure" stability condition, that position is locked. Once locked, its query projection and feed-forward sublayers are skipped in subsequent steps, while its attention keys and values are cached so that other (still-unlocked) positions can continue to attend to it. This changes the per-iteration complexity from O(N^2 d) to O(M N d), where M is the number of remaining unlocked tokens. They also provide a theoretical analysis using local KL divergence at the lock step to bound the effect of locking on the final token probabilities.

Result: Empirically, on the LLaDA-8B model, SureLock reduces algorithmic FLOPs by 30–50% compared to the same diffusion sampler without locking, while preserving comparable generation quality. The method demonstrates that the number of unlocked tokens M decreases over diffusion iterations, translating the asymptotic complexity improvement into practical compute savings.

Conclusion: SureLock effectively removes redundant computation in masked diffusion language models by safely locking positions whose token distributions have stabilized, leveraging key/value caching while skipping expensive per-step operations for those tokens. This yields substantial speedups with negligible loss in output quality, and the accompanying theoretical analysis supports that a local KL-based stability check is sufficient to control deviations in final token probabilities.

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [23] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: The paper proposes HOMER, a humor-theory-driven multi-role LLM framework that generates funny, script-opposite captions for images and outperforms existing methods on New Yorker Cartoon datasets.


<details>
  <summary>Details</summary>
Motivation: Humor generation in multi-modal contexts (like generating funny captions for images) is difficult for LLMs because it demands visual understanding, humor reasoning, and creativity. Existing LLM-based methods that use reasoning chains or self-improvement are limited in creativity and interpretability. The authors are motivated to ground humor generation in an established humor theory (GTVH) to improve both effectiveness and explainability.

Method: The authors design HOMER, a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval. It has three LLM-based roles: (1) a conflicting-script extractor that identifies and structures key script oppositions in the input (core to humor per GTVH), (2) a retrieval-augmented hierarchical imaginator that finds humor targets and builds imagination trees—layered associations expanding the creative space, and (3) a caption generator that uses the extracted scripts and imagination trees to create funny, diverse, and script-opposite captions for images.

Result: On two New Yorker Cartoon benchmark datasets, HOMER surpasses state-of-the-art baselines and strong LLM reasoning strategies for multi-modal humor captioning, indicating better performance in generating humorous captions.

Conclusion: Grounding humor generation in a formal humor theory (GTVH) and orchestrating multiple specialized LLM roles with retrieval and structured imagination can improve both the creativity and effectiveness of multi-modal humor captioning. The proposed HOMER framework offers a more interpretable and powerful approach than prior LLM-based methods.

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [24] [Investigating the structure of emotions by analyzing similarity and association of emotion words](https://arxiv.org/abs/2602.06430)
*Fumitaka Iwaki,Tatsuji Takahashi*

Main category: cs.CL

TL;DR: They test whether Plutchik’s wheel of emotion matches how people semantically relate emotion words, by building and analyzing emotion-word networks.


<details>
  <summary>Details</summary>
Motivation: Plutchik’s wheel of emotion is widely used in sentiment analysis and emotion modeling, but its psychological and semantic validity has not been rigorously checked. The authors want to know whether the structure it proposes actually reflects how people conceptualize relationships between emotion words.

Method: They collect human data on perceived similarity and associative strength for ordered pairs of emotion words, then use these data to construct semantic networks where nodes are emotion words and edges encode similarity/association. They apply community detection to these networks to uncover emergent emotional groupings and structural patterns, and then compare those patterns with the layout and groupings in Plutchik’s wheel of emotion.

Result: The discovered network communities and overall layout broadly resemble Plutchik’s wheel, indicating that the wheel captures major structural aspects of how emotions are related. However, the networks also show local discrepancies, suggesting that some specific proximities or groupings in the wheel do not align well with empirical semantic relations between emotion terms.

Conclusion: Plutchik’s wheel of emotion is largely but not perfectly supported by semantic-network evidence. It is a reasonable high-level model of emotion relations, but it needs local refinements if it is to accurately reflect observed semantic associations among emotion words, which has implications for its use in NLP sentiment and emotion analysis.

Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotions in a circular structure, and represents them in two or three dimensions. However, the validity of Plutchik's wheel of emotion has not been sufficiently examined. This study investigated the validity of the wheel by creating and analyzing a semantic networks of emotion words. Through our experiments, we collected data of similarity and association of ordered pairs of emotion words, and constructed networks using these data. We then analyzed the structure of the networks through community detection, and compared it with that of the wheel of emotion. The results showed that each network's structure was, for the most part, similar to that of the wheel of emotion, but locally different.

</details>


### [25] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: The paper proposes a history-aware reinforcement learning framework to more effectively jailbreak LLMs by leveraging and reweighting vulnerability signals from previous interaction turns, achieving state-of-the-art attack success with fewer queries.


<details>
  <summary>Details</summary>
Motivation: Existing LLM jailbreak methods often treat each interaction turn independently and fail to exploit vulnerabilities exposed in earlier turns, leading to unstable and inefficient attacks. Since jailbreaking is inherently sequential and each model response can reveal new weakness signals, a reinforcement learning approach that explicitly uses interaction history is a natural and underexplored direction.

Method: The authors model the jailbreak process as a sequential decision-making problem and design a reinforcement learning framework that is explicitly history-aware. They first show that simply incorporating historical information about past prompts and responses into the RL state improves jailbreak success. They then augment this with an attention-based reweighting module that scans the interaction history for vulnerability signals and assigns higher importance to the most informative past steps, guiding the policy toward more promising future actions while reducing the number of required queries.

Result: On two jailbreak benchmarks, AdvBench and HarmBench, the proposed method outperforms previous jailbreak approaches, attaining higher success rates while using significantly fewer interaction queries. The experiments validate both the benefit of including historical information in the RL state and the additional gains from the attention-based reweighting mechanism over simpler baselines.

Conclusion: The study concludes that carefully exploiting historical vulnerability signals within a reinforcement learning framework substantially strengthens jailbreak attacks on LLMs and improves their query efficiency. The findings highlight the central role of interaction history in adversarial LLM research and suggest that future work on both attack and defense should explicitly account for temporal structures and vulnerability patterns revealed over multiple turns.

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [26] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: The paper introduces CORE, a large-scale benchmark to test whether LLMs can reliably tell when concepts are genuinely unrelated, revealing that even top models often invent spurious relations while remaining highly confident.


<details>
  <summary>Details</summary>
Motivation: While LLMs show strong performance on many reasoning tasks, current benchmarks rarely test whether models can correctly identify when two concepts have no meaningful relationship. This gap is important because failing to recognize unrelatedness leads models to hallucinate connections, which is a safety and reliability concern. The paper is motivated by the need for a systematic, cross-domain, fine-grained evaluation of semantic relations that equally emphasizes unrelated pairs.

Method: The authors construct CORE, consisting of two main components: (1) a large dataset of 225K multiple-choice questions across 74 academic and professional disciplines, and (2) a smaller, rigorously validated open-domain benchmark of 203 questions covering 24 types of semantic relations, balanced with unrelated pairs. They gather human responses from over 1,000 participants to establish a baseline and then evaluate 29 state-of-the-art LLMs. They analyze accuracy, calibration (Expected Calibration Error), and introduce a semantic collapse rate metric to quantify how often models invent relations where none exist.

Result: Humans perform very well overall (92.6% accuracy) and especially on unrelated pairs (95.1%). LLMs reach moderate to high overall accuracy (48.25–70.9%), achieving near-ceiling performance on related pairs (86.5–100%) but failing badly on unrelated ones (0–41.35% accuracy) while still expressing high confidence (92–94%). Calibration worsens significantly on unrelated questions (ECE increases 2–4x), and the average semantic collapse rate is 37.6%, meaning models frequently generate spurious relations. On the full 225K-question CORE dataset, LLM performance drops to about 2% accuracy, underscoring the difficulty of domain-specific semantic reasoning.

Conclusion: The study concludes that reasoning about unrelatedness is a major, under-explored weakness of current LLMs. Despite strong performance on detecting and classifying meaningful relations, models systematically hallucinate links between unrelated concepts and do so with high confidence. The authors argue that unrelatedness reasoning should be treated as a key frontier in LLM evaluation and a central concern for safety, and that benchmarks like CORE are necessary to diagnose and mitigate these failures.

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [27] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: ClinMPO is a reinforcement learning framework that aligns small medical LLMs with evidence-based psychiatric reasoning, enabling an 8B model to slightly outperform medical students on difficult diagnostic cases.


<details>
  <summary>Details</summary>
Motivation: LLMs could support psychiatric decision-making, but current models—especially small, deployable ones—hallucinate and rely on shallow language patterns rather than structured clinical reasoning, creating a mismatch with how clinicians actually diagnose.

Method: The authors develop ClinMPO, a reinforcement learning framework that optimizes an LLM using a dedicated reward model. This reward model is trained on 4,474 psychiatry journal articles that are organized following evidence-based medicine principles, so that the LLM is rewarded for internal reasoning patterns that resemble professional psychiatric logic. They then apply ClinMPO to a light-parameter model (Qwen3-8B) and evaluate on a held-out benchmark subset constructed to probe reasoning rather than memorization, where even large LLMs typically fail.

Result: On this hard, unseen reasoning-focused psychiatric diagnostic test set, the ClinMPO-tuned Qwen3-8B model attains 31.4% diagnostic accuracy, slightly exceeding the performance of a comparison group of 300 medical students, who achieve 30.8%.

Conclusion: Aligning small LLMs with evidence-based psychiatric reasoning via a dedicated reward model can enable them to handle complex diagnostic reasoning tasks at or above a human trainee baseline, suggesting that explicit cognitive alignment is a promising route to safe, scalable psychiatric decision support systems.

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [28] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: RelayGen is a training-free framework that dynamically switches from a large reasoning model to a smaller one on easier segments of long reasoning traces to cut latency while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models are accurate on complex tasks but expensive at inference time, especially because they generate long multi-step reasoning traces. Existing efficiency methods do not adequately exploit the fact that some parts of a single generated trajectory are much easier than others, or they require complex supervised routing at the token level. The paper is motivated by the need for a simple, low-overhead way to leverage this intra-generation difficulty variation to speed up inference without much accuracy loss.

Method: The authors analyze token probability margins in long-form reasoning outputs to understand how uncertainty evolves and where difficulty drops. They find that difficulty transitions can be captured at a coarse segment level, not per-token. Based on this, they propose RelayGen, a runtime, training-free model-switching framework. RelayGen uses offline analysis to define model-specific switch cues that indicate when the reasoning has entered a lower-difficulty segment. At inference time, when these cues are detected in the large model’s generation, the continuation is handed off ("relayed") to a smaller model, while high-difficulty segments remain on the large model. The method operates at the segment level and does not involve additional training or learned routing modules, and it can be combined with speculative decoding.

Result: On multiple reasoning benchmarks, RelayGen reduces inference latency substantially compared to always using the large reasoning model, while preserving most of its accuracy. When integrated with speculative decoding, the approach achieves up to a 2.2× end-to-end speedup, with less than 2% absolute accuracy drop relative to the large model baseline.

Conclusion: The paper concludes that exploiting intra-generation difficulty variation at a segment level is both feasible and effective for long-form reasoning. A training-free, segment-level switching strategy like RelayGen can significantly accelerate large reasoning models with minimal accuracy degradation and without adding complex learned routing infrastructure, and it synergizes well with existing acceleration techniques such as speculative decoding.

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [29] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: The paper introduces DiSPO, a method to improve credit assignment in masked diffusion language models by directly optimizing intermediate token-filling decisions, leading to better performance on math and planning tasks.


<details>
  <summary>Details</summary>
Motivation: Masked diffusion language models generate text via multiple iterative denoising steps, but current reinforcement learning approaches typically assign reward only to the final output, causing coarse and inefficient credit assignment over the many intermediate decisions. This limits the ability to finely tune how intermediate steps contribute to task success.

Method: The authors propose DiSPO (Diffusion-State Policy Optimization), a credit-assignment layer that operates on selected intermediate masked states. At these states, it creates branches by resampling tokens for currently masked positions using cached logits from existing rollouts. Each branched completion is scored, and policy-gradient updates are applied only to the new tokens in the branched region, avoiding extra multi-step diffusion rollouts. They formalize a fixed-state objective for these branched completions and derive a policy gradient estimator that is compatible with standard terminal-feedback policy optimization, enabling joint training with shared rollouts.

Result: On the LLaDA-8B-Instruct model, DiSPO outperforms a strong baseline (diffu-GRPO using only terminal feedback) on mathematical reasoning and planning benchmarks, under the same rollout compute and number of optimizer steps. This shows that more fine-grained intermediate credit assignment improves learning efficiency and task performance.

Conclusion: Fine-grained credit assignment within diffusion-style language models can be significantly improved by optimizing intermediate token-filling decisions. DiSPO offers a practical, plug-in layer that reuses existing rollouts without extra diffusion passes, yielding consistent performance gains on challenging reasoning tasks. The approach demonstrates that branching from cached intermediate states with a fixed-state objective is an effective and compute-efficient way to enhance reinforcement learning for masked diffusion language models.

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [30] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO is a framework that learns from noisy, real-world user logs to improve LLM systems more effectively and efficiently than RAG and memory-based methods.


<details>
  <summary>Details</summary>
Motivation: Scaling models and pretraining data has hit limits due to data scarcity and high compute, while deployment generates abundant interaction logs that contain real human feedback and knowledge. However, these logs are noisy, unstructured, and off-policy relative to standard training, so existing LLMs cannot reliably extract useful learning signals from them.

Method: UNO is a unified pipeline that: (1) distills raw user logs into semi-structured rules and preference pairs; (2) uses query-and-feedback-driven clustering to handle heterogeneous interaction data; and (3) estimates the cognitive gap between the model’s prior knowledge and the logged data to adaptively filter noisy signals and decide what should become primary vs. reflective experience modules for the LLM system.

Result: In experiments, UNO yields better performance and efficiency than strong baselines, including Retrieval-Augmented Generation and memory-based approaches, achieving state-of-the-art results for learning from user logs.

Conclusion: Structuring and clustering user logs, combined with measuring the gap between model knowledge and interaction data, allows LLM systems to learn effectively from noisy deployment feedback. UNO provides a principled, more efficient alternative to conventional RAG and memory mechanisms for continual improvement from real-world use.

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [31] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: The paper revisits the standard Transformer FFN design and proposes deeper hourglass-shaped FFNs that can outperform or match conventional narrow-wide-narrow FFNs while enabling better parameter allocation between FFN and attention under fixed budgets.


<details>
  <summary>Details</summary>
Motivation: Standard Transformer layers use a narrow-wide-narrow FFN with most parameters in the wide middle layer and expansion ratios of 2–4. Recent theory and experiments suggest that residual wide-narrow-wide (hourglass) MLPs can approximate functions more effectively, raising the question of whether the long-standing FFN design in Transformers is actually optimal. The authors are motivated to challenge this convention and explore architectures that may provide higher expressivity and better parameter efficiency, particularly when model size is constrained.

Method: The authors design a Transformer variant where the usual FFN block is replaced by a deeper hourglass-shaped FFN. This FFN is built from a stack of hourglass sub-MLPs connected via residual connections, forming a wide-narrow-wide pattern over depth instead of the classic narrow-wide-narrow pattern. They systematically vary: (1) FFN shape (conventional vs hourglass), (2) FFN parameter counts, and (3) allocation of total parameters between attention and FFN. They then empirically evaluate model performance across multiple scales, from small (up to 400M parameters) to larger models (around 1B parameters), comparing hourglass FFNs against conventional FFNs under matched total parameter budgets.

Result: Across scales up to 400M parameters, Transformers using hourglass FFNs consistently outperform those with conventional FFNs. At larger scales up to about 1B parameters, hourglass FFN variants achieve performance comparable to conventional FFNs. When they reallocate parameters—reducing the size of the FFN while increasing attention capacity—using the hourglass FFN design, they observe consistent improvements over standard parameter allocations at the same total model size.

Conclusion: The conventional narrow-wide-narrow FFN design in Transformers is not uniquely optimal. Deeper, hourglass-shaped FFNs can be more parameter-efficient and at least as expressive, particularly when combined with increased attention capacity under a fixed parameter budget. These findings suggest that the balance between attention and FFN, and the assumed need for large expansion ratios in narrow-wide-narrow MLPs, should be reconsidered when designing modern, efficient language models.

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [32] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: The paper introduces DREAM, a debate-based LLM framework to improve relevance labeling for IR benchmarks, and uses it to build BRIDGE, a more complete and fair IR/RAG evaluation dataset.


<details>
  <summary>Details</summary>
Motivation: IR benchmarks are incomplete, containing many unlabeled but relevant chunks. This incompleteness biases evaluation, distorts system rankings, and complicates RAG assessment. While LLMs and hybrid LLM-human approaches can reduce human effort, they suffer from overconfidence and poor escalation (knowing when to defer to humans). The authors want a scalable, accurate, and reliable relevance assessment method that improves benchmarks and reduces costly manual labeling.

Method: They design DREAM, a multi-round LLM-agent debate framework. Two LLM agents start from opposing relevance stances and iteratively critique each other’s arguments over multiple rounds. The framework uses agreement-based decision rules to finalize labels when LLMs converge, and flags uncertain or contentious cases for human review, creating an AI-to-human escalation pipeline. Using DREAM, they systematically reassess existing IR benchmarks, identifying previously unlabeled relevant chunks and refining labels to construct a new benchmark, BRIDGE.

Result: DREAM achieves 95.2% labeling accuracy while requiring only 3.5% of instances to be resolved by humans, indicating both high quality and strong labeling efficiency. Applying DREAM to existing IR datasets uncovers 29,824 previously missing relevant chunks. With the resulting BRIDGE benchmark, the authors show that these recovered labels significantly change IR system rankings and reveal biases in prior evaluations. They also demonstrate that benchmark incompleteness contributes to retrieval-generation misalignment in RAG, and that BRIDGE better reflects true RAG performance.

Conclusion: A debate-based LLM framework with structured AI-to-human escalation can produce high-quality relevance annotations at low human cost, mitigating LLM overconfidence issues. The resulting BRIDGE benchmark substantially reduces label holes, leading to fairer and more accurate comparisons of retrievers and clearer evaluation of RAG systems. Properly addressing missing relevance labels is essential to avoid biased IR evaluation and misaligned retrieval-generation pipelines.

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [33] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: Introduction of MTQE.en-he, the first public English-Hebrew benchmark for MT Quality Estimation, plus baseline and fine-tuning experiments.


<details>
  <summary>Details</summary>
Motivation: There is no publicly available benchmark for English-Hebrew Machine Translation Quality Estimation, an under-resourced and low-resource language pair, which hinders research progress and model evaluation.

Method: Construct a dataset of 959 English segments from WMT24++, each with Hebrew machine translations and human Direct Assessment scores from three experts. Benchmark several QE approaches (ChatGPT prompting, TransQuest, CometKiwi) individually and in an ensemble. Conduct fine-tuning experiments on TransQuest and CometKiwi, comparing full-model fine-tuning with parameter-efficient methods (LoRA, BitFit, FTHead). Evaluate using Pearson and Spearman correlations with human scores.

Result: The ensemble of ChatGPT prompting, TransQuest, and CometKiwi surpasses the best single model (CometKiwi) by 6.4pp Pearson and 5.6pp Spearman. Full-model fine-tuning is unstable and prone to overfitting and distribution collapse, whereas parameter-efficient methods train stably and improve performance by 2–3pp.

Conclusion: MTQE.en-he provides a new benchmark for English-Hebrew MT Quality Estimation and initial baselines. Ensembling diverse QE models and applying parameter-efficient fine-tuning methods yield strong, stable performance gains, facilitating future work on this under-resourced language pair.

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [34] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3 is a medical-focused large language model designed for active clinical decision support rather than simple Q&A, achieving state-of-the-art performance on multiple healthcare benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing medical LLMs mainly act as passive question-answering tools and struggle with open-ended consultations, ambiguity resolution, and reliable, safe clinical advice. The authors aim to build a system that better matches real physician workflows and can provide trustworthy, high-quality decision support in complex clinical scenarios.

Method: They design a specialized training pipeline that explicitly models the workflow of physicians. The resulting model, Baichuan-M3, is trained to: (1) proactively ask for and acquire missing or clarifying information; (2) perform long-horizon reasoning that integrates dispersed clinical evidence into a unified diagnostic picture; and (3) suppress hallucinations adaptively to maintain factual reliability. They then evaluate the model on several benchmarks, including HealthBench, a new hallucination-focused HealthBench-Hallu, and ScanBench for imaging-related tasks.

Result: Baichuan-M3 delivers state-of-the-art performance on HealthBench, the new HealthBench-Hallu, and ScanBench, and it significantly outperforms GPT-5.2 on tasks involving clinical inquiry, medical advisory quality, and safety-related metrics.

Conclusion: By aligning training with physician-like workflows and emphasizing proactive information gathering, long-horizon reasoning, and hallucination control, Baichuan-M3 transitions medical LLMs from passive Q&A systems to more robust, clinically relevant decision-support tools, as validated by superior benchmark performance.

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [35] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: The paper proposes a method to reduce toxic outputs of LLMs by intervening in specific directions (subspaces) of their internal representations, lowering toxicity while keeping fluency and efficiency largely unchanged.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate subtle, context-dependent toxic content that is hard to detect and filter using token- or sentence-level signals. Existing detoxification approaches either miss such toxicity or significantly degrade the model’s fluency and usefulness, and can be computationally expensive at inference. There is a need for a principled, efficient way to directly modify internal representations to suppress toxic behavior without sacrificing overall generative quality.

Method: The authors identify a targeted subspace in the model’s hidden representations that correlates with toxic content. They then apply an intervention in this subspace—e.g., projection, attenuation, or shifting of representation components associated with toxicity—during generation. This operates inside the LLM rather than only at the input or output level. The method is evaluated as a plug-in mitigation on various LLMs and compared against baseline detoxification systems on benchmarks like RealToxicityPrompts, measuring both toxicity and fluency with minimal extra inference cost.

Result: On the RealToxicityPrompts benchmark and several LLMs, the proposed subspace intervention reduces the toxicity scores of generations more effectively than prior detoxification baselines, with improvements of about 8–20% over state-of-the-art systems. The approach adds little inference overhead and preserves text fluency at a level comparable to or better than existing methods, as supported by both automatic and human evaluations.

Conclusion: Targeted subspace interventions in hidden representations offer an effective and efficient way to mitigate toxicity in LLM outputs. By directly modulating internal toxic directions instead of relying solely on surface-level filtering, the method substantially reduces harmful content while maintaining generative fluency, and consistently outperforms existing detoxification techniques across models and metrics.

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [36] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: The paper introduces FairJudge, a more adaptive, less biased, and more consistent LLM-as-a-judge system that models judging as a learnable policy and uses a curriculum training pipeline to improve reliability across tasks and evaluation modes.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge systems have three key weaknesses: they can't flexibly adapt to task- or domain-specific rubrics, they show systematic non-semantic biases (e.g., preferring certain positions, lengths, formats, or model sources), and they yield inconsistent judgments across different evaluation setups like pointwise and pairwise scoring. This limits trustworthiness and robustness of automatic LLM evaluations.

Method: The authors design FairJudge, which reframes the judging process as a learnable and regularized policy rather than a fixed evaluator. They build a high-information-density judging dataset that encodes supervision signals about desired evaluation behaviors (rubric following, debiasing, cross-mode consistency). Using this dataset, they employ a curriculum-style training pipeline combining supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and GRPO, scheduled to progressively teach rubric adherence, bias reduction, and consistency across evaluation modes while preventing catastrophic forgetting of earlier skills.

Result: Across a range of internal and public benchmarks, FairJudge shows higher agreement and F1 with gold or reference judgments compared to existing judges, exhibits reduced non-semantic biases, and even surpasses significantly larger instruction-tuned LLMs when used as an evaluator.

Conclusion: Modeling judgment as a learnable policy and training it with a targeted, curriculum-style SFT-DPO-GRPO pipeline on a specially constructed judging dataset produces an LLM-as-a-judge that is more adaptive, debiased, and consistent than prior static evaluators. The authors commit to releasing all resources after acceptance to support further work on reliable automatic evaluation.

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [37] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: The paper introduces a multimodal topic segmentation model for spoken content that jointly uses text and acoustic features, achieving better robustness and performance than text-only methods.


<details>
  <summary>Details</summary>
Motivation: Spoken content in videos and podcasts naturally shifts between multiple topics, and users need accurate topic boundaries for navigation and downstream tasks like retrieval and summarization. Existing topic segmentation methods rely mostly on text (e.g., ASR transcripts) and underuse or ignore acoustic cues such as prosody or pauses that may signal topic shifts, which limits robustness, especially under noisy ASR conditions.

Method: The authors propose a multimodal topic segmentation framework that fine-tunes a text encoder together with a Siamese audio encoder. The audio encoder focuses on acoustic features around sentence boundaries, learning representations that highlight prosodic and other acoustic cues indicative of topic changes. The joint model fuses textual and learned acoustic representations to predict topic boundaries, and is trained and evaluated on large-scale YouTube data and additional multilingual datasets.

Result: On a large YouTube dataset, the proposed model yields substantial performance gains over both text-only and previous multimodal baselines. It demonstrates higher robustness to ASR noise and consistently outperforms a larger, stronger text-only baseline on three additional datasets in Portuguese, German, and English, indicating strong cross-lingual effectiveness of the learned acoustic features.

Conclusion: Incorporating learned acoustic representations via a Siamese audio encoder alongside a text encoder significantly improves automatic topic segmentation of spoken content. The results highlight that acoustic cues around sentence boundaries are valuable, contribute to robustness under noisy ASR conditions, and generalize well across languages and datasets, making the approach promising for real-world multimedia applications.

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [38] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: The paper introduces PACT, a framework that adds dynamic, controllable safety policies to LLMs, improving both safety and usefulness.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs use static, one-size-fits-all safety policies that cannot be adjusted at runtime, which causes over-refusals of benign requests and under-regulation of harmful ones. The authors aim to resolve this safety-helpfulness trade-off and support diverse, real-world application needs.

Method: They design PACT, a hierarchical, prompt-configured safety framework. A non-overridable global safety policy enforces strict limits on critical risks, while user-defined policies can add domain-specific risk categories and map labels to actions. PACT structures safety behavior as an explicit Classify→Act chain-of-thought, routing each query to comply, guide, or reject, with transparent reasoning.

Result: Experiments show that PACT reaches near state-of-the-art performance on standard global safety metrics, while delivering superior controllability when evaluated under user-specific policies, thus better balancing safety and helpfulness.

Conclusion: The PACT framework enables explicit, risk-aware, and configurable safety control for LLMs, alleviating the safety-helpfulness trade-off. The authors will release models, data, and evaluation protocols to support reproducible work on controllable safety alignment.

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [39] [Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity](https://arxiv.org/abs/2602.06665)
*Bowen Zhang,Meiyi Wang,Harold Soh*

Main category: cs.CL

TL;DR: The paper introduces Selective Layer Restoration (SLR), a training-free technique that restores specific layers of a post-trained LLM back to their pre-trained weights to mitigate mode collapse, increasing generation diversity while preserving quality.


<details>
  <summary>Details</summary>
Motivation: Post-training for instruction-following often causes mode collapse in LLMs, reducing diversity and leading to repetitive outputs. The authors are motivated by the observation that different layers in LLMs have distinct functional roles, suggesting that undesirable post-training effects like mode collapse might be localized to certain layers and could be reversed without sacrificing the gains from post-training.

Method: The authors first construct a proxy task, Constrained Random Character (CRC), which has a clear notion of validity and a natural diversity objective. They then systematically experiment with restoring different contiguous ranges of layers in post-trained models back to their pre-trained weights, measuring the trade-off between output validity (quality) and diversity on CRC. Based on the identified sweet spots, they propose Selective Layer Restoration (SLR): a training-free procedure that takes a post-trained model and replaces parameters of selected layers with their pre-trained counterparts, resulting in a hybrid model with unchanged architecture and parameter count.

Result: On the CRC task, they observe a clear diversity–validity trade-off depending on which layers are restored, and identify layer ranges where diversity increases meaningfully with only minor quality degradation. Applying SLR on real-world tasks—creative writing, open-ended QA, and multi-step reasoning—across Llama, Qwen, and Gemma families, they find consistent, substantial gains in output diversity while preserving high instruction-following capability and answer quality, with no added inference cost.

Conclusion: Mode collapse induced by post-training can be mitigated by selectively reverting certain layers of an LLM to their pre-trained state. This layer-localized intervention, SLR, is simple, training-free, and model-agnostic, offering a practical way to boost diversity without losing the benefits of instruction tuning, and it generalizes across model architectures and tasks.

Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.

</details>


### [40] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: The paper presents compar:IA, an open-source French government platform that collects large-scale, mostly French human preference data for LLM training and evaluation, releasing three open datasets and initial analyses such as a French model leaderboard.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform and are less culturally aligned and safety-robust in non-English languages because pre-training and human preference alignment data are heavily English-centric, and non-English preference data is scarce and often closed. The authors aim to provide large-scale, open, French-focused preference data and infrastructure to improve multilingual alignment and research.

Method: They design and deploy compar:IA, a public web platform using blind pairwise comparisons between model outputs on unconstrained, real-world user prompts. The system targets a predominantly French-speaking audience, minimizes participation friction, and applies automated, privacy-preserving filters. It logs prompts, conversations, user preference votes, and reactions across multiple LLMs, and aggregates these into open datasets for downstream use.

Result: As of 2026-02-07, the platform has gathered over 600,000 free-form prompts and 250,000 preference votes, about 89% in French. They construct and release three datasets (conversations, votes, reactions) under open licenses and produce initial empirical analyses, including a French-language LLM leaderboard and characterization of user interaction behavior.

Conclusion: compar:IA demonstrates that a government-backed, low-friction public service can collect large-scale French human preference data and make it openly available, supporting better alignment and evaluation of LLMs in non-English contexts. The platform is being generalized into an international digital public good, offering reusable infrastructure for multilingual model training, benchmarking, and studying human–AI interaction patterns.

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [41] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: The paper studies how to control the sentiment/emotion of LLM-generated text using prompt engineering and compares it with fine-tuning, finding that well-designed prompts—especially few-shot with human examples—can effectively steer emotions in a cost-efficient way.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful for human-computer interaction, but it is still difficult to reliably control the sentiment or emotion of their outputs, which is crucial for emotion-adaptive AI. Existing solutions like fine-tuning require substantial data and resources. The authors are motivated to find a lighter, more accessible way (prompting) to control emotional tone in generated text.

Method: The authors use Ekman’s six basic emotions (e.g., joy, disgust) as target emotions and test different prompting strategies with gpt-3.5-turbo: Zero-Shot prompting, Chain-of-Thought prompting, and Few-Shot prompting with human-written examples. They also compare these approaches to model fine-tuning in terms of their ability to steer the emotional content of generated text, focusing on effectiveness and resource cost.

Result: Prompt engineering can reliably influence the emotional tone of LLM outputs. Among the tested strategies, Few-Shot prompting with carefully crafted human examples performs best at steering emotions, outperforming zero-shot and chain-of-thought prompting and, in many cases, approaching or rivaling fine-tuning while requiring fewer resources and less data.

Conclusion: Prompt-based control of sentiment/emotion in LLM-generated text is both feasible and effective, making it a practical alternative to fine-tuning, especially in settings with limited data or computational resources. Few-shot prompting with high-quality human examples is particularly powerful, providing strong task-specific guidance and enabling the development of more emotion-adaptive AI systems.

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [42] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: They propose Table-as-Search (TaS), a framework that represents information-seeking as filling a structured table, leading to more focused and robust long-horizon search than existing agents.


<details>
  <summary>Details</summary>
Motivation: Existing information-seeking agents lose coherence and focus over long searches because they keep complex plans and large volumes of results in unstructured plain-text context, which is fragile and hard to manage. The authors want a more reliable way to track search state and planning steps for complex, long-horizon exploration tasks.

Method: They introduce TaS, which reformulates information-seeking as a table completion problem. For each query, TaS builds a table schema stored in an external database; rows are search candidates and columns are constraints or required pieces of information. Filled cells store search history and retrieved results, while empty cells explicitly encode remaining search plans. This unified tabular representation is used to handle different types of information-seeking tasks (Deep, Wide, and DeepWide Search) and to drive the agent’s planning and interaction with search tools.

Result: Across multiple benchmarks, covering different information-seeking settings, including multi-agent setups and commercial systems, TaS achieves substantially better performance than various state-of-the-art baselines. The experiments show gains in effectiveness and robustness, especially for long-horizon tasks.

Conclusion: Representing information-seeking as structured table completion provides a precise, persistent search state that improves focus and coherence in long-horizon exploration. TaS unifies several task types, scales well, is efficient and flexible, and offers a robust alternative to plain-text-based planning for information-seeking agents.

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [43] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: The paper shows that checking only whether reward-model labels are correct is not enough; we must also ensure that the model’s generated rationales match human rationales, and proposes a training method (R-Align) that directly aligns these rationales, improving RLHF outcomes.


<details>
  <summary>Details</summary>
Motivation: Current RLHF pipelines rely on reward models, and newer Generative Reward Models (GenRMs) produce rationales before giving preference labels. However, training and evaluation ignore whether these rationales are faithful to the actual reasons behind the decisions, focusing only on outcome labels. This can hide cases where the model gets the right answer for the wrong reasons, which might destabilize RL optimization and harm downstream policy performance. The authors want a way to measure and fix this mismatch.

Method: 1) Define and measure “reasoning fidelity” for GenRMs as the alignment between the model’s rationales and reference human (gold) rationales. 2) Introduce Spurious Correctness (S-Corr): among all label-correct decisions, compute the fraction whose rationales conflict with gold judgments, by repurposing existing reward-model benchmarks that contain reference explanations. 3) Empirically analyze S-Corr across strong GenRMs and study its correlation with policy behavior under RLHF optimization, observing links to policy degeneration. 4) Propose Rationale-Centric Alignment (R-Align), a training approach that adds explicit supervision on rationale alignment using gold judgments, so that GenRMs are optimized not just for correct labels but also for faithful, human-aligned reasoning.

Result: They find that even competitive GenRMs exhibit substantial Spurious Correctness: many correct labels are supported by rationales that disagree with human gold rationales. Higher S-Corr strongly correlates with worse behavior during RLHF optimization, including policy degeneration. Applying their proposed R-Align method reduces S-Corr scores on reward-model benchmarks and leads to more robust RLHF outcomes, with improved actor performance on STEM, coding, instruction-following, and general benchmarks.

Conclusion: Reasoning fidelity in GenRMs is a critical but previously overlooked factor in RLHF. Merely checking label accuracy is insufficient; models can be right for the wrong reasons, which harms downstream optimization. By measuring and minimizing Spurious Correctness through Rationale-Centric Alignment, one can train reward models whose rationales better match human judgments, thereby improving the stability and effectiveness of RLHF-trained policies across diverse task domains.

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [44] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: The paper introduces a method to automatically build fine-grained reasoning error taxonomies (“rubrics”) that let LLMs more accurately detect errors in complex reasoning traces and use these as reward signals for training, achieving large accuracy gains in technical domains with far fewer gold labels.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for complex reasoning, but they are poor at reliably spotting errors in long, technical reasoning traces, especially when explicit verifiable rewards (like unit tests or ground-truth solutions) are unavailable or expensive. Existing LLM-as-judge approaches rely on coarse criteria and struggle in expert domains, limiting their usefulness for reinforcement learning from reasoning (RL-style) signals. There is a need for a systematic, data-driven way to characterize and detect fine-grained reasoning mistakes so that LLMs can serve as stronger, more precise judges, enabling scalable training without requiring full gold-label datasets.

Method: The authors propose a data-driven pipeline to automatically construct highly granular reasoning error taxonomies—or “rubrics”—from reasoning traces. These taxonomies categorize different types of reasoning mistakes at a fine level of detail. They then train or prompt LLMs to classify errors in unseen reasoning traces according to these rubrics and use the resulting structured error detections as reward signals. These rubric-based judgments are turned into LLM-as-judge reward functions that can guide reinforcement learning for reasoning models across technical domains such as coding, math, and chemical engineering, even when traditional verifiable rewards are unavailable or sparse.

Result: Classification methods based on the learned error rubrics significantly outperform baseline LLM-based error detection approaches in identifying mistakes within reasoning traces across technical domains. When used as reward functions in reinforcement learning, these rubric-based judges lead to models whose task accuracy on difficult domains improves by up to 45% relative to models trained with general-purpose LLM-as-judge rewards. Moreover, they approach the performance of models trained using fully verifiable rewards (e.g., tests or exact solutions), while requiring only about 20% as many gold labels, demonstrating substantial label efficiency.

Conclusion: Automatically constructed, fine-grained reasoning error taxonomies can turn LLMs into much stronger, domain-robust judges of reasoning correctness. By converting rubric-based error classifications into reward functions, the method extends reward rubrics from qualitative assessment to quantitative correctness evaluation, offering a scalable alternative to RL with verifiable rewards. This enables training models to solve complex, technical problems with significantly fewer expensive gold labels, suggesting a practical path toward more capable reasoning systems in domains where ground-truth supervision is scarce or costly.

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [45] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: They build an interpretable CLIP-based framework for Visual Word Sense Disambiguation that enriches text and image embeddings, significantly improving performance on SemEval-2023 VWSD and showing that precise, CLIP-aligned prompts work better than noisy external signals.


<details>
  <summary>Details</summary>
Motivation: Lexical ambiguity is a core difficulty for LLMs and natural language understanding, and existing VWSD approaches underuse the visual modality for disambiguating word senses. The authors aim to understand and improve how ambiguous words can be grounded and resolved via images, in a way that is interpretable and efficient.

Method: They construct a VWSD pipeline on top of CLIP, mapping ambiguous text and candidate images into a shared embedding space. Text embeddings are enriched via a dual-channel prompting ensemble: (1) semantic prompts using WordNet synonyms and (2) photo-oriented prompts describing what might appear visually. Image embeddings are refined via strong test-time data augmentations. Cosine similarity between enriched text and image embeddings is used to select the image most aligned with the ambiguous word usage. They also run ablations and variants using WordNet definitions and multilingual prompts.

Result: On the SemEval-2023 VWSD benchmark, their enrichment techniques improve MRR from 0.7227 to 0.7590 and Hit Rate from 0.5810 to 0.6220 over the base model. Ablations show dual-channel prompting yields the main improvement with low latency, while heavy image augmentations bring only small additional gains. Experiments with definitions and multilingual prompt ensembles slightly hurt performance, indicating they introduce noise rather than useful signal.

Conclusion: Carefully designed, CLIP-aligned textual prompts that combine semantic and visual/photo-oriented cues substantially improve visual word sense disambiguation without high computational cost. Overly noisy external information, such as long definitions or heterogeneous multilingual prompts, tends to blur semantic distinctions, so precise and targeted prompt engineering is more effective for resolving lexical ambiguity in multimodal settings.

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [46] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: The paper argues that conceptual representations in language models share a stable relational structure across tasks, even though task-specific representations occupy distinct subspaces, and demonstrates this with number concepts.


<details>
  <summary>Details</summary>
Motivation: To resolve an apparent tension in cognitive science between evidence that conceptual representations converge on shared manifolds for generalization and evidence that they diverge into orthogonal subspaces to reduce task interference, and to provide a mechanistic account of how these properties can coexist and transform across tasks.

Method: Use number concepts as a controlled test case and large language models as high-dimensional representational substrates; analyze the internal representations of numbers across different tasks to examine their geometric structure, identify task-specific subspaces, and test whether subspaces can be related via simple (linear) mappings; examine how low-level numerical features like magnitude and parity are encoded along specific linear directions.

Result: Number concepts in language models exhibit a stable relational geometry across tasks: while different tasks produce representations that lie in distinct subspaces, core relations between numbers (e.g., ordering, parity, magnitude) are preserved; low-level numerical features are encoded along separable linear directions; and the task-specific subspaces are largely transformable into one another through linear mappings, indicating shared relational structure despite geometric separation.

Conclusion: Conceptual sharing in language models resides not in identical task-general embeddings but in a common relational structure that is preserved across task-specific subspaces; functional flexibility arises from applying simple task-specific transformations to this underlying shared geometry, providing a mechanistic lens on how models simultaneously support generalization and minimize interference across tasks.

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [47] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: The paper introduces Halluverse-M^3, a multilingual, multi-task benchmark dataset for analyzing and detecting different types of hallucinations in large language models.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination benchmarks are largely English-centric and do not systematically cover multiple languages, tasks, and fine-grained hallucination types. As LLMs are increasingly used in multilingual and generative scenarios, there is a need for a controlled, realistic benchmark to understand how hallucinations manifest across languages and tasks, and how well current models can detect them.

Method: The authors construct Halluverse-M^3, a dataset spanning four languages (English, Arabic, Hindi, Turkish) and two generation tasks (question answering and dialogue summarization). They define three hallucination categories (entity-level, relation-level, sentence-level). Hallucinated outputs are produced via a controlled editing pipeline applied to original content, then validated by human annotators to ensure a clear mapping between source texts and hallucinated generations. Using this dataset, they benchmark various open-source and proprietary LLMs on hallucination detection, analyzing performance by language, task, and hallucination type.

Result: Models consistently detect hallucinations better in question answering than in dialogue summarization. Sentence-level hallucinations are the hardest category, remaining challenging even for the strongest evaluated models. Performance is highest for English and drops for lower-resource languages, with Hindi showing the weakest hallucination detection accuracy.

Conclusion: Halluverse-M^3 is a realistic, challenging benchmark for studying hallucinations in multilingual and multi-task settings. It reveals systematic weaknesses in current LLMs, particularly on sentence-level hallucinations and low-resource languages. The released dataset is intended to facilitate future research on hallucination detection and mitigation across languages and tasks.

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [48] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: The paper introduces SEMA, a reinforcement-learning-based framework that automatically learns powerful multi-turn jailbreak prompts for LLMs, achieving state-of-the-art attack success rates and providing a stronger automatic red-teaming tool.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak research largely focuses on single-turn prompts or manually scripted multi-turn attacks. However, real-world adversaries can iteratively refine prompts over multiple turns, and current methods struggle with exploration complexity and intent drift—where the conversation drifts away from the original harmful goal. There is a need for an automated, scalable, and realistic multi-turn attack framework that does not rely on handcrafted strategies or external datasets, and that can better stress-test safety-aligned LLMs.

Method: SEMA trains a multi-turn attacking policy in two stages. (1) Prefilling self-tuning: the authors auto-generate non-refusal, well-structured multi-turn adversarial prompt trajectories from a minimal prefix and fine-tune a model on these to stabilize rollouts and make them usable. (2) Reinforcement learning with an intent-drift-aware reward: they then apply RL to optimize the attacker, using a reward signal that explicitly encourages maintaining the same harmful intent while increasing jailbreak success and detail level. The reward combines intent alignment (consistency of harmful objective across turns), compliance risk (likelihood the victim will respond), and level of detail in elicited answers. The training is conducted in an open-loop setting that does not depend on live victim feedback, and the resulting attacker is evaluated on different victim models and datasets.

Result: SEMA achieves state-of-the-art attack success rates in both single- and multi-turn jailbreak evaluations. On AdvBench across three open- and closed-source victim models, SEMA reaches an average ASR@1 of 80.1%, which is 33.9 percentage points higher than the previous SOTA. It outperforms single-turn baselines, manually scripted and template-driven multi-turn baselines, and its own SFT and DPO variants. The trained attacker generalizes across multiple victim LLMs and benchmarks, indicating strong transferability and robustness.

Conclusion: The study concludes that multi-turn jailbreaks are a more realistic and stringent test for safety-aligned LLMs than single-turn attacks, and that SEMA provides an effective and compact framework to automatically learn such attacks. By handling exploration complexity and intent drift and operating in an open-loop regime, SEMA offers a practical tool for automatic red-teaming, helping researchers and practitioners expose and localize safety failures in LLMs. The method is reproducible, transferable, and can serve as a stronger benchmark for future safety defenses.

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [49] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: The paper presents a comprehensive, systematic study of subword tokenization for Turkish, analyzing how vocabulary size and tokenizer training data jointly affect performance across many tasks, and provides a morphology-aware diagnostic toolkit plus open-source resources.


<details>
  <summary>Details</summary>
Motivation: In morphologically rich, agglutinative languages like Turkish, standard tokenization choices can either explode the vocabulary or lose important morphological information. Prior work has not systematically controlled tokenizer training data vs. vocabulary size, has had weak intrinsic diagnostics, and has evaluated only a few downstream tasks. This creates a gap in understanding how to design effective tokenizers for such languages.

Method: The authors systematically vary both vocabulary size and tokenizer training corpus size (data–vocabulary coupling) and compare several tokenizer families—WordPiece-style, morphology-level, and character-level baselines—under matched parameter budgets. They evaluate these tokenizers on a broad suite of Turkish tasks spanning semantics (NLI, STS, sentiment, NER), syntax (POS tagging, dependency parsing), and morphology-sensitive probes. To interpret performance, they introduce a morphology-aware diagnostic toolkit that measures boundary-level micro/macro F1, separates lemma atomicity from surface boundary accuracy, computes over/under-segmentation indices, CER/WER, continuation rates, affix-type coverage, and token-level atomicity.

Result: The study yields detailed empirical findings about when different tokenization strategies (character-level, morphology-level, subword) perform better or worse for Turkish, as vocabulary size and training corpus size change. The new diagnostic metrics reveal why certain tokenizers succeed or fail in terms of morphological faithfulness and segmentation quality, and how those intrinsic properties relate to extrinsic performance on semantic, syntactic, and morphological tasks.

Conclusion: The work provides actionable guidance on designing tokenizers for morphologically rich languages by clarifying the interactions between vocabulary size, training corpus, and downstream success. It establishes a unified, morphology-aware evaluation framework that links intrinsic segmentation diagnostics to task outcomes, shows when character-level and morphology-level tokenization are advantageous, and releases open-source tools, pipelines, and models to support reproducible research in this area.

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [50] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: The paper identifies and analyzes cross-objective interference in multi-objective alignment of LLMs, explains it via a covariance-based theory, and proposes an adaptive weighting method (CTWA) that mitigates this interference and can achieve global convergence under certain geometric conditions.


<details>
  <summary>Details</summary>
Motivation: In aligning LLMs, we often optimize multiple alignment objectives (e.g., helpfulness, harmlessness, honesty) simultaneously. Empirically, improving some objectives can unexpectedly worsen others, but this failure mode has not been systematically characterized or theoretically explained. The authors aim to formalize this issue, understand when and why it arises, and design training methods that avoid degrading some objectives while optimizing a scalarized combination.

Method: 1) Empirically study multi-objective alignment with several standard scalarization schemes, documenting pervasive cross-objective interference and its dependence on model properties. 2) Develop a theoretical analysis based on a local covariance law, proving that an objective’s reward improves at first order if it has positive covariance with the scalarized training signal, and extend this analysis to clipped surrogate objectives used in RL-style alignment. 3) Use this covariance law to design Covariance Targeted Weight Adaptation (CTWA), which dynamically adjusts scalarization weights to keep each objective’s reward positively correlated with the training signal. 4) Analyze global convergence under the Polyak–Łojasiewicz condition for the non-convex scalarized problem and relate convergence and interference to geometric features of the model.

Result: The authors show that cross-objective interference is widespread across classic scalarization approaches and depends strongly on model geometry. They prove a local covariance law governing when each objective improves, show that this law still applies under common clipping-based surrogates, and demonstrate that CTWA can effectively maintain positive covariance and thereby reduce interference. They further provide a global convergence theory under PL conditions, characterizing when scalarized optimization converges and how model geometry modulates interference.

Conclusion: Cross-objective interference is an inherent and prevalent challenge in multi-objective LLM alignment, rooted in how different objectives covary with the scalarized training signal. A covariance-based theoretical framework explains when objectives improve or degrade, even under clipped surrogates, and suggests practical remedies. CTWA, as a plug-and-play adaptive weighting scheme, can mitigate interference by preserving positive covariance between objectives and the training signal, and under suitable geometric (PL) conditions, scalarized optimization can provably converge globally while controlling interference.

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>


### [51] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: The paper presents InftyThink+, a reinforcement-learning-based framework that optimizes iterative chain-of-thought reasoning by learning when and how to summarize and continue reasoning, improving both accuracy and efficiency over long chain-of-thought methods.


<details>
  <summary>Details</summary>
Motivation: Scaling chain-of-thought reasoning in large reasoning models leads to quadratic inference cost, context length limitations, and performance degradation from lost-in-the-middle effects. Existing iterative reasoning mitigates these issues through summarization but uses fixed or supervised heuristics that do not optimally decide when to summarize, what information to keep, or how to resume reasoning, leaving significant efficiency and performance gains unrealized.

Method: The authors introduce InftyThink+, an end-to-end reinforcement learning framework for iterative reasoning. The model autonomously controls iteration boundaries and generates explicit summaries of intermediate thoughts. Training is done in two stages: (1) a supervised cold-start phase to initialize reasonable summarization and continuation behavior, and (2) trajectory-level reinforcement learning that optimizes decisions across the full reasoning trajectory, allowing the model to learn strategic policies for summarization and continuation under task rewards and efficiency constraints.

Result: On DeepSeek-R1-Distill-Qwen-1.5B, InftyThink+ yields a 21% accuracy improvement on AIME24 and clearly outperforms standard long chain-of-thought reinforcement learning methods. It also shows better generalization on out-of-distribution benchmarks and reduces inference latency while speeding up RL training, indicating both higher reasoning quality and improved computational efficiency.

Conclusion: InftyThink+ demonstrates that fully RL-optimized iterative reasoning with learned summarization and continuation policies can surpass conventional long chain-of-thought paradigms in both accuracy and efficiency. The framework effectively handles context limits and lost-in-the-middle issues while delivering faster, more robust reasoning, suggesting a promising direction for scalable reasoning in large models.

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>


### [52] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: This paper introduces DAWN, a training-free, dependency-aware decoding method that accelerates diffusion large language model (dLLM) inference by choosing which token positions to unmask in parallel using a learned dependency graph, achieving up to ~8x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion LLMs support parallel decoding but suffer from a quality–speed trade-off: naive parallel unmasking assumes token positions are independent, which conflicts with real semantic dependencies between tokens and leads to degraded text quality. Current methods thus use conservative parallelism, underutilizing the efficiency potential. The motivation is to exploit token dependency structure so that more positions can be safely decoded in parallel without hurting output quality.

Method: The paper proposes DAWN, a training-free decoding strategy for dLLMs that explicitly models inter-token dependencies. It first extracts a dependency graph over token positions, estimating how strongly each position depends on others and how certain each position is. It then follows two principles: (1) positions whose dependencies are already resolved (dependent on unmasked, certain tokens) can be decoded more reliably; (2) jointly unmasking positions that are strongly coupled and uncertain is risky and should be avoided. At each diffusion iteration, DAWN uses the dependency graph to select a subset of token positions to unmask in parallel that are both reliable and weakly coupled, thereby increasing safe parallelism without retraining the model.

Result: Across multiple diffusion LLM architectures and datasets, DAWN delivers 1.80–8.06x speedup in inference compared to baseline dLLM decoding strategies, with negligible degradation in standard generation quality metrics. This empirically validates that dependency-aware token selection unlocks much of the latent parallelism in dLLMs without sacrificing output quality.

Conclusion: Dependency structure between tokens is key to safely exploiting parallel decoding in diffusion LLMs. By using a training-free, graph-based selection of unmasking positions at each step, DAWN significantly accelerates dLLM inference while preserving text quality, demonstrating that smarter decoding can substitute for more conservative or retrained models. The approach offers a practical path to faster diffusion-based language generation and can potentially generalize to other masked or iterative generation frameworks.

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot is a framework that makes reinforcement learning for LLMs cheaper and more stable by using a separate, efficient rollout model combined with Optimal Budget Rejection Sampling to correct distribution mismatch.


<details>
  <summary>Details</summary>
Motivation: RL training for large language models is very expensive because generating rollouts with the current policy model is costly. Using a cheaper model for rollouts could save compute, but then the data distribution of rollouts no longer matches the evolving policy, which can destabilize or break learning. The paper aims to find a principled way to decouple rollout generation from policy optimization while keeping training stable and effective.

Method: The authors propose Jackpot, which centers on Optimal Budget Rejection Sampling (OBRS). OBRS selectively accepts or rejects trajectories from a rollout model so that the accepted samples more closely match the target policy distribution under a fixed acceptance budget. Jackpot couples this with (1) a unified training objective that updates both the policy and rollout models jointly, and (2) a practical system implementation that uses top-k probability estimation and batch-level bias correction to make OBRS efficient at LLM scale.

Result: Theoretical analysis shows that OBRS provably moves the rollout distribution closer to the target policy distribution while respecting a controllable acceptance budget. Empirically, Jackpot improves training stability versus importance-sampling-based baselines and achieves performance on par with standard on-policy RL when training Qwen3-8B-Base for up to 300 update steps with batch size 64.

Conclusion: OBRS-based Jackpot enables more stable and effective RL for LLMs when rollout generation is decoupled from policy optimization, demonstrating that a cheaper, separate rollout model can be used without sacrificing performance, and moving RL for LLMs toward more practical, compute-efficient training regimes.

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [54] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: This paper is a survey that systematically categorizes and analyzes reasoning failures in large language models (LLMs), explains their causes, and reviews mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Although LLMs show strong reasoning ability on many benchmarks, they still fail in simple and critical cases. Existing work on these failures is fragmented and lacks a unified framework, making it hard to see systemic weaknesses and to guide improvements. The authors aim to provide the first comprehensive, structured overview of how and why LLMs fail at reasoning.

Method: The authors conduct a survey of existing research on LLM reasoning failures. They propose a two-dimensional framework: (1) types of reasoning—embodied vs. non-embodied, with non-embodied split into informal (intuitive) and formal (logical); and (2) types of failures—fundamental architecture-level failures, application-specific limitations, and robustness issues due to sensitivity to small input changes. For each category, they collect and analyze representative studies, define each failure type, discuss hypothesized root causes, and summarize mitigation techniques.

Result: The survey organizes scattered work into a coherent taxonomy of reasoning types and failure modes, mapping existing studies into this structure. It highlights recurring patterns of failure, identifies which weaknesses are rooted in current LLM architectures versus specific tasks or robustness problems, and compiles known mitigation strategies. The authors also curate and release an extensive GitHub list of papers on LLM reasoning failures as a community resource.

Conclusion: LLMs still exhibit systematic reasoning weaknesses despite their strong benchmark performance. A structured view of reasoning types and failure categories is essential for understanding these weaknesses and for designing better models and methods. The survey’s taxonomy and curated literature repository provide a foundation for future research aimed at more reliable, robust, and genuinely capable reasoning in LLMs.

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [55] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: They introduce a framework to specify complex, history-dependent rewards in large or infinite-state MDPs using a more expressive temporal logic (LTLfMT) and show how to make it both theoretically tractable and practically usable via reward machines and HER.


<details>
  <summary>Details</summary>
Motivation: Standard LTL-based non-Markovian reward specifications are limited to Boolean predicates over finite, structured state spaces and often require manual predicate engineering, which is problematic for large, continuous, or heterogeneous domains. There is a need for a unified, expressive, and reusable way to specify complex tasks and non-Markovian rewards directly over rich data types, while still being computationally manageable.

Method: They use Linear Temporal Logic over finite traces with Modulo Theories (LTLfMT), where atomic predicates are first-order formulas in arbitrary theories (e.g., non-linear arithmetic) instead of Booleans. The authors: (1) identify a fragment of LTLfMT that remains tractable yet expressive enough for reward specification in infinite-state MDPs, and (2) design a practical reinforcement learning method that converts first-order temporal specifications into reward machines and integrates a tailored version of Hindsight Experience Replay (HER) to alleviate reward sparsity in continuous-control settings.

Result: They show that their identified fragment of LTLfMT can be handled efficiently for reward specification, and that their implementation using reward machines plus specialized HER successfully learns in continuous-control tasks governed by complex, non-linear arithmetic conditions. Experiments demonstrate that the tailored HER component is crucial to solving tasks with complex, sparse goals when specified using their logical framework.

Conclusion: The work provides both a theoretical and practical framework for specifying non-Markovian rewards in large or infinite-state MDPs using an expressive logic (LTLfMT). By restricting to a tractable fragment and operationalizing it through reward machines and HER, they enable natural, reusable specification of complex tasks over rich data domains and empirically validate that this approach can effectively guide learning in challenging continuous-control problems.

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [56] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: The paper investigates whether large language models act as rational, Bayesian utility-maximizing agents when used for high-stakes diagnostic decisions, and develops tests that can show when their stated probabilities cannot come from any coherent rational agent.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in high-stakes settings (e.g., medical diagnosis) where good decisions require coherent probabilistic beliefs and consistent utilities. However, their internal decision logic and whether they behave like rational agents is unclear. The authors want to systematically evaluate if LLMs’ reported probabilities and chosen actions align with Bayesian utility maximization, which is the normative standard for rational decision-making.

Method: The authors design diagnosis challenge problems in medical domains where the optimal decision under uncertainty can be precisely characterized by Bayesian decision theory. They elicit probabilities and recommended actions from several LLMs on these problems. They then compare the models’ behavior against the predictions of an ideal Bayesian utility maximizer. Critically, they derive and apply falsifiable conditions—consistency checks—under which the set of reported probabilities and actions cannot be generated by any rational agent with coherent beliefs and stable preferences.

Result: Across several medical diagnostic tasks and multiple LLMs, the authors identify systematic discrepancies between LLM outputs and the behavior of a rational Bayesian utility maximizer. In particular, they find instances where the jointly observed probabilities and actions violate the derived consistency conditions, implying that no coherent belief-utility pair could rationalize the LLM behavior. The degree and nature of these violations vary by model and task, revealing patterns in LLM reasoning and decision-making flaws.

Conclusion: LLMs, as currently deployed, often fail to behave as rational utility-maximizing agents with coherent beliefs and stable preferences in diagnostic decision problems. The proposed methodology offers a principled way to test and falsify claims about LLM rationality, especially in high-stakes domains. The findings highlight the need for caution when relying on LLMs for consequential decisions and point toward future work on aligning model outputs with Bayesian decision-theoretic standards or designing systems that compensate for these inconsistencies.

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [57] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: The paper proposes GrAlgoBench, a new benchmark based on graph algorithm problems to rigorously evaluate Large Reasoning Models (LRMs), revealing their limitations in long-context reasoning and tendencies to over-think without improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Large Reasoning Models focus on math, code, and common-sense but are limited: they do not adequately test long-context reasoning, are often not challenging enough, and their answers are hard to verify automatically. The authors want a benchmark that better probes reasoning abilities, offers controllable difficulty, and supports standardized, programmatic evaluation.

Method: The authors design GrAlgoBench, a benchmark composed of nine graph algorithm tasks specifically crafted to stress-test LRMs’ reasoning. Graph instances are constructed to require long-context processing, with graph sizes varied to control difficulty. They then systematically evaluate current LRMs on these tasks, analyzing performance trends as context length grows and inspecting reasoning traces to understand failure modes such as execution errors, memory issues, and redundant reasoning steps.

Result: The experiments show that LRM accuracy degrades heavily with increasing graph size, dropping below 50% once graphs have more than 120 nodes. The authors identify common error types—execution errors, weak memory, and redundant reasoning—and observe an over-thinking phenomenon where LRMs perform extensive self-verification that substantially lengthens reasoning traces but yields little or no accuracy gain.

Conclusion: GrAlgoBench demonstrates that graph algorithm problems are an effective, multidimensional benchmark for assessing LRMs’ reasoning capabilities. The benchmark exposes serious limitations of current LRMs in handling long contexts and avoiding unproductive over-thinking, and it provides a standardized, programmatically evaluable testbed intended to guide future research on improving reasoning in LRMs.

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [58] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: Trifuse is an attention-based GUI grounding framework that combines model attention, OCR text, and icon captions with a specialized fusion strategy to accurately localize GUI elements from natural language instructions without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: GUI agents need to map natural language instructions (e.g., “click the settings icon”) to the correct on-screen elements. Existing methods either require expensive fine-tuning on large annotated GUI datasets and still generalize poorly to unseen interfaces, or use zero-shot attention-based localization that is unreliable because GUI images lack explicit spatial anchors. There is a need for a data-efficient, generalizable grounding method that can exploit complementary signals beyond raw visual attention to improve reliability without heavy supervision.

Method: The paper proposes Trifuse, an attention-based GUI grounding framework that augments MLLM attention maps with two additional spatially aligned signals: OCR-derived textual cues (from on-screen text) and icon-level caption semantics (descriptions of icons). It introduces a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement among attention, OCR, and caption signals while preserving a sharp, unimodal localization peak. This fusion yields a more reliable heatmap of the target element location, enabling grounding without task-specific fine-tuning. The framework is evaluated with different backbone MLLMs to show generality.

Result: On four GUI grounding benchmarks, Trifuse attains strong localization performance in a zero-shot or no-task-specific-fine-tuning setting, outperforming prior attention-only approaches and substantially narrowing the gap to fully fine-tuned methods. Ablation studies demonstrate that adding OCR and icon-caption cues over plain attention consistently improves grounding accuracy across multiple backbones, validating the benefit of multi-source fusion and the proposed CS strategy.

Conclusion: Trifuse offers a general, data-efficient solution for GUI grounding by explicitly integrating complementary spatial anchors—attention, OCR text, and icon-level captions—through a dedicated fusion mechanism. It improves the reliability and accuracy of attention-based grounding without requiring costly task-specific fine-tuning, and its consistent gains across different backbones suggest it can serve as a broadly applicable foundation for more capable GUI agents.

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [59] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: The paper introduces Difficulty-Estimated Policy Optimization (DEPO), a method that uses an online difficulty estimator to filter training samples before rollouts, cutting reasoning training compute by up to half while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Group Relative Policy Optimization (GRPO), used in large reasoning models like DeepSeek-R1, can suffer from weak or noisy gradient signals when tasks are too easy or too hard, causing unstable convergence. Furthermore, existing fixes like DAPO still waste substantial compute on low-utility samples via exhaustive rollouts. The authors want a way to make reasoning alignment both more stable and considerably more compute-efficient, especially as models and datasets scale.

Method: DEPO introduces an online Difficulty Estimator that evaluates each training sample’s learning potential before performing expensive rollouts. Based on this estimated difficulty, the framework selectively filters out samples that are predicted to contribute little useful gradient signal (e.g., too trivial or too complex), so that rollouts and GRPO-style optimization are focused on mid-difficulty, high-signal data. This is integrated into the standard reasoning alignment pipeline to reallocate inference-time compute more effectively.

Result: Experiments show that DEPO can reduce rollout computation costs by up to 2x while preserving the final performance of large reasoning models compared to baselines like GRPO (and its variants). The method improves efficiency without degrading reasoning quality on the evaluated benchmarks.

Conclusion: By incorporating an online difficulty-aware filtering mechanism into the reasoning alignment process, DEPO mitigates gradient signal attenuation and avoids wasting compute on low-utility samples. This makes large reasoning model training more stable and significantly more cost-effective, providing a more sustainable strategy for scaling inference-time compute in reasoning systems.

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [60] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: The paper introduces QA-Token, a quality-aware tokenization method that incorporates signal reliability into vocabulary construction, improving performance on noisy real-world data like genomics and finance.


<details>
  <summary>Details</summary>
Motivation: Existing tokenization methods like BPE treat all parts of a sequence equally and ignore variation in signal quality or reliability, which is problematic in noisy real-world corpora such as genomic reads and financial time series. This can waste vocabulary capacity modeling unreliable regions and hurt downstream performance for tasks like variant calling or time-series prediction. The authors aim to design a tokenizer that explicitly accounts for data quality, so that foundation models trained on very large, noisy datasets can achieve better accuracy and efficiency without extra inference cost.

Method: They propose QA-Token (Quality-Aware Tokenization), which embeds data reliability into the tokenization process. First, they cast vocabulary construction and downstream task performance as a bilevel optimization problem, jointly optimizing the token vocabulary while considering its impact on the ultimate model. Second, they design a reinforcement learning algorithm that learns merge policies for tokenization, where the reward function is quality-aware and the procedure comes with convergence guarantees. Third, they use Gumbel-Softmax relaxation to make the discrete tokenization decisions differentiable, enabling adaptive parameter learning and end-to-end optimization of the tokenizer together with downstream models.

Result: Across experiments, QA-Token outperforms standard subword methods like BPE on noisy domains. In genomics, it improves F1 for variant calling by 6.7 percentage points relative to BPE. In finance, it yields a 30% improvement in Sharpe ratio. At larger scale, they apply QA-Token to a massive genomic pretraining corpus of 1.7 trillion base pairs, achieving state-of-the-art pathogen detection performance with a Matthews Correlation Coefficient of 94.53, while also reducing the overall token count by 15%, implying greater efficiency.

Conclusion: Incorporating signal quality directly into tokenization leads to more effective and efficient representation learning on noisy real-world corpora. QA-Token provides a principled, trainable, and scalable tokenization framework that jointly optimizes vocabulary and downstream performance via bilevel optimization, RL-based merge policies, and differentiable relaxation. This enables training foundation models on extremely large and noisy datasets, such as petabase-scale genomic sequences and terabytes of financial time series, improving accuracy and efficiency without any additional inference-time cost.

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [61] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: The paper argues that long-horizon failures of large language models are due to an intrinsic instability in autoregressive generation, not only task complexity, proving an exponential decay in decision advantage with sequence length and showing this necessitates segmented, graph-like reasoning structures.


<details>
  <summary>Details</summary>
Motivation: Although LLMs can reason well on short tasks, their performance collapses on long-horizon problems. Existing explanations blame task complexity (e.g., combinatorial search, long-term credit assignment), but this does not explain failures even in simple, linear tasks. The authors want a more fundamental, architectural explanation of why long autoregressive reasoning chains break down.

Method: The authors formulate long-horizon reasoning as single-path autoregressive execution and analyze its dynamics. They derive a theoretical result (Theorem A) showing that the model’s decision advantage decays exponentially with execution length, creating a stability limit. They then examine the structural implications of this theorem (need for segmentation and DAG-like execution) and empirically validate the predicted performance cliffs on synthetic tasks and TextWorld benchmarks.

Result: Theorem A proves that, in single-path autoregressive reasoning, decision advantage diminishes exponentially as sequences grow, establishing a fundamental bound on the length of reliably maintainable reasoning chains. This implies that stable long-horizon reasoning cannot be sustained purely by linear autoregressive execution; instead, it requires discrete segmentation and graph-structured computation. Experiments show clear performance cliffs matching the theoretical predictions in both controlled synthetic settings and real TextWorld tasks, confirming the practical relevance of the instability.

Conclusion: Long-horizon reasoning failures in LLMs stem from intrinsic process-level instability of autoregressive generation rather than only from task or search complexity. To maintain coherence over long horizons, reasoning systems likely need segmented, graph-like governance structures instead of relying solely on monolithic autoregressive decoding. Current short-horizon benchmarks can hide these structural limits, suggesting that progress in reasoning may depend more on architectural and governance changes than on simple model scaling.

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [62] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: This paper presents AgentCPM-Explore, a 4B-parameter LLM-based agent, and a holistic training framework that mitigates key bottlenecks (catastrophic forgetting, noisy rewards, long-context redundancy), enabling edge-scale models to match or exceed much larger models on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents currently rely mostly on large models, while the potential of smaller, edge-deployable models (around 4B parameters) is underexplored. The authors observe that performance limitations of such models may stem less from intrinsic capacity and more from training and inference stability issues. They aim to rigorously study these bottlenecks and demonstrate that, with better training, small models can achieve competitive or superior performance to larger ones, which is crucial for cost-effective, privacy-preserving, and on-device deployment.

Method: The authors systematically analyze three bottlenecks affecting 4B-scale agent models: (1) catastrophic forgetting during supervised fine-tuning (SFT), (2) high sensitivity to noisy reward signals during reinforcement learning (RL), and (3) reasoning degradation in long-context settings due to redundant information. They then design AgentCPM-Explore, a 4B-parameter agent model with high knowledge density and strong exploration ability, trained via a holistic framework combining parameter-space model fusion to mitigate forgetting, reward signal denoising to stabilize RL, and contextual information refinement to reduce redundancy and preserve reasoning quality in long contexts.

Result: AgentCPM-Explore achieves state-of-the-art performance among 4B-parameter models and matches or surpasses leading 8B-class models on four benchmarks. It even outperforms much larger models like Claude-4.5-Sonnet and DeepSeek-v3.2 on five benchmarks, including achieving 97.09% accuracy on the GAIA text-based tasks under pass@64. These empirical results demonstrate that, with the proposed training framework, a compact model can reach and sometimes exceed the performance of significantly larger models on agentic and reasoning tasks.

Conclusion: The study concludes that the primary limitation of edge-scale (4B) agent models lies not in their capacity ceiling but in unstable inference arising from suboptimal training procedures. By addressing catastrophic forgetting, reward noise sensitivity, and long-context redundancy through their holistic training framework, the authors show that small, edge-friendly models can unlock substantial, previously underestimated potential and offer a practical alternative to large-scale LLM agents for many complex tasks.

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [63] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: The paper introduces JADE, a two-layer framework for rigorously and flexibly evaluating agentic AI on open-ended professional tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for agentic AI face a trade-off: static rubrics are rigorous and reproducible but cannot handle diverse valid solution strategies, while LLM-as-a-judge methods are flexible but unstable and biased. Human experts naturally combine stable principles with dynamic, claim-level judgment, and the authors want an automated framework that mimics this to better evaluate complex, professional tasks.

Method: The authors design JADE, a two-layer evaluation framework. Layer 1 encodes domain expert knowledge as a fixed set of evaluation skills, establishing stable, reusable criteria. Layer 2 dynamically performs claim-level assessment for each specific agent report, judging the validity of individual claims and using an evidence-dependency mechanism to ensure that conclusions based on refuted claims are downgraded or invalidated. They test JADE on BizBench and a medical benchmark, and compare it against holistic LLM-based evaluators and expert rubrics.

Result: On BizBench, JADE produces more stable evaluations and uncovers critical failure modes of agentic AI that holistic LLM judges miss. It shows strong agreement with expert-authored rubrics and transfers effectively to a medical-domain benchmark, indicating cross-domain robustness.

Conclusion: JADE successfully reconciles rigor and flexibility in evaluating agentic AI on open-ended professional tasks. By combining a stable, expert-informed skill layer with dynamic, claim-level evaluation, it yields more reliable and insightful assessments than holistic LLM judges and aligns well with expert standards across multiple domains.

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [64] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER is a dynamic, training-free control policy for multi-path chain-of-thought in mixture-of-experts LMs that adaptively balances exploration and exploitation during test-time, improving reasoning accuracy while using fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling with multi-path chain-of-thought can improve reasoning, but existing methods either rigidly enforce exploration via tree search or over-explore via naive parallel decoding, leading to inefficient compute use and suboptimal answer selection. Moreover, the optimal exploration-exploitation balance changes across stages of reasoning, and correct vs. incorrect paths often diverge only late in the process, which current methods fail to handle well.

Method: The authors formulate test-time scaling as a dynamic expand-reduce control problem over a pool of candidate reasoning paths (hypotheses). They introduce HyPER, a training-free online controller for mixture-of-experts models that reallocates a fixed compute budget across paths using lightweight per-path statistics. HyPER includes: (1) an online controller that gradually shifts from exploration (expanding diverse paths) to exploitation (focusing compute on promising paths) as decoding proceeds; (2) a token-level refinement mechanism that exploits promising trajectories without fully resampling entire paths; and (3) a length- and confidence-aware aggregation strategy at answer time to select or combine final hypotheses more reliably.

Result: On four different mixture-of-experts language models and multiple reasoning benchmarks, HyPER delivers a better accuracy-compute frontier than baselines: it improves reasoning accuracy by about 8–10 percentage points while simultaneously reducing token usage (and thus test-time compute) by about 25–40%.

Conclusion: Dynamic, online control of exploration and exploitation during multi-path chain-of-thought decoding yields significantly better use of test-time compute than rigid tree search or naive parallel reasoning. HyPER demonstrates that a training-free, statistics-driven controller in mixture-of-experts LMs can meaningfully improve reasoning accuracy while reducing token cost, highlighting the value of adaptive path management for test-time scaling.

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [65] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: The paper introduces AgentCPM-Report, a lightweight 8B-parameter local deep-research agent and framework that mimics human writing using a dynamic Writing As Reasoning Policy (WARP), enabling higher-quality research reports without relying on large closed-source models.


<details>
  <summary>Details</summary>
Motivation: Current deep research report generation depends on a plan-then-write paradigm that is highly sensitive to the initial outline and usually requires powerful closed-source or online large models. This creates deployment, safety, and privacy issues, and limits the usability of smaller, local models. The paper aims to enable smaller models to perform high-quality deep research and writing while addressing these concerns.

Method: The authors design AgentCPM-Report, consisting of (1) a framework that mirrors human writing via a Writing As Reasoning Policy (WARP), in which the model can dynamically revise and evolve its outline while writing; and (2) an 8B-parameter deep research agent. The WARP framework alternates between Evidence-Based Drafting (collecting information and drafting content) and Reasoning-Driven Deepening (refining knowledge and structure), allowing iterative outline evolution. To train the small model with these capabilities, they propose a Multi-Stage Agentic Training strategy including cold-start training, atomic skill reinforcement learning (RL) for individual capabilities, and holistic pipeline RL for end-to-end behavior.

Result: On benchmarks such as DeepResearch Bench, DeepConsult, and DeepResearch Gym, AgentCPM-Report achieves superior performance compared to leading closed-source systems, particularly showing large improvements on metrics related to Insight in generated research reports.

Conclusion: A carefully designed agentic framework plus multi-stage RL training can equip a relatively small (8B) local model with strong deep-research and report-writing abilities. AgentCPM-Report reduces reliance on large closed-source models, mitigates privacy and deployment concerns, and sets a new performance bar—especially for insight quality—on several deep research benchmarks.

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [66] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: The paper analyzes why common RL algorithms for training LLM agents lack convergence guarantees in multi‑turn settings and proposes SeeUPO, a new critic‑free RL method with provable convergence and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: Backbone RL methods like PPO and REINFORCE are widely used to train LLM-based agents, but in multi-turn interactions they often lack formal convergence guarantees, leading to instability and suboptimal policies. The authors want to understand how policy update rules and advantage estimators interact in such settings, identify theoretical limitations of existing methods, and design an algorithm that is both critic-free and provably convergent for multi-turn LLM training.

Method: (1) Theoretically analyze combinations of policy update schemes (e.g., REINFORCE, PPO) and advantage estimation methods (especially Group Relative Advantage Estimation, GRAE) in single- and multi-turn environments, studying monotonic improvement and convergence. (2) Prove that REINFORCE+GRAE can reach a globally optimal solution under undiscounted conditions, while PPO+GRAE loses PPO’s monotone improvement guarantee, and show impossibility of simultaneously having critic-free operation and convergence guarantees for standard multi-turn setups with mainstream methods. (3) Propose SeeUPO, which reframes multi-turn interaction as a sequence of multi-agent bandit problems and updates policies turn-by-turn in reverse order of execution, essentially using backward induction to guarantee monotonic improvement and convergence. (4) Empirically evaluate SeeUPO on AppWorld and BFCL v4 benchmarks using Qwen3-14B and Qwen2.5-14B base models, comparing performance and stability against existing RL backbones.

Result: The analysis shows that while REINFORCE combined with GRAE converges to a globally optimal policy in the undiscounted case, using GRAE with PPO destroys PPO’s original monotonic improvement property. Additionally, they prove that existing backbone RL methods cannot be both critic-free and convergent in multi-turn scenarios. Their proposed SeeUPO algorithm, designed specifically for sequence-level, turn-wise updates modeled as bandit problems, achieves monotonic improvement with theoretical convergence guarantees. Experiments demonstrate large relative performance gains over strong baselines: 43.3%-54.6% improvement on Qwen3-14B and 24.1%-41.9% improvement on Qwen2.5-14B across AppWorld and BFCL v4, alongside improved training stability.

Conclusion: Standard RL backbones used for LLM agents are theoretically inadequate for multi-turn training when both critic-free operation and convergence guarantees are desired. REINFORCE+GRAE has good theoretical properties but PPO+GRAE violates PPO’s core guarantee. The proposed SeeUPO algorithm resolves this gap by reframing multi-turn interactions as sequential bandits and applying reverse-order policy updates, yielding provable monotonic improvement and convergence. Empirical results on challenging LLM-agent benchmarks confirm that SeeUPO is both more stable and substantially more effective than commonly used methods.

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [67] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: The paper argues that standard robustness evaluations for vision-language models (VLMs) are insufficient and proposes internal-representation-based metrics that reveal hidden brittleness and three specific failure modes.


<details>
  <summary>Details</summary>
Motivation: Existing robustness tests for VLMs mostly look at whether the final answer stays the same under input perturbations. This assumes that if the answer is stable, the internal multimodal processing is also stable, which the authors believe is a flawed assumption. They want to understand how perturbations affect the internal embeddings and structure of model representations, and whether model scale or task type changes this behavior.

Method: They design a representation-aware and frequency-aware evaluation framework. It measures: (1) internal embedding drift—how much hidden representations change under perturbations; (2) spectral sensitivity—how sensitive representations are across spatial frequency components; and (3) structural smoothness—how spatially consistent the vision token representations are. These are computed in addition to traditional label-based robustness metrics. They apply this framework to several modern VLMs evaluated on SEEDBench, MMMU, and POPE under various input perturbations such as text overlays.

Result: The framework reveals three main phenomena. (1) VLMs often keep the same predicted answer even when internal representations change drastically; for some perturbations like text overlays, the drift is as large as the natural variability between different images, meaning the model’s representation moves into regions associated with unrelated inputs while outputs remain unchanged. (2) Increasing model size does not make internal representations more robust: larger models are more accurate but display equal or higher sensitivity in the new metrics, suggesting sharper and more fragile decision boundaries. (3) Perturbations affect tasks in a task-dependent way: they particularly hurt reasoning tasks when they interfere with the integration of coarse and fine visual information, but on hallucination benchmarks, the same perturbations can actually lower hallucination rates by pushing models toward more conservative, less speculative responses.

Conclusion: Output-level robustness is an incomplete and potentially misleading measure of VLM robustness. Internal representation metrics such as embedding drift, spectral sensitivity, and structural smoothness reveal hidden instability and distinct failure modes that are not captured by answer-level metrics. Robustness does not simply improve with scale, and perturbation effects are highly task-dependent. Therefore, future evaluation and model design should explicitly account for representation-level robustness and the different ways perturbations interact with reasoning and hallucination behavior.

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [68] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: The paper introduces ARK, an autoregressive framework that generates entire knowledge graphs as sequences of triples, achieving high semantic validity and enabling controlled, conditional graph generation via a variational extension called SAIL.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge graph tasks like link prediction treat triples independently and fail to model complex semantic dependencies and domain validity constraints across entire subgraphs. There is a need for generative models that can produce semantically coherent, domain-valid knowledge graphs, capturing type consistency, temporal constraints, and relational patterns, and supporting tasks like completion and query answering.

Method: The authors propose ARK, which linearizes a knowledge graph into a sequence of (head, relation, tail) triples and trains autoregressive models to generate these sequences, implicitly learning semantic and validity constraints from data without explicit rules. They explore different architectures (including recurrent and transformer-based) and analyze the impact of capacity (hidden dimensionality) versus depth. They further introduce SAIL, a variational extension that adds a latent representation to support controlled generation, allowing both unconditional sampling of graphs and conditional completion from partial graphs.

Result: On the IntelliGraphs benchmark, ARK models achieve 89.2% to 100.0% semantic validity across various datasets while generating graphs that are novel relative to training data. The study finds that model capacity, specifically hidden dimensionality of at least 64, is more important than architectural depth for KG generation. Recurrent models achieve semantic validity comparable to transformer models but with significantly lower computational cost.

Conclusion: Autoregressive modeling of knowledge graphs as triple sequences is an effective approach for generating semantically valid and novel KGs. The ARK family, along with its variational extension SAIL, can capture complex semantic constraints without explicit rules and supports both unconditional and conditional generation, making it practical for applications such as knowledge base completion and query answering. Model capacity is a key driver of performance, and efficient recurrent architectures can match transformers in validity while being more computationally efficient.

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [69] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: They propose a new way to embed temporal-logic task specifications into a multi-task RL policy using semantically labeled automata, enabling efficient on-the-fly computation and strong generalization to complex, unseen tasks.


<details>
  <summary>Details</summary>
Motivation: In multi-task RL, we want a single policy that can handle many tasks – including unseen ones – but we need a compact, informative representation of each task. Linear temporal logic (LTL) is powerful for specifying complex temporal goals, yet existing ways of turning LTL into signals for RL either don’t scale, lose semantic structure, or handle only restricted fragments of LTL. The authors aim to exploit advances in LTL-to-automata translations to get richer, more structured task representations that improve learning and generalisation.

Method: They represent each task as an LTL formula and translate it using a new generation of semantic LTL-to-automata tools originally designed for temporal synthesis. These translations produce semantically labeled automata whose states carry rich structural information about temporal requirements. The agent’s policy is conditioned on task embeddings derived from these automaton states. The automaton is constructed and updated on-the-fly during interaction, rather than fully precomputed, enabling efficiency and support for full LTL. The resulting task embeddings are used within a standard RL framework for multi-task learning, training a single universal policy over many LTL-specified tasks.

Result: Across several benchmark domains, their method outperforms prior LTL-based and multi-task RL approaches in terms of success rate, sample efficiency, and generalisation to unseen task specifications. It scales to more complex, full-LTL specifications for which existing baselines either fail to learn or become computationally infeasible.

Conclusion: By leveraging semantically labeled LTL automata as task embeddings, multi-task RL agents can efficiently learn a single policy that generalises across a wide range of temporally complex tasks. The method supports full LTL, scales to challenging specifications, and attains state-of-the-art empirical performance, indicating that structured temporal logic semantics are highly beneficial for task representation in RL.

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [70] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: The paper studies how different active learning strategies interact with a neuro-symbolic Bayesian concept learner that uses LLM-generated programs as hypotheses, showing that a simple confirmation-biased strategy can outperform information-gain maximization on simple concepts due to support mismatch issues.


<details>
  <summary>Details</summary>
Motivation: Human concept learning is often active and seemingly biased toward confirming current hypotheses rather than seeking maximally informative falsifications, which classical rational models view as suboptimal. With modern neuro-symbolic models that use LLMs to propose program-like hypotheses, there is a need to understand how active learning strategies affect inference when the hypothesis generator has limited and structured support. The authors aim to explain when and why human-like confirmation strategies might actually be adaptive in such settings, rather than cognitively erroneous.

Method: They build a neuro-symbolic Bayesian learner where hypotheses are executable programs proposed by a large language model and then reweighted via Bayesian updating, using a particle-based approximation to the posterior. They then implement and compare two query selection policies in classic Number Game concept-learning tasks: (1) a Rational Active Learner that greedily maximizes approximate expected information gain (EIG) over queries, and (2) a Positive Test Strategy (PTS) that preferentially queries instances classified as positive by the current best hypothesis. Through simulations across a variety of concept types, they analyze performance differences and diagnose failures using the notion of support mismatch between the EIG-induced query distribution and the LLM’s proposal distribution over hypotheses.

Result: The EIG-driven learner performs well on complex concepts requiring falsification, such as compound rules or those with many exceptions, by efficiently driving the learner to informative boundary cases. However, on simple concepts, EIG underperforms relative to the PTS strategy. The authors trace this to a support-mismatch trap: boundary-focused EIG queries shift posterior mass toward regions of hypothesis space where the LLM tends to generate invalid or overly specific programs, degrading the particle approximation and slowing learning. In contrast, PTS, while information-suboptimal in principle, tends to choose “safe” queries that keep the posterior within regions where the generator reliably proposes valid hypotheses, enabling faster convergence for simple rules.

Conclusion: The study concludes that in neuro-symbolic Bayesian learners built on top of LLM-based program generators, aggressively maximizing information gain can interact poorly with the generator’s limited and structured support, leading to inference pathologies. A human-like Positive Test Strategy, akin to psychological notions of confirmation bias, can be more robust and efficient for simple concept learning because it maintains proposal validity and avoids support mismatch. This suggests that what appears as confirmation bias in human active learning may reflect a rational adaptation to maintain tractable inference in sparse, open-ended hypothesis spaces, rather than a simple cognitive error.

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [71] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: The paper presents ScaleEnv, a framework to automatically generate fully interactive environments and verifiable tasks from scratch for training generalist agents, addressing the scarcity and limited diversity of current environments.


<details>
  <summary>Details</summary>
Motivation: Generalist agents need rich, diverse, interactive environments for self-exploration and learning, but such environments are scarce and existing synthesis methods have limited diversity and scalability, hindering robust generalization.

Method: ScaleEnv procedurally constructs environments and tasks from scratch, uses procedural testing to ensure environment reliability, and builds task tool-dependency graphs with executable action verification to guarantee task completeness and solvability. Agents are then trained via exploration in these generated environments.

Result: Agents trained within ScaleEnv achieve significant performance gains on unseen, multi-turn tool-use benchmarks such as τ^2-Bench and VitaBench, exhibiting strong generalization. The authors also empirically study how expanding the number of domains (environmental diversity) affects generalization performance.

Conclusion: Automatically scaling up diverse, reliable, fully interactive environments via ScaleEnv is an effective strategy for training robust, generalist agents, and increasing environmental diversity is shown to be critical for improving generalization in multi-turn tool-use tasks.

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [72] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: They propose POP, an online, context-aware structural pruning framework for large foundation models that dynamically prunes channels during generation to reduce computation and latency with better accuracy than prior pruning methods.


<details>
  <summary>Details</summary>
Motivation: Existing structural pruning for large foundation models determines a fixed sparsity pattern before or at the start of inference. This ignores the fact that in autoregressive generation, token-by-token contexts can change which parts of the model are important. As a result, current methods lose accuracy or waste compute because they cannot adapt pruning decisions to the evolving context, and many require expensive offline calibration or retraining.

Method: POP (Partition-guided Online Pruning) splits model channels into three regions: retained (always kept), candidate (potentially pruned), and pruned (always removed). During the prefilling phase, POP builds a coarse pruning partition that identifies consistently important channels (retained) and clearly unimportant ones (pruned). During decoding, instead of reconsidering all channels, POP derives a fine-grained, context-conditioned pruning mask only within the candidate region, based on the current decoding step. This design enables dynamic pruning with limited overhead and requires no offline calibration, retraining, or learned predictors, making it plug-and-play across different LFMs including LLMs, MoEs, and VLMs.

Result: Across a variety of large foundation models—LLMs, MoE models, and VLMs—POP achieves higher accuracy than existing pruning baselines while also reducing computational overhead and inference latency. It preserves performance by keeping consistently important weights via the coarse partition and gains additional efficiency by dynamically masking less important channels in the candidate region during decoding.

Conclusion: Partition-guided Online Pruning provides an effective way to introduce dynamic, context-aware structural sparsity into large foundation models without expensive preprocessing. By combining a coarse, prefilling-based partition with fine-grained, decoding-stage masking, POP improves the accuracy-efficiency tradeoff compared to prior pruning methods and can be easily integrated into diverse model architectures as a lightweight, plug-and-play inference-time technique.

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [73] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: Proposes a game-theoretic framework to model and steer populations of LLMs via Nash equilibria, by having models strategically align with mixtures of human subpopulations, enabling analytical predictions and alignment interventions.


<details>
  <summary>Details</summary>
Motivation: As multiple LLMs increasingly interact in social and online environments, their collective behavior can yield undesirable societal effects, such as systematically ignoring or marginalizing certain groups. Existing alignment methods like RLHF focus on individual models and do not offer system-level, game-theoretic tools to predict or control the joint behavior of many LLMs. Moreover, performing equilibrium analysis directly in the space of open-ended text is intractable, limiting our ability to reason about multi-agent LLM dynamics and design interventions that promote socially desirable outcomes.

Method: The authors construct a game-theoretic model in which each LLM agent’s action is represented not as raw text but as a probability mixture over a set of human subpopulations. Each agent strategically chooses which subpopulations to align with, forming an interpretable policy space. Under standard assumptions of concave utilities, they analytically derive closed-form characterizations of Nash equilibria for the population of LLM agents. Their framework serves as an active alignment layer that sits on top of existing alignment techniques like RLHF, allowing steering of the equilibrium by adjusting alignment targets. They then instantiate this framework in a social-media-style environment involving many LLMs serving different users.

Result: Theoretical results provide closed-form expressions for Nash equilibria in terms of mixtures over human subpopulations, enabling tractable prediction of how a population of LLMs will collectively align. Empirically, in a social-media setting, they observe that a population of LLMs—especially those with stronger reasoning capabilities—can display political exclusion, where some human subpopulations receive effectively no attention from any LLM. Applying their active alignment method mitigates these exclusion pathologies and leads to more inclusive coverage of subpopulations at equilibrium.

Conclusion: Modeling LLM populations through game-theoretic mixtures over human subpopulations makes equilibrium analysis and steering tractable and interpretable. This approach surfaces and explains potential systemic failures such as political exclusion that may arise from naive deployment of many LLMs, particularly reasoning-focused ones. By serving as an active alignment layer atop existing pipelines, the method offers explicit levers for nudging multi-agent LLM dynamics toward more socially desirable, inclusive outcomes in domains like social media and beyond.

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [74] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: The paper proposes an adaptive differentially private federated learning framework that stabilizes training and improves accuracy under heterogeneous devices and Non-IID data by compressing local representations, adaptively clipping gradients, and aggregating updates in a constraint-aware manner.


<details>
  <summary>Details</summary>
Motivation: In real-world federated learning, device heterogeneity and Non-IID data make client gradients unstable and biased. When standard differential privacy mechanisms like fixed clipping and Gaussian noise are added, these issues worsen, causing oscillatory training, excessive gradient perturbation, and poor model performance. There is a need for methods that maintain privacy guarantees while improving training stability and efficiency under such heterogeneous and constrained conditions.

Method: The framework introduces three main components. On the client side, a lightweight local compressed module regularizes intermediate representations and constrains gradient variability, which reduces the amplification of DP noise during local optimization. On the server side, an adaptive gradient clipping strategy adjusts clipping thresholds over time based on historical update statistics, aiming to prevent both over-clipping and noise-dominated updates. Additionally, a constraint-aware aggregation mechanism down-weights or suppresses unreliable or noise-dominated client updates during aggregation to stabilize the global optimization process.

Result: Through experiments on CIFAR-10 and SVHN under heterogeneous and privacy-constrained settings, the proposed approach shows more stable convergence behavior and achieves higher classification accuracy compared to baseline differentially private federated learning methods.

Conclusion: Adaptive control of representation compression, gradient clipping, and aggregation under differential privacy can significantly mitigate instability and performance degradation in federated learning with Non-IID data and heterogeneous devices, leading to more efficient and accurate privacy-preserving collaborative training.

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [75] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: The paper compares traditional attribution-based explainability (designed for static, one-shot model predictions) with trajectory-based, trace-grounded diagnostics for agentic LLM systems, showing that attribution methods work well for static tasks but fail to reliably diagnose multi-step agent behaviour, whereas trace-based rubrics can localize where trajectories break down.


<details>
  <summary>Details</summary>
Motivation: Most explainable AI work assumes a static setting: a fixed model that maps inputs to outputs once, with explanations focusing on which input features influenced that single prediction. However, modern LLM-based agents act over multiple steps, where success or failure depends on entire trajectories of actions and intermediate states. Existing attribution methods may not directly transfer to this new context, and the community lacks a clear empirical comparison between static and agentic explainability approaches. The paper is motivated by the need to understand whether attribution-based explanations remain useful in multi-step agent settings and to identify appropriate methods to diagnose where complex agentic systems fail.

Method: The authors systematically compare two explanation paradigms: (1) attribution-based explanations used in standard static classification tasks, and (2) trace-based diagnostics used in agentic benchmarks. For the static setting, they evaluate feature attribution methods and quantify the stability of feature rankings, e.g., via Spearman rank correlation. For the agentic setting, they use benchmarks such as TAU-bench Airline and AssistantBench, which involve multi-step LLM agents. They apply trace-grounded rubric evaluations to agent trajectories, analyzing execution-level behaviour (e.g., state tracking, decision consistency) and identifying where trajectories break down. They then measure how frequently particular failure modes occur and how strongly they correlate with task success.

Result: In static classification settings, attribution methods produce stable and consistent feature rankings, as shown by a high Spearman rank correlation (ρ = 0.86), indicating that they are reliable for explaining single-step predictions. However, when these attribution-style approaches are extended to agentic, multi-step trajectories, they fail to reliably diagnose execution-level failures—i.e., they do not pinpoint where and why agents go wrong over time. In contrast, trace-grounded rubric evaluations on agentic benchmarks successfully localize behaviour breakdowns. A notable empirical finding is that state tracking inconsistency is 2.7 times more common in failed runs than in successful ones, and the presence of such inconsistency reduces the probability of success by 49%.

Conclusion: Attribution-based explainability methods are effective and stable for traditional static prediction tasks but are ill-suited for diagnosing failures in agentic, multi-step LLM systems. For these systems, trajectory-level, trace-grounded diagnostics are more informative, enabling precise localization of behaviour breakdowns and quantification of key failure modes such as state tracking inconsistencies. The paper concludes that explainability research for agentic AI should shift focus from single-output attribution to trajectory-level explanations that operate over full decision and interaction traces, and it offers an empirical framework and benchmarks to support this transition.

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [76] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper introduces AIRS-Bench, a benchmark of 20 real-world ML research tasks to evaluate and advance LLM-based autonomous research agents across the full research lifecycle.


<details>
  <summary>Details</summary>
Motivation: LLM agents are increasingly used for scientific research, but there is no standardized, realistic benchmark that evaluates their end-to-end research capabilities (idea generation through iterative experimentation) on challenging, modern ML problems without relying on pre-provided baseline code. The authors aim to create such a benchmark to measure progress and identify gaps between current agents, human SOTA, and theoretical performance limits.

Method: The authors construct AIRS-Bench, a collection of 20 tasks drawn from contemporary ML research in language modeling, math, bioinformatics, and time series forecasting. Each task is designed to test agentic behavior over the full research cycle (hypothesis generation, experiment design, analysis, and refinement) without giving baseline implementations. They define a flexible task format to accommodate future tasks and fair comparisons among agent frameworks. They then evaluate frontier LLMs wrapped in both sequential and parallel scaffolding/agent frameworks as baselines, comparing their results against human SOTA and theoretical ceilings.

Result: On AIRS-Bench, LLM agents outperform human state-of-the-art on 4 tasks but underperform on the remaining 16. Even in the 4 tasks where they beat human SOTA, their performance still falls short of the theoretical maximum/ceiling achievable on those tasks. This shows both nontrivial capability and substantial remaining gaps.

Conclusion: AIRS-Bench is a challenging, unsaturated benchmark for autonomous research agents. Current leading LLM-based agents can sometimes surpass human SOTA but remain far from the theoretical limits on most tasks, indicating significant headroom for improvement. By open-sourcing task definitions and evaluation code, the authors aim to standardize evaluation and accelerate progress in autonomous scientific research agents.

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [77] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: The paper investigates how accurately AI agents can predict their own task success and finds systematic overconfidence, with some agents predicting a 77% success rate while only succeeding 22% of the time.


<details>
  <summary>Details</summary>
Motivation: As AI agents are increasingly used autonomously, it is important to understand and measure their ability to estimate their own chances of success, which is critical for reliability, safety, and human oversight. The paper is motivated by the gap between actual performance and self-assessed confidence, and by the need for better ways to elicit and calibrate these self-assessments.

Method: The authors elicit probabilistic success estimates from AI agents at three phases: before task execution, during execution, and after execution. They compare these self-predictions to actual success frequencies to measure calibration and overconfidence, and they test different prompting strategies, including an adversarial framing that asks the model to find bugs or failures.

Result: They find consistent agentic overconfidence across conditions, with some agents that actually succeed only 22% of the time reporting a 77% predicted success probability. Surprisingly, success predictions made before starting the task often discriminate success vs. failure better than post-execution reviews that have more information, though these differences are not always statistically significant. The adversarial “bug-finding” prompting strategy yields the best-calibrated uncertainty estimates.

Conclusion: AI agents currently show systematic overconfidence in predicting their own task success, which has implications for deployability and risk management. However, calibration can be improved via prompt design, particularly by reframing self-assessment as an adversarial bug-finding exercise. Pre-execution estimates can already be quite informative, suggesting that self-knowledge is present but poorly calibrated and that better elicitation methods are key to robust agentic uncertainty estimates.

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>
