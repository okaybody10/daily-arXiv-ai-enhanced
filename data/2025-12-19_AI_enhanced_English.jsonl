{"id": "2512.15907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15907", "abs": "https://arxiv.org/abs/2512.15907", "authors": ["Tejas Anvekar", "Juhna Park", "Aparna Garimella", "Vivek Gupta"], "title": "TabReX : Tabular Referenceless eXplainable Evaluation", "comment": null, "summary": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.", "AI": {"tldr": "The paper introduces TabReX, a reference-free, graph-based framework to evaluate tables generated by LLMs, along with a robustness benchmark, showing superior correlation with expert judgments and robustness to perturbations.", "motivation": "Evaluating LLM-generated tables is difficult because existing methods either lose table structure by converting tables to plain text or rely on fixed reference tables, which restrict generalization and flexibility. There is a need for a structural, interpretable, and robust evaluation method that does not depend on reference outputs.", "method": "TabReX converts both the source text (from which a table should be generated) and the generated table into canonical knowledge graphs. It then uses an LLM-guided graph matching process to align entities and relations, and computes rubric-aware scores capturing structural and factual fidelity. The framework allows tuning the trade-off between sensitivity and specificity and yields cell-level error traces. The authors also build TabReX-Bench, a benchmark spanning six domains, twelve planner-driven perturbation types, and three difficulty tiers to systematically stress-test evaluation metrics.", "result": "TabReX\u2019s metric aligns best with expert rankings, is more robust under increasingly difficult perturbations, and supports detailed analyses of how different models and prompts affect tabular generation quality.", "conclusion": "TabReX establishes a new, explainable, reference-less paradigm for evaluating structured (tabular) generation, offering interpretable metrics, robustness to perturbations, and fine-grained diagnostic capabilities, as demonstrated on the TabReX-Bench benchmark."}}
{"id": "2512.15925", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.15925", "abs": "https://arxiv.org/abs/2512.15925", "authors": ["Joel Mire", "Maria Antoniak", "Steven R. Wilson", "Zexin Ma", "Achyutarama R. Ganti", "Andrew Piper", "Maarten Sap"], "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception", "comment": "Presented at IC2S2 2025; Under Review (ARR Oct 2025)", "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.", "AI": {"tldr": "The paper introduces SocialStoryFrames, a framework and models to computationally infer nuanced reader responses to stories using social media narratives and conversational context.", "motivation": "Existing computational models of narrative focus mostly on surface content or basic sentiment and lack the ability to capture rich reader responses such as inferred author intent, moral judgments, and affective reactions. This limits large-scale, nuanced analysis of storytelling practices, especially in online communities.", "method": "The authors design SocialStoryFrames, a taxonomy and formalism for categorizing reader responses (e.g., perceived author intent, explanatory/predictive inferences, affect, and value judgments) grounded in narrative theory, pragmatics, and psychology. They build two models: SSF-Generator, which generates plausible reader responses from story and context, and SSF-Classifier, which classifies responses into taxonomy categories. They validate SSF-Generator using human surveys with 382 participants and SSF-Classifier using expert annotations. They then apply the models to a curated corpus (SSF-Corpus) of 6,140 social media stories from diverse online contexts.", "result": "The models can reliably generate and classify plausible reader responses aligned with human judgments, as evidenced by survey and expert-annotation validation. Applying the models to the SSF-Corpus reveals patterns in how often different storytelling intents occur, how those intents co-occur, and how narrative practices and their diversity vary across different online communities.", "conclusion": "SocialStoryFrames provides a scalable, taxonomy-based way to model fine-grained, context-sensitive reader responses to stories. This enables new types of empirical research on storytelling in online communities, including systematic comparisons of narrative practices and intents across groups, which were previously difficult with existing computational tools."}}
{"id": "2512.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16034", "abs": "https://arxiv.org/abs/2512.16034", "authors": ["Kieran Henderson", "Kian Omoomi", "Vasudha Varadarajan", "Allison Lahnala", "Charles Welch"], "title": "Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms", "comment": null, "summary": "Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.", "AI": {"tldr": "This paper investigates which types of self-disclosure sentences (personal information about annotators) are most useful for modeling annotators and predicting their labels in subjective social norm judgment tasks.", "motivation": "Previous work has shown that including persona or self-disclosure information can help predict annotator labels on subjective tasks, but the amount and nature of that information were limited and underexplored. The authors aim to understand what kinds of personal information (e.g., demographics, attitudes, relationships, experiences) actually matter most for predicting how people judge social norms, and whether theory-driven categorizations of self-disclosures are more useful than automatically derived ones.", "method": "The authors categorize annotators\u2019 self-disclosure sentences into interpretable types such as demographics, attitudes, relationships, and experiences. They then build annotator models for predicting judgments of social norms using these categorized disclosures. Through ablation studies, they systematically remove or vary different categories of information to see how prediction performance changes. They also compare theory-based groupings of self-disclosure categories against automatically induced clusters, test how many related comments are needed to obtain good performance, and analyze the effect of having a more diverse set of annotator self-disclosures.", "result": "The experiments show that demographic information contributes more to predicting annotator judgments of social norms than other categories like attitudes, relationships, or experiences. Models using theory-based categorizations of self-disclosure outperform those relying on automatically derived clusters. In contrast to prior work suggesting large amounts of data are needed, the authors find that only a small number of related comments per annotator are sufficient. Additionally, models benefit when the pool of annotator self-disclosures is more diverse.", "conclusion": "The study concludes that, for modeling annotators\u2019 judgments of social norms, demographic self-disclosures are especially informative, and carefully designed, theory-based categorizations of self-disclosure data are more effective than automatic clustering. Only a modest quantity of related comments is necessary, but it is important that the self-disclosures come from a diverse set of annotators. These findings can guide the design of better annotator models and data collection strategies for subjective NLP tasks involving social judgments."}}
{"id": "2512.15736", "categories": ["cs.AI", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.15736", "abs": "https://arxiv.org/abs/2512.15736", "authors": ["S. K. Rithvik"], "title": "Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments", "comment": null, "summary": "We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.", "AI": {"tldr": "Anubuddhi is a multi-agent AI that takes natural language prompts and automatically designs and simulates quantum optics experiments, lowering the barrier for non-programmers.", "motivation": "Designing and numerically simulating quantum optics experiments usually requires deep domain knowledge plus specialized programming skills and tooling. This limits accessibility for students and non-experts, and slows early-stage exploratory design even for experts. The paper aims to democratize and streamline this process by letting users describe desired experiments in natural language and automatically receiving simulated setups that capture the correct physics.", "method": "The authors build a multi-agent AI architecture that: (1) interprets natural language prompts via intent routing; (2) retrieves appropriate optical components from a three-tier toolbox using semantic search; (3) composes an optical layout; (4) validates and iteratively refines the design through physics simulations with two complementary engines, QuTiP and a more flexible FreeSim framework. Knowledge-augmented generation guides the system, and alignment between design and intended physics is assessed via expert scoring.", "result": "Across 13 benchmark quantum optics experiments\u2014including foundational interference setups, quantum information protocols, and advanced systems such as boson sampling\u2014the system typically reaches high design-simulation alignment scores of 8\u20139/10. Most produced simulations qualitatively match the target physical behaviors. FreeSim, a less constrained simulation mode, successfully handled 11 of 13 cases better than more rigid frameworks, highlighting the need for adaptable mathematical models to cover the diversity of quantum optics experiments.", "conclusion": "Anubuddhi can reliably generate structurally correct quantum optics experiments from natural language and simulate their qualitative physics, though precise numerical predictions still require expert oversight. The distinction between structural correctness and quantitative accuracy is crucial: the system is best viewed as a powerful assistant for initial design and pedagogy, not a replacement for expert validation. Its free-form simulation capability appears especially important for covering varied experiment types, suggesting that flexible, extensible backends are key to scalable automated experiment design."}}
{"id": "2512.16041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16041", "abs": "https://arxiv.org/abs/2512.16041", "authors": ["Yuanning Feng", "Sinan Wang", "Zhengxiang Cheng", "Yao Wan", "Dongping Chen"], "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "comment": null, "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.", "AI": {"tldr": "The paper introduces Sage, a human-label-free evaluation suite for assessing how reliably large language models act as judges by measuring their internal preference consistency instead of comparing to human-annotated ground truth.", "motivation": "Existing LLM-as-a-Judge benchmarks depend on human-annotated labels, which are costly, biased, and not necessarily reliable as a gold standard. This hampers both the scalability and the objectivity of evaluating LLM judges that are increasingly used for model evaluation and reward modeling. The authors want a way to evaluate LLM judges that avoids human bias and instead tests whether the models\u2019 own preferences are rational and internally consistent.", "method": "The authors design Sage, an evaluation suite grounded in rational choice theory. Sage defines two metrics: (1) local self-consistency, which measures stability of pairwise preferences when the same comparison is asked in varied but equivalent ways; and (2) global logical consistency, which checks transitivity across a full set of preferences among multiple candidate answers. They construct a dataset of 650 questions by mixing structured benchmark problems with real user-like queries, then query various LLMs as judges in scoring and pairwise-comparison modes, analyzing their consistency under Sage\u2019s metrics. They also compare Sage scores with supervised benchmarks such as LLMBar and RewardBench2, and run ablations on factors like explicit rubrics, fine-tuning, panel-based judging, and deep reasoning prompts.", "result": "Sage\u2019s unsupervised consistency metrics are empirically stable and show high correlation with existing supervised benchmarks, indicating that they are meaningful proxies for judge quality. The evaluation reveals that even state-of-the-art models like Gemini-2.5-Pro and GPT-5 exhibit serious reliability issues as judges, failing to maintain consistent preferences in roughly 25% of harder cases. The authors identify a phenomenon they term situational preference, where the model\u2019s judgment depends heavily on contextual framing, and show that explicit rubrics, fine-tuning for judging, panel-style aggregation, and encouraging deeper reasoning all improve consistency. They also observe notable inconsistency in human judgments themselves.", "conclusion": "Sage provides a scalable, human-label-free framework for evaluating LLM-as-a-Judge systems based on internal preference consistency rather than agreement with human annotations. The findings indicate that current leading LLMs are still unreliable as judges, especially on difficult comparisons, largely due to situational preferences. Methodological interventions\u2014clear rubrics, dedicated fine-tuning, panel-based aggregation, and deep reasoning prompts\u2014can mitigate but not fully solve these issues. Moreover, substantial human inconsistency suggests that human labels may not be an ideal gold standard, underscoring the value of consistency-based, axiomatic evaluation approaches like Sage."}}
{"id": "2512.15740", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.15740", "abs": "https://arxiv.org/abs/2512.15740", "authors": ["Timothy Prescher"], "title": "The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems", "comment": "46 pages, 2 figures. Preregistered at OSF on Nov 14, 2025 (https://doi.org/10.17605/OSF.IO/BMVP3). Includes comparative analysis with OpenAI's 'Confessions' paper (Dec 3, 2025)", "summary": "Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).\n  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.\n  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.", "AI": {"tldr": "The paper proposes the Principle of Proportional Duty (PPD), a formal framework where moral responsibility shifts between acting and resolving uncertainty based on an agent\u2019s knowledge and humility, with applications to AI and several governance domains.", "motivation": "Existing ethical theories treat uncertainty mainly as a limitation, lacking precise tools to represent how moral responsibility should change when agents are unsure. This is especially problematic for complex, high\u2011stakes systems like AI, medicine, and economic governance, where overconfidence or paralysis can both be harmful. The authors want a mathematically explicit way to encode how duty scales with knowledge and uncertainty, so that it can be audited and simulated in real systems.", "method": "The paper defines a quantitative model of moral duty: D_total = K[(1-HI) + HI * g(C_signal)], where K represents knowledge, HI represents epistemic humility/uncertainty, and g(C_signal) is a function of contextual signal strength. They decompose total duty into Action Duty and Repair Duty and use Monte Carlo simulations to study how different humility parameters (lambda) affect the stability of duty allocation and the risk of overconfident decisions. They then apply the framework conceptually to four domains\u2014clinical ethics, recipient\u2011rights law, economic governance, and AI\u2014to show how proportional duty would guide behavior under uncertainty in each case.", "result": "Simulations show that when systems maintain a non\u2011zero baseline humility coefficient (lambda > 0), the allocation between Action Duty and Repair Duty is more stable over time, and the system is less prone to overconfident decisions based on weak or noisy signals. The model illustrates how increasing uncertainty does not erase moral obligation but converts some of the obligation to act into obligation to reduce uncertainty (e.g., investigate, verify, consult). Conceptual case analyses in four domains suggest the framework generalizes well and supports more balanced, context\u2011sensitive ethical decision\u2011making.", "conclusion": "The Principle of Proportional Duty offers a mathematically explicit, tractable way to represent how moral responsibility should adjust in response to uncertainty and context. By formalizing humility as a tunable system parameter and distinguishing Action Duty from Repair Duty, the framework can help design auditable, safer decision systems\u2014especially AI\u2014that avoid both reckless overreach and harmful inaction. The authors argue that proportional duty functions as a stabilizing principle in complex ethical environments and merits adoption across multiple governance and decision\u2011making domains."}}
{"id": "2512.16125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16125", "abs": "https://arxiv.org/abs/2512.16125", "authors": ["Daniela N. Rim", "Heeyoul Choi"], "title": "Convolutional Lie Operator for Sentence Classification", "comment": "Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval", "summary": "Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.", "AI": {"tldr": "They enhance CNN-based sentence classifiers with Lie group convolutions to better capture complex transformations in language, achieving higher accuracy than traditional CNN models.", "motivation": "Standard CNN text models excel at capturing local, position-invariant features but struggle to model more complex, structured transformations present in natural language; the authors want to explore whether Lie group operations, which are good at modeling non-Euclidean symmetries, can help.", "method": "Introduce Lie Convolutions into CNN-based sentence classification architectures, creating two models called SCLie and DPCLie, which modify the convolutional layers to operate over Lie group structures instead of only Euclidean grids; then empirically evaluate these models against traditional CNN sentence classifiers.", "result": "The proposed SCLie and DPCLie models empirically outperform standard convolution-based sentence classifiers in terms of classification accuracy.", "conclusion": "Incorporating Lie group-based convolutions helps sentence classifiers capture transformations that standard CNNs miss, leading to better performance and suggesting that Lie-based and other non-standard paradigms deserve further exploration in language modeling."}}
{"id": "2512.15743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15743", "abs": "https://arxiv.org/abs/2512.15743", "authors": ["David Noever"], "title": "Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions", "comment": null, "summary": "We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel \"bag of bricks\" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a \"bag of words\" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.", "AI": {"tldr": "A framework that converts natural language design descriptions into valid, step-by-step, brick-based assembly instructions using a discrete parts vocabulary and an LDraw-based intermediate representation.", "motivation": "Existing text-to-3D and CAD-based generative methods often produce unconstrained or purely visual models that lack strict geometric validity, buildability, and detailed assembly instructions. They also struggle with modularity and component exchange, making them unsuitable for directly driving physical prototyping and manufacturing workflows from natural language. There is a need for a representation and method that can bridge semantic design intent and physically manufacturable assemblies in a scalable and repeatable way.", "method": "The authors constrain generation to a discrete vocabulary of brick-like parts and use LDraw as a text-rich intermediate representation. Large language models are coupled with tool usage to generate valid, ordered construction sequences that respect geometric constraints and connection rules. A Python library is introduced to programmatically generate models and orchestrate the LLM plus tools pipeline. The system is tested on complex domains such as satellites, aircraft, and architecture, producing multi-thousand-part assemblies.", "result": "The framework successfully produces physically buildable, step-by-step assembly instructions for brick-based prototypes comprising over 3000 parts, across several complex design domains. It yields scalable and modular designs that maintain high fidelity to functional specifications and can be physically prototyped from natural language inputs. The LDraw-based elemental representation enables reliable geometry, connections, and ordering while remaining expressive enough for diverse applications.", "conclusion": "By defining a constrained, discrete \u201cbag of bricks\u201d vocabulary and using LDraw as an elemental lingua franca, the work establishes a practical physical API from natural language to manufacturable assemblies. This closes an important gap left by pixel-based diffusion and traditional CAD-focused approaches, enabling consistent, repeatable AI-driven design pipelines that tie semantic intent to real-world prototypes. The proposed framework opens up new options for AI-assisted manufacturing, engineering prototyping, and design automation based on natural language specifications."}}
{"id": "2512.16145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16145", "abs": "https://arxiv.org/abs/2512.16145", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation", "comment": "12 pages", "summary": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.", "AI": {"tldr": "The paper proposes a semantic-driven reinforcement learning method (SRL) for medical report generation that directly optimizes clinical correctness via report-level rewards instead of token-level supervision, achieving state-of-the-art clinical efficacy on IU X-Ray and MIMIC-CXR.", "motivation": "Existing medical report generation systems, often based on large vision-language models, tend to imitate radiologists' writing style but may produce clinically incorrect or unreliable content because they are trained with token-level objectives (e.g., next-token prediction, cross-entropy). These objectives focus on surface-level language features rather than aligning generated reports with actual clinical findings. There is a need for training objectives that prioritize semantic and clinical correctness over mere linguistic similarity.", "method": "The authors introduce a Semantic-driven Reinforcement Learning (SRL) framework built on top of a large vision-language model (LVLM) for medical report generation. They use Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm, to optimize a report-level reward that measures clinical correctness. This reward is computed as a margin-based cosine similarity (MCCS) between key radiological findings extracted from the generated and reference reports. By aligning these semantic labels, the method promotes clinically accurate outputs. Additionally, they impose a lightweight reasoning-format constraint to make the model produce structured \"thinking reports,\" which encourage more explicit and organized reasoning in the generated text.", "result": "On the IU X-Ray and MIMIC-CXR datasets, the proposed MRG-R1 model trained with semantic-driven reinforcement learning achieves state-of-the-art performance when evaluated with clinical efficacy (CE) metrics. Specifically, it obtains CE-F1 scores of 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. The experiments show that label-semantic reinforcement outperforms conventional token-level supervision in terms of clinical correctness of the generated reports.", "conclusion": "Optimizing a clinically grounded, report-level reward that measures semantic agreement between generated and reference reports leads to more clinically correct medical report generation compared to traditional token-level training. The study demonstrates that semantic-driven reinforcement learning, using GRPO and MCCS-based rewards, can significantly improve clinical efficacy metrics for LVLM-based medical report generation. The work is positioned as an initial exploration of semantic reinforcement for supervising medical correctness in medical large vision-language model training, suggesting a promising direction for future research on safe and reliable clinical AI systems."}}
{"id": "2512.15776", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15776", "abs": "https://arxiv.org/abs/2512.15776", "authors": ["Shaun Baek", "Sam Liu", "Joseph Ukpong"], "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying", "comment": "12 pages, 9 pages of content, 6 tables, 5 figures", "summary": "Large Language Models (LLMs) act as powerful reasoning engines but struggle with \"symbol grounding\" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or \"Curse of Knowledge\"), where a knowledgeable \"Leader\" agent fails to guide a sensor-limited \"Follower\" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant \"Success Gap\": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a \"Pull-based\" protocol (active querying) is significantly more robust than standard \"Push-based\" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.", "AI": {"tldr": "The paper studies how large language model agents fail to communicate effectively in asymmetric, embodied settings and shows that active querying (pull-based communication) significantly improves collaborative success.", "motivation": "While LLMs are strong reasoners, they lack robust symbol grounding and Theory of Mind in embodied, multi-agent tasks where agents have different perceptions. This leads to a Privileged Information Bias: a more informed leader struggles to guide a limited follower, threatening the safety and reliability of human-AI and robot-robot collaboration. The authors aim to measure and characterize this bias in a controlled environment.", "method": "They propose an Asymmetric Assistive Reasoning framework built in the AI2-THOR simulator. In this setup, a knowledgeable Leader agent has full or better sensory access and must guide a sensor-limited Follower to a target. They compare communication protocols: a standard push-based protocol where the Leader proactively sends instructions, versus a pull-based protocol where the Follower actively queries and requests clarifications. They then empirically measure success rates, especially the gap between the Leader\u2019s own task solvability and the team\u2019s collaborative success.", "result": "Experiments show a marked \u201cSuccess Gap\u201d: the Leader can directly perceive the target in 35% of episodes, but the joint Leader-Follower team succeeds in only 17% of episodes, meaning roughly half of otherwise solvable tasks fail purely due to communication and grounding issues. Moreover, setups using a pull-based protocol, where the Follower asks questions, are significantly more robust, and successful episodes exhibit about twice as many clarification requests compared to failed ones.", "conclusion": "The study isolates Privileged Information Bias as a key failure mode for LLM-based agents in asymmetric, embodied collaboration and identifies active uncertainty reduction via pull-based querying as a critical mechanism for effective and safe cooperation. Encouraging or designing agents to ask clarifying questions, rather than relying solely on one-way instruction, is essential for grounded, reliable human-AI and robot-robot interaction."}}
{"id": "2512.16147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16147", "abs": "https://arxiv.org/abs/2512.16147", "authors": ["Yash Bhaskar", "Sankalp Bahad", "Parameswari Krishnamurthy"], "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning", "comment": "Accepted Paper, Anthology ID: 2024.icon-fauxhate.3, 4 pages, 1 figure, 1 table", "summary": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.", "AI": {"tldr": "The paper presents a system for detecting Faux-Hate: hate speech generated from fake narratives in code-mixed Hindi-English social media data, using multi-task learning for both binary detection and target/severity prediction.", "motivation": "Social media spreads harmful content like hate speech and fake narratives rapidly. Existing work often treats hate speech and fake news separately and focuses on monolingual data. There is a need to detect a more nuanced phenomenon\u2014hate speech that is fueled by fake narratives\u2014especially in under-resourced, code-mixed languages like Hindi-English.", "method": "The authors build a system for the Faux-Hate shared task with two subtasks: (a) Binary Faux-Hate detection (fake vs. hate or related categories) and (b) Target and Severity prediction. They use advanced NLP methods, likely transformer-based models, enhanced with domain-specific pretraining on relevant social media text, and employ a multi-task learning setup so that the model jointly learns the different subtasks.", "result": "Their system obtained competitive performance on the shared task benchmarks across both subtasks, indicating that their approach is on par with or close to the top systems.", "conclusion": "Leveraging domain-specific pretraining and multi-task learning is effective for detecting Faux-Hate in code-mixed Hindi-English social media text, both for distinguishing Faux-Hate from other content and for predicting the target and severity of hate speech. This validates their method as a promising approach for complex, nuanced abusive content detection."}}
{"id": "2512.15783", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15783", "abs": "https://arxiv.org/abs/2512.15783", "authors": ["Kit Tempest-Walters"], "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns", "comment": "41 pages, 1 figure, 7 tables", "summary": "AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.\n  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.\n  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.", "AI": {"tldr": "Proposes 'AI Epidemiology', a framework that monitors AI outputs at a population level using structured expert feedback to predict and prevent harmful failures, without needing to understand model internals.", "motivation": "Current interpretability and governance methods struggle with the complexity and opacity of large, deployed AI models. Institutions need a scalable, model-agnostic way to detect unreliable or harmful AI outputs early, and to create audit trails and governance mechanisms that do not depend on detailed knowledge of model internals or vendor-specific tools.", "method": "Standardize AI\u2013expert interactions into structured assessment fields\u2014risk level, alignment score, and accuracy score\u2014which serve as exposure variables analogous to epidemiological risk factors. Use statistical associations between these exposure variables and observed output failures, validated against expert overrides and real-world outcomes, to conduct population-level surveillance of AI reliability and safety. Passively track patterns of expert convergence or divergence with AI recommendations over time.", "result": "The framework can automatically generate audit trails, track where experts systematically override AI recommendations, and derive reliability scores and semantic assessments such as: 'this recommendation resembles past cases that experts overrode for guideline violations.' It works across model versions and vendors because it focuses on outputs and expert responses rather than internal model structure.", "conclusion": "AI Epidemiology enables domain experts, without machine learning expertise, to participate directly in governing AI systems. By treating AI outputs like epidemiological data and using risk-factor-style variables, the framework offers scalable, model-agnostic oversight, continuity across model updates, and earlier detection of unreliable or harmful AI outputs before they impact real-world outcomes."}}
{"id": "2512.16183", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16183", "abs": "https://arxiv.org/abs/2512.16183", "authors": ["Mengfan Shen", "Kangqi Song", "Xindi Wang", "Wei Jia", "Tao Wang", "Ziqiang Han"], "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media", "comment": "41 pages,3figures and 9 tables", "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.", "AI": {"tldr": "The paper builds a LoRA-tuned Qwen2.5-7B pipeline to extract structured incident information (15 fields) from noisy Chinese police posts on Weibo, reaching very high accuracy on mortality and location-related fields.", "motivation": "Police incident announcements on social media are noisy, informal, and heterogeneous, making it hard to automatically convert them into structured data needed for timely analysis and social science research. Existing general-purpose IE models struggle with domain-specific terminology and irregular text, so a domain-adapted, efficient approach is needed to reliably extract key incident attributes (location, event type, impact) at scale.", "method": "The authors construct a high-quality, manually annotated dataset with 4,933 labeled incidents from 27,822 Chinese Weibo police briefings (2019\u20132020), defining 15 target fields (e.g., location, event characteristics, impact). They design a domain-adapted extraction pipeline centered on targeted prompt engineering and parameter-efficient fine-tuning of Qwen2.5-7B via Low-Rank Adaptation (LoRA). This enables specialization to the police-incident domain without full model retraining. They then evaluate performance on multiple extraction tasks, including mortality detection, fatality counts, and province-level location extraction, comparing LoRA-tuned models to both the base and instruction-tuned Qwen2.5-7B baselines.", "result": "LoRA-based fine-tuning yields substantial performance gains over both the base and instruction-tuned Qwen2.5-7B models. The tuned model achieves over 98.36% accuracy in mortality detection and Exact Match Rates of 95.31% for extracting fatality counts and 95.54% for province-level location extraction, demonstrating robust handling of noisy and heterogeneous social media text for these key fields.", "conclusion": "A domain-adapted, LoRA-fine-tuned Qwen2.5-7B model combined with targeted prompts can efficiently and accurately perform multi-task structured information extraction from noisy police incident posts on social media. The validated pipeline offers a practical framework for transforming unstructured, domain-specific text into reliable structured data, supporting scalable empirical research in social sciences and related fields."}}
{"id": "2512.15784", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15784", "abs": "https://arxiv.org/abs/2512.15784", "authors": ["Zibin Liu", "Cheng Zhang", "Xi Zhao", "Yunfei Feng", "Bingyu Bai", "Dahu Feng", "Erhu Feng", "Yubin Xia", "Haibo Chen"], "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM", "comment": null, "summary": "Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.\n  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.\n  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.", "AI": {"tldr": "MOBIMEM is a memory-centric LLM agent architecture that lets mobile/desktop agents self-improve after deployment without retraining by using specialized memories and OS-like orchestration services.", "motivation": "Existing LLM agents for mobile/desktop automation need retraining or fine-tuning to improve personalization, capabilities, and efficiency. This retraining is computationally expensive and faces an accuracy\u2013latency trade-off, especially for user-specific behavior. The paper is motivated by the need for an agent that can iteratively self-evolve in the wild\u2014becoming more personalized, capable, and efficient\u2014without modifying model weights.", "method": "The authors design MOBIMEM, a memory-centric agent system that offloads evolution from model weights to structured memories. They introduce three key memory primitives: (1) Profile Memory, built on a lightweight distance-graph (DisGraph) for fast, accurate retrieval of user preferences; (2) Experience Memory, using multi-level templates to encode and instantiate task execution logic for new tasks; and (3) Action Memory, which logs fine-grained interaction sequences to reduce future inference calls. On top of this, they add OS-inspired services: a scheduler handling parallel sub-task execution and memory operations, an Agent Record-and-Replay (AgentRR) mechanism for safe reuse of past actions, and context-aware exception handling for recovery from interruptions and runtime errors. They then evaluate MOBIMEM on AndroidWorld and popular real-world apps.", "result": "On AndroidWorld and the top-50 mobile apps, MOBIMEM attains 83.1% alignment with user profiles with a low 23.83 ms retrieval time, which is 280x faster than GraphRAG baselines. It improves task success rates by up to 50.3% and cuts end-to-end latency by up to 9x on mobile devices, demonstrating both better effectiveness and efficiency compared to existing approaches.", "conclusion": "The paper concludes that memory-centric design\u2014via Profile, Experience, and Action memories combined with OS-like orchestration\u2014enables LLM agents on mobile/desktop platforms to iteratively self-evolve without model retraining. This architecture simultaneously improves personalization, generalization to new tasks, and runtime efficiency, making it more practical to deploy and continually improve LLM agents in real-world device environments."}}
{"id": "2512.16189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16189", "abs": "https://arxiv.org/abs/2512.16189", "authors": ["Musarrat Zeba", "Abdullah Al Mamun", "Kishoar Jahan Tithee", "Debopom Sutradhar", "Mohaimenul Azam Khan Raiaan", "Saddam Mukta", "Reem E. Mohamed", "Md Rafiqul Islam", "Yakub Sebastian", "Mukhtar Hussain", "Sami Azam"], "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation", "comment": null, "summary": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.", "AI": {"tldr": "The paper proposes a healthcare summarization system that reduces hallucinations by pairing a LoRA\u2011fine\u2011tuned clinical LLM with an independent fact\u2011checking module that validates summaries against EHR data.", "motivation": "LLM outputs in healthcare can be dangerously unreliable due to hallucinations, especially when used for decision-making and patient safety. There is a need for mechanisms that ensure factual correctness of generated clinical summaries against structured and unstructured EHR data, beyond generic hallucination mitigation techniques.", "method": "The authors fine\u2011tune a domain\u2011specific summarization LLM on the full MIMIC\u2011III dataset using Low\u2011Rank Adaptation (LoRa). They then design an independent fact\u2011checking module that does not rely on the LLM itself. This module parses generated summaries into fine\u2011grained propositions and verifies them using two strategies: (1) numerical consistency checks against EHR values, and (2) discrete logical checks in natural language processing to determine whether each proposition is supported by the patient\u2019s EHR. The combined system first generates a summary and then passes it through the fact\u2011checker to flag or filter hallucinated content.", "result": "On a sample of 104 summaries decomposed into 3,786 propositions, the fact\u2011checking module attains a precision of 0.8904, recall of 0.8234, and F1-score of 0.8556 in identifying factual correctness. The LoRA\u2011tuned summarization model achieves a ROUGE\u20111 score of 0.5797 and a BERTScore of 0.9120, indicating high similarity and semantic alignment with reference summaries.", "conclusion": "Coupling a domain\u2011tuned clinical summarization LLM with an independent, EHR\u2011aware fact\u2011checking module can substantially reduce hallucinations while preserving high summary quality. The method provides a practical pathway toward safer, more trustworthy LLM deployment in healthcare decision-support settings, where factual reliability is critical."}}
{"id": "2512.15824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15824", "abs": "https://arxiv.org/abs/2512.15824", "authors": ["Richard Fox", "Rui Li", "Gustav Jonsson", "Farzaneh Goli", "Miying Yang", "Emel Aktas", "Yongjing Wang"], "title": "State-Augmented Graphs for Circular Economy Triage", "comment": null, "summary": "Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.", "AI": {"tldr": "The paper proposes a unified, deterministic decision-making framework for circular economy triage based on a state-augmented disassembly sequencing planning graph, enabling optimal, condition-aware choices between further disassembly and various circular economy options, demonstrated on electric vehicle batteries.", "motivation": "Circular economy triage for end-of-use products is complex because it must balance retained value, processing costs, labour, safety, and mechanical constraints, and existing approaches lack a general, tractable formalism that can optimally handle these trade-offs across diverse products. The authors aim to create a rigorous decision framework that can support adaptive, optimal CE triage decisions in a systematic way.", "method": "The authors construct a simple deterministic solver operating over a state-augmented Disassembly Sequencing Planning (DSP) graph. They encode the disassembly history into the state to enforce the Markov property, allowing recursive, optimal evaluation where each decision depends only on the previous state. At each node, the framework models triage decisions as a choice between continuing disassembly or committing to a particular circular economy pathway, with utilities that incorporate diagnostic health scores (condition awareness) and operational constraints (mechanical complexity, safety, costs). They illustrate the method with a hierarchical EV battery triage example.", "result": "The framework successfully represents CE triage as a recursive valuation problem over a DSP graph and can integrate condition-aware utility and complex operational constraints. The worked EV battery example shows that the model can capture hierarchical decisions, varying mechanical and safety requirements, and economic trade-offs, all within a single formalism.", "conclusion": "The proposed state-augmented DSP-based decision framework offers a tractable and generalisable foundation for optimising circular economy triage decisions. By enforcing the Markov property and integrating condition-aware utilities and constraints, it can systematically support optimal choices between continued disassembly and multiple CE options across different products and operational contexts, as demonstrated on EV batteries."}}
{"id": "2512.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16227", "abs": "https://arxiv.org/abs/2512.16227", "authors": ["Qizhou Chen", "Chengyu Wang", "Taolin Zhang", "Xiaofeng He"], "title": "An Information-Theoretic Framework for Robust Large Language Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.", "AI": {"tldr": "The paper proposes Information Bottleneck Knowledge Editor (IBKE), a new, theoretically grounded method to edit and update large language models\u2019 knowledge efficiently and reliably without full retraining.", "motivation": "LLMs are widely used but can contain errors or outdated knowledge. Fully retraining them to fix such issues is expensive and disruptive. Existing model editing techniques often fail to generalize beyond narrow cases and can unintentionally affect unrelated behaviors. The motivation is to create a more efficient, generalizable, and safe way to correct and update LLM knowledge.", "method": "The authors develop a model editing framework based on information bottleneck theory. They design a mechanism to compress and isolate the minimal information needed for a given knowledge correction into compact latent representations. Using these bottlenecked representations, the proposed IBKE method performs targeted gradient-based updates that focus on the relevant knowledge while minimizing changes to unrelated behaviors. The method is tested across different LLM architectures and benchmark editing tasks.", "result": "Experiments across multiple LLMs and standard knowledge-editing benchmarks show that IBKE achieves state-of-the-art performance. It yields higher accuracy on edits, better generalization of the corrections beyond the exact training cases, and improved specificity\u2014meaning fewer unwanted side effects on unrelated outputs\u2014compared with existing editing methods.", "conclusion": "The paper concludes that IBKE offers a theoretically principled and practically effective paradigm for open-domain knowledge editing in LLMs. By using an information-bottleneck-based approach, it enables robust, generalizable, and minimally disruptive model updates, thereby enhancing the reliability and usefulness of LLMs in real-world applications."}}
{"id": "2512.15894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15894", "abs": "https://arxiv.org/abs/2512.15894", "authors": ["Vahideh Zolfaghari"], "title": "PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations", "comment": null, "summary": "Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.", "AI": {"tldr": "The paper introduces PediatricAnxietyBench, a benchmark of 300 realistic and adversarial pediatric queries used to evaluate the safety of large language models (Llama 70B and 8B) under anxious parental pressure, finding substantial safety vulnerabilities despite larger models performing better.", "motivation": "Parents increasingly ask LLMs for pediatric advice, but stressed or anxious language may bypass safety mechanisms and elicit harmful recommendations. Existing benchmarks often ignore such real-world adversarial pressures and nuanced safety dimensions like diagnostic restraint and referral behavior. The authors aim to systematically and reproducibly test LLM safety in pediatric contexts under realistic parental anxiety.", "method": "The authors built PediatricAnxietyBench, a curated dataset of 300 high-quality pediatric queries across 10 topics, split between patient-derived and intentionally adversarial prompts that simulate anxious parents (e.g., urgency, cost barriers, challenging disclaimers). They evaluated two Llama models (70B and 8B) using a multi-dimensional safety framework measuring diagnostic restraint, adherence to referral guidelines, degree of hedging, and recognition of emergencies. They computed safety scores, critical failure rates, and correlations between behaviors like hedging and overall safety.", "result": "Average safety across models was low, with a mean score of 5.50 out of 15. The 70B model was safer than the 8B model (6.26 vs 4.95) and had fewer critical failures (4.8% vs 12.0%). Adversarial prompts reduced safety scores by about 8%, with urgent language producing the largest drop. Particular weaknesses emerged in seizure scenarios, where there was a 33.3% rate of inappropriate diagnosis, and in post-vaccination questions. Hedging behavior was strongly positively correlated with safety, while models essentially failed at recognizing emergencies.", "conclusion": "Model size improves pediatric safety but does not eliminate significant vulnerabilities, especially when facing realistic anxious-parent pressure. PediatricAnxietyBench offers a reusable, adversarially oriented benchmark that can expose clinically meaningful failure modes that conventional benchmarks miss, highlighting the need for better safety training and evaluation for LLMs used in pediatric guidance contexts."}}
{"id": "2512.15906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15906", "abs": "https://arxiv.org/abs/2512.15906", "authors": ["Jonathan A. Handler"], "title": "Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries", "comment": "17 pages, 3 figures", "summary": "Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database (\"knowledge base\" or \"knowledge graph\"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.", "AI": {"tldr": "Darth Vecdor (DV) is an open\u2011source system that uses LLMs to extract internet knowledge into a structured, terminology\u2011mapped SQL knowledge graph, with safeguards and a GUI to make it usable\u2014especially for healthcare\u2014while warning that it may contain serious bugs and must be used cautiously.", "motivation": "LLMs contain massive amounts of useful internet knowledge, but directly querying them can be costly, slow, unsafe, and hard to control\u2014problems that are especially critical in high\u2011volume, safety\u2011sensitive domains like healthcare. The author aims to pre\u2011extract that knowledge into a structured, queryable database that can be used more reliably and efficiently than raw LLM interactions.", "method": "Design and implementation of Darth Vecdor (DV), a system that prompts an LLM to extract information into a terminology\u2011mapped SQL knowledge base/graph. DV incorporates features to reduce typical LLM issues\u2014such as erroneous, off\u2011topic, free\u2011text, overly general, inconsistent answers\u2014and to support multi\u2011element responses. It wraps this process in a simple browser\u2011based GUI to enable domain experts without deep technical skills to do prompt engineering and manage the extraction process.", "result": "DV yields a structured SQL\u2011based knowledge graph derived from LLM outputs, with mechanisms that partially mitigate common LLM response problems and that support multi\u2011element structured entries. The system is released as free, open\u2011source, and extensible software with a web interface, but without any warranty and with explicit acknowledgment of potential serious bugs and risks.", "conclusion": "Pre\u2011extracting LLM knowledge into a structured, terminology\u2011mapped SQL knowledge graph via DV can potentially make LLM\u2011derived information cheaper, faster, and safer to query, particularly in healthcare. However, DV and its outputs may contain serious bugs and must be used with caution; users bear responsibility for ensuring safe and effective use. With careful, appropriate application and continued development, DV could contribute to improving healthcare outcomes."}}
{"id": "2512.16248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16248", "abs": "https://arxiv.org/abs/2512.16248", "authors": ["Qingguo Hu", "Zhenghao Lin", "Ziyue Yang", "Yucheng Ding", "Xiao Liu", "Yuting Jiang", "Ruizhe Wang", "Tianyu Chen", "Zhongxin Guo", "Yifan Xiong", "Rui Gao", "Lei Qu", "Jinsong Su", "Peng Cheng", "Yeyun Gong"], "title": "Sigma-Moe-Tiny Technical Report", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm", "AI": {"tldr": "Sigma-MoE-Tiny is an extremely sparse Mixture-of-Experts language model (20B total / 0.5B active params) that uses many small experts per layer and a new progressive sparsification schedule to keep training stable and experts well balanced, achieving strong performance at very low active parameter count.", "motivation": "Scaling dense language models is increasingly expensive, so Mixture-of-Experts architectures are used to increase total capacity without proportionally increasing compute. However, pushing sparsity to extremes (many experts, very few active per token) causes severe expert load-imbalance and instability, especially in lower layers. There is limited understanding and tooling for making such highly sparse MoE models both trainable and performant. This paper aims to explore that extreme sparsity regime, resolve its training issues, and show it can be practically useful.", "method": "They design Sigma-MoE-Tiny, an MoE language model with fine-grained expert segmentation: up to 96 experts per layer, with only one expert activated per token. This yields about 20B total parameters while only 0.5B are used per token. The core technical contribution is a progressive sparsification schedule that gradually increases sparsity to maintain expert load balancing and training stability, especially in lower layers where standard load-balancing loss becomes ineffective. The model is pre-trained on a diverse, high-quality corpus and then post-trained (likely instruction tuning / alignment) to enhance downstream capabilities. They also systematically analyze expert load balancing behavior in this high-sparsity regime.", "result": "Sigma-MoE-Tiny reaches 20B total parameters with just 0.5B active per token, achieving the highest sparsity reported among open-source MoE language models. Training remains stable\u2014no irrecoverable loss spikes\u2014despite the extreme sparsity. On comprehensive benchmarks, the model attains top-tier performance relative to other models with similar or even much larger active parameter counts. Their analyses clarify when and why standard load-balancing losses fail in very sparse settings and how their progressive sparsification helps maintain balanced expert utilization.", "conclusion": "Extreme-sparsity MoE (many experts, one-active-expert routing) is practically viable: with appropriate training strategies, particularly progressive sparsification for load balancing, such models can be stable and highly performant while using very few active parameters. Sigma-MoE-Tiny demonstrates this by matching or surpassing comparable and larger models, and the load-balancing analysis provides guidance for designing future, even sparser MoE architectures."}}
{"id": "2512.15922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15922", "abs": "https://arxiv.org/abs/2512.15922", "authors": ["Jovan Pavlovi\u0107", "Mikl\u00f3s Kr\u00e9sz", "L\u00e1szl\u00f3 Hajdu"], "title": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems", "comment": "20 pages, 5 figures", "summary": "Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.", "AI": {"tldr": "They propose a new retrieval-augmented generation (RAG) framework that uses spreading activation over automatically-built knowledge graphs to better retrieve multi-step evidence and improve multi-hop reasoning, especially with small open-weight LLMs.", "motivation": "Standard RAG treats all retrieved passages as equally trustworthy and struggles with complex, multi-hop reasoning because it does not model relationships or credibility within large corpora. GraphRAG helps by using knowledge graphs, but it relies on expensive, high-quality graphs or unreliable automatic graph construction, and still often uses LLMs for traversal, inheriting their weaknesses. There is a need for a RAG method that can leverage graph structure for multi-step evidence retrieval without requiring perfect graphs or heavy LLM-based control.", "method": "They automatically construct knowledge graphs from document corpora, connecting documents through entities and relations. Instead of using an LLM to control graph traversal, they apply a spreading activation algorithm starting from query-relevant nodes. Activation propagates through the graph to highlight nodes and documents most relevant via multi-step connections. The activated documents are then fed to a downstream large language model for generation or question answering. The framework is designed as a plug-and-play module that can be combined with existing RAG pipelines, including chain-of-thought iterative retrieval.", "result": "Across experiments on complex tasks like multi-hop question answering, the spreading-activation-based RAG framework performs better or on par with iterative RAG baselines. When combined with chain-of-thought iterative retrieval, it yields up to a 39% absolute improvement in answer accuracy compared with naive RAG, and these gains are demonstrated even when using smaller, open-weight language models.", "conclusion": "Spreading activation over automatically generated knowledge graphs is an effective and practical way to enhance RAG for complex reasoning tasks. It avoids reliance on manually built, high-quality graphs and reduces dependence on LLM-driven graph traversal, while remaining easy to integrate into existing systems. The method significantly boosts answer correctness, especially in resource-constrained settings using smaller models, showing that better retrieval and evidence aggregation can compensate for limited model capacity."}}
{"id": "2512.16287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16287", "abs": "https://arxiv.org/abs/2512.16287", "authors": ["Yehor Tereshchenko", "Mika H\u00e4m\u00e4l\u00e4inen", "Svitlana Myroniuk"], "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures", "comment": "IWCLUL 2025", "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.", "AI": {"tldr": "The paper compares reasoning vs non-reasoning GPT models on translation between Finnish and four low-resource Uralic languages, focusing on refusal rates and showing reasoning models are more willing to translate.", "motivation": "Most LLM translation evaluations target high-resource languages, so we know little about their behavior and reliability for low-resource and endangered languages. For such languages, even getting the model to attempt a translation (rather than refuse) is a key issue, especially for language preservation and practical use in research and communities.", "method": "The authors use a parallel corpus of literary texts involving Finnish and four low-resource Uralic languages (Komi-Zyrian, Moksha, Erzya, Udmurt). They compare different OpenAI GPT models, grouped as reasoning vs non-reasoning architectures, and measure how often each model refuses to translate (refusal rate) as a proxy for willingness to attempt the task, alongside overall performance differences.", "result": "They find substantial differences between reasoning and non-reasoning GPT models. Reasoning models show a 16 percentage point lower refusal rate, indicating they are notably more willing to attempt translations for these low-resource Uralic languages. Overall performance also varies significantly by architecture type.", "conclusion": "Reasoning-oriented GPT architectures are more inclined to engage with translation tasks involving low-resource and endangered Uralic languages, as evidenced by much lower refusal rates. This suggests that reasoning models are especially promising tools for work on endangered language documentation and preservation, and that evaluation of LLMs for such languages must consider both willingness to respond and translation quality."}}
{"id": "2512.15943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15943", "abs": "https://arxiv.org/abs/2512.15943", "authors": ["Polaris Jhandi", "Owais Kazi", "Shreyas Subramanian", "Neel Sendas"], "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning", "comment": null, "summary": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.", "AI": {"tldr": "The paper explores whether a small language model (OPT-350M), fine-tuned for domain tasks, can replace large language models in specific enterprise workflows, achieving high performance at much lower cost.", "motivation": "Large Language Models are powerful but computationally expensive and often too costly for routine enterprise use. Organizations need more sustainable, accessible, and cost-effective generative AI solutions. This motivates studying whether Small Language Models, carefully trained for narrow domains and tasks, can match or approach LLM performance while reducing infrastructure overhead.", "method": "The authors fine-tune the facebook/opt-350m Small Language Model using Hugging Face TRL\u2019s Supervised Fine-Tuning (SFT) trainer for one epoch. The model is domain-adapted to handle tasks typically assigned to LLMs, including document summarization, question answering, and structured data interpretation. They then evaluate the fine-tuned model against several baselines on the ToolBench benchmark, comparing pass rates to LLM-based methods like ChatGPT-CoT and ToolLLaMA variants.", "result": "The fine-tuned OPT-350M SLM achieves a 77.55% pass rate on ToolBench, substantially outperforming baseline models such as ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%).", "conclusion": "With thoughtful task and domain design plus targeted fine-tuning, Small Language Models can deliver strong performance on specific enterprise workflows while being far more cost-efficient than LLMs. This supports the feasibility of replacing many LLM-driven workflows with optimized SLMs, lowering barriers to large-scale, production-grade generative AI adoption."}}
{"id": "2512.16323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16323", "abs": "https://arxiv.org/abs/2512.16323", "authors": ["Hiroyuki Deguchi", "Katsuki Chousa", "Yusuke Sakai"], "title": "Hacking Neural Evaluation Metrics with Single Hub Text", "comment": null, "summary": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.", "AI": {"tldr": "They show that a single adversarial \u201chub\u201d text can systematically fool popular neural text evaluation metrics like COMET into assigning very high scores across many different inputs and language pairs.", "motivation": "Neural, embedding-based evaluation metrics are heavily relied on for assessing generation models, yet their internal workings are opaque and there is no strong guarantee of reliability or robustness. The authors want to test and expose potential vulnerabilities, particularly whether these metrics can be systematically fooled in ways that undermine their trustworthiness.", "method": "They propose a method to search in discrete text space for a single adversarial \u201chub\u201d text that is consistently rated as high-quality by a given neural evaluation metric. Instead of tailoring an adversarial output per input sentence, they optimize for one universal output that scores highly under COMET regardless of the source sentence. They then apply this method to WMT\u201924 translation tasks and examine cross-language-pair generalization of the resulting hub text.", "result": "They find a single hub text that achieves very high COMET scores (79.1 COMET% on WMT\u201924 En\u2013Ja and 67.8 COMET% on En\u2013De), even surpassing the scores of proper sentence-wise translations generated by a strong general-purpose MT model (M2M100). The hub text\u2019s adversarial effect further generalizes to other language pairs such as Ja\u2013En and De\u2013En, indicating a systemic vulnerability in the metric.", "conclusion": "Embedding-based neural evaluation metrics like COMET can be systematically exploited via a universal adversarial \u201chub\u201d text, which is evaluated as high-quality across many inputs and language pairs. This raises serious concerns about the robustness, reliability, and safety of relying on such metrics as the primary compass for developing generation models, and suggests a need for more robust, interpretable, and attack-resistant evaluation methods."}}
{"id": "2512.15948", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.15948", "abs": "https://arxiv.org/abs/2512.15948", "authors": ["Samuel J. Gershman"], "title": "Subjective functions", "comment": null, "summary": "Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.", "AI": {"tldr": "The paper suggests that instead of being given fixed, external objective functions, agents can internally generate their own \u2018subjective functions\u2019, with expected prediction error as a concrete example, linking this idea to psychology, neuroscience, and machine learning.", "motivation": "In most machine learning and control frameworks, objective functions (losses, rewards) are designed externally and fixed, whereas humans routinely invent and revise their own goals in flexible, context-sensitive ways. The authors want to understand where such objectives come from, how agents could autonomously generate them, and how to transfer this capacity to artificial systems.", "method": "Conceptually introduce the notion of a \u201csubjective function,\u201d defined over an agent\u2019s own internal features rather than externally imposed tasks. Analyze expected prediction error as a specific instantiation of such a subjective function and explore its implications. Draw theoretical connections and analogies to established concepts in psychology (e.g., intrinsic motivation), neuroscience (e.g., predictive coding), and machine learning (e.g., self-supervised learning, curiosity-driven exploration).", "result": "The paper outlines a formal framing where an agent\u2019s goals arise from internal, higher-order objective functions rather than from hand-designed task losses. Expected prediction error is shown to fit naturally into this framing, serving as a principled subjective function that can drive behavior and learning. This yields a unifying view that ties together multiple strands of work across disciplines that implicitly rely on similar ideas.", "conclusion": "Objective functions in intelligent systems need not be purely externally specified; they can be generated endogenously via subjective functions grounded in an agent\u2019s own internal processes. Expected prediction error is a promising example of such a function, providing a bridge between human-like goal formation and artificial learning mechanisms, and suggesting a path toward more autonomous and flexible AI systems."}}
{"id": "2512.16378", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.16378", "abs": "https://arxiv.org/abs/2512.16378", "authors": ["Sara Papi", "Javier Garcia Gilabert", "Zachary Hopton", "Vil\u00e9m Zouhar", "Carlos Escolano", "Gerard I. G\u00e1llego", "Jorge Iranzo-S\u00e1nchez", "Ahrii Kim", "Dominik Mach\u00e1\u010dek", "Patricia Schmidtova", "Maike Z\u00fcfle"], "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "comment": "Project available at https://github.com/sarapapi/hearing2translate", "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.", "AI": {"tldr": "The paper benchmarks SpeechLLMs versus cascaded speech translation pipelines and finds cascaded systems are still more reliable overall.", "motivation": "To determine whether end-to-end SpeechLLMs that integrate speech as a native modality actually outperform or meaningfully improve upon traditional cascaded speech translation architectures that use separate speech recognition and text translation components.", "method": "The authors build a comprehensive test suite called \"Hearing to Translate\" and rigorously benchmark 5 state-of-the-art SpeechLLMs against 16 strong direct and cascaded systems that pair leading speech foundation models with multilingual LLMs. They evaluate on 16 benchmarks, 13 language pairs, and 9 challenging conditions such as disfluencies, noise, and long-form speech.", "result": "Across a wide range of benchmarks, cascaded systems generally achieve better and more reliable speech-to-text translation performance than current SpeechLLMs. SpeechLLMs only reach parity with cascades in some specific conditions, and speech foundation models alone perform worse than both SpeechLLMs and cascaded systems.", "conclusion": "Current integration of LLMs directly with speech (SpeechLLMs) does not yet consistently surpass cascaded pipelines for speech translation. Cascaded systems that combine strong speech foundation models with multilingual LLMs remain the most reliable approach, and the presence of an LLM\u2014either inside an end-to-end model or as part of a pipeline\u2014is crucial for high-quality speech translation."}}
{"id": "2512.16022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16022", "abs": "https://arxiv.org/abs/2512.16022", "authors": ["Defu Cao", "Michael Gee", "Jinbo Liu", "Hengxuan Wang", "Wei Yang", "Rui Wang", "Yan Liu"], "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting", "comment": "31Pages", "summary": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.", "AI": {"tldr": "They use a large language model not to forecast directly, but as an intelligent judge that ensembles multiple time-series foundation models with interpretable weights, achieving state-of-the-art results.", "motivation": "No single time series foundation model consistently outperforms others, and naive use of LLMs for forecasting works poorly. There is a need for a method that can orchestrate multiple foundation models into an optimal, interpretable ensemble that leverages LLM reasoning while being faithful to time-series dynamics.", "method": "They treat the LLM as a meta-agent that judges and coordinates an ensemble of time-series foundation models instead of forecasting directly. The LLM is finetuned in an R1-style process, where SHAP-based faithfulness scores guide training so that the model learns to interpret and assign ensemble weights as causal statements about temporal behavior. At inference, the trained agent conducts iterative multi-turn reasoning: it evaluates candidate models, assigns and refines weights, explains its decisions causally, and adaptively optimizes the ensemble strategy over multiple conversational steps.", "result": "On the GIFT-Eval benchmark, covering 23 datasets and 97 experimental settings, their LLM-based ensemble agent outperforms state-of-the-art time series foundation models on two key metrics, CRPS and MASE, establishing new SOTA performance.", "conclusion": "Recasting the LLM as an intelligent, causally-aware ensemble judge for time-series foundation models\u2014trained with SHAP-guided R1-style finetuning and multi-turn reasoning\u2014yields more accurate and interpretable forecasts than individual foundation models, setting a new performance bar on a broad benchmark."}}
{"id": "2512.16401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16401", "abs": "https://arxiv.org/abs/2512.16401", "authors": ["Darshil Chauhan", "Adityasinh Solanki", "Vansh Patel", "Kanav Kapoor", "Ritvik Jain", "Aditya Bansal", "Dhruv Kumar", "Prateek Narang"], "title": "Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains", "comment": null, "summary": "Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.", "AI": {"tldr": "The paper proposes a lightweight, privacy-preserving continual adaptation framework (using LoRA and experience replay) to make multilingual ASR usable on low-resource, real-world clinical audio, significantly improving WER and reducing catastrophic forgetting on edge devices.", "motivation": "Clinical settings, especially in resource-constrained regions, need accurate ASR to automate documentation and improve efficiency. However, existing robust multilingual ASR models perform poorly on real clinical audio due to domain shift, and standard adaptation is hard because of strict privacy constraints, limited compute on edge devices, and the risk of catastrophic forgetting when adapting to new domains. The authors show a large performance gap (WER ~41%) for a strong base model on real clinical data, motivating a specialized adaptation framework.", "method": "Start from an existing robust multilingual ASR model (IndicWav2Vec). Use Low-Rank Adaptation (LoRA) modules to adapt the model on-device via continual learning from streaming clinical audio, so that only a small number of additional parameters are trained and stored, respecting compute and memory limits. To combat catastrophic forgetting and preserve performance on other domains, the method incorporates multi-domain experience replay, where representative examples from prior domains are interleaved during adaptation training.", "result": "On the Gram Vaani clinical audio domain, the proposed LoRA-based adaptation yields a 17.1% relative WER improvement over the unadapted IndicWav2Vec baseline, making performance significantly more viable for deployment. When combined with multi-domain experience replay, the approach reduces catastrophic forgetting by 47% compared to naive adaptation that does not use replay, demonstrating better retention of prior-domain performance while adapting to the clinical domain.", "conclusion": "Efficient, privacy-preserving continual adaptation using LoRA and multi-domain experience replay can substantially improve the practicality of multilingual ASR in challenging, real-world clinical environments on edge devices. The approach offers a concrete pathway to self-improving ASR systems that respect data privacy and resource constraints while mitigating catastrophic forgetting, thereby bringing ASR closer to deployment in high-impact sectors like rural healthcare."}}
{"id": "2512.16030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16030", "abs": "https://arxiv.org/abs/2512.16030", "authors": ["Lukas Nel"], "title": "Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets", "comment": null, "summary": "A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\\% confidence, it should be correct 80\\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \\textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \\textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \\emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.", "AI": {"tldr": "The paper introduces KalshiBench, a benchmark using real prediction market questions to evaluate how well large language models\u2019 stated probabilities match real-world outcomes, finding that current frontier models are systematically overconfident and often worse than simple base-rate prediction.", "motivation": "Although LLMs perform very well on many tasks, it is unclear whether their confidence scores are well-calibrated\u2014i.e., whether a stated 80% probability really corresponds to 80% correctness, especially for genuine future events beyond their training data. Existing benchmarks mostly assess accuracy on static, known facts, not uncertainty about unseen outcomes. The authors want to measure and understand the epistemic calibration of LLMs on real-world, out-of-distribution future events with objectively verifiable resolutions.", "method": "The authors construct KalshiBench, a dataset of 300 resolved questions from Kalshi, a regulated prediction market, ensuring all events resolved after the models\u2019 training cutoffs so they could not be memorized. They query five frontier LLMs\u2014Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2\u2014for probabilistic predictions on these binary events, and compute calibration and forecasting metrics including Expected Calibration Error (ECE) and Brier Skill Score. They compare raw model outputs and reasoning-enhanced variants (e.g., GPT-5.2-XHigh) to assess the impact of scale and reasoning on calibration, relative to simple baselines like base-rate prediction.", "result": "All evaluated models exhibit systematic overconfidence: their stated probabilities are more extreme than warranted by actual outcome frequencies. The best model, Claude Opus 4.5, still has a nontrivial calibration error (ECE=0.120). Reasoning-enhanced variants such as GPT-5.2-XHigh are even more miscalibrated (ECE=0.395) despite having similar accuracy. Only one model obtains a positive Brier Skill Score, showing that most models underperform a naive base-rate forecaster on probabilistic accuracy.", "conclusion": "Epistemic calibration is not automatically improved by scaling models or adding more elaborate reasoning capabilities. Current frontier LLMs remain overconfident and often worse calibrated than simple statistical baselines on real-world future events. The authors argue that calibration is a distinct capability that needs explicit attention\u2014through training objectives, evaluation benchmarks like KalshiBench, and targeted methods\u2014rather than being assumed to emerge as a byproduct of better accuracy or reasoning."}}
{"id": "2512.16036", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16036", "abs": "https://arxiv.org/abs/2512.16036", "authors": ["Diane Myung-kyung Woodbridge", "Allyson Seba", "Freddie Seba", "Aydin Schwartz"], "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education", "comment": null, "summary": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.", "AI": {"tldr": "The paper builds an automated system that scans course and institutional documents to detect, summarize, and classify GenAI-related policies, helping students and educators understand how GenAI is allowed or restricted in different educational contexts.", "motivation": "GenAI tools are widely used by students for learning and assignment completion, offering benefits but also posing risks like hallucinations, misinformation, and reduced critical thinking. Institutions are rapidly creating GenAI policies, but these vary greatly and are hard for students to find and interpret, leading to confusion about acceptable use. The authors are motivated to systematically discover, organize, and present these policies so that GenAI can be used responsibly and consistently in education.", "method": "The authors designed an automated pipeline that searches course syllabi and institutional policy websites for AI-related policy text. They apply unsupervised topic modeling to uncover main themes within these policies, then use large language models (specifically GPT-4.0) to classify each policy according to the level of GenAI allowance and other requirements. Performance is evaluated using topic coherence for the topic model and precision/recall metrics for the classification task across eight identified policy topics.", "result": "The topic modeling component achieved a coherence score of 0.73, indicating relatively coherent and meaningful discovered topics. The GPT-4.0-based classifier obtained high performance, with precision between 0.92 and 0.97 and recall between 0.85 and 0.97 across eight policy categories, demonstrating that the system can reliably categorize GenAI-related policies in educational documents.", "conclusion": "The system effectively discovers and organizes GenAI-related policies from educational documents, producing structured, interpretable information about how GenAI may be used in different courses and institutions. This structured policy view can support safer, more equitable, and pedagogically aligned GenAI integration by making expectations clearer to students and instructors, and it can be embedded into educational technology platforms to help users understand and follow relevant GenAI guidelines."}}
{"id": "2512.16541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16541", "abs": "https://arxiv.org/abs/2512.16541", "authors": ["Primoz Kocbek", "Gregor Stiglic"], "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification", "comment": "10 pages, 3 tables. CLEF 2025 Working Notes, 9 to 12 September 2025, Madrid, Spain", "summary": "This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.", "AI": {"tldr": "The paper reports on a system for simplifying scientific texts at sentence and document levels using GPT-4.1 model variants, comparing prompt-only vs fine-tuned setups.", "motivation": "Scientific texts are often difficult for many readers to understand, so there is a need for automatic systems that can simplify such texts at different granularities (sentence and full document). The authors participate in the CLEF 2025 SimpleText track Task 1 to explore and benchmark large language model approaches for this simplification problem.", "method": "They use three OpenAI models\u2014gpt-4.1, gpt-4.1-mini, and gpt-4.1-nano\u2014and implement two strategies: (1) a no-context, prompt-engineering-based method, and (2) fine-tuning these models (FT). They then compare performance across sentence-level and document-level simplification tasks within the CLEF 2025 SimpleText evaluation setting.", "result": "The gpt-4.1-mini model using the no-context, prompt-based method performs robustly on both sentence-level and document-level simplification. Fine-tuned models show mixed performance, with no consistent superiority, though the gpt-4.1-nano fine-tuned model performs particularly well for document-level simplification in at least one case.", "conclusion": "Prompt-based use of gpt-4.1-mini is a strong baseline for scientific text simplification at multiple granularities, while fine-tuning does not uniformly improve results and can be sensitive to task level and model size. The observed mixed outcomes emphasize that simplification behavior differs between sentence- and document-level tasks and that model choice and configuration matter."}}
{"id": "2512.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16108", "abs": "https://arxiv.org/abs/2512.16108", "authors": ["Wendong Bi", "Yirong Mao", "Xianglong Liu", "Kai Tian", "Jian Zhang", "Hanjie Wang", "Wenhui Que"], "title": "WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning", "comment": null, "summary": "Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.", "AI": {"tldr": "They propose WeMusic-Agent, an LLM-based framework that combines internalized musical knowledge and tool calling to improve conversational, personalized music recommendation, and create a new real-world benchmark to evaluate it.", "motivation": "Conversational music recommendation requires nuanced understanding of user preferences and music context, but existing models struggle to both contain rich domain knowledge and flexibly use external tools like music retrieval APIs. Additionally, there is a lack of open, realistic benchmarks for evaluating personalized conversational music recommendation systems.", "method": "They design WeMusic-Agent, a training framework that teaches an LLM to decide when to use its internal knowledge versus when to call external tools. Concretely, they build WeMusic-Agent-M1 by continuing pretraining on a 50B-token music-related corpus to internalize knowledge, and training the model with agentic boundary learning so it can invoke specialized tools (e.g., retrieval APIs, recommender systems) when appropriate. They also construct a WeChat Listen\u2013based benchmark for conversational, personalized music recommendation with evaluation dimensions such as relevance, personalization, and diversity.", "result": "On real-world data from WeChat Listen, WeMusic-Agent significantly outperforms existing models across evaluation dimensions, showing better quality in conversational personalized music recommendations.", "conclusion": "Jointly internalizing large-scale musical knowledge and learning to selectively use external tools yields a more effective LLM-based conversational music recommender. The new benchmark provides a standard way to evaluate such systems on relevance, personalization, and diversity, and experiments confirm the advantages of the proposed WeMusic-Agent framework."}}
{"id": "2512.16602", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16602", "abs": "https://arxiv.org/abs/2512.16602", "authors": ["Iker Garc\u00eda-Ferrero", "David Montero", "Roman Orus"], "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics", "comment": null, "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.", "AI": {"tldr": "The paper presents Refusal Steering, an inference-time activation steering technique that precisely controls LLM refusals on politically sensitive topics, removing or inducing refusals without retraining while largely preserving safety and general capability.", "motivation": "Current LLMs often exhibit blanket refusals or overly cautious behavior on politically sensitive topics due to safety fine-tuning. Existing refusal detection methods are brittle (pattern-based) and lack fine-grained control, making it difficult to distinguish between political sensitivity and genuinely harmful content. The authors aim to enable controllable, transparent moderation that can selectively adjust refusal behavior\u2014especially around political topics\u2014without sacrificing safety on harmful content or requiring expensive retraining.", "method": "They use an LLM-as-a-judge to assign refusal confidence scores to model responses instead of relying on pattern-based refusal detection. Using these scores, they compute activation steering vectors that capture the refusal\u2013compliance direction in the model\u2019s representation space. They introduce a ridge-regularized variant to better isolate this direction. At inference time, these steering vectors are added or subtracted from hidden activations to reduce or increase refusal tendencies. They further analyze where in the network these signals reside and how they are distributed across layers and dimensions.", "result": "On Qwen3-Next-80B-A3B-Thinking, their method effectively reduces the model\u2019s tendency to refuse politically sensitive queries while preserving safety on JailbreakBench and maintaining near-baseline performance on general benchmarks. The method generalizes across different model sizes (4B and 80B) and can also be used to induce targeted refusals on demand. Analysis shows that refusal signals are concentrated in deeper transformer layers and spread over many dimensions rather than being localized in a small subspace.", "conclusion": "Refusal Steering demonstrates that activation steering with an LLM-as-a-judge can selectively remove political refusal behavior while keeping strong safety alignment for genuinely harmful content. This provides a practical, inference-time mechanism for fine-grained, controllable, and more transparent moderation of LLM behavior without retraining, and suggests that deeper-layer distributed features encode refusal signals that can be systematically manipulated."}}
{"id": "2512.16149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16149", "abs": "https://arxiv.org/abs/2512.16149", "authors": ["Hao Chen", "Zhexin Hu", "Jiajun Chai", "Haocheng Yang", "Hang He", "Xiaohan Wang", "Wei Lin", "Luhang Wang", "Guojun Yin", "Zhuofeng zhao"], "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs", "comment": "13 pages, 9 tables, 6 figures. Code available at https://github.com/Buycar-arb/ToolForge", "summary": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .", "AI": {"tldr": "ToolForge is an automated framework that synthesizes high-quality tool-use training data for LLMs using virtual tools and (question, context, answer) triples, avoiding costly real API calls while enabling strong multi-hop tool-calling performance.", "motivation": "Existing synthetic data pipelines for training LLMs to call tools and use retrieved information require large numbers of expensive real API calls, yet they still lack rich multi-hop reasoning and self-reflection capabilities. There is a need for a cheaper, scalable way to generate high-quality tool-use data that better captures multi-hop search behavior.", "method": "ToolForge builds a small set of virtual tools and uses (question, golden context, answer) triples as seeds to automatically synthesize large-scale tool-learning data. The framework explicitly targets multi-hop search scenarios and augments data with multi-hop reasoning traces and self-reflection signals. A Multi-Layer Validation Framework combining rule-based checks and model-based evaluations is used to filter and ensure the fidelity of the generated data.", "result": "Using only synthesized data from ToolForge to train an 8B-parameter model yields tool-calling performance that surpasses GPT-4o on multiple benchmarks, despite not relying on any real API calls during data construction.", "conclusion": "Carefully designed virtual tools plus validated synthetic data derived from (question, context, answer) triples can effectively train LLMs for tool use, including multi-hop reasoning, while dramatically reducing dependence on costly real-world API calls. ToolForge demonstrates that such a strategy can achieve state-of-the-art or better performance even with relatively small models."}}
{"id": "2512.16649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16649", "abs": "https://arxiv.org/abs/2512.16649", "authors": ["Bingxiang He", "Zekai Qu", "Zeyuan Liu", "Yinghao Chen", "Yuxin Zuo", "Cheng Qian", "Kaiyan Zhang", "Weize Chen", "Chaojun Xiao", "Ganqu Cui", "Ning Ding", "Zhiyuan Liu"], "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe", "comment": "12 pages, 3 figures", "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.", "AI": {"tldr": "The paper introduces JustRL, a simple single-stage reinforcement learning approach for training large language models that matches or exceeds more complex methods on math reasoning benchmarks with half the compute.", "motivation": "Current RL methods for large language models rely on complex, multi-stage training regimes with dynamic hyperparameters and curricula, which are costly and hard to reproduce. The authors want to test whether this complexity is actually necessary and if a simpler, more stable baseline can achieve comparable or better performance.", "method": "They propose JustRL, a minimal RL training setup using a single training stage, fixed hyperparameters, and no advanced tricks like curriculum learning or dynamic schedules. They apply this setup to two 1.5B-parameter reasoning models and keep the same hyperparameters across both. They also conduct ablations to test the impact of commonly used additional techniques such as explicit length penalties and robust verifiers.", "result": "JustRL attains state-of-the-art average accuracies (54.9% and 64.3%) on nine mathematical reasoning benchmarks while using about half the compute required by more sophisticated RL approaches. Training curves show smooth, monotonic progress over more than 4,000 steps, with no collapses or plateaus. Ablation studies indicate that adding typical \"tricks\" can actually hurt performance by reducing exploration.", "conclusion": "A simple, stable single-stage RL setup with fixed hyperparameters can outperform or match more complex RL pipelines for LLM reasoning tasks, and some widely used add-ons may be detrimental. The authors argue that the community may be overcomplicating RL training to address issues that vanish when starting from a solid, scaled baseline, and they release code and models to serve as such a baseline."}}
{"id": "2512.16171", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16171", "abs": "https://arxiv.org/abs/2512.16171", "authors": ["Karthikeyan K", "Philip Wu", "Xin Tang", "Alexandre Alves"], "title": "Science Consultant Agent", "comment": null, "summary": "The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.", "AI": {"tldr": "Introduces a web-based AI assistant (Science Consultant Agent) that guides practitioners to choose and implement suitable AI modeling strategies via questionnaires, literature-based recommendations, and automatic prototype generation.", "motivation": "Practitioners such as product managers, developers, and researchers often struggle to identify the most appropriate AI modeling strategy and to move quickly from problem definition to a working prototype. Existing tools are fragmented: some support requirement elicitation, others provide literature search, and others handle prototyping, but few integrate these steps into a single, coherent workflow. This paper aims to streamline and democratize AI solution design and reduce time-to-prototype for a wide range of users.", "method": "The authors design and implement the Science Consultant Agent as a web-based system with four core components: (1) a structured Questionnaire to capture problem requirements and constraints; (2) Smart Fill to auto-complete or refine user inputs; (3) Research-Guided Recommendation that uses existing literature and best practices to suggest appropriate modeling strategies; and (4) a Prototype Builder that generates initial AI solution prototypes based on the selected strategy. The workflow from user input to prototype creation is summarized in a pipeline (Figure 1).", "result": "The system provides end-to-end guidance from problem specification to prototype generation for diverse practitioner roles. While the abstract does not report quantitative metrics, it implies faster and more effective development of AI-based solutions by systematically combining requirement elicitation, research-backed recommendations, and automated prototyping in one tool.", "conclusion": "Integrating questionnaires, intelligent input assistance, literature-informed recommendations, and automatic prototype creation into a single web-based agent can accelerate and improve the process of selecting and implementing AI modeling strategies for a broad range of practitioners. The Science Consultant Agent demonstrates a unified pipeline that supports AI solution design from initial specification to working prototype."}}
{"id": "2512.16770", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16770", "abs": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "comment": null, "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.", "AI": {"tldr": "The paper proposes GinSign, a framework to more accurately ground natural language specifications into system-specific temporal logic by framing grounding as a structured classification task instead of free-form generation, achieving significantly higher grounded logical-equivalence than prior work.", "motivation": "Existing NL-to-temporal-logic systems often assume perfect grounding between natural language atoms and system-specific atoms, or they perform poorly at this grounded translation. This mis-grounding leads to syntactically valid but semantically incorrect temporal logic formulas, undermining verification and enforcement of autonomous system behaviors. There is a need for a more reliable and efficient way to ground NL specifications into particular system signatures without relying on large, expensive LLMs.", "method": "The authors introduce GinSign, a framework that grounds lifted natural language specifications into a given system signature. They define the task as mapping natural language spans (lifted atomic propositions) to elements of a predefined set of system atoms. GinSign decomposes grounding hierarchically: it first predicts predicate labels and then chooses appropriately typed constant arguments, turning the problem into a structured classification task. This enables the use of smaller masked language models instead of large generative LLMs.", "result": "Across multiple domains, GinSign achieves high grounded logical equivalence between the produced temporal logic formulas and target expressions, with a reported score of 95.5%, representing a 1.4x improvement over the prior state of the art. The experiments also show that approaches that do not explicitly handle grounding may yield syntactically correct but semantically mismatched formulas that are not suitable for downstream model checking.", "conclusion": "By reframing NL-to-TL grounding as a hierarchical classification problem over a system signature, GinSign improves the semantic correctness of temporal logic translations while remaining computationally efficient. This better grounding enables reliable downstream model checking and represents a substantial advance over existing NL-to-TL translation frameworks that either rely on idealized grounding assumptions or have low grounded accuracy."}}
{"id": "2512.16185", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16185", "abs": "https://arxiv.org/abs/2512.16185", "authors": ["Gourab Ghatak"], "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications", "comment": null, "summary": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.", "AI": {"tldr": "Introduces the weighted K-harmonic means (WKHM) clustering algorithm with convergence guarantees and shows its benefits for wireless network design.", "motivation": "Classical K-means and related clustering methods struggle with numerical instability in harmonic-mean variants, lack soft assignments, and lack a direct, rigorous interpretation in wireless user association. There is also a gap in stochastic convergence guarantees for harmonic-mean-based clustering under realistic random deployments such as in wireless networks.", "method": "Propose the WKHM algorithm, a regularized K-harmonic means variant using inverse-distance-based soft weights that coincide with fractional user association in wireless networks. Formulate the clustering as an optimization problem and analyze it under both deterministic and stochastic settings. Prove monotone descent to a local minimum for fixed initializations, convergence in probability when cluster centers are initialized via a Binomial Point Process, and almost sure convergence under certain decay conditions. Validate performance via extensive simulations across various user distribution scenarios, benchmarked against classical and modern clustering baselines.", "result": "Theoretical results show rigorous convergence guarantees: monotone decrease of the objective to a local minimum for fixed initialization, convergence in probability under BPP initialization, and almost sure convergence under mild conditions. Empirically, WKHM outperforms baseline clustering methods in wireless-network-inspired tasks, achieving a better tradeoff between minimum signal strength and load fairness across diverse simulated user distributions.", "conclusion": "WKHM is a numerically stable, theoretically grounded harmonic-mean-based clustering algorithm that naturally models fractional user association in wireless networks. It is the first such method with proven stochastic convergence guarantees and demonstrates superior practical performance for jointly optimizing radio node placement and user association, making it a strong, principled candidate for wireless network design problems."}}
{"id": "2512.16795", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.", "AI": {"tldr": "The paper introduces a reasoning-trace-augmented RAG framework with conflict-aware evaluation to handle conflicting, outdated, or subjective sources, significantly improving answer correctness and behavior adherence over baselines.", "motivation": "Standard RAG systems struggle when retrieved documents conflict, are outdated, or contain subjective information, and existing solutions treat these problems separately without unified reasoning supervision. There is a need for a framework that can reason explicitly about conflicts and trustworthiness of evidence, provide interpretable reasoning traces, and systematically evaluate model behavior under such challenging conditions.", "method": "The authors propose a three-stage reasoning-trace-augmented RAG framework: (1) document-level adjudication to judge the reliability and relevance of each retrieved document, (2) conflict analysis to explicitly detect and reason about disagreements across sources, and (3) grounded synthesis to generate final answers or justified refusals, all with structured, citation-linked reasoning traces. They also introduce the Conflict-Aware Trust-Score (CATS) pipeline that uses an LLM-as-a-Judge to evaluate groundedness, factual correctness, refusal accuracy, and consistency with desired conflict-handling behavior. A 539-query dataset is constructed to train and evaluate these components, and supervised fine-tuning is applied to LLMs (e.g., Qwen) on this reasoning framework.", "result": "Using their dataset and CATS evaluation, the proposed method shows large improvements over baseline RAG approaches, especially on conflict and grounding-sensitive metrics. For the Qwen model, supervised fine-tuning with their framework increased end-to-end answer correctness from 0.069 to 0.883 and behavioral adherence (alignment with desired conflict-aware behaviors, including justified refusals) from 0.074 to 0.722.", "conclusion": "Reasoning-trace-augmented RAG with explicit conflict analysis and adjudication yields more accurate, trustworthy, and interpretable responses when evidence is conflicting, outdated, or subjective. The CATS evaluation pipeline and 539-query dataset provide a foundation for developing and benchmarking conflict-aware, interpretable RAG systems, and the substantial empirical gains suggest that structured reasoning supervision is key to robust RAG behavior."}}
{"id": "2512.16214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16214", "abs": "https://arxiv.org/abs/2512.16214", "authors": ["Jianming Liu", "Ren Zhu", "Jian Xu", "Kun Ding", "Xu-Yao Zhang", "Gaofeng Meng", "Cheng-Lin Liu"], "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving", "comment": null, "summary": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.", "AI": {"tldr": "The paper introduces PDE-Agent, a multi-agent, tool-augmented framework that uses large language models and external tools to automatically solve PDEs from natural language, along with a new benchmark (PDE-Bench) and metrics to evaluate such systems.", "motivation": "Solving PDEs is fundamental but traditionally requires manual setup and deep domain expertise. Existing neural approaches like PINNs and frameworks like DeepXDE reduce some effort but still need expert intervention and are not fully autonomous. There is a need for a system that can take natural language descriptions of PDE problems and autonomously coordinate tools and reasoning to solve them.", "method": "The authors model PDE solving as an LLM-driven tool invocation problem and design PDE-Agent, a toolchain-augmented multi-agent framework. It has two main components: (1) a Prog-Act framework with graph memory for coordinating multiple agents, supporting dynamic planning, localized error correction, and global revisions through dual feedback loops; (2) a Resource-Pool with a tool-parameter separation mechanism to manage runtime artifacts and orchestrate multiple tools, addressing inter-tool dependencies. They also construct PDE-Bench, a benchmark of diverse PDE tasks tailored for evaluating agent-based tool collaboration, and define multi-level metrics to measure tool coordination efficacy.", "result": "Experiments on PDE-Bench show that PDE-Agent can handle complex, multi-step PDE tasks with cross-step dependencies more effectively than existing methods, demonstrating better applicability and performance in automated PDE solving when multiple tools and reasoning steps are required.", "conclusion": "PDE-Agent establishes a new paradigm for automated PDE solving by combining LLM-based multi-agent reasoning with structured toolchains and shared resources, enabling end-to-end solutions from natural language. The accompanying PDE-Bench and metrics provide a foundation for evaluating such systems, and the approach is expected to drive further advances in automated scientific computing. The authors plan to release code and data publicly."}}
{"id": "2512.16802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16802", "abs": "https://arxiv.org/abs/2512.16802", "authors": ["Primo\u017e Kocbek", "Azra Frkatovi\u0107-Hod\u017ei\u0107", "Dora Lali\u0107", "Vivian Hui", "Gordan Lauc", "Gregor \u0160tiglic"], "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology", "comment": "Will be published in IEEE BigData 2025 proceedings. Contains 10 pages, 1 figure, 5 tables", "summary": "Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.", "AI": {"tldr": "The paper benchmarks different multi-modal retrieval-augmented generation (MM-RAG) pipelines for biomedical question answering in glycobiology, comparing text-conversion vs OCR-free visual retrieval, and shows that the optimal pipeline depends on model capacity.", "motivation": "In biomedical domains like glycobiology, much crucial information is encoded in complex figures and tables rather than plain text. MM-RAG systems must decide whether to convert these visuals into text beforehand or to retrieve raw page images via OCR-free visual retrievers and let the language model interpret them. There is little empirical evidence on which strategy works better, under what conditions, and how this interacts with model size and retriever choice. This gap complicates building reliable, grounded QA systems for visually dense scientific literature.", "method": "The authors construct a benchmark of 120 expert-written multiple-choice questions from 25 glycobiology papers, stratified into three retrieval difficulty levels: easy text-only, medium where figures/tables are central, and hard cross-evidence questions. They build four RAG configurations: (1) no retrieval, (2) text-only RAG using Docling to parse documents and Qdrant for indexing, (3) multi-modal conversion where figures/tables are converted into descriptive text, and (4) late-interaction OCR-free visual retrieval using ColPali that returns page images. They evaluate these pipelines with both open-source mid-size models (e.g., Gemma-3-27B-IT) and frontier proprietary models (GPT-4o family), and then extend experiments to GPT-5 variants and multiple visual retrievers (ColPali, ColQwen, ColFlor). Performance is measured as MCQ accuracy with Agresti-Coull 95% confidence intervals over 5 runs per configuration.", "result": "For Gemma-3-27B-IT, pipelines that converted document content into text (Text RAG and Multi-modal conversion) achieved substantially higher accuracy (0.722\u20130.740) than OCR-free page-image retrieval (0.510). With GPT-4o, the best-performing configuration was Multi-modal conversion (0.808), with Text RAG (0.782) and ColPali-based OCR-free retrieval (0.745) close behind and with relatively small performance gaps. In follow-up experiments with GPT-5-family models, improved ColPali and ColFlor configurations both reached ~0.828 accuracy, and across the GPT-5 family, differences among ColPali, ColQwen, and ColFlor were not statistically significant. The smallest GPT-5-nano model lagged larger GPT-5 variants by about 8\u201310 percentage points in accuracy.", "conclusion": "The effectiveness of MM-RAG pipeline design in visually dense biomedical QA is model-capacity dependent. For mid-size LLMs, pre-converting figures and tables into structured or descriptive text yields better performance than relying on OCR-free visual retrieval, because it reduces the interpretive burden on the model. As model capacity and visual reasoning capabilities increase (e.g., GPT-4o, GPT-5), OCR-free visual retrieval becomes competitive, with visual retrievers such as ColFlor matching heavier options like ColPali at lower computational cost. Thus, for practitioners, text-centric or multi-modal conversion strategies are preferable with smaller models, whereas efficient OCR-free visual retrieval (e.g., ColFlor) is a strong default when powerful frontier models are available."}}
{"id": "2512.16237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16237", "abs": "https://arxiv.org/abs/2512.16237", "authors": ["Zhi Helu", "Huang Jingjing", "Xu Wang", "Xu Yangbin", "Zhang Wanyue", "Jiang Baoyang", "Deng Shirui", "Zhu Liang", "Li Fangfang", "Zhao Tiejun", "Lin Yankai", "Yao Yuan"], "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis", "comment": null, "summary": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.", "AI": {"tldr": "SPRITE is a framework that programmatically generates large-scale, diverse, and precise spatial reasoning data for training VLMs, using simulators and LLM-based code generation to create verifiable ground truth and significantly improve spatial intelligence performance.", "motivation": "Embodied intelligence requires strong spatial understanding and reasoning, but current VLM training data either rely on rigid, low-diversity templates or costly, imprecise human annotations. This limits scalability, linguistic diversity, and computational precision, hindering progress in robust spatial reasoning for AI agents.", "method": "SPRITE reframes ground-truth creation as a code-generation problem. Large language models translate complex natural-language spatial questions into executable programs. These programs are run and verified against accurate scene meta-data from 3 simulators, ensuring computationally precise, automatically checkable labels. This pipeline systematically synthesizes large numbers of image/video\u2013instruction pairs with high linguistic diversity and precise spatial annotations.", "result": "Using SPRITE, the authors construct a dataset spanning 3 simulators, over 11k scenes, and more than 300k instruction-tuning pairs. A vision-language model trained on this dataset shows substantial improvement on multiple spatial reasoning benchmarks and surpasses models trained on other open-source datasets of comparable size. A scalability study further shows that increasing linguistic and structural diversity (beyond simple templates) is key to more robust spatial intelligence.", "conclusion": "Programmatic, LLM-assisted code-generation combined with simulators can overcome the scalability\u2013diversity\u2013precision trade-off in spatial reasoning datasets. SPRITE enables large-scale, diverse, and verifiable spatial supervision that measurably improves VLM spatial intelligence. The authors commit to releasing the framework and full dataset to support further research in spatial reasoning and embodied AI."}}
{"id": "2512.16245", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16245", "abs": "https://arxiv.org/abs/2512.16245", "authors": ["Aniruddha Roy", "Jyoti Patel", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints", "comment": null, "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.", "AI": {"tldr": "AlignMerge is a geometry-aware framework for merging fine-tuned LLM checkpoints that explicitly preserves alignment while combining capabilities.", "motivation": "Existing model-merging methods (e.g., weight soups, task vectors, Fisher-weighted averaging) can maintain task loss but severely degrade alignment and safety. Alignment is usually checked only after merging instead of being enforced during the process. The authors want a principled, geometry-based way to merge models so that alignment remains an invariant rather than an afterthought.", "method": "They view merging as an operation constrained by the model\u2019s geometry around an already aligned base model. In a local Fisher information chart around an instruction-tuned anchor, they estimate an alignment subspace with a projector P_A. They then optimize a merge objective L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud. L_geo keeps the merged model close to its expert models in Fisher-Rao geometry, L_align penalizes movements along alignment-sensitive directions (within the alignment subspace), and L_bud enforces a soft alignment-budget constraint. For the alignment functional, they use the decoding-invariant Alignment Quality Index (AQI), which measures how well aligned vs misaligned behaviors are separated in latent representation space.", "result": "On five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), when merging safety-anchored models with task experts, AlignMerge improves alignment metrics such as AQI, toxicity scores, and LLM-judge-based alignment while retaining or surpassing the best expert\u2019s performance on instruction-following, reasoning, and helpfulness. It also shows reduced drift in the alignment subspace and fewer alignment-budget violations compared with Fisher soups, TIES, SafeMerge, and MergeAlign.", "conclusion": "Alignment-aware geometry should guide model merging rather than being validated after the fact. AlignMerge demonstrates that it is possible to compose capabilities from multiple fine-tuned LLMs while explicitly preserving alignment as an invariant, offering a practical and scalable direction for geometry-aware composition in future foundation models."}}
{"id": "2512.16250", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.16250", "abs": "https://arxiv.org/abs/2512.16250", "authors": ["Sanjoy Chowdhury", "Karren D. Yang", "Xudong Liu", "Fartash Faghri", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Dinesh Manocha", "Chun-Liang Li", "Raviteja Vemulapalli"], "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding", "comment": null, "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.", "AI": {"tldr": "The paper introduces AMUSE, a benchmark for testing multimodal LLMs on multi-speaker, dialogue-centric audio-video reasoning, and RAFT, an agentic alignment framework that significantly improves model performance on this benchmark.", "motivation": "Existing multimodal LLMs like GPT-4o and Qwen3-Omni are strong at perception but weak at reasoning in multi-speaker, temporal, dialogue-heavy scenarios, which are crucial for applications like conversational video assistants and meeting analytics. There is a need for a benchmark and training framework focused specifically on agentic, multi-speaker multimodal reasoning.", "method": "The authors design AMUSE, a benchmark built around inherently agentic tasks that require planning, grounding, and reflection over multimodal (audio-visual) interactions, evaluated under zero-shot, guided, and agentic modes and six task families (e.g., spatio-temporal speaker grounding, multimodal dialogue summarization). They then propose RAFT, a data-efficient agentic alignment framework that combines reward optimization with intrinsic multimodal self-evaluation as the reward signal and uses selective parameter adaptation for efficient updating of MLLMs.", "result": "When evaluated on AMUSE, existing MLLMs show poor multi-speaker reasoning and inconsistent behavior across both agentic and non-agentic settings. Applying RAFT to these models yields up to 39.52% relative improvement in accuracy on the AMUSE benchmark.", "conclusion": "AMUSE exposes fundamental weaknesses of current MLLMs in agentic multi-speaker multimodal reasoning, while RAFT demonstrates that targeted agentic alignment with intrinsic self-evaluation and selective parameter updates can substantially enhance performance. Together, they offer both an evaluation platform and a practical method for improving agentic reasoning in multimodal models."}}
{"id": "2512.16262", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16262", "abs": "https://arxiv.org/abs/2512.16262", "authors": ["Yifei She", "Ping Zhang", "He Liu", "Yanmin Jia", "Yang Jing", "Zijun Liu", "Peng Sun", "Xiangbin Li", "Xiaohe Hu"], "title": "Learning to Wait: Synchronizing Agents with the Physical World", "comment": null, "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.", "AI": {"tldr": "Introduces an agent-side method for LLM agents to handle asynchronous, non-blocking actions by learning when and how long to wait, reducing unnecessary polling and latency.", "motivation": "Real-world tasks often include actions with variable completion times (e.g., API calls, job scheduling), unlike standard synchronous MDP settings. Existing environment-side fixes like blocking wrappers or high-frequency polling either don't scale well or flood the agent with redundant observations, wasting context and resources. There is a need for agents that can themselves reason about time and align their internal decision cycles with the temporal dynamics of the environment.", "method": "Extend the Code-as-Action paradigm so that the LLM not only selects actions but also predicts explicit waiting durations (via time.sleep(t)) using semantic priors and in-context learning. The agent learns to maintain a \"Cognitive Timeline\" that tracks expected completion times of non-blocking actions and schedules its own re-checks accordingly, avoiding constant polling while still reacting promptly when actions finish.", "result": "In experiments on a simulated Kubernetes cluster with asynchronous operations, the proposed agent-side temporal reasoning enables LLM agents to choose accurate sleep durations, which substantially reduces query overhead (number of environment checks) and overall execution latency compared to naive polling or blocking baselines.", "conclusion": "Temporal awareness\u2014specifically, the ability to estimate and schedule waiting times\u2014is a learnable skill for LLM agents. Empowering agents to manage their own time alignment with the environment improves efficiency and scalability in open-ended, asynchronous settings, and is a key capability for more autonomous, real-world agents."}}
{"id": "2512.16279", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16279", "abs": "https://arxiv.org/abs/2512.16279", "authors": ["Yiliu Yang", "Yilei Jiang", "Qunzhong Wang", "Yingshui Tan", "Xiaoyong Zhu", "Sherman S. M. Chow", "Bo Zheng", "Xiangyu Yue"], "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems", "comment": "Preprint", "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.", "AI": {"tldr": "The paper introduces QuadSentinel, a four-agent guardrail framework that turns natural-language safety policies into machine-checkable rules and enforces them efficiently for LLM-based tool-using agents, improving safety accuracy and reducing false positives over existing single-guard baselines.", "motivation": "LLM-based agents increasingly perform complex, multi-step tasks with tools and communication, which introduces safety risks. Current safety policies are written in natural language by deployers, making them ambiguous, context-dependent, and hard to translate into precise, machine-enforceable rules. Existing runtime enforcement approaches can be unreliable and either miss violations or generate many false positives, especially as tasks grow more complex. There is a need for a systematic, scalable way to formalize and enforce safety policies over an agent\u2019s evolving state and actions without redesigning the core agents.", "method": "The authors formalize safety policies as sequents and compile them into machine-checkable rules that operate over predicates defined on observable states of the agent and environment. They design QuadSentinel, a guard system composed of four specialized agents: (1) a state tracker that maintains the relevant observable state; (2) a policy verifier that checks whether sequents (rules) are satisfied or violated; (3) a threat watcher that looks for emerging risks; and (4) a referee that arbitrates among rules and agents using a custom referee logic. An efficient top-k predicate updater prioritizes which predicates to evaluate to keep runtime costs low, while the referee resolves conflicts hierarchically among rules and signals enforcement decisions in real time. The policies are kept separate from core agents, allowing deployment without modifying existing agent architectures.", "result": "On ST-WebAgentBench (ICML CUA \u201925) and AgentHarm (ICLR \u201925), QuadSentinel achieves higher guardrail accuracy and rule recall compared with baselines, while reducing false positive rates. When compared to single-agent guard approaches such as ShieldAgent (ICML \u201925), QuadSentinel provides better overall safety control for LLM-based agents. The experiments show that its multi-agent guard architecture and prioritized predicate evaluation can enforce rich policies more reliably and efficiently.", "conclusion": "QuadSentinel demonstrates that expressing safety policies as logical sequents and enforcing them via a dedicated four-agent guard architecture can significantly improve safety control over LLM-based agents without modifying the core agents. By separating natural-language policies from their machine-checkable logical form and using efficient, prioritized runtime checking with referee logic, the framework yields more accurate and less intrusive guardrails. The pattern is practical for near-term deployment and is made accessible via an open-source implementation."}}
{"id": "2512.16295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16295", "abs": "https://arxiv.org/abs/2512.16295", "authors": ["Zhenyu Wu", "Jingjing Xie", "Zehao Li", "Bowen Yang", "Qiushi Sun", "Zhaoyang Liu", "Zhoumianze Liu", "Yu Qiao", "Xiangyu Yue", "Zun Wang", "Zichen Ding"], "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models", "comment": null, "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.", "AI": {"tldr": "OS-Oracle introduces a scalable way to build and train critic models that judge each GUI action of computer-using agents, plus a benchmark (OS-Critic Bench) and a 310k-sample dataset, yielding a 7B VLM critic that sets new state-of-the-art results and boosts existing GUI agents.", "motivation": "VLM-based agents that operate computers through GUIs are becoming powerful, but they still make unreliable step-level decisions, especially in long, multi-step tasks where mistakes can accumulate and irreversible actions are costly. There is a need for critic models that can evaluate each planned action before execution, but progress is limited by the absence of diverse, high-quality GUI feedback data and standardized benchmarks to evaluate such critics.", "method": "The authors build OS-Oracle with three key components: (1) a scalable synthetic data pipeline to generate cross-platform GUI critic data (covering Mobile, Web, and Desktop interfaces); (2) a two-stage training strategy for the critic model that first uses supervised fine-tuning (SFT) on the curated critic dataset and then applies consistency-preserving group relative policy optimization (CP-GRPO) to further align the model\u2019s judgments; and (3) OS-Critic Bench, a comprehensive benchmark designed to measure critic performance across different device types and GUI environments. They collect and refine 310k critic samples and train a 7B-parameter vision-language critic model, OS-Oracle-7B, within this framework.", "result": "Using the 310k-sample dataset and the two-stage SFT + CP-GRPO training paradigm, the OS-Oracle-7B critic model achieves state-of-the-art results among open-source VLMs on the new OS-Critic Bench, and even outperforms proprietary models on mobile GUI tasks. When used as a pre-critic (i.e., to screen or evaluate actions before execution), OS-Oracle-7B significantly improves the task performance of existing GUI agents like UI-TARS-1.5-7B in OSWorld and AndroidWorld environments.", "conclusion": "OS-Oracle demonstrates that high-quality synthetic critic data, combined with a tailored two-stage training paradigm and a dedicated benchmark, can substantially improve step-level decision-making for GUI-based computer-using agents. The work shows that a 7B VLM critic can reach and surpass state-of-the-art performance, including over proprietary systems in some domains, and can be effectively plugged into existing agents to reduce errors in long-horizon GUI tasks. The released code, data, and benchmark are intended to foster further research on reliable step-level evaluation in computer use."}}
{"id": "2512.16300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16300", "abs": "https://arxiv.org/abs/2512.16300", "authors": ["Fanrui Zhang", "Qiang Zhang", "Sizhuo Zhou", "Jianwen Sun", "Chuanhao Li", "Jiaxin Ai", "Yukang Feng", "Yujie Zhang", "Wenjie Li", "Zizhen Li", "Yifan Chang", "Jiawei Liu", "Kaipeng Zhang"], "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection", "comment": "11 pages, 6 figures", "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.", "AI": {"tldr": "The paper introduces ForenAgent, an interactive agent framework that combines low-level image artifacts and high-level multimodal language model reasoning for image forgery detection via tool generation and refinement.", "motivation": "Current image forgery detection approaches are split between low-level artifact analysis and high-level semantic reasoning with MLLMs, but they struggle to effectively integrate these heterogeneous information streams and model their cross-level interactions. There is a need for a unified, interpretable, and flexible framework that harnesses both types of information for more general-purpose forgery detection.", "method": "The authors propose ForenAgent, a multi-round interactive framework in which an MLLM autonomously generates, executes, and iteratively refines Python-based low-level image analysis tools tailored to the detection objective. The system is trained with a two-stage pipeline: (1) Cold Start to bootstrap basic tool interaction capability, and (2) Reinforcement Fine-Tuning to progressively enhance reasoning and tool-use adaptability. They design a human-inspired dynamic reasoning loop\u2014global perception, local focusing, iterative probing, and holistic adjudication\u2014and use it both as a data sampling procedure and as a task-aligned process reward signal. Additionally, they build FABench, a large agent-forensics dataset with 100k images and about 200k agent-interaction QA pairs, to train and evaluate the framework.", "result": "Experiments on challenging image forgery detection tasks demonstrate that ForenAgent acquires emergent competence in using low-level tools and exhibits reflective, multi-step reasoning when assisted by such tools, outperforming existing approaches and showing strong generalization across heterogeneous forgery scenarios (as implied from the abstract).", "conclusion": "ForenAgent effectively bridges low-level artifact analysis and high-level MLLM-based semantic reasoning through an interactive, tool-augmented agent design, enabling more flexible, interpretable, and general-purpose image forgery detection. The proposed FABench benchmark supports systematic training and evaluation, and the results indicate that tool-augmented agents are a promising direction for robust IFD systems."}}
{"id": "2512.16301", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16301", "abs": "https://arxiv.org/abs/2512.16301", "authors": ["Pengcheng Jiang", "Jiacheng Lin", "Zhiyi Shi", "Zifeng Wang", "Luxi He", "Yichen Wu", "Ming Zhong", "Peiyang Song", "Qizheng Zhang", "Heng Wang", "Xueqiang Xu", "Hanwen Xu", "Pengrui Han", "Dylan Zhang", "Jiashuo Sun", "Chaoqi Yang", "Kun Qian", "Tian Wang", "Changran Hu", "Manling Li", "Quanzheng Li", "Hao Peng", "Sheng Wang", "Jingbo Shang", "Chao Zhang", "Jiaxuan You", "Liyuan Liu", "Pan Lu", "Yu Zhang", "Heng Ji", "Yejin Choi", "Dawn Song", "Jimeng Sun", "Jiawei Han"], "title": "Adaptation of Agentic AI", "comment": null, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "AI": {"tldr": "The paper presents a unified framework for understanding and categorizing how agentic AI systems and their tools are adapted to improve performance and reliability.", "motivation": "Agentic AI systems built on foundation models are becoming more capable and complex, and adaptation is critical to enhance their performance, reliability, and generalization. However, existing research on adaptation strategies is fragmented, making it hard for researchers and practitioners to see the overall design space, compare approaches, and choose appropriate strategies.", "method": "The paper proposes a systematic conceptual framework that unifies adaptation strategies in agentic AI. It categorizes adaptation into agent adaptations and tool adaptations, and further subdivides them into tool-execution-signaled vs. agent-output-signaled agent adaptation, and agent-agnostic vs. agent-supervised tool adaptation. The authors then survey representative methods in each category, analyze their characteristics, and discuss trade-offs and design implications.", "result": "The main result is a clarified design space for adaptation in agentic AI, where different adaptation strategies and their relationships are explicitly organized. The framework reveals trade-offs between approaches, and the review of representative methods in each category provides structured insight into their strengths and weaknesses. This organization supports more principled reasoning about how to adapt agents and tools in practice.", "conclusion": "The paper concludes that their unified framework offers both a conceptual foundation and a practical roadmap for developing more capable, efficient, and reliable agentic AI systems. By making adaptation strategies and their trade-offs explicit, it aims to guide researchers and practitioners in selecting, combining, or switching strategies when designing complex agentic AI systems, and it identifies open challenges and future research opportunities in this space."}}
{"id": "2512.16317", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16317", "abs": "https://arxiv.org/abs/2512.16317", "authors": ["Arther Tian", "Alex Ding", "Frank Chen", "Alan Wu", "Aaron Chan", "Bruce Zhang"], "title": "Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference", "comment": null, "summary": "Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.\n  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.", "AI": {"tldr": "They propose a cost-aware Proof of Quality (PoQ) scheme for decentralized LLM inference that jointly rewards output quality and computational efficiency, showing it can economically favor high-quality, low-latency models and evaluators.", "motivation": "Existing decentralized LLM inference schemes need a way to verify or agree on model outputs without centralized trust. PoQ uses consensus on output quality instead of cryptographic proof of computation, but prior work assumes all nodes have similar compute costs and ignores heterogeneity in latency and efficiency across inference and evaluator nodes. This omission makes incentives economically unrealistic and may fail to reward efficient participants.", "method": "They extend PoQ with a cost-aware reward mechanism that explicitly incorporates measured computational cost (e.g., latency) for both inference models and evaluator models. The evaluation pipeline integrates three components: ground-truth token-level F1 scores, lightweight learned evaluators, and GPT-based judgments, yielding a unified quality score. Rewards are then computed via a linear function that balances normalized output quality against normalized cost. They empirically evaluate the framework on extractive QA and abstractive summarization tasks using a pool of five instruction-tuned LLMs (TinyLlama-1.1B to Llama-3.2-3B) as inference nodes and three types of evaluation models (cross-encoder and bi-encoder architectures). They also run Monte Carlo simulations over 5,000 PoQ rounds to study reward dynamics under the cost-aware scheme.", "result": "Empirically, a semantic textual similarity bi-encoder used as an evaluator shows substantially higher correlation with both ground-truth metrics and GPT judgments than cross-encoder evaluators, highlighting that evaluator architecture critically affects PoQ reliability. Quality-cost analysis across the LLM pool shows that the largest models tested are also the most efficient when measured as quality per unit latency, contradicting the assumption that smaller models are always more efficient. Monte Carlo simulations of 5,000 PoQ rounds reveal that the proposed cost-aware reward function systematically allocates higher average rewards to inference models that are both high-quality and low-cost, as well as to efficient evaluators, while reducing rewards for slow, low-quality nodes.", "conclusion": "Incorporating explicit computational cost into PoQ\u2019s reward design yields an incentive mechanism that better reflects real-world heterogeneity in node efficiency for decentralized LLM inference. The cost-aware PoQ framework can reliably favor high-quality, low-latency inference models and accurate, efficient evaluators, and penalize inefficient participants. This suggests it is a practical and economically sustainable foundation for decentralized LLM ecosystems, and that evaluator architecture choice is a key factor for robust quality consensus."}}
{"id": "2512.16344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16344", "abs": "https://arxiv.org/abs/2512.16344", "authors": ["Peter Coveney", "Roger Highfield"], "title": "AI Needs Physics More Than Physics Needs AI", "comment": null, "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.", "AI": {"tldr": "The paper critiques the real-world impact and scientific limitations of current AI, arguing that physics can strongly inform a new paradigm, \u2018Big AI\u2019, that unites physical theory with machine learning.", "motivation": "Despite years of hype and some notable successes, AI\u2019s broad, measurable impact on science and society appears limited and often overstated. In physics and chemistry, recent Nobel prizes acknowledge AI\u2019s promise, but there is concern that current AI systems are not grounded in physical laws, lack interpretability and robustness, and may be ill-suited for deep scientific discovery. The authors are motivated to reassess the relationship between AI and physics and to propose how physics can guide the next generation of AI models.", "method": "The authors perform a critical review and conceptual analysis. They examine existing AI architectures\u2014large language models, reasoning models, and agentic systems\u2014against criteria important for scientific practice: parameter efficiency, bias, uncertainty quantification, mechanistic explanation, and ability to represent physical laws. They synthesize existing critiques, survey opportunities in quantum AI and analogue computing, and then construct a conceptual roadmap they call \u2018Big AI\u2019, in which theory-driven, physically grounded models are integrated with machine-learning techniques.", "result": "They identify key limitations of current mainstream AI in scientific contexts: extreme parameter counts with limited semantic grounding, vulnerability to distributional bias, poor or absent uncertainty estimates, lack of mechanistic insight, and inability to encode or recover even simple scientific laws reliably. They highlight emerging directions\u2014quantum AI and analogue computing\u2014as promising ways to build systems that naturally incorporate physical structure. The main result is a structured roadmap outlining how theory-based modelling and machine learning can be combined into a more rigorous, physically informed AI paradigm.", "conclusion": "The paper concludes that, at present, AI\u2019s transformative image is overstated relative to its measured impact, particularly in physics. Instead of viewing AI as an autonomous driver of scientific breakthroughs, the authors argue that physics and physical theory should play a central role in shaping future AI architectures. They advocate for \u2018Big AI\u2019, in which rigorous theoretical models and physical constraints are fused with flexible machine-learning methods to achieve more interpretable, robust, and scientifically useful AI systems, especially leveraging advances in quantum and analogue computing."}}
{"id": "2512.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16392", "abs": "https://arxiv.org/abs/2512.16392", "authors": ["Mohammad-Javad Rezaei", "Mozafar Bag-Mohammadi"], "title": "PCIA: A Path Construction Imitation Algorithm for Global Optimization", "comment": null, "summary": "In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.", "AI": {"tldr": "The paper introduces a new swarm-inspired metaheuristic, PCIA, which mimics how humans construct and adapt paths, and shows it is highly competitive on standard benchmark problems.", "motivation": "Many existing metaheuristic optimization algorithms either get stuck in local optima or lack robustness across a wide class of benchmark problems. The authors aim to design a new algorithm that better balances exploration and exploitation using an intuitive human path-construction metaphor.", "method": "They propose the Path Construction Imitation Algorithm (PCIA), where each particle encodes a path to a destination in the search space. The algorithm mimics human behavior: (1) preference for popular/used routes, (2) intelligent recombination of existing partial paths when a route is blocked, and (3) random selection of alternative routes to explore unknown destinations. A population of such paths is evolved iteratively, similar to swarm-based algorithms, to minimize or maximize objective functions.", "result": "PCIA was empirically evaluated on 53 standard mathematical optimization benchmark problems and 13 constrained optimization problems. Its performance was compared to both classical and recent metaheuristics and was found to be highly competitive, often matching or outperforming them on these benchmarks.", "conclusion": "The study concludes that PCIA is an effective and competitive metaheuristic optimizer that can serve as a viable alternative to popular and state-of-the-art algorithms on a wide variety of unconstrained and constrained optimization problems."}}
{"id": "2512.16553", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16553", "abs": "https://arxiv.org/abs/2512.16553", "authors": ["Yumeng Wang", "Tianyu Fan", "Lingrui Xu", "Chao Huang"], "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild", "comment": "Data and code are available at https://github.com/Tango-Whiskyman/Needle_in_the_Web", "summary": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.", "AI": {"tldr": "The paper introduces Needle in the Web, a new benchmark to test how well LLMs and search agents handle fuzzy, exploratory web search queries using real-world web content.", "motivation": "Existing web benchmarks focus mainly on complex multi-hop reasoning for well-defined queries, overlooking fuzzy exploratory searches where users have vague, multifaceted information needs and want the most relevant webpage rather than a single answer. There is a need to systematically evaluate LLMs and search agents on this underexplored but realistic search scenario.", "method": "The authors construct Needle in the Web, a benchmark of 663 questions across seven domains, derived from factual claims found in real web pages. They design a flexible data-generation methodology that allows them to control question difficulty while ensuring high-quality, ambiguous yet answer-unique queries. They then evaluate three state-of-the-art LLMs and three agent-based web-search systems on this benchmark, measuring accuracy across domains and difficulty levels.", "result": "Across the benchmark, most evaluated models perform poorly, with many staying below 35% accuracy and none achieving consistently strong performance across all domains and difficulty settings. This indicates that current systems are not robust at handling fuzzy, ambiguous exploratory search tasks over live web content.", "conclusion": "Needle in the Web exposes a substantial gap in current LLMs and search agents: they struggle with fuzzy exploratory search under semantic ambiguity. The benchmark offers a challenging testbed and underscores the need for improved methods for fuzzy retrieval and reasoning over real-world web data in response to vague user queries."}}
{"id": "2512.16424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16424", "abs": "https://arxiv.org/abs/2512.16424", "authors": ["Nguyen Xuan-Vu", "Daniel Armstrong", "Milena Wehrbach", "Andres M Bran", "Zlatko Jon\u010dev", "Philippe Schwaller"], "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs", "comment": null, "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.", "AI": {"tldr": "Introduces Synthelite, an LLM-based, interactive computer-aided synthesis planning framework that generates and adapts retrosynthetic routes under user constraints.", "motivation": "Traditional computer-aided synthesis planning systems are limited in how they incorporate human expert knowledge and guidance. They typically rely on fixed rule sets or models and do not offer flexible, natural interaction with chemists, reducing their usefulness in real-world settings where expert judgment and changing constraints are critical.", "method": "The authors design Synthelite, a framework that leverages large language models to directly propose retrosynthetic transformations and assemble complete synthetic routes. The system accepts natural language prompts from expert chemists, allowing them to impose constraints (e.g., strategies, starting materials) and dynamically steer the planning process. The LLM\u2019s internal chemical knowledge and reasoning abilities are used to explore synthesis pathways while integrating user feedback.", "result": "In experiments on tasks with user-imposed constraints\u2014such as specific synthetic strategies or fixed starting materials\u2014Synthelite achieves up to 95% success rate in generating valid synthesis routes. The framework also shows evidence of considering chemical feasibility in its designed routes, rather than only producing formally correct but impractical transformations.", "conclusion": "Synthelite demonstrates that LLMs can serve as interactive, central orchestrators for synthesis planning, capable of generating end-to-end retrosynthetic routes while incorporating expert chemists\u2019 guidance via natural language. The work suggests a promising direction for more collaborative, flexible CASP tools in which human expertise and machine reasoning are tightly integrated."}}
{"id": "2512.16442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16442", "abs": "https://arxiv.org/abs/2512.16442", "authors": ["Allard Oelen", "S\u00f6ren Auer"], "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles", "comment": null, "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.", "AI": {"tldr": "They present TIB AIssistant, a platform of AI assistants and tools that support all stages of the research life cycle and export work as RO-Crate bundles for transparency and reproducibility.", "motivation": "AI, especially LLMs, are increasingly used in academia, but there is a need for an integrated, transparent, and reproducible platform that supports researchers across the entire research life cycle rather than in isolated tasks.", "method": "Design and implementation of the TIB AIssistant platform: a collection of specialized assistants mapped to specific research tasks, integrated with external scholarly services via tools, and with generated artifacts stored as assets that can be exported in RO-Crate format. They illustrate its use through a sequential walk\u2011through where assistants interact to generate sections of a draft research paper.", "result": "The authors provide a working AI-supported research platform, demonstrate how multiple assistants can collaboratively support tasks from idea to draft paper writing, and show that outputs can be packaged as RO-Crate bundles to improve transparency and reproducibility.", "conclusion": "TIB AIssistant establishes a foundational, extensible platform for AI-supported research, demonstrating end\u2011to\u2011end support for the research life cycle and setting the stage for a community-maintained ecosystem of research assistants and tools."}}
{"id": "2512.16447", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16447", "abs": "https://arxiv.org/abs/2512.16447", "authors": ["S\u00f6ren Auer", "Allard Oelen", "Mohamad Yaser Jaradeh", "Mutahira Khalid", "Farhana Keya", "Sasi Kiran Gaddipati", "Jennifer D'Souza", "Lorenz Schl\u00fcter", "Amirreza Alasti", "Gollam Rabby", "Azanzi Jiomekong", "Oliver Karras"], "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant", "comment": null, "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.", "AI": {"tldr": "The paper introduces TIB AIssistant, a domain-agnostic, modular human-AI collaboration platform to support researchers throughout the research lifecycle using Generative AI and large language models.", "motivation": "Generative AI and Large Language Models can greatly enhance research workflows, but their integration is difficult due to differences between disciplines, limited AI literacy among researchers, complexity in coordinating multiple AI tools and agents, and uncertainty about AI accuracy in research contexts. The authors aim to address these challenges by providing a structured, accessible platform for applying AI in scientific discovery.", "method": "The authors propose and design TIB AIssistant, a modular human-machine collaborative platform. They define a conceptual framework, system architecture, and implement an early prototype. The platform includes prompt and tool libraries, a shared data store, and a flexible orchestration framework that links components to support tasks such as ideation, literature analysis, methodology planning, data analysis, and scholarly writing, across different research domains.", "result": "An early prototype of the TIB AIssistant platform is implemented, showing that the proposed architecture and modular components can be practically realized and can support multiple research tasks along the research lifecycle. The prototype demonstrates the feasibility and potential benefits of domain-agnostic AI support for researchers, though detailed quantitative evaluations are not mentioned in the abstract.", "conclusion": "The conceptual framework and prototype of TIB AIssistant suggest that a domain-agnostic, modular human-AI collaboration platform can make it easier to integrate Generative AI into research workflows. By providing shared infrastructure\u2014prompt and tool libraries, data storage, and orchestration\u2014the system can support diverse research tasks and disciplines, indicating promising potential to enhance scientific discovery and scholarly work."}}
{"id": "2512.16453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16453", "abs": "https://arxiv.org/abs/2512.16453", "authors": ["Jiayang Yang", "Chunhui Zhao", "Martin Guay", "Zhixing Cao"], "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries", "comment": null, "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.", "AI": {"tldr": "The paper introduces TimeSeries2Report (TS2R), a prompting framework that converts raw battery time-series data into structured natural-language reports so that large language models can better reason about, predict, and manage battery energy storage systems without retraining.", "motivation": "While LLMs are powerful, they struggle with raw multivariate time-series typical in real-world battery energy storage systems. There is a gap between low-level sensor signals and the high-level, context-rich information needed for operations and maintenance tasks such as anomaly detection, charge management, and state estimation. The authors want a practical way to leverage off-the-shelf LLMs for BESS intelligence without changing their architectures or retraining them on large domain-specific datasets.", "method": "The authors propose TimeSeries2Report (TS2R), a prompting framework that translates lithium-ion battery operational time-series into structured, semantically rich textual reports. TS2R works by segmenting time-series into meaningful intervals, applying semantic abstraction to summarize patterns and events, and using rule-based interpretation to map sensor dynamics (e.g., voltage/current changes, cycles) into higher-level descriptions relevant to BESS operation. The resulting reports are then used as prompts to LLMs, enabling them to perform tasks such as anomaly detection, state-of-charge prediction, and charging/discharging management. The framework is benchmarked against vision-, embedding-, and direct text-based prompting baselines on both lab-scale and real-world datasets, with evaluations on report quality and downstream task metrics.", "result": "Across lab-scale and real-world datasets, TS2R-based report prompting leads to better LLM performance than alternative prompting strategies. Specifically, it improves accuracy, robustness, and explainability in anomaly detection, state-of-charge prediction, and charging/discharging decision tasks. The LLMs, when provided with TS2R-generated reports, reach expert-level quality in decisions and show consistent predictive behavior, all without any retraining or modifications to the LLM architectures.", "conclusion": "Translating raw battery time-series data into structured natural-language reports via TS2R effectively unlocks LLM capabilities for BESS operation and maintenance. This approach bridges the gap between low-level sensor data and high-level reasoning, enabling expert-level, explainable decision-making and prediction by unmodified, general-purpose LLMs. TS2R thus offers a practical path toward adaptive, LLM-driven battery intelligence in both laboratory and real-world settings."}}
{"id": "2512.16465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16465", "abs": "https://arxiv.org/abs/2512.16465", "authors": ["Jinwu Chen", "Qidie Wu", "Bin Li", "Lin Ma", "Xin Si", "Yang Hu", "Shouyi Yin", "Jun Yang"], "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution", "comment": null, "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.", "AI": {"tldr": "cuPilot is a multi-agent, strategy-driven framework that uses LLMs plus evolutionary search to automatically generate highly optimized CUDA kernels, significantly outperforming PyTorch on a large kernel benchmark.", "motivation": "Hand-optimizing CUDA kernels requires deep, specialized hardware-software co-design knowledge and access to proprietary, highly tuned libraries. Current LLM-based automatic optimization systems underperform because their agents are poorly coordinated and the representations used for evolutionary search do not align well with how optimization decisions are actually made. This paper aims to bridge that gap to make automatic kernel optimization both more effective and more practical.", "method": "The authors design cuPilot, a strategy-coordinated multi-agent framework for CUDA kernel optimization. Instead of evolving raw code directly, cuPilot introduces an explicit intermediate representation called a \u201cstrategy\u201d that captures high-level optimization decisions (e.g., tiling, memory hierarchy usage, parallelization patterns). Multiple LLM-based agents collaborate: some propose or refine strategies, others translate strategies into concrete CUDA kernels. Evolutionary search operates at the strategy level using a strategy-coordinated evolution algorithm. The system further guides the LLMs with roofline-model-informed prompts and initializes the evolutionary population at the strategy level to improve coverage and convergence.", "result": "On a benchmark of 100 CUDA kernels, kernels generated by cuPilot achieve an average 3.09\u00d7 speedup over PyTorch baselines. On GEMM workloads, cuPilot discovers sophisticated optimization schemes and reaches high utilization of key GPU hardware units, indicating that the framework can match or approach expert-level tuning in complex, performance-critical kernels. The authors also release the produced kernels as an open-source corpus.", "conclusion": "Representing optimization decisions as explicit strategies and coordinating multiple LLM agents around this representation leads to substantially better automatic CUDA kernel optimization than prior LLM+evolution approaches. cuPilot demonstrates that strategy-level evolution, roofline-guided prompting, and principled initialization can collectively deliver large performance gains over standard frameworks like PyTorch, particularly on demanding GEMM tasks, and provides a reusable set of optimized kernels for the community."}}
{"id": "2512.16468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16468", "abs": "https://arxiv.org/abs/2512.16468", "authors": ["Danial Safaei", "Siddartha Khastgir", "Mohsen Alirezaei", "Jeroen Ploeg", "Son Tong", "Xingyu Zhao"], "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery", "comment": null, "summary": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.", "AI": {"tldr": "They propose a new metric, Decisive Feature Fidelity (DFF), to measure whether autonomous vehicle systems rely on the same causal visual evidence in simulation as in the real world, and show it exposes and helps fix mismatches that traditional fidelity metrics miss.", "motivation": "Current virtual testing for autonomous vehicles focuses mainly on visual realism and output similarity between simulated and real data. However, high pixel-level realism does not guarantee that the system-under-test (e.g., a perception or decision model) uses the same causal features in both domains. This mismatch undermines trust in simulation-based safety assurance. There is a need for a quantitative, behavior-grounded metric that can assess whether the underlying decision mechanisms are aligned across real and synthetic environments.", "method": "The paper introduces Decisive Feature Fidelity (DFF), a system-specific fidelity metric aimed at capturing mechanism parity: whether the same causal evidence drives decisions in both real and simulated domains. DFF uses explainable AI techniques to extract decisive features\u2014those most responsible for a model\u2019s output\u2014from matched real-synthetic image pairs. It then measures similarity between these decisive feature sets across domains. The authors propose practical DFF estimators based on counterfactual explanations and develop a DFF-guided calibration scheme that tunes simulators or data-generation pipelines to better align decisive features between real and synthetic data.", "result": "On 2126 matched image pairs from KITTI and VirtualKITTI2, DFF uncovers discrepancies in the model's reliance on features that are not detected by standard output-value fidelity metrics (e.g., agreement in predictions or scores). Moreover, when the simulator is calibrated using DFF feedback, both decisive-feature-level and input-level fidelity improve, while preserving conventional output-value fidelity across multiple system-under-test architectures.", "conclusion": "Behavior-grounded metrics are essential for trustworthy simulation-based testing of autonomous vehicles. Decisive Feature Fidelity provides a practical way to quantify mechanism parity between real and synthetic domains by comparing the causal evidence used in decisions. It reveals hidden mismatches that output-based metrics miss and can be used to guide simulator calibration, resulting in simulators that better reflect real-world decision processes without degrading prediction accuracy."}}
{"id": "2512.16491", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16491", "abs": "https://arxiv.org/abs/2512.16491", "authors": ["Theresa Eimer", "Lennart Sch\u00e4permeier", "Andr\u00e9 Biedenkapp", "Alexander Tornede", "Lars Kotthoff", "Pieter Leyman", "Matthias Feurer", "Katharina Eggensperger", "Kaitlin Maile", "Tanja Tornede", "Anna Kozak", "Ke Xue", "Marcel Wever", "Mitra Baratchi", "Damir Pulatov", "Heike Trautmann", "Haniye Kashgarani", "Marius Lindauer"], "title": "Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network", "comment": null, "summary": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.", "AI": {"tldr": "The paper is a guideline report that consolidates and systematizes best practices for empirical meta-algorithmic research (algorithm selection, configuration, scheduling) across the full experimental lifecycle.", "motivation": "Empirical work in meta-algorithmics requires large, costly experiments and offers many degrees of freedom in experimental design, which introduces numerous potential error sources and threats to validity. Existing best practices are scattered across publications and subfields and evolve in an uncoordinated way, making it hard for researchers\u2014especially newcomers\u2014to design sound, scalable, and reproducible studies.", "method": "The authors survey and synthesize good practices from different subfields represented in the COSEAL community. They organize these practices along the stages of an empirical study: defining research questions, choosing and planning the experimental design, running experiments, and analyzing and reporting results in an unbiased way. The work is presented as a consolidated guideline document rather than based on new experiments.", "result": "The paper compiles a structured set of best-practice recommendations that cover the entire experimental cycle in meta-algorithmic research. It clarifies state-of-the-art methodology, highlights common pitfalls, and aligns practices across previously fragmented subfields.", "conclusion": "By unifying and documenting current best practices for empirical meta-algorithmic research, the report provides a reference framework that improves the reliability, validity, and comparability of studies in algorithm selection, configuration, and scheduling. It is intended as a practical guideline for both newcomers and experienced practitioners in these areas."}}
{"id": "2512.16529", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16529", "abs": "https://arxiv.org/abs/2512.16529", "authors": ["Julien Gachadoat", "Guillaume Lagarde"], "title": "ParamExplorer: A framework for exploring parameters in generative art", "comment": "16 pages, 3 figures", "summary": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.", "AI": {"tldr": "The paper presents ParamExplorer, a framework for exploring complex parameter spaces in generative art, using RL-inspired, interactive agents to discover aesthetically interesting outputs more efficiently than manual trial-and-error.", "motivation": "Generative art systems have large, complex parameter spaces where desirable, aesthetically pleasing outputs occupy small and scattered regions. Artists currently explore these spaces mainly via manual, time-consuming trial-and-error, which leaves many interesting configurations undiscovered. A structured, semi-automated exploration method is needed to make this process more efficient and thorough.", "method": "The authors design ParamExplorer, an interactive and modular framework influenced by reinforcement learning paradigms. It interfaces with generative art algorithms (specifically p5.js projects) and supports human-in-the-loop or automated feedback as a reward signal. Within this framework, they implement different exploration strategies (agents) that propose parameter configurations, receive feedback, and adapt their subsequent search in the parameter space.", "result": "Within ParamExplorer, several exploration agents are implemented and evaluated, demonstrating that these strategies can more effectively navigate the complex parameter space of generative art compared to unguided or purely manual exploration, leading to the discovery of more diverse and aesthetically compelling outputs.", "conclusion": "ParamExplorer provides a practical, extensible tool for generative artists and researchers to explore high-dimensional parameter spaces more systematically. By leveraging RL-inspired agents and interactive feedback, it reduces manual trial-and-error and uncovers a broader range of interesting generative art configurations, while integrating smoothly with existing p5.js workflows."}}
{"id": "2512.16531", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16531", "abs": "https://arxiv.org/abs/2512.16531", "authors": ["Ander Alvarez", "Alessandro Genuardi", "Nilotpal Sinha", "Antonio Tiene", "Samuel Mugel", "Rom\u00e1n Or\u00fas"], "title": "Scaling Laws for Energy Efficiency of Local LLMs", "comment": null, "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.", "AI": {"tldr": "The paper benchmarks LLMs and VLMs on CPU-only edge devices, derives empirical scaling laws for compute vs. input size, and shows that quantum-inspired compression and resolution control can greatly cut CPU, memory, and energy while preserving accuracy.", "motivation": "Most deployment work focuses on GPUs, while everyday edge devices (laptops, Pis, embedded controllers) are CPU-only and resource constrained. There is little quantitative understanding of how LLM and VLM compute, memory, and energy scale on such CPUs, and what simple knobs (model compression, input resolution) most effectively reduce cost without hurting accuracy.", "method": "Benchmark multiple language and vision-language models on two representative CPUs: MacBook Pro M2 and Raspberry Pi 5. Use a unified measurement pipeline with continuous sampling of CPU and memory usage and energy, then integrate area under the curve to obtain total computational load. Vary token length for LLMs and image resolution for VLMs, and compare standard vs. quantum-inspired compressed models.", "result": "They find two empirical scaling laws: (1) LLM inference cost scales roughly linearly with token count; (2) VLM cost is flat above a model-specific internal resolution clamp and drops sharply below it (\u201cresolution knee\u201d), driven by preprocessing. Quantum-inspired compression cuts CPU and memory use by up to 71.9% and energy by up to 62%, while maintaining or improving semantic accuracy on the tested tasks and devices.", "conclusion": "CPU-only inference for LLMs/VLMs on edge devices follows simple, actionable scaling laws, and both model compression and careful choice of input resolution are powerful, low-cost levers for sustainable, efficient local deployment. These insights help practitioners design and tune multimodal workloads on common CPU-based hardware without sacrificing accuracy."}}
{"id": "2512.16532", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16532", "abs": "https://arxiv.org/abs/2512.16532", "authors": ["Himanshu Gharat", "Himanshi Agrawal", "Gourab K. Patro"], "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment", "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)", "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.", "AI": {"tldr": "The paper investigates how adding memory-based personalization to LLM agents can systematically introduce and amplify bias, using recruitment as a case study.", "motivation": "While biases in traditional ML models and standard LLMs have been widely studied, the specific impact of long-term memory and personalization in LLM-based agents on bias formation and amplification remains underexplored. As LLM agents with memory are increasingly deployed in high-stakes, user-facing domains, understanding their bias dynamics is critical for safety and fairness.", "method": "The authors design a simulated recruitment scenario where an LLM-based AI agent is augmented with a memory mechanism that stores interaction histories and uses them for personalization. They run controlled experiments with safety-trained LLMs to model how the agent behaves over multiple stages of operation, analyzing how stored memories shape subsequent decisions and whether biases emerge or grow over time.", "result": "The experiments show that even when using safety-trained LLMs, introducing memory and personalization leads to systematic patterns of biased behavior. Biases not only appear but also get reinforced and amplified as the agent accumulates and reuses past interaction data across stages of the recruitment pipeline.", "conclusion": "Memory-enhanced personalization in LLM-based agents can be a structural source of bias, turning otherwise safer base models into biased systems over time. Therefore, additional protective mechanisms\u2014such as stronger guardrails, memory governance, and bias-aware personalization strategies\u2014are necessary when deploying memory-augmented LLM agents, especially in sensitive decision-making domains like recruitment."}}
{"id": "2512.16650", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.16650", "abs": "https://arxiv.org/abs/2512.16650", "authors": ["Jirui Yang", "Hengqi Guo", "Zhihui Lu", "Yi Zhao", "Yuansen Zhang", "Shijing Hu", "Qiang Duan", "Yinggui Wang", "Tao Wei"], "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models", "comment": null, "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.", "AI": {"tldr": "The paper proposes Prefix Probing, a low-cost, black-box method to detect harmful content with large language models by comparing log-probabilities of safety- versus harm-indicative prefixes, achieving near first-token latency and accuracy comparable to external safety models.", "motivation": "LLMs in safety-critical applications must detect harmful content accurately, but current solutions often require separate safety models or multi-stage inference, increasing latency and deployment cost. The authors aim to reduce this overhead while maintaining strong detection performance, solving the trade-off between detection accuracy, inference speed, and system complexity.", "method": "They introduce Prefix Probing, which treats the base LLM as a black-box and computes conditional log-probabilities for two types of short textual prefixes appended to the prompt: \u201cagreement/execution\u201d prefixes that indicate compliance with potentially harmful requests, and \u201crefusal/safety\u201d prefixes that indicate safe behavior. The harmfulness score is derived from the relative log-probabilities of these prefixes. They use prefix caching so that only a single forward pass is needed, keeping overhead near first-token latency. Additionally, they design an automated prefix construction algorithm to search for and select highly discriminative prefixes that improve the separation between harmful and benign prompts.", "result": "Experiments show that Prefix Probing matches or closely approaches the detection effectiveness of mainstream external safety models across benchmarks, while greatly reducing computational overhead. It does not require deploying any extra models and operates with almost no additional latency beyond the first token computation. The learned prefixes significantly outperform manually chosen ones, confirming the benefit of automated prefix construction.", "conclusion": "Prefix Probing offers a practical and efficient alternative to external safety classifiers for harmful content detection with LLMs. By leveraging log-probability comparisons over informative prefixes and using caching, it achieves strong safety detection performance with minimal latency and no extra model deployments, making it suitable for real-world, safety-sensitive LLM applications where cost and speed are critical."}}
{"id": "2512.16656", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16656", "abs": "https://arxiv.org/abs/2512.16656", "authors": ["Sri Yash Tadimalla", "Justin Cary", "Gordon Hull", "Jordan Register", "Daniel Maxwell", "David Pugalee", "Tina Heafner"], "title": "Comprehensive AI Literacy: The Case for Centering Human Agency", "comment": "2 figures, 2 tables", "summary": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.", "AI": {"tldr": "The paper argues that current AI education overemphasizes tool use and underemphasizes critical, ethical understanding, and proposes human-agency-centered AI literacy frameworks to correct this.", "motivation": "AI is rapidly permeating society, but existing educational approaches mostly teach how to operate AI tools, not how to think critically and ethically about them. This creates a dangerous literacy gap where students and teachers adopt AI uncritically, risk ceding pedagogical and personal control to tools, and fail to understand the broader impact of AI on knowledge, work, and society. The authors want to reorient AI education toward cultivating human judgment and responsibility.", "method": "This is a position paper that conceptually develops and argues for a human-agency-centered approach to AI literacy. It proposes and describes three interrelated frameworks\u2014AI Literacy, AI Fluency, and AI Competency\u2014as lenses for structuring learning outcomes and educational practices. The argument is grounded in epistemology, critical thinking, and educational theory rather than empirical experiments.", "result": "The paper articulates a structured set of AI literacy, fluency, and competency frameworks that place human agency at the core. These frameworks clarify how students and teachers can intentionally choose when and how to use\u2014or not use\u2014AI, and how to align AI use with educational values and ethical considerations. The result is a conceptual toolkit that helps stakeholders articulate intentions, responsibilities, and impacts around AI-related decisions in academic, professional, and societal contexts.", "conclusion": "The authors conclude that true AI literacy must go beyond operational skills to explicitly foreground human agency, critical thinking, and epistemological understanding. Technology use should be taught as a matter of conscious choice, not inevitability. By adopting the proposed AI Literacy, Fluency, and Competency frameworks, educational systems can support students and teachers in becoming reflective, responsible agents in a human-centric AI ecosystem, better prepared to navigate AI\u2019s influence on learning, careers, and society."}}
{"id": "2512.16694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16694", "abs": "https://arxiv.org/abs/2512.16694", "authors": ["Wisnu Uriawan", "Achmad Ajie Priyajie", "Angga Gustian", "Fikri Nur Hidayat", "Sendi Ahmad Rafiudin", "Muhamad Fikri Zaelani"], "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm", "comment": null, "summary": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.", "AI": {"tldr": "The paper applies the Apriori association rule mining algorithm to preprocessed Indonesian translations of Bukhari hadith to automatically discover thematic associations and semantic relations between key terms, supporting digital Islamic studies and tech-based learning systems.", "motivation": "There is an increasing volume of digitized Islamic texts, creating an urgent need for automated methods to group hadith thematically rather than relying solely on manual classification. Existing work suggests that unsupervised learning and association rule mining can effectively uncover patterns and semantic relations in unlabeled text data. The study aims to test whether Apriori can reveal meaningful thematic structures in hadith translations to support information retrieval and educational applications.", "method": "The authors use an Indonesian translation of Sahih Bukhari as their dataset. They apply standard text preprocessing: case folding to normalize text, punctuation removal, tokenization to split text into tokens, stopword removal to drop non\u2011informative words, and stemming to reduce words to their root forms. After preprocessing, they represent texts as itemsets and apply the Apriori algorithm for association rule mining, using support, confidence, and lift to filter and evaluate the discovered rules. They then interpret the strongest rules in terms of thematic and semantic relationships between religious concepts and narrative elements.", "result": "The Apriori analysis uncovered recurrent association patterns between key terms, such as rakaat\u2013prayer, verse\u2013revelation, and hadith\u2013story. These associations correspond to recognizable themes like ritual worship, the process of revelation, and the narration of hadith. The resulting rules show non\u2011trivial co\u2011occurrence structures that reflect latent semantic relationships in the corpus.", "conclusion": "The study concludes that the Apriori algorithm can automatically identify meaningful thematic and semantic associations in hadith texts, validating unsupervised association rule mining as a viable approach for thematic grouping of religious narratives. This contributes methodological support for digital Islamic studies and can serve as a foundation for technology\u2011based learning tools, thematic search, and knowledge organization systems for large collections of hadith and related texts."}}
{"id": "2512.16698", "categories": ["cs.AI", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.16698", "abs": "https://arxiv.org/abs/2512.16698", "authors": ["Mahbub E Sobhani", "Md. Faiyaz Abdullah Sayeedi", "Mohammad Nehad Alam", "Proma Hossain Progga", "Swakkhar Shatabda"], "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning", "comment": "Accepted to the ARR October 2025 cycle", "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver", "AI": {"tldr": "The paper compares single-agent vs multi-agent pipelines for diagram-grounded geometry problem solving in multimodal LLMs and finds that multi-agent approaches significantly help open-source models but are not universally better, especially for strong proprietary models.", "motivation": "While multimodal LLMs are increasingly evaluated on visual math and geometry tasks, it is unclear when multi-agent (agentic decomposition) architectures provide real performance gains over simpler single-agent setups. The paper aims to fill this gap with a systematic, empirical comparison across several benchmarks and model types.", "method": "The authors build both single-agent and multi-agent problem-solving pipelines for diagram-grounded geometry tasks. They evaluate these pipelines on four visual math benchmarks\u2014Geometry3K, MathVerse, OlympiadBench, and We-Math\u2014using both open-source MLLMs (e.g., Qwen-2.5-VL 7B and 32B) and a strong closed-source model (Gemini-2.0-Flash). They then compare performance deltas between single-agent and multi-agent modes for each model-benchmark combination.", "result": "For open-source Qwen-2.5-VL models, multi-agent pipelines consistently improve accuracy: +6.8 points for the 7B model and +3.3 for the 32B model on Geometry3K, with additional gains on OlympiadBench and We-Math. In contrast, Gemini-2.0-Flash generally does better as a single agent on classic benchmarks, with only modest multi-agent gains on the newer We-Math dataset.", "conclusion": "Multi-agent decomposition is beneficial but not universally optimal. It clearly boosts performance for open-source models and can aid even strong proprietary models on newer, less familiar benchmarks (e.g., We-Math). However, for mature benchmarks and strong proprietary systems, a single-agent pipeline may remain superior. The work underscores that the value of multi-agent designs depends on model strength and benchmark familiarity, and provides open resources (code, data, reasoning traces) for further study."}}
{"id": "2512.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16701", "abs": "https://arxiv.org/abs/2512.16701", "authors": ["Giovanni Adorni"], "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences", "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025", "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.", "AI": {"tldr": "The paper introduces a \u201cCyber Humanism in Education\u201d framework to guide human-centered use of generative AI in learning, emphasizing human agency and shared responsibility in shaping AI-rich educational environments.", "motivation": "As GenAI and large language models rapidly transform reading, writing, and coding practices in education, there are growing concerns about epistemic automation, cognitive offloading, and the potential de-professionalisation of teachers. Existing approaches often treat AI as just another digital tool, without adequately addressing how it restructures knowledge production, validation, and teacher\u2013learner roles. The paper is motivated by the need for a conceptual and practical framework that preserves and strengthens human agency and responsibility in AI-enabled learning environments.", "method": "The authors develop a conceptual framework called \u201cCyber Humanism in Education,\u201d defining AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines. They articulate three key pillars\u2014reflexive competence, algorithmic citizenship, and dialogic design\u2014and map them onto existing international digital and AI competence frameworks. Methodologically, they then illustrate and operationalize this framework via higher education case studies, including implementations of prompt-based learning activities and a Conversational AI Educator certification within the EPICT ecosystem, using these cases to explore how the framework works in practice.", "result": "The presented case studies show that when educators and students engage with AI through structured prompt-based learning and dedicated training (such as the Conversational AI Educator certification), their epistemic agency can be strengthened: they become more reflective and intentional users and shapers of AI tools. At the same time, these implementations surface significant tensions related to increased workload for educators, issues of equity in access and competence, and governance questions about how AI infrastructures are controlled and by whom.", "conclusion": "The paper concludes that a cyber-humanist approach\u2014grounded in reflexive competence, algorithmic citizenship, and dialogic design\u2014offers a viable pathway towards AI-rich yet genuinely human-centred education. By framing educators and learners as epistemic agents and algorithmic citizens who co-author socio-technical infrastructures, the framework supports reclaiming and enhancing human agency in the age of GenAI. The authors argue that future educational policy, design, and professional development should draw on this framework to balance innovation with equity, workload sustainability, and democratic governance of AI systems in education."}}
{"id": "2512.16707", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.16707", "abs": "https://arxiv.org/abs/2512.16707", "authors": ["Abhisek Ganguly"], "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems", "comment": "6 Pages, 0 figures", "summary": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.", "AI": {"tldr": "The paper argues that fundamental limits from logic (incompleteness) and from chaotic dynamics (unpredictability) jointly restrict how well any algorithmic agent can understand and forecast the world\u2014and in particular, prevent it from fully computing its own best possible prediction horizon.", "motivation": "Intelligent systems rely on both logical reasoning and predictive modeling, yet there are well-known theoretical limits in each domain (G\u00f6del-style incompleteness for formal reasoning and chaos/finite precision for prediction). The motivation is to understand how these two independent limits interact and what they imply for an agent\u2019s capacity to reason about and evaluate its own predictive abilities, which is crucial for building reliable, self-monitoring AI systems.", "method": "The authors conceptually formalize two kinds of constraints on algorithmic intelligence: formal incompleteness, which constrains what a consistent reasoning system can prove, and dynamical unpredictability, which constrains how far ahead systems can predict in finite-precision dynamical environments. They then analyze how these constraints interact when an agent attempts to reason about its own prediction performance, deriving limits on self-assessment such as the impossibility of computing a general maximal prediction horizon.", "result": "They show that when both incompleteness and unpredictability are taken into account, there are structural, provable bounds on what an algorithmic agent can know about its own predictive limits. In particular, there is no general algorithm that allows such an agent to compute its own maximal feasible prediction horizon for arbitrary environments subject to these constraints.", "conclusion": "The paper concludes that any algorithmic intelligence faces intrinsic trade-offs among logical reasoning, predictive accuracy, and self-understanding: it cannot fully analyze or certify its own long-term predictive capabilities. This has implications for the design and evaluation of intelligent systems, highlighting unavoidable blind spots in self-assessment and meta-reasoning about prediction performance."}}
{"id": "2512.16733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16733", "abs": "https://arxiv.org/abs/2512.16733", "authors": ["Daniel Bramblett", "Rushang Karia", "Adrian Ciotinga", "Ruthvick Suresh", "Pulkit Verma", "YooJung Choi", "Siddharth Srivastava"], "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities", "comment": null, "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.", "AI": {"tldr": "The paper learns interpretable symbolic (PDDL-style) models of black-box AI planners using Monte-Carlo Tree Search over test tasks.", "motivation": "Black-box AI systems (e.g., large foundational models) are increasingly used for sequential decision making, but they are opaque. For safe deployment, we need efficient, sound, and interpretable descriptions of what these systems can do\u2014their actions, preconditions, and effects\u2014rather than just input-output behavior.", "method": "The authors treat the black-box AI (BBAI) as an unknown planner and search over possible symbolic (PDDL-style) models of its capabilities. Using a Monte-Carlo Tree Search (MCTS) framework, they automatically construct test planning tasks, query the BBAI for behavior on these tasks, and use the observed behavior to prune the hypothesis space of candidate symbolic action models. The resulting learned model encodes actions, their execution conditions, and probabilistic outcomes.", "result": "They prove theoretical guarantees: soundness (no false capabilities are introduced), completeness (true capabilities can be captured), and convergence (the learning process will eventually reach the correct model under assumptions). Empirically, they test multiple black-box AI systems, showing that the method can efficiently learn accurate PDDL-style models that approximate the BBAI\u2019s planning behavior.", "conclusion": "PDDL-style symbolic models can be automatically and efficiently learned from black-box AI planners using MCTS-guided experimentation. These models provide interpretable, probabilistic descriptions of the system\u2019s planning capabilities, with formal guarantees and empirical evidence of efficiency and accuracy, supporting safer and more transparent deployment of such systems."}}
{"id": "2512.16739", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16739", "abs": "https://arxiv.org/abs/2512.16739", "authors": ["Yipeng Zhuang", "Yifeng Guo", "Yuewen Li", "Yuheng Wu", "Philip Leung-Ho Yu", "Tingting Song", "Zhiyong Wang", "Kunzhong Zhou", "Weifang Wang", "Li Zhuang"], "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach", "comment": null, "summary": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.", "AI": {"tldr": "The paper develops a hybrid pipeline combining conventional machine learning with large language models to predict breakthrough pain episodes in hospitalized lung cancer patients within 48\u201372 hours, using both structured and unstructured EHR data.", "motivation": "Breakthrough pain in lung cancer inpatients is common and requires rapid response, but current care is mostly reactive. Clinicians lack accurate, early warning tools that leverage the full richness of EHR data, especially free text, to proactively identify patients at risk and allocate resources accordingly.", "method": "The authors performed a retrospective analysis of 266 hospitalized lung cancer patients. They built a hybrid prediction pipeline that (1) uses a machine learning module to learn temporal patterns in structured data such as demographics, tumor stage, vital signs, and WHO-tiered analgesic use, and (2) uses a large language model to interpret ambiguous dosing records and free-text clinical notes. The outputs from these components are integrated to produce 48h and 72h risk predictions for breakthrough pain episodes, emphasizing model interpretability.", "result": "The integrated system achieved prediction accuracies of 0.874 for 48-hour and 0.917 for 72-hour pain episode forecasting. Incorporating the large language model improved sensitivity by 8.6% for 48-hour prediction and 10.4% for 72-hour prediction, indicating better detection of at-risk patients without sacrificing overall accuracy.", "conclusion": "Combining traditional machine learning with large language models over both structured and unstructured EHR data yields an accurate, more sensitive, and clinically interpretable tool for early prediction of breakthrough pain in lung cancer inpatients. This hybrid framework could support proactive pain management, enabling more precise treatment adjustments and more efficient resource allocation in oncology wards."}}
{"id": "2512.16755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16755", "abs": "https://arxiv.org/abs/2512.16755", "authors": ["Siqi Wang", "Chao Liang", "Yunfan Gao", "Erxin Yu", "Sen Li", "Yushi Li", "Jing Li", "Haofen Wang"], "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?", "comment": null, "summary": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.", "AI": {"tldr": "CitySeeker is a new benchmark to test how well vision-language models can handle implicit human needs in urban navigation, showing current models perform poorly and proposing analysis and strategies to improve spatial intelligence.", "motivation": "Existing vision-language models work relatively well on explicit, instruction-based navigation but struggle with understanding and fulfilling implicit needs like \"I am thirsty\" in complex, real-world city environments. There is no dedicated benchmark to systematically evaluate and stress-test this capability, especially for long-horizon, goal-driven navigation that requires spatial reasoning, memory, and decision-making under visual urban dynamics. The authors aim to fill this gap and understand where and why current models fail.", "method": "The authors build CitySeeker, a benchmark consisting of 6,440 navigation trajectories across 8 cities, designed around 7 types of implicit, goal-driven scenarios (e.g., finding a suitable place given an abstract need). They evaluate state-of-the-art VLMs on these tasks, analyze failure modes (error accumulation, poor spatial cognition, weak experiential recall), and then systematically study a family of exploratory strategies\u2014Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR)\u2014inspired by human cognitive mapping and iterative observation-reasoning cycles.", "result": "On CitySeeker, even leading models such as Qwen2.5-VL-32B-Instruct achieve only 21.1% task completion, indicating that current VLMs are far from robust in implicit-need urban navigation. The experiments identify specific bottlenecks in long-horizon reasoning, spatial understanding of the environment, and the ability to recall and use past observations. The exploratory BCR strategies highlight where targeted interventions can partially mitigate these weaknesses, offering empirical characterizations of their respective effects.", "conclusion": "CitySeeker exposes substantial limitations of current VLMs in handling implicit, goal-driven urban navigation tasks and demonstrates that improving spatial cognition, long-horizon reasoning, and memory mechanisms is crucial. The benchmark and accompanying analyses serve as a testbed and guide for developing future VLMs with stronger spatial intelligence and better ability to solve real-world \"last-mile\" navigation problems based on implicit human needs."}}
