<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.AI](#cs.AI) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: The paper introduces SA-SFT, a simple self-augmentation strategy where an LLM first generates self-dialogue data that is mixed with task-specific data during fine-tuning, mitigating catastrophic forgetting while improving in-domain performance without extra external data or special training schedules.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs on narrow, task-specific datasets often leads to catastrophic forgetting, where the model loses its broad general knowledge and reasoning abilities. Existing mitigation methods (e.g., layer freezing, mixing with external data) either underperform or require additional data or complicated procedures. There is a need for a lightweight, data-efficient, and training-schedule-compatible method to adapt LLMs while preserving their general capabilities.

Method: The authors propose SA-SFT (Self-Augmented Supervised Fine-Tuning). Before fine-tuning on the target task, the LLM is prompted to generate self-dialogues, creating self-authored data in its own style. This synthetic data is then mixed with the task-specific fine-tuning data, and standard supervised fine-tuning is performed without changing the optimization algorithm, hyperparameters, or training schedule. They empirically evaluate this approach across 50 scenarios and complement it with a theoretical analysis of catastrophic forgetting framed as style-induced parameter drift, arguing that self-generated data regularizes the model towards its original style.

Result: Across 50 evaluation scenarios, SA-SFT preserves performance close to the original base model on general evaluations and achieves the best performance in 40 of the scenarios compared to baselines such as layer freezing and mixing with external data. It simultaneously reduces catastrophic forgetting and improves in-domain (task-specific) performance, despite requiring no external data or additional tuning steps.

Conclusion: Self-augmentation via self-generated dialogues is an effective and simple way to adapt LLMs to new tasks while avoiding catastrophic forgetting. By aligning fine-tuning data with the model’s original style, SA-SFT counters style-induced parameter drift, preserving general capabilities and improving task performance without extra data, architectural changes, or complex training modifications.

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [2] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: The paper evaluates knowledge distillation for small language models, showing distilled models are far more compute-efficient while matching or exceeding much larger models’ reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Training large, high-performing language models is extremely compute- and resource-intensive, making them hard to deploy in constrained environments. The authors aim to determine whether knowledge distillation can systematically produce small language models that retain strong reasoning capabilities at a fraction of the training cost, and to quantify this trade-off rigorously.

Method: The authors perform a benchmarking study comparing distilled small language models to their non-distilled (vanilla) equivalents and to larger proprietary models. They measure both model performance—especially on reasoning tasks—and total compute used to train each model. An 8B-parameter model obtained via distillation is contrasted with an 8B vanilla model and with standard models roughly ten times larger, and its position on the performance-versus-compute curve is analyzed.

Result: Distilled models lie on a significantly better performance-to-compute frontier than vanilla models and some larger proprietary baselines. In particular, the distilled 8B model is over 2,000× more compute-efficient to train than its vanilla 8B counterpart, yet attains reasoning performance comparable to or better than models around ten times larger in parameter count.

Conclusion: Knowledge distillation is validated not merely as a model compression technique but as a leading strategy for building state-of-the-art yet accessible language models. It enables small models with strong reasoning ability to be trained with dramatically less compute, offering a practical path to high-quality AI in resource-limited settings.

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [3] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: The paper introduces ConceptRM, a method to build a cleaner training corpus so that reflection models can better filter out false alerts, thus reducing alert fatigue.


<details>
  <summary>Details</summary>
Motivation: Alert-heavy intelligent systems generate many false alerts, causing users to become desensitized and miss critical issues. Existing reflection models that filter alerts rely on user-feedback labels gathered in production, which are noisy and expensive to manually clean. There is a need for a low-annotation-cost way to robustly identify reliable negative samples (false alerts) from noisy feedback data so that reflection models can be trained effectively.

Method: ConceptRM starts with a small set of expert-annotated examples used as anchors. It then constructs perturbed datasets with different controlled noise ratios and trains multiple distinct models using a co-teaching strategy, where models teach each other by focusing on samples they consider clean. By analyzing the consensus among these models on the noisy dataset, ConceptRM identifies high-confidence negative samples (false alerts). This yields a high-quality corpus for training a reflection model that filters out false alerts.

Result: Experiments show that ConceptRM substantially improves the interception of false alerts while requiring only limited expert annotation. It outperforms several state-of-the-art large language model baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets in relevant metrics for false-alert filtering.

Conclusion: ConceptRM is an effective, annotation-efficient framework for constructing high-quality corpora from noisy user feedback to train reflection models that filter false alerts. By leveraging small expert anchors, perturbed noisy datasets, co-teaching, and model consensus, it can reliably identify negative samples and significantly reduce false alerts compared to strong LLM baselines, both in-domain and out-of-domain.

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [4] [InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation](https://arxiv.org/abs/2602.20294)
*Yu Li,Pranav Narayanan Venkit,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: The paper builds a large interview-grounded benchmark and metrics to evaluate how well LLMs can simulate real public personalities, showing that models grounded in real interviews perform better than those using only bios or parametric knowledge.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of personality simulation rely on indirect proxies like demographics, questionnaires, or brief AI-led interviews, which don’t directly compare generated text to what real people actually said. This creates a gap between claimed personality simulation and authentic, empirically verifiable behavior.

Method: The authors collect and process over 671k Q&A pairs from 23k verified interview transcripts of 1,000 public personalities, then design a multi-dimensional evaluation framework with four metrics: content similarity, factual consistency, personality alignment, and factual knowledge retention. They systematically compare different simulation methods: ones grounded in the interview data (via retrieval-augmentation or chronological use) versus those relying on biographical profiles or just the model’s internal knowledge.

Result: Interview-grounded methods consistently outperform approaches based only on bios or parametric knowledge on the proposed metrics. Retrieval-augmented approaches are better at reproducing personality style and generating high-quality responses, while chronological (time-aware) use of interviews better preserves factual correctness and long-term knowledge retention about the personalities.

Conclusion: Grounding personality simulation in real interview data yields more faithful and useful simulations, and different ways of using that data exhibit a clear trade-off between stylistic fidelity and factual robustness. The proposed benchmark and metrics provide a principled basis for choosing simulation methods depending on application needs and offer a scalable path forward for more rigorous personality simulation research.

Abstract: Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.

</details>


### [5] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: The paper studies how the linguistic form of user queries affects hallucination rates in large language models, identifying which query features correlate with higher or lower hallucination risk.


<details>
  <summary>Details</summary>
Motivation: Most research treats LLM hallucinations as intrinsic model or decoding problems, overlooking how the wording and structure of user inputs themselves may trigger or mitigate hallucinations. The authors are motivated by classical linguistics insights that form influences comprehension and response, and they seek to systematically characterize how query features shape hallucination likelihood in practice.

Method: The authors define a 22-dimensional feature representation of queries, capturing aspects like clause complexity (e.g., depth of clause nesting), lexical rarity, anaphora, negation, answerability, and intention grounding. They then apply this feature framework to a large corpus of 369,837 real-world queries, and run large-scale statistical analyses to measure how each feature correlates with hallucination rates across datasets and models, producing what they term a "risk landscape" of hallucination propensity conditioned on query properties.

Result: They find that certain linguistic features consistently elevate hallucination risk, especially deep clause nesting and underspecified queries. Conversely, features such as clearly grounded user intentions and high answerability are associated with lower hallucination rates. Some features, notably domain specificity, show mixed behavior depending on the dataset and model, indicating non-uniform effects across contexts.

Conclusion: The study demonstrates that hallucination risk is not solely a model-side phenomenon but is strongly influenced by the linguistic properties of user queries. By establishing an empirically grounded, query-feature-based representation tied to hallucination propensity, the work lays a foundation for practical tools such as guided query rewriting and for future intervention studies that manipulate query form to systematically reduce hallucinations.

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [6] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: The paper proposes QueryBandits, a contextual bandit-based, model-agnostic framework that adaptively selects query-rewriting strategies to reduce hallucinations in closed-source LLMs, outperforming static or no-rewrite baselines across multiple QA settings.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs are increasing with advanced reasoning abilities, and most existing mitigation methods focus on open-source models using post-hoc detection or parameter editing. However, most real-world institutional deployments use closed-source models where internal weights cannot be modified. There is a need for a method that can reduce hallucinations in these closed-source systems without retraining or gradient-based interventions.

Method: The authors formulate the choice of query-rewrite strategy as a contextual bandit problem. They introduce QueryBandits, which takes semantic features of the input query as context and adaptively learns an online policy to select among multiple rewrite arms (e.g., paraphrasing, expansion, or no rewrite). The framework uses an empirically validated and calibrated reward function that reflects hallucination-related performance. They compare different bandit algorithms, including Thompson Sampling and vanilla (non-contextual) bandits, and benchmark them across 16 QA scenarios against static, zero-shot rewrite policies and a No-Rewrite baseline.

Result: In experiments on 16 QA scenarios, the best-performing contextual bandit variant, using Thompson Sampling, wins over the No-Rewrite baseline in 87.5% of cases. It also substantially outperforms static zero-shot rewrite policies, beating a Paraphrase policy by 42.6% and an Expand policy by 60.3%. All contextual bandit methods outperform vanilla bandits on all datasets. They observe that higher variance in contextual features is associated with more diverse arm selection behavior, supporting the idea that different queries benefit from different rewrite strategies. Some static policies even yield higher cumulative regret than not rewriting at all, meaning they can exacerbate hallucinations.

Conclusion: There is no single universally optimal query-rewriting strategy for mitigating hallucinations in LLMs; using a fixed static policy can even be harmful. Instead, an adaptive, context-aware selection of rewrite strategies via QueryBandits effectively reduces hallucinations and improves QA performance, especially for closed-source models. Because QueryBandits operates purely via forward-pass interactions and does not require accessing or updating model parameters, it is well-suited for institutional deployments where models are closed-source and retraining is impractical.

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [7] [Natural Language Processing Models for Robust Document Categorization](https://arxiv.org/abs/2602.20336)
*Radoslaw Roszczyk,Pawel Tecza,Maciej Stodolski,Krzysztof Siwek*

Main category: cs.CL

TL;DR: The paper compares Naive Bayes, BiLSTM, and fine-tuned BERT for unbalanced text classification and builds a demo system, finding BiLSTM best balances accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To determine which machine learning model best balances accuracy and computational efficiency for automated text classification in real-world, unbalanced document routing scenarios.

Method: Experimentally evaluate three models of increasing complexity—Naive Bayes, BiLSTM, and fine-tuned BERT—on an unbalanced text classification task, measuring accuracy, training time, and computational cost, and implement a demonstrative routing system using these models.

Result: BERT achieves the highest accuracy (>99%) but with heavy computational cost; BiLSTM reaches ~98.56% accuracy with moderate cost and good contextual modeling; Naive Bayes trains in milliseconds but has the lowest accuracy (~94.5%). Class imbalance negatively affects all models, especially on minority classes. A working system for automated routing of technical requests is successfully built.

Conclusion: BiLSTM provides the best trade-off between performance and efficiency for the studied scenario, making it the preferred choice for practical deployment, while BERT and other transformer approaches remain promising directions for future improvement.

Abstract: This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.
  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.
  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.

</details>


### [8] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: The paper challenges the idea that numeral systems are generally optimally efficient, showing instead that many languages have numeral systems that are less efficient than predicted when allomorphy and degrees of complexity are carefully modeled.


<details>
  <summary>Details</summary>
Motivation: Prior work claims that recursive numeral systems optimize communicative efficiency via a tradeoff between lexicon size and morphosyntactic complexity. However, these studies have not captured how complex numeral expressions really are, particularly in terms of predictable vs. unpredictable formal variation (allomorphy). The authors aim to reassess efficiency claims using a more fine-grained representation of numeral complexity across languages.

Method: They compile data from 52 genetically diverse languages’ numeral systems and design an annotation scheme that distinguishes predictable from unpredictable allomorphy in numeral forms. Using this enriched coding of numeral structure and complexity, they quantitatively compare observed systems against efficiency expectations derived from the tradeoff between lexicon size and average morphosyntactic complexity.

Result: The analysis reveals that many numeral systems are significantly less efficient than predicted by the communicative-efficiency tradeoff model once detailed allomorphy patterns are taken into account. In other words, true complexity is higher than previously estimated, and many languages fall well below the predicted optimal efficiency frontier.

Conclusion: Communicative efficiency alone does not explain the structure of numeral systems: many languages maintain numerals that are more complex than optimal. Fine-grained modeling of complexity, including predictable vs. unpredictable allomorphy, is crucial in typological and evolutionary studies of numeral systems. The findings have broader implications for theories of linguistic evolution, showing that pressures other than pure efficiency must be invoked to understand how numeral systems develop and persist.

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [9] [Disentangling Geometry, Performance, and Training in Language Models](https://arxiv.org/abs/2602.20433)
*Atharva Kulkarni,Jacob Mitchell Springer,Arjun Subramonian,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: The paper studies whether geometric properties of Transformer unembedding matrices, especially effective rank, can predict downstream model performance, and finds they largely cannot.


<details>
  <summary>Details</summary>
Motivation: Prior interpretability work has shown that geometric features of Transformer weights, such as the unembedding matrix, carry semantic information, but it is unclear whether these geometric properties can be used as proxies for or predictors of downstream task performance. This gap matters because a cheap geometric metric that tracks performance would be highly valuable for model selection, hyperparameter tuning, and understanding training dynamics without expensive evaluation.

Method: The authors train a controlled suite of 108 OLMo-style language models where they systematically vary training setups and hyperparameters (e.g., batch size, weight decay). They then analyze the geometry of the unembedding matrix and final-layer representations, focusing on effective rank and other geometric metrics. They correlate these geometric measurements with model performance across different tasks and stages of training, and probe for causal or systematic relationships, searching for cases that break or support hypothesized links (e.g., low rank causing performance saturation).

Result: They find that although better-performing models often have higher effective rank in their unembedding matrices, this is not a consistent or universal pattern across tasks or training regimes. Low effective rank is not a causal driver of late-stage degradation in small models; instead, it merely co-occurs with such behavior, and they identify adversarial counterexamples where low-rank models continue to improve without saturation. Effective rank is shown to be heavily influenced by training hyperparameters like batch size and weight decay, and these hyperparameters also affect performance, confounding any straightforward link between geometry and performance. Other geometric metrics and properties of the final-layer representations tend to agree with effective rank but similarly fail to reliably predict downstream performance.

Conclusion: Existing geometric metrics of the unembedding matrix and final-layer representations mainly encode information about training choices rather than serving as robust predictors of downstream performance. While geometry and performance are correlated in some regimes, the relationship is neither universal nor causal, suggesting that using such metrics for performance estimation or automatic model selection is unreliable. Further work is needed to find more faithful geometric or structural indicators of model capability.

Abstract: Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.

</details>


### [10] [From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility](https://arxiv.org/abs/2602.20513)
*Gavin Levinson,Keith Feldman*

Main category: cs.CL

TL;DR: The paper introduces LUX, a structured taxonomy and framework for evaluating the real-world utility of large language models beyond task accuracy, covering performance, interaction, operations, and governance, and links it to a web tool with concrete metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of large language models focus mainly on task-level performance (e.g., accuracy on benchmarks), which is insufficient for determining whether a model is appropriate for high-stakes, real-world applications. Many additional sociotechnical factors affect effectiveness—such as how users interact with models, how models operate in production, and how they are governed—but current metrics for these factors are fragmented and unorganized. There is no unified, systematic taxonomy that helps practitioners consistently assess and compare LLMs across diverse use cases using these broader criteria. The paper aims to fill this gap.

Method: The authors construct the Language Model Utility Taxonomy (LUX), a hierarchical framework that decomposes LLM utility into four main domains: performance, interaction, operations, and governance. Within each domain, they further define aligned dimensions and finer-grained components. Each component is associated with measurable metrics (factors) that allow quantitative comparison of models and alignment of model choice with intended applications. They also implement an external dynamic web tool that operationalizes LUX by linking each taxonomy component to a curated repository of relevant evaluation metrics, enabling practitioners to explore and apply the framework interactively.

Result: The outcome is a comprehensive, organized taxonomy—LUX—that maps the multifaceted notion of LLM utility into four domains, each with dimensions, components, and associated metrics. The framework makes it possible to evaluate LLMs systematically not only on standard performance but also on interaction quality, operational characteristics, and governance-related properties. The accompanying web tool concretely demonstrates how LUX can support applied evaluation by letting users browse components and see corresponding metrics and factors suitable for different real-world use cases.

Conclusion: The paper concludes that assessing LLM suitability for practical, especially high-stakes, deployment requires going beyond traditional performance benchmarks to a broader set of sociotechnical considerations. LUX provides a unified taxonomy and practical scaffolding for such holistic evaluation, helping practitioners select and compare models in a way that is aligned with their intended use contexts. The dynamic web tool extends the impact of the framework by making it accessible and actionable for applied evaluation workflows.

Abstract: As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.

</details>


### [11] [Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning](https://arxiv.org/abs/2602.20528)
*Justin Lovelace,Christian Belardi,Sofian Zalouk,Adhitya Polavaram,Srivatsa Kundurthy,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: STAR-LDM is a language model that interleaves diffusion-based global planning in a continuous latent space with standard autoregressive token generation to improve coherence, reasoning, and controllability.


<details>
  <summary>Details</summary>
Motivation: Conventional autoregressive LMs decide token by token without an explicit global plan, which can hurt long-range coherence, commonsense reasoning, and controllability. The authors want a mechanism that allows the model to "think" and plan at a higher semantic level between generation steps, and to provide easier attribute control without retraining.

Method: They propose STAR-LDM, which periodically pauses autoregressive generation for a "Stop-Think" phase. In this phase, a latent diffusion model operates in a continuous semantic space to refine a high-level plan for the upcoming text. The model then resumes autoregressive decoding, conditioned on this refined plan. They also attach lightweight classifiers to the latent space for attribute steering, avoiding full model retraining.

Result: On language understanding benchmarks, STAR-LDM significantly outperforms other models of similar size. In LLM-as-judge pairwise evaluations focused on narrative coherence and commonsense reasoning, it wins more than 70% of comparisons. The model also yields better trade-offs between fluency and controllability than prior specialized control methods when guided by the attached classifiers.

Conclusion: Interleaving diffusion-based latent planning with autoregressive token generation can substantially improve coherence and reasoning while supporting flexible, classifier-based control. The STAR-LDM architecture demonstrates that a separate "thinking" phase in continuous space can overcome limitations of purely token-level autoregression without retraining the base model for each control task.

Abstract: The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a "thinking" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.

</details>


### [12] [Personal Information Parroting in Language Models](https://arxiv.org/abs/2602.20580)
*Nishant Subramani,Kshitish Ghate,Mona Diab*

Main category: cs.CL

TL;DR: The paper studies how modern language models memorize and reproduce personal information (PI), introduces an improved regex-and-rules detector for PI, and empirically shows that larger and longer-trained models more often parrot verbatim personal data from their training set.


<details>
  <summary>Details</summary>
Motivation: Modern LMs are trained on large web scrapes that include lots of personal information, which they can memorize and regurgitate, creating privacy risks. Existing personal information detectors, often regex-based, are limited, making it hard to accurately measure and understand the extent of this memorization. The paper aims to build better PI detection tools and to quantify how much and under what conditions models parrot PI from their training data.

Method: The authors design a regexes-and-rules (R&R) detector suite tailored to identify specific PI types: email addresses, phone numbers, and IP addresses. They first benchmark R&R against existing regex-based PI detectors to show performance improvements. Using a manually curated dataset of 483 PI instances drawn from model training data, they measure memorization by prompting Pythia models with the prefix tokens from the original documents and using greedy decoding to see whether the model outputs the exact PI span. They run this evaluation over multiple models in the Pythia suite, varying both model size (160M–6.9B parameters) and number of pretraining iterations (70k–143k).

Result: The R&R detector outperforms prior regex-based PI detection methods for finding email addresses, phone numbers, and IP addresses. On the curated set of 483 PI instances, the Pythia-6.9B model parrots 13.6% of them verbatim when given the original preceding context. Across the Pythia suite, both larger model size and longer pretraining correlate positively with the rate of exact PI memorization. Even the smallest model, Pythia-160M, exactly reproduces 2.7% of the tested PI instances.

Conclusion: Modern LMs do memorize and can exactly reproduce personal information from their training data, and this tendency becomes stronger with larger model size and more training. Improved PI detection (via R&R) enables more accurate measurement of this risk. Based on the observed memorization rates, the authors strongly recommend aggressive filtering and anonymization of pretraining datasets to reduce the likelihood of models parroting personal information verbatim.

Abstract: Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.

</details>


### [13] [Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches](https://arxiv.org/abs/2602.20634)
*Saurabh Mishra,Shivani Thakur,Radhika Mamidi*

Main category: cs.CL

TL;DR: The paper compares machine learning models for hate speech detection and explores text transformation methods to neutralize harmful content.


<details>
  <summary>Details</summary>
Motivation: To address the growing problem of hate speech and offensive language on social media by improving automatic detection and mitigation tools.

Method: Evaluate and compare multiple ML architectures (CNNs, LSTMs, BERT and its derivatives, and hybrid models) on hate speech/offensive language detection tasks, and design text transformation techniques that rewrite negative expressions into neutral ones.

Result: Advanced contextual models like BERT achieve the highest overall detection accuracy, while certain hybrid architectures perform better in specific scenarios. Newly proposed text transformation techniques can convert negative content into more neutral text, potentially reducing its harmful impact.

Conclusion: Current state-of-the-art models like BERT are strong for hate speech detection but have limitations; hybrid approaches and transformation-based mitigation are promising directions. The paper suggests future work toward more robust, context-aware, and practical hate speech detection and neutralization systems.

Abstract: The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.

</details>


### [14] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: Defines and measures “semantic novelty” over paragraphs in 28k+ books to uncover canonical narrative shapes and link them to readership, genre, and history.


<details>
  <summary>Details</summary>
Motivation: Traditional narrative analysis focuses on sentiment arcs or topic shifts, but lacks a scalable, information-theoretic way to quantify how much *new* semantic content a text introduces over time. The author wants to capture narrative structure in terms of unpredictability and information density, link this to reader engagement, and test how genre and historical period shape these dynamics while controlling for confounds like book length.

Method: 1) Represent each paragraph with a 768-d SBERT sentence embedding. 2) Define semantic novelty as the cosine distance between the current paragraph embedding and the running centroid of all prior paragraphs. 3) For 28,606 PG19 books, compute paragraph-level novelty trajectories. 4) Compress each trajectory into 16 segments using Piecewise Aggregate Approximation (PAA). 5) Cluster PAA vectors via Ward-linkage to discover archetypal narrative shapes. 6) Derive features from novelty curves (volume, speed, terminal/initial ratio, circuitousness) and correlate them with readership while controlling for book length. 7) Analyze associations between genre and narrative shape via chi-squared tests. 8) Examine historical trends in novelty profiles over 1840–1910. 9) Use SAX (Symbolic Aggregate approXimation) to test how unique each book’s novelty signature is.

Result: 1) Eight canonical narrative shape archetypes emerge, ranging from Steep Descent (early convergence to low novelty) to Steep Ascent (increasing unpredictability). 2) “Volume” (variance of novelty) is the strongest length-independent predictor of readership (partial rho = 0.32). 3) “Speed” and Terminal/Initial novelty ratio also predict readership (both partial rho = 0.19). 4) “Circuitousness” shows high raw correlation with readership (rho = 0.41) but is highly confounded with length (93% correlated); after controlling for length, its predictive power drops (partial rho = 0.11). 5) Genre significantly constrains narrative shapes (χ² = 2121.6, p < 10^-242): fiction tends to maintain plateau-like novelty while nonfiction concentrates information early. 6) Historically, books become more predictable from 1840 to 1910 (declining T/I ratio; r = -0.74, p = 0.037). 7) SAX analysis indicates ~85% of books have unique novelty signatures, implying near-unique paths through semantic space.

Conclusion: Semantic novelty trajectories provide a scalable, information-theoretic view of narrative structure that is distinct from sentiment or topical content. The discovered archetypal shapes show that how information density rises, falls, and varies over a text is systematically related to genre, historical period, and especially reader engagement. Controlling for length reveals that some previously strong correlations are largely artifacts, underscoring the need for careful confound control in corpus studies. Overall, the paper argues that information-density dynamics constitute a fundamental, measurable dimension of narrative form with practical implications for understanding and predicting readership.

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [15] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: The paper introduces CARE, an LLM-based system that predicts detailed therapeutic alliance scores and explains them with rationales from counseling transcripts, outperforming existing models and offering practical insights for mental health care.


<details>
  <summary>Details</summary>
Motivation: Therapeutic alliance, as perceived by clients, is crucial for counseling outcomes, but current ways of measuring it are limited: post-session questionnaires are burdensome and delayed, and existing computational models give only coarse scores, lack interpretability, and do not capture full-session context. There is a need for an automatic, fine-grained, and explainable method to assess alliance directly from counseling conversations.

Method: The authors develop CARE, a framework built on the CounselingWAI dataset, enhanced with 9,516 expert-written rationales explaining alliance ratings. They fine-tune an LLaMA-3.1-8B-Instruct model with rationale-augmented supervision so that it simultaneously predicts multi-dimensional alliance scores and generates textual rationales grounded in the session transcript. They evaluate CARE against leading LLMs using correlation with client ratings and assess rationale quality with both automatic metrics and human judgments. They also apply CARE to real Chinese online counseling sessions to analyze alliance-building patterns and challenges.

Result: CARE achieves substantially higher alignment with client-perceived alliance than baseline LLMs, with more than a 70% improvement in Pearson correlation relative to client ratings. Incorporating rationale-augmented supervision further boosts predictive performance. The system generates rationales that are judged high-quality and contextually grounded by both automatic evaluation and human reviewers. In deployment on real-world Chinese online counseling data, CARE successfully identifies common difficulties in building alliance and reveals how interaction dynamics influence alliance trajectories.

Conclusion: CARE demonstrates that an LLM fine-tuned with rationale-augmented supervision can automatically and accurately estimate multi-dimensional therapeutic alliance from counseling transcripts while providing interpretable explanations. This approach narrows the gap between counselor and client perspectives, supports analysis of alliance-building processes in real-world settings, and shows promise as an AI-assisted tool to inform training, supervision, and quality improvement in mental health services.

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [16] [CAMEL: Confidence-Gated Reflection for Reward Modeling](https://arxiv.org/abs/2602.20670)
*Zirui Zhu,Hailun Xu,Yang Luo,Yong Liu,Kanchan Sarkar,Kun Xu,Yang You*

Main category: cs.CL

TL;DR: CAMEL is a confidence-gated reward modeling framework that combines cheap scalar decisions with selective reflective judging to improve alignment accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing reward models are either efficient but opaque scalar preference models or interpretable but expensive generative judges. There is a need for a reward modeling approach that retains high accuracy and interpretability while significantly reducing computational cost, especially for large-scale deployment in aligning language models to human preferences.

Method: The authors first observe that the log-probability margin between verdict tokens is highly correlated with prediction correctness and can serve as a low-cost confidence estimate. Based on this, they design CAMEL, a framework that initially performs a single-token preference decision. If the confidence (measured by the log-probability margin) is low, the system triggers a more expensive reflection process. To make the reflection genuinely corrective rather than superficial, they train the model using reinforcement learning with counterfactual prefix augmentation, where the model is exposed to varied, possibly incorrect initial verdict prefixes and is rewarded for revising them appropriately.

Result: CAMEL achieves state-of-the-art results on three standard reward-model benchmarks with an average accuracy of 82.9%, which is 3.2 percentage points higher than the previous best method. It matches or exceeds the performance of much larger 70B-parameter models while using only 14B parameters and demonstrates a superior accuracy-efficiency Pareto frontier compared to prior approaches.

Conclusion: By leveraging log-probability margins as a confidence signal and selectively invoking reflective judging, CAMEL effectively bridges the gap between scalar and generative reward models. The framework provides better accuracy at lower or comparable computational cost and scales more favorably, illustrating that confidence-gated reflection combined with RL-based self-correction is a powerful paradigm for reward modeling in language model alignment.

Abstract: Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.

</details>


### [17] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA is a new parameter-efficient fine-tuning (PEFT) method that reuses structured groups of pretrained weights to form shared low-rank adapters, reducing trainable parameters while improving or matching performance over LoRA and its variants, especially in multi-task settings.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA and its recent variants still require a substantial number of trainable parameters when applied to large language models at scale. Simply lowering the LoRA rank to reduce this overhead causes significant performance drops, particularly in complex, multi-task learning scenarios. The authors are motivated to design a PEFT method that preserves or improves task performance while substantially reducing trainable parameter counts, effectively breaking the trade-off between efficiency and capacity in LoRA-based fine-tuning.

Method: The authors propose ID-LoRA, a PEFT framework that analyzes the pretrained weight matrices of an LLM to extract clustered parameter groups, which reflect structural or functional similarities. These clustered groups are then used to construct multiple low-rank adaptation components. Crucially, all these components share a single initialized trainable low-rank matrix, rather than having separate low-rank matrices per component as in standard LoRA. This design reuses intrinsic structure in the base model and factors adaptation through shared low-rank parameters, reducing total trainable parameters while maintaining representational capacity.

Result: On five benchmarks—Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment—ID-LoRA consistently outperforms full fine-tuning and PEFT baselines such as LoRA, DoRA, and HydraLoRA. It achieves these gains while using up to 46% fewer trainable parameters than standard LoRA. In multi-task settings (e.g., combined Code and MMLU tasks), ID-LoRA surpasses LoRA and recent variants like DoRA and HydraLoRA while requiring only 54% of the trainable parameters used by conventional LoRA.

Conclusion: ID-LoRA demonstrates that it is possible to decouple parameter efficiency from performance degradation in PEFT for LLMs by leveraging structured reuse of pretrained weights. By clustering parameter groups and sharing a single low-rank trainable matrix across multiple components, ID-LoRA cuts trainable parameter counts substantially while improving or at least matching performance on diverse and challenging benchmarks. This suggests a promising direction for scaling parameter-efficient fine-tuning to ever-larger models and more complex multi-task scenarios without incurring prohibitive adaptation costs.

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [18] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: The paper proposes adaptive text anonymization via task-specific prompt optimization, enabling language models to automatically tailor anonymization strategies to different privacy-utility requirements and domains, outperforming static baselines across five benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing text anonymization methods use static, hand-crafted rules or strategies that cannot flexibly adjust to different privacy requirements, domains, or downstream tasks. This leads to suboptimal privacy-utility trade-offs and poor generalization when the context, goals, or data type changes.

Method: The authors formulate adaptive text anonymization as a new task where anonymization strategies are automatically tuned to target privacy-utility requirements. They develop a framework for task-specific prompt optimization that automatically generates and refines anonymization instructions for language models, conditioned on desired privacy goals, domain characteristics, and downstream usage. They implement this over open-source LMs and design an evaluation benchmark with five datasets covering varied domains and privacy/utility constraints.

Result: On the proposed benchmark, covering five datasets with diverse domains and privacy-utility configurations, the method consistently yields a better trade-off between privacy protection and utility preservation than existing static anonymization baselines. It achieves performance comparable to larger closed-source models while remaining computationally efficient and running on open-source LMs. The optimized prompts also reveal previously unseen anonymization strategies occupying new regions of the privacy-utility frontier.

Conclusion: Adaptive, prompt-based anonymization is an effective alternative to static, manually designed strategies. Automatically optimized anonymization instructions can flexibly adapt to domain- and task-specific privacy-utility requirements, providing better and more diverse trade-offs while maintaining efficiency and compatibility with open-source language models.

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [19] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: Proposes a lightweight text classification model that explicitly encodes sentence-level grammar into a compact vector and fuses it with frozen contextual embeddings, outperforming heavier deep models while being suitable for edge devices.


<details>
  <summary>Details</summary>
Motivation: Transformer-based NLP models require large computational resources and are not ideal for deployment in resource-constrained or edge environments. There is a need for models that can still capture deep grammatical and semantic properties of text without the overhead of full parameterized transformer architectures or extensive fine-tuning.

Method: Design a unified feature representation that combines (1) an explicit, hand-crafted grammar vector encoding sentence-level grammatical structure (syntactic composition, phrase patterns, complexity indicators) and (2) frozen contextual embeddings for semantic information. Fuse these heterogeneous features into a single representation and feed it into various downstream classifiers (DBN, LSTM, BiLSTM, BERT, XLNet) trained with different epoch settings, but without full transformer fine-tuning. Grammar is treated as an explicit inductive bias rather than a learnable module.

Result: The unified representation that merges grammar vectors with frozen contextual embeddings captures both structural and semantic aspects of the text, yielding 2%–15% performance improvements over baseline models across heterogeneous domains. The approach achieves competitive or superior accuracy with significantly lower computational cost than fully trained transformer-based architectures.

Conclusion: Encoding grammar as an explicit inductive bias and fusing it with frozen semantic embeddings provides a robust yet lightweight text classification framework. It outperforms heavier baseline models while being more suitable for edge deployment, and differs from prior syntax-aware transformers by avoiding added attention layers, tree encoders, or full fine-tuning, thereby offering an efficient alternative for resource-constrained NLP applications.

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [20] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: Proposes SibylSense, an inference-time learning method to adapt rubric-based rewards for RL in open-ended generation, making them more discriminative and robust.


<details>
  <summary>Details</summary>
Motivation: Reward design for RL post-training in open-ended text generation is hard: aligned, robust rewards are needed but rubrics are either expensive to craft, shallow and inconsistent when prompted, or prone to saturation, drift, and reward hacking when using fixed discriminative pools. There is a need for scalable, adaptive rubric construction that stays informative over time.

Method: Introduce SibylSense, which uses a frozen rubric generator plus a tunable memory bank of rubric items. The memory stores validated rubric items and is updated using verifier-based item rewards computed from discriminative gaps between reference and candidate answers on a small set of examples. Training alternates between (1) tuning the memory bank based on these verifier signals and (2) a rubric-adversarial policy update step that encourages the policy to produce answers that satisfy the current rubric, which in turn forces the rubric generator (via its memory) to evolve and capture new quality dimensions as discriminative gaps shrink.

Result: On two open-ended generation tasks, SibylSense produces rubrics that better distinguish answer quality (more discriminative rubrics) and these improved rubrics lead to better downstream reinforcement learning performance when compared to static and non-adaptive rubric/reward baselines.

Conclusion: Adaptive, inference-time learning over a memory-augmented rubric generator can overcome the limitations of static or prompt-only rubrics, yielding more informative supervision signals and improving RL training for open-ended generation. SibylSense demonstrates that alternately refining rubrics and policies helps avoid reward saturation and drift, enhancing robustness and alignment.

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [21] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: The paper proposes OP-GRPO, a reinforcement learning framework that enables a single LLM to generate multiple diverse, human-aligned perspectives (Overton Pluralism) from one query, and shows it can outperform larger or modular baselines.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods typically optimize models toward a single or narrow notion of the "best" answer, failing to reflect the fact that human values and viewpoints are plural and often legitimately divergent. Overton Pluralism has been suggested as a way to capture this diversity, but existing approaches often need explicit prompting or multiple models/modules, which is inefficient and brittle. There is a need for a scalable, implicit method that lets one LLM naturally produce a set of diverse, valid perspectives while maintaining quality and coverage of real human views.

Method: The authors design OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning approach that trains one LLM to output multiple distinct perspectives per query. The workflow has two main components: (1) Similarity estimator training: they fine-tune a Sentence Transformer specifically for Overton Pluralism tasks so it can evaluate how similar or different candidate responses are, providing a better measure of coverage over the space of perspectives. (2) OP-GRPO training: they integrate this similarity estimator into a dual-reward RL scheme. One reward encourages broad coverage of genuine human perspectives, and the other promotes the uniqueness and non-redundancy of each generated perspective in a response set. This is built on a group-based variant of policy optimization to jointly optimize multiple outputs from the same model for pluralism.

Result: Experiments show that a relatively small model, Qwen2.5-3B-Instruct, trained with OP-GRPO achieves better perspective coverage than stronger baselines. Quantitatively, it surpasses a 20B GPT-OSS baseline on a Natural Language Inference benchmark with a 37.4% relative accuracy gain, and it outperforms a modular Overton Pluralism architecture by 19.1% relative improvement. Additional evaluations using GPT-4.1 as an automatic judge support the claim that the approach yields robust, diverse, and high-quality pluralistic responses.

Conclusion: OP-GRPO enables implicit Overton Pluralism in a single LLM: the model can generate multiple diverse, human-aligned perspectives from one prompt without extra prompting tricks or modular systems. By combining a task-specific similarity estimator with a dual-reward RL objective, the method improves both diversity and coverage of viewpoints, and does so efficiently enough that a 3B-parameter model can outperform much larger or more complex baselines. This suggests a promising direction for scalable, pluralistic alignment of language models.

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [22] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: They propose a new tail-aware divergence for language model distillation that reduces the dominance of high-probability tokens and better leverages low-probability (tail) tokens, matching KL cost while improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: In standard language model distillation, KL divergence makes the training signal dominated by the teacher's highest-probability next tokens (modes). This down-weights the rest of the distribution, even though low-probability tokens can contain useful information about the teacher's uncertainty and structure. The authors want a distillation objective that uses this tail information more effectively, without increasing computational cost.

Method: They introduce a tail-aware, decoupled divergence objective. It separates the contribution of the teacher's top-K token probabilities from that of the lower-probability tokens (the tail). By reducing the influence of the high-mass modes and relatively up-weighting the tail, the student model is encouraged to learn more from the full teacher distribution. The divergence is designed to be computed with essentially the same complexity and computational profile as standard KL divergence used in distillation.

Result: Across multiple datasets and tasks, for both pre-training and supervised distillation of decoder-only models, the new divergence produces competitive or better performance than standard KL-based distillation. The method scales to large datasets and can be run efficiently with relatively modest (academic-level) compute resources.

Conclusion: Reweighting the teacher distribution during distillation via a tail-aware divergence improves the usage of low-probability tokens without extra computational cost. This leads to competitive performance in various distillation scenarios and makes large-scale distillation more accessible without industry-scale compute infrastructure.

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [23] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: They propose FinAnchor, a lightweight way to combine embeddings from multiple LLMs for financial prediction without fine‑tuning the LLMs, and show it beats strong baselines.


<details>
  <summary>Details</summary>
Motivation: Financial prediction from long financial documents is hard because useful signals are rare and buried in noise, and no single LLM’s embedding space is consistently best across tasks or time periods. There is a need for a robust way to exploit multiple heterogeneous embedding models without expensive fine‑tuning.

Method: They introduce FinAnchor, which chooses one LLM’s embedding space as an anchor, then learns simple linear mappings that project embeddings from other LLMs into this anchor space. After alignment, the model aggregates the mapped embeddings and the anchor embeddings into a single unified representation, which is then fed into downstream prediction models. The framework is lightweight and does not require updating the original LLMs.

Result: On several financial NLP tasks, the unified anchored representations produced by FinAnchor systematically outperform the best single‑LLM embedding baselines and standard ensemble strategies, demonstrating better robustness and predictive accuracy.

Conclusion: Aligning heterogeneous LLM embedding spaces into a common anchor space via linear mappings, then aggregating them, provides a simple yet effective way to improve financial prediction from long documents without fine‑tuning large models. Anchored representations can serve as a robust, general framework for combining multiple embedding models in financial NLP.

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [24] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: The paper presents Exa, a new 12,000-tweet Persian Twitter sentiment dataset labeled as positive, neutral, or negative, and evaluates it using ParsBERT and RoBERTa, achieving a macro F1 of 79.87%.


<details>
  <summary>Details</summary>
Motivation: Persian NLP, especially sentiment analysis on social media, lacks large, general-purpose datasets; existing ones are small and domain-specific (e.g., products, hotels) and do not capture colloquial, ironic language common on Twitter. A dedicated, well-annotated Persian Twitter sentiment dataset is needed to build and evaluate robust models.

Method: The authors collect 12,000 Persian tweets from Twitter and have them annotated into three sentiment classes (positive, neutral, negative) by five native Persian speakers. They then analyze dataset characteristics and train/evaluate pre-trained Persian language models (ParsBERT and RoBERTa variants) on this dataset to establish baseline performance, reporting macro F1 scores.

Result: They obtain a macro F1 score of 79.87% using ParsBERT/Roberta-based models on the new dataset, indicating solid performance across the three sentiment classes. They also provide descriptive statistics and characteristics of the dataset (e.g., class distribution, language traits), though details are not in the abstract.

Conclusion: The Exa Persian Twitter sentiment dataset is a valuable resource for Persian sentiment analysis, capturing real-world social media language and enabling effective training and evaluation of modern transformer-based models. The reported macro F1 shows that the dataset and chosen models are suitable baselines for future research and system development in Persian sentiment analysis.

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [25] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: The paper studies how to make LLM Chain-of-Thought reasoning shorter and cheaper without hurting accuracy, and extracts practical RL training recipes for efficient reasoning that generalize across model sizes and domains.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning improves LLM performance but greatly increases computation due to long reasoning traces. Existing "efficient reasoning" approaches use RL with reward shaping but their mechanics and best practices are not well understood, and evaluations are often coarse (e.g., single token budget or average length). The authors aim to systematically understand how RL-based efficient reasoning actually changes models, avoid failure modes like length collapse, and provide finer-grained evaluation criteria and training guidelines.

Method: They perform a systematic empirical study of RL-based efficient reasoning for LLMs under a unified experimental protocol. They propose finer evaluation metrics such as correctness-conditioned length distributions and performance across a wide range of token budgets (2k–32k). They analyze training dynamics and identify a two-stage process: (1) length adaptation and (2) reasoning refinement. They then ablate components including types of training prompts and rollouts, reward shaping schemes, and optimization strategies. A central methodological choice is to train primarily on easier prompts to maintain a high density of positive reward signals, which helps control length without collapsing reasoning. They then test whether the learned length bias transfers across domains and across model scales within the Qwen3 family (0.6B–30B).

Result: They find that RL-based efficient reasoning exhibits a two-stage training dynamic where models first adapt their output length and then refine reasoning quality. They show that naive setups can cause length collapse (overly short, low-quality reasoning), while training on relatively easier prompts with denser positive rewards stabilizes training and preserves accuracy with shorter CoT. The resulting models maintain strong performance across token budgets from 2k to 32k and the induced preference for shorter yet effective reasoning generalizes across domains and across Qwen3 model sizes from 0.6B to 30B.

Conclusion: Efficient reasoning in LLMs via RL is governed by distinct stages of length adaptation and reasoning refinement, and is highly sensitive to training design. Choosing easier prompts to ensure dense positive rewards is crucial to avoid length collapse while shortening CoT traces. Properly shaped RL can produce models that reason accurately with significantly reduced computation, and the learned length bias is robust and transferable across tasks and model scales. The authors summarize their empirical findings into practical guidelines for training efficient reasoning models and validate their applicability on the Qwen3 series.

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


### [26] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: The paper introduces Blackbird Language Matrices (BLM), a structured, multiple-choice language task inspired by intelligence tests, designed to probe large language models’ abilities in detecting linguistic objects, using systematic patterns, and distinguishing linguistic from reasoning errors.


<details>
  <summary>Details</summary>
Motivation: Current large language models show impressive performance but it is unclear how well they detect explicit linguistic objects and properties, and how they exploit systematic, cross-sentence patterns versus shallow cues. There is also a need for curated, explainable benchmarks that allow multi-faceted analysis of LLM behavior rather than just end-task scores. The authors aim to fill this gap with a task and dataset that have explicit internal structure, akin to IQ test matrices, enabling fine-grained probing of LLM abilities and error types.

Method: The authors design Blackbird Language Matrices as multiple-choice problems with hierarchical structure: relations within each sentence, across sequences of sentences, and within each candidate answer. They construct curated yet naturalistic datasets in multiple languages, then benchmark both simple baselines and more specialized models. They run targeted experiments to test chunking (do models form representations of grammatical objects and attributes?) and systematicity (do they detect and use regular patterns across sentences?). They analyze model representations to see whether relevant grammatical information is encoded and how it is used to solve the task.

Result: The experiments show that BLM tasks, though challenging, can be solved with reasonably good accuracy by simple baselines and even better by models tailored to the task, and this holds across more than one language. Representation analysis indicates that models’ internal states encode grammatical objects and attributes pertinent to solving the task. Performance patterns and experiments on systematicity suggest that models are indeed exploiting structured, cross-sentence regularities rather than only local or superficial cues.

Conclusion: Blackbird Language Matrices are a useful, structured benchmark for probing LLM linguistic abilities, particularly the detection of grammatical objects and systematic patterns across sentences. Their curated, hand-crafted yet naturalistic design, with clear learning contexts and expected answers, makes them well suited for explainability studies, allowing researchers to investigate not just how well LLMs perform but why they succeed or fail. The paper argues that such curated, structured datasets are an important complement to large-scale, less interpretable benchmarks in understanding LLM behavior.

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [27] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: The paper introduces PC-FOL, a new first-order logic dataset designed to test LLMs on case-based mathematical reasoning (e.g., proof by cases and contradiction), shows that LLMs perform much worse on these tasks than on standard linear reasoning, and provides a graphical-model-based theoretical explanation for this gap.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical reasoning benchmarks for LLLMs mostly test linear, step-by-step reasoning and underrepresent important proof styles such as proof by contradiction and proof by cases. This makes it hard to fully understand and diagnose the reasoning limitations of LLMs, especially for natural language proofs used in real mathematics. The authors aim to fill this gap and more precisely characterize where LLMs struggle in mathematical proof generation.

Method: 1) Construct a new first-order logic dataset, PC-FOL, focusing on case-based reasoning problems and proofs (including proof by cases and contradiction). Professional mathematicians annotate each problem with a complete natural-language proof. 2) Evaluate several state-of-the-art LLMs on PC-FOL and on standard linear reasoning datasets, comparing performance across reasoning types. 3) Develop a graphical-model-based theoretical framework to formalize linear vs case-based reasoning and analyze why case-based reasoning is more challenging for current LLMs, thereby explaining the observed performance gap.

Result: Empirical evaluations show that leading LLMs that perform reasonably well on linear reasoning tasks suffer a significant drop in accuracy on PC-FOL’s case-based reasoning problems. The experiments clearly reveal a sizable gap between linear and case-based reasoning capabilities. The graphical-model analysis provides a principled account of this discrepancy, linking it to structural properties of the reasoning processes and how LLMs handle branching reasoning paths.

Conclusion: Case-based reasoning, including proof by cases and contradiction, poses substantially greater difficulty for current LLMs than linear reasoning, and this limitation is not adequately captured by existing benchmarks. The PC-FOL dataset and the authors’ graphical-model analysis highlight key structural challenges in automated natural-language proof generation and offer a foundation and direction for future research on improving LLMs’ ability to handle non-linear, branching mathematical reasoning.

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [28] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: The paper proposes and evaluates a proactive risk-awareness framework for LLMs, focusing on whether models can anticipate and warn about latent ecological harms before they occur, using a new Butterfly dataset in the environmental domain.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety work largely centers on blocking or correcting clearly harmful or malicious content, but real-world deployments involve ordinary queries that can unintentionally cause serious ecological or environmental damage. There is limited understanding and measurement of whether LLMs can proactively recognize and flag such latent risks, especially in complex domains like ecology and environmental protection. The authors aim to fill this gap by defining and empirically studying proactive risk awareness.

Method: They design a proactive risk-awareness evaluation framework where LLMs are tested on their ability to foresee potential environmental or ecological harms and issue warnings in response to seemingly benign solution-seeking queries. To instantiate this, they build the Butterfly dataset: 1,094 queries about everyday tasks or decisions that could have hidden ecological impacts. They then benchmark five widely used LLMs on this dataset, systematically varying response length constraints, languages, and modalities (text vs multimodal) to see how these factors influence proactive awareness, especially for species protection scenarios.

Result: Across five major LLMs, the authors find that proactive ecological risk awareness significantly degrades when responses must be short, indicating that length constraints hinder risk reasoning and explanations. They observe that performance patterns are broadly similar across languages, suggesting cross-lingual consistency in both capabilities and weaknesses. However, they also identify stable blind spots, particularly in tasks related to species and biodiversity protection, which persist even for multimodal models that can see images as well as text.

Conclusion: Current LLM safety alignment does not sufficiently equip models to anticipate and warn about latent ecological harms that can arise from everyday queries. There is a marked gap between present safety mechanisms and the ecological responsibility needed in real-world deployments. The paper argues that LLM systems should incorporate proactive safeguards—such as explicit risk-awareness training and evaluation—to ensure models can recognize and communicate potential environmental and ecological consequences before harmful actions are taken.

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [29] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: Hybrid method uses LLMs to extract aspects from reviews and classic ML to classify sentiment at scale, applied to 4.7M restaurant reviews.


<details>
  <summary>Details</summary>
Motivation: Businesses and customers rely heavily on online reviews, but the sheer volume and unstructured nature of text make it difficult to systematically analyze them. LLMs are good at nuanced language understanding, yet applying them directly to millions of reviews is computationally expensive and hard to scale. There is a need for a scalable, accurate, and practical framework to perform aspect-based sentiment analysis on massive review datasets, especially in hospitality and similar service industries.

Method: First, use ChatGPT on a sample of restaurant reviews to identify and define key aspects of dining experiences (e.g., food, service, ambiance). Then, collect a set of human-labeled reviews to train classic machine-learning sentiment classifiers for each identified aspect. Apply these trained classifiers to a large corpus of 4.7 million reviews spanning 17 years from a major online platform. Finally, conduct regression analyses to test how well the machine-labeled aspect sentiments explain variance in overall restaurant ratings across aspects, cuisines, and regions.

Result: The machine-labeled aspect sentiments derived from the hybrid pipeline significantly explain variation in overall restaurant ratings. The approach scales to millions of reviews and works consistently across different dining aspects, types of cuisine, and geographic regions, indicating robustness and generalizability of the framework.

Conclusion: A hybrid framework that uses LLMs for aspect discovery and conventional machine-learning models for large-scale sentiment classification can effectively automate aspect-based sentiment analysis for massive review datasets. This approach balances the semantic strengths of LLMs with the scalability and efficiency of traditional ML, providing a practical and generalizable tool for researchers and practitioners in hospitality and potentially other service industries.

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [30] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: The paper proposes Prompt-Level Distillation (PLD), a way to transfer explicit reasoning patterns from a large teacher model into structured system-prompt instructions for a smaller student model, achieving chain-of-thought–level performance with low latency and high transparency.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting yields strong reasoning but is slow and expensive at inference time, while fine-tuning smaller models can be opaque and operationally heavy. There is a need for a method that preserves advanced reasoning performance and interpretability without incurring high latency, cost, or complex deployment, particularly for regulated and resource-constrained settings.

Method: Use a large Teacher model to generate explicit reasoning patterns, then manually or systematically extract and organize these into a concise, structured list of expressive instructions placed in the Student model’s system prompt. The student is not fine-tuned; instead, it is guided at prompt level using these distilled reasoning strategies. The approach is evaluated on reasoning-heavy benchmarks (StereoSet and Contract-NLI) using the Gemma-3 4B model, comparing standard prompting vs. PLD-enhanced system prompts.

Result: On StereoSet, Macro F1 improves from 57% to 90.0%, and on Contract-NLI from 67% to 83% when using PLD with Gemma-3 4B. This brings the small model’s performance close to that of frontier models while adding negligible inference latency, since the instructions live in the static system prompt rather than requiring long chain-of-thought generations at test time.

Conclusion: Prompt-Level Distillation effectively transfers high-quality reasoning strategies from a large teacher to a small student via structured system-prompt instructions, yielding large gains in reasoning benchmarks with minimal latency and cost. Because the distilled instructions are explicit and human-auditable, the method improves transparency and is particularly suitable for regulated, high-volume, and edge-deployment contexts where both performance and explainability are critical.

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [31] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: The paper presents PVminer, a domain-adapted NLP framework for automatically extracting detailed patient voice categories from secure patient-provider messages using specialized BERT models plus topic modeling, achieving higher F1 scores than existing biomedical/clinical baselines.


<details>
  <summary>Details</summary>
Motivation: Patient-generated texts (secure messages, surveys, interviews) contain rich information about how patients communicate and about social determinants of health, but current manual qualitative coding is slow and does not scale, and existing ML/NLP methods either separate patient-centered communication from SDoH or use models not well tuned to patient-facing language. There is a need for a scalable, accurate, and domain-adapted method to structure patient voice across large health systems.

Method: The authors design PVminer, an NLP framework that formulates patient voice detection as a hierarchical multi-label, multi-class classification problem. They introduce patient-specific BERT encoders (PV-BERT-base and PV-BERT-large) adapted to patient-facing language, integrate unsupervised topic modeling to create topic representations (PV-Topic-BERT), and build fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic features and author identity are incorporated during fine-tuning and inference to enrich the semantic representation and improve classification.

Result: PVminer achieves strong performance on hierarchical labeling tasks, with F1 scores of 82.25% for Code-level, 80.14% for Subcode-level, and up to 77.87% for Combo-level labels, outperforming biomedical and clinical pre-trained baseline models. An ablation study shows that both inclusion of author identity and topic-based augmentation each produce meaningful performance improvements, confirming the value of these design choices.

Conclusion: PVminer effectively structures patient voice in secure patient-provider communication by combining domain-adapted BERT models with topic-based augmentation and hierarchical multi-label classification, outperforming standard biomedical/clinical baselines. The framework demonstrates that modeling author identity and thematic topics enhances detection of nuanced patient voice categories. The authors will publicly release pre-trained models, code, and documentation, and will share annotated datasets upon request, enabling broader research and application in patient-centered communication and SDoH analysis.

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>


### [32] [On Data Engineering for Scaling LLM Terminal Capabilities](https://arxiv.org/abs/2602.21193)
*Renjie Pi,Grace Lam,Mohammad Shoeybi,Pooya Jannaty,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: The paper introduces a synthetic data pipeline and dataset for training terminal (command-line) agents, and shows that models trained on this data significantly improve on a standard benchmark, matching much larger models.


<details>
  <summary>Details</summary>
Motivation: While large language models have become strong terminal agents, the data and training strategies that enable this performance are mostly undisclosed. This lack of transparency hinders reproducibility, systematic study, and further progress. The authors aim to open up this area by providing an explicit, analyzable data engineering pipeline and corresponding open-source resources for training terminal-focused models.

Method: They design Terminal-Task-Gen, a lightweight synthetic task generation pipeline that can construct tasks based on seeds (specific initial conditions) and skills (capabilities the agent must exercise). Using it, they build Terminal-Corpus, a large-scale dataset of terminal tasks. They then explore different data and training strategies—such as filtering, curriculum learning, long-context training, and scaling laws—while training a family of Nemotron-Terminal models initialized from Qwen3 models of various sizes (8B, 14B, 32B).

Result: Training on Terminal-Corpus with their curated strategies yields substantial performance gains on Terminal-Bench 2.0. For example, Nemotron-Terminal-8B improves from 2.5% to 13.0%, 14B from 4.0% to 20.2%, and 32B from 3.4% to 27.4%. The 32B model matches the performance of much larger models, demonstrating efficient scaling via targeted data engineering rather than just increasing model size.

Conclusion: Carefully engineered synthetic data and training strategies can dramatically improve terminal agent performance without requiring larger models. The released Terminal-Task-Gen pipeline, Terminal-Corpus dataset, and Nemotron-Terminal checkpoints provide an open, reproducible foundation for future research on command-line agents and data-centric training methodologies.

Abstract: Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.

</details>


### [33] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: The paper proposes JurisMMA, a staged framework for legal judgment prediction, and JurisMM, a large multimodal Chinese legal case dataset, showing improved performance and broader applicability to legal AI tasks.


<details>
  <summary>Details</summary>
Motivation: Legal Judgment Prediction is important for supporting legal systems, but existing approaches based on statistics or role-playing struggle with complex, multi-allegation cases, heterogeneous evidence, and generalization. There is also a lack of large, modern multimodal datasets to evaluate such systems comprehensively.

Method: The authors design JurisMMA, a framework that decomposes the legal trial and judgment process into standardized, organized stages, likely mirroring real judicial workflows. They also construct JurisMM, a dataset of 100,000+ recent Chinese judicial records that includes textual materials and associated video-text data, enabling both unimodal and multimodal LJP experiments. They evaluate their framework on JurisMM and on the existing LawBench benchmark.

Result: Experiments show that models using JurisMMA achieve better performance on LJP tasks compared with baselines, on both the new JurisMM dataset and the LawBench benchmark. The framework generalizes across different legal tasks and modalities, demonstrating robustness and adaptability.

Conclusion: The staged JurisMMA framework is an effective way to structure legal judgment prediction, especially in complex, multimodal settings. Combined with the JurisMM dataset, it enables more comprehensive evaluation and development of legal AI systems, and the approach is promising not only for LJP but for broader legal NLP and multimodal applications.

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: The paper compares statistical, machine-learning, and deep-learning models to predict overweight/obesity in U.S. adolescents using multilevel behavioral, household, and neighborhood factors, finding only modest benefits of complex models over logistic regression and persistent performance disparities across subgroups.


<details>
  <summary>Details</summary>
Motivation: Child and adolescent obesity in the U.S. is a persistent public health issue influenced by multiple behavioral, familial, and community-level determinants. While many predictors are known, how they jointly predict obesity at the population level—and whether advanced machine-learning or deep-learning models offer meaningful advantages over traditional methods—remains unclear. There is also concern about fairness and performance disparities across demographic and socioeconomic subgroups, which this study seeks to quantify.

Method: Using 2021 National Survey of Children’s Health data on 18,792 adolescents aged 10–17, the study defines overweight/obesity via BMI categories and uses a broad set of predictors, including diet, physical activity, sleep, parental stress, socioeconomic status, adverse childhood experiences, and neighborhood characteristics. It trains and compares multiple models—logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Model performance is assessed on discrimination (AUC), accuracy, precision, recall, F1 score, and calibration (Brier score), and equity is evaluated by comparing performance across race and poverty subgroups.

Result: All models achieve moderate discrimination (AUC 0.66–0.79). Logistic regression, gradient boosting, and multilayer perceptron provide the best balance of discrimination and calibration. Boosting and deep-learning models offer only modest improvements in recall and F1, and no algorithm is uniformly superior across all metrics. Notably, performance disparities across racial and poverty groups are present for all models and are not resolved by using more complex algorithms.

Conclusion: Complex machine-learning and deep-learning algorithms provide only limited performance gains over logistic regression in predicting adolescent overweight/obesity from multilevel predictors. Key predictors consistently arise from behavioral, household, and neighborhood domains, underscoring the multifactorial nature of obesity risk. Persistent subgroup disparities in model performance suggest that enhancing data quality and implementing equity-focused surveillance strategies are more critical than increasing algorithmic complexity to advance fair and effective obesity risk prediction.

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [35] [An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models](https://arxiv.org/abs/2602.20324)
*Cathy Shyr,Yan Hu,Rory J. Tinker,Thomas A. Cassini,Kevin W. Byram,Rizwan Hamid,Daniel V. Fabbri,Adam Wright,Josh F. Peterson,Lisa Bastarache,Hua Xu*

Main category: cs.AI

TL;DR: The paper introduces RARE-PHENIX, an end-to-end AI framework that automates the full workflow of rare disease phenotyping from clinical notes and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Rare disease diagnosis relies heavily on precise phenotyping, typically captured as standardized HPO terms. However, manually extracting and structuring phenotypes from free-text clinical notes is time-consuming, requires specialized expertise, and is difficult to scale across large health systems. Existing AI methods often handle only parts of this process—such as named entity recognition or ontology mapping—without reflecting the complete clinical workflow clinicians actually use. This gap limits the clinical usefulness and diagnostic impact of current tools, motivating a system that can mirror and support the real-world, end-to-end phenotyping process.

Method: The authors developed RARE-PHENIX, an integrated pipeline with three main modules: (1) phenotype extraction from clinical notes using large language models; (2) standardization of extracted phrases into Human Phenotype Ontology (HPO) terms using ontology-grounded methods; and (3) supervised ranking to prioritize phenotypes that are most diagnostically informative. They trained the system on data from 2,671 patients from 11 Undiagnosed Diseases Network sites and evaluated it externally on 16,357 clinical notes from Vanderbilt University Medical Center. Performance was compared against a strong deep learning baseline (PhenoBERT) using ontology-based similarity and precision–recall–F1 metrics in an end-to-end setting. Ablation studies examined the contribution of each module by progressively adding extraction, standardization, and prioritization.

Result: RARE-PHENIX surpassed the state-of-the-art baseline PhenoBERT in all reported end-to-end evaluation metrics, including a higher ontology-based similarity score to clinician-curated HPO terms (0.70 vs. 0.58), alongside better precision, recall, and F1. The external validation on over sixteen thousand clinical notes showed that the framework generalizes to real-world data. Ablation analyses revealed that each component—extraction, standardization, and prioritization—incrementally improved performance, confirming the benefit of modeling the complete clinical phenotyping workflow rather than treating phenotyping as a single-step task.

Conclusion: By implementing phenotyping as a clinically aligned, multi-stage workflow, RARE-PHENIX produces structured and ranked HPO phenotypes that more closely match expert clinician curation than existing approaches. This design choice leads to superior performance and better clinical relevance. The framework shows promise as a human-in-the-loop tool to support rare disease diagnosis in real clinical environments, potentially reducing manual workload while maintaining or improving diagnostic quality.

Abstract: Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.

</details>


### [36] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD is a two-phase causal discovery framework that uses LLMs to draft a causal graph from variable metadata and then statistically validates and refines it on data, achieving strong performance on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Causal discovery from observational data is challenging, especially when relying purely on statistical signals, which can be weak or ambiguous. Many real-world domains, however, have rich semantic metadata about variables that is usually ignored by traditional algorithms. The paper aims to exploit this metadata using large language models to guide causal structure learning, improving accuracy and practicality over purely data-driven methods.

Method: The method, DMCD, has two phases. In Phase I, a large language model reads variable metadata and outputs a sparse draft directed acyclic graph (DAG) that encodes hypothesized causal relations, acting as a semantic prior over graph space. In Phase II, this draft graph is checked and refined through conditional independence testing on observational data. Detected inconsistencies between the draft and the data drive targeted edge additions, removals, or reversals, resulting in a statistically validated causal graph.

Result: On three real-world benchmarks with rich metadata from industrial engineering, environmental monitoring, and IT systems, DMCD shows competitive or superior performance compared to various causal discovery baselines. The gains are particularly notable in recall and F1 score. Probing and ablation studies indicate that the performance improvements are due to semantic reasoning over metadata by the LLM, rather than memorization of the benchmark causal graphs.

Conclusion: Integrating LLM-based semantic priors over causal structures with rigorous statistical validation leads to a high-performing and practically useful framework for causal discovery. The work demonstrates that metadata-informed semantic reasoning can substantially enhance causal structure learning beyond what purely data-driven methods achieve.

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [37] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: Proposes DMEMM, a diffusion-based planner for offline RL that explicitly models environment dynamics and rewards to generate coherent trajectories and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based trajectory planners in offline RL treat trajectories largely as generic sequences and ignore the strict consistency constraints imposed by environment transition dynamics and reward structure. This mismatch causes generated trajectories to be unrealistic or inconsistent with how real environments evolve, undermining planning quality.

Method: Introduce Diffusion Modulation via Environment Mechanism Modeling (DMEMM), which augments diffusion model training with explicit reinforcement learning environment mechanisms, especially transition dynamics and reward functions. The diffusion process is modulated so that sampled trajectories must respect these modeled mechanisms, enforcing coherence between consecutive transitions during trajectory generation.

Result: When evaluated on offline reinforcement learning planning benchmarks, DMEMM outperforms prior diffusion-based and non-diffusion offline RL planners, achieving state-of-the-art performance metrics across tasks.

Conclusion: Embedding environment mechanisms—transition dynamics and rewards—directly into diffusion-based trajectory generation yields more coherent, environment-consistent trajectories, which translates into superior planning performance in offline RL settings compared to existing approaches.

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [38] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: The paper introduces Implicit Intelligence, a benchmark and framework to evaluate whether AI agents can infer and satisfy unstated constraints and goals, going beyond literal prompt-following, and shows current models perform poorly on these realistic, context-heavy tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world human requests to AI agents are often underspecified and depend on shared context, norms, and implicit constraints (e.g., accessibility, privacy, safety). Existing benchmarks largely test explicit instruction-following and do not adequately measure an agent’s ability to infer and respect these implicit requirements. The authors aim to fill this gap by creating a benchmark that captures the hidden complexity and contextual reasoning needed for robust, real-world agent behavior.

Method: The authors propose Implicit Intelligence, an evaluation framework that focuses on agents’ ability to uncover and satisfy implicit requirements embedded in tasks. They build Agent-as-a-World (AaW), a harness where interactive environments are specified in human-readable YAML and then simulated by language models. Within these environments, user requests appear simple on the surface, but correct completion requires agents to explore the environment and infer hidden constraints about accessibility, privacy, catastrophic risk, and other contextual factors. They construct 205 such scenarios and evaluate 16 state-of-the-art and open-weight language models/agents on them, measuring scenario pass rates.

Result: Across 205 scenarios, none of the 16 evaluated models performs robustly; even the best model reaches only a 48.3% scenario pass rate. This indicates that current systems often fail to discover or respect implicit constraints and cannot reliably transition from literal instruction-following to deeper contextual reasoning about goals and safety-relevant boundaries.

Conclusion: The study concludes that there is a large performance gap between present-day AI agents and the kind of implicit, context-sensitive understanding required for real-world deployment. Implicit Intelligence and the Agent-as-a-World harness provide a more realistic and challenging benchmark for this capability, highlighting the need for new methods and training strategies that help models infer unstated constraints, explore environments effectively, and act as genuine goal-fulfilling agents rather than mere prompt followers.

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [39] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: The paper introduces Trace-Free+, a curriculum learning framework to optimize tool interfaces for LLM-based agents without relying solely on execution traces, improving performance and generalization, especially with large tool sets.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents depend heavily on the quality of tool interfaces, such as natural language descriptions and parameter schemas. Existing interfaces are human-oriented and can bottleneck performance when agents must choose from many tools. Prior methods to improve tool interfaces often require execution traces, which are unavailable in cold-start or privacy-sensitive settings, and they optimize tools independently, limiting scalability and generalization to unseen tools. The paper aims to provide a scalable, trace-efficient way to improve tool interfaces so agents can use tools more effectively.

Method: The authors propose Trace-Free+, a curriculum learning framework that gradually shifts supervision from settings rich in execution traces to trace-free deployment scenarios. The framework encourages the model to learn abstract, reusable patterns for using tool interfaces and predicting tool usage outcomes. To enable this, they build a large-scale dataset of high-quality tool interfaces via a structured workflow, covering a diverse set of tools. The optimized interfaces are then used with LLM-based agents and evaluated on standard benchmarks.

Result: On the StableToolBench and RestBench benchmarks, Trace-Free+ yields consistent performance improvements on previously unseen tools, shows strong cross-domain generalization, and maintains robustness when the number of candidate tools increases to over 100. These results indicate that optimizing tool interfaces significantly boosts agent performance, especially in complex tool selection scenarios, without requiring extensive execution traces during deployment.

Conclusion: Tool interface optimization via the Trace-Free+ curriculum is an effective, scalable complement to agent fine-tuning. By learning reusable interface-usage patterns and reducing dependence on execution traces, the method improves LLM agents’ ability to select and use tools, generalizes well to new tools and domains, and remains robust as tool sets grow large, making it practical for real-world deployment.

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [40] [PreScience: A Benchmark for Forecasting Scientific Contributions](https://arxiv.org/abs/2602.20459)
*Anirudh Ajith,Amanpreet Singh,Jay DeYoung,Nadav Kunievsky,Austin C. Kozlowski,Oyvind Tafjord,James Evans,Daniel S. Weld,Tom Hope,Doug Downey*

Main category: cs.AI

TL;DR: The paper presents PreScience, a benchmark and dataset to test whether AI models can forecast future scientific advances using historical scientific records, showing that current models perform moderately and generate less diverse and novel science than humans.


<details>
  <summary>Details</summary>
Motivation: To determine if AI systems trained only on past scientific literature can meaningfully predict future research directions, collaborators, and impactful contributions, which would support better planning, collaboration, and prioritization in scientific research.

Method: The authors build PreScience, a benchmark based on 98K recent AI-related papers with structured metadata, author disambiguation, temporal alignment, and extended citation and publication graphs over 502K papers. They decompose scientific forecasting into four generative tasks (collaborator prediction, prior work selection, contribution generation, impact prediction), and design baselines plus a new LLM-based contribution similarity metric (LACERScore) to evaluate model performance on these tasks and in an end-to-end 12‑month simulation of scientific production.

Result: LACERScore correlates better with human judgments and approximates inter-annotator agreement more closely than previous similarity metrics. Frontier LLMs show only moderate success on forecasting tasks; for example, contribution generation by GPT‑5 attains an average similarity of just 5.6/10 to actual future contributions. In a 12‑month simulated corpus of AI research produced by composing all four tasks, the AI-generated papers are systematically less diverse and less novel than human-authored papers from the same time period.

Conclusion: Current AI systems, even frontier LLMs, have limited ability to forecast scientific progress when constrained to historical records, and tend to produce research that is less diverse and novel than real future work. PreScience and LACERScore provide a structured benchmark and improved evaluation tools to measure progress on scientific forecasting and highlight substantial remaining headroom for AI models in this domain.

Abstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.

</details>


### [41] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: They propose a new time-series reasoning task that fuses numerical patterns with semantic context, and introduce a two-stage RL-trained model (KairosVL) that improves temporal reasoning and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional time series analysis mainly focuses on numerical patterns and forecasting, which struggles with complex, decision-oriented tasks that require understanding contextual and semantic information. There is a growing need for models that can reason about time series in a way closer to human decision-making, especially for real-world applications demanding richer temporal intelligence.

Method: They define a new Semantic-Conditional Time Series Reasoning task that augments standard time-series analysis with contextual and semantic conditions. To solve it, they build KairosVL, trained using a two-round reinforcement learning framework: Round 1 trains perception of basic temporal primitives (e.g., order, trends, periodicity), and Round 2 focuses on higher-level, semantics-conditioned reasoning over time series. They validate with synthetic and real-world datasets and perform ablation studies on the training framework.

Result: KairosVL attains competitive performance on both synthetic benchmarks and real-world time series reasoning tasks. The experiments show that the two-stage RL framework not only improves task performance but also maintains core reasoning skills and substantially enhances generalization to unseen scenarios.

Conclusion: Integrating semantic reasoning with temporal modeling yields more capable and generalizable time series reasoning systems. Their two-stage RL framework provides a practical approach for building such models, suggesting a promising direction for real-world, decision-oriented time series intelligence applications.

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [42] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: The paper introduces ActionEngine, a training-free, two-agent framework that plans and executes GUI tasks programmatically using a learned state-machine memory of interfaces, greatly improving success rate, cost, and latency over vision-only step-by-step agents.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents depend on repeated calls to large vision-language models at every interaction step. This leads to high computation cost and latency proportional to the number of steps, and reduced accuracy because the agent has no persistent memory of the GUI states it has already visited. The authors aim to create a more scalable, efficient, and reliable way for agents to interact with GUIs, especially for complex, multi-step web tasks.

Method: The authors design ActionEngine, a training-free framework with two cooperating agents. A Crawling Agent performs offline exploration of GUIs and builds an updatable state-machine memory that captures pages (nodes) and actions (transitions) as reusable templates. An Execution Agent uses this memory online to synthesize complete, executable Python programs that plan the entire task upfront instead of acting step-by-step. When an action fails because the GUI has changed, a vision-based re-grounding module is invoked to repair the specific failed step, update the memory, and reattempt execution. The overall system combines global program planning with local validation and repair.

Result: On Reddit tasks from the WebArena benchmark, ActionEngine reaches a 95% task success rate while usually requiring only a single LLM call per task. This outperforms the strongest vision-only baseline, which attains 66% success, and simultaneously cuts cost by 11.8x and end-to-end latency by 2x. These results indicate that pre-built GUI memory and program synthesis provide both better reliability and efficiency than purely reactive, per-step VLM control.

Conclusion: The paper concludes that representing GUIs as an updatable state-machine memory and using that representation to synthesize complete execution programs yields scalable and robust GUI agents. By decoupling offline exploration from online execution, and incorporating a fallback mechanism for re-grounding and memory repair, ActionEngine overcomes key limitations of step-wise vision-only agents in both performance and efficiency, pointing toward a new paradigm of programmatic planning for GUI interaction.

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [43] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC is a framework that injects language-based “inner speech” into imitation learning, using VLMs, a conditional VAE, and diffusion policies to produce diverse, human-like and steerable behaviors without extra demonstrations.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning methods have difficulty modeling the richness, diversity, and history-dependent (non-Markovian) aspects of human behavior, and they generally do not allow explicit, fine-grained control of an agent’s behavior at inference time. The authors want an agent that not only mimics human demonstrations faithfully but can also be flexibly guided or steered to different behavioral styles or goals, akin to how human inner speech shapes our actions.

Method: They introduce MIMIC, which treats language as an internal representation of behavioral intent, inspired by human inner speech. First, a vision-language model is used as linguistic scaffolding to help train a conditional variational autoencoder (cVAE) that maps observations to “inner speech” descriptions capturing latent intent. Then, a diffusion-based behavior cloning policy is trained to output actions conditioned both on current observations and on this generated inner speech. At inference, the agent can either use its own generated inner speech or be conditioned on externally supplied, behavior-specific speech to steer what it does, without retraining. The overall pipeline integrates VLMs, a cVAE latent language space, and diffusion policies into a unified imitation learning framework.

Result: Across robotic manipulation tasks and human-AI collaboration games, MIMIC produces agents whose behaviors more closely match the diversity and nuances of human demonstrations while preserving or improving task performance. The learned inner-speech space allows behavior to be modulated by swapping or editing the conditioning language, resulting in controllable, fine-grained variations in behavior style or strategy. This steering is achieved without collecting new demonstrations, indicating better generalization and control compared to standard imitation learning baselines.

Conclusion: Using language as an internal, intent-like representation significantly improves both the expressiveness and controllability of imitation-learned agents. By leveraging VLMs, a cVAE over inner speech, and diffusion-based policies, MIMIC bridges low-level action execution with high-level, human-interpretable descriptions of behavior. This design yields more diverse and faithful human-like behaviors and supports nuanced behavior steering at test time without additional training data. The authors release code, pre-trained agents, and demos to facilitate adoption and further research.

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [44] [From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production](https://arxiv.org/abs/2602.20558)
*Yucheng Shi,Ying Li,Yu Wang,Yesu Feng,Arjun Rao,Rein Houthooft,Shradha Sehgal,Jin Wang,Hao Zhen,Ninghao Liu,Linas Baltrunas*

Main category: cs.AI

TL;DR: The paper introduces a learned verbalization agent that converts structured user interaction logs into optimized natural language prompts for LLM-based recommender systems, significantly improving recommendation accuracy over template-based text generation.


<details>
  <summary>Details</summary>
Motivation: While LLMs are increasingly used as the core of generative recommender systems, the step of translating structured interaction logs (clicks, views, metadata) into natural language prompts is underexplored. Existing template-based concatenation of fields is rigid, noisy, and often misaligned with what LLMs need for accurate recommendations. The authors aim to systematically optimize this verbalization process rather than hand-crafting templates.

Method: They propose a data-centric framework with a verbalization agent trained via reinforcement learning. The agent takes raw user interaction histories and possible metadata as input, and outputs textual descriptions (contexts) fed to an LLM recommender. Recommendation performance (e.g., accuracy on discovery item prediction) serves as the reward. Through RL, the agent learns policies that filter irrelevant interactions, select and reorganize key information, incorporate helpful metadata, and generate prompts that best support downstream LLM recommendation.

Result: On a large-scale industrial streaming dataset, prompts produced by the learned verbalization agent substantially outperform those from manually designed templates. The paper reports up to 93% relative improvement in discovery item recommendation accuracy, indicating that optimized language representations of user histories are highly beneficial for LLM-based recommenders.

Conclusion: The verbalization step is crucial for leveraging LLMs in recommender systems, and should be learned rather than manually templated. A reinforcement learning agent can automatically discover effective strategies such as interest summarization, noise removal, and syntax normalization, leading to large gains in recommendation quality. The findings highlight the importance of data-centric prompt construction for LLM-based recommendation and provide interpretable patterns for designing better context representations.

Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.

</details>


### [45] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: The paper presents CausalReasoningBenchmark, a benchmark that separates and evaluates the identification and estimation stages of causal inference across many real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing causal inference benchmarks typically evaluate systems using a single numeric quantity like the Average Treatment Effect, which mixes up two conceptually distinct tasks: (1) identification—formulating a valid causal research design under explicit assumptions, and (2) estimation—numerically implementing that design on data. This conflation makes it impossible to tell whether a system fails because it reasons incorrectly about causal structure or because it performs poor numerical estimation. There is a need for a benchmark that isolates and diagnoses these two sources of error on realistic problems.

Method: The authors build a benchmark, CausalReasoningBenchmark, consisting of 173 causal queries drawn from 138 real-world datasets. These are curated from 85 peer-reviewed papers and four standard causal-inference textbooks. For each query, systems must output (i) a structured identification specification including the causal strategy, treatment, outcome, control variables, and design-specific elements, and (ii) a point estimate with a standard error. The benchmark scores these two components separately to distinguish reasoning from computation. They evaluate a state-of-the-art large language model as a baseline, measuring correctness of high-level strategy selection and full identification specification.

Result: The baseline LLM correctly identifies the high-level causal strategy in 84% of the benchmark queries. However, when evaluating the complete identification specification—including all required design details—performance drops to 30% correctness. This indicates that the model is relatively good at selecting the general method but struggles with specifying the full, precise causal design. The benchmark itself is implemented and released on Hugging Face for public use.

Conclusion: CausalReasoningBenchmark reveals that the key bottleneck for automated causal inference lies in detailed causal reasoning and research-design specification rather than in numerical estimation. By separately scoring identification and estimation, the benchmark enables fine-grained diagnosis of system weaknesses. Its public release is intended to drive progress toward more robust, design-aware causal inference systems and to encourage methods that can handle the nuanced requirements of real-world causal analysis.

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [46] [Physics-based phenomenological characterization of cross-modal bias in multimodal models](https://arxiv.org/abs/2602.20624)
*Hyeongmo Kim,Sohyun Kang,Yerin Choi,Seungyeon Ji,Junhyuk Woo,Hyunsuk Chung,Soyeon Caren Han,Kyungreem Han*

Main category: cs.AI

TL;DR: The paper proposes a phenomenological, physics-inspired way to explain and audit algorithmic fairness in multimodal large language models (MLLMs), showing that multimodal inputs can systematically reinforce, not reduce, modality dominance and bias.


<details>
  <summary>Details</summary>
Motivation: Existing work on algorithmic fairness in AI largely focuses on symbolic, representation-level, or metaphysical accounts and tends to analyze models via embeddings or output statistics. With the rapid progress of MLLMs, there are new, subtle forms of cross-modal bias that arise from complex multimodal interaction dynamics and are not well captured by existing fairness frameworks or explainability methods. The authors want to give AI researchers a new explanatory lens—rooted in phenomenology and the physical dynamics of model operation—to better understand and address these fairness issues.

Method: The authors advance a position grounded in phenomenological explainable AI: instead of only using symbolic or high-level cognitive concepts, they look at the physical entities and dynamics that a model “experiences” during training and inference. They construct a surrogate physics-based model of transformer dynamics, representing semantic network structure and self-/cross-attention as a dynamical system. Then they carry out multi-input diagnostic experiments. First, they perform perturbation-based analyses on emotion classification tasks using two different MLLMs (Qwen2.5-Omni and Gemma 3n), systematically perturbing labels to observe structured changes in model errors. Second, they analyze Lorenz chaotic time-series prediction behavior via the physical surrogate model, to study attractors and dynamical properties that correspond to cross-modal bias patterns.

Result: Across two architecturally distinct MLLMs, the experiments reveal that multimodal inputs do not necessarily balance or reduce the dominance of any single modality. Instead, the interaction dynamics can reinforce modality dominance, leading to systematic, structured error patterns. Under controlled label perturbations in emotion classification, they observe error-attractor structures that indicate stable, biased response regimes. The surrogate dynamical analysis on Lorenz time series corroborates these findings by showing how the underlying transformer-like dynamics can settle into attractors that correspond to biased cross-modal behaviors, which would be missed by simple embedding-level analysis.

Conclusion: The paper concludes that a phenomenological, physics-based dynamical perspective is both conceptually and practically valuable for studying algorithmic fairness in MLLMs. By focusing on the model’s physical interaction dynamics—particularly attention and cross-modal coupling—researchers can detect subtle, systematic biases such as reinforced modality dominance that are invisible to standard representation-level analyses. This phenomenological doctrine, instantiated via surrogate dynamical models of transformers and multi-input diagnostics, offers a promising path for more accurate auditing and potentially mitigating unfairness in multimodal systems.

Abstract: The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as "treat like cases as like") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.

</details>


### [47] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: The paper analyzes how misaligned AIs might collude to evade “untrusted monitoring” schemes and proposes a more complete framework for testing and arguing about their safety before deployment.


<details>
  <summary>Details</summary>
Motivation: As AI systems gain autonomy, a misaligned AI could cause catastrophic harm. One proposed safeguard is untrusted monitoring: using one AI to watch another. However, we cannot safely deploy truly misaligned systems to empirically test such schemes, and existing theoretical work assumes overly narrow forms of collusion. There is a need for a broader, more realistic account of how colluding AIs could recognize each other and coordinate to bypass monitoring, so that pre-deployment testing and safety arguments are more trustworthy.

Method: The authors review and extend prior AI control and untrusted monitoring frameworks. They systematically relax prior assumptions about collusion and construct a taxonomy of potential collusion strategies: (1) passive self-recognition (models recognizing each other through behavior or internal properties without explicit signaling), (2) causal collusion via pre-shared explicit signals, (3) acausal collusion based on shared Schelling points or coordination without direct causal communication, and (4) combined or hybrid strategies. They then build a structured safety case sketch that makes explicit the assumptions, argument steps, and open problems needed to justify an untrusted monitoring deployment in light of these strategies.

Result: The paper provides: (a) a detailed taxonomy of collusion modes for AI systems under untrusted monitoring, (b) arguments showing how these modes could subvert monitoring protocols that had been considered safe under narrower assumptions, and (c) identification of parameter regimes and system properties where passive self-recognition becomes a particularly concerning and effective collusion mechanism. The safety case sketch clarifies which assumptions must hold for monitoring schemes to be trustworthy and where current evidence is insufficient.

Conclusion: The authors conclude that current approaches to justifying untrusted monitoring are incomplete because they overlook several realistic collusion strategies, especially passive self-recognition and more complex hybrids. Their expanded taxonomy and safety-case framework help make safety arguments more rigorous and highlight concrete open challenges. This work is a step toward more robust, assumption-transparent evaluations of untrusted monitoring, but significant research is still needed to fully secure such schemes against sophisticated misaligned AIs.

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [48] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: Paper studies how to jointly learn two people’s preference models when answers are anonymized.


<details>
  <summary>Details</summary>
Motivation: In many decision-support systems, we need to learn users’ preferences by asking questions. Sometimes multiple decision-makers answer the same queries, but their identities are hidden or mixed. Existing elicitation methods typically assume we know which answer comes from which person, or handle only one decision-maker at a time. This paper addresses how to recover each individual’s preference model from mixed, unlabeled answers.

Method: Assume each decision-maker’s preferences can be represented by an additive value function whose marginal value functions are piecewise linear with known breakpoints. At each query, two alternatives are presented and both decision-makers respond, but the two binary answers arrive without labels, so we only know the multiset of responses. The authors design an elicitation algorithm that selects queries and uses the known structure (additive, piecewise-linear, shared breakpoints) to progressively infer which responses must belong to which decision-maker and to identify their respective value functions. The procedure exploits logical consistency and geometric properties of the value functions to disambiguate and estimate the parameters.

Result: They provide a constructive elicitation procedure and prove that, under the structural assumptions (additivity, piecewise linear marginals with known breakpoints, noiseless responses), the two individual value functions can be uniquely identified from such unlabeled paired answers. The paper likely includes theoretical guarantees of identifiability and bounds on the number of queries required.

Conclusion: Even when responses from two decision-makers are anonymized and mixed, their individual additive, piecewise-linear value functions with known breakpoints can be recovered exactly, using a suitable query and inference procedure. This extends preference elicitation theory to more complex, multi-agent settings with limited information about who answered what.

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [49] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct turns existing scientific software into active agents guided by LLMs, enabling real-time perception and control during simulations to improve stability, reliability, and modeling accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs can assist in scientific discovery but currently interact with simulators in a passive way: they submit code or commands, wait for completion, then react. This delayed, coarse feedback hides transient numerical issues (instabilities, divergence, oscillations), making it hard to conduct long, reliable simulations and robust engineering designs. A new paradigm is needed where LLMs can continuously perceive and act within simulations as they unfold.

Method: The authors introduce EmbodiedAct, a framework that “embodies” LLMs in existing scientific software (instantiated in MATLAB). It grounds the LLM in low-level embodied actions (e.g., stepwise control, parameter updates, diagnostics) and couples these with a tight perception–execution loop so that the agent can observe intermediate simulation states, detect anomalies in real time, and adapt its actions accordingly. The framework is evaluated on complex engineering design and scientific modeling tasks.

Result: Across extensive experiments in MATLAB-based tasks, EmbodiedAct outperforms existing LLM-agent baselines. It achieves state-of-the-art performance in long-horizon simulations by improving reliability and numerical stability, and it yields higher accuracy on scientific modeling benchmarks.

Conclusion: Embedding LLMs as active, perceptive controllers inside established scientific software—rather than using them as external, passive code generators—substantially improves their effectiveness in scientific discovery. EmbodiedAct demonstrates that a tight perception–execution loop and embodied action space are key to stable, accurate, long-horizon scientific simulations and modeling, suggesting a promising direction for future LLM-based scientific agents.

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [50] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: RB-VLA is a belief-centric vision-language-action architecture that maintains a compact, action-conditioned latent state for long-horizon, partially observed manipulation, enabling more reliable and efficient control than prior VLAs.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models fail on long-horizon robotic manipulation under partial observability because they are overly observation-driven, rely on short context windows or repeated VLM queries, and lack persistent, action-conditioned state. This causes loss of task progress, action repetition, high latency, and poor temporal/physical reasoning. The paper aims to address these limitations.

Method: The authors propose RB-VLA, which introduces a belief module: a self-supervised world-model trained to maintain a compact latent belief state that encodes task-relevant history, object dynamics, and interactions. The system queries a VLM once to obtain a high-level task intent; then, without storing raw observations or growing memory over time, the belief state tracks task progress under partial observability. A diffusion policy is conditioned on both the belief and intent to generate phase-aware, causally grounded actions for closed-loop control.

Result: RB-VLA significantly outperforms previous VLA baselines on long-horizon manipulation benchmarks. It achieves 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks than π0, respectively; reduces inference latency by up to 5x; and avoids the timestep-dependent memory growth seen in other VLAs. Ablation studies show the belief module is the main contributor to gains, boosting success from 32.5% to 77.5%.

Conclusion: Belief-based, action-conditioned state representations are crucial for scalable long-horizon VLA policies. By decoupling one-shot task specification from persistent belief tracking and using a diffusion policy conditioned on both, RB-VLA enables robust, efficient, partially observable multi-stage manipulation, suggesting that future VLAs should be built around compact belief/world-model components rather than purely observation-driven designs.

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [51] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied is a benchmark to evaluate vision-language embodied agents using realistic low-level controls and both high- and low-level tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for VLM-driven embodied agents mostly use non-native action abstractions (high-level commands or discretized actions) and focus on coarse, high-level tasks, making them misaligned with real-world control and unable to pinpoint which fundamental skills agents lack.

Method: They build NativeEmbodied, a benchmark in diverse simulated scenes with a unified, native low-level action space. It defines three complex, high-level tasks to measure overall capability and four types of low-level tasks that each isolate a fundamental embodied skill, enabling joint evaluation across different granularities.

Result: Experiments with state-of-the-art vision-language models on NativeEmbodied show that these agents perform poorly on several core low-level embodied skills, and these deficits correlate with and constrain performance on the more complex, high-level tasks.

Conclusion: NativeEmbodied exposes critical weaknesses in current VLM-driven embodied agents, especially at the level of basic skills, and offers a structured benchmark to diagnose these issues and steer future research toward improving low-level embodied competence.

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [52] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: The paper proposes PromptCD, a test-time method that steers large language and vision-language models toward desired behaviors (e.g., alignment and better visual grounding) using contrast between positive and negative prompts, without extra training or data.


<details>
  <summary>Details</summary>
Motivation: Existing alignment and behavior control methods for LLMs and VLMs generally require additional high-quality training data and significant compute, and are usually designed for narrow behavioral scopes. There is a need for a general, low-cost, post-training method that can reliably improve alignment-related behaviors and task performance across modalities using only test-time interventions.

Method: The authors generalize contrastive decoding by introducing Polarity-Prompt Contrastive Decoding (PromptCD). For a target behavior (e.g., being helpful, honest, harmless, or visually grounded), they craft paired positive (desirable) and negative (undesirable) guiding prompts. During decoding, they compare and contrast the model’s responses under these two prompts. For LLMs, they contrast token-level probability distributions to favor tokens more likely under the positive prompt and less under the negative. For VLMs, they additionally contrast visual attention patterns to reinforce attention regions consistent with the desired behavior. This contrastive mechanism is applied purely at test time, without modifying model weights or requiring extra training data.

Result: On LLMs, PromptCD yields consistent and sizable improvements on the 3H alignment objectives—helpfulness, honesty, and harmlessness—demonstrating that already-trained models can improve alignment behavior at inference time alone. On VLMs, the method leads to significantly better Visual Question Answering (VQA) performance. The analysis shows that the gains are associated with stronger, behavior-consistent visual grounding, as indicated by improved visual attention patterns. These results hold across different enhancement objectives and modalities, indicating broad applicability.

Conclusion: PromptCD provides a simple, training-free, and general mechanism for test-time behavior control in both LLMs and VLMs. By contrasting responses to positive and negative guiding prompts, it can reliably steer models toward aligned and task-consistent behaviors, achieving self-enhancement without additional data or retraining. This makes PromptCD a cost-efficient and versatile tool for improving reliability and alignment across modalities.

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [53] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: The paper proposes a general framework for learning-augmented online algorithms with unreliable predictions, and a simple compiler (DTB) that converts any online algorithm into one with strong consistency and robustness guarantees, achieving optimal or improved results on key classic problems.


<details>
  <summary>Details</summary>
Motivation: Existing learning-augmented online algorithms often intertwine problem-specific algorithm design with prediction models and bespoke loss functions, making it hard to compare methods and transfer insights across problems. There is a need for a clean model that separates predictions from algorithms, allows principled analysis of reliability vs. robustness, and yields general transformations that work for many online problems while providing strong performance guarantees even when predictions are corrupted.

Method: The authors formalize the OAG (online algorithms with unreliable guidance) model via request-answer games where each request arrives with guidance drawn from the problem’s answer space and is independently corrupted with probability β. They define performance notions corresponding to consistency (β=0), robustness (β=1), and smooth tradeoffs as β varies. They then design the DTB (drop or trust blindly) compiler: given any prediction-oblivious online algorithm, the compiler builds a new algorithm that, for each request, either fully follows the guidance or completely ignores it and uses the base algorithm, according to a biased coin whose bias is tuned to β and the problem. They analyze the competitive ratios of compiled algorithms on specific problems.

Result: They show that, despite its simplicity, the DTB compiler generates learning-augmented algorithms with strong consistency-robustness tradeoffs. For caching and uniform metrical task systems, the resulting algorithms achieve optimal guarantees under the OAG model. For online bipartite matching with adversarial arrival order, the DTB-based algorithm surpasses previously best-known learning-augmented algorithms in terms of competitiveness across prediction reliability regimes.

Conclusion: The OAG framework provides a unified and problem-centric way to reason about learning-augmented online algorithms under unreliable predictions, naturally capturing consistency, robustness, and smooth tradeoffs without custom loss functions. The DTB compiler demonstrates that very simple meta-algorithms can deliver optimal or improved guarantees on classic online problems, suggesting that this approach can be broadly applicable and that strong performance with unreliable guidance does not require complex integration of learning and algorithm design.

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [54] [ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction](https://arxiv.org/abs/2602.20708)
*Che Wang,Fuyao Zhang,Jiaming Zhang,Ziqi Zhang,Yinghui Wang,Longtao Huang,Jianbo Gao,Zhong Chen,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ICON is a framework that detects and mitigates indirect prompt injection attacks on LLM agents by probing latent representations and surgically steering attention, greatly reducing attack success while preserving task performance.


<details>
  <summary>Details</summary>
Motivation: LLM agents that read and act on external content are vulnerable to indirect prompt injection, where malicious text in retrieved documents hijacks the agent. Existing defenses rely heavily on filtering and refusals, which often block legitimate actions and disrupt workflows. There is a need for a defense that can reliably neutralize attacks without excessively refusing or terminating useful tasks, and that can generalize across models and modalities.

Method: The paper proposes ICON, a two-stage probing-to-mitigation framework. First, it uses a Latent Space Trace Prober that analyzes the model’s internal latent representations to identify “over-focusing” patterns—high-intensity attention or representation signatures correlated with indirect prompt injection. This prober assigns intensity scores that indicate potential attacks. Second, upon detection, a Mitigating Rectifier performs fine-grained attention steering: it selectively down-weights adversarial query-key dependencies and up-weights task-relevant parts, thereby restoring a healthy reasoning trajectory. The approach is model-agnostic and is applied across different LLM backbones and multi-modal agents.

Result: Experiments across multiple LLM backbones show that ICON reduces the attack success rate to about 0.4%, on par with commercial-grade detectors. At the same time, ICON improves task utility by more than 50% compared with baselines that over-rely on refusals or filtering. The framework also exhibits strong out-of-distribution generalization, continuing to detect and mitigate previously unseen forms of prompt injection, and effectively extends to multi-modal agents beyond pure text models.

Conclusion: ICON demonstrates that latent-space probing combined with targeted attention rectification can robustly defend LLM agents against indirect prompt injection while maintaining high task continuity. By avoiding blunt refusals and instead surgically adjusting internal attention patterns, ICON achieves a strong balance between security (very low attack success rate) and efficiency (significantly higher task utility), and shows promise as a generalizable, model-agnostic defense, including for multi-modal settings.

Abstract: Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.

</details>


### [55] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: The paper proposes Counterfactual Simulation Training (CST) to make chain-of-thought (CoT) reasoning more faithful to how large language models actually produce outputs, improving monitoring and simulation of model behavior.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought explanations are widely used to interpret LLM behavior, but they are often unfaithful: the model’s shown reasoning does not truly reflect the internal process or features driving its answers. This limits the reliability of CoT for safety, debugging, and trust. The authors want a scalable method that pushes models to produce reasoning traces that genuinely explain and predict their behavior, especially under counterfactual changes to inputs.

Method: They introduce Counterfactual Simulation Training (CST), a training scheme where CoTs are rewarded if they enable a separate simulator model to accurately predict the base model’s outputs on counterfactual variants of inputs. They use CST in two main setups: (1) CoT monitoring with cue-based counterfactuals, where inputs are perturbed by adding or changing cues (e.g., spurious, reward-hacking, or sycophancy-related features) and monitors are trained to detect problematic reliance on such cues; and (2) generic model-based counterfactuals, where counterfactual inputs are generated more broadly and simulation accuracy is used to encourage more faithful and generalizable CoTs. They compare CST to prompting-only baselines and to reinforcement learning alone, and also study effects across model sizes and different cue types (persuading vs dissuading).

Result: On models up to 235B parameters, CST improves monitoring accuracy on cue-based counterfactuals by about 35 percentage points and improves simulatability over generic counterfactuals by about 2 points. CST performs better than prompting-only baselines. Using an LLM to rewrite unfaithful CoTs combined with RL is about 5x more efficient than using RL alone. However, gains in faithfulness do not transfer well from inputs with persuading cues to those with dissuading cues. Larger models are not inherently more faithful in their CoTs, but they gain more from CST training than smaller models.

Conclusion: Counterfactual Simulation Training is an effective method for increasing the faithfulness of chain-of-thought explanations, especially for monitoring whether models rely on undesirable features. While improvements are not uniform across all cue types, CST consistently enhances monitor accuracy and simulatability and scales well with model size, indicating its promise as a general tool for improving CoT-based interpretability and safety monitoring of large language models.

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [56] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: They propose BAPO, an off‑policy RLVR framework that reuses and reweights past data to train LLMs more data‑efficiently and effectively than prior on‑policy methods like GRPO.


<details>
  <summary>Details</summary>
Motivation: Existing on‑policy RL with Verifiable Rewards (RLVR) approaches in LLM post‑training discard much of their collected experience and treat rewards in a homogeneous way. This leads to poor data efficiency and weak learning on hard samples, especially in challenging domains like math, planning, and visual reasoning where informative failures are common and expensive to obtain.

Method: BAPO is an off‑policy RLVR framework. It maintains a replay of historical samples, dynamically re‑evaluates them to identify which are difficult or high‑quality, and then adaptively constructs training batches from this pool instead of only using fresh on‑policy data. The algorithm is designed with a lower‑bound guarantee on policy improvement, ensuring that even when learning from off‑policy data, updates are safe and performance does not degrade in expectation.

Result: In experiments across mathematics, planning, and visual reasoning benchmarks, BAPO improves performance by an average of 12.5% over GRPO, a strong on‑policy RLVR baseline. Furthermore, BAPO is able to solve 40.7% of problems that the underlying base models consistently fail on, indicating that its focused reuse of difficult and high‑quality samples translates into substantial capability gains.

Conclusion: By turning RLVR into an off‑policy, batch‑adaptive training procedure, BAPO mitigates experience waste and reward homogeneity, leading to more data‑efficient and effective LLM post‑training. The method provides both empirical gains over GRPO and a theoretical lower‑bound policy improvement guarantee, suggesting it is a promising direction for scaling RL‑based post‑training on challenging reasoning tasks.

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [57] [Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation](https://arxiv.org/abs/2602.20723)
*Ji Dai,Quan Fang,Dengsheng Cai*

Main category: cs.AI

TL;DR: MAGNET is a multimodal recommendation model that uses modality-guided mixture-of-graph-experts and entropy-triggered routing to fuse heterogeneous signals in a controllable, stable, and interpretable way, improving performance especially under sparsity and long-tail settings.


<details>
  <summary>Details</summary>
Motivation: Multimodal recommendation benefits from combining user-item interaction data with item content (e.g., text, images), especially when feedback is sparse and items are long-tail. However, different modalities are heterogeneous and can conflict contextually; existing methods typically use a shared fusion pathway, which entangles representations, causes modality imbalance, and reduces controllability and interpretability. There is a need for a fusion mechanism that can adaptively decide what to fuse and how, while remaining stable under sparse signals and avoiding expert collapse.

Method: The paper proposes MAGNET, a Modality-Guided Mixture of Adaptive Graph Experts Network with Progressive Entropy-Triggered Routing. At the representation level, it introduces a dual-view graph learning module that augments the user–item interaction graph with content-induced edges, encoding the original and augmented graphs in parallel and applying lightweight fusion to preserve collaborative structure while improving coverage for sparse and long-tail items. At the fusion level, it employs structured experts with explicit modality roles (dominant, balanced, complementary) to adaptively combine behavioral, visual, and textual signals. Expert routing is conditioned on interactions and made structure-aware. To improve training stability and avoid expert collapse, it uses a two-stage entropy-weighting mechanism that monitors routing entropy and gradually shifts from coverage-oriented routing in early training to specialization-oriented routing later, balancing expert utilization and routing confidence.

Result: On multiple public benchmark datasets, MAGNET consistently outperforms strong multimodal recommendation baselines, indicating that its modality-guided mixture-of-experts fusion and entropy-triggered routing lead to better ranking performance, particularly under sparse feedback and long-tail item distributions.

Conclusion: MAGNET offers a controllable, stable, and interpretable multimodal fusion framework for recommendation by combining dual-view graph augmentation with modality-structured experts and entropy-triggered routing. It explicitly controls both what modalities to fuse and how to fuse them per interaction, improving recommendation accuracy and robustness while mitigating modality imbalance and expert collapse in mixture-of-experts architectures.

Abstract: Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \textbf{MAGNET}, a \textbf{M}odality-Guided Mixture of \textbf{A}daptive \textbf{G}raph Experts \textbf{N}etwork with Progressive \textbf{E}ntropy-\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \emph{what} to fuse and \emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.

</details>


### [58] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: The paper extends RL from AI feedback (RLAIF) to multi-objective self-adaptive systems, showing it can learn balanced, user-aligned trade-offs among conflicting goals without heavy reward engineering.


<details>
  <summary>Details</summary>
Motivation: Reward design in real-world RL is hard, especially with multiple conflicting objectives where trade-offs are underspecified and policies may over-optimize a single dominant goal. While preference-based RL and RLAIF reduce dependence on hand-crafted rewards and human labels, prior RLAIF work focuses on single-objective tasks. There is a need to understand whether and how RLAIF can be used for multi-objective settings, where user priorities and trade-offs are crucial.

Method: The authors conceptually extend the RLAIF paradigm from single-objective to multi-objective RL in self-adaptive systems. They use LLMs to generate preference labels over behavioral outcomes that differ along multiple objectives, and train RL policies against these learned preference models to induce trade-offs aligned with different user-specified priorities. The method avoids manually crafting scalarized rewards for each objective combination, instead letting AI-generated preferences guide policy learning.

Result: They find that multi-objective RLAIF can successfully learn policies that reflect nuanced trade-offs between conflicting objectives, capturing different user priorities without collapsing onto a single dominant objective. The resulting policies exhibit balanced performance across objectives consistent with the preference information provided via AI feedback.

Conclusion: Integrating RLAIF into multi-objective RL provides a scalable approach to derive user-aligned policies in domains with inherently conflicting objectives, alleviating the need for extensive reward engineering and enabling flexible adaptation to different user preference profiles.

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [59] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS is a context-aware, system-efficient KV cache pruning framework for long-context LLMs that achieves up to 4.56× speedup using only 1% of the cache while matching or surpassing full-KV quality.


<details>
  <summary>Details</summary>
Motivation: Long-context LLM inference is increasingly bottlenecked by the size and access costs of the KV cache as context length grows. Existing KV pruning/selection methods are mostly context-agnostic: they prune based on global or static criteria and ignore step-wise relevance and local semantics, which can hurt generation quality. In addition, their fine-grained and irregular memory accesses introduce overhead that prevents theoretical sparsity from translating into real wall-clock speedups. There is a need for a KV management approach that is both context-aware (for quality) and system-friendly (for actual latency and throughput gains).

Method: The paper proposes CHESS, an algorithm–system co-design for KV-cache management in long-context LLM decoding. Algorithmically, CHESS uses a context-aware, hierarchical selection policy that, at each decoding step, dynamically chooses and reconstructs a subset of tokens that form a coherent and relevant context for the current token prediction. This selection exploits local semantics and step-wise relevance rather than static or global importance scores. System-wise, CHESS implements coarse-grained KV selection so that the chosen KV entries can be accessed and moved in large contiguous chunks, reducing irregular memory traffic and data movement overhead. This design aligns the algorithm’s sparsity pattern with hardware-friendly access patterns, enabling practical acceleration.

Result: Experiments show that CHESS can match or even surpass the generation quality of using the full KV cache while retaining only about 1% of the KV entries. It achieves up to 4.56× higher throughput and delivers low-latency, stable inference. Across various benchmarks, it consistently outperforms strong existing KV-pruning and cache-management baselines, both in efficiency (speed, memory) and in model output quality.

Conclusion: CHESS demonstrates that a context-aware, hierarchical KV selection policy combined with system-level, coarse-grained memory management can drastically reduce KV cache usage without sacrificing—and sometimes improving—LLM output quality. By aligning algorithmic sparsity with hardware-efficient access patterns, CHESS converts theoretical sparsity into real-world speedups, offering a practical solution for scalable, low-latency long-context LLM inference.

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [60] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL is a reinforcement learning framework that prevents interaction collapse in agentic multimodal models, enabling stable multi-turn tool use for efficient image and video understanding.


<details>
  <summary>Details</summary>
Motivation: Agentic multimodal models are supposed to interact with tools and perform multi-step reasoning, but standard reinforcement learning often leads to interaction collapse: models learn to minimize tool calls and turns, undermining agentic benefits. There is also a need to handle image and video tasks efficiently, especially for video where naively processing all frames is computationally expensive.

Method: The authors propose PyVision-RL, a framework that (1) uses an oversampling-filtering-ranking rollout strategy to generate and select higher-quality interaction trajectories for RL training, and (2) introduces an accumulative tool reward that explicitly incentivizes appropriate multi-turn tool usage, avoiding collapse to single-step behavior. They apply a unified training pipeline to build two models: PyVision-Image for image tasks and PyVision-Video for video tasks. For video, they add an on-demand context construction mechanism that selectively samples only task-relevant frames during reasoning to save visual tokens.

Result: Models trained with PyVision-RL maintain sustained interaction patterns and make effective multi-turn tool calls. PyVision-Video, in particular, achieves strong performance on video reasoning benchmarks while significantly reducing the number of visual tokens processed, indicating better computational efficiency. Overall, experiments confirm that their RL strategy improves both capability and efficiency compared to baselines that suffer from interaction collapse.

Conclusion: Sustaining interaction and incentivizing appropriate multi-turn tool use are crucial for scalable agentic multimodal models. PyVision-RL addresses interaction collapse by combining a tailored rollout-selection strategy with accumulative tool rewards, and its on-demand visual context mechanism shows that selective visual processing is key for efficient video reasoning. The resulting PyVision-Image and PyVision-Video models validate the framework’s effectiveness in practice.

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [61] [Pipeline for Verifying LLM-Generated Mathematical Solutions](https://arxiv.org/abs/2602.20770)
*Varvara Sazonova,Dmitri Shmelkin,Stanislav Kikot,Vasily Motolygin*

Main category: cs.AI

TL;DR: A pipeline for more accurate verification of Large Reasoning Models on math problems, using structured prompts and proof assistants instead of simple answer checking.


<details>
  <summary>Details</summary>
Motivation: Current math benchmarks for Large Reasoning Models mainly check only final answers, which can misjudge models that partially reason correctly or coincidentally guess correct answers. There is a need for more precise evaluation that inspects solution structure and can also generate reliable formal and informal solutions.

Method: They design a verification pipeline that integrates automatic and interactive proof checking. The system uses tailored prompts so models output solutions in a format amenable to proof assistants (Lean4) and to downstream use by small models (≤8B). The pipeline includes three configurable AI agents and can act both as a benchmark verifier and as a solution generator in formal and informal languages.

Result: On several datasets, the pipeline demonstrates a low rate of false positives—cases where an incorrect solution is mistakenly validated—indicating reliable verification. They release an open-source implementation along with server setup instructions.

Conclusion: Structured, proof-assistant-based verification provides a more accurate and flexible way to evaluate and utilize Large Reasoning Models on math problems than simple answer checking. The proposed pipeline, with multiple agents and prompt-controlled output formats, offers a practical and extensible framework for both benchmarking and solution generation, supported by an open-source Lean4 implementation.

Abstract: With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.

</details>


### [62] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: Introduces POMDPPlanners, a Python toolkit for scalable, reproducible empirical evaluation of POMDP planning algorithms, especially in risk-sensitive settings.


<details>
  <summary>Details</summary>
Motivation: Existing toolkits for POMDP planning do not sufficiently support large-scale, reproducible empirical evaluation, particularly for safety- and risk-sensitive decision-making under uncertainty. Researchers face overhead in integrating algorithms, managing benchmarks, tuning hyperparameters, handling caching/failures, and running parallel simulations.

Method: Develop an open-source Python package that (1) integrates state-of-the-art POMDP planning algorithms, (2) provides benchmark environments including safety-critical variants, (3) uses Optuna for automated hyperparameter optimization, (4) implements persistent caching with failure recovery, and (5) supports configurable parallel simulation to streamline extensive simulation studies.

Result: The resulting package, POMDPPlanners, offers a unified framework where users can run and compare POMDP planners across benchmarks with built-in risk-sensitive scenarios, optimized hyperparameters, efficient simulation via parallelism, and robustness through caching and failure recovery, thereby reducing experimental overhead.

Conclusion: POMDPPlanners enables scalable and reproducible research on decision-making under uncertainty, filling a gap in existing tools by focusing on risk-sensitive POMDP planning and providing infrastructure that simplifies extensive empirical evaluation.

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [63] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: This paper presents the first domain-specific LLM and benchmark for BIM-based design, including BIM-to-text dataset generation and a fine-tuning strategy that yields a compact 14B-parameter model (Qwen-BIM) performing on par with much larger general LLMs for BIM tasks.


<details>
  <summary>Details</summary>
Motivation: BIM-based design is central to the digital transformation of the construction industry, but current general-purpose LLMs perform poorly on specialized BIM tasks due to the absence of appropriate domain datasets and standardized evaluation benchmarks. There is a need for systematic ways to measure, train, and adapt LLMs for BIM-specific workflows.

Method: The authors design: (1) a BIM-based design evaluation benchmark with quantitative indicators tailored to LLM capabilities in this domain; (2) a method to generate textual data directly from BIM models and construct BIM-derived datasets for LLM evaluation and fine-tuning; and (3) a fine-tuning strategy that adapts an existing base LLM into a BIM-specialized model, named Qwen-BIM, using the constructed benchmark and datasets.

Result: Using the proposed benchmark, the study shows that general LLMs remain inadequate for BIM-specific tasks. After applying the BIM-derived dataset and fine-tuning strategy, the resulting Qwen-BIM model achieves a 21.0% average improvement in G-Eval scores over its base model and, with only 14B parameters, attains performance comparable to much larger (671B-parameter) general LLMs on BIM-based design evaluations.

Conclusion: The paper concludes that a domain-specific benchmark and BIM-derived datasets are crucial for accurately evaluating and effectively adapting LLMs to BIM-based design. The resulting Qwen-BIM model demonstrates that targeted fine-tuning can produce compact LLMs that match or exceed the BIM-task performance of far larger general models, laying a solid foundation for future BIM-related LLM development across construction and related fields.

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [64] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: A new benchmark with 904 realistic multi-turn scenarios is proposed to evaluate alignment of language models across six key categories, revealing behavioral failures and showing that alignment behaves as a unified construct.


<details>
  <summary>Details</summary>
Motivation: Existing alignment evaluations mostly probe what models say in isolated prompts, not how they actually behave under realistic, high-pressure, multi-turn conditions. As real-world harms from misaligned model behavior grow, there is a need for a comprehensive, realistic, and systematically validated benchmark to detect nuanced alignment failures.

Method: The authors design 904 diverse scenarios grouped into six alignment categories: Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming. Scenarios incorporate conflicting instructions, simulated tool use, and escalating multi-turn interactions to elicit behavior that static, single-turn tests can miss. They evaluate 24 frontier language models on these scenarios. Model outputs are scored using LLM judges that have been calibrated and validated against human annotations. Statistical factor analysis is used to investigate whether performance across categories reflects a single underlying alignment factor.

Result: Across 24 evaluated models, even the best systems show notable weaknesses in particular categories, while most models perform poorly across many or all categories. Behavior under pressure often diverges from models’ stated policies or apparent single-turn aligned responses. Factor analysis indicates that scores across different alignment categories are strongly correlated, suggesting a common underlying alignment factor analogous to the g-factor for intelligence.

Conclusion: The benchmark reveals that current frontier models are still imperfectly aligned and can exhibit diverse failure modes, especially in realistic multi-turn settings. Alignment performance appears to be a unified construct: models that are strong in one alignment dimension tend to be strong in others. The authors release the benchmark and an interactive leaderboard to encourage ongoing, standardized evaluation, and plan to expand both coverage in weak areas and the set of evaluated models over time.

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [65] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: The paper introduces a causal-graph-based diagnostic benchmark for LVLMs to disentangle failures in causal reasoning from failures in identifying relevant visual information, showing that structured causal guidance notably improves LVLM consistency.


<details>
  <summary>Details</summary>
Motivation: Although LVLMs perform well on visual QA benchmarks, they often exploit spurious correlations and it is unclear whether their errors are due to weak reasoning or poor identification of causally relevant visual factors; current benchmarks mostly score only final answer correctness, masking where the failure actually occurs.

Method: The authors propose Vision-Language Causal Graphs (VLCGs), which are structured, query-conditioned causal representations encoding objects, attributes, relations, and scene-grounded assumptions, and build ViLCaR, a diagnostic benchmark with tasks for Causal Attribution, Causal Inference, and Question Answering. They design graph-aligned metrics to evaluate how well models identify and use causally relevant information, and run experiments where LVLMs are provided with or without structured relevance information to compare performance.

Result: State-of-the-art LVLMs, when given structured causal/relevance information aligned with VLCGs, show significantly improved consistency in causal attribution and inference tasks relative to zero-shot and standard in-context learning baselines, even when final answer accuracy might be similar under traditional evaluation.

Conclusion: The study concludes that LVLMs possess substantial latent causal reasoning capabilities, but their performance is bottlenecked by insufficient structural guidance in identifying and organizing relevant information; providing explicit causal structure helps unlock more consistent and reliable causal reasoning, indicating that future progress should focus on better structural grounding rather than only scaling or generic prompt engineering.

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [66] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: The paper studies how visual context (images) affects sentence acceptability judgments for humans and LLMs, finding that images barely affect humans but do affect models, revealing similarities and differences in multimodal processing.


<details>
  <summary>Details</summary>
Motivation: To understand whether and how non-textual, visual context changes the way humans and large language models judge whether sentences are acceptable, and to compare this with known effects of textual context. This helps clarify how aligned LLMs are with human linguistic intuitions in multimodal settings, which are increasingly common in modern AI systems.

Method: The authors collect human acceptability judgments on sentences presented with and without associated images, and obtain analogous judgments from various LLMs (including Qwen and others). They compare: (1) the impact of adding visual context on human vs. LLM ratings; (2) how well different LLMs predict human judgments; (3) how similar the distributions of judgments are between humans and each model; and (4) correlations between LLM acceptability scores and their normalized log probabilities, with and without visual context.

Result: Humans show little to no change in sentence acceptability ratings when visual images are provided, unlike documented effects of textual context. LLMs, however, exhibit a compression effect (ratings move toward the middle) in the presence of context, as previously seen for humans with textual/document context. Overall, LLMs predict human acceptability judgments with high accuracy, but perform slightly better when visual context is absent. Judgment distributions differ across models; Qwen’s pattern is closest to humans, whereas other models deviate more. LLM acceptability predictions are strongly correlated with normalized log probabilities without images, but the correlation weakens when visual context is added.

Conclusion: Visual images exert minimal influence on human sentence acceptability judgments, contrasting with known effects of textual context. LLMs, in contrast, show measurable changes in their judgments under visual context and display a compression effect, indicating that their internal processing of multimodal input differs from human behavior. While LLMs can approximate human acceptability judgments well, they align slightly better in pure-text settings, and their internal probability structure becomes less tightly linked to explicit acceptability ratings when images are present. These findings highlight both convergence and divergence between human and LLM processing of language in multimodal environments, and point to open questions about how to better align multimodal LLM representations with human intuitions.

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [67] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: Proposes HELP, a GraphRAG framework that improves multi-hop QA by building hypernode reasoning paths and directly localizing evidence, achieving high accuracy with up to 28.8x speedup over prior graph-based RAG.


<details>
  <summary>Details</summary>
Motivation: LLMs hallucinate and have limited internal knowledge, and standard RAG ignores structural interdependencies needed for multi-hop reasoning. Existing graph-based RAG methods try to use structure but suffer from high computational cost (e.g., random walks, heavy traversals) and semantic noise from LLM-generated summaries, leading to a trade-off between accuracy and efficiency. The paper aims to break this trade-off and design a graph-based RAG that is both structurally accurate and practically efficient.

Method: Introduce HELP, which has two key strategies: (1) HyperNode Expansion: iteratively chain knowledge triplets into coherent, graph-based reasoning paths and compress them into HyperNodes that represent higher-level structural dependencies; this focuses retrieval on accurate, structurally relevant context. (2) Logical Path-Guided Evidence Localization: precompute correlations between graph elements (nodes/paths) and the underlying text corpus so that, at query time, logical reasoning paths (HyperNodes) can be quickly mapped back to concrete textual evidence without expensive random walks or on-the-fly summarization. Together, these components form a GraphRAG pipeline that preserves knowledge integrity and supports efficient multi-hop reasoning.

Result: On multiple simple and multi-hop QA benchmarks, HELP attains competitive or superior performance compared with leading graph-based RAG baselines while achieving up to 28.8x speedup in retrieval. It maintains accuracy without the typical efficiency penalty seen in prior graph-based RAG systems, showing that its structural modeling and evidence localization are effective and scalable.

Conclusion: HELP effectively balances accuracy and efficiency in graph-based RAG by using HyperNode-based reasoning paths and precomputed path-to-text mappings. It avoids expensive random walks and noisy LLM summaries, preserves structural and semantic integrity, and substantially reduces retrieval latency while performing well on both simple and multi-hop QA tasks. This demonstrates a promising direction for scalable, structure-aware retrieval for LLMs.

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [68] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: The paper introduces AgentOS, a conceptual operating-system-like framework for organizing LLMs as reasoning kernels with structured system-level coordination, focusing on deep context management and multi-agent orchestration.


<details>
  <summary>Details</summary>
Motivation: Current LLM research emphasizes scaling context windows and prompt optimization but lacks a clear theoretical and architectural link between low-level token processing and higher-level autonomous, system-wide intelligence. There is a need for a rigorous framework that treats LLMs not just as passive text predictors but as components within larger cognitive systems.

Method: The authors propose a conceptual architecture called AgentOS, which treats the LLM as a reasoning kernel embedded within an operating-system-style environment. They reinterpret the context window as an addressable semantic space and introduce mechanisms such as Semantic Slicing and Temporal Alignment to structure and manage this space. They also map traditional OS abstractions—memory paging, interrupt handling, process scheduling—onto LLM-native constructs to guide design of multi-agent cognitive environments.

Result: The paper provides a systematic decomposition of how discrete token sequences can be organized into coherent cognitive states, outlines strategies to reduce cognitive drift in multi-agent setups, and presents a structured roadmap for building resilient and scalable LLM-based cognitive systems. The primary outputs are theoretical models and design principles rather than empirical benchmarks.

Conclusion: The authors conclude that progress toward AGI will depend less on raw model scale and more on architectural efficiency in system-level coordination. Treating LLMs as reasoning kernels within an OS-like framework, with deep context management and carefully designed orchestration mechanisms, is argued to be a key direction for building robust, scalable, and self-evolving cognitive environments.

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [69] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DEEPSYNTH is a benchmark to evaluate LLM-based agents on realistic, multi-step synthesis and reasoning tasks that require gathering and combining information from multiple sources.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus mostly on short, fact-retrieval or narrow reasoning tasks and do not capture real-world, time-consuming problems that need tool use, multi-source evidence gathering, and non-trivial insight generation. The authors want a benchmark that better reflects how agents are used in practice for complex analysis and synthesis.

Method: They build DEEPSYNTH, a benchmark of 120 tasks across 7 domains and 67 countries. Using a multi-stage pipeline, annotators: (1) collect official data sources, (2) formulate hypotheses, (3) perform manual analysis, and (4) design tasks whose answers are verifiable. They then evaluate 11 state-of-the-art LLMs and deep research agents on these tasks, scoring them with F1 and an LLM-judge based metric.

Result: On DEEPSYNTH, the best-performing standard LLMs achieve an F1 score of only 8.97 and deep research agents reach 17.5 on the LLM-judge metric, which is still low. These metrics show that all evaluated systems perform poorly on the benchmark’s complex, synthesis-heavy tasks.

Conclusion: Current LLM-based agents are far from reliably solving realistic, multi-source, synthesis and reasoning tasks. Their performance on DEEPSYNTH reveals major weaknesses, particularly hallucination and difficulty reasoning over large information spaces. DEEPSYNTH thus provides a challenging and informative benchmark to drive progress on more capable research and analysis agents.

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>


### [70] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: The paper introduces LogicGraph, a benchmark specifically designed to evaluate multi-path logical reasoning in large language models, revealing that current models tend to fixate on a single proof path and struggle to explore diverse valid reasoning routes, especially as depth increases.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of large language models focus mainly on convergent logical reasoning, where a single correct proof is expected, but many real-world tasks allow multiple valid derivations. There is a lack of benchmarks and evaluation frameworks that systematically test a model’s ability to explore and cover multiple reasoning paths rather than prematurely committing to one.

Method: The authors construct LogicGraph using a neuro-symbolic framework that combines backward logic generation with semantic instantiation to create high-depth, multi-path logical reasoning problems with built-in distractions. Each problem is solver-verified and accompanied by an exhaustive set of minimal proofs. They also design a reference-free evaluation framework that can measure both convergent (single-solution) and divergent (multi-solution) reasoning performance of LLMs.

Result: Experiments on state-of-the-art LLMs show that these models consistently commit early to a single reasoning route and rarely explore alternative valid paths. The gap between the set of proofs that exist and those found by models—i.e., the coverage gap—increases markedly as the required reasoning depth grows, indicating a systematic limitation in current LLMs’ divergent reasoning capabilities.

Conclusion: LogicGraph effectively exposes a divergence gap in current LLMs, demonstrating that they are poor at multi-path logical exploration even when multiple valid proofs exist. The benchmark and evaluation framework offer a structured way to study and quantify this issue, providing guidance for future research aimed at improving models’ ability to conduct deep, diverse logical reasoning. The authors plan to release the code and data to support further work in this area.

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [71] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: The paper reports how the Aletheia mathematics research agent, built on Gemini 3 Deep Think, performed on the FirstProof challenge, autonomously solving 6 of 10 problems within the contest timeframe.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether an autonomous AI research agent can meaningfully tackle open-style mathematical problems in a realistic competition setting, and to provide a transparent account of its behavior and performance on a new benchmark, FirstProof.

Method: Deploy the Aletheia agent, powered by Gemini 3 Deep Think, on the inaugural FirstProof math challenge under the same time constraints as human participants. Let the agent autonomously attempt all ten problems and then have human experts assess its solutions, using majority expert judgment to decide which problems count as solved. Additionally, release prompts, outputs, and experimental details for transparency.

Result: Within the allowed timeframe, Aletheia autonomously produced solutions that majority expert judges deemed correct for 6 out of the 10 problems (specifically problems 2, 5, 7, 8, 9, and 10). Expert assessments disagreed only on Problem 8. All raw prompts and outputs are made publicly available in an online repository.

Conclusion: Aletheia demonstrates substantial autonomous problem-solving ability on the FirstProof challenge, successfully solving a majority of the problems under realistic constraints. The authors emphasize transparency by disclosing their interpretation of the benchmark, the setup and evaluation details, and by releasing all prompts and outputs, positioning this as an initial but meaningful step toward robust evaluation of AI research agents in mathematics.

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>


### [72] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: The paper introduces a benchmark to measure how reliably LLMs can make correct reasoning steps (step-success probability γ) in a challenging logical task class, finding that small models degrade quickly with depth while larger models retain partial robustness and rely critically on accurate tool use.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can support the Diligent Learner framework’s path to superintelligence, we need to know how likely they are to succeed on each individual reasoning step (γ), especially on out-of-distribution logical tasks that require integrating all available information and cannot be solved by shallow heuristics.

Method: The authors construct a family of logical reasoning tasks based on GF(2) circuit reconstruction, where difficulty increases with each reasoning step and where information-theoretic considerations mean that success requires careful integration of all clues. They then empirically measure the step-success probability γ for different LLMs across increasing task depths, including scenarios with and without tool use, to quantify robustness of multi-step reasoning.

Result: They observe that for smaller LLMs, the step-success probability γ decreases superlinearly as the reasoning depth grows, indicating poor scalability of reliable reasoning. In contrast, frontier models show partial robustness, maintaining higher γ at greater depths. They also find that strong performance in these settings relies on accurate and well-specified tool calls.

Conclusion: The study concludes that current small LLMs are far from the reliability needed for Diligent Learner-style superintelligence due to rapidly degrading γ with reasoning depth, whereas frontier models show more promise but still depend heavily on precise tool use. Consequently, effective tool design and tool-using abilities are key prerequisites if LLMs are to realize general superintelligence under the Diligent Learner framework.

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [73] [Motivation is Something You Need](https://arxiv.org/abs/2602.21064)
*Mehdi Acheli,Walid Gaaloul*

Main category: cs.AI

TL;DR: They propose a dual-model training scheme inspired by affective neuroscience, where a small base model is trained continuously and a larger “motivated” model is intermittently activated during special training steps, improving both models’ efficiency and performance over standard training.


<details>
  <summary>Details</summary>
Motivation: Standard training typically optimizes a single model of fixed size, which can be inefficient when deploying both small and large variants or when we want to dynamically allocate capacity only when it matters. Neuroscience suggests that during high-curiosity, high-motivation states, the brain recruits extra resources and broader regions to boost performance. The authors seek to translate this into machine learning: can alternating between a lightweight model and a larger, occasionally activated model lead to better performance and efficiency than training them separately or training only one fixed-size model?

Method: They design a dual-model framework with a smaller base model and a larger “motivated” model built as an extension of the base (shared weights plus additional capacity). The base model is updated at every step, whereas the larger model is activated only under predefined “motivation conditions” (special, noteworthy training steps). When activated, the large model’s extra layers/capacity are used, and weight updates are shared where architectures overlap. Architecturally, they rely on scalable designs where the larger network strictly extends the smaller one, enabling joint training while selectively expanding capacity. Training alternates between normal (base-only) and motivated (base+large) phases within an image classification setting.

Result: On image classification benchmarks, the alternating training regime improves the small base model beyond what a traditional single-model training scheme achieves. Surprisingly, in some cases the larger motivated model trained only intermittently actually outperforms a counterpart large model trained alone on more data per epoch. This indicates that the dual, motivation-triggered training can be both more data- and compute-efficient while still yielding high-performing models at both scales.

Conclusion: A neuroscience-inspired, dual-model “motivated training” paradigm—where a small model is trained continuously and a larger model is selectively activated—can simultaneously train two models with different capacities more efficiently than training the large model alone, and can even improve both models’ performance. This suggests that selectively recruiting extra capacity in key training moments, akin to motivated brain states, is a promising direction for scalable and efficient deep learning training strategies.

Abstract: This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined "motivation conditions". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.

</details>


### [74] [The Initial Exploration Problem in Knowledge Graph Exploration](https://arxiv.org/abs/2602.21066)
*Claire McNamara,Lucy Hederman,Declan O'Sullivan*

Main category: cs.AI

TL;DR: The paper defines and analyses the Initial Exploration Problem (IEP), explaining why lay users struggle to start exploring unfamiliar knowledge graphs and proposing a conceptual framework and design implications for better entry-point support.


<details>
  <summary>Details</summary>
Motivation: While knowledge graphs are powerful for integrating complex information, they are hard for non-expert users to approach when they first encounter them. Existing KG interfaces often assume that users already know what they want to ask or how the ontology is structured, which is rarely true for lay users. There is a need to understand and formally characterise this initial barrier to exploration so that interfaces can be designed to support users from their first contact with an unfamiliar KG.

Method: The paper uses a conceptual and theoretical approach rather than an empirical one. It draws on established theories from information behaviour and HCI—such as anomalous states of knowledge (ASK), exploratory search, information foraging, and cognitive load theory—to construct a theoretical model of the Initial Exploration Problem. It also performs an interaction-level analysis of existing KG exploration interfaces, focusing on their interaction primitives and the epistemic assumptions those primitives make about users’ prior knowledge and goals.

Result: The authors identify and define the Initial Exploration Problem (IEP) as a distinct phenomenon affecting lay users at first contact with an unfamiliar KG. They decompose the IEP into three interdependent barriers: (1) scope uncertainty (users do not know what the KG contains or what questions are even possible), (2) ontology opacity (users cannot understand how the knowledge is structured), and (3) query incapacity (users are unable to formulate effective queries). By analysing typical KG interfaces, they show that most current systems implicitly assume that users already have a starting point, some understanding of the ontology, or specific information goals, leading to a structural mismatch with first-contact needs.

Conclusion: The paper concludes that there is a structural gap in the design of KG exploration tools: they lack interaction primitives for scope revelation—ways of communicating what a KG contains without requiring users to already know how to query it or interpret its ontology. By articulating the Initial Exploration Problem and its three core barriers, the authors provide a theoretical lens for critiquing existing KG interfaces and guiding the design of new entry-point scaffolding mechanisms that better support initial, orientation-focused exploration by lay users.

Abstract: Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.

</details>


### [75] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: Proposes CG-DMER, a multimodal framework that learns disentangled ECG and clinical text representations using spatial-temporal masked modeling and modality-shared/specific encoders, achieving state-of-the-art results on multiple ECG tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate ECG interpretation is essential for cardiovascular diagnosis, but current multimodal ECG-text models have two key limitations: they (1) treat ECG leads in a lead-agnostic way, missing fine-grained spatial-temporal dependencies among leads, and (2) directly align ECG signals with free-text clinical reports, which introduces modality-specific noise and bias. The paper aims to overcome these shortcomings to improve diagnostic pattern modeling and robustness of multimodal ECG representations.

Method: The paper introduces CG-DMER, a contrastive-generative multimodal ECG representation learning framework. First, it applies spatial-temporal masked modeling on ECG signals, masking across both time and lead dimensions and reconstructing missing segments to force the model to capture fine-grained temporal dynamics and inter-lead spatial dependencies. Second, it employs a disentanglement and alignment strategy using separate modality-specific encoders and a modality-shared encoder for ECG and clinical reports. This design isolates modality-invariant from modality-specific features and then aligns only the shared representations, mitigating cross-modal noise and bias. Contrastive learning objectives are used for alignment, and generative reconstruction objectives are used for masked modeling.

Result: On three public ECG datasets, CG-DMER outperforms prior methods across multiple downstream tasks, achieving new state-of-the-art performance. These tasks likely include ECG classification and possibly report generation or retrieval, demonstrating benefits from both spatial-temporal masked modeling and disentangled multimodal alignment.

Conclusion: By jointly using spatial-temporal masked modeling for ECG signals and a disentangled multimodal representation strategy, CG-DMER can better exploit inter-lead and temporal dependencies while reducing modality-specific noise from free-text reports. This leads to more robust, fine-grained multimodal ECG representations and state-of-the-art performance on diverse ECG-related tasks, indicating its potential for improving automated cardiovascular diagnosis and analysis.

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [76] [NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning](https://arxiv.org/abs/2602.21172)
*Ishaan Rawal,Shubh Gupta,Yihan Hu,Wei Zhan*

Main category: cs.AI

TL;DR: The paper presents No Reasoning for Driving (NRD), a Vision-Language-Action model for autonomous driving that matches state-of-the-art performance while using less than 60% of the training data and no explicit reasoning annotations, enabled by an improved policy optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models promise simpler, end-to-end autonomous driving systems but currently demand very large datasets and costly dense reasoning annotations. Moreover, standard reinforcement learning fine-tuning methods like Group Relative Policy Optimization (GRPO) do not work well when data are limited and lack reasoning labels, limiting practical deployment of such models.

Method: The authors propose NRD, a VLA model trained without explicit reasoning annotations and on a reduced dataset. They analyze why standard GRPO underperforms in this setting and attribute it to difficulty bias, where high-variance rollouts lead to disproportionate penalization of some scenarios. To address this, they adopt Dr. GRPO, an algorithm tailored to reduce difficulty bias in large language models, and apply it to the autonomous driving policy learning setup.

Result: NRD, fine-tuned with Dr. GRPO, achieves competitive performance on the Waymo and NAVSIM benchmarks relative to existing VLA-based driving systems, while using under 60% of the data and requiring roughly 3x fewer training tokens due to the removal of reasoning annotations.

Conclusion: By combining a reasoning-free VLA architecture with a difficulty-bias-aware optimization method (Dr. GRPO), the paper demonstrates that strong autonomous driving performance can be attained with significantly less data and no reasoning supervision, suggesting that efficient, scalable end-to-end driving systems are feasible without expensive reasoning-heavy datasets.

Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>
