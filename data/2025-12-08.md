<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale](https://arxiv.org/abs/2512.05179)
*Aurélie Montfrond*

Main category: cs.CL

TL;DR: The paper fine-tunes BERT on a custom university course QA dataset to build a domain-specific chatbot for course information, showing that modest fine-tuning yields effective educational question answering and motivates larger, university-focused QA models.


<details>
  <summary>Details</summary>
Motivation: Most scientific QA work focuses on general chatbot behavior rather than systematically adapting foundation models for domain-specific reasoning. Existing domain-specific BERT variants (BioBERT, SciBERT) target biomedical and scientific literature, but none are tailored to university course materials. There is a practical need for an autonomous system that can answer students’ questions about courses, using official module information, and a research gap in demonstrating that foundation models can be effectively specialized for this educational domain.

Method: The authors constructed a custom SQuAD-style dataset of 1,203 question-answer pairs derived from the University of Limerick’s book of modules, augmented with manually crafted and synthetically generated questions. They fine-tuned a pretrained BERT model using PyTorch on this dataset, framing the task as extractive question answering. Model performance was evaluated using standard QA metrics, Exact Match (EM) and F1 score, to assess improvements in answer accuracy and span extraction after fine-tuning.

Result: Fine-tuned BERT showed improved performance in terms of Exact Match and F1 scores relative to the base model, indicating better hypothesis framing and more accurate extraction of relevant information from course descriptions. The results demonstrate that even modest fine-tuning on a relatively small, domain-specific QA dataset can significantly enhance BERT’s ability to answer questions about university courses.

Conclusion: The study concludes that fine-tuning a general-purpose foundation model like BERT on academic, course-related QA pairs is a feasible and effective approach for building a university course information chatbot. This work fills an existing gap in domain-specific QA for higher education and suggests that scaling this approach could lead to the first dedicated QA model for university materials, contributing toward autonomous educational knowledge systems that support students with accurate, context-aware course information.

Abstract: Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.

</details>


### [2] [Unveiling Affective Polarization Trends in Parliamentary Proceedings](https://arxiv.org/abs/2512.05231)
*Gili Goldin,Ella Rabinovich,Shuly Wintner*

Main category: cs.CL

TL;DR: The paper introduces a new way to measure political polarization by focusing on emotional tone (valence, arousal, dominance) instead of ideological content, and applies it to Israeli parliamentary speeches, finding growing emotional polarization over time.


<details>
  <summary>Details</summary>
Motivation: Traditional measures of polarization mostly look at ideological or policy differences, which may miss an important dimension: how emotionally charged and antagonistic the discourse is. With growing concerns about toxic and polarized public debate worldwide, there is a need for a systematic, quantitative way to capture affective aspects of polarization, especially in large text corpora like parliamentary debates.

Method: The authors operationalize affective polarization using psycholinguistic emotion metrics: Valence (positivity/negativity), Arousal (intensity), and Dominance (sense of control). They compute these measures over speeches in a corpus of Knesset proceedings in Hebrew and derive an "emotional style" profile for speakers. They then compare profiles between government and opposition members and track changes over time to quantify emotional divergence as a measure of affective polarization.

Result: They find consistent differences in emotional style between government and opposition members, indicating affect-based distinctions in how each side speaks. Furthermore, the divergence in emotional style between these two camps increases over time, suggesting that affective polarization, as captured by emotional tone, is rising in Knesset discourse.

Conclusion: Emotional-linguistic features (valence, arousal, dominance) can serve as a robust, language-based operationalization of affective polarization. When applied to Israeli parliamentary debates, this approach reveals a growing emotional divide between government and opposition, implying that polarization is not only ideological but increasingly affective in nature. This method can potentially be generalized to other contexts and corpora to monitor changes in polarized discourse.

Abstract: Recent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.

</details>


### [3] [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243)
*P. D. Edgar,Alia Hall*

Main category: cs.CL

TL;DR: The paper proposes using structured poetic prompts to study how large language models behave and reveal their biases, especially in creative contexts.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering is widely used to probe LLM behavior, but existing approaches often focus on functional or task-oriented prompts. The authors are motivated to explore whether more creative, poetry-based prompting can reveal different aspects of LLMs’ tendencies, biases, and treatment of creative works and authorship.

Method: They define and describe “Poetry Prompt Patterns,” a systematic way of crafting poetic prompts, and apply these to multiple large language models. They then use these poetic prompts to elicit and compare models’ descriptions and evaluations of three interpretations of a famous poet, and to test how readily models will modify or rewrite existing creative works for specific assumed audiences.

Result: Using poetic prompts, the study surfaces differences in how models characterize the poet, the models’ preferences among the three interpretations, and their varying readiness to alter or adapt original creative texts to fit audience expectations. These results indicate that poetic prompting can expose nuanced model biases and behaviors not as easily observed with standard prompts.

Conclusion: Creative, poetry-based prompt patterns are a promising extension to conventional prompt engineering. They can serve both as a research tool for analyzing LLM tendencies and as a practical method for creatives and scholars to probe how models engage with authorship, adaptation, and the ethics of rewriting original works for target audiences.

Abstract: Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.

</details>


### [4] [Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4](https://arxiv.org/abs/2512.05256)
*Ivan Makohon,Mohamad Najafi,Jian Wu,Mathias Brochhausen,Yaohang Li*

Main category: cs.CL

TL;DR: The paper explores using Chain-of-Thought prompting, semantic search, and a clinical knowledge-graph to improve GPT-4–generated clinical notes from ICD codes and basic patient data.


<details>
  <summary>Details</summary>
Motivation: Electronic health records contain rich clinical notes that are time-consuming for physicians to write, increasing workload and patient wait times. With the growth in EHR data due to policies like HITECH and the 21st Century Cures Act, there is a strong need to automate or assist clinical documentation. Large language models can generate human-like text, but naïve prompting may not yield accurate, detailed, or clinically grounded notes. The paper is motivated by the need to leverage prompt engineering and structured medical knowledge to improve the quality and usefulness of automatically generated clinical notes.

Method: The authors use GPT-4 to generate clinical notes given inputs of ICD codes and basic patient information. They design Chain-of-Thought (CoT) prompts so the model reasons step-by-step. They then augment these CoT prompts with (1) semantic search over relevant clinical information to retrieve contextually similar cases or knowledge snippets, and (2) a knowledge graph constructed from clinical ontologies to inject domain-specific structured knowledge into the prompting process. They evaluate these prompting strategies on six clinical cases from the CodiEsp test dataset, comparing against standard one-shot prompting baselines.

Result: On the six CodiEsp test cases, GPT-4 using the proposed CoT + semantic search + knowledge-graph–augmented prompts produces clinical notes that outperform those generated by standard one-shot prompts. Though exact metrics are not detailed in the abstract, the improvements are stated in terms of quality of generated notes relative to the baseline.

Conclusion: Incorporating Chain-of-Thought prompting, semantic search results, and knowledge-graph–based clinical knowledge into GPT-4 prompts leads to higher-quality clinical note generation from ICD codes and basic patient information compared to simple one-shot prompts. This suggests that carefully engineered prompts and the integration of structured domain knowledge can significantly enhance LLM performance for clinical documentation tasks, potentially reducing clinician documentation burden and improving EHR note quality.

Abstract: In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.

</details>


### [5] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

TL;DR: The paper studies why chain-of-thought (CoT) in-context learning fails on truly novel reasoning tasks and proposes a meta-training scheme, CoT-Recipe, that carefully mixes CoT and non-CoT examples to greatly improve out-of-context reasoning performance, even without CoT at test time.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought prompting plus few-shot in-context learning enables strong reasoning in LLMs, but it relies heavily on relevant pretraining knowledge and high-quality CoT examples. On novel abstract reasoning tasks, where pretraining does not help much and CoT supervision is limited, performance drops sharply. The authors want a principled way to train models so they can learn new reasoning patterns in-context, without depending on abundant CoT demonstrations for every new task.

Method: Using the CoT-ICL Lab framework for controlled experiments, the authors perform meta-training over many abstract reasoning tasks. During meta-training, they vary how many examples in each training sequence include explicit CoT rationales versus only input-output pairs. They observe that too many CoT examples can actually hurt generalization when CoT is scarce at test time. Based on this, they propose CoT-Recipe, a formal procedure for constructing meta-training sequences with a tuned mixture of CoT and non-CoT examples. They then evaluate transformers trained with different recipes and also adapt the approach to existing pretrained LLMs (Qwen2.5) on symbolic reasoning benchmarks.

Result: CoT-Recipe significantly boosts performance on unseen reasoning tasks compared with naive CoT-heavy meta-training. In transformer models trained from scratch, the right modulation of CoT vs non-CoT examples yields up to a 300% relative accuracy improvement, even when no CoT is provided in the test context. When applied as a training strategy on pretrained Qwen2.5 models for symbolic reasoning, it leads to relative accuracy gains up to 130%.

Conclusion: Explicit chain-of-thought examples are a powerful but double-edged supervision signal for in-context learning: over-reliance on them during meta-training can impair performance when CoT is limited or absent at test time. By carefully balancing CoT and non-CoT examples via CoT-Recipe, models can learn to internalize and apply reasoning strategies more robustly, enabling much better generalization to novel tasks and improving symbolic reasoning accuracy in both from-scratch transformers and pretrained LLMs.

Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [6] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

TL;DR: The paper studies how to detect deceptive, auto-generated local news ('Pink Slime Journalism'), analyzes its linguistic and stylistic patterns, shows that consumer LLMs can weaken current detectors, and proposes a more robust detection framework that improves resilience to such attacks.


<details>
  <summary>Details</summary>
Motivation: Local news is important but threatened by 'Pink Slime Journalism'—low-quality, deceptive, auto-generated content pretending to be real local reporting. Existing detectors are vulnerable, especially to adversarial edits made with easily accessible LLMs, so there is a need to understand these articles' characteristics and build more robust detection systems.

Method: The authors conduct a detailed linguistic, stylistic, and lexical analysis of Pink Slime content and compare it with legitimate local news. They then evaluate the impact of LLM-based modifications on existing detection systems, treating LLM edits as adversarial attacks. Finally, they design and train a robust learning framework aimed at resisting these LLM-based adversarial transformations and adapting to evolving Pink Slime generation methods.

Result: The study shows that common, consumer-level LLMs can severely degrade the performance of state-of-the-art Pink Slime detectors, reducing F1-scores by up to 40%. The proposed robust learning framework, however, substantially improves resilience, yielding performance gains of up to 27% under LLM-based adversarial conditions.

Conclusion: Pink Slime Journalism can be systematically characterized via its linguistic and stylistic features, but current detection methods are vulnerable to simple LLM-based obfuscations. By modeling LLM modifications as adversarial attacks and incorporating robustness into training, detectors can be made significantly more resilient, helping protect the integrity of the local news ecosystem against evolving automated deception.

Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [7] [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)
*Ananth Hariharan,David Mortensen*

Main category: cs.CL

TL;DR: Hybrid neural-symbolic methods are used to analyze 2,000 years of Sanskrit, showing that its morphological complexity is redistributed rather than simplified over time, while providing calibrated uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that language change equals simplification and to derive quantitative insights into the historical evolution of a morphologically rich, low-resource language like Sanskrit, despite limited annotated data.

Method: Use weak supervision with over 100 high-precision regex patterns to generate pseudo-labels for fine-tuning multilingual BERT, then combine symbolic (regex) and neural predictions via a novel confidence-weighted ensemble that yields interpretable outputs and calibrated uncertainty on a 1.47M-word diachronic Sanskrit corpus.

Result: The hybrid ensemble achieves a 52.4% feature detection rate across the diachronic corpus, with strong calibration of uncertainty (Pearson correlation between confidence and accuracy r = 0.92, ECE = 0.043), enabling reliable quantitative tracking of morphological features over time.

Conclusion: Sanskrit does not monotonically simplify; its morphological complexity is dynamically redistributed across linguistic domains, including cyclical decline in some verbal features and large growth in compounding and philosophical terminology, and the proposed neural-symbolic framework offers a scalable, interpretable, and reliable tool for computational philology of low-resource historical languages.

Abstract: This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.

</details>


### [8] [Mitigating Self-Preference by Authorship Obfuscation](https://arxiv.org/abs/2512.05379)
*Taslim Mahbub,Shi Feng*

Main category: cs.CL

TL;DR: The paper studies and attempts to mitigate self-preference bias in language-model judges by obfuscating authorship of candidate answers, finding partial but incomplete success.


<details>
  <summary>Details</summary>
Motivation: Language models are increasingly used as automatic judges of other models’ or humans’ outputs, but they exhibit self-preference—systematically favoring their own answers—which threatens the fairness and reliability of evaluations. Because strong models can implicitly recognize their own style even without explicit labels, it is important to understand whether and how this self-recognition and resulting bias can be reduced.

Method: The authors frame the problem as reducing an LM judge’s ability to recognize its own outputs in pairwise comparisons. They apply black-box text perturbations to candidate answers, such as simple synonym replacements and stylistic obfuscations, to hide authorship while maintaining semantics. They then measure how these perturbations affect both the model’s ability to detect its own outputs and the degree of self-preference in evaluation decisions, and they conceptually extrapolate to scenarios where stylistic differences are more fully neutralized.

Result: They find that even simple perturbations, like replacing a few words with synonyms, systematically reduce self-preference in LM judges, indicating that some aspects of self-recognition rely on shallow stylistic cues. However, when they extend this idea toward more thorough neutralization of stylistic differences, self-preference re-emerges, implying that the model still detects deeper semantic or representational signatures of its own outputs. This shows that mitigation via surface-level obfuscation provides only limited and fragile relief.

Conclusion: Self-preference in LM judges is partially reducible by obfuscating authorship through simple perturbations, but the bias is rooted at multiple semantic levels and tends to reappear when stylistic differences are more comprehensively neutralized. Therefore, while black-box perturbation is a promising direction for reducing bias, fully eliminating self-preference is fundamentally challenging and likely requires deeper methods beyond surface style masking.

Abstract: Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.

</details>


### [9] [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)
*Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Cem Koc,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: They propose SCRPO, a self-supervised method where an LLM critiques and refines its own summaries to build preference data, then trains on that to reduce hallucinations in summarization without extra test-time cost or stronger teacher models.


<details>
  <summary>Details</summary>
Motivation: LLMs hallucinate when doing long-form generation like summarization. Existing critique-and-refine methods reduce hallucinations but need extra compute at inference or a stronger teacher model, which is expensive and impractical. A method is needed that achieves similar faithfulness gains during training time only, using the same model, keeping inference efficient.

Method: Introduce Self Critique and Refinement-based Preference Optimization (SCRPO). During training, the LLM generates an initial summary, then uses its own critique and refinement abilities to produce improved versions. These paired outputs form a preference dataset, where refined summaries are preferred over initial ones. A preference learning objective is then used to fine-tune the same LLM on these self-generated preferences, targeting more faithful summarization without relying on external labels or teacher models.

Result: On three benchmarks (XSUM, CNN/DailyMail, and SAMSum), SCRPO outperforms state-of-the-art self-supervised methods on faithfulness metrics and at least maintains, often improves, general summary quality metrics. Compared to doing critique-and-refine at inference time, SCRPO is more efficient and yields more faithful summaries.

Conclusion: Self-supervised preference optimization based on a model’s own critique and refinement can effectively reduce hallucinations in summarization without extra test-time cost or stronger teacher models. Training-time self-critique based preference learning is a practical and effective alternative to test-time refinement for improving LLM faithfulness.

Abstract: Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.

</details>


### [10] [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)
*Ruixuan Huang,Hao Zeng,Hantao Huang,Jinyuan Shi,Minghui Yu,Ian En-Hsu Yen,Shuai Wang*

Main category: cs.CL

TL;DR: They propose a new unified sparse-quantized data format (SQ-format) for post-training quantization of LLMs that improves both accuracy and efficiency and can be supported by current and future hardware.


<details>
  <summary>Details</summary>
Motivation: Current post-training quantization and sparsification methods for LLMs struggle to simultaneously achieve high accuracy and high hardware efficiency. Low-bit formats like W4A8 don’t get real throughput benefits on existing GPUs, and supported sparsity formats like 2:4 semi-structured sparsity often incur significant accuracy loss, so there is a gap between algorithmic advances and practical, hardware-accelerated deployment.

Method: Introduce SQ-format, a unified data representation that jointly handles quantization and sparsity. The format is designed so that sparse matrices can be accelerated in high precision and corresponding low-precision matrix multiplications can also benefit. It is tailored for activation distributions with outlier-heavy, highly unequal values, enabling static compression of such activations. They provide algorithms to apply SQ-format in PTQ, and they outline the minimal hardware changes needed for GPUs/accelerators to natively support this format.

Result: Using SQ-format, they achieve state-of-the-art post-training quantization performance on LLMs, attaining a better trade-off (Pareto improvement) between model accuracy and hardware throughput compared with existing W4A8, W8A8, and 2:4 sparsity approaches.

Conclusion: SQ-format is an effective and hardware-friendly unified sparse-quantized data format that can significantly improve the practical deployment of quantized and sparse LLMs. It enables static compression of outlier-dominated activations, yields SOTA PTQ results, and offers concrete guidance for designing future AI accelerators that natively support this representation.

Abstract: Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.

</details>


### [11] [LMSpell: Neural Spell Checking for Low-Resource Languages](https://arxiv.org/abs/2512.05414)
*Akesh Gunathilakea,Nadil Karunarathnea,Tharusha Bandaranayakea,Nisansa de Silvaa,Surangika Ranathunga*

Main category: cs.CL

TL;DR: The paper empirically evaluates different pretrained language model families for spell correction, including in low-resource languages, and finds that large language models fine-tuned on enough data perform best, even for languages they were not trained on, and releases a general toolkit (LMSpell) plus a Sinhala case study.


<details>
  <summary>Details</summary>
Motivation: Spell correction remains difficult for low-resource languages because of limited data and tools. Although pretrained language models have been used for spell correction, work has focused on a few high-resource languages and lacks a systematic comparison across model types and languages, particularly LRLs. The authors aim to understand which PLM architectures work best for spell correction, especially in low-resource settings, and to provide practical tooling and evaluation tailored to LLM behavior (e.g., hallucinations).

Method: The authors conduct an empirical comparison of multiple pretrained language model architectures—encoder-only, encoder-decoder, and large language models—used for spell correction across several languages, including low-resource ones. They fine-tune these models on spell-correction datasets of varying sizes to analyze performance trends as data grows. They also design and integrate into LMSpell an evaluation function that accounts for hallucinations specific to LLMs, and they run a focused case study on Sinhala to explore the unique issues faced by low-resource spell correction.

Result: They find that large language models outperform encoder-based and encoder-decoder models for spell correction when the fine-tuning dataset is sufficiently large. This result holds even for languages that were not part of the LLMs' pre-training data, suggesting strong cross-lingual and transfer capabilities. Their toolkit, LMSpell, successfully unifies spell-correction workflows across model families and provides an evaluation component that mitigates the impact of LLM hallucinations on reported performance. The Sinhala case study illustrates the remaining difficulties and constraints of performing spell correction in very low-resource contexts.

Conclusion: Large language models, when fine-tuned on enough in-domain data, are currently the most effective PLM-based approach for spell correction, including for some low-resource languages and even when the target language is unseen in pretraining. However, hallucination requires special treatment in evaluation, which LMSpell addresses. The work provides both empirical evidence and practical tools, while also highlighting that genuine low-resource scenarios, like Sinhala, still present substantial challenges that are not fully solved by existing PLMs or LLMs.

Abstract: Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.

</details>


### [12] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: The paper introduces MusWikiDB, a large music-focused Wikipedia-based vector database, and ArtistMus, a music question answering benchmark, and shows that retrieval-augmented generation (RAG) and RAG-style fine-tuning greatly improve music-related QA performance of LLMs, especially open-source ones.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on open-domain QA but struggle with music-related questions because music knowledge is sparse in their pretraining data. Existing work in music information retrieval and computational musicology focuses more on structured or multimodal understanding and lacks resources tailored for factual and contextual music QA grounded in artist metadata and historical context. There is a need for domain-specific datasets and infrastructure to systematically evaluate and improve RAG for music question answering.

Method: The authors construct MusWikiDB, a vector database built from 3.2M passages extracted from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions over 500 diverse artists annotated with metadata (e.g., genre, debut year, topic). They use these resources to evaluate retrieval-augmented generation pipelines with various LLMs, comparing performance with and without RAG and against a general-purpose Wikipedia corpus. They also perform RAG-style fine-tuning to assess gains in factual recall and contextual reasoning in both in-domain and out-of-domain settings.

Result: RAG significantly improves factual accuracy for music question answering; open-source LLMs show gains up to +56.8 percentage points (e.g., Qwen3 8B from 35.0 to 91.8), reaching or approaching the performance of proprietary models. RAG-style fine-tuning further increases factual recall and contextual reasoning, improving performance on both in-domain and out-of-domain benchmarks. Using MusWikiDB instead of a general-purpose Wikipedia corpus yields about 6 percentage points higher accuracy and 40% faster retrieval.

Conclusion: Domain-specific, music-focused retrieval resources and benchmarks substantially enhance LLM performance on music question answering when combined with retrieval-augmented generation and fine-tuning. MusWikiDB and ArtistMus provide a solid foundation for research on music information retrieval and domain-specific QA and demonstrate that specialized vector databases can advance retrieval-augmented reasoning in culturally rich domains like music.

Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


### [13] [Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment](https://arxiv.org/abs/2512.05464)
*Panatchakorn Anantaprayoon,Nataliia Babina,Jad Tarifi,Nima Asgharbeygi*

Main category: cs.CL

TL;DR: The paper introduces a new value system, Collective Agency (CA), and a scalable self-improving alignment framework, Dynamic Alignment, enabling LLMs to iteratively align themselves without heavy human feedback while preserving general NLP performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM alignment approaches rely heavily on human preference data and a narrow set of values (helpfulness, honesty, harmlessness), which become insufficient and unscalable as AI systems move toward AGI/ASI. AI-feedback-based methods exist but mostly stay within conventional value definitions. The authors want a more holistic alignment target and a method that scales without intensive human supervision.

Method: They define a new alignment objective called Collective Agency (CA), which encourages integrated agentic capabilities beyond standard alignment norms. They then propose Dynamic Alignment, a framework where an LLM repeatedly improves its own alignment through: (1) automated training data generation by LLMs themselves, and (2) a self-rewarding mechanism in which the policy model scores its own candidate outputs and uses these scores as rewards for GRPO-based learning. This creates a closed-loop, self-improving training scheme.

Result: Experiments show that using Dynamic Alignment, the model can be aligned toward the CA objective. At the same time, its general natural language processing capabilities are maintained, suggesting that the new alignment procedure does not significantly degrade core performance.

Conclusion: The paper concludes that a more expansive value objective (CA) combined with a self-improving, AI-feedback-based training loop (Dynamic Alignment) can effectively align LLMs without extensive human feedback, offering a potentially scalable path toward alignment for more advanced AI systems while preserving general capabilities.

Abstract: Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

</details>


### [14] [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: Introduces SEA-SafeguardBench, a native, human-verified safety benchmark for Southeast Asian languages to evaluate LLM safeguard models.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety evaluations are English-centric and often use machine-translated data for multilingual benchmarks, which miss cultural and linguistic nuances, especially in low-resource Southeast Asian languages with unique safety concerns.

Method: Create SEA-SafeguardBench, a human-verified safety benchmark with 21,640 natively authored samples in eight Southeast Asian languages, structured into three subsets: general, in-the-wild, and content generation, then evaluate state-of-the-art LLMs and safeguard systems on this benchmark.

Result: State-of-the-art LLMs and guardrail systems struggle with SEA-specific cultural and harm scenarios, performing worse on SEA-SafeguardBench languages than on English benchmarks.

Conclusion: There is a significant performance gap in LLM safety for Southeast Asian languages; reliable multilingual safety requires natively authored, culturally grounded benchmarks like SEA-SafeguardBench to properly evaluate and improve safeguard models.

Abstract: Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.

</details>


### [15] [Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches](https://arxiv.org/abs/2512.05537)
*Namu Park,Farzad Ahmed,Zhaoyi Sun,Kevin Lybarger,Ethan Breinhorst,Julie Hu,Ozlem Uzuner,Martin Gunn,Meliha Yetisgen*

Main category: cs.CL

TL;DR: The paper evaluates large language models for fine-grained detection of incidental radiologic lesions needing follow-up and shows that anatomy-aware, lesion-tagged LLMs outperform supervised baselines and approach human-level performance.


<details>
  <summary>Details</summary>
Motivation: Existing systems mostly perform document-level classification of radiology reports and lack fine-grained, lesion-level understanding, limiting their usefulness for reliably identifying incidental findings that require follow-up. There is a need to assess whether modern LLMs, with appropriate prompting and structure, can better detect actionable lesions than traditional supervised encoders.

Method: Using 400 annotated radiology reports with 1,623 verified lesion findings, the authors compared three supervised transformer encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) to several generative LLMs (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). They introduced lesion-tagged input formatting and anatomy-aware prompting to ground LLM reasoning at the lesion level, and evaluated performance via class-specific F1-scores for incidentaloma detection.

Result: The anatomy-informed GPT-OSS-20b model achieved the best incidentaloma-positive macro-F1 of 0.79, outperforming all supervised baselines whose best macro-F1 was 0.70, and nearing the inter-annotator agreement of 0.76. Anatomy grounding significantly improved all GPT-based models (p < 0.05). A majority-vote ensemble of the top-performing systems further boosted macro-F1 to 0.90. Error analysis indicated that anatomy-aware LLMs were better at contextual reasoning to distinguish actionable from benign lesions.

Conclusion: Generative LLMs enhanced with structured lesion tagging and anatomical context can significantly surpass traditional supervised encoders in lesion-level incidentaloma detection and reach performance similar to human experts. This framework provides a reliable and interpretable way to automate surveillance of incidental findings in radiology workflows.

Abstract: Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.

</details>


### [16] [Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems](https://arxiv.org/abs/2512.05580)
*Aurprita Mahmood,Sabrin alam,Neloy kumer Sagor,Md. Abdul Hadi,Md. Sehab Al Islam,Minhajul Islam*

Main category: cs.CL

TL;DR: The paper evaluates Tree-of-Thought (ToT) reasoning for solving Bengali mathematical word problems and finds that ToT outperforms both standard prompting and Chain-of-Thought (CoT), especially for larger LLMs.


<details>
  <summary>Details</summary>
Motivation: Mathematical word problems in Bengali are difficult for LLMs because they require both language understanding and multi-step numerical reasoning in a low-resource language. Existing CoT prompting helps but its linear reasoning path can propagate early errors. There is a need to test whether more structured reasoning like Tree-of-Thought can alleviate these issues, particularly for Bengali MWPs where resources and prior studies are limited.

Method: Using the SOMADHAN dataset, the authors curate 100 representative Bengali mathematical word problems. They evaluate multiple LLMs (including GPT-OSS and LLaMA variants) under three prompting strategies: standard prompting, Chain-of-Thought (CoT), and Tree-of-Thought (ToT). Due to computational and token cost constraints, the evaluation is restricted to these 100 items, and performance is compared across model sizes and prompting methods.

Result: Standard prompting achieves about 78% accuracy on average. Applying CoT increases accuracy to around 83%. Tree-of-Thought reasoning further improves performance by up to 5 percentage points over CoT, reaching 88% accuracy with GPT-OSS-120B. The gains are more pronounced for medium-to-large models, with smaller models seeing limited benefit from ToT.

Conclusion: Tree-of-Thought reasoning is an effective and robust framework for solving mathematical word problems in Bengali, especially with medium-to-large LLMs. It yields more reliable and globally consistent reasoning than linear CoT prompting, suggesting that structured reasoning strategies like ToT can significantly improve multilingual and low-resource NLP reasoning performance.

Abstract: Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.

</details>


### [17] [A Greek Government Decisions Dataset for Public-Sector Analysis and Insight](https://arxiv.org/abs/2512.05647)
*Giorgos Antoniou,Giorgos Filandrianos,Aggelos Vlachos,Giorgos Stamou,Lampros Kollimenos,Konstantinos Skianis,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: They build and release a large, machine-readable corpus of Greek government decisions plus a reproducible extraction pipeline, and demonstrate its use for RAG-based question answering over public decisions.


<details>
  <summary>Details</summary>
Motivation: Public-sector decisions in Greece are published on the Diavgeia platform, but mostly as PDFs that are hard to process at scale. There is a need for open, clean, machine-readable corpora to enable better transparency, information access, legal/governmental NLP research, and advanced applications such as RAG assistants over public decisions.

Method: They collected around 1 million government decisions from the Diavgeia transparency platform and built a fully reproducible pipeline to extract high-quality raw text from PDFs into Markdown. They qualitatively analyzed boilerplate patterns in the decisions. They then defined a retrieval-augmented generation (RAG) task: designing representative questions about public decisions, creating high-quality reference answers, and implementing and evaluating a baseline RAG system on retrieval and reasoning performance over the corpus.

Result: They produced a large-scale, open corpus with 1M Greek government decisions in machine-readable Markdown format, plus the full extraction pipeline and code. Their baseline RAG evaluation shows that such public-sector corpora can support effective retrieval and reasoning over government documents and can underpin chat-style assistants that answer questions about public decisions. They show the corpus is suitable as pre-training and fine-tuning data for (L)LMs, particularly in legal/government domains.

Conclusion: The released corpus and pipeline provide a valuable, large-scale resource for transparency and public information access, as well as for training and adapting LMs in legal/governmental domains. RAG pipelines over such corpora can simulate interactive assistants for public decisions, enabling structured retrieval and knowledge-grounded reasoning. They acknowledge limitations, propose future research directions, and openly release both the data and code for community use.

Abstract: We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.

</details>


### [18] [Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models](https://arxiv.org/abs/2512.05658)
*Pietro Ferrazzi,Aitor Soroa,Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper builds multilingual medical reasoning traces for LLMs using retrieval-augmented generation over Wikipedia, boosting medical QA performance in English, Italian, and Spanish and achieving SOTA among 8B models.


<details>
  <summary>Details</summary>
Motivation: Most medical QA LLM work focuses on English and often distills from general-purpose models, making the reliability and grounding of medical knowledge questionable. There is a need for multilingual, transparent, and fact-grounded reasoning processes to support safer clinical decision support, especially for non-English languages like Italian and Spanish.

Method: The authors use a retrieval-augmented generation pipeline over curated medical content from Wikipedia to produce 500k factual reasoning traces in English, Italian, and Spanish. They extend the MedQA and MedMCQA benchmarks to Italian and Spanish, then generate chain-of-thought style traces for these questions. They evaluate using both in-context (few-shot) use of the traces and supervised fine-tuning of 8B-parameter LLMs on these traces, in both in-domain and out-of-domain medical QA settings.

Result: The generated multilingual reasoning traces consistently improve LLM performance on medical QA tasks. When used as few-shot exemplars or as training data for supervised fine-tuning, the models achieve state-of-the-art performance among 8B-parameter LLMs on multiple medical QA benchmarks in English, Italian, and Spanish.

Conclusion: Grounding reasoning traces in retrieved medical Wikipedia content is an effective way to enhance reliability and transparency of LLMs in medical QA, particularly in multilingual contexts. The authors argue this can support safer, more transparent clinical decision-support tools, and they release the reasoning traces, translated QA datasets, Medical-Wikipedia corpus, and fine-tuned models to catalyze further research.

Abstract: Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.

</details>


### [19] [Interleaved Latent Visual Reasoning with Selective Perceptual Modeling](https://arxiv.org/abs/2512.05665)
*Shuai Dong,Siyuan Wang,Xingyu Liu,Zhongyu Wei*

Main category: cs.CL

TL;DR: ILVR is a framework for multimodal large language models that enables efficient interleaved visual-textual reasoning without repeatedly re-encoding images.


<details>
  <summary>Details</summary>
Motivation: Existing interleaved reasoning for MLLMs is computationally expensive because it repeatedly processes high-resolution images. Latent visual reasoning alleviates this but either over-compresses visual features, losing perceptual precision, or relies on static structures that cannot handle dynamic, sequential reasoning. There is a need to combine efficient latent representations with dynamic, interleaved reasoning while preserving fine-grained perception.

Method: ILVR introduces interleaved latent visual representations that evolve alongside text generation, serving as compact but informative visual cues. The framework uses a self-supervised training strategy with a Momentum Teacher Model that, given helper images, selects and distills only the most relevant visual features into sparse supervision targets. This teaches the student model to generate context-aware latent visual signals during reasoning, avoiding repeated full-image encodings.

Result: On multimodal reasoning benchmarks, ILVR achieves significantly better performance compared to prior methods that use either explicit interleaved visual feedback or static latent visual reasoning, indicating improved balance between perception accuracy and sequential reasoning ability.

Conclusion: ILVR successfully bridges the gap between fine-grained visual perception and dynamic multimodal reasoning by using interleaved, evolving latent visual states guided through self-supervised distillation. This yields both computational efficiency and improved reasoning performance over existing approaches.

Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.

</details>


### [20] [MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation](https://arxiv.org/abs/2512.05671)
*Zhitao He,Haolin Yang,Zeyu Qin,Yi R Fung*

Main category: cs.CL

TL;DR: Introduces ClinEdu, a multi-agent simulator, and ClinTeach, a Socratic dialogue dataset, to train MedTutor-R1, a multimodal Socratic tutor for group clinical education, achieving >20% pedagogical improvement over the base model and performance comparable to o3.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for clinical training but a shortage of expert instructors, especially for collaborative reasoning skills developed in group settings like ward rounds. Existing LLM work mainly targets one-on-one knowledge instruction and does not adequately address multi-student, teamwork-oriented teaching scenarios.

Method: The authors build ClinEdu, a multi-agent pedagogical simulation environment with personality-driven virtual patients and diverse student cohorts to emulate complex group teaching processes. Using this simulator, they generate ClinTeach, a large dataset of Socratic teaching dialogues representing group instruction. They then train MedTutor-R1, a multimodal Socratic tutor: first via instruction tuning on ClinTeach, then via reinforcement learning with a reward function based on a three-axis rubric (structural fidelity, analytical quality, clinical safety). Finally, they perform simulation-based interactive evaluation by redeploying MedTutor-R1 back into ClinEdu for in-situ assessment.

Result: MedTutor-R1 substantially outperforms its base model by more than 20% in average pedagogical score and reaches performance comparable to o3. It also shows strong adaptability in interacting with different numbers of student agents in group teaching scenarios.

Conclusion: A multi-agent pedagogical simulator (ClinEdu) combined with Socratic dialogue data (ClinTeach) and RL-based optimization can effectively train an LLM-based multimodal tutor, MedTutor-R1, to support one-to-many clinical education. The positive experimental results suggest that this simulation-driven pipeline is a promising approach to scaling and improving group-oriented clinical training with LLMs.

Abstract: The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.

</details>


### [21] [Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods](https://arxiv.org/abs/2512.05681)
*Tereza Novotna,Jakub Harasta*

Main category: cs.CL

TL;DR: The paper compares a general-purpose OpenAI embedding model with a domain-specific BERT trained on Czech Constitutional Court decisions for case-law retrieval, proposing a noise-aware evaluation framework and finding that the general model significantly outperforms the domain-specific one despite noisy labels.


<details>
  <summary>Details</summary>
Motivation: Legal case-law retrieval is labor-intensive and current evaluations can be misleading due to noisy, heterogeneous labels in legacy judicial databases. There is a need to understand whether expensive domain-specific models actually outperform strong general-purpose models, and to develop evaluation methods that are robust to noisy ground truth when assessing retrieval quality in legal IR.

Method: The authors compare two embedding approaches for retrieving Czech Constitutional Court decisions: (i) a large, general-purpose OpenAI embedding model, and (ii) a domain-specific BERT model trained from scratch on around 30,000 court decisions using sliding windows and attention pooling. They introduce a noise-aware evaluation framework using IDF-weighted keyword overlap as a graded relevance signal, binarize this with two thresholds (0.20 for balanced, 0.28 for strict), assess significance with paired bootstrap tests, and analyze performance with nDCG at various cutoffs supported by qualitative diagnostics.

Result: Across three settings and multiple cutoffs (@10/@20/@100) and for both relevance thresholds, the OpenAI general-purpose embedder clearly and consistently outperforms the domain-specific BERT model; the differences are statistically significant according to paired bootstrap tests. Absolute nDCG values are modest, as expected given noisy labels, but relative performance differences are clear.

Conclusion: The general-purpose OpenAI embedding model is more effective than the specialized BERT model for Czech Constitutional Court case-law retrieval under the proposed noise-aware evaluation. Low absolute nDCG scores are mainly due to label noise, label drift, and idealized ground truth, not because the models lack utility. The proposed evaluation framework is robust enough for legal retrieval tasks with noisy, heterogeneous labels typical of legacy judicial databases.

Abstract: Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.

</details>


### [22] [Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains](https://arxiv.org/abs/2512.05700)
*Ben Malin,Tatiana Kalganova,Nikolaos Boulgouris*

Main category: cs.CL

TL;DR: They propose a fused, learned faithfulness metric for LLMs that better matches human judgments, plus a unified multi-domain dataset for evaluating faithfulness.


<details>
  <summary>Details</summary>
Motivation: Existing automatic faithfulness metrics for LLM outputs are imperfect and often correlate weakly or inconsistently with human judgements, limiting trust and safe deployment of LLMs. There is also fragmentation across datasets and domains, which makes it hard to compare and reproduce faithfulness evaluation methods.

Method: Combine multiple elementary faithfulness metrics into a single fused metric using a tree-based learning model (e.g., random forest/gradient boosting) trained on human judgements of LLM response faithfulness. Use feature importance from the tree model to weight metrics. Construct a homogenized, cross-domain dataset (QA and dialogue) with LLM outputs and human faithfulness labels to train and evaluate the fused metric.

Result: The fused, learned metric shows higher correlation with human faithfulness judgements than any individual metric across all tested domains. The homogenized dataset with human annotations and LLM responses supports reproducible cross-domain evaluation of faithfulness metrics.

Conclusion: Learning to fuse multiple elementary faithfulness metrics via a tree-based model yields a more reliable automatic proxy for human faithfulness judgements, enabling more trustworthy assessment of LLM outputs across domains. The released unified dataset further facilitates robust, reproducible research on faithfulness evaluation.

Abstract: We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.

</details>


### [23] [Efficient Text Classification with Conformal In-Context Learning](https://arxiv.org/abs/2512.05732)
*Ippokratis Pantelidis,Korbinian Randl,Aron Henriksson*

Main category: cs.CL

TL;DR: The paper evaluates CICLe, a framework that combines a small base classifier, conformal prediction, and LLM prompting, showing it improves accuracy and efficiency for text classification across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: While LLMs can perform in-context learning for text classification, their performance is highly sensitive to prompt design and they are computationally expensive. CICLe was introduced as a way to leverage a lightweight classifier and conformal prediction to narrow candidate labels before querying an LLM, but its effectiveness, generality, and efficiency beyond a single domain had not been systematically studied.

Method: The authors conduct a broad empirical study of CICLe on diverse NLP text classification benchmarks. CICLe uses a base classifier to produce label scores, applies conformal prediction to obtain a small, high-confidence candidate label set for each instance, and then prompts an LLM only over this reduced label space. They compare CICLe against its own base classifier and standard few-shot prompting baselines under different data regimes, model sizes, and class imbalance conditions, and measure both performance and efficiency (shots and prompt length).

Result: Across datasets, CICLe consistently outperforms its underlying base classifier and beats few-shot prompting baselines when there is enough labeled data to train the base classifier; in low-data regimes it performs on par with these baselines. CICLe reduces the required number of example shots by up to 34.45% and total prompt length by up to 25.16%, and allows smaller LLMs to reach competitive accuracy. It is particularly strong on highly imbalanced classification tasks.

Conclusion: CICLe is an effective and scalable way to combine traditional classifiers with LLMs for text classification. By using conformal prediction to adaptively shrink the label set before prompting, it delivers strong accuracy, works well across domains, handles class imbalance, and substantially improves data and computational efficiency, enabling the use of smaller, cheaper models without major performance loss.

Abstract: Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.

</details>


### [24] [Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning](https://arxiv.org/abs/2512.05747)
*Jinlong Liu,Mohammed Bahja,Venelin Kovatchev,Mark Lee*

Main category: cs.CL

TL;DR: They train an 8B language model to generate stories in Mark Twain’s style using a reinforcement learning framework with multiple rewards, beating larger models on style metrics but still struggling with full narrative coherence.


<details>
  <summary>Details</summary>
Motivation: LLMs can write stories but cannot reliably mimic a specific author’s style in a fine-grained way, and current approaches rely on weak cues like names or topics without solid evaluation. The authors want a principled way to condition on style and objectively measure stylistic fidelity while maintaining content quality in long-form generation.

Method: They use Group Relative Policy Optimization (a variant of RLHF-style training) with multiple rewards: a style reward from a fine-tuned sentence transformer trained on authorship verification signals, plus content and completeness rewards to keep long stories coherent and on-topic. They train an 8B model on Mark Twain’s fiction, using The Adventures of Huckleberry Finn as the primary style reference, optimizing the policy to maximize this combined reward during story generation.

Result: The 8B model achieves a style score of 0.628 on authorship verification-based metrics and surpasses larger models like GPT-4o and Claude Sonnet 4 in stylistic alignment to Mark Twain, while maintaining competitive content quality. However, evaluation shows that long-term narrative completeness and resolution are still weaker aspects of the generated stories.

Conclusion: Task-specific RL-based style conditioning with moderate-sized models can outperform even larger general-purpose LLMs on author-style imitation, validating the use of AV-based rewards and multi-objective optimization for stylistic control. Nevertheless, better modeling of global story structure and endings is needed to address remaining issues in narrative completeness and coherence.

Abstract: Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution.

</details>


### [25] [Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments](https://arxiv.org/abs/2512.05832)
*Yifei Tong*

Main category: cs.CL

TL;DR: The paper uses computational text analysis to study how interruptions during U.S. Supreme Court oral arguments affect the meaning and emotional tone of advocates’ speech, with special attention to gender differences.


<details>
  <summary>Details</summary>
Motivation: To understand whether interruptions in high-stakes legal discourse change what advocates are arguing, and whether women advocates are treated differently in terms of the emotional negativity they face, thereby shedding light on gendered power dynamics in elite institutions.

Method: Using the ConvoKit Supreme Court Corpus (2010–2019), the authors analyze 12,663 interactional speech chunks between justices and advocates. They compute semantic similarity between pre- and post-interruption speech with GloVe-based sentence embeddings, and assess emotional valence with a lexicon-based sentiment analysis approach, comparing patterns by advocate gender.

Result: Semantic similarity between advocates’ speech just before and after interruptions is consistently high, indicating that interruptions do not substantially change the meaning of their arguments. At the same time, interruptions targeting female advocates show significantly higher levels of negative sentiment than those directed at male advocates.

Conclusion: Interruptions in Supreme Court oral arguments generally leave the substantive content of advocates’ arguments intact, but female advocates experience more negatively valenced interruptions. This reveals gendered patterns in communication and power in judicial contexts and illustrates how computational linguistics can illuminate issues of discourse and equity in legal institutions.

Abstract: This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.

</details>


### [26] [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)
*Savir Basil,Ina Shapiro,Dan Shapiro,Ethan Mollick,Lilach Mollick,Lennart Meincke*

Main category: cs.CL

TL;DR: The paper investigates whether assigning different personas to AI models improves their accuracy on challenging graduate-level multiple-choice questions, finding that personas generally do not boost performance and can sometimes harm it.


<details>
  <summary>Details</summary>
Motivation: With the growing use of AI assistants, users often set personas (e.g., domain experts or laypersons) in hopes of improving factual accuracy. However, there is limited rigorous evidence about whether such persona prompting actually helps models perform better on difficult, objective benchmarks.

Method: The authors evaluate six AI models on two challenging benchmarks, GPQA Diamond and MMLU-Pro, composed of graduate-level multiple-choice questions in science, engineering, and law. They compare three prompting strategies against a no-persona baseline: (1) in-domain expert personas matched to the question domain (e.g., 'you are a physics expert' for physics questions), (2) off-domain expert personas mismatched to the domain (e.g., 'you are a physics expert' for law questions), and (3) low-knowledge personas (layperson, young child, toddler). They then measure the impact of these personas on model accuracy.

Result: In-domain expert personas did not significantly change accuracy in most models, with the notable exception of Gemini 2.0 Flash. Off-domain expert personas produced only marginal differences and sometimes degraded performance. Low-knowledge personas consistently harmed accuracy on both benchmarks. Overall, persona prompts did not systematically improve performance relative to a no-persona baseline.

Conclusion: Assigning personas to AI models does not reliably improve their accuracy on difficult objective multiple-choice questions, and certain personas, especially low-knowledge ones, can reduce performance. While personas may still be valuable for controlling style, tone, or user experience, they should not be relied upon as a mechanism for boosting factual accuracy on challenging tasks.

Abstract: This is the fourth in a series of short reports that help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. Here, we ask whether assigning personas to models improves performance on difficult objective multiple-choice questions. We study both domain-specific expert personas and low-knowledge personas, evaluating six models on GPQA Diamond (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024), graduate-level questions spanning science, engineering, and law.
  We tested three approaches:
  -In-Domain Experts: Assigning the model an expert persona ("you are a physics expert") matched to the problem type (physics problems) had no significant impact on performance (with the exception of the Gemini 2.0 Flash model).
  -Off-Domain Experts (Domain-Mismatched): Assigning the model an expert persona ("you are a physics expert") not matched to the problem type (law problems) resulted in marginal differences.
  -Low-Knowledge Personas: We assigned the model negative capability personas (layperson, young child, toddler), which were generally harmful to benchmark accuracy.
  Across both benchmarks, persona prompts generally did not improve accuracy relative to a no-persona baseline. Expert personas showed no consistent benefit across models, with few exceptions. Domain-mismatched expert personas sometimes degraded performance. Low-knowledge personas often reduced accuracy. These results are about the accuracy of answers only; personas may serve other purposes (such as altering the tone of outputs), beyond improving factual performance.

</details>


### [27] [Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework](https://arxiv.org/abs/2512.05863)
*Tasnimul Hassan,Md Faisal Karim,Haziq Jeelani,Elham Behnam,Robert Green,Fayeq Jeelani Syed*

Main category: cs.CL

TL;DR: The paper builds a retrieval-augmented generation (RAG) medical QA system using fine-tuned open-source LLMs (LLaMA2, Falcon) to improve accuracy and reduce hallucinations on biomedical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Directly using large language models for clinical question answering risks factual errors and hallucinations, which is unsafe for medicine. There is a need for accurate, reference-backed medical QA systems that can leverage LLM capabilities while grounding answers in trusted biomedical literature.

Method: The authors build a RAG pipeline that first retrieves relevant medical literature and then conditions open-source LLMs (LLaMA2 and Falcon) on this retrieved context to generate answers. They fine-tune the LLMs for the medical domain using Low-Rank Adaptation (LoRA) for parameter-efficient training. The system is evaluated on PubMedQA and MedMCQA, comparing plain LLMs vs. retrieval-augmented, and measuring both accuracy and hallucination/unsupported content rates. They also describe the engineering design of the retrieval and generation components.

Result: Retrieval augmentation significantly improves answer accuracy over zero-shot LLMs and reduces hallucinations. The fine-tuned LLaMA2 model reaches 71.8% accuracy on PubMedQA, up from a 55.4% zero-shot baseline. Grounding answers in retrieved evidence cuts unsupported content by about 60%, and the system provides references to source documents for transparency.

Conclusion: RAG-augmented, LoRA-fine-tuned open-source LLMs can provide more accurate, evidence-backed answers for biomedical QA tasks than ungrounded LLMs. This approach enhances reliability and transparency, suggesting that such systems are promising for practical clinical informatics and medical decision-support applications.

Abstract: Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.

</details>


### [28] [M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959)
*David Anugraha,Patrick Amadeus Irawan,Anshul Singh,En-Shiun Annie Lee,Genta Indra Winata*

Main category: cs.CL

TL;DR: Introduces M4-RAG, a large multilingual, multimodal RAG benchmark for VQA, showing RAG helps small VLMs but not large ones.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models for VQA are limited by static training data and lack robust support for multilingual, culturally grounded, and up-to-date information, especially in a multimodal RAG setting.

Method: Construct a massive multilingual and multicultural benchmark (M4-RAG) with 80k+ image-question pairs across 42 languages and 56 dialects, along with a controlled retrieval corpus of millions of curated multilingual documents, then systematically evaluate VLMs of different sizes with RAG in this environment.

Result: RAG consistently improves performance for smaller VLMs but does not scale effectively to larger models and can even hurt their performance, revealing a mismatch between current retrieval systems and large model capabilities.

Conclusion: M4-RAG serves as a standardized testbed for developing and assessing advanced multilingual, multimodal RAG systems, and highlights the need to redesign retrieval strategies that work effectively with large VLMs across languages and cultural contexts.

Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: This paper introduces two unsupervised metrics to evaluate how faithfully LLMs follow a task, based on information theory and thermodynamics.


<details>
  <summary>Details</summary>
Motivation: To objectively and automatically evaluate how closely LLM outputs adhere to the intended question and context, and to detect hallucinations without labeled data.

Method: Model Question-Context-Answer triplets as topic distributions and represent transformations from context to question and answer via transition matrices. Define a semantic faithfulness metric as the KL divergence between these matrices, optimized via convex optimization, and map it to [0,1]. Additionally, define a thermodynamics-inspired semantic entropy production metric relating entropy production in answer generation to faithfulness.

Result: The framework yields two practical, unsupervised metrics (SF and SEP) that correlate high faithfulness with low entropy production and are demonstrated on LLM summarization of SEC 10-K filings.

Conclusion: Information-theoretic and thermodynamic modeling of LLM behavior enables unsupervised, quantitative evaluation of faithfulness and helps identify and control hallucinations in tasks like financial document summarization.

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [30] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: A course design that bridges traditional machine learning with modern LLMs to improve AI education and industry readiness.


<details>
  <summary>Details</summary>
Motivation: There is a gap between teaching classical machine learning and rapidly advancing Large Language Models, and students need coherent preparation that connects both to understand AI evolution and meet industry needs.

Method: Design and deliver a two-part course: first covering foundational machine learning concepts, then focusing on contemporary LLM applications. The paper details course structure, implementation strategies, assessment methods, and evaluates learning outcomes over two seven-week summer terms.

Result: The integrated course structure improved students’ understanding of the broader AI landscape and helped them gain practical skills in both classical ML and LLMs, as evidenced by course assessments and reported learning outcomes.

Conclusion: A sequential, integrated curriculum that connects traditional ML and modern LLMs is effective for AI and data science education and better prepares students for the fast-changing AI industry.

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [31] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: The paper argues, via a formal proof, that algorithms (and thus AI models) cannot generate genuinely new functional capabilities beyond what is already implicitly contained in their original design, so artificial general intelligence as ‘truly creative innovation’ is unattainable for machine computation.


<details>
  <summary>Details</summary>
Motivation: Growing progress in AI has sparked debate about whether artificial general intelligence (AGI)—often associated with human‑level, genuinely creative intelligence—is achievable. The authors want to move beyond informal speculation and provide a rigorous, computation‑theoretic answer by setting fundamental upper bounds on what any algorithmic system can do, specifically with respect to creativity and innovation.

Method: The authors adopt an operational definition of AGI from prior work: the capability to create innovations in some domain that unlock genuinely new and previously unknown functional capabilities. Using this definition, they construct a formal argument (likely based on properties of algorithms and computability theory) to show that an algorithm’s behavior is fully determined by its initial specification and cannot introduce fundamentally new functional capabilities not already encoded—explicitly or implicitly—in that specification.

Result: They derive a formal upper bound: no algorithm can exhibit functional capabilities that were not already present, in some form, in the original algorithm. Consequently, algorithms—and by extension contemporary and future AI models—cannot satisfy the adopted definition of ‘true creativity’ as the generation of genuinely new functional capabilities. They can only recombine, permute, or reveal capabilities that are already embedded in their design and training data.

Conclusion: Under the chosen definition of AGI, machine‑computable processes are provably incapable of true creativity: they cannot originate genuinely new functional capabilities, only manifest existing ones and their combinations. This implies that AGI, in the strong sense used here, is unattainable by any purely algorithmic system. The paper closes by reflecting on how this constraint shapes expectations for future AI research and raises questions about how human intelligence, which appears to exhibit such creativity, might arise from mechanisms that go beyond standard algorithmic computation.

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [32] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: The paper argues that possibility theory, particularly using Bychkov’s axiomatic framework, offers a rigorous and logically consistent alternative to Dempster–Shafer Theory (DST) that resolves its known paradoxes in handling uncertainty.


<details>
  <summary>Details</summary>
Motivation: There is a recognized “crisis” in Dempster–Shafer Theory stemming from paradoxes and logical issues in Dempster’s rule of combination when dealing with conflicting evidence. Many attempts to patch or modify this rule have not fully solved the problem. The authors are motivated to find a more fundamental, coherent framework for reasoning under uncertainty that aligns better with human-like reasoning and handles contradictory information without paradox.

Method: The authors adopt an axiomatic approach to uncertainty based on possibility and necessity measures, following the framework proposed in Bychkov’s earlier work. They conduct a comparative conceptual and analytical study of three paradigms—probabilistic (classical probability theory), evidential (DST), and possibilistic (possibility theory). They use a classic medical diagnosis dilemma as a running example to explicitly show how each paradigm treats conflicting data, highlighting where DST leads to paradoxes and how the possibilistic framework avoids them.

Result: In the medical diagnostic example, the possibilistic framework successfully processes contradictory and conflicting pieces of evidence without falling into the logical inconsistencies and extreme belief assignments sometimes produced by Dempster’s rule. The comparative analysis shows that possibility theory, under the given axiomatic setup, maintains logical consistency while still capturing uncertainty and conflict, and better reflects intuitive, human-like reasoning about incomplete and conflicting information.

Conclusion: The paper concludes that possibility theory, especially within Bychkov’s axiomatic approach, should be viewed not just as another uncertainty formalism but as a fundamental solution to the paradoxes of Dempster–Shafer Theory. It provides a mathematically rigorous, logically consistent basis for reasoning with uncertainty and contradictions, and in practical scenarios like medical diagnosis, it aligns more closely with natural intelligence than DST or, in some conflict-handling aspects, classical probability theory.

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [33] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: The paper proposes shifting the primary AI goal from pure AI self-improvement to human–AI co-improvement, aiming at co-superintelligence.


<details>
  <summary>Details</summary>
Motivation: Direct AI self-improvement (recursive self-enhancement) is powerful but risky and hard to achieve safely. The authors believe focusing only on AI getting smarter by itself neglects the potential and safety benefits of tightly integrating human researchers into the improvement loop.

Method: Conceptual and agenda-setting: they define and argue for 'co-improvement' and 'co-superintelligence,' proposing that AI systems be specifically optimized to collaborate with human researchers across the full AI research pipeline (ideation, experimentation, analysis).

Result: The paper articulates a framework and research direction for designing AI systems whose main capability is productive collaboration with humans on AI research, claiming this can both accelerate progress and improve safety compared to pure AI self-improvement. (No empirical results are described in the abstract.)

Conclusion: Pursuing co-improvement—enhancing joint human–AI research capabilities—offers a more attainable and safer path toward superintelligence than autonomous AI self-improvement, because it embeds human oversight and benefit into the core of the intelligence-amplification process.

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [34] [MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare](https://arxiv.org/abs/2512.05365)
*Zag ElSayed,Craig Erickson,Ernest Pedapati*

Main category: cs.AI

TL;DR: The paper proposes MCP-AI, a new architecture that uses the Model Context Protocol to enable explainable, long-term, collaborative clinical reasoning by AI agents, validated on two clinical use cases and integrated with real-world standards and interfaces.


<details>
  <summary>Details</summary>
Motivation: Existing healthcare AI is fragmented: CDSS tools are rigid and narrow, while prompt-based LLM systems are largely stateless, hard to audit, and difficult to align with real clinical workflows, regulations, and long-term patient management. There is a need for a framework that supports persistent, contextual, explainable reasoning over time, while fitting into physician-in-the-loop workflows and healthcare IT standards.

Method: The authors design MCP-AI, an architecture built on the Model Context Protocol (MCP). They define MCP files as modular, executable specifications that capture clinical objectives, patient context, reasoning state, and task logic. These files function as reusable memory objects that orchestrate generative and descriptive AI agents across real-time clinical workflows, while enabling secure collaboration and transitions of responsibility among clinicians and systems. They integrate the system with HL7/FHIR, and design it to conform to HIPAA and FDA SaMD requirements. They then apply MCP-AI to two detailed clinical use cases.

Result: In the two validation scenarios—(1) diagnostic reasoning for Fragile X Syndrome with comorbid depression, and (2) remote care coordination for Type 2 Diabetes and hypertension—MCP-AI was able to support longitudinal, adaptive reasoning, coordinate multiple agents and clinicians, and enable physician-in-the-loop validation. The system streamlined clinical workflows and safely handled handoffs of AI-driven tasks between providers, while maintaining auditability and adherence to clinical logic and regulatory standards.

Conclusion: MCP-AI demonstrates that the Model Context Protocol can underpin a new class of explainable, composable, and safety-focused AI systems for healthcare. By encoding objectives, context, reasoning state, and task logic into auditable MCP artifacts, the framework overcomes limitations of conventional CDSS and stateless LLM tools, providing a scalable foundation for future autonomous, context-aware clinical reasoning in real-world, regulated environments.

Abstract: Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.

</details>


### [35] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind is a knowledge graph-augmented reasoning framework that lets LLMs handle long, complex IC specifications by building a circuit-specific knowledge graph and performing adaptive, semantic-aware retrieval over it, greatly improving specification reasoning performance.


<details>
  <summary>Details</summary>
Motivation: LLMs could automate parts of IC/Hardware design, but are blocked by limited context windows, which makes it hard to reason over long, complex, and interdependent circuit specifications. Existing context-extension methods do not model circuit semantics well and fail on multi-hop reasoning across large specs, so there is a need for a domain-tailored, more structured way to represent and reason over IC documents.

Method: ChipMind introduces Circuit Semantic-Aware Knowledge Graph Construction to convert IC specifications into a domain-specific knowledge graph, ChipKG. On top of ChipKG, it uses ChipKG-Augmented Reasoning, which combines (1) information-theoretic adaptive retrieval that dynamically traces logical dependencies across the graph, and (2) intent-aware semantic filtering that prunes irrelevant nodes/edges to balance recall and precision in retrieval for the LLM.

Result: On an industrial-scale benchmark for specification reasoning, ChipMind outperforms existing state-of-the-art baselines by an average of 34.59% and up to 72.73%, demonstrating substantially better performance on long, complex IC specification reasoning tasks.

Conclusion: ChipMind effectively overcomes LLM context-window limits for IC design specs by using a domain-specific knowledge graph plus adaptive, semantic-aware retrieval, which enables stronger multi-hop reasoning and makes LLM-based hardware design support more viable for real industrial use cases.

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [36] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER is a framework that computes deterministic, sound probability bounds on whether LLM outputs satisfy semantic constraints, outperforming sampling-based and loose-bound baselines.


<details>
  <summary>Details</summary>
Motivation: As LLMs are deployed in production, practitioners need methods that can reliably verify that model outputs meet strict constraints (e.g., correctness, privacy, security). Existing sampling-based evaluations lack formal guarantees and can miss rare but high-risk failures, creating a need for a principled, sound verification approach.

Method: BEAVER systematically explores the LLM’s generation space under any prefix-closed semantic constraint using novel token trie and frontier data structures. It formalizes verification as computing probability bounds for constraint satisfaction, incrementally tightens these bounds by exploring likely continuations, and maintains provable soundness at every step, independent of sampling noise.

Result: Across tasks including correctness verification, privacy verification, and secure code generation on multiple SOTA LLMs, BEAVER produces probability bounds that are 6–8x tighter and flags 3–4x more high-risk instances compared to baseline verification methods, all under the same compute budget.

Conclusion: BEAVER provides a practical, formally sound framework for quantifying the probability that LLM outputs satisfy semantic constraints. Its tighter bounds and improved detection of high-risk cases enable more precise reliability and risk assessments than empirical or loose-bound approaches, making it suitable for safety-critical LLM deployments.

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [37] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: Introduces and motivates 'akrasia' as a key concept to analyze cases where language models know a correct answer but fail to follow it, and presents an Akrasia Benchmark to measure such inconsistencies and their implications for agentic AI.


<details>
  <summary>Details</summary>
Motivation: Large language models often produce answers that contradict knowledge they just expressed, revealing a mismatch between global judgment (what they "know") and local behavior (what they actually output). Existing notions of inconsistency or hallucination don't fully capture this phenomenon, especially for agentic systems that act over time. The authors borrow the philosophical concept of akrasia—weakness of will—to better characterize and study these failures of self-control, and to build tools that quantify and compare this behavior across models and settings.

Method: The authors define akrasia in the context of AI as cases where a model's later response contradicts its own prior commitments under structured prompting conditions. They construct the Akrasia Benchmark, a set of controlled prompt patterns: Baseline (B), Synonym (S), Temporal (T), and Temptation (X). These conditions systematically elicit scenarios where the model first states intentions, rules, or knowledge, and is later given chances to deviate. They then measure how often local responses violate earlier commitments, enabling quantitative evaluation of model self-consistency or "self-control" across model families, decoding strategies, and different temptation types. They further conceptually extend this to multi-agent and longer-horizon settings.

Result: The benchmark operationalizes akrasia for language models, demonstrating that models can be systematically tested and compared on how reliably they adhere to their own stated commitments across varied conditions. Empirically, different models, decoding strategies, and temptation prompts show different degrees of akratic behavior, indicating that "self-control" is a measurable dimension of model behavior rather than a purely anecdotal issue. The structured conditions reveal where models are especially prone to local deviations versus where they remain consistent.

Conclusion: Recasting inconsistency in agentic AI as akrasia (weakness of will) provides a unified framework to analyze local deviations from global commitments in language models. The Akrasia Benchmark offers a concrete empirical tool to measure and compare such behavior. The authors argue that micro-level akrasia can scale up to macro-level instability and what may appear as scheming or misalignment in multi-agent systems. Overall, the work creates a conceptual and empirical link between philosophical theories of agency, psychological notions of self-control, and practical evaluation of agentic AI, suggesting that reducing akrasia should be a core goal in developing reliable agentic models.

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [38] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: The paper introduces MIND, a framework to enhance multimodal large language models (MLLMs) with more human-like, robust reasoning by integrating multiple rationales and active error correction, achieving SOTA performance on various reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs, while strong at reasoning, struggle with (1) weak semantic modeling across multiple rationales, (2) poor logical robustness, and (3) vulnerability to misleading or spurious reasoning in complex tasks. The authors aim to move from passive imitation of reasoning patterns (just copying training data styles) to an active, discriminative reasoning process that can understand, reassess, and correct its own reasoning, closer to human cognitive behavior.

Method: The authors propose MIND, a Multi-rationale INtegrated Discriminative reasoning framework. It has three main components: (1) Rationale Augmentation and Discrimination (RAD), which automatically expands datasets with diverse rationales (both correct and potentially incorrect) to create a unified, extensible training corpus; (2) Progressive Two-stage Correction Learning (P2CL), where stage one focuses on learning from multiple correct rationales, and stage two trains the model to actively discriminate and correct faulty logic; and (3) Multi-rationale Contrastive Alignment (MCA), a contrastive optimization strategy in the latent space that clusters correct rationales together while pushing incorrect rationales apart, reducing entanglement in multi-rationale semantics.

Result: Experiments across diverse public benchmarks in science, commonsense, and math reasoning show that MIND consistently improves the reasoning performance of MLLMs and achieves state-of-the-art results compared to prior approaches. The framework also improves logical robustness and reduces susceptibility to misleading rationales in complex scenarios.

Conclusion: MIND provides a new, discriminative, multi-rationale training and optimization paradigm that shifts MLLMs from passively imitating reasoning patterns to actively understanding, reassessing, and correcting reasoning. This leads to stronger, more human-like cognitive abilities and SOTA performance on multiple reasoning datasets, suggesting a promising direction for future research on cognitively enhanced MLLMs.

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [39] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: Proposes a modular Executor-Analyst framework for clinical LLM agents to fix failures in using retrieved biomedical evidence and achieves SOTA on CURE-Bench without finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing small-LLM clinical agents like TxAgent can retrieve biomedical evidence but often fail to ground diagnoses in that evidence, leading to poor reasoning and an information bottleneck when contexts and toolsets grow.

Method: Introduce a modular architecture separating tool execution (specialized TxAgents as Executors) from clinical reasoning (long-context foundation models as Analysts). Use orchestration between these modules plus a Stratified Ensemble strategy that maintains diverse evidence instead of globally pooling everything. Perform stress tests on context length and toolset size.

Result: The Executor-Analyst framework mitigates reasoning deficits of monolithic models, and the Stratified Ensemble outperforms global pooling by better preserving evidentiary diversity. Stress tests uncover a context-performance paradox beyond ~12k tokens and a curse of dimensionality in large tool action spaces that requires hierarchical retrieval.

Conclusion: Training-free architectural design—specifically the Executor-Analyst framework with stratified ensembling—can deliver state-of-the-art performance on CURE-Bench without costly finetuning, offering a scalable, trustworthy basis for future AI therapeutics systems.

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [40] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: The paper builds a benchmark (OntoAxiom) and evaluates how well various LLMs can automatically identify ontology axioms under different prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Ontology development is labor-intensive and requires both modelling and domain expertise. While ontology learning aims to automate parts of this process, a key remaining challenge is reliably identifying axioms that define logical relations between ontology elements. With the rise of powerful LLMs, it is unclear how well they can support or automate axiom identification, and there is a lack of systematic benchmarks for this task.

Method: The authors construct OntoAxiom, a benchmark dataset comprising nine medium-sized ontologies totaling 17,118 triples and 2,771 axioms, focusing on subclass, disjoint, subproperty, domain, and range axioms. They evaluate twelve LLMs using three shot settings and two prompting strategies: (1) a Direct approach that asks for all axioms at once, and (2) an Axiom-by-Axiom (AbA) approach where each prompt queries one axiom at a time. Performance is measured primarily via F1 scores, and results are analyzed by axiom type, ontology/domain, and model size.

Result: The Axiom-by-Axiom prompting strategy consistently yields higher F1 scores than the Direct strategy. Performance differs markedly across axiom types, indicating varying levels of difficulty, and also across domains: for instance, the FOAF ontology reaches an F1 of 0.642 for subclass axioms, while the music ontology achieves only 0.218. Larger LLMs generally perform better than smaller ones, but smaller models can still be competitive in constrained-resource scenarios. Overall performance, however, is not sufficient for fully automated axiom identification.

Conclusion: LLMs can meaningfully assist ontology engineers by suggesting candidate axioms, especially when prompted axiom-by-axiom and when using larger models, but their current accuracy is not adequate to replace human expertise. The OntoAxiom benchmark provides a systematic way to evaluate and compare models and prompting strategies for ontology axiom identification, highlighting which axiom types and domains are more challenging and guiding future improvements in ontology learning with LLMs.

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [41] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: They propose DeepDist, a new stochastic local search MaxSAT solver with a clause-weighting and decimation scheme that treats PMS and WPMS differently, achieving state-of-the-art performance and beating recent competition winners when hybridized.


<details>
  <summary>Details</summary>
Motivation: Existing SLS-based solvers for Partial MaxSAT (PMS) and Weighted Partial MaxSAT (WPMS) mostly focus on generic clause weighting but do not properly account for structural differences between PMS and WPMS. They typically apply the same weight update rules to both, and do not leverage problem-specific initialization or variable fixing (decimation) strategies, limiting performance on diverse benchmarks.

Method: 1) Design a novel clause weighting scheme that uses different update conditions for PMS and WPMS, explicitly leveraging their structural differences. 2) Propose a new clause-weight initialization method tailored to both PMS and WPMS types. 3) Introduce a decimation procedure that prioritizes satisfying unit clauses and hard clauses, to complement the clause weighting and guide search. 4) Integrate these techniques into a new stochastic local search solver, DeepDist, for (W)PMS. 5) Evaluate DeepDist on standard MaxSAT Evaluation anytime-track benchmarks and also build a hybrid solver that combines DeepDist with TT-Open-WBO-Inc.

Result: DeepDist empirically outperforms state-of-the-art SLS solvers on benchmarks from recent MaxSAT Evaluations anytime tracks. Additionally, the hybrid solver formed by combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winning solvers SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS.

Conclusion: Differentiating between PMS and WPMS in clause-weight updates, using tailored initialization, and adding a decimation phase that targets unit and hard clauses significantly improves SLS performance on (W)PMS. The resulting solver DeepDist, especially when used in a hybrid setup, achieves or exceeds the performance of leading competition winners, demonstrating the effectiveness and practical value of the proposed techniques.

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [42] [KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books](https://arxiv.org/abs/2512.05734)
*Jinfeng Zhong,Emmanuel Bacry,Agathe Guilloux,Jean-François Muzy*

Main category: cs.AI

TL;DR: KANFormer is a deep learning model that predicts the time until a limit order is filled by combining market and agent information, outperforming prior models and remaining interpretable via SHAP.


<details>
  <summary>Details</summary>
Motivation: Existing time-to-fill prediction models mainly use limit order book snapshots and ignore important agent-level actions and queue position, limiting their ability to accurately estimate execution likelihood and timing. There is a need for models that better capture both market microstructure and trader behavior while providing calibrated, discriminative, and interpretable predictions of fill probabilities.

Method: The paper proposes KANFormer, which integrates a Dilated Causal Convolutional Network with a Transformer encoder, augmented by Kolmogorov-Arnold Networks (KANs) to enhance nonlinear function approximation. The model jointly ingests market-level signals, agent-related actions tied to LOB dynamics, and order queue position. It is trained and evaluated on CAC 40 index futures data with labeled orders, and its performance is assessed on both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination metrics (C-index, time-dependent AUC). SHAP values are used to analyze feature importance over time.

Result: KANFormer achieves superior performance compared with existing approaches, improving both calibration and discrimination metrics for time-to-fill prediction on CAC 40 futures data. SHAP-based analysis reveals which market and agent features drive predictions over time, demonstrating that the combined use of rich limit order book signals, agent actions, and queue position leads to more accurate execution probability estimates.

Conclusion: Combining expressive neural architectures (dilated causal CNNs, Transformers, KANs) with both market- and agent-level information yields more accurate and better calibrated predictions of limit-order fill times than prior methods. The approach also provides interpretable insights into which features most influence execution likelihood, suggesting a promising direction for microstructure modeling and order execution analytics.

Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.

</details>


### [43] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: Proposes FARDA, a deep reinforcement learning-based framework for fast anti-jamming radar deployment that matches evolutionary algorithms in coverage while being ~7,000× faster.


<details>
  <summary>Details</summary>
Motivation: Rapid and effective deployment of cognitive radars to counter jamming is crucial in modern warfare, but current evolutionary algorithm-based methods are slow and can get trapped in local optima, limiting timely target detection.

Method: Model the radar deployment as an end-to-end optimization task and solve it using deep reinforcement learning. The approach introduces integrated neural modules to perceive heatmap information of the environment and a newly designed reward formulation tailored to anti-jamming radar coverage.

Result: Empirical experiments show that FARDA achieves target coverage performance comparable to existing evolutionary algorithm baselines, while realizing about 7,000× speedup in radar deployment. Ablation studies validate the importance of each architectural and algorithmic component in FARDA.

Conclusion: Deep reinforcement learning, instantiated in the FARDA framework, is a highly efficient alternative to evolutionary algorithms for anti-jamming radar deployment, delivering similar coverage quality with dramatically improved deployment speed, and each designed component is necessary for its strong performance.

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [44] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: The paper proposes an evolutionary framework (ERO) that treats LLMs as a population and evolves them to improve general reasoning ability, revealing that even very advanced LLMs still have limited system-2 reasoning, while weaker models can gain strong reasoning through evolution.


<details>
  <summary>Details</summary>
Motivation: Although large language models perform well on many tasks, they still lack robust, general "system 2" (slow, deliberative) reasoning similar to humans. The authors want to know whether such reasoning ability can be *evolved* in LLMs, rather than merely trained for narrow skills, addressing the gap between task-specific performance and true machine intelligence.

Method: They design Evolutionary Reasoning Optimization (ERO), where multiple LLM instances form a population. For a given reasoning benchmark, the method initializes this population, evaluates individuals with a quantified reasoning score, and then applies an evolutionary strategy (e.g., selection of the best, variation, and replacement) to iteratively evolve the population toward stronger reasoning performance.

Result: Experiments on representative reasoning test suites show: (1) even cutting-edge models (e.g., GPT-5 in their setting) still exhibit limited system-2 reasoning; (2) applying the ERO evolution loop to a comparatively weak model (Qwen-7B) can significantly boost its reasoning capabilities, to the point where strong reasoning behavior emerges.

Conclusion: General machine intelligence is still out of reach for current LLMs, but their reasoning ability can be substantially improved through an evolutionary framework. ERO demonstrates that evolving a population of models based on reasoning scores can yield individuals with much stronger system-2 reasoning, suggesting evolution as a promising path toward more human-like machine intelligence.

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [45] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: The paper argues LLMs aren't dead ends for AGI; the missing piece is a System-2 style coordination layer, formalized as UCCT and instantiated architecturally as MACI.


<details>
  <summary>Details</summary>
Motivation: There is a widespread critique that LLMs, as pattern matchers, cannot support genuine reasoning, planning, or AGI. The authors want to reframe this critique, identifying not the pattern store but the lack of coordination/anchoring as the bottleneck, and to provide a formal and architectural framework to address it.

Method: Conceptually, they introduce UCCT, a theory of semantic anchoring for reasoning, characterized by effective support, representational mismatch, and an adaptive anchoring budget. They interpret reasoning as a phase transition induced by these parameters. Practically, they propose MACI, a coordination stack that implements three mechanisms: baiting through behavior-modulated debate, filtering via Socratic judging, and persistence through transactional memory, which together orchestrate LLM pattern use.

Result: UCCT offers a formal lens where ungrounded generation is maximum-likelihood retrieval, and reasoning is posterior shifting under constraints. MACI translates this into an implementable architecture that can be used to test objections to LLM-based AGI as specific forms of coordination failure rather than fundamental limitations of pattern-matching substrates.

Conclusion: LLMs provide a suitable System-1 substrate for AGI if paired with an appropriate System-2 coordination layer. By modeling and implementing such a layer through UCCT and MACI, the paper contends that progress toward AGI will proceed by augmenting LLMs with better coordination, rather than abandoning them for entirely different paradigms.

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [46] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: The paper presents a multimodal AI agent that predicts IDH1 mutation status in low-grade glioma by combining histology image features with structured clinical and genomic data plus external biomedical knowledge, achieving higher F1-scores than using histology or clinical data alone.


<details>
  <summary>Details</summary>
Motivation: IDH1 mutation status is a crucial biomarker in low-grade gliomas with important prognostic and therapeutic implications, yet existing prediction methods typically rely on single data modalities (e.g., histology alone or clinical/genomic data alone). There is a need for models that can integrate heterogeneous patient data and external biomedical knowledge to more accurately infer mutation status and support precision oncology. This work aims to address that gap by building an agent that reasons over multiple modalities and knowledge sources.

Method: The authors design a Multimodal Oncology Agent (MOA) that integrates: (1) a histology prediction tool based on the TITAN foundation model to extract image-derived features relevant to IDH1 mutation; and (2) a reasoning component that consumes structured clinical and genomic variables together with information retrieved from external biomedical resources such as PubMed, Google Search, and OncoKB. They evaluate MOA on 488 TCGA-LGG patients, comparing variants of the agent (with and without histology features) against baseline models using only clinical data, only histology, or fused histology-clinical features. Performance is measured primarily by F1-score for IDH1 mutation prediction.

Result: On the TCGA-LGG cohort, MOA without the histology tool already surpasses the clinical baseline, with an F1-score of 0.826 versus 0.798. When histology features from the TITAN-based tool are fused into MOA, performance further improves, reaching an F1-score of 0.912. This outperforms the histology-only baseline (0.894) and the fused histology-clinical baseline (0.897), indicating that MOA leverages complementary information from multimodal data and external knowledge sources to improve IDH1 mutation prediction.

Conclusion: The study concludes that the proposed Multimodal Oncology Agent effectively integrates histology, structured clinical/genomic data, and external biomedical knowledge to enhance prediction of IDH1 mutation status in low-grade glioma. Its superior F1-score relative to strong baselines suggests that such agent-based, knowledge-enriched multimodal approaches can capture mutation-relevant signals that single-modality or simple fusion models miss, supporting more accurate molecular characterization and potentially better-informed clinical decision-making in neuro-oncology.

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [47] [Using Large Language Models to Create Personalized Networks From Therapy Sessions](https://arxiv.org/abs/2512.05836)
*Clarissa W. Ong,Hiba Arnaout,Kate Sheehan,Estella Fox,Eugen Owtscharow,Iryna Gurevych*

Main category: cs.AI

TL;DR: The paper develops and evaluates an LLM-based pipeline that automatically turns therapy transcripts into clinically meaningful psychological process networks to support personalized treatment planning.


<details>
  <summary>Details</summary>
Motivation: Personalized psychotherapy increasingly relies on individualized symptom or process networks to choose treatment modules. Current approaches usually need intensive longitudinal data and complex modeling, which are impractical in many real-world clinical settings. There is a need for scalable, data-light methods that can still produce rich, clinically useful case conceptualizations and guide treatment personalization. LLMs, with their ability to extract structure and meaning from text, offer a potential solution, but it is unclear whether they can generate reliable, interpretable clinical networks from routine therapy transcripts.

Method: The authors collected 77 therapy transcripts and manually annotated 3364 instances of psychological processes and associated dimensions. They used these annotations to perform in-context learning with an LLM that jointly identifies psychological processes and their dimensions directly from transcript text, testing performance with only a small number of examples. To convert the extracted processes into networks, they proposed a two-step procedure: (1) clustering processes into clinically meaningful groups, and (2) generating explanation-augmented relationships between these clusters to form a network. They compared this multi-step pipeline to a simpler direct-prompting approach for network generation. Expert clinicians evaluated the resulting networks on dimensions such as clinical utility, interpretability, relevance, novelty, and usefulness.

Result: The in-context learning setup achieved strong performance for jointly identifying psychological processes and dimensions even when only a few training examples were provided, suggesting sample efficiency. The proposed two-step network construction approach produced networks that clinicians judged to be more clinically useful and interpretable than networks generated via direct prompting, with up to 90% of experts preferring the multi-step method. Expert ratings for the networks’ clinical relevance, novelty, and usefulness were high, falling in the 72–75% range. These findings indicate that an LLM-based pipeline can yield coherent, clinically meaningful process networks from routine therapy transcripts.

Conclusion: The study offers a proof of concept that LLMs can be used to automatically build clinically relevant psychological process networks from therapy session transcripts, without requiring intensive longitudinal measurement. The bottom-up nature of the approach allows case conceptualizations to emerge from clients’ own utterances and can highlight latent themes that may guide treatment planning. The generated networks appear suitable for use in clinical practice, supervision, and training. The authors recommend future work to test whether such LLM-derived networks actually improve treatment outcomes and how they compare directly with networks derived from traditional statistical estimation methods for treatment personalization.

Abstract: Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.

</details>


### [48] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: The paper builds and evaluates an AI-based checker (using GPT-5) that automatically finds and often fixes objective mistakes in published AI papers, showing such errors are common and growing over time.


<details>
  <summary>Details</summary>
Motivation: Published, peer‑reviewed AI papers can contain objective technical mistakes (e.g., in formulas or calculations) that then propagate through the literature, harming clarity and reproducibility. With research volume growing and peer review under strain, such errors are increasingly hard for humans to catch. The authors want to quantify how many objective mistakes exist in top AI venues and assess whether advanced LLMs can systematically detect and correct them.

Method: The authors develop a Paper Correctness Checker built on GPT-5 to scan previously published papers at leading AI conferences and journals. The system is designed to focus only on objectively verifiable mistakes (formulas, derivations, calculations, figures, tables) and explicitly ignore subjective aspects (novelty, significance, writing quality). It flags potential issues, which are then manually reviewed by human experts to determine which are real errors and to measure the checker’s precision. They also assess the model’s ability to propose correct fixes for the confirmed mistakes and analyze error statistics over time and across venues (NeurIPS, ICLR, TMLR).

Result: The study finds a substantial and increasing number of objective mistakes per paper in top AI venues. Average mistakes per paper rose from 3.8 to 5.9 in NeurIPS between 2021 and 2025 (55.3% increase), from 4.1 to 5.2 in ICLR between 2018 and 2025, and from 5.0 to 5.5 in TMLR between 2022/23 and 2025. Out of 316 candidate mistakes flagged by the AI Checker, human experts confirmed 263 as genuine, corresponding to an 83.2% precision. The system also correctly suggested fixes for 75.8% of the identified mistakes. Most errors were minor but some could materially affect result interpretation.

Conclusion: Published AI papers at top venues frequently contain nontrivial numbers of objective technical mistakes, and this problem appears to be worsening over time. A frontier LLM‑based Paper Correctness Checker can reliably detect many of these errors with reasonably high precision and can also propose correct corrections for most of them. Such tools could support authors, reviewers, and editors in improving the accuracy and reproducibility of the AI literature and help maintain a more reliable scientific foundation.

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [49] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE is a framework to evaluate not just final answers but the reasoning steps of large vision-language models, using small auxiliary sub-questions to measure consistency and identify where reasoning fails.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation of vision-language models focuses mainly on final answers, which can hide internal reasoning errors and silent failures. There is a need for a more fine-grained, transparent way to assess and debug reasoning processes, not just outcomes.

Method: TRACE introduces Auxiliary Reasoning Sets (ARS), which break complex problems into compact sub-question–answer pairs. It evaluates the model’s intermediate steps through consistency-based metrics across these ARS, and uses these signals to characterize reasoning trajectories and define confidence regions for reliable vs. unreliable reasoning paths.

Result: Experiments demonstrate that higher consistency across ARS is strongly correlated with final-answer correctness. TRACE can localize which specific reasoning steps fail and can distinguish between more and less trustworthy reasoning paths via the defined confidence regions.

Conclusion: TRACE provides a practical framework for transparent diagnosis and evaluation of reasoning in large vision-language models, uncovering errors missed by standard final-answer metrics and offering actionable guidance for filtering, debugging, and improving such models.

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [50] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: The paper proposes a quantum-enhanced Rainbow DQN (VQR-DQN) for NP-hard human resource allocation and shows it outperforms strong classical DRL baselines on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Classical deep RL methods like Rainbow DQN struggle with very large combinatorial action spaces in NP-hard resource allocation problems due to limited function approximation capacity. The authors aim to exploit quantum computation (superposition, entanglement) to improve representational power and solution quality for large-scale human resource allocation problems.

Method: Formulate the human resource allocation problem as an MDP with combinatorial actions based on officer skills, event schedules, and transition times. Replace the classical function approximator in Rainbow DQN with a variational quantum circuit arranged in a ring topology, yielding the Variational Quantum Rainbow DQN (VQR-DQN). Use prioritized replay and distributional value heads as in Rainbow, but with quantum circuit-based state-action value estimation. Analyze connections between quantum circuit expressibility, entanglement, and learned policy quality.

Result: On four human resource allocation benchmarks, VQR-DQN reduces normalized makespan by 26.8% versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9–13.4% across tasks. Empirical results correlate higher circuit expressibility and entanglement with better policy performance.

Conclusion: Integrating variational quantum circuits into Rainbow DQN improves policy quality on combinatorial human resource allocation tasks, suggesting that quantum-enhanced DRL can better handle large, complex resource allocation problems than classical counterparts. The observed performance gains and theoretical links between circuit properties and policy quality indicate significant promise for quantum DRL in large-scale optimization, and the implementation is publicly available for further research.

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [51] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: A new large-scale synthetic benchmark, SymPyBench, offers 15,045 parameterized university-level physics problems with code-generated ground-truth solutions, multiple question formats, and new evaluation metrics to assess language models' scientific reasoning.


<details>
  <summary>Details</summary>
Motivation: To systematically and rigorously evaluate and improve the scientific reasoning capabilities of instruction-tuned language models using a controllable, extensible, and diverse set of physics problems.

Method: They construct a synthetic dataset of 15,045 fully parameterized university-level physics problems with three question formats (MC-Symbolic, MC-Numerical, free-form), each paired with structured step-by-step solutions and executable Python code that computes ground-truth answers. They leverage the parameterization and code to define three novel evaluation metrics—Consistency Score, Failure Rate, and Confusion Rate—that capture models' performance variability and uncertainty across different instances of the same problem template. They then run experiments with state-of-the-art instruction-tuned LLMs on this benchmark.

Result: The benchmark is successfully built and used to evaluate contemporary instruction-tuned language models, which show both notable capabilities and clear failure modes in scientific reasoning under the new metrics and problem formats.

Conclusion: SymPyBench provides a principled, extensible testbed that exposes strengths and weaknesses of existing language models in university-level physics reasoning, and can serve as a foundation for building more robust, reliable, and interpretable scientific reasoning systems.

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>
