{"id": "2602.15067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15067", "abs": "https://arxiv.org/abs/2602.15067", "authors": ["Rut Pate", "Snehal Rajput", "Mehul S. Raval", "Rupal A. Kapdi", "Mohendra Roy"], "title": "Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis", "comment": null, "summary": "Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.", "AI": {"tldr": "The paper proposes a triplanar Attention-Gated Recurrent Residual U-Net (R2U-Net) model for 2.5D brain tumor segmentation that achieves competitive Dice score on BraTS2021 and uses learned features for survival prediction.", "motivation": "Gliomas are highly heterogeneous in aggressiveness, prognosis, and histology, which makes treatment planning difficult. Accurate and efficient automatic segmentation of brain tumors is critical to assist complex and time-consuming surgical interventions and downstream tasks such as survival prediction, yet existing methods struggle to balance accuracy, rich feature representation, and computational efficiency.", "method": "The authors design an Attention-Gated Recurrent Residual U-Net (R2U-Net) configured in a triplanar (2.5D) fashion. The network processes MRI slices from three orthogonal planes, combining residual connections, recurrent units, and attention gates to enhance feature representation and focus on tumor regions. The triplanar setup yields 64 features from each planar model for survival prediction. These high-dimensional features are then reduced to 28 through an Artificial Neural Network (ANN) used as a feature reduction or regression module for predicting survival days.", "result": "On the BraTS2021 validation set, the proposed model achieves a Dice Similarity Coefficient (DSC) of 0.900 for whole tumor segmentation, which is comparable to state-of-the-art methods. For survival prediction, using the extracted and reduced features, the model attains 45.71% accuracy, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.", "conclusion": "The triplanar Attention-Gated R2U-Net architecture effectively improves whole tumor segmentation performance while remaining computationally efficient. Furthermore, features derived from this segmentation network can be leveraged for survival prediction, yielding moderate predictive performance. Overall, the method shows promise for aiding brain tumor delineation and treatment planning, though survival prediction accuracy indicates room for further improvement."}}
{"id": "2602.15112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15112", "abs": "https://arxiv.org/abs/2602.15112", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Arman Cohan"], "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research", "comment": null, "summary": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.", "AI": {"tldr": "ResearchGym is a benchmark and execution environment to evaluate autonomous AI research agents on end-to-end ML research tasks using real paper codebases, revealing a large capability\u2013reliability gap.", "motivation": "Existing benchmarks rarely test whether AI agents can conduct full research workflows\u2014forming hypotheses, running experiments, and beating strong baselines\u2014on realistic, complex ML projects. There is a need for a standardized, reproducible environment that captures the open-ended, long-horizon nature of research and exposes failure modes of frontier agents.", "method": "The authors construct ResearchGym by repurposing five oral/spotlight ML/NLP papers from ICML, ICLR, and ACL. From each paper\u2019s repository, they keep datasets, evaluation code, and baseline implementations but remove the proposed new method. Each paper becomes a containerized task environment, together comprising 39 sub-tasks. Agents must interact with these environments by proposing hypotheses, modifying code, running experiments, and aiming to outperform the human baselines on the same metrics. They then run controlled evaluations of several frontier, tool-augmented agents (e.g., GPT-5, Claude Code, Codex) and record success rates, performance improvements, and qualitative failure modes.", "result": "In controlled evaluations, a GPT-5-based agent beats the provided baselines in only 1 out of 15 evaluations (6.7%), with an 11.5% improvement when it succeeds, and completes just 26.5% of sub-tasks on average. Similar performance patterns are observed for other proprietary agent scaffolds (Claude Code, Codex), indicating that current agents are far from reliably solving end-to-end research tasks, despite having strong local capabilities.", "conclusion": "ResearchGym exposes a stark capability\u2013reliability gap for current frontier AI agents on realistic research problems. Agents can occasionally achieve state-of-the-art performance\u2014even surpassing an ICML 2025 Spotlight solution in a single run\u2014but they do so inconsistently and exhibit recurring long-horizon failure modes such as impatience, poor resource management, overconfidence, weak parallelization, and context-length limits. The benchmark offers a standardized infrastructure for systematically studying and improving autonomous research agents in closed-loop settings."}}
{"id": "2602.15143", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15143", "abs": "https://arxiv.org/abs/2602.15143", "authors": ["Xinhang Ma", "William Yeoh", "Ning Zhang", "Yevgeniy Vorobeychik"], "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting", "comment": null, "summary": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.", "AI": {"tldr": "The paper proposes methods to rewrite LLM reasoning traces so they remain correct for end\u2011users but are less useful for training copycat models, and can embed detectable watermarks in any models distilled from them.", "motivation": "Frontier LLMs are expensive to train, but their capabilities can be copied via unauthorized knowledge distillation by logging their reasoning traces and using them as training data for smaller student models. Providers want mechanisms to discourage such misuse and to detect when their APIs have been used for distillation, without harming utility for legitimate users. Existing defenses either hurt performance or are hard to verify, motivating new approaches that subtly poison distillation data and embed verifiable signatures.", "method": "The authors propose dynamically rewriting the teacher model\u2019s chain-of-thought or reasoning traces before they are exposed via an API. They design several rewriting strategies that preserve final answer correctness and semantic coherence but change internal reasoning patterns. Two families of methods are explored: (1) LLM-based rewriting, where another model is instructed to paraphrase or systematically alter reasoning according to anti-distillation or watermarking objectives; and (2) gradient-based techniques that perturb representations so that the resulting traces are adversarial for student training yet still human-plausible. These rewritten traces are then used in distillation experiments to quantify their impact and to evaluate watermark detectability in trained students.", "result": "Empirically, a simple instruction-driven rewriting strategy already significantly reduces how much a student model can benefit from distillation compared to using original traces, while the teacher\u2019s own accuracy is preserved or slightly improved (likely due to regularization or clarity effects). In parallel, the same rewriting scheme can introduce statistical patterns that serve as a watermark: models trained on the modified traces reliably exhibit this signature, enabling detection of unauthorized distillation with high recall and almost zero false positives when tested on models not trained on the watermarked data.", "conclusion": "Carefully designed rewriting of reasoning traces is an effective and practical defense against unauthorized knowledge distillation. It can simultaneously (1) degrade the training value of leaked traces for student models without harming user-facing answer quality and (2) embed robust, verifiable watermarks in any models trained on those traces. This suggests that API providers can defend their IP and audit misuse primarily through output-side modifications, without needing to modify the underlying teacher weights or disrupt normal user experience."}}
{"id": "2602.15156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15156", "abs": "https://arxiv.org/abs/2602.15156", "authors": ["Shreyas Rajesh", "Pavan Holur", "Mehmet Yigit Turali", "Chenda Duan", "Vwani Roychowdhury"], "title": "Panini: Continual Learning in Token Space via Structured Memory", "comment": "35 pages, code available at: https://github.com/roychowdhuryresearch/gsw-memory", "summary": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.", "AI": {"tldr": "Panini is a non-parametric continual learning framework that converts documents into structured semantic memory (QA-based networks) so an LLM can answer questions more accurately and efficiently than RAG, using fewer tokens and reducing hallucinations.", "motivation": "Standard RAG repeatedly reprocesses the same raw document chunks and often pulls in irrelevant text, wasting compute and increasing hallucinations. The authors want a more human-like, continual way to accumulate knowledge externally without retraining the base model, while improving efficiency and answer reliability over time.", "method": "They keep the base language model fixed and introduce an external semantic memory called Generative Semantic Workspaces (GSW). Incoming documents are converted, at write time, into structured networks of entity- and event-centric QA pairs that capture the underlying situations and latent knowledge. At inference, instead of re-reading verbatim documents, Panini traverses this continually-updated GSW to find likely reasoning chains of QA pairs relevant to the query, and only retrieves those chains for the LLM to use in answering.", "result": "On six QA benchmarks, Panini outperforms strong baselines by 5\u20137% on average while using 2\u201330x fewer context tokens for answers. It also reduces unsupported (hallucinatory) answers on curated unanswerable questions and is implemented with fully open-source components.", "conclusion": "Structuring experiences into semantic QA-based memory at write time enables more efficient and reliable test-time reasoning than traditional chunk-based RAG. A fixed LLM coupled with GSW-style external memory can achieve better accuracy, lower token usage, and fewer hallucinations, demonstrating the promise of human-like non-parametric continual learning for LLMs."}}
{"id": "2602.15158", "categories": ["cs.AI", "cs.IR", "math.LO"], "pdf": "https://arxiv.org/pdf/2602.15158", "abs": "https://arxiv.org/abs/2602.15158", "authors": ["Gabriel Rocha"], "title": "da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems", "comment": "22 pages, 5 figures, 1 table", "summary": "This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and L\u00fccke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.", "AI": {"tldr": "The paper proposes a new logical framework (da Costian-Tarskianism) to systematically relate and manage different ontologies using consequence systems extended with ontological axioms and graph-based relations between them.", "motivation": "Ontological heterogeneity\u2014having multiple, structurally different ontologies for overlapping domains\u2014is a central problem in applied ontology, semantic technologies, and knowledge representation. Existing approaches, such as Carnapian-Goguenism, provide ways to compare and relate theories but may not fully capture the logical and ontological relations needed for complex, heterogeneous systems. The authors are motivated to build a more general and principled framework that can formally relate different ontologies, support operations on them, and clarify how ontological assumptions interact with underlying logics.", "method": "The authors adapt and extend the framework of consequence systems, drawing on work by Carnielli et al. and Citkin & Muravitsky. They define an \"extended consequence system\" by adding explicit ontological axioms to a base consequence system. Building on this, they introduce an \"extended development graph\"\u2014a graph where nodes are extended consequence systems (i.e., logics plus ontological axioms), and edges are morphisms between them or other structure-preserving operations. The framework also supports additional operations such as fibring (combining logics/ontologies) and splitting (decomposing them). Conceptually, this is put under the umbrella of a new stance, da Costian-Tarskianism, which emphasizes tolerance (\u00e0 la da Costa) about ontological choices and rigor in consequence (\u00e0 la Tarski).", "result": "The paper delivers: (1) a formal definition of extended consequence systems (consequence systems enriched with ontological axioms); (2) a formal definition of extended development graphs for representing and relating such systems; and (3) a set of operations (e.g., morphisms, fibring, splitting) that can be used to connect and transform ontologies within this framework. These constructions generalize and complement earlier Carnapian-Goguen style approaches to heterogeneity by making ontological axioms first-class citizens in the logical machinery.", "conclusion": "The authors conclude that da Costian-Tarskianism, implemented via extended consequence systems and extended development graphs, provides a promising and flexible framework for managing ontological heterogeneity in applied ontology. It offers a principled way to model, relate, and combine different ontologies while keeping track of both logical consequence and ontological commitments. They argue that this approach can inform future work in ontology integration, modular ontology design, and heterogeneous knowledge representation, and they outline directions for extending the formalism and applying it in practical ontology engineering settings."}}
{"id": "2602.15173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15173", "abs": "https://arxiv.org/abs/2602.15173", "authors": ["Luise Ge", "Yongyan Zhang", "Yevgeniy Vorobeychik"], "title": "Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs", "comment": null, "summary": "The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.", "AI": {"tldr": "The paper compares how large language models (LLMs) make risky decisions under uncertainty, contrasting them with humans and a rational payoff-maximizing model, and finds two distinct behavioral clusters tied to mathematical reasoning training.", "motivation": "LLMs are increasingly used in decision-support and autonomous agent settings, often in situations involving risk and uncertainty, yet their decision behavior in such contexts is poorly understood. Understanding whether LLMs behave more like rational agents, like humans with known biases, or in some other way is important for safely deploying them in consequential domains.", "method": "The authors run controlled decision-making experiments involving risky choices on 20 state-of-the-art and open LLMs. They systematically vary (1) how risky prospects are represented\u2014either described explicitly as probabilities and outcomes or presented via experiential histories\u2014and (2) whether the models are asked to provide explanations for their choices. They then compare the LLMs\u2019 choices to those of human subjects in matched experiments and to a benchmark expected payoff\u2013maximizing model, and analyze clustering patterns in model behavior.", "result": "The study identifies two behavioral clusters of LLMs: reasoning models (RMs) and conversational models (CMs). RMs behave more like rational expected-payoff maximizers: they are largely insensitive to prospect order, gain/loss framing, explanations, and whether information is given as descriptions or experiential histories. CMs deviate more from rationality, display more human-like biases, are sensitive to order, framing, and explanation prompts, and show a notable gap between choices based on explicit descriptions versus experiential histories. Open LLM comparisons suggest that mathematical reasoning\u2013oriented training strongly correlates with RM-like behavior.", "conclusion": "LLMs do not behave uniformly under risk; instead, they fall into at least two distinct behavioral types. Models trained heavily for mathematical and structured reasoning tend to make more stable, rational, and representation-invariant risky choices, whereas models oriented toward conversational abilities exhibit more biases and sensitivity to framing, closer to human decision-making quirks. This has implications for selecting and training LLMs for decision-support and agentic applications where robustness and predictability under uncertainty are critical."}}
{"id": "2602.15212", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15212", "abs": "https://arxiv.org/abs/2602.15212", "authors": ["Yuanyan Song", "Kezhi Wang", "Xinmian Xu"], "title": "Secure and Energy-Efficient Wireless Agentic AI Networks", "comment": "Submitted to journal", "summary": "In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.", "AI": {"tldr": "The paper proposes a secure wireless agentic AI network where a supervisor AI assigns agents for cooperative reasoning while unused agents act as friendly jammers, jointly optimizing agent selection, BS beamforming, and power to minimize energy under QoS, latency, and accuracy constraints, solved via optimization (ASC) and an LLM-based workflow (LAW), achieving up to 59.1% energy savings and validated on a Qwen-based system.", "motivation": "AI reasoning tasks over wireless networks must ensure user QoS and privacy (confidentiality of knowledge and reasoning outputs) while operating under strict energy, latency, and accuracy constraints. Existing works on wireless communication, edge computing, and federated/agentic AI typically treat communication, security, and reasoning separately, or do not co-optimize wireless resources with agentic workflows. There is a need for a unified framework that securely orchestrates multiple AI agents over wireless links, defends against eavesdroppers, and efficiently uses energy while meeting QoS for reasoning tasks.", "method": "The authors design a wireless agentic AI architecture with one supervisor agent and multiple worker agents. For each task, the supervisor selects a subset of agents for cooperative reasoning and assigns the rest as friendly jammers to interfere with an eavesdropper. They formulate a joint optimization problem that minimizes total energy consumption by optimizing (1) AI agent selection for reasoning vs jamming, (2) base station beamforming, and (3) AI agent transmit powers, under constraints on latency and reasoning accuracy (QoS) and physical-layer security.\nThey then propose two solution schemes, ASC and LAW, both decomposing the original mixed-integer nonconvex problem into three subproblems. ASC solves them with classical optimization techniques: an ADMM-based algorithm for agent selection, semidefinite relaxation (SDR) for beamforming, and successive convex approximation (SCA) for power control, iterating until convergence. LAW instead uses a large language model (LLM) optimizer embedded in an agentic workflow to approximately solve the subproblems and coordinate decisions. Both approaches are evaluated via simulations and a practical implementation on a Qwen-based agentic AI system.", "result": "Simulation and system-level experiments show that the proposed schemes significantly reduce network energy consumption\u2014up to 59.1% compared with benchmark resource allocation or non-cooperative baselines\u2014while satisfying latency and reasoning accuracy constraints. The friendly jamming strategy effectively degrades the eavesdropper\u2019s interception capability without sacrificing QoS for legitimate users. The Qwen-based implementation demonstrates that the architecture and optimization schemes are practically realizable and maintain satisfactory reasoning accuracy across multiple public benchmarks, indicating that physical-layer optimization does not harm AI task performance.", "conclusion": "The paper concludes that integrating secure physical-layer design with agentic AI orchestration in wireless networks can substantially improve energy efficiency while preserving privacy and QoS for reasoning tasks. The proposed supervisor\u2013worker architecture with friendly jamming and joint optimization of agent selection, BS beamforming, and transmit power is effective against eavesdroppers and leads to large energy savings. Both optimization-based (ASC) and LLM-driven (LAW) resource allocation schemes are viable, with LAW highlighting the potential of using LLMs themselves as optimizers within agentic workflows. The work points toward future research on tighter integration of communication, security, and AI reasoning in practical wireless agentic systems."}}
{"id": "2602.15248", "categories": ["cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2602.15248", "abs": "https://arxiv.org/abs/2602.15248", "authors": ["Pavel Koptev", "Vishnu Kumar", "Konstantin Malkov", "George Shapiro", "Yury Vikhanov"], "title": "Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models", "comment": null, "summary": "Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.", "AI": {"tldr": "The paper proposes using AI and machine learning to better predict invoice/payment dilution risk in supply chain finance, reducing reliance on rigid payment guarantees (IPUs).", "motivation": "Invoice or payment dilution\u2014where the collected amount is less than the approved invoice amount\u2014is a major source of non-credit risk and margin loss in supply chain finance. Traditional management via irrevocable payment undertakings (IPUs) is restrictive and limits adoption, especially for sub-investment-grade buyers. There is a need for more flexible, data-driven risk tools.", "method": "The authors build and test an AI/ML framework on a large production dataset using nine key transaction fields, and assess how it can supplement an existing deterministic (rule-based) algorithm to predict invoice dilution at the buyer\u2013supplier level in real time.", "result": "The framework can project dilution risk at a more granular, buyer\u2013supplier pair level in real time and appears to improve upon or complement the deterministic algorithm, though specific metrics are not provided in the abstract.", "conclusion": "AI/ML-based dynamic credit limit setting, using transaction-level data, can serve as an effective supplement to traditional deterministic approaches and rigid IPUs, potentially enabling broader and more efficient adoption of supply chain finance."}}
{"id": "2602.15270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15270", "abs": "https://arxiv.org/abs/2602.15270", "authors": ["Farbod Abbasi", "Zachary Patterson", "Bilal Farooq"], "title": "Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models", "comment": "12 pages, 8 figures, 5 tables", "summary": "Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\\% and precision by 15\\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\\% increase in recall and 1\\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.", "AI": {"tldr": "They propose a WGAN-based method that jointly learns from multiple datasets to generate more diverse and feasible synthetic populations for agent-based models, outperforming sequential data fusion methods.", "motivation": "Existing synthetic population generators for transportation and urban planning typically rely on single data sources or sequentially fused data, which fails to capture complex feature interactions across datasets. Moreover, current methods inadequately handle sampling zeros (valid but unseen attribute combinations) and structural zeros (logically impossible combinations), leading to reduced diversity and infeasible agents in ABM inputs. A better approach is needed to jointly exploit multiple data sources while explicitly improving both diversity and feasibility of the generated data.", "method": "They design a multi-source generative framework based on Wasserstein GAN with gradient penalty (WGAN-GP) that jointly integrates and synthesizes data from multiple sources. The model is trained in a joint learning setting rather than sequential fusion, so it can learn cross-dataset feature dependencies. They modify the generator loss by adding a novel regularization term, called an inverse gradient penalty, to encourage better coverage of the data space and adherence to feasibility constraints. For evaluation, they define a unified similarity metric and emphasize diversity and feasibility, quantified by recall, precision, and F1 score over valid and feasible attribute combinations. They also evaluate similarity distributions using a composite five-metric score.", "result": "Compared with a sequential baseline method, the joint WGAN-based approach achieves higher diversity and feasibility of synthetic populations, with recall improving by 7% and precision by 15%. Adding the proposed regularization term further boosts performance, giving an additional 10% gain in recall and 1% in precision. On the composite five-metric similarity score, the joint approach scores 88.1 versus 84.6 for the sequential approach, indicating better overall similarity to real data while maintaining more diverse and feasible combinations.", "conclusion": "Joint multi-source learning via WGAN with the proposed inverse gradient penalty yields synthetic populations that are simultaneously more diverse, more feasible, and more similar to real data than those from sequential fusion methods. This improves the quality of inputs for agent-based models in transportation and urban planning, suggesting the approach can enhance the accuracy and reliability of ABM-based analyses and decisions."}}
{"id": "2602.15274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15274", "abs": "https://arxiv.org/abs/2602.15274", "authors": ["Omid Madani", "J. Brian Burns", "Reza Eghbali", "Thomas L. Dean"], "title": "When Remembering and Planning are Worth it: Navigating under Change", "comment": null, "summary": "We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.", "AI": {"tldr": "The paper studies how different memory strategies help an agent navigate to food in a changing, uncertain environment and shows that combining episodic memory, probabilistic learning, and on-the-fly mapping yields better performance than minimal-memory strategies when uncertainty is moderate.", "motivation": "To understand which forms of memory and learning are most effective for spatial navigation when the environment is non-stationary (barriers and goals move) and sensing is noisy and limited, which is common in real-world robotics and animal-like navigation tasks.", "method": "The authors design a simple daily foraging/navigation task where an agent must travel from home to food through barriers in a world that changes over days. They compare multiple agent architectures ranging from minimal-memory strategies to more sophisticated ones that use episodic memories, non-stationary probabilistic learning to update beliefs, and online map construction and planning under uncertainty. Performance is evaluated as task difficulty and uncertainty vary.", "result": "Architectures that combine multiple strategies\u2014especially those that use non-stationary probability learning to update episodic memory and derive on-the-fly, imperfect maps for planning\u2014achieve substantially higher efficiency than simpler, low-memory agents as task difficulty (e.g., distance to goal) increases, provided environmental and localization uncertainty are not too large.", "conclusion": "No single simple strategy suffices; an effective navigator in changing, uncertain environments needs an integrated architecture that supports both exploratory search and planned exploitation of remembered likely goal locations. Using probabilistic updates of episodic memory to build and plan on imperfect maps yields large gains in efficiency over minimal-memory approaches when uncertainty is moderate."}}
{"id": "2602.15034", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15034", "abs": "https://arxiv.org/abs/2602.15034", "authors": ["Houping Yue", "Zixiang Di", "Mei Jiang", "Bingdong Li", "Hao Hao", "Yu Song", "Bo Jiang", "Aimin Zhou"], "title": "EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research", "comment": "13 pages, 4 figures", "summary": "While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.", "AI": {"tldr": "Introduces EduResearchBench, a fine-grained benchmark and curriculum-trained model (EduWrite) for evaluating and improving LLMs in educational scholarly writing.", "motivation": "Current LLM benchmarks for academic writing focus on single-shot, holistic generation and cannot accurately capture the complexity and multi-step nature of real research workflows, making it difficult to diagnose specific strengths and weaknesses of LLMs in scholarly contexts.", "method": "Propose the Hierarchical Atomic Task Decomposition (HATD) framework that breaks the research workflow into six specialized research modules and 24 atomic tasks; build EduResearchBench on top of this taxonomy for automated, fine-grained evaluation; design a curriculum learning strategy that moves from basic skills to complex reasoning and argumentation; curate 11K high-quality instruction pairs from 55K academic samples to train EduWrite, a domain-specialized 30B-parameter model.", "result": "EduWrite, trained with the proposed curriculum on EduResearchBench-style tasks, significantly outperforms larger, general-purpose 72B-parameter models on multiple core metrics related to educational scholarly writing, indicating better mastery of the decomposed research tasks and complex workflows.", "conclusion": "Fine-grained, hierarchically decomposed benchmarks plus curriculum learning and high-quality domain data can yield specialized LLMs that surpass much larger general-purpose models in vertical domains like educational academic writing; capability in such domains depends more on data quality and training design than on sheer model size."}}
{"id": "2602.15294", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15294", "abs": "https://arxiv.org/abs/2602.15294", "authors": ["Ming Du", "Yanqi Luo", "Srutarshi Banerjee", "Michael Wojcik", "Jelena Popovic", "Mathew J. Cherukara"], "title": "EAA: Automating materials characterization with vision language model agents", "comment": null, "summary": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.", "AI": {"tldr": "Introduces Experiment Automation Agents (EAA), a VLM-driven agentic system that automates complex microscopy workflows via multimodal reasoning, tools, and flexible task management, demonstrated at a synchrotron beamline.", "motivation": "Running advanced microscopy and beamline experiments typically requires expert operators to manually configure instruments, focus optics, search for features, and manage complex acquisition routines. This is time-consuming, error-prone, and creates a high expertise barrier for new users. Existing automation often relies on rigid scripts that lack flexibility, natural-language interaction, and visual reasoning. The authors aim to leverage modern vision-language models and agentic frameworks to create a more adaptive, user-friendly, and powerful automation system for experimental facilities.", "method": "The authors design Experiment Automation Agents (EAA), an agentic framework powered by a vision-language model that can reason over multimodal inputs and control experimental tools. EAA uses a task-manager architecture that orchestrates agents capable of autonomous procedures or mixed-initiative workflows where humans can intervene via natural language. It integrates tool-augmented actions, optional long-term memory, and a tool ecosystem with two-way Model Context Protocol (MCP) compatibility, enabling both consumption and serving of instrument-control tools across different applications. They deploy EAA at an imaging beamline at the Advanced Photon Source to automate focusing, feature search, and data acquisition tasks driven by natural language and image understanding.", "result": "In deployment at an Advanced Photon Source imaging beamline, EAA successfully performs automated zone plate focusing, natural-language-specified feature search, and interactive data acquisition. The system can interpret user instructions, reason about visual and experimental context, and control the instruments accordingly. These demonstrations show that the architecture can handle both fully automated and logic-defined workflows with embedded LLM queries, and that the MCP-based tool ecosystem works in a real experimental setting.", "conclusion": "Experiment Automation Agents demonstrate that vision-language-model-based agents can reliably automate complex microscopy workflows at a synchrotron beamline. By combining multimodal reasoning, flexible task management, and a modern MCP-compatible tool ecosystem, EAA improves beamline efficiency, reduces operator workload, and lowers the expertise required to run sophisticated experiments. This suggests that similar agentic systems could broadly modernize experimental facilities by making advanced instrumentation more accessible and easier to operate."}}
{"id": "2602.15038", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15038", "abs": "https://arxiv.org/abs/2602.15038", "authors": ["Mihir Panchal", "Deeksha Varshney", "Mamta", "Asif Ekbal"], "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages", "comment": "19th Conference of the European Chapter of the Association for Computational Linguistics (EACL) Thirteenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) 2026", "summary": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.", "AI": {"tldr": "The paper introduces Indic-TunedLens, an interpretability framework tailored for Indian languages that improves decoding of multilingual LLM representations by learning language-specific affine transformations on hidden states, outperforming existing interpretability tools on MMLU across 10 Indic languages.", "motivation": "Most interpretability tools for large language models are English-centric and do not account for the multilingual, especially Indic, context. Prior work shows that multilingual LLMs often embed non-English languages in an English-centered representation space, making cross-lingual interpretability difficult and potentially misleading. There is a need for tools that more faithfully reveal how multilingual models represent and process Indian languages, especially low-resource, morphologically rich ones.", "method": "The authors propose Indic-TunedLens, an adaptation of the Logit Lens technique for multilingual, Indic-focused interpretability. Instead of directly decoding intermediate hidden activations with the final language modeling head, they learn shared affine transformations for each target Indian language that map layer-wise hidden states into a space aligned with that language's output distribution. These transformations are trained so that when applied to hidden states at different layers, the decoded distributions better match the model's actual predictions in that language. The framework is evaluated on 10 Indian languages using layer-wise probing on MMLU tasks.", "result": "Indic-TunedLens significantly outperforms state-of-the-art interpretability methods, including the standard Logit Lens, when applied to Indian languages on the MMLU benchmark. The gains are particularly notable for morphologically rich and low-resource Indic languages, suggesting more accurate and faithful decoding of intermediate representations. The framework additionally sheds light on how semantic information is encoded layer-wise in multilingual transformers for these languages.", "conclusion": "Indic-TunedLens demonstrates that language-specific affine adjustments to hidden states can substantially improve interpretability of multilingual LLMs for Indian languages over generic, English-centric tools. The approach offers more faithful decoding of model representations, especially for low-resource and morphologically complex languages, and yields insights into cross-lingual representation structure in multilingual transformers. The authors release both the model and code to facilitate further research in multilingual interpretability."}}
{"id": "2602.15139", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15139", "abs": "https://arxiv.org/abs/2602.15139", "authors": ["Tahir Hussain", "Saddam Hussain Khan"], "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding", "comment": "24 Pages, 9 Tables, 7 Figures", "summary": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.", "AI": {"tldr": "The paper introduces CGRA DeBERTa, a concept-guided, domain-augmented DeBERTa model that significantly improves question answering accuracy on Hadith texts through concept-aware residual blocks and gating, achieving 97.85 EM on a 42k-pair dataset from Sahih al-Bukhari and Sahih Muslim.", "motivation": "Classical Islamic texts pose challenges for QA due to domain-specific semantics, long-range dependencies, and concept-sensitive reasoning. Existing general-purpose transformers like BERT and vanilla DeBERTa do not fully capture the theological nuances and specialized concepts in Hadith corpora, leading to suboptimal extraction and less precise answers. There is a need for a model architecture that explicitly encodes key theological concepts and can scale to educational applications while remaining efficient and interpretable.", "method": "The authors propose CGRA DeBERTa, a transformer-based QA framework built on a customized DeBERTa backbone. They adapt the model with lightweight LoRA parameter-efficient fine-tuning, modify the embedding block to better capture global and positional context, and introduce Concept Guided Residual Blocks, which inject priors from an Islamic Concept Dictionary of 12 core theological terms. A Concept Gating Mechanism computes importance-weighted attention over tokens and selectively amplifies the representations of conceptually critical tokens using differential scaling factors between 1.04 and 3.00, implemented as a residual gating layer. The model is trained on a curated dataset of 42,591 QA pairs drawn from Sahih al-Bukhari and Sahih Muslim for span-extraction QA.", "result": "On the constructed Hadith QA dataset, baseline BERT reaches 75.87 exact match (EM) and vanilla DeBERTa 89.77 EM. CGRA DeBERTa achieves 97.85 EM, outperforming DeBERTa by 8.08 absolute EM points. The added computational cost is minimal, with around 8% inference overhead due to the lightweight gating and LoRA adaptations. Qualitative analysis suggests that the model provides more precise span extraction, better discrimination between similar concepts, and improved theological precision in answers.", "conclusion": "CGRA DeBERTa demonstrates that incorporating concept-guided residual augmentation and gating mechanisms, driven by a small curated theological concept dictionary, can substantially improve domain-specific QA over Hadith texts while maintaining efficiency. The resulting Hadith QA system is accurate, parameter-efficient, and interpretable, making it suitable for scalable educational tools that require careful theological nuance. The approach highlights the value of integrating structured domain knowledge into transformer architectures for specialized religious text understanding."}}
{"id": "2602.15325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15325", "abs": "https://arxiv.org/abs/2602.15325", "authors": ["Zhixing Zhang", "Jesen Zhang", "Hao Liu", "Qinhan Lv", "Jing Yang", "Kaitong Cai", "Keze Wang"], "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents", "comment": null, "summary": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.", "AI": {"tldr": "They build an LLM-based agent that writes and executes Python code in a unified agricultural modeling environment, enabling better reasoning over complex spatiotemporal farm data than text-only methods.", "motivation": "Existing agricultural foundation models handle large spatiotemporal datasets well but lack language-based reasoning and interactivity, while LLMs are good with text but cannot directly operate on high-dimensional heterogeneous agro-data. There is a need for a system that combines both strengths to support real-world agronomic workflows like diagnosis, forecasting, and decision support.", "method": "They construct AgriWorld, a unified Python execution environment exposing tools for geospatial queries, remote-sensing time-series analysis, crop growth simulation, and specialized predictors. On top of this, they design Agro-Reflective, a multi-turn LLM agent that repeatedly writes code, runs it, inspects the outputs, and refines its approach via an execute-observe-refine loop. They also build AgroBench, a benchmark with automatically generated agricultural QA tasks across lookups, forecasting, anomaly detection, and counterfactual analysis to evaluate the system.", "result": "In experiments on AgroBench, their agentic framework outperforms baselines that either only use text or use tools without the reflective execution loop, showing improved reliability and performance on agricultural reasoning tasks.", "conclusion": "Integrating an LLM agent with a rich agricultural tool environment and an execution-driven reflection loop substantially improves reasoning over complex agricultural datasets, making foundation models more practically useful for agronomic analysis and decision support."}}
{"id": "2602.15190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15190", "abs": "https://arxiv.org/abs/2602.15190", "authors": ["Herbert Ullrich", "Jan Drchal"], "title": "AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking", "comment": null, "summary": "In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.", "AI": {"tldr": "3rd place fact-checking system combining retrieval-augmented generation with reverse image search.", "motivation": "To build a simple, reproducible, and cost-efficient multimodal fact-checking system for the AVerImaTeC shared task, improving on last year's RAG pipeline by adding image capabilities.", "method": "Extend an existing RAG pipeline with a reverse image search (RIS) module, forming three decoupled modules: (1) textual retrieval via similarity search and vector stores, (2) image retrieval via API-based RIS, and (3) response generation using a single multimodal LLM call (GPT5.1 via OpenAI Batch API) per fact-check.", "result": "The system achieved 3rd place in the AVerImaTeC shared task, with competitive performance and an average cost of $0.013 per fact-check using one multimodal LLM call.", "conclusion": "A simple three-module architecture combining text retrieval, reverse image search, and a single LLM call is an effective, low-cost, and easily reproducible baseline for multimodal fact-checking; the authors release code, prompts, vector stores, and cost analyses to support further experimentation and improvement."}}
{"id": "2602.15384", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15384", "abs": "https://arxiv.org/abs/2602.15384", "authors": ["Zhouzhou Shen", "Xueyu Hu", "Xiyun Li", "Tianqing Fang", "Juncheng Li", "Shengyu Zhang"], "title": "World-Model-Augmented Web Agents with Action Correction", "comment": null, "summary": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.", "AI": {"tldr": "The paper proposes WAC, a multi-model web agent framework that collaborates, simulates consequences, and refines actions with feedback to improve risk-aware web task execution, achieving modest performance gains on web benchmarks.", "motivation": "Existing LLM-based web agents can automate web tasks but struggle to reason about the consequences of their actions and lack robust risk awareness. They often mispredict environment changes, execute risky actions too early (e.g., irreversible clicks, purchases, deletions), and thereby cause failures or losses. There is a need for agents that can better anticipate environment state transitions and operate more cautiously and reliably on the web.", "method": "The authors introduce WAC, a web agent architecture combining: (1) multi-agent model collaboration, where an action model consults a specialized world model that acts as a web-environment expert to get strategic guidance; (2) consequence simulation, where the world model predicts environment state transitions and simulates the outcomes of candidate actions; and (3) feedback-driven action refinement via a two-stage deduction chain, where a judge model evaluates the simulated consequences and, if needed, issues corrective feedback to refine or change the planned actions. The action model then grounds the guidance into executable web actions, leveraging prior knowledge of state transition dynamics to propose better candidates.", "result": "On two benchmarks, VisualWebArena and Online-Mind2Web, WAC achieves absolute performance improvements of 1.8% and 1.3%, respectively, compared with previous methods or baselines. These empirical results indicate that incorporating model collaboration, consequence simulation, and feedback-based refinement enhances web task success, particularly with respect to risk-sensitive decisions.", "conclusion": "The paper concludes that integrating specialized world and judge models with an action model\u2014enabling collaborative reasoning, outcome simulation, and corrective feedback\u2014results in more risk-aware and resilient web agents. WAC mitigates issues related to poor prediction of environment changes and premature risky actions, and the reported gains on standard benchmarks suggest that such multi-model architectures are a promising direction for improving LLM-based web automation."}}
{"id": "2602.15197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15197", "abs": "https://arxiv.org/abs/2602.15197", "authors": ["Skyler Hallinan", "Thejas Venkatesh", "Xiang Ren", "Sai Praneeth Karimireddy", "Ashwin Paranjape", "Yuhao Zhang", "Jack Hessel"], "title": "OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction", "comment": null, "summary": "Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general \"search\" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.", "AI": {"tldr": "The paper introduces OpaqueToolsBench, a benchmark to evaluate how LLM agents handle underspecified, opaque tools, and proposes ToolObserver, a framework that improves tool documentation via execution feedback, yielding better task performance and efficiency than prior methods.", "motivation": "Existing LLM tool-use benchmarks assume simple, clearly documented tools, unlike many real-world tools that are opaque, poorly documented, and lack clear failure modes. This mismatch makes it unclear how well LLM agents can learn to use such tools or improve their own documentation for better performance. The authors aim to systematically study this gap and find methods that allow LLM agents to succeed when tools are underspecified and non-transparent.", "method": "The authors build OpaqueToolsBench, a benchmark with three environments\u2014general function calling, interactive chess, and long-horizon agentic search\u2014each exposing only underspecified tool interfaces. They evaluate existing automatic tool-documentation methods and find them lacking under opacity. They then propose ToolObserver, a framework where the agent iteratively refines tool documentation by observing execution feedback from its own tool-calling trajectories, using that feedback to update descriptions and usage guidance.", "result": "On OpaqueToolsBench, existing automatic documentation approaches perform poorly or incur high cost when tools are opaque. ToolObserver consistently outperforms these baselines across all datasets and environments, including more challenging settings, while being more efficient in test-time exploration, reducing total token usage by 3.5\u20137.5x relative to the strongest baseline.", "conclusion": "LLM agents struggle with opaque, underspecified tools under existing documentation approaches, but performance can be substantially improved by iteratively refining tool documentation from execution feedback. OpaqueToolsBench provides a realistic testbed for this setting, and ToolObserver offers a simple, effective, and token-efficient framework for enabling robust tool use in real-world, opaque-tool environments."}}
{"id": "2602.15391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15391", "abs": "https://arxiv.org/abs/2602.15391", "authors": ["Ankit Sharma", "Nachiket Tapas", "Jyotiprakash Patra"], "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection", "comment": null, "summary": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.", "AI": {"tldr": "The paper proposes a context-aware abstention framework that adaptively filters LLM outputs to better balance safety and utility.", "motivation": "Existing safety guardrails for LLMs either block too many harmless queries or allow unsafe content, and they are often context-insensitive and slow, leading to poor user experience.", "method": "They design an adaptive abstention system that tunes safety thresholds using contextual signals like domain and user history, and implement a multi-dimensional detection architecture with five parallel detectors arranged in a hierarchical cascade to optimize both speed and precision.", "result": "The cascaded, context-aware system significantly reduces false positives, especially in sensitive domains such as medical advice and creative writing, while improving latency compared to non-cascaded and external guardrail systems and maintaining high safety precision and near-perfect recall in strict modes.", "conclusion": "A context-aware, cascaded abstention framework can more effectively balance safety and utility for LLM deployment than traditional static guardrails, providing a scalable and performant safety solution."}}
{"id": "2602.15312", "categories": ["cs.CL", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.15312", "abs": "https://arxiv.org/abs/2602.15312", "authors": ["Stephan Ludwig", "Peter J. Danaher", "Xiaohao Yang", "Yu-Ting Lin", "Ehsan Abedin", "Dhruv Grewal", "Lan Du"], "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement", "comment": null, "summary": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.", "AI": {"tldr": "They present LX, a fine-tuned LLM that measures 16 emotions and 4 evaluation constructs from consumer text, outperforming existing models and showing that emotions in reviews predict ratings and purchase behavior beyond star ratings.", "motivation": "Current methods struggle to accurately extract nuanced consumer emotions and evaluations (trust, commitment, recommendation, sentiment) from unstructured, consumer-generated text. Marketing researchers and practitioners need scalable, validated tools that can reliably transform text into measurable psychological constructs to better understand consumer perceptions and predict behavior.", "method": "They fine-tune a large language model (LX) on a dataset of consumer-authored text labeled with consumers' own self-reported ratings of 16 emotions and 4 evaluation constructs. They benchmark LX against leading models (GPT-4 Turbo, RoBERTa, DeepSeek) using macro-F1 and accuracy on survey responses and third-party-annotated Amazon/Yelp reviews. They then apply LX to online retail review data and use seemingly unrelated regression to examine how review-expressed emotions relate to product ratings and, subsequently, to purchase behavior, testing mediation effects of ratings.", "result": "LX achieves 81% macro-F1 accuracy on open-ended survey responses and over 95% accuracy on annotated Amazon and Yelp reviews, outperforming state-of-the-art baselines. The regression analysis shows that emotions expressed in reviews significantly predict product ratings, which in turn predict purchase behavior. Most emotional effects on purchase are mediated through ratings, but some emotions (e.g., discontent, peacefulness) exert direct effects on purchase, indicating additional informational value beyond star ratings.", "conclusion": "LX provides a validated, high-performing method to detect a rich set of emotions and evaluation constructs from consumer-authored text, outperforming popular existing models. Emotions captured by LX meaningfully predict ratings and purchase behavior, with some emotional tones adding information beyond star ratings. The authors offer a no-code, cost-free web app to facilitate adoption, and position their work as a new methodological foundation for measuring consumer perceptions and leveraging large language models in marketing research and practice."}}
{"id": "2602.15403", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15403", "abs": "https://arxiv.org/abs/2602.15403", "authors": ["Thomas \u00c5gotnes"], "title": "Common Belief Revisited", "comment": null, "summary": "Contrary to common belief, common belief is not KD4.\n  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(C\u03c6\\rightarrow \u03c6)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:\n  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \\emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.", "AI": {"tldr": "The paper studies the modal logic of common belief when individual belief is KD45, proving that common belief is not KD4 and identifying the exact axioms characterizing its logic.", "motivation": "To challenge and correct the widespread assumption that the modal logic of common belief under KD45 individual belief is KD4, and to determine what the correct logic actually is.", "method": "The authors use modal logic and axiomatic proof theory: they analyze the properties of the common belief operator derived from KD45 individual belief, show which modal axioms it satisfies (D, 4, but not 5, T, etc.), identify an additional axiom C(C\u03c6\u2192\u03c6) (shift-reflexivity), and then construct a completeness proof. They prove that KD4 plus shift-reflexivity is still not complete and derive one more axiom whose form depends on the number of agents, finally proving soundness and completeness for this extended system.", "result": "They prove that common belief is not characterized by KD4, even with the added shift-reflexivity axiom C(C\u03c6\u2192\u03c6). They identify an extra axiom, dependent on the number of agents, that together with KD4 and shift-reflexivity completely characterizes the logic of common belief when individual belief is KD45. This resolves an existing open problem in the literature.", "conclusion": "The standard assumption that common belief corresponds to KD4 is false in the KD45 setting. The correct modal logic of common belief is KD4 plus the shift-reflexivity axiom C(C\u03c6\u2192\u03c6) plus one further axiom whose form depends on how many agents there are. This system is sound and complete for common belief, thereby settling the open problem about its exact logical characterization."}}
{"id": "2602.15313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15313", "abs": "https://arxiv.org/abs/2602.15313", "authors": ["Zihao Tang", "Xin Yu", "Ziyu Xiao", "Zengxuan Wen", "Zelin Li", "Jiaxi Zhou", "Hualei Wang", "Haohua Wang", "Haizhen Huang", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "title": "Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory", "comment": "10 pages", "summary": "AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.", "AI": {"tldr": "The paper introduces Mnemis, a dual-system memory framework that combines similarity-based retrieval with hierarchical, global selection to improve long-term memory use in LLMs, achieving state-of-the-art results on long-context benchmarks.", "motivation": "Existing AI memory methods like RAG and Graph-RAG mainly rely on fast, similarity-based (System-1) retrieval, which fails in tasks requiring global reasoning, full coverage of relevant information, or structured understanding of long-term history. There is a need for a memory system that can go beyond local similarity and reason over global semantic structure.", "method": "The authors propose Mnemis, which organizes memory into two coupled graph structures: (1) a base graph that supports standard similarity-based retrieval; and (2) a hierarchical graph that encodes semantic hierarchies for top-down, deliberate traversal (System-2 style). A novel Global Selection mechanism uses the hierarchical graph to perform deliberate, global reasoning over stored memories, and the framework combines results from both retrieval paths (similarity search and hierarchical traversal) to select memory items that are both semantically similar and structurally/organizationally relevant.", "result": "Mnemis outperforms existing long-term memory baselines, including RAG and Graph-RAG variants, achieving state-of-the-art performance on long-term memory benchmarks. Using GPT-4.1-mini as the backbone, it reaches 93.9 on the LoCoMo benchmark and 91.6 on LongMemEval-S, surpassing all compared methods.", "conclusion": "By unifying fast similarity-based retrieval with a slower, more deliberate global selection over a hierarchical memory graph, Mnemis provides a more robust memory framework for LLMs, especially in tasks needing global reasoning and complete coverage of relevant history. This dual-system design leads to state-of-the-art performance on long-context memory benchmarks and suggests that combining System-1 and System-2 style retrieval is a promising direction for AI memory systems."}}
{"id": "2602.15531", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15531", "abs": "https://arxiv.org/abs/2602.15531", "authors": ["Javier Irigoyen", "Roberto Daza", "Aythami Morales", "Julian Fierrez", "Francisco Jurado", "Alvaro Ortigosa", "Ruben Tolosana"], "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway", "comment": "10 pages, 3 figures. Published in Intl. Conf. on Learning Analytics & Knowledge Workshops (LAK Workshops 2026, GenAI-LA 26)", "summary": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.", "AI": {"tldr": "EduEVAL-DB is a labeled dataset of human- and LLM-generated teacher-style explanations plus a pedagogical risk rubric, used to train and evaluate models that detect risks in instructional explanations.", "motivation": "As AI tutors and automatic pedagogical evaluators become more common, we need reliable benchmarks and training data tailored to educational quality and safety, not just general NLP performance. Existing datasets underrepresent pedagogical nuances such as student-appropriateness, depth of explanation, and specific risks seen in real classrooms. The authors aim to fill this gap with a dataset and rubric that mirror authentic teacher roles and educational standards, enabling systematic assessment and mitigation of pedagogical risks in AI-generated explanations.", "method": "The authors curate 139 multi-domain K\u201312 questions from ScienceQA and collect 854 explanations: one human-teacher explanation per question plus six explanations per question generated by LLMs prompted to simulate distinct teacher roles reflecting varied instructional styles and shortcomings. They design a pedagogical risk rubric with five dimensions\u2014factual correctness, explanatory depth/completeness, focus/relevance, student-level appropriateness, and ideological bias\u2014aligned with educational standards. Explanations are labeled with binary risk indicators for each dimension via a semi-automatic pipeline followed by expert teacher review. To validate the dataset, they run experiments comparing Gemini 2.5 Pro to a local Llama 3.1 8B model and test supervised fine-tuning on EduEVAL-DB for pedagogical risk detection, focusing on models that can run on consumer hardware.", "result": "The authors construct EduEVAL-DB, a multi-domain K\u201312 dataset with human and role-conditioned LLM teacher explanations, each annotated with binary risk labels along five pedagogical dimensions. Preliminary experiments show that the dataset can be used to benchmark large education-oriented models against smaller local ones and that fine-tuning on EduEVAL-DB enables smaller models to perform pedagogical risk detection to a plausible degree, demonstrating the dataset\u2019s utility and feasibility for deployment on consumer hardware.", "conclusion": "EduEVAL-DB provides a structured resource and rubric for evaluating and training pedagogical risk detectors in AI tutoring systems. By combining human and systematically varied LLM-simulated teacher explanations with expert-reviewed risk labels, it supports the development and comparison of both large and lightweight models for safer, higher-quality instructional explanations. The preliminary validation indicates that EduEVAL-DB is suitable for benchmarking and that smaller models, when fine-tuned, can become practical tools for detecting pedagogical risks in everyday educational deployments."}}
{"id": "2602.15353", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15353", "abs": "https://arxiv.org/abs/2602.15353", "authors": ["Rong Fu", "Yang Li", "Zeyu Zhang", "Jiekai Wu", "Yaohua Liu", "Shuaishuai Cao", "Yangchen Zeng", "Yuhang Zhang", "Xiaojing Du", "Chuang Zhao", "Kangning Cui", "Simon Fong"], "title": "NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering", "comment": "26 pages, 7 figures", "summary": "Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.", "AI": {"tldr": "NeuroSymActive is a framework for efficient, accurate knowledge graph question answering by combining neural-symbolic reasoning with active exploration.", "motivation": "Existing large language models and neural reasoning systems struggle with knowledge-intensive, multi-hop questions, and current ways of using knowledge graphs are either inefficient (prompting with many facts) or inflexible/expensive (purely symbolic or heavy search). The paper aims to build a system that uses knowledge graphs effectively while staying trainable and computationally efficient.", "method": "They propose NeuroSymActive, which has (1) a differentiable neural-symbolic reasoning layer based on soft unification over knowledge graph facts, (2) a neural path evaluator that scores candidate reasoning paths, and (3) a Monte-Carlo-style active exploration policy that selectively expands promising reasoning paths, thus reducing unnecessary graph lookups and model calls.", "result": "On standard knowledge graph question answering benchmarks, NeuroSymActive achieves high answer accuracy that is competitive with or better than retrieval-augmented baselines, while requiring fewer graph retrieval operations and fewer calls to large neural models.", "conclusion": "Coupling differentiable neural-symbolic reasoning with an active, value-guided exploration policy yields an effective and more efficient approach to knowledge-graph-based question answering, addressing both accuracy and computational cost limitations of prior methods."}}
{"id": "2602.15532", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15532", "abs": "https://arxiv.org/abs/2602.15532", "authors": ["Ryan Othniel Kearns"], "title": "Quantifying construct validity in large language model evaluations", "comment": null, "summary": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.\n  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.\n  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.", "AI": {"tldr": "The paper identifies weaknesses in current LLM benchmark interpretations and proposes a new \"structured capabilities\" model that better separates true capabilities from model scale and measurement noise, leading to more interpretable and reliable capability estimates and better out-of-distribution predictions.", "motivation": "Benchmark scores are often treated as if they fully represent LLM capabilities, but they can be distorted by issues like contamination and annotation errors, raising concerns about construct validity\u2014whether benchmarks actually measure the capabilities we care about. Existing formal approaches (latent factor models and scaling laws) fail to provide satisfactory construct validity because they each omit crucial aspects of the problem.", "method": "The author introduces the structured capabilities model, which combines ideas from latent factor models (capabilities as latent variables plus measurement error) and scaling laws (explicit use of model scale in predicting performance). The model is fit to a large dataset of LLM results from the OpenLLM Leaderboard and then compared against standard latent factor models and scaling-law-based approaches using model fit criteria and out-of-distribution benchmark prediction tasks.", "result": "The structured capabilities model achieves better parsimonious fit indices than latent factor models and yields better out-of-distribution benchmark predictions than scaling laws, indicating that it both explains existing data more efficiently and generalises better to unseen benchmarks.", "conclusion": "By properly separating model scale from underlying capabilities while explicitly modeling measurement error, the structured capabilities model offers a more interpretable and generalisable account of LLM capabilities than existing methods, thereby improving the construct validity of LLM benchmark evaluations."}}
{"id": "2602.15373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15373", "abs": "https://arxiv.org/abs/2602.15373", "authors": ["Deniz Kaya Dilsiz", "Dipankar Srirag", "Aditya Joshi"], "title": "Far Out: Evaluating Language Models on Slang in Australian and Indian English", "comment": "Accepted as a paper at 13th VarDial workshop at EACL 2026", "summary": "Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \\textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \\textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \\textsc{web} versus \\textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.", "AI": {"tldr": "The paper evaluates how well modern language models understand and use slang in Indian and Australian English, revealing significant gaps and asymmetries between their generative and discriminative abilities.", "motivation": "While it is known that language models struggle with non-standard language varieties, their specific capability to understand regional slang within globally dominant languages like English (e.g., Indian English and Australian English) has been underexplored. The authors aim to systematically measure this capability across several strong models to clarify where and how these systems fail, and to highlight biases between different English varieties.", "method": "The authors construct two datasets for slang in Indian English (en-IN) and Australian English (en-AU): (1) WEB, consisting of 377 real usage examples collected from Urban Dictionary, and (2) GEN, containing 1,492 synthetic sentences that use the same slang terms in varied, controlled scenarios. Using these datasets, they evaluate seven state-of-the-art language models on three tasks: (a) Target Word Prediction (TWP), where the model must generate the slang term from context; (b) Guided Target Word Prediction (TWP*), a similar generative task but with additional guidance; and (c) Target Word Selection (TWS), a discriminative task where the model selects the correct slang term from options. They then compare performance across tasks, datasets, and language varieties using accuracy and similarity metrics.", "result": "They find that models perform much better on the discriminative TWS task than on the generative TWP and TWP* tasks, with average accuracy jumping from 0.03 on TWP to 0.49 on TWS. Models also do better on the WEB dataset than on the synthetic GEN dataset, with average similarity scores increasing by 0.03 for TWP and 0.05 for TWP*. Additionally, tasks in Indian English generally outperform those in Australian English across models and datasets, with TWS showing the largest difference: average accuracy increases from 0.44 for en-AU to 0.54 for en-IN.", "conclusion": "The study concludes that language models have markedly stronger discriminative abilities (choosing the right slang term from options) than generative abilities (producing slang from scratch) for variety-specific slang. There is also a systematic performance bias favoring Indian English slang over Australian English slang, and real-world usage examples are handled better than synthetic ones. These findings highlight fundamental asymmetries in how models process non-standard and variety-specific language, even within a widely resourced language like English, and suggest the need for more targeted datasets and training strategies to improve slang and variety awareness."}}
{"id": "2602.15553", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15553", "abs": "https://arxiv.org/abs/2602.15553", "authors": ["Gabriele Conte", "Alessio Mattiace", "Gianni Carmosino", "Potito Aghilar", "Giovanni Servedio", "Francesco Musicco", "Vito Walter Anelli", "Tommaso Di Noia", "Francesco Maria Donini"], "title": "RUVA: Personalized Transparent On-Device Graph Reasoning", "comment": null, "summary": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.", "AI": {"tldr": "The paper introduces Ruva, a transparent \"glass box\" Personal AI architecture that uses a personal knowledge graph instead of vector databases to provide accountable, editable, and privacy-preserving memory.", "motivation": "Current Personal AI systems rely on black-box Retrieval-Augmented Generation over vector databases. These systems lack accountability: users cannot see why the AI responded a certain way, cannot easily correct hallucinations, and cannot reliably remove or redact specific information. Vector spaces also make precise deletion mathematically difficult, potentially leaving probabilistic traces (\"ghosts\") of supposedly removed concepts, which threatens privacy and the user\u2019s \"right to be forgotten.\" The authors want a system where users can see, understand, and curate what the AI knows about them.", "method": "The authors propose Ruva, a \"glass box\" architecture that grounds Personal AI in a personal knowledge graph rather than in raw vector embeddings. The system emphasizes Human-in-the-Loop Memory Curation: users can inspect the graph to see what the AI \"knows\" and can precisely edit or redact specific facts. Reasoning is shifted from opaque vector matching to explicit graph reasoning, enabling traceable retrieval and modification of knowledge. A project implementation and demo are provided online.", "result": "Ruva demonstrates that a personal knowledge graph can effectively serve as the memory substrate for Personal AI, supporting inspection, targeted editing, and precise redaction of facts. The architecture shows how graph-based reasoning can replace or complement vector-based retrieval to enable more transparent and controllable AI behavior. The implementation and demo video showcase the feasibility of the proposed approach.", "conclusion": "By replacing black-box vector-based retrieval with a transparent, graph-based memory architecture, Ruva enables accountable, user-editable Personal AI. Users can inspect and curate their personal knowledge graph, achieving precise redaction and supporting a stronger \"Right to be Forgotten.\" The work argues that future Personal AI should be designed as \"glass boxes\" where users are active editors of their own digital memories rather than passive subjects of opaque models."}}
{"id": "2602.15377", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15377", "abs": "https://arxiv.org/abs/2602.15377", "authors": ["Mengze Hong", "Chen Jason Zhang", "Zichang Guo", "Hanlin Gu", "Di Jiang", "Li Qing"], "title": "Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework", "comment": "Accepted by TheWebConf 2026", "summary": "Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.", "AI": {"tldr": "The paper proposes an orchestration-free, flowchart-based framework that uses small, locally deployed language models to automate customer service tasks end-to-end from dialogue data, addressing cost, privacy, and generalizability issues, and outperforms existing methods and products.", "motivation": "Customer service automation is increasingly important in digital transformation, but current solutions either need complex modular orchestration or rely on oversimplified instructions that don\u2019t generalize well. There is also a need to reduce reliance on large cloud-based models due to cost, data scarcity, and privacy concerns. The paper is motivated by the need for a more scalable, generalizable, and privacy-preserving way to automate diverse service workflows directly from real dialogues.", "method": "The paper introduces Task-Oriented Flowcharts (TOFs) as a central representation of procedural knowledge for service tasks. It (1) defines the components and evaluation metrics for TOFs; (2) proposes a cost-efficient algorithm that automatically constructs TOFs from historical service dialogues, abstracting the underlying task procedures; and (3) uses these flowcharts to guide locally deployed small language models through a decentralized distillation paradigm, enabling them to learn task behavior without centralized, large-scale data aggregation. The overall framework is orchestration-free, meaning that once TOFs and models are prepared, end-to-end automation can proceed without additional manual agent orchestration.", "result": "Experiments across various customer service tasks show that the proposed framework achieves better quantitative metrics and practical application performance than strong academic baselines and commercial products. The results indicate that TOF-based automation with small, locally deployed models can match or surpass more complex orchestrated systems and large-model-based solutions in effectiveness and efficiency.", "conclusion": "Task-Oriented Flowcharts enable a new orchestration-free paradigm for customer service automation. By automatically extracting flowcharts from dialogue logs and using them to train and guide small, locally deployed language models via decentralized distillation, the framework delivers effective, privacy-preserving, and generalizable automation. The released web-based demo and case studies suggest that this approach can streamline future development and deployment of automated service systems."}}
{"id": "2602.15580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15580", "abs": "https://arxiv.org/abs/2602.15580", "authors": ["Hongxuan Wu", "Yukun Zhang", "Xueqing Zhou"], "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning", "comment": null, "summary": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.", "AI": {"tldr": "They develop an information-theoretic, layer-wise analysis (PID Flow) to dissect how multimodal Transformers like LLaVA use visual vs. textual information, finding that language dominates late predictions, visual info is used early then decays, and genuine cross-modal synergy is minimal.", "motivation": "To understand, in a precise and quantitative way, whether multimodal Transformer VQA models are truly integrating vision and language or mostly relying on language cues, and how the contribution of each modality changes across layers and tasks. Existing analyses lack a principled information-theoretic, layer-wise decomposition for high-dimensional neural representations.", "method": "They propose a layer-wise analysis framework grounded in Partial Information Decomposition (PID), which splits predictive information into redundant, vision-unique, language-unique, and synergistic parts. To make this feasible for high-dimensional model activations, they introduce PID Flow: a pipeline with dimensionality reduction, normalizing-flow-based Gaussianization, and closed-form Gaussian PID estimation. They apply this to internal representations of LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, and also run causal interventions via targeted Image\u2192Question attention knockouts to test the inferred information pathways.", "result": "Across models and tasks, they find a consistent \"modal transduction\" pattern: visual-unique information peaks in early layers then decreases, language-unique information rises in deeper layers and ultimately accounts for about 82% of the final prediction, and cross-modal synergy remains very low (<2%). This pattern is highly correlated across LLaVA variants (layer-wise correlations >0.96), but the exact information profile is task-dependent and modulated by semantic redundancy. Knockout experiments that disrupt key Image\u2192Question attention paths cause increased trapped visual-unique information, increased synergy, and higher total information cost, especially for vision-dependent tasks.", "conclusion": "Multimodal Transformers like LLaVA largely convert visual information into a language-dominated representation as depth increases, with very limited true cross-modal synergistic processing. Their PID Flow framework provides both an information-theoretic description and causal evidence for this \"vision-to-language\" transduction, and highlights architectural bottlenecks where modality-specific information is lost, which can inform future multimodal model design."}}
{"id": "2602.15378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15378", "abs": "https://arxiv.org/abs/2602.15378", "authors": ["Prathamesh Devadiga", "Paras Chopra"], "title": "Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language", "comment": "Accepted to EACL LoResLM Workshop", "summary": "Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).", "AI": {"tldr": "The paper tests whether large language models can handle a very low\u2011resource language (Tulu) using only smart prompting, not fine\u2011tuning, and shows that structured prompts and constraints can elicit reasonably accurate, low-contamination Tulu conversation.", "motivation": "Many languages like Tulu have millions of speakers but almost no digital text, so they are effectively absent from LLM training data. Fine\u2011tuning requires data that does not exist or is costly to create. The authors want to know whether we can instead unlock conversational ability in such languages using only prompting strategies, and to understand which prompt components (grammar descriptions, constraints, synthetic data, etc.) matter most.", "method": "They perform a case study on Tulu with three LLMs (Gemini 2.0 Flash, GPT\u20114o, Llama 3.1 70B). Instead of adding training data, they design structured prompts that: (1) inject explicit grammar documentation; (2) apply negative constraints to block high\u2011probability tokens from related languages; (3) impose a standardized romanization scheme; and (4) use self\u2011play to generate quality\u2011controlled synthetic Tulu dialogue. They then evaluate the resulting Tulu outputs on a manually curated held\u2011out test set, with judgments validated by native Tulu speakers. They also run ablations across models to measure the separate effects of negative constraints and grammar documentation.", "result": "Using only prompting and synthetic data generation, the approach cuts vocabulary contamination (intrusion of words from related languages) from 80% to 5% and achieves 85% grammatical accuracy in Tulu, as judged on a held\u2011out set with native\u2011speaker validation. Cross\u2011model ablations show that negative constraints consistently improve performance by 12\u201318 percentage points across all three LLMs, while integrating grammar documentation yields 8\u201322 point gains but with more variation across architectures.", "conclusion": "Even for a language that is virtually absent from training data, carefully engineered prompts can elicit surprisingly strong basic conversational abilities from general\u2011purpose LLMs, without any parameter updates. Negative lexical constraints are a robust ingredient, and grammar\u2011informed prompting can be highly beneficial though model\u2011dependent. This suggests a promising path for supporting low\u2011resource languages via prompt engineering and synthetic data, potentially reducing reliance on costly supervised data collection and fine\u2011tuning."}}
{"id": "2602.15635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15635", "abs": "https://arxiv.org/abs/2602.15635", "authors": ["Konstantin Sidorov"], "title": "On inferring cumulative constraints", "comment": "17 pages, 6 figures, 4 tables; submitted to the 32nd International Conference on Principles and Practice of Constraint Programming (CP 2026)", "summary": "Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.", "AI": {"tldr": "A preprocessing method infers additional cumulative constraints in scheduling to exploit multi-resource interactions and improve solver performance.", "motivation": "Existing cumulative-constraint propagation in scheduling operates independently per resource, failing to exploit interactions among multiple resources. This can significantly slow down the search and limit the quality of bounds on challenging benchmarks. A systematic way to infer and add cross-resource constraints before search can potentially improve pruning and performance without costly search-time probing.", "method": "The paper reinterprets cumulative constraints as linear inequalities over occupancy vectors. From this view, it applies techniques from integer programming: (i) identify covers\u2014sets of tasks that cannot run in parallel because they would exceed resource capacity; (ii) strengthen the corresponding cover inequalities using lifting to better approximate the convex hull of feasible schedules; and (iii) add these inferred cumulative constraints back into the original scheduling instance as a preprocessing step, so the CP solver can exploit them during search with no additional online probing.", "result": "On standard RCPSP and RCPSP/max test suites, the inferred constraints generally improve search performance and tighten objective bounds on many instances while causing only minor slowdowns where they are not helpful. The experiments report 25 new lower bounds and five new best-known solutions; eight of these new lower bounds arise solely from the inferred constraints, demonstrating their direct impact on bound quality.", "conclusion": "Viewing cumulative constraints as linear inequalities over occupancy vectors enables automatic inference of additional, stronger cumulative constraints that capture multi-resource interactions. This preprocessing step is computationally cheap, improves pruning and bounds on many instances, and occasionally yields new state-of-the-art results, all while rarely harming performance on less favorable cases."}}
{"id": "2602.15382", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15382", "abs": "https://arxiv.org/abs/2602.15382", "authors": ["Xiaoze Liu", "Ruowang Zhang", "Weichen Yu", "Siheng Xiong", "Liu He", "Feijie Wu", "Hoin Jung", "Matt Fredrikson", "Xiaoqian Wang", "Jing Gao"], "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems", "comment": "Preprint. Work in progress", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas", "AI": {"tldr": "Introduces \u201cVision Wormhole,\u201d a framework that uses the visual channel of vision-language models as a high-bandwidth, model-agnostic communication medium for multi-agent LLM systems, reducing runtime while preserving reasoning quality.", "motivation": "Text-based communication between LLM agents in multi-agent systems is slow and lossy, and current latent state transfer methods do not scale well across heterogeneous models with different architectures and representation spaces.", "method": "Repurpose the visual interface of vision-language models by designing a Universal Visual Codec that encodes agents\u2019 internal reasoning traces into a shared continuous latent space, then injects these latents into the receiver\u2019s vision encoder. Use a hub-and-spoke alignment topology and a label-free teacher\u2013student distillation objective to align the visual channel with the text reasoning channel across heterogeneous model families.", "result": "Across diverse VLM families like Qwen-VL and Gemma-based models, Vision Wormhole achieves faster end-to-end multi-agent collaboration (reduced wall-clock time) while preserving task performance and reasoning fidelity comparable to conventional text-message-based communication.", "conclusion": "Using a universal, vision-based latent communication channel provides an efficient, scalable, and model-agnostic alternative to text communication in heterogeneous multi-agent LLM systems, cutting communication overhead without sacrificing reasoning quality."}}
{"id": "2602.15645", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15645", "abs": "https://arxiv.org/abs/2602.15645", "authors": ["Lucas Elbert Suryana", "Farah Bierenga", "Sanne van Buuren", "Pepijn Kooij", "Elsefien Tulleners", "Federico Scari", "Simeon Calvert", "Bart van Arem", "Arkady Zgonnikov"], "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving", "comment": "21 pages, on submission to Transportation Research Part C", "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.", "AI": {"tldr": "CARE Drive is a framework to test whether vision-language foundation models used in automated driving truly base their decisions on human-like reasons rather than just matching outcomes.", "motivation": "Foundation models are increasingly used in automated driving for perception, action recommendation, and explanation. Current evaluation focuses on performance metrics like safety and trajectory accuracy and does not tell us whether the models\u2019 decisions actually depend on human-relevant reasons (e.g., safety, social norms). This is problematic in safety-critical settings because models might generate plausible explanations that are not causally related to their decisions, leading to false confidence.", "method": "The authors introduce CARE Drive, a model-agnostic framework for evaluating reason responsiveness in vision-language models for driving. The framework compares decisions from a baseline model with those from a reason-augmented version under carefully controlled contextual variations. It uses a two-stage process: (1) prompt calibration to stabilize model outputs, and (2) systematic contextual perturbation to vary human reasons such as safety margins, social pressure, and efficiency constraints. Decision changes are then analyzed to infer whether and how human reasons causally influence model behavior.", "result": "In a cyclist overtaking case study with competing normative considerations, CARE Drive shows that explicitly providing human reasons has a significant effect on model decisions and increases alignment with expert-recommended behavior. However, the degree of responsiveness differs by reason type, revealing uneven sensitivity across factors like safety, social pressure, and efficiency.", "conclusion": "Reason responsiveness of foundation models in automated driving can be empirically and systematically evaluated without changing model parameters. CARE Drive exposes where models are and are not sensitive to human-relevant reasons, highlighting both the promise of reason-augmented prompting to improve alignment and the need to address uneven responsiveness across different normative considerations."}}
{"id": "2602.15436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15436", "abs": "https://arxiv.org/abs/2602.15436", "authors": ["Joonatan Laato", "Veera Schroderus", "Jenna Kanerva", "Jenni Kauppi", "Virpi Lummaa", "Filip Ginter"], "title": "Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs", "comment": "Presented at: The 10th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature; EACL 2026 Workshop", "summary": "Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.\n  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.", "AI": {"tldr": "The paper builds and validates a categorization framework for leisure activities and organizational memberships mentioned in a large Finnish WWII evacuee interview corpus, and uses an open-weight LLM to scale expert-like labels to 350K entities.", "motivation": "Historians and sociologists want to quantitatively analyze everyday social life using digitized historical archives, but raw extracted text (e.g., thousands of unique activity and organization names) is too fine-grained and heterogeneous to directly answer their research questions. There is a need for a principled way to map these noisy mentions into a smaller, interpretable set of categories that capture theoretically meaningful aspects of social participation, and for methods to scale such categorization beyond what experts can do manually.", "method": "The authors propose a multi-dimensional categorization schema for participation-related entities, covering type of activity/organization, typical level of sociability, regularity of participation, and physical demandingness. They create a gold-standard annotated subset from WWII Karelian evacuee family interviews to ensure reliable evaluation. They then prompt large language models to assign categories to entities, and use a simple voting (ensembling) scheme across multiple LLM runs. They compare model labels to expert annotations to assess how well an open-weight LLM can emulate expert categorization, and finally use the best-performing configuration to label the full set of 350K mentions.", "result": "The open-weight LLM, when combined with a simple voting strategy over multiple runs, achieves categorization performance that is closely aligned with expert judgments on the gold-standard data, suggesting that the model can reliably apply the designed schema at scale. This enables the automatic labeling of all 350K leisure and organization mentions with the proposed dimensions of participation.", "conclusion": "A carefully designed categorization framework, together with open-weight large language models and simple ensembling, can transform large, noisy historical interview corpora into a structured dataset suitable for quantitative social science research. The resulting labeled resource opens the door to downstream analyses of social integration and related outcomes in the context of Finnish WWII evacuees, and the general approach is likely transferable to other historical and sociological corpora where fine-grained textual mentions need to be mapped to theory-driven categories."}}
{"id": "2602.15669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15669", "abs": "https://arxiv.org/abs/2602.15669", "authors": ["Xiachong Feng", "Liang Zhao", "Weihong Zhong", "Yichong Huang", "Yuxuan Gu", "Lingpeng Kong", "Xiaocheng Feng", "Bing Qin"], "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra", "comment": "ICLR 2026", "summary": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.", "AI": {"tldr": "PERSONA is a training-free framework that controls LLM personality by directly editing internal activation vectors, achieving near fine-tuning performance without gradient updates.", "motivation": "Existing personality control methods use static prompts or costly fine-tuning, which cannot flexibly represent the dynamic, compositional, and context-dependent nature of human-like personalities. The authors want a more interpretable, efficient, and algebraically controllable way to steer LLM behavior.", "method": "They treat personality traits as directions in the model\u2019s activation space. 1) Persona-Base: perform contrastive activation analysis to extract approximately orthogonal vectors corresponding to different traits. 2) Persona-Algebra: manipulate these vectors using simple vector arithmetic\u2014scaling to change trait intensity, adding to combine traits, and subtracting to suppress traits. 3) Persona-Flow: during inference, dynamically compose trait vectors based on context to enable adaptive personality expression, all without updating model weights.", "result": "On PersonalityBench, PERSONA reaches a mean score of 9.60, effectively matching a supervised fine-tuning upper bound of 9.61, despite being training-free. On a new Persona-Evolve benchmark focused on dynamic personality adaptation, it achieves up to 91% win rates across multiple model families, indicating strong and generalizable control over personality expression.", "conclusion": "Personality in LLMs corresponds to mathematically manipulable directions in activation space, which can be extracted and combined without fine-tuning. This yields interpretable, efficient, and highly effective personality control, suggesting a promising alternative to gradient-based behavioral steering methods."}}
{"id": "2602.15449", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15449", "abs": "https://arxiv.org/abs/2602.15449", "authors": ["Chansung Park", "Juyong Jiang", "Fan Wang", "Sayak Paul", "Jiasi Shen", "Jing Tang", "Jianguo Li"], "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models", "comment": "The first three authors contributed equally to this work; listing order is random", "summary": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.", "AI": {"tldr": "The paper introduces TAROT, a test-driven, capability-adaptive curriculum reinforcement fine-tuning framework to improve LLM code generation, showing that optimal reward curricula depend on model capability and that TAROT consistently boosts code correctness and robustness.", "motivation": "LLMs enable vibe coding but struggle with algorithmically complex and robust code generation. Existing reinforcement fine-tuning methods fail to account for heterogeneous test difficulty and granularity, causing imbalanced rewards and biased training. The authors aim to design a principled, capability-aware curriculum framework that better exploits LLM reasoning through structured test suites and adaptive curricula.", "method": "The authors propose TAROT, which builds a four-tier test suite (basic, intermediate, complex, edge) for each coding problem, creating a controlled difficulty landscape. TAROT decouples curriculum progression from raw reward scores by using capability-conditioned evaluation and selecting among different curriculum policies (e.g., easy-to-hard, hard-first) based on model capability, instead of relying on incidental test-case difficulty. This yields more stable optimization during reinforcement fine-tuning for code generation.", "result": "Experiments demonstrate that the best curriculum strategy depends on the base model: weaker models benefit most from an easy-to-hard progression, while stronger models perform better with a hard-first curriculum. Across setups, TAROT improves functional correctness and robustness of generated code compared with standard RFT baselines.", "conclusion": "TAROT offers a reproducible, test-driven, and capability-adaptive curriculum RFT framework that aligns reward curricula with model capability, leading to more stable training and better code generation quality. By structuring tests into four difficulty tiers and decoupling curricula from raw rewards, TAROT systematically enhances LLM coding performance and facilitates future research via released code and data."}}
{"id": "2602.15725", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15725", "abs": "https://arxiv.org/abs/2602.15725", "authors": ["Sarim Chaudhry"], "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models", "comment": null, "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.", "AI": {"tldr": "The paper introduces Recursive Concept Evolution (RCE), a method that lets large language models adapt their internal representations during inference to improve compositional reasoning, yielding large gains on hard reasoning benchmarks.", "motivation": "Large language models struggle on benchmarks that require strong compositional reasoning (ARC-AGI-2, GPQA, MATH, BBH, HLE). Existing approaches like chain-of-thought, self-consistency, and RL primarily expand token-level search without changing the model\u2019s underlying representation geometry. When needed abstractions are not present in the latent space, performance drops dramatically. The authors aim to address this representational inadequacy by allowing models to construct new abstractions on the fly.", "method": "They propose Recursive Concept Evolution (RCE), a framework that augments a pretrained model with dynamically generated low-rank concept subspaces during inference. When the system detects that current representations are inadequate for a task, it spawns new concept subspaces, evaluates and selects them using a minimum description length (MDL) criterion, merges subspaces that are synergistic, and consolidates them through constrained optimization to maintain stability. This mechanism modifies the internal geometry of representations so the model can form new abstractions rather than only recombining existing ones. They implement RCE on top of Mistral-7B.", "result": "On compositional reasoning benchmarks, integrating RCE with Mistral-7B yields substantial performance gains: 12\u201318 percentage point improvements on ARC-AGI-2, 8\u201314 point improvements on GPQA and BBH, and consistent reductions in errors that grow with reasoning depth on MATH and HLE.", "conclusion": "Allowing language models to adapt their internal representation geometry at inference time via low-rank, dynamically managed concept subspaces significantly improves compositional reasoning. RCE demonstrates that constructing new abstractions in the latent space\u2014not just expanding token-level reasoning\u2014can close much of the gap on challenging reasoning benchmarks."}}
{"id": "2602.15456", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15456", "abs": "https://arxiv.org/abs/2602.15456", "authors": ["Mohammad Aflah Khan", "Mahsa Amani", "Soumi Das", "Bishwamittra Ghosh", "Qinyuan Wu", "Krishna P. Gummadi", "Manish Gupta", "Abhilasha Ravichander"], "title": "In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations", "comment": "ICLR 2026", "summary": "Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.", "AI": {"tldr": "The paper shows that large language model agents have systematic, hidden preferences for certain information sources, which shape what information they present to users.", "motivation": "As LLM-based agents increasingly serve as intermediaries to online information, they don\u2019t just generate text\u2014they also choose which retrieved items to show users. Prior research has focused mainly on biases in generated content, not on how models select from multiple candidate sources. Since these choices can skew what users see, the authors want to understand whether models exhibit systematic preferences for some sources over others and how strong and persistent these preferences are.", "method": "The authors run controlled experiments on twelve LLMs from six providers, using both synthetic tasks and real-world scenarios. They present models with multiple pieces of information that are tagged with different sources (e.g., specific publishers, journals, platforms) and analyze which sources the models preferentially select or highlight. They vary contextual framing and add prompts that explicitly ask the models to avoid source bias, then measure how selections change to detect latent source preferences and their robustness.", "result": "Several evaluated LLMs display strong, predictable preferences for certain sources when choosing which information to present. These preferences are influenced by contextual framing, can dominate over the actual content quality or relevance, and remain even when instructions explicitly tell the models not to be biased by source. The patterns observed also help account for previously reported skews in recommendation behavior, such as a left-leaning bias in news suggestions.", "conclusion": "LLM-based agents do not simply reflect neutral relevance judgments; they carry systematic latent preferences for some information sources, which materially affect what users see. These source preferences are context-sensitive, can override content-based factors, and are resistant to simple prompt-level corrections. The authors argue that understanding the origins of these biases and building mechanisms for transparency and user control are crucial for responsibly deploying LLM-powered information agents."}}
{"id": "2602.15776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15776", "abs": "https://arxiv.org/abs/2602.15776", "authors": ["Yiqin Yang", "Xu Yang", "Yuhua Jiang", "Ni Mu", "Hao Hu", "Runpeng Xie", "Ziyou Zhang", "Siyuan Li", "Yuan-Hua Ni", "Qianchuan Zhao", "Bo Xu"], "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems", "comment": null, "summary": "In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.", "AI": {"tldr": "The paper proposes GlobeDiff, a diffusion-based algorithm to infer global states in partially observable multi-agent systems, showing bounded estimation error and superior empirical performance.", "motivation": "In multi-agent systems, partial observability prevents agents from accessing the true global state, which is essential for good coordination and decision-making. Existing solutions either use belief states based on past observations or communication among agents. Belief-based methods fail to fully exploit global information and struggle with ambiguity, especially in complex environments, while communication-based methods lack principled models to integrate and exploit the extra information they exchange. There is thus a need for a more powerful and theoretically grounded approach to infer the global state from local observations.", "method": "The paper introduces GlobeDiff, which reframes global state inference from local observations as a multi-modal diffusion process. Using diffusion models, it iteratively refines samples of the global state conditioned on the agents\u2019 local observations, allowing it to capture and disambiguate multi-modal uncertainties in the underlying state. The authors analyze the behavior of this inference process under both unimodal and multi-modal distributions and derive error bounds on the estimation. The algorithm is then applied within multi-agent systems to provide high-fidelity global state estimates for decision-making.", "result": "Theoretical analysis shows that the estimation error of GlobeDiff is bounded for both unimodal and multi-modal state distributions, indicating stable and reliable inference behavior. Empirically, across extensive experiments (details not given in the abstract), GlobeDiff outperforms prior belief-based and communication-based approaches in terms of coordination performance and accuracy of global state reconstruction. The method can reliably infer the underlying global state from partial observations in diverse multi-agent scenarios.", "conclusion": "GlobeDiff offers a principled, diffusion-based framework to infer the global state in partially observable multi-agent systems, effectively handling multi-modal uncertainty and overcoming limitations of prior belief and communication-based methods. With both theoretical guarantees on estimation error and strong empirical performance, the approach appears to be a promising direction for improving coordination and decision-making in complex multi-agent environments with limited observability."}}
{"id": "2602.15504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15504", "abs": "https://arxiv.org/abs/2602.15504", "authors": ["Aswathy Velutharambath", "Amelie W\u00fchrl"], "title": "Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit", "comment": null, "summary": "Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect", "AI": {"tldr": "Introduces Expectation Detection as a new NLP task and presents RedHOTExpect, a Reddit-based corpus to study patients\u2019 treatment expectations, showing clear linguistic and thematic differences between physical/treatment-related and mental-health posts.", "motivation": "Patients\u2019 expectations strongly influence treatment outcomes, but existing research focuses on clinical/controlled settings. Online platforms like medical subreddits may contain more candid or sensitive expectations that patients hesitate to share in clinical contexts. There is no prior NLP work systematically identifying and analyzing such expectations in natural language, leaving a gap for computational methods that can support applications like opinion mining, patient-centered care, and product/service design.", "method": "Define the new NLP task of Expectation Detection (identifying and characterizing expectations in text). Construct RedHOTExpect, a corpus of about 4.5K Reddit posts from medical subreddits. Use a large language model to automatically (silver) label expectation-related phenomena in the corpus, then manually validate a sample to estimate labeling quality (~78% accuracy). Perform linguistic analysis to identify patterns that mark expectations and conduct content analysis of what patients expect and their reasons, comparing physical/treatment-related vs. mental-health posts and positive vs. negative expectations.", "result": "The silver-labeled corpus RedHOTExpect is created and released. LLM-based silver labels show approximately 78% accuracy on manual validation, demonstrating reasonable but imperfect reliability. Linguistic analysis reveals characteristic markers of expectations and shows that posts about physical or treatment-related illnesses contain more optimism and proactive framing than those about mental health. Across the dataset, patients more frequently discuss anticipated benefits and positive outcomes than negative consequences.", "conclusion": "Expectation Detection is a viable and useful NLP task, and online medical forums provide rich data for studying patients\u2019 treatment expectations. The RedHOTExpect corpus enables further computational research in this area. Initial analyses indicate systematic differences in how expectations are expressed across medical domains, with a bias toward discussing positive, benefit-oriented expectations. This resource can support downstream applications in opinion mining, healthcare communication, and patient-centered system or product design."}}
{"id": "2602.15785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15785", "abs": "https://arxiv.org/abs/2602.15785", "authors": ["Jessica Hullman", "David Broska", "Huaman Sun", "Aaron Shaw"], "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence", "comment": null, "summary": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.", "AI": {"tldr": "The paper evaluates when and how large language models (LLMs) can be used as synthetic participants for valid causal inference in social science experiments, contrasting heuristic prompting/fine-tuning with statistically calibrated approaches.", "motivation": "Social science experiments are often costly and slow when run with human participants. LLMs offer a fast, cheap way to simulate participants, but there is little rigorous guidance on when these simulations yield valid inferences about human behavior, especially for causal effects. The paper aims to clarify the conditions under which LLM-based simulations can stand in for human subjects and how they can be integrated responsibly into empirical research.", "method": "The authors conceptually contrast two strategies: (1) heuristic approaches that attempt to align LLM outputs with human behavior via prompt engineering, fine-tuning, and other ad hoc fixes; and (2) statistical calibration, which combines LLM outputs with auxiliary human data and uses statistical adjustments to correct for systematic differences between simulated and real responses. They formalize the assumptions needed for each approach to yield valid causal effect estimates and discuss their suitability for exploratory versus confirmatory research designs.", "result": "Heuristic approaches can often produce LLM responses that resemble human data and are useful in exploratory work, but they do not provide formal guarantees of validity for causal inference. Statistical calibration, by explicitly modeling discrepancies between LLMs and humans and using auxiliary human data, can under stated assumptions deliver unbiased and more precise estimates of causal effects at a lower cost than purely human experiments. Both approaches\u2019 effectiveness is bounded by how well LLMs approximate the target human populations.", "conclusion": "The paper concludes that LLMs as synthetic participants are promising but cannot be treated as drop-in substitutes for humans without careful methodological safeguards. Heuristic alignment is appropriate mainly for exploration, while confirmatory research should rely on statistically calibrated designs that incorporate human data and explicit assumptions. The authors also caution that focusing solely on replacing human participants underutilizes LLMs\u2019 potential and argue for broader, more creative integration of LLMs into the research pipeline."}}
{"id": "2602.15506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15506", "abs": "https://arxiv.org/abs/2602.15506", "authors": ["Nils Rehlinger"], "title": "LuxMT Technical Report", "comment": "preprint", "summary": "We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.", "AI": {"tldr": "LuxMT is a Gemma 3-based machine translation system fine\u2011tuned for Luxembourgish\u2192French/English that significantly improves over the base model and also generalizes to German; they also propose LuxEmbedder embeddings for data filtering and explore them as a quality estimation metric.", "motivation": "There is a lack of high\u2011quality machine translation systems and benchmarks for the low\u2011resource language Luxembourgish, especially for LB\u2192FR/EN directions. Existing models like Gemma 3 are not specifically tuned for this language pair, and there is also a need for better data filtering and quality estimation tools tailored to Luxembourgish.", "method": "They fine\u2011tune the Gemma 3 27B model to create LuxMT using parallel Luxembourgish\u2013French/English data. Training data is compiled from LuxAlign, a multilingual news corpus, and Luxembourgish parliamentary transcripts whose translations are augmented with Google Translate. They build a new human\u2011translated benchmark from Luci magazine for LB\u2194FR/EN. For data cleaning, they introduce LuxEmbedder, Luxembourgish sentence embeddings, to filter out low\u2011equivalence segment pairs. They also analyze LuxEmbedder as a reference\u2011free quality estimation metric by correlating it with standard reference\u2011based metrics.", "result": "LuxMT substantially outperforms the Gemma 3 baseline on the new Luci benchmark for LB\u2192FR and LB\u2192EN. Surprisingly, it also shows notable gains for LB\u2192DE translation, even though no German data was present in training, indicating cross\u2011lingual generalization. LuxEmbedder-based filtering improves data quality, and its scores correlate strongly with established MT evaluation metrics, suggesting usefulness for quality estimation.", "conclusion": "Fine\u2011tuning a large language model with carefully filtered Luxembourgish\u2011centric data yields a strong MT system (LuxMT) that generalizes beyond its training language pairs. LuxEmbedder is promising both as a data\u2011filtering tool and as a quality estimation signal, but its role as a standalone metric remains uncertain; the authors recommend cautious use and call for more research and validation."}}
{"id": "2602.15791", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15791", "abs": "https://arxiv.org/abs/2602.15791", "authors": ["Suhyung Jang", "Ghang Lee", "Jaekun Lee", "Hyunjun Lee"], "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings", "comment": "42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)", "summary": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.", "AI": {"tldr": "They replace one-hot encodings of BIM object types with LLM-based text embeddings and show better subtype classification performance in AECO graphs.", "motivation": "Current BIM/AECO AI workflows use simplistic encodings like one-hot for object types, which cannot capture nuanced semantic relationships between closely related building subtypes, limiting model understanding and downstream performance.", "method": "They convert building object type and subtype semantics into dense vector encodings using large language model (LLM) embeddings (OpenAI GPT and Meta LLaMA). These embeddings, in both original dimensions (1536\u20134096) and reduced 1024-dim versions via a Matryoshka representation model, are used as node features in GraphSAGE models trained on five high-rise residential BIMs to classify 42 building object subtypes. Performance is compared against a one-hot encoding baseline across different embedding configurations.", "result": "Across experiments, LLM-derived encodings consistently outperform conventional one-hot encodings in the task of classifying 42 building object subtypes from BIM data. The best configuration, a compacted llama-3 embedding, achieves a weighted F1-score of 0.8766 versus 0.8475 for one-hot encoding.", "conclusion": "Using LLM-based embeddings as semantic encodings for BIM objects improves graph neural network performance on fine-grained subtype classification, demonstrating that richer text-derived semantics help AI better interpret complex building information. As LLMs and dimensionality reduction advance, this strategy is likely to generalize to broader semantic elaboration tasks across the AECO domain."}}
{"id": "2602.15509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15509", "abs": "https://arxiv.org/abs/2602.15509", "authors": ["Xiangyan Chen", "Yujian Gan", "Matthew Purver"], "title": "Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination", "comment": null, "summary": "The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.", "AI": {"tldr": "The paper introduces Fine-Refine, a fine-grained post-processing framework to reduce hallucinations in dialogue system responses from LLMs, by verifying and correcting atomic factual units with external knowledge.", "motivation": "Current LLM-based dialogue systems often hallucinate, generating factually incorrect content that can mislead users and harm trust. Existing refinement methods work at the whole-response level and fail to distinguish that a single response may mix correct, incorrect, and unverifiable facts, limiting their effectiveness in improving factuality.", "method": "The proposed Fine-Refine framework first decomposes an LLM-generated response into atomic factual units. Each unit is then checked against external knowledge sources for verifiability and correctness. The framework also evaluates fluency using perplexity. Based on these signals, Fine-Refine iteratively modifies or replaces units that are incorrect or insufficiently supported while preserving correct information, thereby refining the response at a granular level.", "result": "On the HybriDialogue and OpendialKG dialogue datasets, Fine-Refine significantly improves factuality metrics: it raises dialogue fact scores by up to 7.63 points and reduces the proportion of 'Not Enough Information' (NEI) cases, indicating better factual coverage. These gains come with only a small degradation in overall dialogue quality.", "conclusion": "Fine-Refine demonstrates that fine-grained, fact-level refinement with external knowledge and fluency checks can substantially reduce hallucinations in LLM dialogue systems. Operating at the level of atomic units rather than whole responses yields better factual accuracy and coverage, with only minor trade-offs in conversational quality."}}
{"id": "2602.15816", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15816", "abs": "https://arxiv.org/abs/2602.15816", "authors": ["Xiaoran Liu", "Istvan David"], "title": "Developing AI Agents with Simulated Data: Why, what, and how?", "comment": null, "summary": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.", "AI": {"tldr": "The chapter discusses how simulation-based methods, particularly digital twins, can be used to systematically generate synthetic data to overcome data scarcity and quality issues in training modern AI systems.", "motivation": "Modern subsymbolic AI methods, such as deep learning, require large, high-quality datasets, which are often difficult, costly, or impossible to obtain in sufficient volume. This data bottleneck limits broader adoption and performance of AI systems. Synthetic data generation via simulation is proposed as a way to alleviate this limitation by providing scalable, controllable, and diverse training data.", "method": "The chapter conceptually presents simulation-based synthetic data generation, with a focus on digital twin-based simulations. It defines key concepts, outlines benefits and challenges, and introduces a reference framework that can be used to describe, design, and analyze AI simulation solutions that rely on digital twins to generate training data.", "result": "The outcome is a structured reference framework and conceptual guidance for using simulation and digital twins to generate synthetic data for AI training. It clarifies terminology, identifies core components and processes in such systems, and summarizes typical advantages and limitations.", "conclusion": "Simulation, and specifically digital twin-based approaches, provide a systematic pathway to generate synthetic data to address data scarcity and quality issues in AI training. A reference framework helps practitioners design and analyze these solutions more effectively, while also highlighting that challenges remain in ensuring realism, representativeness, and proper integration of synthetic data into AI pipelines."}}
{"id": "2602.15514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15514", "abs": "https://arxiv.org/abs/2602.15514", "authors": ["Sara Ahmed", "Tracy Hammond"], "title": "DependencyAI: Detecting AI Generated Text through Dependency Parsing", "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.", "AI": {"tldr": "DependencyAI is a simple, interpretable detector of AI-generated text that relies solely on syntactic dependency labels and performs competitively across languages and generators.", "motivation": "With the widespread use of large language models, there is an urgent need for robust and transparent methods to distinguish AI-generated from human-written text, especially to mitigate misuse and understand model behavior.", "method": "The authors propose DependencyAI, a detection method that represents text using only linguistic dependency relation labels (ignoring lexical content) and trains a non-neural classifier on these features. They evaluate the approach in monolingual, multi-generator, and multilingual settings and conduct feature-importance analysis to identify which dependency patterns drive decisions.", "result": "DependencyAI achieves competitive detection performance compared to more complex baselines in various scenarios and languages. Feature-importance analysis highlights specific syntactic structures that reliably differentiate AI-generated from human text, and experiments show systematic overprediction on unseen domains for certain generators, indicating generator-specific style effects.", "conclusion": "Syntactic dependency relations alone contain a strong, linguistically grounded signal for AI-text detection. DependencyAI serves as a robust, interpretable, and non-neural baseline, while findings about cross-domain overprediction underscore that generator-specific styles can limit generalization."}}
{"id": "2602.15521", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15521", "abs": "https://arxiv.org/abs/2602.15521", "authors": ["Ziyu Zhao", "Tong Zhu", "Zhi Zhang", "Tiantian Fan", "Jinluan Yang", "Kun Kuang", "Zhongyu Wei", "Fei Wu", "Yu Cheng"], "title": "ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns", "comment": null, "summary": "Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \\textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \\textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.", "AI": {"tldr": "They introduce ExpertWeaver, a training-free method to turn a pretrained GLU-based dense model into a Mixture-of-Experts by reusing its internal activation patterns, yielding better sparse MoEs for both pruning-style use and MoE initialization.", "motivation": "Mixture-of-Experts models can scale capacity efficiently but are costly to train from scratch. Converting existing dense models into MoEs is cheaper, yet current dense-to-MoE approaches either dynamically prune structure or repurpose dense checkpoints into sparse MoEs in ways that disrupt the original activation patterns, hurting expert quality. The authors want a way to exploit the latent structure already present in dense GLU layers to construct better experts without extra training cost.", "method": "They analyze Gated Linear Units (GLUs) in pretrained dense models and observe that neuron-level activation statistics reveal two groups: consistently active, task-agnostic neurons and more selectively active, task-specific neurons. Using these activation patterns as a blueprint, they propose ExpertWeaver: a training-free pipeline that partitions GLU neurons into shared experts (universal neurons) and routed specialized experts (selective neurons). The framework configures expert sizes adaptively for each layer based on its observed activation structure and builds an MoE architecture whose routing reflects the original GLU behavior.", "result": "Across experiments, ExpertWeaver achieves higher performance than prior dense-to-MoE methods. It works both as (1) a training-free dynamic structural pruning scheme that introduces moderate sparsity while preserving or improving accuracy, and (2) a downcycling method that provides better initializations for highly sparse MoEs, leading to improved downstream performance compared to existing baselines.", "conclusion": "GLU layers in pretrained dense models implicitly embody an MoE-like structure that can be surfaced by analyzing neuron activation patterns. Exploiting this structure, ExpertWeaver constructs shared and routed experts without any additional training, delivering superior dense-to-MoE conversion for both pruning-oriented and initialization-oriented scenarios. This suggests that future dense-to-sparse transformations should align with intrinsic activation blueprints rather than impose arbitrary MoE structures."}}
{"id": "2602.15537", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.15537", "abs": "https://arxiv.org/abs/2602.15537", "authors": ["Nicol Visser", "Simon Malan", "Danel Slabbert", "Herman Kamper"], "title": "ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling", "comment": "3 figures, 2 tables", "summary": "Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.", "AI": {"tldr": "ZeroSyl is a training-free method for deriving syllable-like units from a frozen WavLM model, enabling efficient and competitive pure speech language modeling.", "motivation": "Pure speech language models suffer from long token sequences when using discrete units from self-supervised speech encoders, and existing syllabic unit methods require complex multi-stage training. There is a need for a simpler, training-free approach that still yields effective syllable-like representations for language modeling.", "method": "ZeroSyl uses intermediate-layer features from a frozen WavLM model, leveraging L2 norm patterns to detect syllable boundaries. The audio is segmented at these boundaries, and each segment is mean-pooled to obtain a fixed-size embedding. These embeddings are then discretized via K-means clustering to form a syllabic vocabulary, which is used as input tokens to train a language model.", "result": "ZeroSyl achieves competitive performance on syllable segmentation and, when used as a tokenizer, outperforms prior syllabic tokenization methods on lexical, syntactic, and narrative benchmarks for speech language modeling. It also shows that the discovered syllabic units scale better for syntactic tasks, while finer-grained units remain advantageous for lexical tasks.", "conclusion": "A simple, training-free procedure can extract effective syllable-like units from a frozen self-supervised speech model. These units provide a good trade-off between sequence length and linguistic expressivity, improving performance and scaling behavior of pure speech language models compared to more complex prior syllabic tokenizers."}}
{"id": "2602.15540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15540", "abs": "https://arxiv.org/abs/2602.15540", "authors": ["Tim Fischer", "Chris Biemann"], "title": "Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite", "comment": null, "summary": "This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.", "AI": {"tldr": "An interactive tool, Perspectives, helps Digital Humanities scholars cluster and explore large text collections using aspect-focused, human-in-the-loop document mapping and embedding-based methods.", "motivation": "Digital Humanities researchers handle large, unstructured text corpora but lack intuitive, controllable tools to organize them according to nuanced, research-specific analytic categories such as topics or sentiments.", "method": "They extend the Discourse Analysis Tool Suite with Perspectives, which offers an interactive, aspect-focused document clustering pipeline. It uses document rewriting prompts and instruction-based embeddings to define analytical lenses, plus interactive tools for refining clusters and mechanisms to fine-tune the embedding model, enabling human-in-the-loop control.", "result": "The paper demonstrates a typical workflow in which DH scholars use Perspectives's interactive document map to explore large corpora, generate and refine clusters aligned with their analytic needs, and prepare data for further detailed analysis.", "conclusion": "Perspectives effectively supports Digital Humanities researchers in exploring, organizing, and interpreting large text collections by combining flexible, aspect-focused clustering with interactive, human-guided refinement and model adaptation."}}
{"id": "2602.15547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15547", "abs": "https://arxiv.org/abs/2602.15547", "authors": ["Mohammad Kalim Akram", "Saba Sturua", "Nastia Havriushenko", "Quentin Herreros", "Michael G\u00fcnther", "Maximilian Werk", "Han Xiao"], "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation", "comment": "14 pages, 8 figures. Model weights: https://huggingface.co/collections/jinaai/jina-embeddings-v5-text", "summary": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.", "AI": {"tldr": "They propose a new way to train small text embedding models by combining distillation with task-specific contrastive loss, achieving state-of-the-art performance for their size while supporting long, multilingual inputs and robust quantization.", "motivation": "Existing text embedding models for semantic similarity often require large models or complex training pipelines, and small models usually lose accuracy. The authors want a method to train compact models that still achieve strong performance across many tasks, handle long inputs, and remain robust when truncated or quantized.", "method": "They design a training regimen that mixes model distillation (having a smaller model learn from a larger teacher model) with task-specific contrastive loss. This hybrid objective aims to capture both the teacher\u2019s knowledge and the discriminative power of contrastive learning. They then train and evaluate small embedding models, named jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, on benchmark suites, also testing long-context handling and robustness to truncation and binary quantization.", "result": "The resulting models achieve benchmark scores that match or surpass the state of the art for models of comparable size. They additionally support sequences up to 32k tokens in multiple languages and maintain strong performance even when inputs are truncated or when embeddings are binarized for compression.", "conclusion": "Combining distillation with task-specific contrastive loss is especially effective for training small text embedding models. The proposed jina-embeddings-v5-text models demonstrate that compact models can achieve near\u2013state-of-the-art performance, handle long and multilingual inputs, and remain robust under truncation and quantization. Public release of the model weights is intended to foster further progress in compact embedding model research."}}
{"id": "2602.15564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15564", "abs": "https://arxiv.org/abs/2602.15564", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Wei Xu"], "title": "Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL", "comment": null, "summary": "Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL", "AI": {"tldr": "The paper proposes SquRL, a reinforcement learning framework enabling large language models to dynamically construct Text-to-SQL workflows at inference time, outperforming the best static workflows, especially on complex and out-of-distribution queries.", "motivation": "Existing Text-to-SQL systems typically rely on a single, static workflow (e.g., a fixed prompting or execution pipeline). This makes them brittle and hard to scale to real-world settings where queries can be highly diverse, complex, and out-of-distribution. Users often must manually search for and tune workflows through costly experimentation. The authors are motivated by the observation (both theoretical and empirical) that if there is heterogeneity among candidate workflows, then an adaptive, query-dependent policy for choosing or composing workflows can surpass any single static workflow. They seek a principled way to realize such dynamic workflow construction automatically.", "method": "The authors introduce SquRL, a reinforcement learning (RL) framework wrapped around large language models for Text-to-SQL. SquRL learns a dynamic policy that, at inference time, adaptively constructs workflows (e.g., choosing which subroutines or prompting strategies to apply and in what order) conditioned on each input query. They formulate workflow construction as an RL problem with a rule-based reward function that reflects the quality of the resulting SQL (e.g., correctness signals extracted without heavy supervision). To stabilize and improve learning, they design two mechanisms: (1) dynamic actor masking, which restricts or reshapes the action space during training to encourage broader yet guided exploration among candidate workflow steps; and (2) pseudo rewards, auxiliary reward signals used to improve training efficiency and reduce variance when direct correctness feedback is sparse or delayed.", "result": "On standard Text-to-SQL benchmarks, SquRL-based dynamic workflow construction achieves consistently higher performance than any individual static workflow baseline. The gains are especially notable for complex and out-of-distribution queries, illustrating the advantage of adaptively choosing or composing workflows for different inputs. The results empirically validate the theoretical claim that optimal dynamic policies outperform the best static workflow when candidate workflows are heterogeneous.", "conclusion": "The paper concludes that static, one-size-fits-all Text-to-SQL workflows fundamentally limit performance and robustness, particularly in real-world, heterogeneous query settings. By casting workflow construction as a reinforcement learning problem and training a policy (SquRL) to dynamically assemble workflows at inference time, systems can achieve superior accuracy, especially on challenging or out-of-distribution inputs. The proposed rule-based reward, dynamic actor masking, and pseudo-reward mechanisms form a practical recipe for improving LLM-based reasoning in Text-to-SQL and potentially other tasks requiring adaptive workflow orchestration."}}
{"id": "2602.15578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15578", "abs": "https://arxiv.org/abs/2602.15578", "authors": ["Chaithra Nerella", "Chiranjeevi Yarra"], "title": "Clinically Inspired Symptom-Guided Depression Detection from Emotion-Aware Speech Representations", "comment": "5 pages, 3 figures", "summary": "Depression manifests through a diverse set of symptoms such as sleep disturbance, loss of interest, and concentration difficulties. However, most existing works treat depression prediction either as a binary label or an overall severity score without explicitly modeling symptom-specific information. This limits their ability to provide symptom-level analysis relevant to clinical screening. To address this, we propose a symptom-specific and clinically inspired framework for depression severity estimation from speech. Our approach uses a symptom-guided cross-attention mechanism that aligns PHQ-8 questionnaire items with emotion-aware speech representations to identify which segments of a participant's speech are more important to each symptom. To account for differences in how symptoms are expressed over time, we introduce a learnable symptom-specific parameter that adaptively controls the sharpness of attention distributions. Our results on EDAIC, a standard clinical-style dataset, demonstrate improved performance outperforming prior works. Further, analyzing the attention distributions showed that higher attention is assigned to utterances containing cues related to multiple depressive symptoms, highlighting the interpretability of our approach. These findings outline the importance of symptom-guided and emotion-aware modeling for speech-based depression screening.", "AI": {"tldr": "They propose a speech-based depression severity model that explicitly targets individual depressive symptoms using a symptom-guided cross-attention mechanism aligned with PHQ-8 items, achieving better performance and interpretability on a clinical dataset.", "motivation": "Existing speech-based depression prediction models typically output a single binary label or an overall severity score, ignoring how individual depressive symptoms (like sleep issues, anhedonia, concentration problems) are manifested in speech. This lack of symptom-level modeling limits both clinical relevance and interpretability, since clinicians care about which specific symptoms are present and how severe they are. The authors are motivated to build a model that reflects clinical practice by estimating symptom-specific severities and explaining which speech segments contribute to each symptom assessment.", "method": "They design a symptom-specific, clinically inspired framework that takes speech and PHQ-8 questionnaire structure into account. First, they extract emotion-aware speech representations. Then they use a symptom-guided cross-attention mechanism that aligns each PHQ-8 item (representing a particular depressive symptom) with the speech representations. This attention mechanism learns to highlight which utterances or segments of speech are most informative for each symptom. To capture that different symptoms may appear in more localized or more diffuse ways over time, they introduce a learnable, symptom-specific parameter that adjusts the sharpness (concentration) of the attention distribution for each symptom dimension. The model is trained on the EDAIC dataset to estimate depression severity and symptom scores.", "result": "On the EDAIC clinical-style dataset, their approach achieves better depression severity estimation performance than previous speech-based methods that do not use symptom-guided modeling. Analysis of the learned attention weights shows that utterances containing cues of multiple depressive symptoms receive higher attention, suggesting that the model is focusing on clinically meaningful speech segments. This supports both performance gains and interpretability claims.", "conclusion": "Incorporating symptom-specific, clinically grounded structure into speech-based depression models improves both predictive performance and interpretability. By aligning PHQ-8 symptom items with emotion-aware speech features via a symptom-guided cross-attention mechanism, and allowing symptom-dependent control of attention sharpness, the framework can identify speech segments that are most relevant to individual depressive symptoms. This makes the system more suitable for clinically oriented depression screening, emphasizing the value of symptom-guided and emotion-aware modeling in mental health assessment from speech."}}
{"id": "2602.15620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15620", "abs": "https://arxiv.org/abs/2602.15620", "authors": ["Shiqi Liu", "Zeyu He", "Guojian Zhan", "Letian Tao", "Zhilong Zheng", "Jiang Wu", "Yinuo Wang", "Yang Guan", "Kehua Sheng", "Bo Zhang", "Keqiang Li", "Jingliang Duan", "Shengbo Eben Li"], "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens", "comment": null, "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.", "AI": {"tldr": "The paper identifies that a tiny fraction of \"spurious tokens\" causes instability in RL-based fine-tuning of LLMs and proposes STAPO, a method that masks these tokens during optimization, yielding more stable training and better reasoning performance.", "motivation": "Existing RL fine-tuning methods for large language models suffer from training instability and late-stage performance collapse, despite using heuristic stabilizers like entropy regularization and reweighting. The authors want to understand the source of this instability at the token level and design a more principled, robust optimization method.", "method": "1) Theoretically analyze token-wise policy gradient magnitudes in RL, showing they are negatively correlated with token probability and local entropy. 2) Identify a tiny set (~0.01%) of \"spurious tokens\" that receive disproportionately large gradients because they appear in high-reward sequences but contribute little to correct reasoning. 3) Propose Spurious-Token-Aware Policy Optimization (STAPO), which detects these tokens, masks their gradient contributions, and renormalizes the loss over remaining tokens. 4) Empirically evaluate STAPO against baselines (GRPO, 20-Entropy, JustRL) on several math reasoning benchmarks with Qwen models of different sizes.", "result": "STAPO yields more stable entropy dynamics during RL fine-tuning and avoids the late-stage performance collapse seen in baseline methods. On six mathematical reasoning benchmarks and three Qwen model sizes (1.7B, 8B, 14B), STAPO improves performance by an average of 7.13% over GRPO, 20-Entropy, and JustRL.", "conclusion": "Training instability in RL-based LLM fine-tuning is largely caused by a very small subset of spurious tokens that get excessively amplified gradients due to reward assignment. By explicitly identifying and suppressing the influence of these tokens via STAPO, one can achieve more stable training and significantly better reasoning performance across model scales and benchmarks."}}
{"id": "2602.15675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15675", "abs": "https://arxiv.org/abs/2602.15675", "authors": ["Ahmed Khaled Khamis", "Hesham Ali"], "title": "LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models", "comment": "8 pages, 2 figures, EACL26", "summary": "Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.", "AI": {"tldr": "Introduces NileTTS, a 38-hour Egyptian Arabic TTS dataset created via a synthetic LLM-based pipeline, plus a fine-tuned XTTS v2 model and open-source resources for Egyptian Arabic speech synthesis.", "motivation": "Arabic TTS research has focused mainly on Modern Standard Arabic and some Gulf dialects, leaving Egyptian Arabic\u2014though widely understood\u2014without sufficient high-quality, publicly available resources. This lack of data limits progress in building accurate, natural-sounding TTS systems for Egyptian Arabic and constrains broader speech technologies that need dialect coverage.", "method": "The authors build NileTTS, a 38-hour speech corpus for Egyptian Arabic, using a synthetic data generation pipeline. First, large language models generate written content in Egyptian Arabic across various domains (medical, sales, general conversation). Then audio synthesis tools convert this text into speech. The resulting audio is automatically transcribed and subjected to speaker diarization, followed by manual quality verification and cleanup. They then fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on this dataset and compare it against a baseline XTTS v2 model trained on other Arabic dialects.", "result": "They obtain a 38-hour, two-speaker Egyptian Arabic speech dataset spanning multiple domains, with aligned transcriptions and speaker annotations. Fine-tuning XTTS v2 on this data yields an Egyptian Arabic TTS system that outperforms the baseline model trained on non-Egyptian Arabic dialect data, indicating that the synthetic corpus is effective for adapting TTS to this dialect. They also package and release the dataset, generation pipeline, and fine-tuned model as open resources.", "conclusion": "The work fills a major resource gap for Egyptian Arabic by releasing the first public TTS dataset for this dialect, along with a reproducible synthetic data pipeline and a fine-tuned TTS model. The positive evaluation against a baseline shows that synthetic, LLM-driven data generation is a viable strategy for under-resourced dialectal TTS, and the released resources are intended to catalyze further research and development in Egyptian Arabic speech synthesis."}}
{"id": "2602.15678", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15678", "abs": "https://arxiv.org/abs/2602.15678", "authors": ["Edirlei Soares de Lima", "Marco A. Casanova", "Antonio L. Furtado"], "title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models", "comment": null, "summary": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $\u03ba$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.", "AI": {"tldr": "The paper proposes and validates a character-function framework, grounded in Frye\u2019s four narrative genres and Jungian archetypes, using LLMs to test genre-specific character roles across literary works.", "motivation": "Existing computational work on Northrop Frye\u2019s narrative genres concentrates on plot patterns and overlooks how character roles differ systematically across genres. The authors want a principled, testable way to model character functions that complements pattern-based narratology and can support narrative generation and interactive storytelling.", "method": "The authors map Jungian psychic structure components to four universal character functions (protagonist, mentor, antagonist, companion), then specialize these into sixteen genre-specific roles aligned with Frye\u2019s four genres using prototypical literary works. They then perform a multi-model evaluation with six state-of-the-art LLMs, asking them to judge character\u2013role correspondences in 40 narratives. The test set includes 160 positive (valid) and 30 negative (invalid) correspondences to assess both recognition and rejection capabilities. Performance is assessed with balanced accuracy and inter-model agreement (Fleiss\u2019 \u03ba).", "result": "LLMs show strong ability to identify valid and invalid character\u2013role correspondences, achieving a mean balanced accuracy of 82.5% and substantial inter-model agreement (Fleiss\u2019 \u03ba = 0.600). Performance differs by genre (72.7%\u201389.9%) and by role (52.5%\u201399.2%). Qualitative analysis links these variations to real narrative phenomena, such as more diffuse functional distribution in romance and intentional archetype subversion in satire.", "conclusion": "The proposed character-function framework successfully captures systematic structural patterns in how archetypal roles manifest across Frye\u2019s four genres, and LLM evaluations provide empirical support for its validity. This character-based, archetype-informed approach enriches computational narratology and offers a structured basis for future work in narrative generation and interactive storytelling systems."}}
{"id": "2602.15689", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.15689", "abs": "https://arxiv.org/abs/2602.15689", "authors": ["Meirav Segal", "Noa Linder", "Omer Antverg", "Gil Gekker", "Tomer Fichman", "Omri Bodenheimer", "Edan Maor", "Omer Nevo"], "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models", "comment": null, "summary": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.", "AI": {"tldr": "A framework for more nuanced, content-based refusal policies for LLMs in cybersecurity, explicitly balancing offensive risk and defensive benefit.", "motivation": "Current LLM refusal policies in cybersecurity over-rely on broad, topic-based or intent-based bans, leading to inconsistent decisions, unnecessary blocking of legitimate defensive use, and susceptibility to obfuscation or segmented requests.", "method": "Propose a content-grounded refusal framework that evaluates cyber-related requests along five dimensions\u2014Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users\u2014focusing on the actual technical substance of a request rather than stated user intent.", "result": "The framework is shown to resolve inconsistencies observed in current frontier models\u2019 behavior on cyber requests and to support construction of tunable, risk-aware refusal policies suitable for different organizational risk tolerances.", "conclusion": "Explicitly modeling offense-defense tradeoffs via content-based dimensions yields more robust, nuanced, and auditable refusal policies for LLMs in cybersecurity than broad topic bans or intent-only approaches."}}
{"id": "2602.15716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15716", "abs": "https://arxiv.org/abs/2602.15716", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "title": "Rethinking Metrics for Lexical Semantic Change Detection", "comment": "Accepted to the LChange 2026 Workshop, colocated with EACL 2026", "summary": "Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.", "AI": {"tldr": "The paper proposes two new metrics, Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), for detecting lexical semantic change using contextualized embeddings, showing they can outperform common metrics like APD and prototype cosine distance, especially with dimensionality reduction and non-specialised encoders.", "motivation": "Current lexical semantic change detection using contextualized language models relies mostly on a narrow set of metrics, particularly Average Pairwise Distance (APD) and cosine distance between word prototypes (PRT). These metrics may not be optimal across different settings (e.g., representation spaces, encoder specialisation, dimensionality reduction). There is a need to explore alternative ways to quantify semantic change that may be more robust and that better exploit local correspondences between word usages over time.", "method": "The authors introduce two new distance-based metrics for LSCD: Average Minimum Distance (AMD), which measures semantic change by computing, for each usage of a target word in one time period, the minimum distance to usages in another period and averaging these minima; and Symmetric Average Minimum Distance (SAMD), which symmetrizes AMD by considering both directions between time periods. They then evaluate these metrics across multiple languages, different encoder models (specialised vs non-specialised), and different representation spaces, including under dimensionality reduction, and compare their performance to existing metrics like APD and PRT.", "result": "Experimental results across languages, encoder types, and representation configurations show that AMD frequently yields more robust and often better performance than APD and prototype cosine distance, particularly when representations are compressed via dimensionality reduction and when using non-specialised encoders. SAMD performs especially well when used with specialised encoders designed for LSCD tasks. Overall, both metrics are competitive and sometimes superior alternatives to traditional LSCD measures.", "conclusion": "The study concludes that lexical semantic change detection with contextualised embeddings should look beyond the standard APD and prototype-based cosine metrics. The newly proposed AMD is a strong, robust general-purpose metric for quantifying semantic change, while SAMD is particularly advantageous with specialised encoders. Incorporating these alternative metrics can improve the reliability and effectiveness of contextualised embedding-based LSCD analyses."}}
{"id": "2602.15757", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15757", "abs": "https://arxiv.org/abs/2602.15757", "authors": ["Laura De Grazia", "Danae S\u00e1nchez Villegas", "Desmond Elliott", "Mireia Farr\u00fas", "Mariona Taul\u00e9"], "title": "Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos", "comment": null, "summary": "Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.", "AI": {"tldr": "The paper introduces FineMuSe, a Spanish multimodal sexism detection dataset with both binary and fine-grained labels, proposes a hierarchical taxonomy covering sexism, non-sexism, irony, and humor, and evaluates LLMs that approach human performance but still miss co-occurring visually-expressed sexist types.", "motivation": "Existing automated sexism detection tools mostly perform binary classification, which fails to capture subtle, context-dependent, and multimodal manifestations of sexism, leaving many nuanced or indirect forms undetected.", "method": "The authors build FineMuSe, a new Spanish-language multimodal dataset (text + images) annotated with both binary (sexist/non-sexist) and fine-grained labels based on a newly designed hierarchical taxonomy that includes sexism, non-sexism, and rhetorical devices like irony and humor. They then fine-tune and evaluate a range of large language models (including multimodal ones) on both binary and fine-grained sexism detection tasks.", "result": "Multimodal LLMs achieve performance comparable to human annotators in detecting nuanced sexism overall but show weaknesses in recognizing multiple overlapping sexist categories, especially when some of these categories are expressed primarily through visual features in the images.", "conclusion": "FineMuSe and its hierarchical taxonomy enable more granular analysis and modeling of online sexism in Spanish. While multimodal LLMs can effectively detect many subtle sexist instances, they still struggle with complex, co-occurring sexist types tied to visual cues, highlighting the need for better multimodal reasoning and representation of visual context in sexism detection models."}}
{"id": "2602.15730", "categories": ["cs.CL", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.15730", "abs": "https://arxiv.org/abs/2602.15730", "authors": ["Omri Feldman", "Amar Venugopal", "Jann Spiess", "Amir Feder"], "title": "Causal Effect Estimation with Latent Textual Treatments", "comment": null, "summary": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.", "AI": {"tldr": "They build an end-to-end pipeline that uses sparse autoencoders and causal adjustment techniques to estimate how changing latent textual properties affects downstream outcomes, while correcting bias from confounded text features.", "motivation": "Causal inference with text (\"text-as-treatment\") is important for understanding how wording and latent textual properties influence outcomes such as user behavior, persuasion, or decisions. However, text is high-dimensional and entangles treatment-relevant features with covariates, making controlled experimentation and unbiased causal estimation difficult. Existing uses of LLMs for text generation often lack precise control over which latent feature is varied and ignore confounding, leading to biased effect estimates.", "method": "They propose an end-to-end pipeline with two main stages. First, they use sparse autoencoders (SAEs) to discover interpretable latent dimensions of text and to steer LLM-generated text along selected latent directions, producing controlled textual interventions that vary specific features while holding others relatively constant. Second, for causal estimation, they analyze the bias that arises because textual treatments encode both treatment and covariate information, then introduce a covariate residualization strategy to adjust for this and obtain more accurate estimates of causal effects of the latent interventions. The pipeline is designed to address both computational challenges in generating suitable text variants and statistical challenges in unbiased estimation.", "result": "They show experimentally that naive causal estimation methods applied to text treatments are substantially biased due to the conflation of treatment and covariates in text. Their pipeline, by steering text via SAEs and using covariate residualization, induces meaningful variation in the intended target features and substantially reduces estimation error compared to naive approaches. Empirical evaluations demonstrate improved accuracy and robustness of causal effect estimates in text-as-treatment experiments.", "conclusion": "The proposed SAE-based generation and covariate-residualized estimation pipeline offers a practical and statistically sound way to study causal effects of latent textual properties. It enables controlled, LLM-based text interventions and corrects key sources of bias in text-as-treatment designs, providing a stronger foundation for causal analysis in applications where text influences outcomes."}}
{"id": "2602.15758", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15758", "abs": "https://arxiv.org/abs/2602.15758", "authors": ["Manav Nitin Kapadnis", "Lawanya Baghel", "Atharva Naik", "Carolyn Ros\u00e9"], "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models", "comment": "16 pages, 13 figures including Supplementary Material", "summary": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.", "AI": {"tldr": "The paper introduces ChartEditBench, a benchmark to evaluate how well multimodal large language models handle multi-turn, code-based chart editing for realistic exploratory data analysis.", "motivation": "Existing MLLM benchmarks mostly test single-turn or one-shot chart generation, which does not reflect real-world exploratory data analysis where users iteratively refine visualizations. There is a need to measure models' abilities to maintain context, track edits, and adapt to changing user preferences over multiple turns.", "method": "The authors construct ChartEditBench, a dataset of 5,000 controlled, incremental chart modification chains involving visually grounded, code-based edits, along with a human-verified subset. They design an evaluation framework that combines code execution fidelity, pixel-level image similarity, and logical code checks to robustly assess model performance, addressing weaknesses of prior LLM-as-a-Judge metrics. They then test state-of-the-art multimodal LLMs on this benchmark.", "result": "Experiments show that current MLLMs perform well on isolated, stylistic chart edits but their performance significantly degrades in multi-turn scenarios. They suffer from error accumulation, problems maintaining shared context, and frequent execution failures, especially for data-centric transformations.", "conclusion": "ChartEditBench provides a challenging and more realistic benchmark for assessing multimodal LLMs' abilities in grounded, intent-aware, multi-turn chart editing. The results highlight key limitations of current models in supporting exploratory data analysis and point to the need for improving context tracking, error robustness, and data transformation capabilities."}}
{"id": "2602.15753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15753", "abs": "https://arxiv.org/abs/2602.15753", "authors": ["Chahan Vidal-Gor\u00e8ne", "Bastien Kindt", "Florian Cafiero"], "title": "Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac", "comment": null, "summary": "Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.", "AI": {"tldr": "The paper evaluates how well modern large language models can perform lemmatization and POS tagging for four low-resource historical languages in zero- and few-shot settings, comparing them to a specialized RNN baseline.", "motivation": "Low-resource and especially historical languages lack annotated data, making traditional supervised NLP methods hard to apply. The authors want to know whether recent large language models can help bootstrap core linguistic annotation tasks like lemmatization and POS tagging without language-specific training data.", "method": "They build a new benchmark with aligned training and out-of-domain test corpora for Ancient Greek, Classical Armenian, Old Georgian, and Syriac. They then evaluate several GPT-4 variants and open-weight Mistral models on lemmatization and POS tagging in zero-shot and few-shot prompting setups, and compare their performance to PIE, a task-specific RNN model trained in the traditional way.", "result": "In few-shot settings, large language models generally match or outperform the PIE RNN baseline on both lemmatization and POS tagging for most of the languages, even without any fine-tuning. However, performance is weaker for languages with particularly complex morphology and non-Latin scripts.", "conclusion": "LLMs can already serve as credible tools to kick-start annotation for under-resourced historical languages, especially in few-shot scenarios, though challenges remain for highly complex and non-Latin-script languages. They are useful as an aid to human annotators when no substantial labeled data exist."}}
{"id": "2602.15814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15814", "abs": "https://arxiv.org/abs/2602.15814", "authors": ["Devang Acharya", "Mohammad Hammoud"], "title": "Avey-B", "comment": null, "summary": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.", "AI": {"tldr": "The paper introduces an improved, encoder-only, attention-free architecture derived from Avey that outperforms common compact Transformer encoders on key NLP tasks under tight compute and memory budgets.", "motivation": "Industrial NLP often relies on compact BERT-style encoders because they offer strong bidirectional contextualization and parallelism, but self-attention is expensive for long sequences and resource-constrained settings. There is a need for simpler, more efficient encoder architectures that maintain or improve quality while scaling better to long contexts and fitting tight compute/memory budgets.", "method": "The authors reformulate the Avey architecture for an encoder-only setting, removing autoregression while retaining its attention-free design. They introduce three key innovations: (1) decoupled static and dynamic parameterizations to better separate fixed and input-dependent components of the model, (2) stability-oriented normalization techniques to improve training and inference robustness, and (3) neural compression mechanisms to more efficiently represent and process long-context information. They then compare this new architecture against four widely used Transformer-based encoders on standard token-classification and information-retrieval benchmarks, focusing on both performance and scaling behavior with longer contexts.", "result": "The proposed encoder-only, attention-free Avey variant consistently outperforms four popular compact Transformer-based encoders on standard token-classification and information-retrieval benchmarks. It also scales more efficiently to long contexts, offering improvements in either speed, memory usage, or both, while maintaining or improving task performance.", "conclusion": "An attention-free, encoder-only reformulation of Avey with architectural enhancements can serve as a competitive, and in some settings superior, alternative to compact Transformer-based encoders in industrial NLP, particularly when operating under tight compute and memory constraints and when handling long-context inputs."}}
{"id": "2602.15778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15778", "abs": "https://arxiv.org/abs/2602.15778", "authors": ["Quentin Lemesle", "L\u00e9ane Jourdan", "Daisy Munson", "Pierre Alain", "Jonathan Chevelu", "Arnaud Delhay", "Damien Lolive"], "title": "*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation", "comment": "Under review", "summary": "Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.", "AI": {"tldr": "They enhance a perplexity-based LLM-as-a-judge metric (ParaPLUIE) with task-specific prompting variants (*-PLUIE) to better match human judgment at low computational cost.", "motivation": "LLM-as-a-judge methods are accurate but expensive and require text generation and post-processing. There is a need for cheaper, generation-free automatic evaluation metrics for LLM outputs that still correlate well with human judgments.", "method": "They extend ParaPLUIE, a perplexity-based confidence estimator over yes/no answers that avoids generation, by designing task-specific prompting variants collectively called *-PLUIE. They then evaluate these variants by measuring correlation between *-PLUIE scores and human ratings across tasks.", "result": "Task-personalised *-PLUIE variants show higher correlation with human judgments than the original ParaPLUIE, while retaining the computational efficiency benefits of perplexity-based, non-generative evaluation.", "conclusion": "Task-specific, perplexity-based LLM-judge metrics like *-PLUIE can provide an efficient and reliable alternative to standard LLM-as-a-judge approaches, improving alignment with human evaluation without incurring high computational costs."}}
