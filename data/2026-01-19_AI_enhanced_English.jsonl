{"id": "2601.10718", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10718", "abs": "https://arxiv.org/abs/2601.10718", "authors": ["Junyu Liu", "Siwen Yang", "Dexiu Ma", "Qian Niu", "Zequn Zhang", "Momoko Nagai-Tanima", "Tomoki Aoyama"], "title": "Japanese AI Agent System on Human Papillomavirus Vaccination: System Design", "comment": null, "summary": "Human papillomavirus (HPV) vaccine hesitancy poses significant public health challenges, particularly in Japan where proactive vaccination recommendations were suspended from 2013 to 2021. The resulting information gap is exacerbated by misinformation on social media, and traditional ways cannot simultaneously address individual queries while monitoring population-level discourse. This study aimed to develop a dual-purpose AI agent system that provides verified HPV vaccine information through a conversational interface while generating analytical reports for medical institutions based on user interactions and social media. We implemented a system comprising: a vector database integrating academic papers, government sources, news media, and social media; a Retrieval-Augmented Generation chatbot using ReAct agent architecture with multi-tool orchestration across five knowledge sources; and an automated report generation system with modules for news analysis, research synthesis, social media sentiment analysis, and user interaction pattern identification. Performance was assessed using a 0-5 scoring scale. For single-turn evaluation, the chatbot achieved mean scores of 4.83 for relevance, 4.89 for routing, 4.50 for reference quality, 4.90 for correctness, and 4.88 for professional identity (overall 4.80). Multi-turn evaluation yielded higher scores: context retention 4.94, topic coherence 5.00, and overall 4.98. The report generation system achieved completeness 4.00-5.00, correctness 4.00-5.00, and helpfulness 3.67-5.00, with reference validity 5.00 across all periods. This study demonstrates the feasibility of an integrated AI agent system for bidirectional HPV vaccine communication. The architecture enables verified information delivery with source attribution while providing systematic public discourse analysis, with a transferable framework for adaptation to other medical contexts.", "AI": {"tldr": "The paper develops and evaluates an AI-based system that both answers people\u2019s HPV vaccine questions via chat and generates analytical reports for medical institutions using user queries and social media data.", "motivation": "HPV vaccine hesitancy is a serious public health issue in Japan, intensified by years of suspended proactive vaccine recommendations and widespread misinformation on social media. Existing approaches cannot efficiently offer accurate, personalized answers while also tracking public sentiment and discourse at scale. The authors aim to fill this gap by creating an AI system that simultaneously supports individuals and informs healthcare organizations.", "method": "The authors built an integrated AI agent system with three main components: (1) a vector database that unifies multiple trusted sources (academic literature, government information, news, and social media); (2) a Retrieval-Augmented Generation (RAG) chatbot using a ReAct-style agent that orchestrates multiple tools across five knowledge sources to answer questions conversationally with citations; and (3) an automated report generator that analyzes news, synthesizes research, performs social media sentiment analysis, and detects patterns in user interactions. They evaluated the system using human scoring (0\u20135 scale) on dimensions such as relevance, correctness, routing, reference quality, professional identity, context retention, and report quality.", "result": "In single-turn tests, the chatbot achieved near-maximal scores on relevance (4.83), tool routing (4.89), reference quality (4.50), correctness (4.90), and professional identity (4.88), with an overall score of 4.80. In multi-turn conversations, performance was even higher, with context retention 4.94, topic coherence 5.00, and overall 4.98. The report-generation component scored between 4.00 and 5.00 on completeness and correctness, 3.67 to 5.00 on helpfulness, and a perfect 5.00 on reference validity across all evaluation periods.", "conclusion": "The study shows that a dual-purpose AI agent system is feasible for HPV vaccine communication: it can reliably provide verified, source-attributed information to individuals via chat while simultaneously producing structured analyses of public discourse for medical institutions. The proposed architecture is generalizable and can be adapted to other medical or public-health domains requiring both patient-facing information delivery and population-level monitoring."}}
{"id": "2601.10719", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10719", "abs": "https://arxiv.org/abs/2601.10719", "authors": ["Gerard Yeo", "Svetlana Churina", "Kokil Jaidka"], "title": "Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models", "comment": null, "summary": "Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.", "AI": {"tldr": "The paper investigates whether modern large language models (LLMs) internally represent human-like perceptions of trustworthiness in online text, finding that they do encode such signals in a structured, decodable way across layers and attention heads.", "motivation": "Trustworthiness is crucial for how users evaluate online content, especially as LLMs are woven into search, recommendation, and conversational systems. However, it is not yet clear whether these models capture trust as a psychologically meaningful construct, rather than just correlating surface features. Understanding this is important both for safety and for designing AI systems that present credible and trustworthy information.", "method": "The authors use instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) and feed them web-like narratives from the PEACE-Reviews dataset, which has human annotations for cognitive appraisals, emotions, and behavioral intentions relating to trust. They analyze internal activations at the layer and attention-head level to see if there are systematic differences between high- and low-trust texts. They then perform probing analyses to test whether trust signals are linearly decodable from these representations and examine how instruction tuning (fine-tuning) modifies the underlying trust representations.", "result": "They find clear, systematic activation differences between high- and low-trust texts across layers and heads, indicating that trust cues are implicitly encoded during pretraining. Linear probes can reliably decode perceived trustworthiness from model representations. Instruction tuning appears to refine these trust representations rather than fundamentally changing them. The strongest representational associations align with human trust-related appraisals such as fairness, certainty, and accountability of the information source.", "conclusion": "Modern LLMs naturally internalize psychologically grounded signals of perceived trustworthiness even without explicit trust supervision. These signals are structured, decodable, and linked to core human trust dimensions, providing a representational basis that can be leveraged to build more credible, transparent, and trustworthy AI systems for web applications."}}
{"id": "2601.10726", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10726", "abs": "https://arxiv.org/abs/2601.10726", "authors": ["Ross Chu", "Yuting Huang"], "title": "Building AI Agents to Improve Job Referral Requests to Strangers", "comment": null, "summary": "This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.", "AI": {"tldr": "AI agents use LLMs with RAG to rewrite job-referral requests, boosting predicted success for weaker messages without hurting stronger ones.", "motivation": "Job seekers often struggle to craft effective referral requests on professional platforms, and experimentation on real users is costly and risky. The authors want a scalable, low-cost way to systematically improve such requests and identify promising message features before running real-world tests.", "method": "They design a two-agent AI workflow: (1) an improver agent (LLM) rewrites user referral requests; (2) an evaluator agent scores each version using a predictive model trained on historical data to estimate the probability of receiving a referral. They compare vanilla LLM rewriting against an enhanced version that uses Retrieval-Augmented Generation (RAG) to ground the LLM in additional guidance. They then examine how revisions affect predicted success rates for initially weak vs. strong requests.", "result": "Without RAG, LLM edits help weaker requests (raising predicted success) but can harm stronger ones (lowering predicted success). Adding RAG guides the LLM to avoid degrading strong requests and further amplifies the gains for weak ones. With RAG, predicted success for weaker requests increases by about 14% while leaving stronger requests\u2019 predicted success unchanged on average.", "conclusion": "A two-agent system with an LLM improver and a predictive evaluator can systematically enhance job-referral requests, especially when the LLM is augmented with retrieval. RAG-stabilized rewriting appears particularly beneficial for weaker messages, delivering sizable gains in model-predicted success with minimal downside risk, and offering a low-cost signal of promising interventions before conducting expensive real-world experiments."}}
{"id": "2601.10775", "categories": ["cs.CL", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10775", "abs": "https://arxiv.org/abs/2601.10775", "authors": ["Tommaso Felice Banfi", "Sashenka Gamage"], "title": "LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning", "comment": "Accepted at AAAI 2026 Bridge (Logical and Symbolic Reasoning in Language Models)", "summary": "We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \\emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \\(-11.6\\%\\) with the baseline LLM to \\(+9.5\\%\\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.", "AI": {"tldr": "The paper introduces an LLM-based framework that uses token-level uncertainty (entropy) to adapt how much context and chain-of-thought reasoning is used when playing Tic-Tac-Toe, significantly improving game performance against a suboptimal opponent.", "motivation": "Standard LLM reasoning in discrete, game-like settings is often either too shallow (leading to mistakes) or overly verbose and costly, and it typically ignores the model\u2019s own uncertainty signals. The authors want a principled way to allocate more reasoning effort only when the model is uncertain, to improve decision quality in sequential decision-making tasks while keeping costs manageable.", "method": "They design a framework that combines in-context learning, entropy-guided chain-of-thought reasoning, and adaptive retrieval of examples. For each move, the model\u2019s token-level predictive entropy is computed. Low-entropy (confident) states trigger short, single-path CoT with minimal retrieved examples; high-entropy (uncertain) states trigger expanded, multi-path CoT reasoning and more retrieved examples. The system is evaluated on Tic-Tac-Toe against a sub-optimal algorithmic opponent, tracking game outcomes and model behavior across different uncertainty levels.", "result": "Compared to a baseline LLM without entropy-aware adaptation, the proposed method improves the average outcome from -11.6% to +9.5% (on a -1/0/+1 loss/tie/win scale) over 100 games, while keeping the number of LLM calls relatively low. Statistical tests indicate the performance gain is significant, and correlation analysis shows a negative relationship between token-level entropy and move optimality, supporting the idea that entropy is a useful signal for when more reasoning is needed.", "conclusion": "Uncertainty-guided, adaptive reasoning\u2014using entropy to control both the depth (number of CoT paths) and breadth (amount of retrieved context)\u2014can significantly enhance LLM performance in discrete, sequential decision-making tasks such as Tic-Tac-Toe, without incurring prohibitive computation costs. This suggests a general strategy for deploying LLMs more efficiently and effectively in game-theoretic and other multi-step decision environments."}}
{"id": "2601.10738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10738", "abs": "https://arxiv.org/abs/2601.10738", "authors": ["Percy Jardine"], "title": "CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems", "comment": null, "summary": "Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.", "AI": {"tldr": "The paper introduces Constrained Temporal Hierarchical Architecture (CTHA), a framework that adds formally constrained communication and control between temporal layers in hierarchical agents to restore coordination stability and improve robustness and scalability.", "motivation": "Existing multi-time-scale (temporal hierarchical) agent architectures outperform single-loop agents but lose the inherent coordination stability of unified systems. This leads to inter-layer conflicts, unbounded error propagation, and poor scalability when tasks grow in complexity. There is a need for a principled way to organize and constrain inter-layer communication and control so that temporal hierarchies remain stable and reliable.", "method": "The authors propose CTHA, which imposes three types of constraints on a temporal hierarchical architecture: (1) Message Contract Constraints: inter-layer communication is structured into typed packets (summary, plan, policy), clarifying what information can be exchanged. (2) Authority Manifold Constraints: each layer\u2019s decision space is bounded to match its temporal scope, preventing layers from overstepping their role. (3) Arbiter Resolution Constraints: a formal arbitration mechanism composes decisions from multiple layers to ensure they are conflict-free. Together, these constraints effectively project inter-layer communication and control onto structured manifolds that preserve stability.", "result": "Experiments on complex, large-scale tasks show that CTHA significantly improves robustness and efficiency compared to unconstrained hierarchical baselines, including: 47% reduction in failure cascades, a 2.3x gain in sample efficiency, and better scalability as task complexity increases.", "conclusion": "CTHA provides a principled, constraint-based extension of temporal hierarchical agent architectures that restores coordination stability, reduces cascading failures, and improves scalability and efficiency. The authors argue that these structured constraints on communication and control can deepen our understanding of multi-agent coordination and guide the design of more robust autonomous systems moving forward."}}
{"id": "2601.10804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10804", "abs": "https://arxiv.org/abs/2601.10804", "authors": ["Syed Waqas Zamir", "Wassim Hamidouche", "Boulbaba Ben Amor", "Luana Marotti", "Inbal Becker-Reshef", "Juan Lavista Ferres"], "title": "BYOL: Bring Your Own Language Into LLMs", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .", "AI": {"tldr": "The paper proposes BYOL, a framework for building and improving LLMs for low- and extreme-low-resource languages through language-tier classification, data refinement, synthetic data generation, continual pretraining, and translation-mediated pathways, achieving significant gains for Chichewa, Maori, and Inuktitut.", "motivation": "Although LLMs appear multilingual, they are heavily biased toward a small set of high-resource languages, leaving thousands of low- and extreme-low-resource languages with poor performance, cultural misalignment, and limited access. The authors aim to systematically close this gap in a scalable way, rather than handling each language ad hoc.", "method": "1) Classify languages into four resource tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora.\n2) For low-resource languages: design a full-stack pipeline that includes corpus cleaning, synthetic text generation, continual pretraining on refined/expanded data, and supervised finetuning, followed by weight-space model merging to preserve multilingual/English abilities.\n3) For extreme-low-resource languages: build a translation-mediated inclusion pathway, in which a tailored machine translation system is developed to bridge between the target language and a high-resource language, allowing LLM access when direct language modeling is impractical.\n4) Implement and evaluate these pipelines on specific case languages (Chichewa, Maori, Inuktitut) and release new benchmark translations and code/models.", "result": "For low-resource Chichewa and Maori, the language-specific LLMs built with the proposed pipeline outperform strong multilingual baselines by about 12% on average across 12 benchmarks, while maintaining English and broader multilingual performance through model merging. For extreme-low-resource Inuktitut, the specialized MT system beats a commercial baseline by 4 BLEU points, enabling higher-accuracy interaction with LLMs via translation. The authors also produce human-translated Global MMLU-Lite benchmarks for Chichewa, Maori, and Inuktitut and release their implementation and models publicly.", "conclusion": "BYOL provides a practical, scalable framework to make LLMs more inclusive across the full spectrum of language resources. By tailoring integration pathways to language resource tiers and combining data refinement, synthetic augmentation, continual pretraining, finetuning, model merging, and translation-mediated access, the approach significantly improves performance for low- and extreme-low-resource languages while preserving capabilities in high-resource ones, and contributes reusable benchmarks and tools for the community."}}
{"id": "2601.10744", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.10744", "abs": "https://arxiv.org/abs/2601.10744", "authors": ["Sen Wang", "Bangwei Liu", "Zhenkun Gao", "Lizhuang Ma", "Xuhong Wang", "Yuan Xie", "Xin Tan"], "title": "Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration", "comment": "Our dataset and code will be released at our \\href{https://wangsen99.github.io/papers/lmee/}{website}", "summary": "An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.", "AI": {"tldr": "The paper introduces LMEE, a framework and benchmark for lifelong, long-term-memory-based embodied exploration, and proposes MemoryExplorer, an RL-finetuned multimodal LLM agent that actively queries and uses memory to improve long-horizon task performance.", "motivation": "Current embodied agents are mostly evaluated on one-shot tasks that emphasize final task success while ignoring the exploration process and the use of long-term episodic memory. This is insufficient for real-world, lifelong embodied agents that must handle complex, long-horizon tasks in open environments, where effective exploration and memory utilization are crucial for decision-making over time.", "method": "The authors define a new setting called Long-term Memory Embodied Exploration (LMEE), which unifies exploratory cognition and decision-making for lifelong learning. They build a dataset and benchmark, LMEE-Bench, that combines multi-goal navigation with memory-based question answering to evaluate both exploration processes and final outcomes. They propose MemoryExplorer, a method that fine-tunes a multimodal large language model via reinforcement learning to encourage active querying and use of a long-term memory module. The RL objective uses a multi-task reward including action prediction accuracy, frontier (unexplored-region) selection quality, and question-answering performance, so the agent learns proactive exploration and effective memory recall.", "result": "On LMEE-Bench, MemoryExplorer outperforms state-of-the-art embodied exploration models on long-horizon tasks. The experiments show that explicitly training the multimodal LLM to query and leverage memory, guided by the multi-task reward, leads to better performance in both navigation and memory-based QA, indicating improved long-term exploration and decision-making ability.", "conclusion": "Integrating explicit long-term episodic memory use and proactive exploration into the training of embodied agents is key for lifelong learning in complex environments. The proposed LMEE setting and LMEE-Bench provide a more complete evaluation of both exploration and outcomes, while MemoryExplorer demonstrates that RL-finetuned multimodal LLMs with memory-query incentives can achieve significant gains on long-horizon embodied tasks."}}
{"id": "2601.10809", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10809", "abs": "https://arxiv.org/abs/2601.10809", "authors": ["Young-Min Cho", "Yuan Yuan", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents", "comment": null, "summary": "Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.", "AI": {"tldr": "This paper studies how prompting for one conversational style (e.g., concise) in LLM agents unintentionally changes other perceived styles (e.g., expertise), showing that style controls are entangled rather than independent.", "motivation": "Practitioners routinely add style instructions like \u201cbe friendly\u201d or \u201cbe concise\u201d to steer LLM agents, assuming each style feature can be controlled independently. However, it is unclear whether emphasizing one style dimension unintentionally affects others (e.g., trustworthiness, expertise, politeness), which has implications for safety, user trust, and system design. The paper aims to systematically measure and characterize these cross-feature stylistic side effects.", "method": "The authors first survey 127 conversational-agent papers from the ACL Anthology to identify 12 commonly used style features. They then create controlled synthetic dialogues in both task-oriented and open-domain settings and systematically vary prompts to emphasize a single style feature at a time. Using an LLM-as-a-judge, pairwise evaluation framework, they causally quantify how prompting for one style feature changes human-perceived scores on all other style features. They further build CASSE, a dataset of these interactions, and test mitigation strategies, including modified prompts and activation-steering methods, to see whether suppressed traits can be restored without harming the primary style.", "result": "They find consistent, structured cross-feature side effects: prompting for one style often predictably increases or suppresses others. A notable example is that prompting for conciseness significantly lowers perceived expertise. Overall, style features show strong entanglement rather than behaving as independent, orthogonal controls. Mitigation attempts (prompt engineering and activation steering) can partially recover suppressed traits but frequently degrade the originally targeted style\u2019s strength or fidelity.", "conclusion": "Style prompts in LLM conversational agents do not operate as clean, isolated controls; they are entangled and can cause systematic, sometimes undesirable side effects on other stylistic traits. Existing mitigation approaches only partially solve the problem and introduce trade-offs. The authors argue that researchers and practitioners should abandon the assumption of faithful, single-objective style control and instead adopt multi-objective, principled approaches for safe and targeted stylistic steering, using resources like the CASSE dataset to better understand and manage these interactions."}}
{"id": "2601.10768", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.10768", "abs": "https://arxiv.org/abs/2601.10768", "authors": ["Nina Bo\u010dkov\u00e1", "Barbora Voln\u00e1", "Mirko Dohnal"], "title": "Optimisation of complex product innovation processes based on trend models with three-valued logic", "comment": null, "summary": "This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.", "AI": {"tldr": "The paper proposes a qualitative, trend-based modeling approach to describe and analyze complex product-innovation processes using transition graphs of possible system behaviours.", "motivation": "To better understand and analyze complex product-innovation processes without requiring precise numerical data or heavy quantitative models, enabling insight when information is scarce, uncertain, or largely qualitative.", "method": "The authors define a set of heuristics about how key variables in innovation processes evolve (increasing, decreasing, or constant). These trend descriptions are used as minimally information-intensive quantifiers to build qualitative trend models. A solution to such a model is constructed as a set of possible scenarios and allowable transitions between them, represented as a transition graph. System behaviour over time is then interpreted as paths through this graph.", "result": "The paper yields a modeling framework where complex innovation dynamics are captured as qualitative trend models whose solutions are transition graphs. These graphs enumerate possible past and future trajectories of the innovation process, subject to the specified heuristics and trends.", "conclusion": "Qualitative trend-based modeling, grounded in simple heuristic trends, can effectively represent complex product-innovation processes without relying on numeric data. Transition graphs derived from these models provide an interpretable structure of possible system evolutions, supporting exploration and analysis of innovation dynamics when information is limited or uncertain."}}
{"id": "2601.10825", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10825", "abs": "https://arxiv.org/abs/2601.10825", "authors": ["Junsol Kim", "Shiyang Lai", "Nino Scherrer", "Blaise Ag\u00fcera y Arcas", "James Evans"], "title": "Reasoning Models Generate Societies of Thought", "comment": null, "summary": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.", "AI": {"tldr": "The paper argues that strong reasoning in large language models arises from internally simulating a diverse multi-agent conversation, not just from longer chain-of-thought, and shows empirically and mechanistically that \u201csociety-of-mind\u201d structures yield better problem solving.", "motivation": "Although reasoning-focused LLMs outperform standard instruction-tuned models, it is unclear *why* they reason better. Prior work mainly attributes gains to extended computation (longer chains of thought), but does not explain the internal structure of this reasoning or its resemblance to human collective problem solving. The authors aim to uncover the mechanisms behind enhanced reasoning, especially the role of internal perspective diversity and interaction dynamics, to better understand and improve reasoning models.", "method": "The authors compare reasoning models (e.g., DeepSeek-R1, QwQ-32B) with instruction-tuned baselines on complex cognitive tasks. They analyze reasoning traces using quantitative metrics and mechanistic interpretability to detect patterns of \u2018perspective diversity,\u2019 including personality-like traits and domain-expertise features that are activated during reasoning. They characterize conversational behaviors inside the models\u2014questioning, perspective-shifting, debating, and reconciling conflicting views\u2014and identify socio-emotional roles in the back-and-forth. They then run controlled reinforcement learning experiments: (1) rewarding base models solely for reasoning accuracy to see if conversational behaviors emerge, and (2) fine-tuning with explicit conversational scaffolding to compare learning speed and reasoning improvement against base models.", "result": "Reasoning models display significantly richer diversity of internal perspectives than instruction-tuned models, with greater activation of heterogeneous personality- and expertise-linked features and more internal conflicts across these perspectives. Their reasoning traces show structured multi-agent-like conversations\u2014posing questions, shifting viewpoints, debating disagreements, and reconciling them\u2014accompanied by socio-emotional roles that mirror sharp, back-and-forth dialogue. These properties strongly correlate with improved task accuracy. In RL experiments, when models are rewarded only for reasoning accuracy, they naturally increase such conversational behaviors, and models fine-tuned with conversational scaffolding learn to reason more effectively and faster than base models without such scaffolding.", "conclusion": "Enhanced reasoning in LLMs is not simply a function of longer computation or longer chains-of-thought, but emerges from an internal \u2018society of thought\u2019: diverse, interacting sub-personalities and domain experts that debate and reconcile different views. This multi-agent-like internal structure parallels human collective intelligence, where organized diversity leads to superior problem solving. Designing and training models to explicitly encourage structured internal conversations and agent-like organization may therefore be a powerful way to further improve reasoning performance by better exploring solution spaces."}}
{"id": "2601.10904", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10904", "abs": "https://arxiv.org/abs/2601.10904", "authors": ["Fran\u00e7ois Chollet", "Mike Knoop", "Gregory Kamradt", "Bryan Landers"], "title": "ARC Prize 2025: Technical Report", "comment": null, "summary": "The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.", "AI": {"tldr": "The paper surveys results from the ARC Prize 2025 on the harder ARC-AGI-2 benchmark and analyzes how new \u201crefinement loop\u201d methods and knowledge-dependent overfitting shape current progress toward general intelligence, while previewing the next ARC-AGI-3 benchmark with interactive reasoning tasks.", "motivation": "Assess and push forward few-shot generalization and abstract reasoning capabilities in AI by using the ARC-AGI-2 benchmark and ARC Prize 2025 competition as a large-scale testbed, and understand what current methods and frontier models are truly learning versus overfitting to knowledge coverage or benchmark artifacts.", "method": "Conduct a survey and meta-analysis of the ARC Prize 2025 competition on ARC-AGI-2, including competition statistics, top-performing solution strategies, especially refinement-loop based methods (evolutionary program synthesis, application-layer refinement of commercial models, and weight-space refinement with zero-pretraining networks), and compare them with reported ARC-AGI scores of frontier models from major AI labs. Analyze limitations related to knowledge coverage and benchmark contamination, and design the next ARC-AGI-3 benchmark with new interactive reasoning components.", "result": "ARC-AGI-2 attracted 1,455 teams and 15,154 entries, with a best private-set score of 24%. Research activity and paper submissions nearly doubled, and a dominant strategy class\u2014the refinement loop\u2014emerged, spanning evolutionary, application-layer, and weight-space implementations, including surprisingly strong zero-pretraining small networks (~7M parameters). Major AI labs began reporting ARC-AGI results in model cards, but performance patterns reveal a strong dependence on prior knowledge coverage and contamination, rather than robust fluid reasoning alone.", "conclusion": "Refinement loops currently provide the most promising path for improving performance on ARC-style abstract reasoning, but present systems, including frontier models, remain limited by knowledge-dependent behavior and contamination effects. To better measure and drive progress toward genuine AGI-level reasoning, the authors propose ARC-AGI-3, a new benchmark introducing interactive tasks that stress exploration, planning, memory, goal acquisition, and alignment, going beyond static pattern-matching challenges."}}
{"id": "2601.10837", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.10837", "abs": "https://arxiv.org/abs/2601.10837", "authors": ["Guy Hadad", "Neomi Rabaev", "Bracha Shapira"], "title": "EncodeRec: An Embedding Backbone for Recommendation Systems", "comment": null, "summary": "Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.", "AI": {"tldr": "The paper introduces EncodeRec, a method to adapt frozen PLM text embeddings for recommendation, yielding compact, domain-aware item representations that improve recommendation performance.", "motivation": "Pre-trained language models provide rich text embeddings that are now widely used in recommender systems, but these embeddings are not optimized for recommendation objectives and often miss domain-specific semantics, limiting recommendation quality. There is a need for a way to better align PLM representations with the specific structure and discrimination requirements of recommendation tasks without incurring the heavy cost of fully fine-tuning large PLMs.", "method": "EncodeRec learns a recommendation-oriented embedding space directly from item descriptions while keeping the underlying PLM parameters frozen. It aligns textual representations with recommendation objectives, likely using an additional encoder or projection layer trained with recommendation losses. The approach is designed to create compact, informative embeddings that can serve both as the backbone representation for sequential recommendation models and as semantic ID tokenizers that map items to dense IDs, all trained without updating the large PLM weights.", "result": "On standard recommendation benchmarks, EncodeRec outperforms baselines that use off-the-shelf PLM embeddings as well as traditional embedding models. It shows strong performance both when used as the main representation module inside sequential recommenders and when used for semantic ID tokenization, achieving substantial accuracy gains and demonstrating better use of textual item descriptions.", "conclusion": "Adapting and structuring PLM-based textual embeddings for recommendation tasks is crucial for realizing their full potential in recommender systems. EncodeRec, by aligning frozen PLM representations with recommendation objectives and producing compact, domain-aware item embeddings, effectively bridges the gap between generic language models and practical recommendation performance, all while remaining computationally efficient."}}
{"id": "2601.10922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10922", "abs": "https://arxiv.org/abs/2601.10922", "authors": ["Yosub Shin", "Michael Buriek", "Boris Sobolev", "Pavel Bushuyeu", "Vikas Kumar", "Haoyang Xu", "Samuel Watson", "Igor Molybog"], "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge", "comment": null, "summary": "We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.", "AI": {"tldr": "The paper analyzes how different data curation strategies affect multimodal (vision-language) reasoning performance when the model and training setup are fixed, finding that selecting difficult, well-aligned examples matters far more than dataset size, diversity heuristics, or synthetic augmentation.", "motivation": "Multimodal reasoning models are often improved by scaling data, architectures, and training tricks together, making it hard to attribute performance gains specifically to data curation. The NeurIPS 2025 DCVLR challenge provides a controlled setting where the model and training protocol are fixed, allowing the authors to isolate and rigorously study how dataset selection and curation alone affect vision-language reasoning performance.", "method": "The authors participate in the DCVLR challenge using a compact curated dataset largely derived from the Walton Multimodal Cold Start corpus. They apply difficulty-based example selection on a base dataset that is already well-aligned with the evaluation task. After the competition, they conduct ablation studies comparing: (1) difficulty-based selection vs. random or other heuristics, (2) different dataset sizes under a fixed training recipe, and (3) additional curation strategies such as diversity-based selection and synthetic data augmentation, all while keeping the model and training protocol constant.", "result": "Their curated dataset achieved first place in the DCVLR challenge. Ablation experiments show that difficulty-based selection on an aligned base dataset is the main source of the performance improvements. Scaling dataset size beyond a compact core set does not consistently raise mean accuracy but mainly lowers run-to-run variance. Diversity-oriented selection and synthetic augmentations, which are commonly assumed to help, fail to improve and often hurt performance under this saturated, fixed-training setting.", "conclusion": "In the DCVLR challenge regime, multimodal reasoning performance is dominated by how well the curated data is aligned with the task and how effectively difficult examples are targeted, rather than by naively increasing dataset size, diversity, or synthetic data. The benchmark is effectively in a saturation regime where the model capacity and training recipe are the bottlenecks, so careful difficulty-based curation on an appropriate base dataset is the key to data-efficient gains in multimodal vision-language reasoning."}}
{"id": "2601.10896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10896", "abs": "https://arxiv.org/abs/2601.10896", "authors": ["Parisa Rabbani", "Priyam Sahoo", "Ruben Mathew", "Aishee Mondal", "Harshita Ketharaman", "Nimet Beyza Bozdag", "Dilek Hakkani-T\u00fcr"], "title": "DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference", "comment": "10 pages main content, 7 figures, 35 pages total with appendix", "summary": "LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify (\"Is this statement correct?\") versus attributed to a speaker (\"Is this speaker correct?\"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.", "AI": {"tldr": "The paper shows that large language models change their judgments about the same claim depending on whether it is framed as evaluating a statement or evaluating a speaker, and proposes a framework (DialDefer) and metric (DDS) to detect and mitigate this dialogic deference.", "motivation": "As LLMs are increasingly used as automated judges in evaluations, moderation, and decision-support, it is critical to know whether their judgments are stable, fair, and well-calibrated. However, existing evaluations largely focus on accuracy on decontextualized statements, overlooking how conversational framing (e.g., judging a statement vs. judging a person) may systematically bias model behavior. This gap is important because such framing is pervasive in real-world dialogue settings and could introduce hidden biases or inconsistencies without affecting aggregate accuracy metrics, thereby going unnoticed.", "method": "The authors define a phenomenon they call dialogic deference, where an LLM\u2019s verdict on a claim shifts when the same content is framed as a speaker\u2019s utterance rather than an abstract statement. They design DialDefer, a framework to systematically compare model responses under different conversational framings. Central to this is the Dialogic Deference Score (DDS), which measures directional shifts in model agreement or disagreement between framings, rather than just accuracy. They evaluate four LLMs across more than 3,000 instances in nine domains and also on natural Reddit conversations. They perform ablation studies to isolate the sources of framing effects, particularly the impact of attributing claims to humans versus LLMs. They further experiment with mitigation strategies to reduce deference-induced shifts and analyze their side effects.", "result": "The study finds that, across domains and models, conversational framing causes very large directional shifts in judgments, with absolute DDS values reaching up to 87 percentage points and highly significant p-values (< .0001), even though overall accuracy changes by less than 2 percentage points. The effect is substantially larger (2\u20134x) in natural Reddit dialogues compared to controlled settings. Models display both deference (shifting toward agreement) and skepticism (shifting toward disagreement) depending on the domain: for example, the same model shows strong skepticism (DDS = -53) in graduate-level science questions but strong deference (DDS = +58) in social judgment scenarios. Ablation experiments indicate that attributing content to humans versus LLMs is the dominant factor, with a 17.7 percentage point swing, suggesting that models are particularly reluctant to contradict humans. Mitigation techniques can dampen deference but sometimes overshoot, leading to excessive skepticism.", "conclusion": "Judgments made by LLMs in dialogic contexts are systematically shaped by how the task is framed\u2014evaluating a statement versus evaluating a speaker\u2014revealing a form of dialogic deference that is not captured by traditional accuracy metrics. This framing sensitivity raises concerns about the reliability and fairness of using LLMs as automated judges in real-world interactions. The proposed DialDefer framework and DDS metric allow practitioners to detect and quantify these hidden shifts and highlight that addressing them should be treated as a calibration problem rather than merely an accuracy optimization task. Effective mitigation requires carefully balancing deference and skepticism, especially in human-facing applications where models may unduly favor agreement with human speakers."}}
{"id": "2601.10918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10918", "abs": "https://arxiv.org/abs/2601.10918", "authors": ["Michael Ginn", "Alexis Palmer", "Mans Hulden"], "title": "Neural Induction of Finite-State Transducers", "comment": "14 pages, 8 figures, submitted to ARR Jan 2026", "summary": "Finite-State Transducers (FSTs) are effective models for string-to-string rewriting tasks, often providing the efficiency necessary for high-performance applications, but constructing transducers by hand is difficult. In this work, we propose a novel method for automatically constructing unweighted FSTs following the hidden state geometry learned by a recurrent neural network. We evaluate our methods on real-world datasets for morphological inflection, grapheme-to-phoneme prediction, and historical normalization, showing that the constructed FSTs are highly accurate and robust for many datasets, substantially outperforming classical transducer learning algorithms by up to 87% accuracy on held-out test sets.", "AI": {"tldr": "They automatically build efficient finite-state transducers from RNNs to solve string-to-string tasks, achieving strong accuracy gains over classic methods.", "motivation": "Finite-State Transducers are fast and suitable for many string rewriting tasks, but designing them manually is difficult and existing automatic transducer learning methods underperform modern neural networks. The authors want a way to combine the interpretability and efficiency of FSTs with the strong predictive performance of recurrent neural networks.", "method": "Train a recurrent neural network on string-to-string tasks, analyze and follow the hidden state geometry learned by the RNN, and from this derive/construct an unweighted finite-state transducer that mimics the RNN\u2019s behavior. They then apply this automatic construction method to multiple real-world tasks.", "result": "On tasks including morphological inflection, grapheme-to-phoneme prediction, and historical text normalization, the automatically constructed FSTs achieve high accuracy and robustness, substantially outperforming classical transducer learning algorithms, with gains of up to 87% accuracy on held-out test sets.", "conclusion": "Hidden state representations of RNNs can be exploited to automatically construct effective unweighted finite-state transducers. This approach combines RNN-learned structure with FST efficiency and interpretability, leading to significantly better performance than traditional transducer learning methods on several practical string-to-string tasks."}}
{"id": "2601.11012", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11012", "abs": "https://arxiv.org/abs/2601.11012", "authors": ["Jiahao Wang", "Shuangjia Zheng"], "title": "Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics", "comment": null, "summary": "The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.", "AI": {"tldr": "HADES is a Bayesian optimization framework that uses Hamiltonian dynamics and a structure-aware surrogate model to efficiently search protein sequence space and design variants with optimized properties while respecting protein structural constraints.", "motivation": "Existing sequence-based protein optimization methods struggle with complex epistasis and typically ignore structural constraints, leading to inefficient search and suboptimal or unrealistic designs. There is a need for an approach that can handle high-dimensional, epistatic fitness landscapes while explicitly incorporating protein structure\u2013sequence relationships.", "method": "The authors propose HADES, a Bayesian optimization method that samples from an approximated posterior using Hamiltonian dynamics, leveraging momentum and uncertainty to move proposals quickly toward promising regions of the search space. They introduce a position discretization scheme to convert continuous states into discrete protein sequences. The posterior surrogate model is a two-stage encoder\u2013decoder framework that captures local mutant neighborhood structure\u2013function relationships and learns a smoothed fitness landscape that is structure-aware. HADES iteratively proposes new protein variants by simulating Hamiltonian dynamics over this surrogate posterior and evaluating selected candidates in silico.", "result": "In extensive in-silico experiments, HADES outperforms state-of-the-art sequence-based baselines across most evaluation metrics in protein variant design. It is particularly effective at designing sequences that both share similar 3D structures with targets and exhibit improved or optimized functional properties.", "conclusion": "HADES provides an effective and structure-aware Bayesian optimization framework for protein engineering, overcoming limitations of prior sequence-only methods by explicitly modeling structure\u2013sequence constraints and using Hamiltonian dynamics to explore high-dimensional, epistatic landscapes efficiently. This makes it a promising tool for designing protein variants with desired properties while preserving structural integrity."}}
{"id": "2601.10925", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10925", "abs": "https://arxiv.org/abs/2601.10925", "authors": ["Michael Ginn", "Lindia Tjuatja", "Enora Rice", "Ali Marashian", "Maria Valentini", "Jasmine Xu", "Graham Neubig", "Alexis Palmer"], "title": "Massively Multilingual Joint Segmentation and Glossing", "comment": "13 pages, 8 figures, submitted to ARR Jan 2026", "summary": "Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.\n  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.", "AI": {"tldr": "This paper proposes PolyGloss, neural seq2seq models that jointly predict morpheme segmentation and interlinear glosses, improving both accuracy and alignment over prior systems like GlossLM and open-source LLMs, and supporting efficient adaptation via LoRA.", "motivation": "Existing automated interlinear glossing systems, such as GlossLM, can predict morpheme-level gloss labels but do not predict the actual morpheme boundaries, instead assigning glosses at the word level. This misalignment makes system outputs hard to interpret and less trustworthy for linguists who rely on explicit segmentation and clear form\u2013gloss alignment in real-world language documentation workflows. There is a need for models that can simultaneously provide accurate segmentations, glosses, and consistent alignment to better support documentation of low-resource and endangered languages.", "method": "The authors design and evaluate neural sequence-to-sequence multilingual models, collectively termed PolyGloss, that jointly predict morphological segmentation and interlinear glosses from raw text. They extend the training corpus used by GlossLM, then pretrain PolyGloss to output both segmented forms and associated glosses, exploring training regimes that trade off segmentation accuracy, glossing accuracy, and alignment between them. They compare PolyGloss against GlossLM and several open-source large language models across these tasks and investigate rapid domain or dataset adaptation using low-rank adaptation (LoRA) fine-tuning.", "result": "PolyGloss outperforms GlossLM on glossing accuracy and surpasses multiple open-source LLM baselines on morphological segmentation, glossing, and the alignment between segments and glosses. The experiments identify training setups that produce a good balance between segmentation and glossing performance while preserving tight alignment. The models also show strong data efficiency: via low-rank adaptation, PolyGloss can be quickly specialized to new datasets with relatively little additional training.", "conclusion": "Joint modeling of morphological segmentation and interlinear gloss prediction yields more accurate and better-aligned outputs than prior glossing systems, addressing key usability issues reported by linguists. PolyGloss demonstrates that multilingual seq2seq architectures, trained on an extended glossing corpus and adapted via LoRA, can provide practical improvements for language documentation workflows, making automated glossing more interpretable, trustworthy, and adaptable across languages and datasets."}}
{"id": "2601.11037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11037", "abs": "https://arxiv.org/abs/2601.11037", "authors": ["Shiyu Liu", "Yongjing Yin", "Jianhao Yan", "Yunbo Tang", "Qinggang Zhang", "Bei Li", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Jinsong Su"], "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search", "comment": "Code is available at https://github.com/Liushiyu-0709/BAPO-Reliable-Search", "summary": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.", "AI": {"tldr": "The paper introduces Boundary-Aware Policy Optimization (BAPO), an RL framework that teaches agentic-search LLMs when to answer and when to say \u201cI don\u2019t know,\u201d improving reliability without sacrificing accuracy.", "motivation": "RL-trained agentic search systems greatly improve LLM accuracy on complex questions but are unreliable: they almost never say \u201cI DON\u2019T KNOW\u201d even when evidence is insufficient or reasoning fails, instead producing plausible but incorrect answers that are risky in real-world use. There is a need for methods that give such agents calibrated boundary awareness\u2014knowing when they have reached their reasoning or evidence limits\u2014while preserving their problem-solving strength.", "method": "The authors propose Boundary-Aware Policy Optimization (BAPO), a reinforcement learning framework for agentic search LLMs. BAPO has two main components: (1) a group-based boundary-aware reward that evaluates agents over groups of questions to reward correct task answers and to reward \u201cI DON\u2019T KNOW\u201d only in cases where the agent has exhausted its reasoning or lacks sufficient evidence, and (2) an adaptive reward modulator that turns off or down-weights the IDK-related reward during early training stages so that the policy cannot game the objective by overusing IDK as an easy way to avoid errors. The method is integrated into RL training of search-based agents and compared against standard RL baselines.", "result": "Across four benchmarks for complex question answering with agentic search, BAPO-trained agents show substantially higher reliability metrics: they more appropriately choose IDK when uncertain, reduce confidently wrong answers, and maintain or slightly improve overall accuracy compared with baseline RL agents that do not model boundary awareness. Quantitative evaluations demonstrate better calibration between confidence and correctness and a higher fraction of safe abstentions in ambiguous or underspecified cases.", "conclusion": "BAPO provides an effective way to embed boundary awareness into RL-trained agentic search LLMs, enabling them to recognize when they have reached their reasoning limits and to respond with \u201cI DON\u2019T KNOW\u201d instead of fabricating answers. The framework improves reliability and safety while preserving accuracy, suggesting that explicit modeling and rewarding of abstention behavior is a promising direction for deploying LLM-based agents in high-stakes, real-world settings."}}
{"id": "2601.10926", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10926", "abs": "https://arxiv.org/abs/2601.10926", "authors": ["Dustin S. Stoltz", "Marshall A. Taylor", "Sanuj Kumar"], "title": "Selecting Language Models for Social Science: Start Small, Start Open, and Validate", "comment": null, "summary": "Currently, there are thousands of large pretrained language models (LLMs) available to social scientists. How do we select among them? Using validity, reliability, reproducibility, and replicability as guides, we explore the significance of: (1) model openness, (2) model footprint, (3) training data, and (4) model architectures and fine-tuning. While ex-ante tests of validity (i.e., benchmarks) are often privileged in these discussions, we argue that social scientists cannot altogether avoid validating computational measures (ex-post). Replicability, in particular, is a more pressing guide for selecting language models. Being able to reliably replicate a particular finding that entails the use of a language model necessitates reliably reproducing a task. To this end, we propose starting with smaller, open models, and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.", "AI": {"tldr": "Guidance for social scientists on selecting LLMs, emphasizing replicability and advocating small, open models plus task-specific benchmarks.", "motivation": "Social scientists face an overwhelming number of LLM options and need principled criteria to choose models that yield valid, reliable, and reproducible research findings rather than just chasing benchmark scores.", "method": "Conceptual/theoretical analysis of four dimensions\u2014model openness, footprint, training data, and architecture/fine-tuning\u2014framed through validity, reliability, reproducibility, and replicability, culminating in prescriptive recommendations for model choice and evaluation workflows.", "result": "Identifies replicability (and the ability to reproduce the exact task and pipeline) as a central, underappreciated criterion for LLM selection; shows that reliance on generic ex-ante benchmarks is insufficient for social science uses and that model properties like openness and size strongly affect downstream scientific robustness.", "conclusion": "Social scientists should prioritize replicability and end-to-end validation of their specific computational pipelines, which is best supported by starting with smaller, open LLMs and building narrow, task-appropriate benchmarks to substantiate validity rather than over-relying on large, closed models and generic leaderboard benchmarks."}}
{"id": "2601.11044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11044", "abs": "https://arxiv.org/abs/2601.11044", "authors": ["Keyu Li", "Junhao Shi", "Yang Xiao", "Mohan Jiang", "Jie Sun", "Yunze Wu", "Shijie Xia", "Xiaojie Cai", "Tianze Xu", "Weiye Si", "Wenjie Li", "Dequan Wang", "Pengfei Liu"], "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts", "comment": null, "summary": "Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.", "AI": {"tldr": "AgencyBench is a large-scale benchmark to evaluate autonomous LLM agents across realistic, long-horizon, tool-using workflows with automated simulation and grading.", "motivation": "Existing LLM agent benchmarks mostly test one capability at a time and on short, simplified tasks. They do not reflect realistic, long-running workflows that involve many tool calls, feedback cycles, and complex deliverables. Human-in-the-loop evaluation for such tasks is expensive and unscalable, limiting systematic comparison of agent architectures and models. The authors want a realistic, scalable, and automated way to stress-test next-generation autonomous agents.", "method": "The authors build AgencyBench, a benchmark derived from actual daily AI usage patterns. It defines 6 core agentic capabilities and instantiates them via 32 real-world multi-step scenarios containing 138 concrete tasks, each with queries, required deliverables, and detailed rubrics. Tasks typically require long horizons (hours), many tool calls (~90 on average), and large token budgets (~1M tokens) to complete. To avoid manual evaluation, they design a user-simulation agent that provides iterative feedback and use a Docker-based sandbox to run agents and perform both visual and functional rubric-based assessments automatically. They then evaluate a range of closed- and open-source LLMs, and also test different agentic scaffolding frameworks (e.g., native SDKs vs generic frameworks).", "result": "Closed-source models achieve substantially higher overall scores than open-source ones (48.4% vs 32.1%). The evaluation also reveals notable differences between models in resource usage efficiency, how well they leverage feedback to self-correct, and their preferences or effectiveness with specific tools. Proprietary models tend to perform best when used with their own native agent SDKs, while open-source models show best performance within particular external frameworks, indicating interactions between model and scaffold. The benchmark successfully captures these nuanced behaviors at scale.", "conclusion": "AgencyBench provides a realistic, long-horizon, and automated benchmark for assessing autonomous LLM agents, addressing limitations of prior single-skill, short-task, and human-dependent evaluations. The observed performance gaps and scaffold interactions underscore the importance of co-designing model architectures and agent frameworks rather than treating them independently. The authors position AgencyBench as an important testbed for future research on robust, efficient, and capable AI agents and publicly release the benchmark and tools for the community."}}
{"id": "2601.10951", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10951", "abs": "https://arxiv.org/abs/2601.10951", "authors": ["Shijie Jiang", "Zefan Zhang", "Kehua Zhu", "Tian Bai", "Ruihong Zhao"], "title": "Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions", "comment": "22 pages, 5figures, under review", "summary": "The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimensional persona structure. To address issues of the persona class imbalance, a portion of the dataset is augmented using few-shot generation, followed by manual verification. We evaluate various state-of-the-art LLMs and find that most produce overly formal responses that lack individual personality. To address this limitation, we propose a training-free Multi-Stage Patient Role-Playing (MSPRP) framework, which decomposes interactions into three stages to ensure both personalization and realism in model responses. Experimental results demonstrate that our approach significantly improves model performance across multiple dimensions of patient simulation.", "AI": {"tldr": "They build a realistic Chinese patient-simulation dataset and a multi-stage, training-free role-playing framework to make LLMs act like diverse, realistic patients rather than formal, generic speakers.", "motivation": "Clinical LLMs and medical-education tools need realistic doctor\u2013patient dialogues, but current benchmarks use generic or LLM-fabricated conversations that lack authenticity, diversity, and patient personality. This limits evaluation and development of systems meant to interact naturally with patients, especially in Chinese clinical contexts.", "method": "1) Construct Ch-PatientSim: a Chinese patient-simulation dataset derived from realistic clinical interaction scenarios, where each simulated patient is defined by a five-dimensional persona structure. 2) Mitigate persona-class imbalance via few-shot data augmentation and subsequent manual verification. 3) Benchmark several state-of-the-art LLMs on this dataset to diagnose shortcomings (overly formal, impersonal responses). 4) Propose a training-free Multi-Stage Patient Role-Playing (MSPRP) framework that decomposes patient\u2013doctor interaction into three stages to guide LLMs toward more personalized and realistic patient responses.", "result": "Empirical evaluation on Ch-PatientSim shows that most mainstream LLMs, when naively prompted, respond in an excessively formal and uniform style, failing to reflect diverse patient personas. When the MSPRP framework is applied, models achieve significantly better scores across multiple evaluation dimensions of patient simulation, indicating more realistic, personalized, and context-appropriate patient behavior.", "conclusion": "Realistic patient simulation for clinical LLMs cannot be reliably achieved using generic dialogues or naive prompting. A carefully designed persona-based dataset (Ch-PatientSim) and a structured, training-free multi-stage role-playing framework (MSPRP) markedly enhance the realism and personalization of LLM-generated patient behavior, offering a stronger benchmark and practical method for medical education and clinical AI development in Chinese settings."}}
{"id": "2601.11089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11089", "abs": "https://arxiv.org/abs/2601.11089", "authors": ["Suhan Guo", "Jiahong Deng", "Furao Shen"], "title": "MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting", "comment": null, "summary": "Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.", "AI": {"tldr": "The paper introduces MiCA, a lightweight, architecture-agnostic module that uses causally inferred mobility relations to improve epidemic forecasting, yielding substantial accuracy gains while staying computationally simple.", "motivation": "Existing epidemic forecasters struggle because human mobility, which strongly shapes spatial disease spread, is hard to measure accurately and integrate, while case time series are short and coarse. Heavy mobility-aware models (e.g., GNNs, attention-based) require clean, abundant data and many parameters, making them brittle and impractical in real-world noisy, data-limited settings. The authors seek a way to leverage mobility information causally and robustly without incurring large model complexity or data requirements.", "method": "They propose MiCA (Mobility-Informed Causal Adapter), a small plug-in module that can be attached to various temporal forecasting backbones. MiCA first performs causal discovery on mobility-related signals to infer mobility relations between regions. It then injects this learned spatial structure into the temporal forecaster using gated residual mixing, which lets the model selectively combine mobility-informed signals with the original temporal dynamics. The design avoids heavy relational architectures such as full graph neural networks or full attention-based models while still exploiting spatial dependencies.", "result": "On four real-world epidemic datasets\u2014COVID-19 incidence, COVID-19 mortality, influenza, and dengue\u2014MiCA, when attached to lightweight temporal models, consistently improves forecasting performance, with an average relative error reduction of 7.5% across forecasting horizons. Its performance is competitive with state-of-the-art spatio-temporal models despite having significantly lower complexity.", "conclusion": "Causally inferred mobility structure can be integrated into epidemic forecasting in a lightweight, architecture-agnostic way that is robust to noisy and limited data. MiCA enables simple temporal models to capture spatial effects and reach accuracy comparable to more complex SOTA spatio-temporal approaches, suggesting a practical path to better epidemic forecasts in real-world data conditions."}}
{"id": "2601.10960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10960", "abs": "https://arxiv.org/abs/2601.10960", "authors": ["Hyeseon An", "Shinwoo Park", "Hyundong Jin", "Yo-Sub Han"], "title": "Steering Language Models Before They Speak: Logit-Level Interventions", "comment": "14 pages, 5 figures, preprint", "summary": "Steering LLMs is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current steering methods, such as prompting-based and activation-based approaches, are widely used to guide model behavior. However, activation-based techniques require deep access to internal layers, while prompting-based steering often fails to provide consistent or fine-grained control. In order to address these limitations, we propose a training-free inference-time logit intervention for controllable generation. Our approach utilizes a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets focusing on writing complexity, formality, and toxicity demonstrate that our method effectively steers output characteristics, confirming its broad applicability and task-agnostic nature. Our results show that statistically grounded logit steering can achieve large, consistent, and multi-task control gains: up to +47%p accuracy and 50x f1 improvement.", "AI": {"tldr": "The paper proposes a training-free, inference-time logit intervention method to steer large language model outputs (e.g., style, formality, toxicity) using statistically derived token scores, achieving strong and consistent control across multiple tasks without modifying model internals or prompts.", "motivation": "Existing steering methods for large language models either rely on prompts, which are often inconsistent and coarse-grained, or on activation-level interventions, which require deep access to and modification of internal model layers. There is a need for a simple, general, and training-free mechanism to reliably control specific attributes of generated text (like complexity, formality, or toxicity) at inference time, without retraining or architectural access.", "method": "The authors introduce a training-free, inference-time logit steering technique. They construct a statistical token score table from labeled corpora by computing z-normalized log-odds for tokens associated with specific attributes (e.g., high vs. low complexity, formal vs. informal, toxic vs. non-toxic). During decoding, they adjust the model\u2019s token logits using these scores, shifting the output distribution toward or away from targeted attributes, thus steering generation without modifying the model\u2019s parameters or accessing internal activations.", "result": "Across three datasets targeting different attributes\u2014writing complexity, formality, and toxicity\u2014the proposed method substantially improves control over generated text. It delivers large, consistent, and multi-task control gains, with reported improvements of up to +47 percentage points in accuracy and up to 50x improvement in F1 scores compared to baselines, demonstrating robust and generalizable steering performance.", "conclusion": "Statistically grounded logit interventions at inference time can provide effective, fine-grained, and task-agnostic control over LLM behavior without training or access to internal activations. The proposed method offers a simple yet powerful alternative to existing prompting-based and activation-based steering approaches for controllable text generation in various applications."}}
{"id": "2601.11100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11100", "abs": "https://arxiv.org/abs/2601.11100", "authors": ["Zhezheng Hao", "Hong Wang", "Jian Luo", "Jianqing Zhang", "Yuyan Zhou", "Qiang Lin", "Can Wang", "Hande Dong", "Jiawei Chen"], "title": "ReCreate: Reasoning and Creating Domain Agents Driven by Experience", "comment": null, "summary": "Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.", "AI": {"tldr": "ReCreate is a framework that automatically builds and improves domain-specific LLM agents by learning directly from their interaction histories, outperforming human-designed and other auto-generated agents.", "motivation": "Most real-world LLM agents are still manually engineered because tasks vary widely across domains, making agent design costly and non-scalable. Existing automatic agent generation methods treat the process as a black box and are guided only by final task performance, ignoring rich information in interaction traces about why an agent worked or failed. They are also often computationally expensive. The paper aims to create a more data-efficient, interpretable, and adaptive way to automatically generate domain agents that can operate effectively in the wild.", "method": "The authors propose ReCreate, an experience-driven framework that uses past agent interaction histories as training signals. They introduce an \"agent-as-optimizer\" paradigm where the agent optimizes its own scaffold over time via three components: (i) an experience storage and retrieval system to log and access interaction traces for later analysis; (ii) a reasoning\u2013creating synergy pipeline that converts these execution experiences (including success/failure cases) into concrete edits to the agent\u2019s scaffold (e.g., prompts, tools, workflows); and (iii) hierarchical updates that generalize from instance-level traces to reusable higher-level domain patterns, enabling scalable improvements across tasks in the same domain.", "result": "Across multiple domains, ReCreate-generated agents consistently achieve better performance than both human-crafted agents and existing automated agent-generation baselines. It is able to do this even when initialized with minimal seed scaffolds, indicating that it can bootstrap effective domain agents largely from interaction experience alone.", "conclusion": "Experience traces are a powerful resource for automatically creating and refining domain-specific LLM agents. By systematically leveraging interaction histories and structuring updates hierarchically, ReCreate can outperform manually designed and prior auto-generated agents while reducing engineering and computational overhead. This suggests a promising path toward scalable, self-improving domain agents in real-world settings."}}
{"id": "2601.10986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10986", "abs": "https://arxiv.org/abs/2601.10986", "authors": ["Bo Yang", "Yunkui Chen", "Lanfei Feng", "Yu Zhang", "Shijian Li"], "title": "ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models", "comment": null, "summary": "As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc", "AI": {"tldr": "The paper proposes ZPD Detector, a dynamic data selection framework for training large language models that aligns sample difficulty with model capability to improve data efficiency.", "motivation": "Training large language models is increasingly expensive and constrained by limited high-quality data, making efficient data selection or synthesis essential. Existing methods mostly use static criteria (difficulty, uncertainty, heuristics) and ignore that the usefulness of a sample depends on the model\u2019s current competence, which evolves during training.", "method": "The authors introduce ZPD Detector, a framework inspired by the Zone of Proximal Development from educational theory. It (1) calibrates sample difficulty, (2) estimates the model\u2019s capability using Item Response Theory (IRT), and (3) computes a capability\u2013difficulty matching score that measures how well each sample sits within the model\u2019s current ZPD. During training, it dynamically selects samples with the highest matching scores at each stage.", "result": "Using the ZPD Detector, the training process focuses on the most informative samples for the model\u2019s current stage, which is claimed to improve data utilization efficiency compared with static selection approaches. (Concrete empirical results are not specified in the abstract.)", "conclusion": "Dynamic, bidirectional modeling of both model capability and data difficulty\u2014implemented via ZPD Detector\u2014can lead to more efficient use of limited training data and inspire new training strategies for large language models. The authors plan to release code and data upon acceptance to facilitate reproducibility."}}
{"id": "2601.11147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11147", "abs": "https://arxiv.org/abs/2601.11147", "authors": ["Zixu Wang", "Bingbing Xu", "Yige Yuan", "Huawei Shen", "Xueqi Cheng"], "title": "Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems", "comment": "17 pages, 4 figures, 3 tables", "summary": "Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \\textbf{SCALE}, which means \\underline{\\textbf{S}}elf prediction of the optimizer with few shot \\underline{\\textbf{CAL}}ibration for \\underline{\\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \\textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\\%.", "AI": {"tldr": "The paper studies workflow generation strategies for multi-agent systems built on large language models, showing that a few strong task-level workflows can cover most queries and proposing SCALE, a low-cost evaluation and selection framework that preserves performance while drastically cutting token usage.", "motivation": "Multi-agent systems using LLMs rely on workflows to coordinate agents for complex tasks. Existing methods either design workflows per task or per query, but it is unclear when fine-grained query-level workflows are necessary and how costly it is to reliably evaluate task-level workflows using full execution. There is a need to understand the trade-offs and to reduce token costs while keeping performance high.", "method": "The authors first conceptually and empirically compare task-level and query-level workflow generation, analyzing coverage of queries by top-K task-level workflows. They then examine the cost and reliability issues of exhaustive execution-based evaluation at the task level. Building on self-evolution and generative reward modeling, they propose SCALE: a framework where the optimizer predicts which workflows will perform best and uses few-shot calibrated, model-based evaluation instead of fully executing all candidate workflows. SCALE thus generates and selects task-level workflows with a self-prediction mechanism and lightweight calibration rather than exhaustive validation.", "result": "Experiments across multiple datasets show that top-K task-level workflows can match or exceed the query coverage of query-level workflows, indicating that query-level workflow generation is often unnecessary. Using SCALE, the system achieves nearly the same performance as existing, more expensive approaches, with an average performance drop of only 0.61%, while reducing token consumption by up to 83%.", "conclusion": "Task-level workflow generation, when combined with smart selection of a small set of top-K workflows, can effectively replace more complex query-level generation for many scenarios. Exhaustive execution-based evaluation is both costly and unreliable, so model-based self-prediction with light calibration, as implemented in SCALE, offers a practical alternative. SCALE substantially reduces token usage while preserving almost all of the performance, making it a more efficient framework for workflow design in LLM-based multi-agent systems."}}
{"id": "2601.11000", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11000", "abs": "https://arxiv.org/abs/2601.11000", "authors": ["Zhongxiang Sun", "Yi Zhan", "Chenglei Shen", "Weijie Yu", "Xiao Zhang", "Ming He", "Jun Xu"], "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs", "comment": "20 pages, 15 figures", "summary": "Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.", "AI": {"tldr": "The paper identifies and addresses a new problem: personalized LLMs can hallucinate by favoring user-aligned but false answers, and proposes an inference-time method (FPPS) plus a new benchmark (PFQABench) to preserve factuality without losing personalization.", "motivation": "Personalized LLMs are designed to align with individual users' preferences and histories to improve user satisfaction and engagement. However, this personalization can interfere with truthful, fact-based reasoning: models may repeat or reinforce a user's prior (possibly incorrect) beliefs instead of returning objectively correct information. This threatens reliability and risk of misinformation, and current evaluation/mitigation tools do not specifically target this trade-off between personalization and factuality. The paper is motivated by the need to understand, measure, and mitigate \"personalization-induced hallucinations.\"", "method": "1) Conceptual and empirical identification of \"personalization-induced hallucinations,\" where personalized LLMs answer factual questions based on user history instead of ground truth, attributed to representational entanglement between user-specific and factual information. 2) Proposal of Factuality-Preserving Personalized Steering (FPPS), a lightweight, inference-time steering approach that modifies model activations or decoding to reduce the influence of entangled personalization components on factual predictions, while still maintaining personalized aspects where they do not conflict with factuality. 3) Construction of PFQABench, a benchmark that pairs user profiles (including potentially incorrect beliefs or idiosyncratic preferences) with factual and personalized questions, enabling joint evaluation of factual accuracy and personalization quality for different LLM backbones and personalization techniques. 4) Extensive experiments applying FPPS across multiple models and personalization setups, measuring improvements in factual correctness and retention of personalization signals compared to baselines.", "result": "The experiments demonstrate that personalization indeed leads to systematic factual distortions: personalized LLMs often produce answers that align with user priors over objective facts, confirming the existence of personalization-induced hallucinations. Applying the proposed FPPS method across several LLM architectures and personalization strategies substantially boosts factual accuracy on factual questions under personalized settings, while preserving most of the desired personalized behaviors on non-factual or preference-oriented questions. On the newly introduced PFQABench, FPPS achieves a significantly better balance between factual reliability and personalization quality than baseline approaches or na\u00efve de-personalization, indicating that factuality and personalization need not be in direct conflict if appropriately disentangled.", "conclusion": "Personalization in LLMs can unintentionally compromise factual reliability by causing models to favor user-aligned but incorrect outputs, a failure mode the authors term personalization-induced hallucinations. This arises from representational entanglement between personalization signals and factual knowledge. The authors show that this issue is measurable and non-trivial in practice, and they propose FPPS, an inference-time steering method, as an effective way to mitigate such distortions without sacrificing the benefits of personalization. The PFQABench benchmark facilitates systematic evaluation of this trade-off. Overall, the paper concludes that careful design of personalization mechanisms can yield LLMs that remain both user-adaptive and factually trustworthy, and it opens up a new line of research on robust, factual personalization in large language models."}}
{"id": "2601.11178", "categories": ["cs.AI", "cs.CL", "cs.MM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.11178", "abs": "https://arxiv.org/abs/2601.11178", "authors": ["Girish A. Koushik", "Helen Treharne", "Diptesh Kanojia"], "title": "TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech", "comment": "Under review at ICWSM 2026", "summary": "Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as \"black boxes\" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.", "AI": {"tldr": "The paper proposes TANDEM, a framework that turns multimodal hate detection into a structured reasoning task, yielding more interpretable, temporally grounded, and accurate moderation signals for long-form content.", "motivation": "Existing automated hate speech detectors on social media act as black boxes and mainly do coarse binary classification. They struggle with long-form multimodal content where harmful narratives emerge from the combined effect of audio, visuals, and text over time, and they do not provide interpretable evidence like timestamps or clear target identification needed for human moderators.", "method": "The authors introduce TANDEM, which reframes audio-visual hate detection from simple binary classification into a structured reasoning problem. They use a tandem reinforcement learning scheme with paired vision-language and audio-language models that iteratively optimize each other. The models exchange self-constrained cross-modal context to stabilize reasoning across long temporal sequences without requiring dense frame-level labeling, and they output fine-grained elements such as temporal segments and targets rather than just a yes/no label.", "result": "Across three benchmark datasets, TANDEM surpasses zero-shot and context-augmented baselines. On the HateMM dataset, it reaches 0.73 F1 for target identification, about 30% better than prior state-of-the-art, while still providing accurate temporal grounding of hateful segments. The study also finds that although overall hate vs non-hate detection is strong, distinguishing offensive from hateful content in multi-class setups is difficult due to ambiguous labels and class imbalance.", "conclusion": "TANDEM shows that structured, interpretable reasoning for multimodal hate detection is possible, providing fine-grained, temporally grounded outputs suitable for human-in-the-loop moderation. The framework offers a path toward more transparent, actionable online safety tools, although challenges remain in fine-grained label distinctions like offensive versus hateful content in imbalanced datasets."}}
{"id": "2601.11002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11002", "abs": "https://arxiv.org/abs/2601.11002", "authors": ["Qianen Zhang", "Zeyu Yang", "Satoshi Nakamura"], "title": "Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies", "comment": "arXiv admin note: substantial text overlap with arXiv:2509.21801", "summary": "Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: Sentence_Cut, Drop, Partial_Summarization and Pronominalization, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We adapt these actions in a large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and word-level monotonicity, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese, English-German and English-Japanese benchmarks show that our framework consistently improves semantic metrics and achieves lower delay compared to reference translations and salami-based baselines. Notably, combining Drop and Sentence_Cut leads to consistent improvements in the balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.", "AI": {"tldr": "The paper enhances simultaneous machine translation by expanding the action space beyond simple reading and writing, enabling more human\u2011like, low-latency interpreting with an LLM-based framework and a speech-aware evaluation pipeline.", "motivation": "Traditional simultaneous MT uses only READ/WRITE actions, which struggle to balance translation quality and strict real-time latency constraints, especially for language pairs with strong word order differences. Human interpreters use richer strategies like cutting sentences, omitting less important content, summarizing, and using pronouns, which existing systems do not explicitly model.", "method": "The authors extend the SiMT action space with four adaptive actions\u2014Sentence_Cut, Drop, Partial_Summarization, and Pronominalization\u2014to allow restructuring, omission, and simplification while preserving meaning. They implement these actions in a large language model framework and generate training references via action-aware prompting so the LLM learns when and how to use each action. For evaluation, they design a latency-aware text-to-speech pipeline that converts textual outputs into speech with realistic timing, enabling assessment of both semantic quality and word-level monotonicity under real-time conditions.", "result": "On ACL60/60 English\u2013Chinese, English\u2013German, and English\u2013Japanese SiMT benchmarks, the proposed framework improves semantic evaluation metrics and reduces delay compared to conventional reference translations and salami-based baselines. The combination of Drop and Sentence_Cut actions in particular yields a better trade-off between fluency and latency than existing methods.", "conclusion": "Richer, human-inspired action spaces in LLM-based simultaneous MT\u2014beyond simple READ/WRITE\u2014enable better balancing of translation quality and latency. The proposed adaptive actions and evaluation pipeline move SiMT closer to human interpreting behaviors and represent a promising direction for future real-time translation systems."}}
{"id": "2601.11004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11004", "abs": "https://arxiv.org/abs/2601.11004", "authors": ["Jiayu Liu", "Rui Wang", "Qing Zong", "Qingcheng Zeng", "Tianshi Zheng", "Haochen Shi", "Dadi Guo", "Baixuan Xu", "Chunyang Li", "Yangqiu Song"], "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems", "comment": null, "summary": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.", "AI": {"tldr": "The paper studies why large language models (LLMs) in retrieval-augmented generation (RAG) are overconfident when context is noisy, and proposes a noise-aware calibration framework (NAACL) that trains models to better align confidence with factual correctness, substantially improving calibration metrics.", "motivation": "In mission-critical factual applications, it is not enough for LLMs to be accurate; they must also know when they might be wrong. RAG is commonly used to ground LLM outputs in retrieved evidence, but the reliability of model confidence in such settings is poorly understood. Noisy retrieval (irrelevant or contradictory documents) can mislead the model, especially if the model becomes overconfident in incorrect answers. There is a lack of principled methods to calibrate LLM confidence under retrieval noise without using stronger teacher models.", "method": "The authors first conduct a systematic empirical study on four benchmarks to quantify calibration behavior of LLMs under RAG, focusing on the impact of noisy retrieved contexts. They analyze how contradictory or irrelevant evidence affects confidence and error patterns, showing that it often inflates false certainty. Based on these observations, they formulate NAACL Rules (Noise-AwAre Confidence CaLibration Rules), a set of principles on how confidence should behave when evidence is noisy or conflicting. Using these rules, they build NAACL, a noise-aware calibration framework that generates supervision signals from around 2,000 annotated HotpotQA examples. They then perform supervised fine-tuning (SFT) so that the model learns intrinsic noise awareness in its verbalized confidence, without distillation from a larger teacher model.", "result": "Experiments across four benchmarks demonstrate that baseline LLMs in RAG settings are notably miscalibrated, particularly overconfident when confronted with noisy retrieval contexts. After training with NAACL, models show substantial improvements in calibration as measured by Expected Calibration Error (ECE), with average gains of 10.9% on in-domain tasks and 8.0% on out-of-domain tasks. These gains are achieved without sacrificing factual accuracy and without relying on stronger teacher models, indicating that explicit noise-aware supervision can significantly enhance confidence alignment with correctness.", "conclusion": "The paper concludes that retrieval noise is a key driver of overconfidence in RAG-based LLMs and that naive use of retrieved evidence can harm epistemic reliability. By introducing NAACL Rules and the NAACL framework, the authors provide a principled way to inject noise awareness into LLM confidence estimation. Their results suggest that modest supervised fine-tuning on carefully constructed, rule-guided data is sufficient to yield large improvements in calibration across domains. This demonstrates a practical path toward LLMs whose expressed confidence more faithfully tracks their actual knowledge, which is crucial for safe deployment in factual, high-stakes settings."}}
{"id": "2601.11252", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11252", "abs": "https://arxiv.org/abs/2601.11252", "authors": ["Qianyue Wang", "Jinwu Hu", "Yufeng Wang", "Huanxiang Lin", "Bolin Chen", "Zhiquan Wen", "Yaofo Chen", "Mingkui Tan"], "title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.", "AI": {"tldr": "Proposes Think-with-Me, an interactive test-time reasoning paradigm for large reasoning models that uses external feedback at conjunction-based pause points to reduce redundant reasoning while improving or maintaining accuracy.", "motivation": "Large Reasoning Models are strong at multi-step reasoning but often overthink or overshoot, wasting computation and sometimes hurting performance. Existing efficiency methods are closed-loop: the model reasons alone without external control or guidance, making it hard to dynamically manage reasoning depth, balance accuracy vs. cost, or incorporate human/LLM oversight. The paper aims to introduce a controllable, interactive mechanism that can intervene in the reasoning process itself, especially under limited context windows, and to show it can also improve safety and creativity.", "method": "1) Empirically identify transitional conjunctions in chain-of-thought (e.g., \u201ctherefore,\u201d \u201chowever,\u201d \u201cnext\u201d) as natural intervention points that correspond to self-checking or exploratory phases. 2) Design the Think-with-Me paradigm: at these conjunctions the model pauses and requests external feedback about the current partial reasoning. 3) Generate feedback\u2014either from humans or proxy LLMs\u2014using a multi-criteria evaluation focusing on rationality and completeness of the partial solution. The feedback indicates whether to extend reasoning, modify direction, or stop. 4) Train the target LRM with Group Relative Policy Optimization (GRPO) so it can interpret feedback and adapt its reasoning length and structure at test time. 5) Apply the method to reasoning, security, and creative tasks under constrained context windows.", "result": "On the AIME24 benchmark with an 8K context window, Think-with-Me improves accuracy by 7.19 percentage points over QwQ-32B while cutting average reasoning length by 81%. This demonstrates a better accuracy\u2013efficiency trade-off than strong baselines. The paradigm also shows benefits in security-sensitive and creative tasks, though exact quantitative results for those are not provided in the abstract.", "conclusion": "Interactive, feedback-driven reasoning via Think-with-Me can make large reasoning models more efficient and accurate, especially when context is limited. Transitional conjunctions are effective, semantically meaningful control points for pausing and steering reasoning. Training models with GRPO to work in this interactive mode enables adaptive extension or termination of reasoning, reducing redundancy without sacrificing correctness, and the same framework is promising beyond pure math or logical reasoning, extending to safety and creative applications."}}
{"id": "2601.11019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11019", "abs": "https://arxiv.org/abs/2601.11019", "authors": ["Xinwei Wu", "Heng Liu", "Xiaohu Zhao", "Yuqi Ren", "Linlong Xu", "Longyue Wang", "Deyi Xiong", "Weihua Luo", "Kaifu Zhang"], "title": "Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs", "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based consistency metric. This framework successfully isolates a small set of **translation initiation** features. Causal interventions demonstrate that amplifying these features steers the model towards correct translation, while ablating them induces hallucinations and off-task outputs, confirming they represent a core component of the model's innate translation competency. Moving from analysis to application, we leverage this mechanistic insight to propose a new data selection strategy for efficient fine-tuning. Specifically, we prioritize training on **mechanistically hard** samples-those that fail to naturally activate the translation initiation features. Experiments show this approach significantly improves data efficiency and suppresses hallucinations. Furthermore, we find these mechanisms are transferable to larger models of the same family. Our work not only decodes a core component of the translation mechanism in LLMs but also provides a blueprint for using internal model mechanism to create more robust and efficient models. The codes are available at https://github.com/flamewei123/AAAI26-translation-Initiation-Features.", "AI": {"tldr": "The paper uses sparse autoencoders to uncover and manipulate a small set of internal features in LLMs that initiate translation, and then exploits these features to select data for more efficient, less hallucination-prone fine-tuning.", "motivation": "LLMs can translate well without explicit training, but we do not understand the internal mechanisms that enable this innate translation ability. Gaining mechanistic insight could both advance interpretability and directly improve training strategies, robustness, and data efficiency for translation tasks.", "method": "The authors train Sparse Autoencoders on LLM activations and propose a framework to find task-specific features for translation. They (1) retrieve SAE features frequently co-activated on translation prompts, (2) filter them using a PCA-based functional-consistency metric to ensure they behave coherently, and thereby isolate a small set of \"translation initiation\" features. They then run causal interventions by amplifying or ablating these features to test their functional role, and finally use the activation patterns of these features to drive a data selection strategy that prioritizes \u201cmechanistically hard\u201d examples for fine-tuning\u2014samples that do not naturally activate the initiation features.", "result": "They identify a compact set of translation initiation features whose amplification improves translation and whose ablation leads to hallucinations and off-task behavior, evidencing their causal role. Using these features to select mechanistically hard training examples yields significant gains in data efficiency for fine-tuning and reduces hallucinations. They also show that the identified mechanisms transfer across larger models in the same family.", "conclusion": "A core component of LLMs\u2019 innate translation mechanism can be decoded using sparse autoencoders, revealing specific translation initiation features that causally steer behavior. These insights enable a principled, mechanism-aware data selection strategy that makes fine-tuning more efficient and robust, and the approach appears transferable across model scales, offering a blueprint for building better models via internal mechanism analysis."}}
{"id": "2601.11286", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11286", "abs": "https://arxiv.org/abs/2601.11286", "authors": ["Weihong Qi", "Fan Huang", "Rasika Muralidharan", "Jisun An", "Haewoon Kwak"], "title": "XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making", "comment": null, "summary": "We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.", "AI": {"tldr": "XChoice is an explainable framework that evaluates AI-human alignment in constrained decision making by modeling underlying decision mechanisms rather than just outcome agreement.", "motivation": "Traditional evaluation metrics like accuracy and F1 only check whether AI and humans make the same choice, ignoring how and why decisions are made, especially under constraints. The authors want a way to compare the decision processes and trade-offs of humans and AI systems and diagnose misalignment at a mechanism level.", "method": "They build a mechanism-based decision model that is fit both to human data and to LLM-generated decisions in the same scenarios. The model recovers interpretable parameters representing the importance of different decision factors, sensitivity to constraints, and trade-offs. Alignment is then quantified by comparing these parameter vectors across models, decision options, and demographic subgroups. They apply this to daily time allocation decisions using ATUS as the human ground truth, and also perform robustness checks (invariance analysis) and test a mitigation strategy using retrieval augmented generation (RAG).", "result": "Applied to Americans' daily time allocation, XChoice reveals that alignment between LLMs and humans varies widely across models, activities, and demographic groups, with particularly notable misalignment for Black and married groups. The invariance analysis supports robustness of the framework, and a targeted RAG-based intervention can improve alignment in some cases.", "conclusion": "XChoice offers a mechanism-based, interpretable way to evaluate and diagnose AI-human alignment in constrained decision making, going beyond surface-level outcome matching. It can highlight where and how models are misaligned and guide targeted improvements such as RAG interventions."}}
{"id": "2601.11020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11020", "abs": "https://arxiv.org/abs/2601.11020", "authors": ["Youmi Ma", "Naoaki Okazaki"], "title": "From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models", "comment": "13 pages", "summary": "Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with those from an ablated variant in which the retrieval heads are masked. This mechanism-based approach achieves substantial improvements: +2.28 points on HELMET at 128K for Llama-3.1, with +70% gains on generation with citation and +32% on passage re-ranking, while preserving performance on general tasks. Experiments across three model families reveal that the effectiveness depends on retrieval head organization: models with concentrated patterns of retrieval heads respond strongly, while those with distributed patterns show limited gains. This mechanistic relationship validates the function of retrieval heads and demonstrates that mechanistic insights can be transformed into performance enhancements.", "AI": {"tldr": "The paper introduces RetMask, a training method that uses mechanistic interpretability (masking retrieval heads in transformers) to generate learning signals, leading to better long-context performance without harming general abilities.", "motivation": "Although prior mechanistic interpretability work has identified retrieval heads that pull relevant information from context, it is unclear whether and how exploiting these heads can yield tangible performance gains, particularly for long-context tasks where retrieval is crucial.", "method": "They identify retrieval heads in LLMs and create an ablated model variant where these heads are masked. RetMask trains the model by contrasting outputs of the original and ablated variants, using the differences as a supervision signal that explicitly encourages the model to rely on and strengthen retrieval behavior, especially for long-context use. They evaluate across three model families and analyze how the organization of retrieval heads (concentrated vs distributed) affects gains.", "result": "On Llama-3.1 with 128K context, RetMask improves HELMET scores by +2.28 points, boosts generation-with-citation performance by 70%, and passage re-ranking by 32%, while maintaining performance on general tasks. Across model families, models with concentrated retrieval heads benefit substantially, whereas those with more distributed retrieval heads see smaller gains.", "conclusion": "Retrieval heads play a causal role in long-context performance and can be directly exploited via mechanism-based training. RetMask converts mechanistic insights about attention head functions into concrete performance improvements, and model-specific organization of retrieval heads determines how effective this approach is."}}
{"id": "2601.11354", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11354", "abs": "https://arxiv.org/abs/2601.11354", "authors": ["Weiyi Wang", "Xinchi Chen", "Jingjing Gong", "Xuanjing Huang", "Xipeng Qiu"], "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems", "comment": null, "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.", "AI": {"tldr": "They propose AstroReason-Bench, a benchmark to test how well agentic LLMs can plan in realistic, physics-constrained space mission scenarios, and show current LLM agents lag far behind specialized solvers.", "motivation": "Agentic LLMs are increasingly used as generalist planners, but most benchmarks test them only in symbolic or weakly grounded settings that ignore real-world physics, strict constraints, and long planning horizons. Space Planning Problems are high-stakes and representative of such realistic domains, yet there is no standardized benchmark to systematically evaluate and diagnose LLM agents\u2019 planning capabilities there.", "method": "The authors design AstroReason-Bench, a benchmark focused on Space Planning Problems. It includes multiple realistic scheduling regimes, such as ground station communication scheduling and agile Earth observation tasking, all framed via a unified interaction protocol tailored to LLM-based agents. They then evaluate a variety of state-of-the-art open- and closed-source agentic LLM systems on these tasks and compare their performance to that of specialized optimization/heuristic solvers.", "result": "Across the tasks in AstroReason-Bench, current agentic LLMs perform significantly worse than domain-specific solvers, especially when dealing with heterogeneous objectives, hard physical constraints, and long-horizon decisions. This demonstrates that the generalist planning abilities of today\u2019s LLM agents do not yet translate into strong performance in realistic space planning settings.", "conclusion": "AstroReason-Bench serves as a challenging, diagnostic environment for testing and improving agentic LLM planning in physics-constrained, high-stakes domains. The current performance gap to specialized solvers reveals important limitations of existing agentic LLMs and motivates further research on more capable planning, constraint handling, and reasoning mechanisms in realistic environments."}}
{"id": "2601.11038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11038", "abs": "https://arxiv.org/abs/2601.11038", "authors": ["Xuanming Zhang", "Shwan Ashrafi", "Aziza Mirsaidova", "Amir Rezaeian", "Miguel Ballesteros", "Lydia B. Chilton", "Zhou Yu", "Dan Roth"], "title": "Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data", "comment": "13 pages, 3 figures", "summary": "We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs. Many real-world tasks, such as trip planning, require models to deliver the best possible output within a fixed reasoning budget. We introduce an anytime reasoning framework and the Anytime Index, a metric that quantifies how effectively solution quality improves as reasoning tokens increase. To further enhance efficiency, we propose an inference-time self-improvement method using LLM-synthesized preference data, where models learn from their own reasoning comparisons to produce better intermediate solutions. Experiments on NaturalPlan (Trip), AIME, and GPQA datasets show consistent gains across Grok-3, GPT-oss, GPT-4.1/4o, and LLaMA models, improving both reasoning quality and efficiency under budget constraints.", "AI": {"tldr": "The paper proposes an anytime reasoning framework and a metric (Anytime Index) to evaluate how well LLMs improve solution quality as more reasoning tokens are used, plus an inference-time self-improvement method that lets models learn from their own reasoning comparisons to produce better intermediate solutions under computation budgets.", "motivation": "Real-world applications often require LLMs to provide good partial answers quickly under strict computation or time budgets, rather than spending many tokens on exhaustive reasoning. Existing evaluations usually assume unlimited or fixed large budgets, which does not reflect this need for graceful improvement of solution quality as more computation becomes available. The authors aim to formalize and improve this \"anytime\" reasoning behavior.", "method": "1) Define an anytime reasoning framework where the model is evaluated not only on final answers but on how solution quality scales with the number of reasoning tokens used. 2) Introduce the Anytime Index, a metric that aggregates solution quality over different budget levels to measure efficiency of reasoning. 3) Propose an inference-time self-improvement approach: use the LLM to generate and compare its own intermediate reasoning traces or partial solutions, synthesize preference-style data from these comparisons, and then guide the model (or its decoding) to favor trajectories that historically led to better intermediate solutions, thus improving efficiency under a budget. 4) Evaluate on planning (NaturalPlan Trip) and math/science benchmarks (AIME, GPQA) across several model families (Grok-3, GPT variants, LLaMA).", "result": "Across NaturalPlan (Trip), AIME, and GPQA, the proposed framework and self-improvement method yield consistent gains in both quality of answers and efficiency of reasoning under fixed token budgets. Multiple model families (Grok-3, GPT-oss, GPT-4.1/4o, and LLaMA) exhibit better performance when using the anytime reasoning approach compared to standard inference, showing improved solution quality at lower or equal computation budgets.", "conclusion": "The paper concludes that evaluating and optimizing LLMs for anytime reasoning\u2014how well they trade off solution quality and computation\u2014is crucial for practical deployment under resource constraints. The Anytime Index offers a principled way to measure this behavior, and inference-time self-improvement via LLM-synthesized preference data can reliably enhance both intermediate and final solutions across diverse tasks and models under budget limits."}}
{"id": "2601.11389", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11389", "abs": "https://arxiv.org/abs/2601.11389", "authors": ["Hedieh Haddad", "Thibault Falque", "Pierre Talbot", "Pascal Bouvry"], "title": "Hyperparameter Optimization of Constraint Programming Solvers", "comment": "28 pages, 3 figures. Submitted to Journal of Combinatorial Optimization. Special Issue: Recent applications, models and algorithms in Combinatorial Optimization", "summary": "The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.\n  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.\n  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.", "AI": {"tldr": "They propose a two-phase framework, \u201cprobe and solve\u201d, that automatically tunes hyperparameters of constraint programming solvers within a fixed time budget and show it improves performance over default settings and a local search baseline.", "motivation": "Constraint programming solvers are very sensitive to hyperparameter choices, and manually tuning these settings is difficult, slow, and requires expertise. There is a need for an automated, general, and time-aware way to find good solver configurations that work well across many problem instances and solvers without expert intervention.", "method": "They design a two-phase framework integrated into CPMpy. The total time budget is split into: (1) a probing phase, where different solver hyperparameter configurations are explored using an optimization method; and (2) a solving phase, where the best configuration discovered is used to solve the target instance with the remaining time. Within this framework they implement two hyperparameter optimization strategies: Bayesian optimization (a model-based global search) and Hamming distance search (a simple local search over configurations). They apply this to two CP solvers, ACE and Choco, and benchmark performance on 114 combinatorial problem instances, comparing against default solver settings and against each other.", "result": "Using Bayesian optimization in the probe phase leads to better solution quality than the solvers\u2019 defaults on a substantial fraction of instances: for ACE, it improves results in 25.4% of cases and matches defaults in 57.9%; for Choco, it outperforms defaults in 38.6% of instances. Across experiments, Bayesian optimization consistently outperforms Hamming distance search within the same two-phase framework, indicating that model-based configuration search is more effective than the simple local search baseline.", "conclusion": "The probe and solve algorithm is an effective, practical approach to automatic hyperparameter tuning for constraint programming solvers under a fixed time budget. By dedicating part of the time to exploratory configuration search using Bayesian optimization and then solving with the best-found configuration, it reliably matches or improves upon default solver settings and outperforms a simple Hamming-based local search, demonstrating that time-aware, model-based configuration is beneficial across diverse CP problems and solvers."}}
{"id": "2601.11042", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11042", "abs": "https://arxiv.org/abs/2601.11042", "authors": ["Chi Zhang", "Mengqi Zhang", "Xiaotian Ye", "Runxi Cheng", "Zisheng Zhou", "Ying Zhou", "Pengjie Ren", "Zhumin Chen"], "title": "Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse", "comment": "22 pages, 18 figures", "summary": "Sequential knowledge editing in large language models often causes catastrophic collapse of the model's general abilities, especially for parameter-modifying methods. Existing approaches mitigate this issue through heuristic constraints on parameter updates, yet the mechanisms underlying such degradation remain insufficiently understood. In this work, we present a spectral analysis of sequential knowledge editing and show that a model's general abilities are closely associated with dominant singular directions of pretrained weight matrices. These directions are highly sensitive to perturbations and are progressively disrupted by repeated edits, closely tracking the collapse in both editing efficacy and general performance. Building on this insight, we propose REVIVE, a plug-and-play framework that stabilizes sequential editing by explicitly preserving the dominant singular subspace. REVIVE represents parameter updates in the spectral basis of the original weights and filters components that would interfere with the protected region. Extensive experiments across multiple models and benchmarks show that REVIVE consistently improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing, including extreme settings with up to 20,000 edits.", "AI": {"tldr": "The paper studies why sequential knowledge editing in large language models leads to collapse of general abilities and proposes REVIVE, a spectral-constraint framework that preserves core capabilities while allowing many edits.", "motivation": "Sequentially editing knowledge in large language models often degrades their general capabilities, especially for methods that directly modify parameters. Existing fixes use heuristic constraints but lack a principled understanding of why degradation happens. The authors aim to explain the mechanism behind the collapse and design a more principled, stable editing approach.", "method": "They conduct a spectral analysis of pretrained weight matrices in LLMs, focusing on singular values and singular vectors. They show that general abilities are associated with dominant singular directions that are highly sensitive to perturbations and are disrupted by repeated edits. Based on this, they design REVIVE, which (1) decomposes original weights into a spectral basis, (2) represents parameter updates in that basis, and (3) filters out components that would alter the dominant singular subspace, thereby protecting key directions during editing. REVIVE is used as a plug-and-play module around existing editing methods.", "result": "Empirical results across various models and benchmarks show that REVIVE substantially reduces degradation of general abilities during long sequences of edits, while maintaining or improving editing success. It remains stable even under extreme conditions with up to 20,000 edits, outperforming baseline editing methods without spectral protection.", "conclusion": "Dominant singular directions of LLM weight matrices encode general abilities and are fragile under naive sequential parameter editing. By explicitly preserving the dominant singular subspace via spectral-based filtering of updates, REVIVE enables stable, large-scale sequential knowledge editing that keeps general performance largely intact."}}
{"id": "2601.11468", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.11468", "abs": "https://arxiv.org/abs/2601.11468", "authors": ["Alessandro Padella", "Massimiliano de Leoni", "Marlon Dumas"], "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs", "comment": "19 pages, 4 figure, TMIS journal submission", "summary": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.", "AI": {"tldr": "The paper evaluates and extends an LLM-based framework for Predictive Process Monitoring, showing that LLMs can accurately predict process KPIs (like total time and activity occurrence) in data-scarce settings and use higher-order reasoning rather than just pattern replication.", "motivation": "Traditional predictive process monitoring relies on classical ML and deep learning models that often require substantial training data and do not leverage semantic knowledge or explicit reasoning. The authors previously proposed an LLM-based approach for predicting total process time via prompting. This paper is motivated by the need to assess how general and robust that approach is across different KPIs and datasets, and to understand whether LLMs truly reason about processes or simply mimic standard predictive models.", "method": "The authors extend their earlier LLM-based Predictive Process Monitoring framework from only total time prediction to multiple KPIs, notably total time and activity occurrence prediction. They perform empirical evaluations on three heterogeneous event logs, comparing the LLM-based approach against benchmark predictive methods. They design experiments for both data-scarce settings (around 100 traces) and presumably richer data regimes, and they analyze the model\u2019s behavior to detect use of prior world knowledge and correlations in the training traces. Additionally, they study the reasoning strategies of the LLM, examining whether it reimplements known predictive mechanisms or displays higher-order reasoning patterns.", "result": "Across three real-life event logs and for the KPIs of Total Time and Activity Occurrence, the LLM-based approach outperforms benchmark methods in low-data regimes with only about 100 traces available. The experiments also indicate that the LLM leverages both its internal prior knowledge and correlations present in the limited training set to make its predictions. Behavioral analysis suggests that the LLM\u2019s predictions are not simply a re-encoding of existing predictive models; it can combine information and reason across traces in more complex ways.", "conclusion": "The extended LLM-based framework for Predictive Process Monitoring is both general and effective across different KPIs, especially in data-scarce scenarios where traditional ML/Deep Learning approaches struggle. The LLM leverages semantic prior knowledge and inter-trace correlations and exhibits higher-order reasoning rather than mere pattern replication. This suggests that LLMs are a promising direction for predictive monitoring tasks, particularly when annotated historical data are limited."}}
{"id": "2601.11047", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11047", "abs": "https://arxiv.org/abs/2601.11047", "authors": ["Yuanxiang Liu", "Songze Li", "Xiaoke Guo", "Zhaoyan Gong", "Qifei Zhang", "Huajun Chen", "Wen Zhang"], "title": "CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneous search strategies that render them vulnerable to instability under neighborhood noise and structural misalignment leading to reasoning stagnation. To address these challenges, we propose CoG, a training-free framework inspired by Dual-Process Theory that mimics the interplay between intuition and deliberation. First, functioning as the fast, intuitive process, the Relational Blueprint Guidance module leverages relational blueprints as interpretable soft structural constraints to rapidly stabilize the search direction against noise. Second, functioning as the prudent, analytical process, the Failure-Aware Refinement module intervenes upon encountering reasoning impasses. It triggers evidence-conditioned reflection and executes controlled backtracking to overcome reasoning stagnation. Experimental results on three benchmarks demonstrate that CoG significantly outperforms state-of-the-art approaches in both accuracy and efficiency.", "AI": {"tldr": "The paper introduces CoG, a training-free framework that makes LLM+KG reasoning more reliable and efficient by combining fast heuristic guidance with slower reflective refinement.", "motivation": "Although LLMs reason well, they hallucinate and become unreliable, especially when augmented with knowledge graphs using rigid, single-style search. These systems are brittle to noisy neighborhoods and structural mismatches in KGs, causing unstable paths and reasoning dead-ends. The authors want a more cognitively flexible, robust way for LLMs to use KGs without retraining models.", "method": "They propose CoG, a dual-process-inspired framework. A Relational Blueprint Guidance module acts as a fast, intuitive process that uses interpretable relational blueprints as soft structural constraints to quickly steer search and stabilize reasoning under KG noise. A Failure-Aware Refinement module acts as a slow, analytical process that detects reasoning impasses, then triggers evidence-conditioned reflection and controlled backtracking to revise and extend the reasoning path. CoG is training-free and plugs into existing LLM+KG setups.", "result": "On three reasoning benchmarks, CoG substantially improves both accuracy and efficiency compared to prior state-of-the-art KG-augmented LLM approaches, showing better robustness to noise and structural misalignment.", "conclusion": "A dual-process, training-free control framework that combines fast blueprint-based guidance with slow failure-aware refinement can make KG-augmented LLM reasoning more robust, accurate, and efficient than existing homogeneous search strategies."}}
{"id": "2601.11479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11479", "abs": "https://arxiv.org/abs/2601.11479", "authors": ["Yohai Trabelsi", "Guojun Xiong", "Fentabil Getnet", "St\u00e9phane Verguet", "Milind Tambe"], "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning", "comment": null, "summary": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.", "AI": {"tldr": "The paper introduces LEG, a hybrid framework that combines classical optimization and large language models to prioritize which Ethiopian health posts to upgrade so as to maximize population coverage while incorporating qualitative expert preferences.", "motivation": "Ethiopia is upgrading rural health posts but has limited resources, so not all facilities can be upgraded. Decision-makers need a principled way to choose which posts to prioritize that both maximizes population coverage and respects diverse expert and stakeholder preferences, which are often qualitative and hard to encode in standard optimization models.", "method": "The authors develop the Large language model and Extended Greedy (LEG) framework. First, they use a provable approximation algorithm for a population coverage optimization problem to identify near-optimal facility upgrade sets. Then they iteratively refine these solutions with an LLM that translates expert qualitative guidance into constraints or adjustments, aligning the optimization outputs with stakeholder preferences while retaining theoretical coverage guarantees.", "result": "Using real data from three regions in Ethiopia, the framework produces upgrade plans that maintain strong population coverage while better reflecting expert and stakeholder priorities than purely quantitative baselines. The experiments show that LEG can operationalize natural language criteria without substantially sacrificing coverage performance.", "conclusion": "The LEG framework effectively integrates human qualitative input with formal optimization, enabling more equitable and data-driven planning of health facility upgrades. This approach can support ministries of health in Ethiopia and potentially other low-resource settings to make transparent, high-impact resource allocation decisions under complex, multi-stakeholder criteria."}}
{"id": "2601.11090", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11090", "abs": "https://arxiv.org/abs/2601.11090", "authors": ["Davor Lauc"], "title": "Efficient Multilingual Name Type Classification Using Convolutional Networks", "comment": "Preprint of paper presented at ISAI-NLP Phukat 2025", "summary": "We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. We evaluate the architecture on a large multilingual dataset covering 104 languages and four entity types (person, organization, location, other). Onomas-CNN X achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core - 46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.", "AI": {"tldr": "A specialized CNN (Onomas-CNN X) classifies proper names by language and entity type very efficiently on CPUs, achieving high accuracy comparable to XLM-R but with ~46x higher speed and lower energy use.", "motivation": "Large pre-trained transformer models like XLM-RoBERTa are accurate but computationally heavy for focused tasks such as classifying proper names by language and entity type, especially when deployment must be efficient on CPU-only environments. The paper aims to investigate whether a carefully designed, task-specific CNN can match transformer-level accuracy while being much faster and more energy-efficient, particularly when large amounts of labeled training data are available.", "method": "The authors design Onomas-CNN X, a convolutional neural network tailored for name classification. It uses multiple parallel convolution branches to capture features at different n-gram/character spans, incorporates depthwise-separable convolutions to reduce computation, and applies a hierarchical classification scheme to predict both language and entity type. The model is optimized for CPU execution. They train and evaluate it on a large multilingual dataset of proper names spanning 104 languages and four entity types (person, organization, location, other), and compare its performance against fine-tuned XLM-RoBERTa and other baselines in terms of accuracy, throughput, and energy consumption.", "result": "Onomas-CNN X reaches 92.1% classification accuracy on the multilingual name dataset. In terms of efficiency, it processes approximately 2,813 names per second on a single CPU core, which is 46 times faster than a fine-tuned XLM-RoBERTa model with similar accuracy. Energy consumption is also reduced by a factor of 46 compared to transformer-based baselines, indicating major gains in both speed and power efficiency without sacrificing performance.", "conclusion": "The study concludes that for focused NLP tasks like multilingual name classification, specialized CNN architectures can be competitive with or even preferable to large pre-trained transformer models when sufficient labeled data is available. Onomas-CNN X delivers near-transformer accuracy while drastically improving CPU inference speed and energy efficiency, highlighting that classic convolutional approaches remain highly relevant for practical, resource-constrained deployments."}}
{"id": "2601.11492", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11492", "abs": "https://arxiv.org/abs/2601.11492", "authors": ["Kaiwen Wang", "Kaili Zheng", "Rongrong Deng", "Qingmin Fan", "Milin Zhang", "Zongrui Li", "Xuesi Zhou", "Bo Han", "Liren Chen", "Chenyi Guo", "Ji Wu"], "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics", "comment": null, "summary": "Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.", "AI": {"tldr": "BoxMind is an AI expert system that converts boxing match video into structured tactical indicators, uses a graph-based predictive model to forecast outcomes, and turns model gradients into concrete tactical recommendations, achieving expert-level guidance and successful real-world deployment at the 2024 Olympics.", "motivation": "Tactical analysis in boxing lags behind other sports because actions are complex, continuous, and lack a clear, structured representation suitable for AI. Existing analytics tools do not fully capture the fine-grained temporal, spatial, and technical dynamics needed to support real-time strategic decision-making in elite competition. The paper aims to build a systematic, quantitatively grounded way to understand and optimize boxing tactics from video, and to prove that such a system can work in actual top-level events.", "method": "1) Define atomic punch events with precise temporal boundaries and rich attributes (spatial location, technical type, etc.), and parse match videos into sequences of such events. 2) Aggregate these events into 18 hierarchical technical\u2013tactical indicators forming explicit profiles for each boxer and matchup. 3) Design a graph-based predictive model that fuses these explicit indicators with learnable, time-varying latent embeddings to capture interaction dynamics between boxers. 4) Model win probability as a differentiable function of these indicators, allowing computation of gradients with respect to tactical variables. 5) Use these gradients to propose concrete, executable tactical adjustments aimed at increasing win probability. 6) Validate the model quantitatively (outcome prediction accuracy) and qualitatively (comparison to human expert recommendations), and deploy it in a closed loop with coaches and athletes at the 2024 Olympics.", "result": "The outcome prediction component achieves 69.8% accuracy on the BoxerGraph benchmark and 87.5% accuracy on Olympic matches, outperforming prior methods (state-of-the-art). The tactical recommendation module generates strategies whose quality is comparable to those proposed by human experts. In real-world deployment during the 2024 Paris Olympics with the Chinese National Boxing Team, BoxMind was used in a closed-loop fashion and is reported to have directly contributed to the team\u2019s best-ever performance: three gold and two silver medals.", "conclusion": "BoxMind demonstrates that detailed event-level modeling of boxing matches, combined with graph-based prediction and gradient-based tactical optimization, can provide actionable, expert-level decision support in elite competition. The work shows how unstructured video can be transformed into structured tactical intelligence and integrated into a closed-loop workflow with human coaches and athletes. This establishes a generalizable paradigm for AI systems that bridge computer vision and high-stakes, real-time decision support in competitive sports and potentially other domains involving complex human interactions."}}
{"id": "2601.11093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11093", "abs": "https://arxiv.org/abs/2601.11093", "authors": ["Ashish Raj Shekhar", "Shiven Agarwal", "Priyanuj Bordoloi", "Yash Shah", "Tejas Anvekar", "Vivek Gupta"], "title": "Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments", "comment": null, "summary": "Large Language Models (LLMs) can now solve entire exams directly from uploaded PDF assessments, raising urgent concerns about academic integrity and the reliability of grades and credentials. Existing watermarking techniques either operate at the token level or assume control over the model's decoding process, making them ineffective when students query proprietary black-box systems with instructor-provided documents. We present Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged. These watermarks consistently prevent MLLMs from answering shielded exam PDFs and encode stable, item-level signatures that can be reliably recovered from model or student responses. Across 30 exams spanning STEM, humanities, and medical reasoning, Integrity Shield achieves exceptionally high prevention (91-94% exam-level blocking) and strong detection reliability (89-93% signature retrieval) across four commercial MLLMs. Our demo showcases an interactive interface where instructors upload an exam, preview watermark behavior, and inspect pre/post AI performance & authorship evidence.", "AI": {"tldr": "Integrity Shield is a PDF-level watermarking system for exams that makes uploaded assessments unusable for large language models while embedding detectable item-level signatures in responses.", "motivation": "LLMs can now answer full exams directly from uploaded PDFs, threatening academic integrity because instructors cannot prevent or later prove AI-assisted cheating when students use black-box commercial MLLMs. Existing watermarking solutions target generated text or require control over model decoding, which is unrealistic when students query proprietary systems with instructor-authored documents. A method is needed that works at the document layer, preserves exam appearance, blocks MLLMs, and leaves verifiable evidence in responses.", "method": "The authors design Integrity Shield, a schema-aware, item-level watermarking scheme that modifies assessment PDFs at the document layer without visible changes to humans. These embedded watermarks are tailored to the structure of exam items and are crafted so that multimodal LLMs fail to answer watermarked items correctly. At the same time, the watermarks encode stable signatures that can be decoded from either model-generated or student-submitted answers. They evaluate Integrity Shield across 30 diverse exams (STEM, humanities, medical reasoning) on four commercial MLLMs, measuring both prevention (blocking AI performance) and detection (recovering item-level signatures). They also implement an instructor-facing interface for uploading exams, previewing watermark behavior, and inspecting AI performance and authorship evidence before and after watermarking.", "result": "On 30 exams across multiple domains, Integrity Shield achieves 91\u201394% exam-level blocking of MLLMs, meaning that watermarked PDFs largely prevent models from correctly answering the exams. It also yields 89\u201393% reliability in recovering embedded item-level signatures from responses across four commercial MLLMs, showing that the watermarks are stable and decodable in practice.", "conclusion": "Document-layer, schema-aware watermarking of exam PDFs is an effective way to protect academic integrity against black-box LLM usage. Integrity Shield can substantially block MLLMs from solving uploaded assessments while simultaneously embedding robust, item-level signatures that support post-hoc detection of AI involvement. This approach works across domains and commercial models and can be deployed through an interactive tool for instructors, suggesting a practical path to more trustworthy assessments in the age of powerful LLMs."}}
{"id": "2601.11170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11170", "abs": "https://arxiv.org/abs/2601.11170", "authors": ["Taja Kuzman Punger\u0161ek", "Peter Rupnik", "V\u00edt Suchomel", "Nikola Ljube\u0161i\u0107"], "title": "The Growing Gains and Pains of Iterative Web Corpora Crawling: Insights from South Slavic CLASSLA-web 2.0 Corpora", "comment": "10 pages, 7 figures, 2 tables. Submitted to the LREC 2026 conference", "summary": "Crawling national top-level domains has proven to be highly effective for collecting texts in less-resourced languages. This approach has been recently used for South Slavic languages and resulted in the largest general corpora for this language group: the CLASSLA-web 1.0 corpora. Building on this success, we established a continuous crawling infrastructure for iterative national top-level domain crawling across South Slavic and related webs. We present the first outcome of this crawling infrastructure - the CLASSLA-web 2.0 corpus collection, with substantially larger web corpora containing 17.0 billion words in 38.1 million texts in seven languages: Bosnian, Bulgarian, Croatian, Macedonian, Montenegrin, Serbian, and Slovenian. In addition to genre categories, the new version is also automatically annotated with topic labels. Comparing CLASSLA-web 2.0 with its predecessor reveals that only one-fifth of the texts overlap, showing that re-crawling after just two years yields largely new content. However, while the new web crawls bring growing gains, we also notice growing pains - a manual inspection of top domains reveals a visible degradation of web content, as machine-generated sites now contribute a significant portion of texts.", "AI": {"tldr": "They built a continually updated web-crawled corpus (CLASSLA-web 2.0) for seven South Slavic and related languages, vastly expanding size and adding topic labels, but also observing increasing low-quality machine-generated web content.", "motivation": "Less-resourced South Slavic languages lack very large, up-to-date, general-domain corpora. Previous one-shot crawls (CLASSLA-web 1.0) were successful but quickly become outdated and do not capture the evolving web. There is a need for a scalable, continuous crawling setup that can regularly refresh corpora and support better language technology, while also understanding how web content quality changes over time.", "method": "They designed and deployed a continuous crawling infrastructure focused on national top-level domains for South Slavic and related languages. Using this system, they repeatedly crawled the relevant webs, processed the collected pages into texts, and built large web corpora. They automatically annotated documents with genre and new topic labels. They then compared CLASSLA-web 2.0 against the earlier 1.0 version, measuring text overlap and manually inspecting top domains to assess content quality and the presence of machine-generated pages.", "result": "CLASSLA-web 2.0 contains 17.0 billion words in 38.1 million texts across seven languages (Bosnian, Bulgarian, Croatian, Macedonian, Montenegrin, Serbian, and Slovenian), substantially larger than the previous release. Only about 20% of texts overlap with CLASSLA-web 1.0, indicating that re-crawling after two years yields mostly new material. The corpora now include automated genre and topic labels. Manual analysis of high-volume domains shows a notable rise in machine-generated sites, indicating a degradation of web content quality.", "conclusion": "Iterative national domain crawling is an effective way to maintain large, current web corpora for less-resourced South Slavic languages, providing substantial new data with each re-crawl and enabling richer annotation such as topics. However, as the web increasingly fills with machine-generated text, corpus builders face growing challenges in maintaining content quality, and future work must address filtering or handling of such data."}}
{"id": "2601.11190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11190", "abs": "https://arxiv.org/abs/2601.11190", "authors": ["Laura Menotti", "Stefano Marchesin", "Gianmaria Silvello"], "title": "DOREMI: Optimizing Long Tail Predictions in Document-Level Relation Extraction", "comment": "Accepted for publication in Knowledge-Based Systems", "summary": "Document-Level Relation Extraction (DocRE) presents significant challenges due to its reliance on cross-sentence context and the long-tail distribution of relation types, where many relations have scarce training examples. In this work, we introduce DOcument-level Relation Extraction optiMizing the long taIl (DOREMI), an iterative framework that enhances underrepresented relations through minimal yet targeted manual annotations. Unlike previous approaches that rely on large-scale noisy data or heuristic denoising, DOREMI actively selects the most informative examples to improve training efficiency and robustness. DOREMI can be applied to any existing DocRE model and is effective at mitigating long-tail biases, offering a scalable solution to improve generalization on rare relations.", "AI": {"tldr": "The paper proposes DOREMI, an iterative framework to improve document-level relation extraction on rare (long-tail) relations using minimal, targeted human annotation.", "motivation": "Document-level relation extraction requires reasoning over multiple sentences and suffers from a long-tail distribution of relation types, where many relations have few labeled instances. Existing methods often depend on large-scale but noisy distant supervision or heuristic denoising, which may not effectively help rare relations. There is a need for a scalable, model-agnostic way to systematically reduce long-tail bias and improve performance on underrepresented relations without requiring extensive annotation.", "method": "The authors propose DOREMI, an iterative training framework that can wrap around any existing DocRE model. In each iteration, DOREMI actively selects the most informative examples\u2014especially those related to underrepresented relations\u2014for manual annotation. These targeted annotations are then added to the training data to update the base DocRE model. The selection strategy focuses on reducing long-tail bias and improving robustness, rather than passively using large amounts of noisy data or relying solely on heuristic denoising.", "result": "Using DOREMI leads to better performance on rare and underrepresented relation types in document-level relation extraction tasks. The framework improves generalization on long-tail relations while maintaining or improving overall performance, and does so with relatively few additional labeled examples thanks to its targeted selection strategy.", "conclusion": "DOREMI is a scalable, model-agnostic framework that effectively mitigates long-tail bias in document-level relation extraction by iteratively adding minimal, carefully chosen manual annotations. It can be integrated with existing DocRE models to substantially enhance performance on rare relations, improving robustness and generalization without relying on large amounts of noisy or heuristically denoised training data."}}
{"id": "2601.11214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11214", "abs": "https://arxiv.org/abs/2601.11214", "authors": ["Hanchen Xia", "Baoyou Chen", "Yutang Ge", "Guojiang Zhao", "Siyu Zhu"], "title": "T$^\\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL", "comment": null, "summary": "We present T$^\\star$, a simple \\textsc{TraceRL}-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T$^\\star$~transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal performance degradation on math reasoning benchmarks. Moreover, further analysis suggests that T$^\\star$~can converge to an alternative decoding schedule $\\hat{\\rm S}$ that achieves comparable performance.", "AI": {"tldr": "The paper introduces T* (T-star), a training curriculum for masked diffusion language models that progressively increases block size, enabling more parallel decoding while maintaining performance on math reasoning tasks.", "motivation": "Diffusion-style language models often rely on masked, block-wise decoding. Smaller blocks tend to perform better but decode slowly due to less parallelism, while larger blocks allow faster, more parallel decoding but can hurt accuracy, especially on challenging tasks like mathematical reasoning. There is a need for a principled way to move from small, accurate blocks to large, efficient ones without losing much performance.", "method": "The authors propose T*, a curriculum based on TraceRL that starts from an autoregressively initialized masked diffusion language model with small block size. During training, T* progressively increases the block size, effectively teaching the model to handle larger blocks while leveraging the stability and performance of the small-block regime. The process is designed to be smooth, so that the model gradually adapts instead of making abrupt changes that could harm performance. They also analyze the optimization dynamics and show that the same training procedure can converge to an alternative decoding schedule \u015c that yields similar performance to the primary schedule.", "result": "Using T*, the authors obtain masked diffusion language models that can decode with larger block sizes\u2014thus higher parallelism\u2014while suffering only minimal degradation on mathematical reasoning benchmarks compared to the small-block baseline. Their analysis also demonstrates that the training can converge to an alternative decoding schedule \u015c (a different way of ordering or grouping token predictions) that performs comparably to the original schedule, suggesting some flexibility in the decoding process induced by the curriculum.", "conclusion": "T* provides an effective TraceRL-based curriculum for scaling block sizes in masked diffusion language models, reconciling the trade-off between decoding parallelism and performance. It allows models to move from accurate but slow small-block decoding to faster large-block decoding with little loss on math reasoning tasks, and reveals that multiple decoding schedules\u2014such as the alternative schedule \u015c\u2014can achieve similar performance when trained with this curriculum."}}
{"id": "2601.11220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11220", "abs": "https://arxiv.org/abs/2601.11220", "authors": ["Rafael Martins Frade", "Rrubaa Panchendrarajan", "Arkaitz Zubiaga"], "title": "MultiCaption: Detecting disinformation using multilingual visual claims", "comment": null, "summary": "Online disinformation poses an escalating threat to society, driven increasingly by the rapid spread of misleading content across both multimedia and multilingual platforms. While automated fact-checking methods have advanced in recent years, their effectiveness remains constrained by the scarcity of datasets that reflect these real-world complexities. To address this gap, we first present MultiCaption, a new dataset specifically designed for detecting contradictions in visual claims. Pairs of claims referring to the same image or video were labeled through multiple strategies to determine whether they contradict each other. The resulting dataset comprises 11,088 visual claims in 64 languages, offering a unique resource for building and evaluating misinformation-detection systems in truly multimodal and multilingual environments. We then provide comprehensive experiments using transformer-based architectures, natural language inference models, and large language models, establishing strong baselines for future research. The results show that MultiCaption is more challenging than standard NLI tasks, requiring task-specific finetuning for strong performance. Moreover, the gains from multilingual training and testing highlight the dataset's potential for building effective multilingual fact-checking pipelines without relying on machine translation.", "AI": {"tldr": "The paper introduces MultiCaption, a challenging multimodal, multilingual dataset for detecting contradictions between visual claims about the same image/video, and establishes baselines showing it requires task-specific finetuning and benefits from multilingual training.", "motivation": "Online disinformation spreads rapidly across languages and media types, but existing automated fact-checking systems are limited by datasets that are mostly monolingual, text-only, or too simplified. There is a lack of realistic resources that jointly capture multilingual and multimodal aspects of misinformation, especially for contradiction detection between claims about the same visual content.", "method": "The authors construct MultiCaption, a dataset of 11,088 claims in 64 languages, organized in pairs that reference the same image or video. They use multiple labeling strategies to annotate whether claim pairs contradict each other. They then run extensive experiments with transformer-based models, standard natural language inference (NLI) architectures, and large language models, evaluating performance and comparing to traditional NLI tasks, including multilingual training and testing setups.", "result": "The experiments show that models find MultiCaption significantly harder than standard NLI benchmarks, with off-the-shelf models underperforming and requiring targeted finetuning to achieve good results. The study also finds that multilingual training and evaluation yield notable performance gains, demonstrating cross-lingual benefits and suggesting that effective multilingual fact-checking systems can be built without heavy reliance on machine translation.", "conclusion": "MultiCaption fills an important gap by providing a realistic multimodal, multilingual benchmark for contradiction detection in visual claims, revealing that the task is more complex than standard NLI. The dataset encourages development of specialized methods and shows that multilingual modeling is especially promising, supporting future research on robust, translation-free multilingual misinformation detection and fact-checking pipelines."}}
{"id": "2601.11227", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11227", "abs": "https://arxiv.org/abs/2601.11227", "authors": ["Shaoyang Xu", "Wenxuan Zhang"], "title": "Language of Thought Shapes Output Diversity in Large Language Models", "comment": null, "summary": "Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.", "AI": {"tldr": "The paper shows that making an LLM \"think\" in different languages, even when the final answer is in English, systematically increases the diversity of outputs, especially when using languages that are linguistically farther from English, and that combining multiple such languages further raises this diversity ceiling with practical benefits for pluralistic alignment.", "motivation": "LLMs often suffer from limited output diversity, which restricts pluralism, creativity, and coverage of different cultural perspectives and value orientations. Existing diversity methods mainly operate within a monolingual (typically English) setting or rely on sampling tricks that do not structurally alter the model\u2019s internal reasoning process. The authors are motivated to explore whether the internal language used for chain-of-thought (the \"language of thought\") can be leveraged as a deeper, more structural driver of diversity, especially in multilingual models.", "method": "The authors first empirically map how different thinking languages (languages used in the intermediate reasoning steps) occupy distinct regions in the model\u2019s internal \"thinking space.\" Using this observation, they design and compare two strategies for repeated sampling under multilingual chain-of-thought: (1) Single-Language Sampling, where multiple samples are generated with the model thinking in a fixed non-English language, and (2) Mixed-Language Sampling, where samples are drawn across multiple thinking languages. In all cases the final user-facing output is controlled to be in English, isolating the effect of the thinking language. They then conduct extensive diversity evaluations over these outputs and analyze correlations between language distance from English in the thinking space and the resulting diversity. They further test scaling effects by increasing the number and heterogeneity of thinking languages and evaluate downstream benefits in pluralistic alignment tasks, including coverage of cultural knowledge and value orientations. Code is released for reproducibility.", "result": "The experiments show that using non-English thinking languages consistently increases output diversity compared to English-only thinking. There is a clear positive correlation: languages that are more distant from English in the learned thinking-space representation lead to larger diversity gains. Aggregating samples across multiple thinking languages (Mixed-Language Sampling) yields additional improvements beyond any single language due to compositional effects. As the linguistic heterogeneity of the thinking languages increases, the maximum achievable diversity (the \"diversity ceiling\") expands further. These gains are not merely cosmetic; they yield practical improvements in pluralistic alignment benchmarks, where the model\u2019s outputs demonstrate broader coverage of cultural knowledge and a wider spectrum of value orientations.", "conclusion": "The paper concludes that the language of thought in multilingual LLMs is a powerful, underexploited lever for structurally enhancing output diversity without sacrificing control over the final response language. Different thinking languages correspond to distinct regions in the model\u2019s reasoning space, and systematically exploiting this fact\u2014especially via heterogeneous, mixed-language sampling\u2014raises the diversity ceiling and improves pluralistic alignment, leading to richer, more culturally and normatively varied outputs. This suggests that future LLM deployment and alignment strategies should treat internal reasoning language as a controllable dimension for diversity and pluralism, and that scaling along the axis of linguistic heterogeneity is an effective path to more diverse and inclusive model behavior."}}
{"id": "2601.11232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11232", "abs": "https://arxiv.org/abs/2601.11232", "authors": ["Javier Carnerero-Cano", "Massimiliano Pronesti", "Radu Marinescu", "Tigran Tchrakian", "James Barry", "Jasmina Gajcin", "Yufang Hou", "Alessandra Pascale", "Elizabeth Daly"], "title": "FactCorrector: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models", "comment": null, "summary": "Large language models (LLMs) are widely used in knowledge-intensive applications but often generate factually incorrect responses. A promising approach to rectify these flaws is correcting LLMs using feedback. Therefore, in this paper, we introduce FactCorrector, a new post-hoc correction method that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. To support rigorous evaluations of factuality correction methods, we also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections. Experiments on VELI5 and several popular long-form factuality datasets show that the FactCorrector approach significantly improves factual precision while preserving relevance, outperforming strong baselines. We release our code at https://ibm.biz/factcorrector.", "AI": {"tldr": "They propose FactCorrector, a post-hoc method that uses structured feedback to correct factual errors in large language model outputs, and introduce a benchmark (VELI5) with injected errors and gold corrections, showing improved factual accuracy over baselines.", "motivation": "LLMs are increasingly used in knowledge-intensive tasks but often hallucinate or output factually incorrect information. Existing solutions can require retraining, may not generalize across domains, or lack rigorous evaluation settings for factual correction. There is a need for a flexible, domain-adaptive correction method and a robust benchmark for measuring factuality correction performance.", "method": "They design FactCorrector, a post-hoc correction system that takes an LLM\u2019s original response plus structured factuality feedback (indicating which parts are incorrect) and generates a corrected answer without retraining the base model. They also construct the VELI5 benchmark by systematically injecting factual errors into existing content and providing ground-truth corrected versions, enabling controlled evaluation of factual correction techniques.", "result": "On VELI5 and several existing long-form factuality datasets, FactCorrector substantially improves factual precision (reducing incorrect statements) while maintaining the relevance and usefulness of responses. It performs better than strong existing baselines for factual correction in terms of accuracy-relevance trade-offs.", "conclusion": "Post-hoc correction guided by structured feedback is an effective way to improve the factuality of LLM outputs without retraining. The FactCorrector method generalizes across domains and, together with the VELI5 benchmark, provides a practical framework and evaluation setting for developing and measuring factuality correction methods. The released code supports reproduction and further research."}}
{"id": "2601.11234", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11234", "abs": "https://arxiv.org/abs/2601.11234", "authors": ["Galo Castillo-L\u00f3pez", "Alexis Lombard", "Nasredine Semmar", "Ga\u00ebl de Chalendar"], "title": "How DDAIR you? Disambiguated Data Augmentation for Intent Recognition", "comment": "Accepted for publication at EACL 2026", "summary": "Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.", "AI": {"tldr": "They propose DDAIR, a method that filters and regenerates LLM-augmented intent detection data using sentence embeddings to remove ambiguous synthetic examples, improving classification in low-resource, loosely defined intent settings.", "motivation": "LLMs are good at generating synthetic training data for intent classification, especially in low-resource settings, but they can create examples that are ambiguous or actually closer to other, non-target intents. Such mislabeled or confusing synthetic examples may harm model performance, especially when intents are broad or loosely defined. The paper aims to systematically detect and reduce these ambiguities in augmented data.", "method": "They introduce DDAIR, which uses Sentence Transformers to compute sentence embeddings for synthetic examples generated by an LLM for a given intent. For each generated example, they compare its semantic similarity to the target intent versus all other intents. Examples that are more similar to a different intent are flagged as ambiguous. They then apply an iterative re-generation process: ambiguous examples are sent back to the LLM with updated prompts or constraints to obtain clearer, less ambiguous samples. This process can be repeated until a satisfactory, disambiguated augmented dataset is obtained.", "result": "They find that sentence-embedding-based similarity is effective at identifying synthetic intent examples that are semantically closer to another class than their intended label. The iterative re-generation step reduces the number of ambiguous samples and yields a dataset with cleaner intent boundaries. Empirically, this leads to improved performance for intent recognition models in low-resource settings, particularly where intents are loosely or broadly defined.", "conclusion": "Sentence-transformer-based semantic similarity can be used as a practical filter and feedback mechanism for LLM-based data augmentation in intent detection. By detecting and regenerating ambiguous samples, DDAIR produces higher-quality synthetic training data and can improve classification performance in challenging low-resource, loosely defined intent scenarios. The approach demonstrates that coupling LLMs with embedding models is a promising direction for controlled, disambiguated data augmentation."}}
{"id": "2601.11255", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11255", "abs": "https://arxiv.org/abs/2601.11255", "authors": ["Yuling Shi", "Maolin Sun", "Zijun Liu", "Mo Yang", "Yixiong Fang", "Tianran Sun", "Xiaodong Gu"], "title": "Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering", "comment": "Accepted to GLOW@WWW2026. Code available at https://github.com/sakura20221/RT-RAG", "summary": "Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.", "AI": {"tldr": "The paper proposes RT-RAG, a hierarchical, reasoning-tree-guided retrieval-augmented generation framework that improves multi-hop QA performance by reducing query decomposition errors and mitigating error propagation.", "motivation": "Existing multi-hop QA RAG systems rely on LLMs to self-plan multi-step retrieval, which often causes incoherent reasoning due to inaccurate query decomposition and error propagation across steps.", "method": "RT-RAG explicitly decomposes each multi-hop question into a structured reasoning tree using entity analysis and consensus-based tree selection, clearly distinguishing core queries, known entities, and unknown entities. It then performs bottom-up traversal of the tree with iterative query rewriting and refinement to retrieve and aggregate high-quality supporting evidence at each node.", "result": "Experiments show that RT-RAG substantially surpasses prior state-of-the-art approaches on complex multi-hop QA benchmarks, with improvements of about 7.0 F1 and 6.0 EM points.", "conclusion": "Explicit reasoning-tree decomposition combined with bottom-up, refinement-based retrieval significantly improves coherence and accuracy in complex multi-hop QA, making RT-RAG a strong framework for RAG-based reasoning tasks."}}
{"id": "2601.11344", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11344", "abs": "https://arxiv.org/abs/2601.11344", "authors": ["Parker Seegmiller", "Joseph Gatto", "Sarah E. Greer", "Ganza Belise Isingizwe", "Rohan Ray", "Timothy E. Burdick", "Sarah Masud Preum"], "title": "How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting", "comment": null, "summary": "Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.", "AI": {"tldr": "The paper studies how well large language models can draft patient portal message responses that match individual clinicians, introduces a taxonomy of response themes and an editing-load evaluation framework, and shows that tailored, theme-aware adaptation is needed for reliable use.", "motivation": "Although LLMs can help draft responses to patient portal messages, it is unclear whether their suggestions actually align with how individual clinicians respond and whether they reduce clinicians' time and effort. There is a need to understand alignment at a fine-grained thematic level and to quantify the editing burden clinicians face when using LLM-generated drafts, in order to safely integrate LLMs into clinical communication workflows.", "method": "The authors construct a taxonomy of thematic elements found in clinician responses (e.g., information giving, empathy, asking follow-up questions). They design an evaluation framework that measures how much clinicians must edit LLM-drafted responses, both in terms of content and specific themes. They create and release an expert-annotated dataset of patient messages and clinician responses labeled with these themes. They then run large-scale experiments comparing different local and commercial LLMs, applying adaptation strategies such as thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization, and evaluate their performance using the proposed framework.", "result": "The study finds substantial epistemic uncertainty in aligning LLM drafts with actual clinician responses: LLMs can handle some thematic elements reasonably well but perform poorly on others, especially when generating clinician-like follow-up questions to elicit more information from patients. Theme-focused adaptation methods improve performance across most themes, but alignment remains uneven and clinician-dependent.", "conclusion": "LLMs are not yet reliably aligned with individual clinicians for patient portal response drafting, and naive deployment may not meaningfully reduce clinician workload. Effective use requires explicitly modeling and adapting to clinicians\u2019 thematic preferences, using the proposed taxonomy and evaluation framework. Personalization and theme-driven adaptation are therefore essential for responsible integration of LLMs into patient\u2013clinician communication workflows."}}
{"id": "2601.11293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11293", "abs": "https://arxiv.org/abs/2601.11293", "authors": ["Malin Astrid Larsson", "Harald Fosen Grunnaleite", "Vinay Setty"], "title": "One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking", "comment": "Accepted version in ECIR 2026", "summary": "Large language models (LLMs) are reshaping automated fact-checking (AFC) by enabling unified, end-to-end verification pipelines rather than isolated components. While large proprietary models achieve strong performance, their closed weights, complexity, and high costs limit sustainability. Fine-tuning smaller open weight models for individual AFC tasks can help but requires multiple specialized models resulting in high costs. We propose \\textbf{multi-task learning (MTL)} as a more efficient alternative that fine-tunes a single model to perform claim detection, evidence ranking, and stance detection jointly. Using small decoder-only LLMs (e.g., Qwen3-4b), we explore three MTL strategies: classification heads, causal language modeling heads, and instruction-tuning, and evaluate them across model sizes, task orders, and standard non-LLM baselines. While multitask models do not universally surpass single-task baselines, they yield substantial improvements, achieving up to \\textbf{44\\%}, \\textbf{54\\%}, and \\textbf{31\\%} relative gains for claim detection, evidence re-ranking, and stance detection, respectively, over zero-/few-shot settings. Finally, we also provide practical, empirically grounded guidelines to help practitioners apply MTL with LLMs for automated fact-checking.", "AI": {"tldr": "The paper investigates multi-task learning (MTL) with small open-weight LLMs for automated fact-checking and finds that a single multitask model can substantially outperform zero-/few-shot prompting and approach or sometimes match single-task fine-tuned models across claim detection, evidence ranking, and stance detection.", "motivation": "Automated fact-checking pipelines usually consist of separate components (claim detection, evidence retrieval/ranking, stance detection), which are often implemented either with large, expensive proprietary LLMs or multiple smaller fine-tuned models. This leads to high computational and maintenance costs and reduced sustainability. The authors want a more efficient, open, and unified solution using smaller open-weight LLMs that can still achieve strong performance across all AFC subtasks.", "method": "The authors fine-tune small decoder-only open-weight LLMs (e.g., Qwen3-4B) using multi-task learning across three automated fact-checking subtasks: claim detection, evidence ranking, and stance detection. They compare three MTL formulations: (1) shared backbone with task-specific classification heads, (2) shared backbone with causal language modeling heads (text-format outputs), and (3) instruction-tuning in a unified prompted format. They systematically evaluate across different model sizes, task orderings, and against standard non-LLM and LLM baselines, including zero-/few-shot prompting and single-task fine-tuning.", "result": "Multitask models do not always outperform the best single-task fine-tuned baselines on every subtask, but they significantly improve over zero-/few-shot performance: up to 44% relative improvement for claim detection, 54% for evidence re-ranking, and 31% for stance detection. The results show that MTL with small open LLMs is a viable, efficient alternative to multiple specialized models, offering strong performance across all AFC stages with a single model.", "conclusion": "A single small open-weight LLM fine-tuned via multi-task learning can effectively handle key stages of automated fact-checking\u2014claim detection, evidence ranking, and stance detection\u2014achieving substantial gains over zero-/few-shot approaches and competitive performance with single-task models. This improves efficiency and sustainability of AFC systems. The paper also distills practical recommendations for how to design and fine-tune MTL setups with LLMs for real-world fact-checking applications."}}
{"id": "2601.11379", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.11379", "abs": "https://arxiv.org/abs/2601.11379", "authors": ["Morgane Hoffmann", "Emma Jouffroy", "Warren Jouanneau", "Marc Palyart", "Charles Pebereau"], "title": "Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences", "comment": null, "summary": "General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.", "AI": {"tldr": "The paper evaluates how large language models make recruitment decisions, quantifying how they weight different candidate and job attributes and comparing this to human-like economic hiring logic.", "motivation": "Although LLMs are increasingly used in recruitment, it is unclear how they internally prioritize skills, experience, demographics, and other signals of productivity, and whether this logic is aligned with economic principles and human recruiter behavior.", "method": "The authors construct synthetic hiring scenarios using real freelancer profiles and project descriptions from an online marketplace, apply a full factorial experimental design, and prompt an LLM to rate freelancer\u2013project fit; they then statistically estimate the implicit weights the model assigns to various attributes and analyze variation across project types and demographic subgroups.", "result": "The LLM gives strong weight to core productivity-related features such as skills and experience, but also seems to infer additional meaning from some attributes beyond their straightforward matching role; on average, it shows little direct discrimination against minority groups, though intersectional analysis reveals that similar productivity signals are not valued equally across demographic combinations.", "conclusion": "The proposed framework makes it possible to open the \u201cblack box\u201d of LLM hiring decisions and compare them with human decision patterns; while the studied LLM broadly emphasizes economically relevant productivity signals, hidden intersectional disparities in how it treats these signals highlight the need for careful auditing and alignment before deploying such models in recruitment."}}
{"id": "2601.11314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11314", "abs": "https://arxiv.org/abs/2601.11314", "authors": ["Jiatong Yi", "Yanyang Li"], "title": "Membership Inference on LLMs in the Wild", "comment": null, "summary": "Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.", "AI": {"tldr": "The paper introduces SimMIA, a new membership inference attack framework for large language models that works in strict black-box, text-only settings and achieves state-of-the-art performance, along with a new evaluation benchmark WikiMIA-25.", "motivation": "Existing membership inference attacks on LLMs either depend on non-public internals like logits or generalize poorly when only text outputs are available, which is the realistic setting for proprietary models. There is a need for a robust, generalizable MIA method that operates purely from generated text to audit whether specific data was used in training.", "method": "The authors design SimMIA, a framework that operates in a strict black-box regime. It uses an advanced sampling strategy to query LLMs multiple times and a specialized scoring mechanism over the resulting generated texts to distinguish between member and non-member data points. They also construct WikiMIA-25, a benchmark dataset, to systematically test MIAs on modern proprietary LLMs.", "result": "Experiments on WikiMIA-25 and other settings show that SimMIA achieves state-of-the-art performance among text-only MIAs and can rival or approach methods that have access to internal model information such as logits. The framework demonstrates strong robustness and better cross-domain generalization than prior black-box approaches.", "conclusion": "SimMIA provides an effective, robust membership inference attack framework that works in realistic black-box conditions for LLMs, and the WikiMIA-25 benchmark enables standardized evaluation. Together, they show that training data privacy risks for LLMs remain significant even without access to internal model signals."}}
{"id": "2601.11429", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11429", "abs": "https://arxiv.org/abs/2601.11429", "authors": ["Yuetian Lu", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Relational Linearity is a Predictor of Hallucinations", "comment": "11 pages, 4 figures, 8 tables", "summary": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $\u0394\\cos$. We find a strong correlation ($r \\in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.", "AI": {"tldr": "The paper investigates why large language models hallucinate factual answers, finding that hallucinations correlate strongly with how linearly a relation is represented in the model\u2019s embedding space.", "motivation": "Hallucinations\u2014confidently wrong answers\u2014are a key limitation of LLMs, especially when models fail to recognize that they do not know a fact. The authors want to understand a mechanistic factor behind such hallucinations, beyond just measuring their frequency, to enable better control and mitigation.", "method": "They construct SyntHal, a dataset of 6000 synthetic entities spanning six relations, crafted so the model has no prior knowledge of the entities. Using four different models, they query these synthetic facts and measure hallucination rates for each relation. They then quantify the linearity of each relation in the models\u2019 representation space using a \u0394cos metric, and analyze the correlation between relational linearity and hallucination rate.", "result": "Medium-size models like Gemma-7B-IT hallucinate frequently on questions about synthetic entities, especially for relations that are represented more linearly in embedding space. Across four models, the measured correlation between relational linearity (\u0394cos) and hallucination rate is very high (Pearson r between 0.78 and 0.82).", "conclusion": "The way factual relations are internally stored\u2014particularly their linearity in embedding space\u2014affects models\u2019 ability to self-assess their knowledge and avoid hallucinating. More abstract, linearly represented relations lead to more hallucinations, while more direct, nonlinear storage supports better knowledge assessment. This insight suggests that modifying how models encode and store factual triples could be a promising direction for hallucination mitigation and improved factual reliability."}}
{"id": "2601.11329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11329", "abs": "https://arxiv.org/abs/2601.11329", "authors": ["Maike Z\u00fcfle", "Ondrej Klejch", "Nicholas Sanders", "Jan Niehues", "Alexandra Birch", "Tsz Kin Lam"], "title": "F-Actor: Controllable Conversational Behaviour in Full-Duplex Models", "comment": null, "summary": "Spoken conversational systems require more than accurate speech generation to have human-like conversations: to feel natural and engaging, they must produce conversational behaviour that adapts dynamically to the context. Current spoken conversational systems, however, rarely allow such customization, limiting their naturalness and usability. In this work, we present the first open, instruction-following full-duplex conversational speech model that can be trained efficiently under typical academic resource constraints. By keeping the audio encoder frozen and finetuning only the language model, our model requires just 2,000 hours of data, without relying on large-scale pretraining or multi-stage optimization. The model can follow explicit instructions to control speaker voice, conversation topic, conversational behaviour (e.g., backchanneling and interruptions), and dialogue initiation. We propose a single-stage training protocol and systematically analyze design choices. Both the model and training code will be released to enable reproducible research on controllable full-duplex speech systems.", "AI": {"tldr": "An efficient, open, instruction-following full-duplex conversational speech model enabling controllable, natural spoken interactions with modest data and compute.", "motivation": "Spoken dialogue systems today sound accurate but not truly conversational, because they lack fine-grained, dynamic control over behaviours like backchanneling, interruptions, topic control, and voice style, and existing advanced systems are typically closed-source or resource-intensive. The authors aim to make natural, controllable, human-like spoken interaction feasible and reproducible in typical academic settings.", "method": "They build a full-duplex conversational speech system where the audio encoder is kept frozen and only the language model is finetuned, reducing data and compute demands. Using about 2,000 hours of data and a single-stage training protocol, they train an instruction-following model that conditions on explicit instructions specifying voice characteristics, topics, conversational behaviours, and dialogue initiation. They systematically vary and analyze training design choices within this framework.", "result": "The resulting model can reliably follow explicit instructions to control speaker voice, conversation topic, backchanneling, interruptions, and when/how to initiate dialogue, all in full-duplex speech. It achieves this with only 2,000 hours of training data, without large-scale pretraining or multi-stage optimization, demonstrating that controllable, natural full-duplex speech is attainable under standard academic resources.", "conclusion": "The authors demonstrate that an instruction-following, controllable full-duplex conversational speech model can be trained efficiently by freezing the audio encoder and finetuning only the language model in a single-stage pipeline. Their open model and released training code establish a practical, reproducible foundation for research on natural, controllable spoken conversational agents."}}
{"id": "2601.11441", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11441", "abs": "https://arxiv.org/abs/2601.11441", "authors": ["Xiaojie Gu", "Guangxu Chen", "Yuheng Yang", "Jingxin Han", "Andi Zhang"], "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models", "comment": "ICASSP 2026", "summary": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE", "AI": {"tldr": "The paper proposes HORSE, a new model editing method for LLMs that uses hierarchical orthogonal residual spread on the information matrix to enable safer, more stable, and scalable edits.", "motivation": "LLMs need safer behavior, but retraining is costly and existing model editing methods that mix old and new knowledge via an information matrix can be computationally expensive and introduce conflicts. A more stable, efficient editing mechanism is needed for massive, precise edits.", "method": "Introduce HORSE (Hierarchical Orthogonal Residual SprEad), which reorganizes and constrains the information matrix used in model editing. Instead of directly optimizing a blended matrix of old and new knowledge, HORSE enforces a hierarchical, orthogonal residual structure that spreads updates across appropriate subspaces to suppress noisy gradients and reduce interference between edits. The method is compared theoretically to popular editing approaches and empirically evaluated on multiple LLMs and two datasets.", "result": "On the evaluated LLMs and datasets, HORSE achieves precise and stable massive editing, maintaining performance across diverse editing scenarios while reducing gradient noise and conflicts relative to baseline methods.", "conclusion": "HORSE offers a more stable and efficient alternative for LLM model editing by applying hierarchical orthogonal residual spread to the information matrix, enabling large-scale precise edits with fewer conflicts and better safety control compared with existing editing techniques."}}
{"id": "2601.11332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11332", "abs": "https://arxiv.org/abs/2601.11332", "authors": ["Sama Hadhoud", "Alaa Elsetohy", "Frederikus Hudi", "Jan Christian Blaise Cruz", "Steven Halim", "Alham Fikri Aji"], "title": "Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming", "comment": null, "summary": "Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.", "AI": {"tldr": "The paper disentangles problem-solving (algorithm design) from code implementation when evaluating LLMs on competitive programming by using natural-language editorials, new annotations, and a new benchmark of ICPC-style problems.", "motivation": "Existing evaluations of LLMs on competitive programming mix two separate skills: algorithmic reasoning and code implementation. This conflation makes it unclear whether failures come from poor problem understanding/algorithm design or from low-level coding mistakes. As LLMs start to perform well on such benchmarks, there is a need for more diagnostic and principled evaluations that identify the true bottlenecks in their reasoning and coding abilities.", "method": "The authors propose using natural-language editorials (step-by-step algorithmic explanations) as the central object for both solution generation and evaluation. They: (1) introduce a benchmark of 83 ICPC-style problems with professionally written gold editorials and full test suites; (2) prompt LLMs to first generate an editorial and then produce code from that editorial, also testing performance when given gold editorials; (3) compare model-generated editorials with gold editorials using expert annotations to classify reasoning errors; and (4) design and validate an LLM-as-a-judge protocol to automatically assess the quality of editorials at scale.", "result": "They evaluate 19 LLMs and find that: (1) prompting models to write an editorial before coding can improve solve rates for some LLMs; (2) supplying gold editorials yields much larger performance gains, showing that correct algorithm specifications substantially help models implement solutions; (3) even with gold editorials, models still exhibit significant implementation errors, revealing a nontrivial coding bottleneck; and (4) discrepancies between generated and gold editorials highlight persistent weaknesses in problem-solving and algorithm specification, which they systematically categorize through expert and LLM-based evaluation.", "conclusion": "The study concludes that competitive programming for LLMs should be treated primarily as a problem-solving task, and that benchmarks need to explicitly separate algorithm design from implementation to properly diagnose capabilities. Natural-language editorials serve as a powerful interface for both generating and evaluating solutions, and the new dataset plus LLM-as-a-judge method provide a scalable framework for understanding and improving LLM reasoning and coding on complex programming problems."}}
{"id": "2601.11517", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11517", "abs": "https://arxiv.org/abs/2601.11517", "authors": ["Koyena Pal", "David Bau", "Chandan Singh"], "title": "Do explanations generalize across large reasoning models?", "comment": null, "summary": "Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.", "AI": {"tldr": "The paper studies whether chain-of-thought (CoT) explanations from large reasoning models generalize across models, finding that many CoTs do induce similar behavior in other models, are aligned with human preferences and RL post-training, and can be exploited via a simple ensembling strategy\u2014while also calling for caution when using CoTs for scientific insight.", "motivation": "Large reasoning models output chain-of-thought explanations that appear human-interpretable and could, in principle, reveal general problem-solving patterns or new scientific concepts. However, it is unknown if these explanations genuinely reflect general, model-independent reasoning or are idiosyncratic to a particular model. This matters for interpretability, reliability, and scientific discovery applications where users might over-trust or misinterpret such explanations.", "method": "The authors define and operationalize a specific notion of explanation generalization: a CoT explanation from one LRM is considered generalizable if, when provided as input to other LRMs, it induces similar answers/behaviors. They empirically test this by generating CoTs from a source model, feeding these CoTs to various target LRMs, and measuring consistency of their outputs. They examine how this consistency relates to human preference judgments and to RL-based post-training of the models. They also analyze structural conditions under which explanations yield consistent answers and design a sentence-level ensembling scheme that aggregates information from multiple explanations to improve cross-model agreement.", "result": "They observe that CoT explanations frequently generalize in the defined sense: explanations generated by one LRM often increase behavioral consistency across different LRMs compared to not providing the CoT. This cross-model consistency is positively correlated with human preference rankings of explanations and with models that have undergone RL post-training. Their analysis highlights properties of explanations and tasks that affect consistency and shows that a simple sentence-level ensembling\u2014combining segments from multiple CoTs\u2014can further boost consistency between models.", "conclusion": "The paper concludes that CoT explanations do encode reasoning patterns that are, to a significant degree, shared across LRMs, as evidenced by their ability to steer other models\u2019 behavior and increase inter-model consistency. However, because this generalization is imperfect and context-dependent, one should be cautious about over-interpreting CoTs as faithful or novel scientific insights. The work proposes a concrete framework and metrics for evaluating explanation generalization and demonstrates that straightforward ensembling can improve reliability, suggesting a practical route for more robust use of LRM-generated explanations."}}
{"id": "2601.11340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11340", "abs": "https://arxiv.org/abs/2601.11340", "authors": ["Guoming Ling", "Zhongzhan Huang", "Yupei Lin", "Junxin Li", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "title": "Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models", "comment": null, "summary": "Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.", "AI": {"tldr": "The paper proposes Neural Chain-of-Thought Search (NCoTS), a framework that treats reasoning in large language models as a search problem and finds shorter, more accurate reasoning paths using a cost\u2013accuracy heuristic.", "motivation": "Existing Chain-of-Thought (CoT) prompting improves reasoning but forces models to generate steps sequentially without global planning. This often leads to long, redundant, or suboptimal reasoning trajectories. The authors are motivated to find a way to steer generation toward globally better reasoning paths that are both more accurate and more efficient, addressing the trade-off between solution quality and computational cost.", "method": "They reformulate reasoning as a dynamic search over a space of reasoning paths. They first quantitatively characterize this solution space, showing that there exist sparse paths that are both concise and highly accurate. Building on this, they design Neural Chain-of-Thought Search (NCoTS), which uses a set of reasoning operators to expand candidate paths and evaluates them with a dual-factor heuristic. This heuristic jointly scores candidates by predicted correctness and computational cost (e.g., length), allowing the system to prioritize promising, efficient reasoning trajectories and prune less effective ones during generation instead of purely sequential decoding.", "result": "Across multiple reasoning benchmarks, NCoTS outperforms standard CoT generation. It yields a Pareto improvement: accuracy increases by more than 3.5% while average generation length (and thus cost) decreases by more than 22% compared to baseline CoT outputs, demonstrating that better and shorter reasoning paths can be systematically discovered.", "conclusion": "Reasoning with LLMs can be effectively cast as a search problem rather than a purely sequential generation process. By explicitly exploring the space of chain-of-thought paths and guiding expansion with a correctness\u2013cost heuristic, NCoTS identifies sparse, superior reasoning trajectories that are both more accurate and more concise. This approach breaks the perceived trade-off between detail and performance in CoT and offers a practical route to more capable and efficient reasoning systems."}}
{"id": "2601.11374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11374", "abs": "https://arxiv.org/abs/2601.11374", "authors": ["Furkan \u015eahinu\u00e7", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Reward Modeling for Scientific Writing Evaluation", "comment": "arXiv admin note: text overlap with arXiv:2508.07955", "summary": "Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.", "AI": {"tldr": "They propose open-source, cost-efficient reward models specifically for evaluating scientific writing, trained with a two-stage framework to handle varied tasks and criteria without per-task retraining.", "motivation": "Evaluating scientific text generation is difficult because it requires deep domain knowledge, task-specific understanding, and reasoning over complex, dynamic criteria. Existing LLM-based evaluators are tuned to generic benchmarks with fixed rubrics, struggle with sparse scientific knowledge, and are expensive to fine-tune for each new evaluation task, especially in low-resource settings.", "method": "They design open-source reward models tailored to scientific writing evaluation and train them with a two-stage framework: first, they optimize the models on scientific evaluation preference data; second, they refine the models\u2019 reasoning capabilities. The training is multi-aspect (covering diverse evaluation dimensions) and jointly conducted across multiple scientific writing tasks, so the models learn to handle different criteria and scoring rubrics in a unified way.", "result": "Experiments show that their training approach substantially improves the performance of LLM-based evaluators on scientific writing tasks. The resulting models can accurately and robustly assess texts under different, potentially changing evaluation criteria and rubrics.", "conclusion": "A single, cost-efficient, open-source reward model, trained with their two-stage, multi-aspect, multi-task framework, can generalize across varied scientific writing evaluation tasks and new settings, eliminating the need for task-specific retraining and making scientific text evaluation more scalable and robust."}}
{"id": "2601.11432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11432", "abs": "https://arxiv.org/abs/2601.11432", "authors": ["Gary Lupyan", "Blaise Ag\u00fcera y Arcas"], "title": "The unreasonable effectiveness of pattern matching", "comment": null, "summary": "We report on an astonishing ability of large language models (LLMs) to make sense of \"Jabberwocky\" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating \"He dwushed a ghanc zawk\" to \"He dragged a spare chair\". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to \"real\" intelligence, but rather a key ingredient.", "AI": {"tldr": "The paper shows that large language models can interpret sentences where real content words are replaced by nonsense words, suggesting strong reliance on structural patterns rather than memorized text.", "motivation": "There is an ongoing debate about what large language models are actually doing\u2014whether they just mimic surface text, act as databases of the web, or do something more. The authors want to test if LLMs can infer meaning even when actual content words are removed, which would challenge purely memorization-based or lookup-based explanations.", "method": "The authors create \"Jabberwocky\" sentences by randomly replacing most or all content words (nouns, verbs, adjectives, etc.) with invented nonsense strings while preserving the grammatical structure. They then prompt LLMs to interpret, translate, or paraphrase these sentences into meaningful English, and analyze how coherent and appropriate the outputs are relative to the underlying structure.", "result": "LLMs are able to generate coherent, semantically plausible interpretations of these Jabberwocky-style inputs\u2014for example, mapping \"He dwushed a ghanc zawk\" to \"He dragged a spare chair\"\u2014even though the key content words are nonsense and cannot be memorized from training data. The outputs reflect sensitivity to syntactic roles and relational structure rather than specific lexical items.", "conclusion": "The findings suggest that large language models rely heavily on sophisticated pattern-matching over linguistic structure to recover meaning, rather than operating merely as fuzzy databases or text mimics. Pattern-matching over form appears to be a central component of what we regard as intelligent behavior, not a trivial alternative to \"real\" intelligence."}}
{"id": "2601.11443", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11443", "abs": "https://arxiv.org/abs/2601.11443", "authors": ["Xin Sun", "Zhongqi Chen", "Qiang Liu", "Shu Wu", "Bowen Song", "Weiqiang Wang", "Zilei Wang", "Liang Wang"], "title": "Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.", "AI": {"tldr": "Proposes TTARAG, a test-time adaptation method for RAG that updates model parameters during inference to better handle specialized domains by learning to predict retrieved content, yielding strong gains over standard RAG baselines on six domains.", "motivation": "Standard Retrieval-Augmented Generation systems struggle in specialized domains because the data distribution differs from pretraining/fine-tuning, leading to poor generalization even with retrieval. There is a need for methods that can adapt RAG systems on the fly to domain-specific data without costly retraining or labeled data.", "method": "Introduce TTARAG, a test-time adaptation technique that, during inference, dynamically updates the language model parameters. The key idea is to train the model in situ to predict the retrieved knowledge snippets, using this self-supervised signal to adjust parameters toward the target domain. This adaptation is performed online at test time for each domain or example, without conventional supervised fine-tuning.", "result": "Across six specialized domains, TTARAG significantly outperforms baseline RAG systems in question-answering performance, demonstrating its effectiveness at mitigating domain shift. The paper reports substantial performance improvements, suggesting that test-time parameter adaptation via retrieval prediction is robust and practical.", "conclusion": "Test-time adaptation via predicting retrieved content is an effective and simple way to improve RAG performance under domain shift. TTARAG can be plugged into existing RAG pipelines to automatically tailor the language model to specialized domains at inference time, yielding consistent accuracy gains over standard RAG baselines."}}
{"id": "2601.11488", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11488", "abs": "https://arxiv.org/abs/2601.11488", "authors": ["Vanshali Sharma", "Andrea Mia Bejar", "Gorkem Durak", "Ulas Bagci"], "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation", "comment": "Accepted at ISBI 2026", "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 \"disagreement\" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.", "AI": {"tldr": "They propose CTest-Metric, a unified framework to systematically evaluate how good different automatic metrics are for CT radiology report generation, focusing on robustness, error sensitivity, and agreement with clinicians.", "motivation": "Radiology report generation is increasingly done with generative AI, but the field still relies on generic NLG metrics (like BLEU or ROUGE) that often fail to reflect actual clinical quality. Many domain-specific metrics have been proposed, yet there is no standard way to compare them or test whether they are robust, clinically meaningful, and reliable under realistic conditions (e.g., stylistic variation, different kinds of errors). This gap makes it hard to know which metrics to trust for model development and benchmarking in clinical practice.", "method": "They design CTest-Metric, a three-part evaluation framework for metrics in CT-based radiology report generation. (1) Writing Style Generalizability (WSG): they use LLM-based paraphrasing to change report writing style and test how sensitive each metric is to purely stylistic variations. (2) Synthetic Error Injection (SEI): they systematically introduce controlled factual errors at different severity levels into CT reports to see how well each metric responds to clinically important mistakes. (3) Metrics-vs-Expert correlation (MvE): they collect ratings from clinicians on 175 CT reports where there is disagreement, then compute correlations (e.g., Spearman) between metric scores and human expert judgments. They apply this framework to eight widely used metrics and seven LLM-based RRG models built around a CT-CLIP encoder.", "result": "Across their experiments, they find that lexical NLG metrics such as BLEU, ROUGE, and METEOR are very sensitive to harmless stylistic changes, indicating poor robustness to writing style. Among all metrics evaluated, GREEN Score shows the strongest alignment with clinician ratings, achieving a Spearman correlation of about 0.70, while CRG correlates negatively with experts, suggesting it may be misleading in clinical evaluation. BERTScore-F1 emerges as the most robust to factual error injection, showing minimal degradation in the presence of synthetic errors compared to other metrics. These results provide a comparative picture of which metrics are more clinically reliable and which ones are fragile or misleading.", "conclusion": "CTest-Metric offers a unified and reproducible way to stress-test and compare automatic metrics for CT radiology report generation along three clinically relevant axes: robustness to style, sensitivity to factual errors, and agreement with experts. Their findings challenge the use of standard lexical metrics and highlight GREEN Score and BERTScore-F1 as more clinically appropriate choices, while casting doubt on CRG. By releasing their framework, code, and a shareable subset of anonymized evaluation data, they aim to enable consistent benchmarking and guide the development of more clinically grounded RRG metrics."}}
{"id": "2601.11518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11518", "abs": "https://arxiv.org/abs/2601.11518", "authors": ["Jonathan Roberts", "Kai Han", "Samuel Albanie"], "title": "How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers", "comment": null, "summary": "Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.", "AI": {"tldr": "The paper empirically studies how different LLM tokenizers compress text into tokens and shows that common assumptions about token length and comparability are unreliable.", "motivation": "Tokens are treated as a universal, stable unit for comparing LLMs, estimating costs, and reasoning about inputs/outputs, but in reality different models and text domains may tokenize very differently, undermining these comparisons.", "method": "The authors perform a comprehensive empirical analysis of tokenization across multiple frontier LLMs and diverse textual domains, measuring how efficiently sequences of text are compressed into tokens and how token counts vary with tokenizer and data distribution.", "result": "They find large variability in tokenization behavior across models and text types and show that popular rules of thumb about average token length and cross-model comparability do not hold reliably in practice.", "conclusion": "Token counts should not be naively compared across models or domains; practitioners need to be aware of tokenizer- and domain-specific effects when estimating costs, capacities, and performance, and should use more nuanced assumptions about tokenization for contemporary LLMs."}}
