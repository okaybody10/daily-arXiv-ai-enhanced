<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.AI](#cs.AI) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: The paper studies how often language models really need long context, finds that most predictions can be made from a short local window, proposes a metric (DaMCL) to detect when long context truly matters, and uses this to adjust decoding and improve performance on long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Context windows are getting very long, but it is unclear how often models genuinely use long-range information vs mostly relying on the most recent tokens. This affects how we evaluate and improve long-context models and suggests there may be a bias toward short-context cues that hurts performance when long-range dependencies are important. The authors aim to quantify this "short-context dominance" and design methods to detect and correct it.

Method: 1) Use large language models as oracles to estimate, for each sequence, the minimum context length (MCL) needed so that predictions based on a truncated prefix match those based on the full context. Measure this across datasets with sequences of 1–7k tokens. 2) Since MCL as defined requires knowing the ground-truth next token and greedy decoding, propose a proxy metric called Distributionally Aware MCL (DaMCL) that compares predictive distributions rather than single tokens, and works with general sampling strategies. 3) Empirically evaluate DaMCL as a detector that classifies sequences as long-context vs short-context dependent via simple thresholding. 4) Design a decoding algorithm that uses this detector: when DaMCL indicates long-context relevance, selectively boost tokens that depend on long-range context, counteracting the short-context bias. 5) Test the impact of this decoding adjustment on Q&A tasks and different model architectures.

Result: 1) For sequences from long-context documents with lengths between 1k and 7k tokens, 75–80% of positions are well-predicted using at most the last 96 tokens, confirming strong short-context dominance. 2) The DaMCL metric, with a simple threshold, effectively distinguishes positions where long context is necessary from those where short context suffices. 3) The proposed decoding algorithm that upweights long-range-relevant tokens, guided by the DaMCL-based detector, improves performance on Q&A tasks across multiple LLM architectures.

Conclusion: Language models, even in long-context settings, mostly rely on short recent prefixes to make predictions, leading to a measurable short-context dominance. However, a smaller subset of tokens truly depends on long-range information and can be detected using the proposed DaMCL metric. By identifying these challenging long-context cases during decoding and boosting their probability, it is possible to mitigate the inherent short-context bias in LLMs and improve their performance on tasks that require long-range reasoning and retrieval.

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [2] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

TL;DR: They build a scalable, low-cost pipeline to specialize a general-purpose retrieval embedding model for finance, using only unlabeled filings and iterative student–teacher training, achieving large gains on financial IR benchmarks.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs and retrievers are expensive and often weak in specialized domains like finance, where precise retrieval quality is critical but labeled data is scarce. Existing embedding models help with latency and cost but underperform in domain-specific information retrieval. There is a need for a way to adapt general-purpose retrieval models to specialized domains without expensive human annotation.

Method: Starting from a general-purpose retrieval embedding model (bi-encoder), they build a student–teacher pipeline that uses LLM-based judgments as a teacher signal. They generate or obtain query–document candidates from an unlabeled corpus, then: (1) use the teacher to assess relevance and distill domain knowledge into the student retriever; (2) interleave retrieval and training—after each training round, the updated student model searches the unlabeled corpus to mine harder positive and negative examples; (3) iteratively retrain the student on these progressively more difficult examples. This creates a compact, domain-specialized retriever optimized for RAG scenarios, without labeled data.

Result: On 21,800 query–document pairs across 14 financial filing types, the specialized retriever improves mean reciprocal rank at 5 (MRR@5) by 27.7% and mean DCG@5 by 44.6% relative to the base model. On the FinanceBench benchmark, it improves NDCG on 3 out of 4 document classes, showing consistent gains in financial-domain retrieval performance.

Conclusion: An iterative student–teacher pipeline can effectively adapt a general-purpose retrieval embedding model to a specialized financial domain using only unlabeled data and LLM-judged relevance. By mining progressively harder positives and negatives and retraining in cycles, the method yields a compact, high-quality retriever for RAG, substantially improving financial information retrieval without human annotation and at lower computational cost than full LLM-based solutions.

Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [3] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: A universal framework (SEA) for aligning time-stamped subtitles with continuous sign language videos using pretrained segmentation and cross-modal embedding models plus efficient dynamic programming.


<details>
  <summary>Details</summary>
Motivation: Existing subtitle-to-sign-video alignment methods are end-to-end, language/dataset-specific, and hard to generalize across different sign and spoken languages. There is a need for a flexible, scalable, and language-agnostic approach that can align subtitles with sign language videos to create high-quality parallel data for sign language processing.

Method: The SEA (Segment, Embed, and Align) pipeline first uses a pretrained model to segment continuous sign language video into individual sign clips. A second pretrained model then embeds each video clip into a shared latent representation space with text. Finally, a lightweight dynamic programming algorithm aligns the sequence of sign embeddings with the subtitle text, enabling efficient CPU-only processing even for hour-long videos. The framework can exploit different resources, ranging from small lexicons to large continuous corpora.

Result: On four sign language datasets, SEA achieves state-of-the-art performance on the subtitle-to-video alignment task, outperforming prior specialized, end-to-end systems. It also runs efficiently, aligning hour-long episodes on CPU within about a minute.

Conclusion: SEA offers a universal, flexible, and efficient framework for aligning subtitles with continuous sign language videos across languages and domains. Its strong empirical results and open-source release position it as a key tool for generating parallel sign-text data and advancing sign language processing research.

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [4] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: The paper introduces universal adversarial suffixes—short token sequences that, when appended to any input, significantly reduce classification accuracy of language models across multiple tasks and model families.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial prompt attacks on language models are often tailored to specific tasks or models, hindering comparability and transferability. There is a need for a more universal, model- and task-agnostic way to degrade LM performance as zero/few-shot classifiers, and to better understand their robustness to input perturbations.

Method: The authors learn short adversarial suffixes (4–10 tokens) that can be appended to any input. They optimize these in a continuous, differentiable form using Gumbel-Softmax relaxation, then discretize them for test-time use. Training maximizes calibrated cross-entropy on the label prediction region, masks out gold label tokens to avoid trivial leakage-based attacks, and includes entropy regularization to prevent the learned suffix from collapsing to degenerate patterns.

Result: A single learned suffix, trained on one base LM, substantially reduces both accuracy and calibrated confidence across diverse classification tasks and generalizes to different model architectures and sizes, including Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B. The attack demonstrates strong transfer both across tasks (sentiment, NLI, paraphrase, commonsense QA, physical reasoning) and across model families.

Conclusion: Universal adversarial suffixes are a powerful and transferable attack on LM-based classifiers, revealing a systemic vulnerability: short, learned token strings can broadly disrupt model predictions across tasks and architectures. This highlights the need for more robust training or defense strategies against input-agnostic prompt attacks.

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [5] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: The paper proposes a reinforcement-learning-based method to learn short adversarial suffixes that reliably degrade language model performance and transfer across models and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial suffixes for language models are usually created with gradient-based or rule-based approaches that are brittle, model-specific, or task-specific, and often do not transfer well to other setups. There is a need for a more general, robust way to learn adversarial triggers that reliably manipulate model predictions across a variety of models and NLP tasks.

Method: The authors cast the adversarial suffix as a policy in a reinforcement learning framework. They freeze the target language model and treat it as a reward oracle, then train the suffix using Proximal Policy Optimization (PPO). The reward is shaped via calibrated cross-entropy to remove label bias and aggregate over multiple surface forms, with the goal of producing suffixes that generalize better across tasks and models. The approach is evaluated on five NLP benchmarks spanning sentiment analysis, natural language inference, paraphrase detection, and commonsense reasoning, using three different LMs (Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, Phi-1.5).

Result: RL-trained adversarial suffixes significantly and consistently reduce model accuracy on all evaluated datasets. Compared to prior adversarial triggers of similar length and style, these learned suffixes transfer better across different tasks and language models, showing higher effectiveness even when applied zero-shot to unseen settings.

Conclusion: Treating adversarial suffix generation as an RL problem with PPO and calibrated cross-entropy rewards yields compact triggers that are both highly effective and more transferable across models and tasks than previous approaches. This suggests that reinforcement learning is a powerful tool for discovering robust adversarial prompts against language models.

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [6] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub is a search-focused platform that links and structures data from ClinicalTrials.gov and PubMed, boosting accessible structured clinical trial data by 83.8% and providing evidence-grounded QA for diverse stakeholders.


<details>
  <summary>Details</summary>
Motivation: Access to comprehensive, structured clinical trial information is fragmented: ClinicalTrials.gov is incomplete, and much relevant detail is buried in unstructured research articles on PubMed. This makes it hard for patients, clinicians, researchers, and policymakers to efficiently find and interpret evidence needed for evidence-based medicine. The authors aim to close this gap by unifying and enriching trial data and making it queryable in an intuitive way.

Method: The authors build ClinicalTrialsHub, which (1) ingests all data from ClinicalTrials.gov, (2) automatically parses full-text articles from PubMed using large language models (e.g., GPT-5.1 and Gemini-3-Pro) to extract structured trial-relevant fields, (3) translates natural-language user questions into structured database queries, and (4) offers an attributed question-answering interface that generates answers grounded in specific source sentences. They evaluate the system via a user study with domain experts and students and via automatic benchmarks of information extraction and QA performance.

Result: ClinicalTrialsHub consolidates registry and literature data and increases access to structured trial information by 83.8% compared to using only ClinicalTrials.gov. The system is shown, through user studies and automatic evaluation, to effectively extract trial data and support accurate, evidence-backed question answering for clinical trial queries.

Conclusion: By integrating registry and literature data and layering LLM-based extraction and QA on top, ClinicalTrialsHub substantially expands and simplifies access to structured clinical trial evidence. This has the potential to better support evidence-based decision making for multiple stakeholder groups and demonstrates that large language models can be safely and usefully applied to augment clinical trial information systems.

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [7] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: The paper assesses how biased Generative Large Language Models (GLLMs) are when used as annotators, by comparing them to human/manual annotations.


<details>
  <summary>Details</summary>
Motivation: As GLLMs are increasingly used to label or code text data for research, it is crucial to know whether their annotations match human coders not just in accuracy metrics like F1, but also in prevalence and downstream substantive conclusions, and whether they introduce systematic biases.

Method: The authors conceptually replicate Boukes (2024) by re‑doing a manual annotation task with several GLLMs (Llama3.1:8B, Llama3.3:70B, GPT‑4o, Qwen2.5:72B). They test five prompts across five annotation concepts (political content, interactivity, rationality, incivility, ideology), then compare GLLM outputs to manual annotations using F1, prevalence, overlap patterns, and impact on downstream analyses.

Result: GLLMs reach seemingly adequate F1 scores relative to human annotations, but they systematically differ in how often they assign categories (prevalence), produce meaningfully different downstream analytical results, and agree more strongly with each other than with manual coders. Moreover, variations in F1 scores do not explain or predict how biased the models’ annotations are.

Conclusion: Even when GLLMs achieve good F1 scores, they can still be systematically biased as annotators: they change prevalence, alter downstream findings, and converge on shared model-based patterns rather than human judgments. Standard metrics like F1 are insufficient to diagnose or quantify this bias, implying researchers should be cautious and use richer validation when deploying GLLMs for annotation.

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [8] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: The paper uses interpretability techniques to understand which source words trigger gendered translations in MT, connecting model attributions to human gender perceptions to better address gender bias.


<details>
  <summary>Details</summary>
Motivation: Existing work on gender bias in MT and LLMs mostly measures bias but does not explain where it originates in the input. There is a need to understand which parts of the source sentence influence gendered decisions in the target language, in order to better diagnose and mitigate bias.

Method: The authors use gender-ambiguous natural source sentences and analyze how a translation model chooses gendered inflections in the target language. They apply contrastive explanations and saliency attribution techniques to identify which source tokens most influence the model’s gender choice. They systematically explore attribution levels, address the lack of a clear saliency scoring threshold, compare model-salient tokens with human judgments of gender cues, and conduct a linguistic analysis of the salient words.

Result: They find that source tokens with high saliency attribution for gender decisions overlap substantially with what humans perceive as gender-relevant cues. They also identify linguistically meaningful categories of salient words that correlate with the model’s gender choices, and clarify how different attribution thresholds affect the interpretation of influential tokens.

Conclusion: Model gender decisions in MT are systematically linked to specific source-word cues that often align with human gender perceptions. Understanding these attribution patterns is crucial for interpreting gendered translations and should be used as a basis for designing more effective methods to mitigate gender bias in translation models.

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [9] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: The paper proposes a reasoning-guided fine-tuning approach for a Korean large language model to detect inappropriate utterances, achieving higher accuracy than standard supervised learning.


<details>
  <summary>Details</summary>
Motivation: Online games and anonymous communities often experience escalation from minor inappropriate remarks to verbal abuse and even criminal behavior; this creates social concern and motivates the need for reliable automatic detection of inappropriate utterances in conversational text, particularly for Korean language scenarios where such tools are underdeveloped.

Method: The authors introduce a soft inductive bias method that explicitly defines reasoning perspectives to guide the language model’s inference process. They fine-tune a Korean large language model (Kanana-1.5) using these constrained reasoning perspectives, compare it against standard supervised learning and other training strategies, and evaluate performance both quantitatively and qualitatively.

Result: Using the proposed soft inductive bias and reasoning-perspective guidance, the fine-tuned Kanana-1.5 model attains an average accuracy of 87.0046 on inappropriate utterance detection, about a 3.89% improvement over standard supervised learning baselines, with qualitative analysis indicating more precise and consistent judgments.

Conclusion: Explicitly incorporating reasoning perspectives as a soft inductive bias during fine-tuning helps Korean large language models move beyond mere pattern or knowledge imitation, yielding more rational, accurate, and consistent decisions for inappropriate utterance detection and demonstrating the method’s effectiveness for building safer communication systems.

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [10] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: Proposes a hierarchical 64×64-grid multi-agent LLM system with a spatial, bandit-driven curriculum and NLL-based confidence to improve long-horizon reasoning on a spatial Tower of Hanoi benchmark while reducing oracle use.


<details>
  <summary>Details</summary>
Motivation: Existing LLM and multi-agent approaches can decompose complex tasks, but they break down on very long-horizon reasoning and become prohibitively expensive, especially in spatial or robotic-style problems. The authors aim to design an architecture that scales to long sequences of decisions while keeping computation and external oracle calls manageable, and that can be trained in a more sample-efficient and reliable way.

Method: They introduce a hierarchical multi-agent architecture where many lightweight agents are arranged on a 64×64 spatial grid. A selective oracle can be queried when agents are uncertain. Training follows a spatial curriculum: only a central region of the grid is trained at first, and the active area is gradually expanded outward so agents master simpler central tasks before peripheral, harder ones. The system estimates each agent’s confidence using Negative Log-Likelihood (NLL) of its outputs, and this feeds into a Thompson Sampling curriculum manager that chooses which spatial regions to train next. Rewards for the bandit are based on both task competence and NLL-based calibration. The method is evaluated on a spatial Tower of Hanoi environment simulating long-horizon planning like robotic manipulation.

Result: On the spatial Tower of Hanoi benchmark, the proposed architecture shows more stable training, better long-range reasoning performance, and lower reliance on the oracle compared to baselines. The distributed agents cooperate effectively over long horizons, and the NLL- plus bandit-based curriculum focuses training on regions where learning progress and calibration gains are highest.

Conclusion: Distributing reasoning across a large grid of lightweight agents, combined with a spatial curriculum managed via Thompson Sampling and NLL-based confidence, can significantly improve long-horizon reasoning while controlling computation and oracle usage. The spatial Tower of Hanoi results suggest that such hierarchical multi-agent schemes are promising for complex planning and robotic-style tasks that require deep, structured reasoning over extended horizons.

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [11] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: Tutorial on Healthcare NLP: overview of key sub-areas, methods, resources, and future challenges, with hands‑on practice for a broad audience.


<details>
  <summary>Details</summary>
Motivation: Healthcare NLP has advanced quickly, but existing surveys are incomplete: they miss emerging tasks like synthetic data generation for privacy, explainable clinical NLP for real-world deployment, and new methodologies such as retrieval‑augmented generation and neural‑symbolic integration of large language models with knowledge graphs. The authors want to fill this gap with a structured, accessible overview tailored to patient and resource needs.

Method: Design a structured tutorial that organizes HealthcareNLP into three hierarchical layers—(1) data/resource layer: annotation guidelines, ethics and governance, and synthetic data; (2) NLP-Eval layer: core clinical NLP tasks (NER, relation extraction, sentiment, coding/linking) and methods, emphasizing explainability; (3) patients layer: tasks around patient engagement, health literacy, translation, simplification, summarization, and support for shared decision making. The tutorial combines conceptual coverage with a hands-on session where participants experiment with HealthcareNLP applications. Materials are shared via a public GitHub repository.

Result: As this is a proposed tutorial rather than an empirical study, there are no experimental results. The main outcome is a well-scoped tutorial curriculum, a three-layer conceptual framework for HealthcareNLP, and accompanying practical materials and demos intended for diverse audiences, from NLP researchers to healthcare practitioners and students.

Conclusion: The tutorial will provide an introductory, yet comprehensive, entry point into modern HealthcareNLP, emphasizing patient-centric and resource-aware perspectives, emerging tasks (e.g., synthetic data, explainability), and new methods (e.g., retrieval‑augmented generation, neural‑symbolic LLM+KG integration). It aims to lower the barrier to entry for non-experts, support responsible and explainable clinical NLP, and highlight open challenges and future research directions.

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [12] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN is an open-source Python framework and UI for running large in-silico surveys with LLMs, enabling systematic evaluation of how questionnaire design and response-generation methods affect alignment with human survey answers.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for in-silico surveys and annotation, but results depend heavily on how questions are presented and how responses are generated. Current practices are often ad hoc, hard to reproduce, and expensive in compute, limiting reliability and comparability of LLM-based survey research.

Method: The authors build QSTN, a Python framework plus a no-code interface for configuring questionnaire-style prompts, varying question structure and prompt perturbations, and plugging in different LLM response-generation strategies. They then run a large-scale empirical study (>40M generated survey responses) to measure how these design choices affect alignment between LLM-generated answers and human survey data, and to compare compute costs across methods.

Result: Empirical evaluation with more than 40 million generated survey responses shows that both the structure of questions and the chosen response-generation methods substantially influence how closely LLM outputs match human survey answers. Some configurations achieve similar or better alignment at significantly lower compute cost than naive setups.

Conclusion: QSTN provides a systematic, reproducible way to design, run, and analyze LLM-based in-silico surveys and annotation tasks. By revealing how questionnaire and generation choices affect alignment with human data and by offering both code and no-code tools, it aims to improve the reliability, efficiency, and accessibility of LLM-based research workflows.

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [13] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: The paper introduces MOSAIC, a multi-agent AI system that automatically annotates clinical conversations across multiple communication frameworks with high accuracy, reducing the need for labor-intensive human coding.


<details>
  <summary>Details</summary>
Motivation: Clinical communication quality is crucial for patient outcomes, but manually annotating patient-provider conversations is slow, costly, inconsistent, and hard to scale. Existing LLM-based annotators are usually single-task, not easily adaptable to different codebooks or clinical domains, and lack interpretability and reliability. The authors aim to create a scalable, flexible, and more trustworthy AI system to support standardized coding of clinical interactions across multiple frameworks and specialties.

Method: The authors design MOSAIC, a LangGraph-based multi-agent system composed of: (1) a Plan Agent that selects the appropriate communication codebook and plans the annotation workflow; (2) an Update Agent that keeps retrieval databases current; (3) multiple Annotation Agents that use codebook-guided retrieval-augmented generation and dynamic few-shot prompting to label the transcripts; and (4) a Verification Agent that performs consistency checks and feedback-based refinement. They train/develop the system on 26 gold-standard annotated clinical transcripts and test it on 50 transcripts spanning rheumatology and OB/GYN. Performance is evaluated by comparing MOSAIC’s labels with trained human coders’ annotations using F1 scores and ablation studies against baselines.

Result: On the test set of 50 transcripts, MOSAIC attains an overall F1 score of 0.928. Performance is particularly strong in the rheumatology subset (F1 = 0.962) and on codes related to Patient Behavior (e.g., questions, preference expression, assertiveness). Ablation experiments show that MOSAIC’s full multi-agent, RAG-based design outperforms baseline models and simpler configurations.

Conclusion: MOSAIC demonstrates that a multi-agent, retrieval-augmented AI architecture can provide accurate, scalable, and adaptable annotation of clinical conversations across domains and communication frameworks. Its strong performance versus human coders and against baselines suggests it can substantially reduce manual annotation burden while preserving or improving reliability, especially for patient behavior coding. This framework may generalize to other clinical domains and codebooks, enabling broader, more systematic study of clinical communication.

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [14] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: First Basque C1 essay-scoring and feedback dataset; fine-tuned open models (RoBERTa-EusCrawl, Latxa) beat major proprietary systems in scoring and feedback quality, with a new evaluation method for feedback.


<details>
  <summary>Details</summary>
Motivation: There is no public dataset or transparent benchmark for automatic essay scoring and pedagogical feedback generation in Basque or similar low-resource languages, especially at advanced CEFR levels. This gap limits research progress, reproducibility, and the development of educationally useful NLP tools for Basque learners. The authors aim to provide both a resource and strong open baselines to catalyze research and reduce dependence on closed, proprietary systems.

Method: 1) Compile a dataset of 3,200 Basque essays (CEFR C1) from HABE, each annotated by expert raters with criterion-specific scores (correctness, richness, coherence, cohesion, task alignment) plus detailed feedback and error examples. 2) Fine-tune open-source encoder models (e.g., RoBERTa-EusCrawl) for AES and decoder/LLM models (Latxa 8B/70B) for both scoring and explanation/feedback generation. 3) Compare these fine-tuned open models against state-of-the-art closed-source systems (e.g., GPT-5, Claude Sonnet 4.5) on scoring consistency and feedback quality. 4) Propose and apply a new evaluation methodology for feedback generation that blends automatic consistency metrics with expert validation of extracted learner errors, focusing on pedagogical quality and criterion alignment.

Result: 1) Encoder-based models remain very strong and reliable for numeric automatic essay scoring in Basque. 2) Supervised fine-tuning of the Latxa LLMs leads to large gains in both scoring and feedback tasks. 3) The best Latxa models outperform closed-source SoTA models (GPT-5, Claude Sonnet 4.5) on scoring consistency and on the perceived quality and usefulness of feedback. 4) The proposed evaluation framework successfully measures both consistency and pedagogical adequacy of feedback and shows that Latxa identifies a broader variety of learner error types than proprietary models.

Conclusion: The authors provide the first public Basque C1 essay dataset with rich annotations and feedback, plus strong open baselines, enabling transparent and reproducible AES and feedback research for a low-resource language. Fine-tuned open models, especially Latxa, can surpass leading proprietary systems in both scoring and pedagogical feedback quality. The novel evaluation methodology offers a more educationally grounded way to assess feedback generation. Overall, the work establishes a solid foundation and benchmark for future NLP and CALL research in Basque and potentially other low-resource languages.

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [15] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: The paper proposes an on-policy post-training method to align language models for low-resource languages while preserving fluency, even when reward models are disfluent, and shows it outperforms common translation-based and multilingual finetuning baselines without needing target-language instruction data.


<details>
  <summary>Details</summary>
Motivation: Most preference-optimization and alignment work has focused on high-resource languages like English and Chinese. Low-resource languages often lack high-quality native-speaker datasets and fluent LMs for synthetic data, making it difficult to build preference-aligned, fluent models. Moreover, reward models used for alignment can themselves be disfluent in low-resource languages, risking degradation of fluency during alignment. The paper aims to bridge this gap by enabling fluent, preference-aligned models for low-resource languages without relying on scarce, target-language instruction-tuning data.

Method: They develop an on-policy post-training method for preference alignment in low-resource languages. Instead of relying on existing instruction-tuning data in the target language, they train by generating data with the model itself (on-policy) and using a reward model to guide preference optimization, even when that reward model may be disfluent. They compare this approach against two strong but common baselines: (1) supervised finetuning on machine-translated instruction data from high-resource languages and (2) multilingual finetuning that jointly trains on multiple languages. The case study focuses on Norwegian Bokmål, where they systematically apply their on-policy training and evaluate its impact.

Result: In the Norwegian Bokmål case study, their on-policy method produces a preference-aligned language model that preserves or improves fluency despite using a potentially disfluent reward model. According to evaluations based on native-speaker judgments, the on-policy approach outperforms the supervised finetuning on machine-translated data and the multilingual finetuning baselines. It achieves better fluency without relying on expensive or hard-to-obtain target-language instruction-tuning datasets.

Conclusion: On-policy preference optimization is particularly effective for low-resource languages, as it can maintain or improve fluency even when alignment uses imperfect, disfluent reward models. The proposed method surpasses common alternatives like machine-translation-based SFT and multilingual finetuning in a Norwegian Bokmål case study, all without requiring native, instruction-tuned data in the target language. This suggests that similar on-policy strategies could be a practical and scalable path for aligning models in many other low-resource languages.

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [16] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: The paper proposes and evaluates aggregation methods for human preference signals in federated RLHF for LLMs, highlighting a new adaptive aggregation scheme that improves fairness while keeping alignment quality high.


<details>
  <summary>Details</summary>
Motivation: Standard RLHF alignment of LLMs usually assumes centralized data and a relatively homogeneous preference distribution, which does not hold in federated learning scenarios with diverse user groups. Existing aggregation rules (min, max, mean of rewards) can overrepresent or underrepresent certain groups, hurting fairness or alignment. There is a need for a principled way to evaluate and manage the trade-off between how well the model is aligned and how fairly it treats different groups’ preferences in a federated setup.

Method: Set up a federated RLHF environment where multiple groups locally evaluate LLM rollouts on Q/A tasks and convert them into reward signals. The central server never sees raw data; it only aggregates group-level rewards. Compare several aggregation rules: standard ones (min, max, average) and a new adaptive aggregation strategy that dynamically reweights each group’s preferences according to its historical alignment performance. Use a PPO-based RLHF pipeline to train the LLM under each aggregation scheme, and define a framework with metrics to jointly measure alignment quality and fairness across groups.

Result: Across experiments on Q/A tasks, the adaptive aggregation scheme achieves better fairness metrics across groups than basic min/max/average aggregation, while retaining alignment performance (overall reward/quality) comparable to the best baselines. The analysis quantifies how different aggregation strategies change the alignment–fairness trade-off in federated RLHF.

Conclusion: Careful design of reward aggregation in federated RLHF is crucial for producing LLMs that are both well-aligned and fair across heterogeneous user groups. The proposed evaluation framework and adaptive aggregation method provide a practical way to balance alignment quality and fairness, enabling more pluralistic alignment of LLMs without violating federated privacy constraints.

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [17] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: The paper explains why gradually increasing Transformer depth during training improves reasoning and efficiency, by showing it fixes poor depth utilization (the Curse of Depth) and creates reusable computational blocks.


<details>
  <summary>Details</summary>
Motivation: Prior work (e.g., MIDAS) showed that gradually growing Transformer depth during training both reduces training cost and improves reasoning, but there was no mechanistic explanation for these benefits. At the same time, analyses of standard pre-layernorm Transformers revealed a Curse of Depth: later layers contribute much less to the final output than early ones, implying poor depth utilization. This paper aims to connect these two lines of work and understand how depth growth changes the internal computation of Transformers.

Method: The authors perform depth-wise mechanistic analyses of Transformers trained with gradual middle stacking (a specific scheme for growing depth used in MIDAS). They compare grown vs. non-grown pre-layernorm Transformers, examining how much each layer contributes to the final output, how the residual stream structure changes with growth, and how computational blocks emerge and become permutable. They also design and test a lightweight modification of the MIDAS training procedure and evaluate it on downstream reasoning benchmarks.

Result: They find that gradual middle stacking leads to more effective use of depth: layers across the network contribute more evenly to the final output, mitigating the Curse of Depth observed in standard models. Growth alters the residual stream, enabling the network to form distinct, roughly interchangeable computational blocks rather than over-relying on early layers. Their modified MIDAS procedure yields additional gains on reasoning benchmarks beyond the original method.

Conclusion: Gradually growing Transformer depth during training does more than save compute; it changes the model’s internal computation. Specifically, it overcomes the limited depth utilization characteristic of standard pre-layernorm Transformers, restructures the residual stream, and enables the emergence of permutable computational circuits. These insights both mechanistically explain MIDAS’s benefits and motivate simple improvements that further boost reasoning performance.

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [18] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: The paper introduces RAGLens, a lightweight, interpretable detector of unfaithful (hallucinated) outputs in Retrieval-Augmented Generation systems using internal LLM representations.


<details>
  <summary>Details</summary>
Motivation: RAG improves factuality by grounding LLM outputs in retrieved documents, but models still generate unfaithful content that contradicts or goes beyond the evidence. Existing hallucination detectors either require expensive training data, rely on costly external LLM judges, or underperform when using internal activations. There is a need for an accurate, low-cost, and interpretable detector tailored to RAG hallucinations that exploits mechanistic interpretability advances.

Method: The authors apply sparse autoencoders (SAEs) to the internal activations of an LLM used in a RAG setup to disentangle and identify features associated with hallucination behavior. They design a pipeline that combines information-based feature selection with additive feature modeling to build RAGLens, a classifier that takes these disentangled features as input to decide whether a RAG output is faithful to the retrieved context. The system also provides feature-level rationales explaining why an output is flagged as hallucinated.

Result: RAGLens outperforms prior hallucination detection methods for RAG in accuracy while being lightweight (no large external judge calls and no massive supervised training). It reliably detects unfaithful generations from internal representations and produces interpretable attributions linking specific SAE features to hallucination behavior, enabling targeted mitigation strategies.

Conclusion: By leveraging SAEs and an information-driven feature modeling pipeline, RAGLens demonstrates that hallucination-related signals are systematically embedded in LLM internal activations and can be extracted for effective detection. The method yields a practical, accurate, and interpretable hallucination detector for RAG systems and offers new insights into how and where hallucination signals are distributed within LLMs.

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: The paper empirically compares Data Oriented Design (DOD) and Object-Oriented Design (OOD) using several A* implementations, showing DOD yields better cache behavior and execution time, especially in multi-threaded settings, while threading overhead can negate benefits for fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: As CPUs gain cores faster than memory latency improves, memory and cache behavior increasingly dominate performance. Traditional OOD often leads to poor data locality, especially in parallel workloads. The paper is motivated by the need to quantify, on real workloads, how DOD versus OOD affects cache efficiency and overall performance, and to understand these effects in both single- and multi-threaded contexts using a well-known algorithm (A*).

Method: The authors implement four variants of the A* search algorithm: ST-OOD, ST-DOD, MT-OOD, and MT-DOD. They run controlled experiments measuring execution time, memory usage, cache misses, and raw system calls under each variant. The focus is on observing how data layout and design paradigm affect cache utilization and performance, and how these effects differ between single-threaded and multi-threaded execution.

Result: DOD outperforms OOD in multi-threaded scenarios, showing faster execution, fewer cache misses, and fewer raw system calls. In some cases, OOD uses slightly less memory or shows marginally better percentage-based cache miss rates, but these do not offset DOD’s advantages on key time- and cache-related metrics. For the fine-grained A* workload, all multi-threaded versions are slower than their single-threaded counterparts due to thread management overhead dominating any parallelism benefits.

Conclusion: The study concludes that DOD consistently offers architectural and performance advantages over OOD in data-intensive operations, particularly in multi-threaded environments, even when raw performance differences for simple algorithms may appear modest. It also warns that naïve multi-threading of fine-grained tasks like A* can hurt performance due to overheads. Overall, DOD is argued to be a more suitable paradigm for exploiting modern hardware in large-scale AI and parallel computing applications.

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [20] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: The paper proposes a shift from partially AI-assisted data management to fully autonomous, agent-driven data estates that manage the complete data lifecycle.


<details>
  <summary>Details</summary>
Motivation: Enterprise data management is complex, large-scale, and spans many interconnected activities (architecture, integration, quality, governance, continuous improvement). Existing AI assistants support specific personas and narrow tasks but cannot deliver end‑to‑end automation. As AI capabilities improve, there is an opportunity to reduce human burden and improve scalability and robustness by aiming for fully autonomous data estates.

Method: The authors conceptually analyze the modern data stack and reframe it as a sequence of stages that could be managed by intelligent agents. They discuss how agents could autonomously handle each stage of the data lifecycle and propose a paradigm shift: from isolated AI tools embedded in components to holistic, agent-based orchestration of the entire data estate. They also identify challenges and research questions rather than presenting a concrete implementation.

Result: The paper does not present empirical results or a deployed system. Instead, it offers a conceptual framework and argues that every stage of the modern data stack can, in principle, be autonomously managed by intelligent agents. It outlines how such agents could streamline the data lifecycle and identifies key open problems to be addressed by the research community.

Conclusion: A more autonomous, agent-driven future for data systems is both desirable and increasingly feasible. Moving from siloed AI applications to holistic autonomous data estates could dramatically change how organizations manage data, benefiting both human users and AI systems that consume the data. The authors call for community discussion, further research, and collaboration to realize this paradigm shift and to resolve the open technical and organizational challenges.

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [21] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV is a training-free method that compresses the KV cache of large reasoning models by evicting and steering at the sentence level, preserving accuracy while reducing memory and generation length.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models incur substantial KV cache cost because chain-of-thought reasoning generates long sequences whose KV states grow linearly, causing memory and throughput bottlenecks. Existing token-wise KV eviction strategies behave poorly for CoT in realistic multi-batch settings: their scores are unstable token by token, padding reduces effective KV budget, accuracy degrades, and they often make the model produce even longer outputs due to semantically unaware eviction that forces repeated reasoning. A better approach is needed that is robust in multi-batch CoT settings, preserves accuracy, and actively prevents redundant generation.

Method: SkipKV is a training-free KV compression technique that works at a coarse-grained sentence level instead of token level. It introduces a sentence-scoring metric to detect and remove highly similar or redundant sentences from the KV cache while preserving semantic coherence of the remaining context. In addition to selective eviction, it adjusts a steering vector during inference to modify hidden activations so that the model is nudged toward concise reasoning and final answers, thereby reducing redundant or repetitive generation. Both eviction and generation control are applied online during CoT inference without retraining the base model.

Result: On multiple reasoning benchmarks, SkipKV maintains substantially better accuracy under similar KV compression budgets, achieving up to 26.7% higher accuracy than existing KV eviction baselines. It also shortens generated sequences, producing up to 1.6× fewer tokens and improving inference throughput by up to 1.7× compared with state-of-the-art KV compression/eviction methods.

Conclusion: Sentence-level, training-free KV cache compression combined with generation steering offers a more effective way to deploy large reasoning models under memory and throughput constraints. By focusing on semantically redundant sentences and discouraging verbose reasoning, SkipKV preserves or improves reasoning accuracy while significantly reducing KV cache size, generation length, and inference latency compared to token-wise state-of-the-art methods, particularly in multi-batch chain-of-thought scenarios.

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [22] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: The paper proposes a secure, scalable proof-of-concept system that uses reasoning-enabled large language models to assist with matching patients to clinical trials, producing interpretable, auditable eligibility assessments rather than simple yes/no decisions.


<details>
  <summary>Details</summary>
Motivation: Manual screening of patients for clinical trial eligibility is slow, labor-intensive, and costly, limiting trial enrollment efficiency and potentially excluding eligible patients. Existing automated or semi-automated methods tend to focus on narrow, binary classification and struggle with heterogeneous EHR data, lack of transparency, and security concerns. There is a need for an AI system that can integrate diverse EHR sources, provide interpretable reasoning, support expert oversight, and adhere to strict privacy and security requirements.

Method: The authors design a proof-of-concept decision-support system that integrates heterogeneous EHR data and applies open-source, reasoning-enabled large language models. Instead of simply labeling a patient as eligible or ineligible, the LLM generates structured eligibility assessments, including explicit reasoning chains tied to trial criteria. The system encodes eligibility as a dynamic state, can suggest trials that might fit now or in the future, and is architected to support secure data handling, expert review, and full audit trails of AI outputs.

Result: The system is demonstrated as a scalable and secure proof-of-concept that can: (1) ingest and harmonize diverse EHR data for trial screening; (2) produce structured, interpretable eligibility assessments powered by LLM reasoning; (3) facilitate human-in-the-loop expert review; and (4) maintain detailed audit logs of AI-generated outputs. It shows the feasibility of using open-source LLMs to augment, rather than replace, clinical trial coordinators in the matching process.

Conclusion: AI-augmented patient-trial matching using reasoning-enabled LLMs is feasible and can transform eligibility assessment from a static yes/no judgment into a dynamic, auditable decision-support process. By integrating heterogeneous EHR data, preserving security, and keeping experts in the loop, such systems can reduce coordinator workload, expand the range of trials considered for each patient, and identify actionable steps that could make patients eligible in the future.

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [23] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: The paper compares ChatGPT and DeepSeek for education and research tasks, examining accuracy, efficiency, and user experience, and finds complementary strengths: ChatGPT in general language tasks and DeepSeek in programming, with both strong in math and medical problem-solving.


<details>
  <summary>Details</summary>
Motivation: To understand how leading LLMs can best support education and research, there is a need to systematically compare their technical characteristics, task performance, and user perceptions, rather than relying on anecdotal impressions.

Method: The authors conduct a background technology analysis of ChatGPT and DeepSeek, run empirical benchmarks in text generation, programming, and specialized domains (math and medicine), and perform a user survey of students, educators, and researchers about their experiences using the models.

Result: ChatGPT outperforms in general language understanding and text generation quality; DeepSeek performs better on programming tasks, attributed to its efficiency-oriented design. Both models generate medically accurate diagnostic suggestions and handle complex math problems effectively. User survey responses validate these quantitative findings and surface practical pros and cons in real educational and research workflows.

Conclusion: ChatGPT and DeepSeek each have distinctive strengths that can be leveraged in complementary ways for educational and research applications: ChatGPT as a strong generalist for language-heavy tasks, and DeepSeek as a more efficient option for programming-intensive work. Both are promising tools for advancing learning and research, though users must be aware of their respective limitations.

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [24] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: The paper designs and evaluates a scalable, low-latency back-end architecture for a mobile diabetes prediction app, meeting strict performance and reliability targets under high concurrency.


<details>
  <summary>Details</summary>
Motivation: With diabetes cases increasing worldwide, there is a need for early prediction tools that are accessible to many users via mobile apps. However, AI-based prediction requires significant computation and data handling, so the back-end must scale to large, concurrent user bases while ensuring low latency and minimal failures. Existing solutions often focus on models, not on robust, production-ready back-end architectures for such health applications.

Method: The authors design a distributed back-end architecture using horizontal scaling of application servers, database sharding to distribute data and reduce contention, and asynchronous communication via a message queue (RabbitMQ) for computationally heavy prediction tasks. They define performance targets—failure rate <5% and average latency <1000 ms—then implement the system and run performance tests under increasing concurrent user loads, measuring latency, error rates, and feature-level performance for key functionalities such as user management, activity tracking, and prediction requests.

Result: Performance tests show that 20 of 24 system features (83%) satisfy the target of <5% failure rate and <1000 ms average latency. High-traffic, read-intensive operations like user profile management, activity tracking, and prediction retrieval meet the goals. The system handles up to 10,000 concurrent users without instability. Asynchronous processing via RabbitMQ significantly reduces errors for compute-intensive prediction operations by queuing requests and preventing loss or failure under heavy load.

Conclusion: The proposed back-end architecture is effective and scalable for a mobile diabetes prediction application, achieving strong reliability and latency performance for most features and supporting up to 10,000 concurrent users. Asynchronous messaging and horizontal scaling are key contributors to robustness, especially for computationally heavy AI prediction requests. The paper suggests that similar architectures can support other large-scale, health-related AI applications with strict performance requirements.

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [25] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: The paper proposes empowerment—an intrinsic motivation signal based on action–outcome mutual information—as a computational bridge between Bayesian causal learning and reinforcement learning, and empirically tests whether children and adults use empowerment cues to infer and intervene on causal structures.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning models struggle with learning explicit causal structure, whereas human learners, especially children, are remarkably good at discovering causal relations. Cognitive science has formal tools (Causal Bayes Nets) that describe human causal reasoning, while reinforcement learning has developed intrinsic motivation signals like empowerment that promote exploratory behavior. The authors are motivated to connect these two traditions to better understand human causal learning and to design machine agents that can learn causal models more effectively.

Method: Conceptually, the paper analyzes empowerment—defined as the mutual information between actions and resulting states—as a signal that both depends on and promotes accurate causal world models. It then proposes that humans may use empowerment-related cues when learning about causation. Empirically, the authors conduct behavioral experiments with children and adults, presenting them with tasks where different actions yield different degrees of control or influence over outcomes. They then measure how participants use these empowerment cues to infer causal relationships and to plan effective interventions.

Result: The study finds systematic patterns in how both children and adults exploit empowerment cues: participants tend to favor actions that increase their control over outcomes and use these cues to infer underlying causal structure. Children, in particular, show strong sensitivity to empowerment-related aspects of the tasks, consistent with the hypothesis that intrinsic motivation for control guides their exploration and learning. These behavioral results align with the empowerment framework’s predictions.

Conclusion: Empowerment provides a promising bridge between Bayesian causal learning and reinforcement learning frameworks. It offers an intrinsic reward signal that both relies on and promotes learning accurate causal models, helping explain why children are so effective at exploratory causal learning. The findings suggest that similar principles could be built into artificial agents to support more human-like causal discovery and intervention planning.

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [26] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: A framework (KPI) that uses medical knowledge graphs, disease prototypes, contrastive learning, and LLM-based explanations to predict diseases from patient-side information more accurately and interpretably, especially for rare diseases.


<details>
  <summary>Details</summary>
Motivation: Current symptom-checker and patient-side prediction models suffer from imbalanced (long-tailed) disease distributions and poor interpretability, leading to biased and unreliable predictions. There is a need for a method that is both accurate for common and rare conditions and capable of producing clinically meaningful, patient-understandable explanations.

Method: 1) Build a unified, structured disease knowledge graph from trusted medical knowledge. 2) Derive clinically meaningful disease prototypes (representative embeddings for diseases). 3) Use contrastive learning to align patient representations with disease prototypes, improving discrimination, particularly for long-tailed diseases. 4) Leverage large language models to generate patient-specific, medically grounded textual explanations based on the model’s reasoning and the knowledge graph/prototypes.

Result: On real-world datasets, KPI achieves better predictive accuracy than state-of-the-art baselines, with particularly improved performance on long-tailed (rare) diseases, and generates explanations that clinicians consider valid and that closely reflect patients’ described experiences.

Conclusion: Integrating structured medical knowledge, prototype-based representations, and LLM-generated explanations yields a disease prediction framework that is both more accurate and more interpretable than prior methods, making it promising for patient-centered, early-stage healthcare applications.

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [27] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: The paper evaluates modern reasoning-focused large language models on mock CFA exams and finds that most can now pass all three levels with high scores, substantially outperforming earlier results.


<details>
  <summary>Details</summary>
Motivation: Earlier studies suggested that large language models perform poorly on CFA exams, casting doubt on their suitability for complex professional finance tasks. However, newer reasoning models have shown strong performance on other advanced exams. The authors want to re-assess LLM capabilities specifically on the CFA, a rigorous and domain-specific benchmark in finance, to see if these newer models can now meet or exceed professional qualification standards.

Method: The authors construct a benchmark using 980 questions from mock CFA exams: three Level I exams, two Level II exams, and three Level III exams. They test several state-of-the-art reasoning models on this benchmark: Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. They apply the same pass/fail criteria used in prior studies for comparability, and separately evaluate performance across CFA levels and question types (multiple-choice vs constructed-response for Level III).

Result: Most of the evaluated reasoning models pass all three CFA levels under the established criteria. Gemini 3.0 Pro ranks highest overall, achieving 97.6% on Level I. GPT-5 leads Level II with 94.3%. For Level III, Gemini 2.5 Pro scores 86.4% on multiple-choice questions, while Gemini 3.0 Pro reaches 92.0% on constructed-response questions. These scores are substantially higher than those reported in earlier work on LLMs and CFA exams.

Conclusion: The study concludes that state-of-the-art reasoning models have substantially advanced to the point where several can reliably pass all three levels of the CFA exam using mock questions. This overturns earlier findings of poor LLM performance and suggests that modern reasoning-focused LLMs are now capable of handling complex, professional finance examination tasks at or above human passing standards.

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [28] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: The paper proposes using generative agents to automatically evaluate LLM-generated business content, reducing the need for slow and expensive human evaluations.


<details>
  <summary>Details</summary>
Motivation: Businesses need high-quality written content, but human creation and evaluation are time-consuming and costly. While LLMs can generate content, there are concerns about its quality and the cost of traditional evaluation methods like human surveys. Hence, there is a need for a faster, cheaper, yet reliable way to assess AI-generated content quality.

Method: The paper introduces Generative Agents that simulate human judgment to evaluate AI-generated content. These agents rate multiple quality dimensions—including coherence, interestingness, clarity, fairness, and relevance—providing rapid, automated assessments of content aligned with business expectations.

Result: Using generative agents enables rapid and cost-effective evaluation of LLM-generated content while approximating human judgments across several quality dimensions. This helps businesses better manage and refine their content pipelines.

Conclusion: Generative agents can significantly streamline and improve automated content generation workflows by offering scalable, low-cost quality evaluation of LLM outputs. This approach helps align LLM-generated content with business requirements and reduces dependence on expensive human evaluations.

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [29] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: The paper derives quantitative scaling laws and coordination principles for multi-agent LLM-based systems, showing when and how different agent architectures help or hurt across tasks.


<details>
  <summary>Details</summary>
Motivation: While LM-based agents are widely used, practitioners lack principled guidance on when to use single vs. multi-agent architectures, what coordination topology to choose, and how performance scales with tools, model capability, and task type. Existing practice is heuristic, and there is no quantitative framework predicting agent performance across diverse real-world benchmarks.

Method: The authors systematically compare five canonical agent architectures (Single, Independent multi-agent, Centralized, Decentralized, Hybrid) instantiated with three LLM families across four benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench). They control for tools and token budgets, yielding 180 configurations. From these runs they compute empirical coordination metrics (efficiency, overhead, error amplification, redundancy) and fit a predictive model of performance, validated with cross-validated R^2. They then analyze dominant effects and interactions, such as tool use vs. coordination, capability thresholds, and topology-specific error propagation.

Result: The predictive model achieves cross-validated R^2 = 0.513 and correctly predicts the optimal coordination strategy in 87% of held-out configurations. They identify three main findings: (1) A tool–coordination trade-off: with fixed compute, tasks requiring heavy tool use are hurt more by multi-agent overhead. (2) Capability saturation: once a single-agent baseline exceeds about 45% accuracy, additional coordination often yields diminishing or negative returns (negative beta coefficient, statistically significant). (3) Topology-dependent error amplification: independent agents amplify errors by 17.2x via unchecked propagation, while centralized coordination limits this to 4.4x. Centralized coordination gives large gains (80.9%) on parallelizable tasks such as financial reasoning; decentralized coordination is best for dynamic web navigation; but on sequential reasoning tasks, all multi-agent schemes hurt performance by 39–70%.

Conclusion: Multi-agent LLM systems do not universally improve performance; their value depends on task structure, tool demands, and model capability. The paper provides a quantitative framework and coordination metrics that let designers predict when to use centralized, decentralized, or no coordination at all, and shows that beyond a capability threshold or on sequential tasks, simpler single-agent setups may be preferable. This constitutes an initial predictive principle of "agentic scaling" grounded in measurable task properties.

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [30] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: They train a small "planner" agent with reinforcement learning to inject reasoning strategies (like self-reflection) into an LLM’s chain-of-thought, turning it into a stronger Reasoning Language Model (RLM) that can outperform much larger models and generalize across tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs can be post-trained with RL to exhibit advanced reasoning behaviors, marked by "aha" moments and strategic CoT patterns like self-reflection and deep thinking. However, such reasoning often requires costly, model-specific RL pipelines and may not be easily transferable across models or tasks. The authors want a general, reusable mechanism that can endow arbitrary LLMs with strong reasoning strategies without retraining each model from scratch, and ideally using simple reward signals.

Method: They introduce rSIM (reinforced Strategy Injection Mechanism), which adds a small planner (leader agent) that guides an LLM (follower agent) during chain-of-thought generation. The planner decides when and how to inject reasoning strategies (e.g., ask the model to reflect, decompose, or think deeper) in an adaptive way. This planner and the LLM are trained jointly via multi-agent reinforcement learning in a leader–follower framework, with simple rule-based rewards that encourage successful problem solving and effective strategy usage. Once trained, the planner can be used as a plug-in on top of various LLMs to steer their reasoning processes, without redoing full RL for each base model.

Result: With rSIM, a relatively small base model, Qwen2.5-0.5B augmented by the planner, becomes a Reasoning Language Model and significantly outperforms a much larger model, Qwen2.5-14B, on reasoning benchmarks. The trained planner shows strong generalization: it can be reused across different LLMs and tasks, consistently boosting reasoning performance. The framework also supports continual learning, where the planner is further trained on new tasks and gradually improves its planning quality and coverage of problem types.

Conclusion: A lightweight, RL-trained planner that adaptively injects reasoning strategies into an LLM’s chain-of-thought can convert generic LLMs into powerful Reasoning LMs, even enabling tiny models to exceed the reasoning performance of much larger ones. Because the planner is reusable, plug-and-play, and supports continual learning, rSIM presents a practical, scalable path for upgrading the reasoning capabilities of a wide range of existing and future LLMs.

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [31] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: The paper builds machine-learning models to quickly predict the California Bearing Ratio (CBR) of subgrade soils using routine soil properties instead of slow, costly lab penetration tests.


<details>
  <summary>Details</summary>
Motivation: CBR is essential for designing pavements and foundations because it quantifies soil bearing capacity. However, conventional CBR tests are labor-intensive, time-consuming, and impractical for large projects or heterogeneous sites. With growing data availability and advances in ML, there is a need for fast, accurate, data-driven alternatives that can generalize across different soil types and geoclimatic regions.

Method: The authors compile a dataset of 382 soil samples from diverse regions in Türkiye, with multiple physicochemical soil properties as input features and CBR as the target. They formulate a supervised regression problem and develop a comprehensive ML framework. Twelve algorithms are trained, validated, and tested: decision tree, random forest, extra trees, gradient boosting, XGBoost, k-nearest neighbors, support vector regression, multi-layer perceptron, AdaBoost, bagging, voting, and stacking regressors. The models are evaluated on separate training, validation, and test sets using metrics including R² to assess generalization and robustness.

Result: Among all evaluated algorithms, the random forest regressor shows the best predictive performance, with R² values of 0.95 on the training set, 0.76 on the validation set, and 0.83 on the test set. These scores indicate strong but not overfit generalization and effective modeling of the nonlinear relationship between soil properties and CBR.

Conclusion: A random-forest-based ML model can reliably predict CBR from routine soil properties, significantly reducing dependence on time-consuming laboratory penetration tests. The study demonstrates that intelligent, data-centric methods are viable tools in geotechnical engineering and can support the digital transformation of infrastructure analysis and design by enabling faster, more efficient assessment of subgrade bearing capacity.

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [32] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: The paper uses AutoML, finding XGBoost best, to predict soil compaction parameters (OMC, MDD) more accurately and generally across diverse soil types, reducing reliance on laborious lab tests.


<details>
  <summary>Details</summary>
Motivation: Accurate soil compaction parameters (OMC and MDD) are essential for safe, stable construction (e.g., road embankments, earth dams). Conventional lab tests are time-consuming and empirical regression models don’t generalize well to different and heterogeneous soils. Many existing ML models also have limited prediction accuracy and poor generalization when trained on diverse soil data. There is a need for a more automated, accurate, and scalable method to predict these parameters across multiple soil types.

Method: The study applies an automated machine learning (AutoML) framework to a heterogeneous soil dataset to predict optimum moisture content (OMC) and maximum dry density (MDD). AutoML is used to automatically select algorithms and tune hyperparameters. Multiple candidate models are explored, and their performance is compared. Extreme Gradient Boosting (XGBoost) is identified as the best-performing algorithm based on metrics such as R-squared, evaluated on a separate test dataset to assess generalization.

Result: Among all ML models evaluated via AutoML, XGBoost achieved the highest prediction performance, with R-squared values of 80.4% for MDD and 89.1% for OMC on an independent dataset. The use of a heterogeneous dataset was shown to improve the generalization capability of the models, enabling accurate predictions across various soil types.

Conclusion: AutoML is an effective approach for predicting soil compaction parameters (OMC, MDD) in construction engineering, automating model selection and hyperparameter tuning while improving accuracy and scalability compared to traditional lab tests and empirical models. XGBoost, in particular, offers strong predictive performance. Incorporating heterogeneous soil data is crucial for achieving better generalization across different soil types, which can lead to more efficient, reliable, and cost-effective construction practices.

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [33] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: Proposes a new explainable AI framework for graph neural networks that is both adaptable and computationally efficient, focusing on how graph structures drive predictions.


<details>
  <summary>Details</summary>
Motivation: GNNs are widely used but are difficult to interpret; existing XAI methods for graphs are either post-hoc (flexible but computationally expensive and sometimes unreliable) or inherently interpretable (but often not generalizable). There is a need for explanations that are faithful, efficient, and suitable for graph-structured data.

Method: Design a novel XAI framework specifically tailored to graph-based machine learning. The framework goes beyond node or feature-level attributions by explicitly modeling and explaining how graph structure and interactions influence GNN predictions, while aiming to be computationally efficient and adaptable across different GNN architectures.

Result: The abstract does not provide concrete experimental results, but implies that the framework can generate explanations that respect graph structure and are more efficient and adaptable than typical post-hoc methods, and more general than many self-interpretable models.

Conclusion: A new graph-tailored XAI framework is proposed to overcome limitations of existing post-hoc and self-interpretable approaches, enabling more reliable and efficient explanations for GNNs that incorporate structural information rather than focusing only on individual features.

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [34] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: The paper uses LLM-based multi-agent simulations to quantify how toxic behavior slows down discussions, showing about a 25% increase in time to reach agreement when a 'toxic' agent is involved.


<details>
  <summary>Details</summary>
Motivation: Measuring the impact of workplace toxicity on operational efficiency is hard with human subjects due to ethical and practical constraints. The authors want a reproducible, ethical way to quantify how toxic behavior affects the efficiency of reaching conclusions in discussions, which is important for understanding real organizational costs.

Method: They build a multi-agent system using large language models to simulate many 1-on-1 adversarial debates. Using Monte Carlo simulations, they run hundreds of discussions comparing a control condition (neutral agents) with treatment conditions where one or more agents receive toxic system prompts. They define convergence time as the number of argument turns needed to reach a conclusion and statistically compare convergence times between conditions.

Result: Conversations that include toxic agents exhibit about a 25% increase in convergence time compared with neutral control conversations, and this difference is statistically significant. This shows that simulated toxicity consistently slows down the process of reaching conclusions in debates.

Conclusion: The authors argue that the observed increase in convergence time—termed the 'latency of toxicity'—can serve as a proxy for financial and productivity losses in real organizations. They conclude that LLM-based agent simulations offer a reproducible and ethical alternative to human-subject experiments for studying how social friction and toxic behavior affect collaborative efficiency.

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [35] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR is a demonstration-free LLM agent framework that coordinates a high-level planner and a low-level policy via reflection, achieving strong performance with less computation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents depend heavily on external demonstrations or retrieval-augmented planning, which makes them brittle, poorly generalizable, and computationally expensive. The authors want a more human-like, flexible, and efficient reasoning mechanism that works with frozen open-source LLMs and does not require costly supervision or retrieval pipelines.

Method: They design DuSAR, a dual-strategy agent framework for a single frozen LLM. DuSAR maintains (1) a high-level holistic plan that guides long-horizon behavior and (2) a context-grounded local policy that decides the next action. A reflection module computes a Strategy Fitness Score to evaluate current progress; based on this score, the agent either revises the global plan when stuck or refines it when meaningful progress is made. The system mimics human metacognition by continuously monitoring and adapting both strategies. They also test optional integration of expert demonstrations to augment DuSAR without changing the core architecture.

Result: On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art results for open-source LLMs (7B–70B). Using Llama3.1-70B, it reaches 37.1% success on ALFWorld (over 2× the previous best of 13.0%) and 4.02% on Mind2Web (also over 2× the strongest baseline). It additionally reduces per-step token usage by 3–9× while preserving high performance. Ablation studies show that removing the dual-strategy coordination harms performance, confirming its importance. Adding expert demonstrations can further improve results, indicating compatibility with external knowledge sources.

Conclusion: DuSAR demonstrates that a single frozen LLM, equipped with a dual-strategy architecture and a lightweight reflection mechanism, can perform strong, efficient, and adaptive reasoning without relying on external demonstrations. The framework substantially improves success rates and reduces token costs on challenging benchmarks, and it can flexibly incorporate expert demonstrations when available.

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [36] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature is an LLM-based framework that automatically designs, refines, and verifies task-aware features for wearable biosignals, improving predictive performance over existing feature-extraction methods.


<details>
  <summary>Details</summary>
Motivation: Wearable biosignals are powerful for healthcare, but current feature extraction is often manual, not well tailored to each task, hard to optimize in a huge feature space, and prone to errors when turning feature ideas into working code. The authors want an automated, context-aware way to generate high-quality, executable features that boost model performance across diverse applications.

Method: The paper introduces DeepFeature, an LLM-driven feature generation framework that: (1) ingests multi-source context (expert knowledge plus task specifications) to propose candidate biosignal features; (2) iteratively refines and re-selects features using performance-based feedback from downstream models; and (3) passes candidate features through multi-layer filtering and verification to generate robust extraction code that runs reliably on wearable biosignal data.

Result: On eight heterogeneous biosignal-based tasks, DeepFeature improves AUROC by about 4.21–9.67% on average compared with baseline feature-extraction approaches, and it surpasses state-of-the-art methods on five tasks while matching them on the remaining three.

Conclusion: Context-aware, LLM-assisted feature generation can systematically design better biosignal features than conventional methods, leading to consistent performance gains without sacrificing robustness of the generated extraction code. DeepFeature demonstrates that combining expert knowledge, task-specific context, iterative refinement, and rigorous code verification is an effective strategy for automating feature engineering in wearable healthcare applications.

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [37] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: The paper proposes the Impact-Driven AI Framework (IDAIF), which links Theory of Change with AI system architecture to build value-aligned, trustworthy AI, demonstrated via case studies in several domains.


<details>
  <summary>Details</summary>
Motivation: AI is increasingly deployed in high-stakes domains, but most current methods focus on technical metrics and overlook broader sociotechnical impacts and value alignment. There is a need for an architectural framework that explicitly connects AI design to desired real-world impacts and ethical constraints.

Method: The authors map the five stages of Theory of Change (Inputs, Activities, Outputs, Outcomes, Impact) to five AI architectural layers (Data, Pipeline, Inference, Agentic, Normative). Within these layers, they use tools such as multi-objective Pareto optimization for aligning multiple values, hierarchical multi-agent orchestration for coordinating outcomes, causal DAGs to reduce hallucinations, and adversarial debiasing with RLHF to promote fairness. They also introduce a separate Assurance Layer with guardian architectures to handle assumption violations and failures.

Result: They provide formal mathematical formulations for each architectural component and illustrate the framework with three detailed case studies in healthcare, cybersecurity, and software engineering, showing how IDAIF can be instantiated in practical systems.

Conclusion: IDAIF reframes AI development from a model-centric process to an impact-centric one, offering a structured, theoretically grounded architecture for building AI that is ethical, trustworthy, and socially beneficial, and giving engineers concrete design patterns to operationalize value alignment and impact tracking.

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [38] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: The paper uses a physical fluid-dynamics setup and a generalist RL agent to study what information is needed to learn and execute high-performance skills, distinguishing between learning and execution requirements.


<details>
  <summary>Details</summary>
Motivation: Many real-world high-skill tasks are performed with minimal external feedback, but understanding what information is necessary to learn such skills versus to execute them is difficult with human subjects due to lack of control and reproducibility. The authors want a clean, reproducible, physical testbed to probe the informational demands of skill acquisition versus deployment and to explore when learning environments are "kind" (informative) or "wicked" (uninformative) despite identical dynamics.

Method: They couple a generalist reinforcement learning agent directly to a physical experiment: a spinning cylinder in a tabletop water channel. The agent can control cylinder rotation to minimize or maximize drag, receiving reward from drag measurements. They compare training with rich, high-dimensional flow-state feedback versus restricted or no flow feedback, and then test open-loop execution by replaying learned control sequences without feedback. They contrast performance across objectives (drag minimization vs maximization).

Result: With rich flow feedback, the RL agent quickly discovers high-performance drag-control policies within minutes of real-world interaction. When the same action sequences are replayed open loop, performance is almost unchanged, showing that feedback is unnecessary for execution. However, removing flow feedback during training prevents the agent from finding good drag-maximization policies, while it can still learn drag-minimization policies, though more slowly and less reliably. Thus, the informational requirements differ between learning and execution and depend on the optimization objective.

Conclusion: The study demonstrates that acquiring a complex physical skill can demand richer information than is needed for its later execution. The same physical system and policy class can present either kind or wicked learning conditions depending solely on the control goal (drag increase vs decrease). This has implications for how we design sensing, feedback, and training regimes in both artificial and biological skill learning, emphasizing that learning-time information structures can be more critical than execution-time feedback.

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [39] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: The paper introduces a new data-centric graph representation and multi-agent framework (AIR) to improve automated program repair at repository scale, achieving high success on SWE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Large Language Model-based tools work well for function-level code generation but struggle with repository-scale automated program repair because they rely on control-centric views and Code Property Graphs, making agents traverse complex structures and irrelevant control flow. There is a need for a more robust, scalable way to locate and repair logic defects across large codebases.

Method: The authors propose Data Transformation Graphs (DTGs), which invert traditional Code Property Graphs by representing data states as nodes and functions as edges so that agents can follow data lineage rather than control flow. They build a multi-agent framework that combines data-integrity-based navigation with control-flow reasoning, implemented as the Autonomous Issue Resolver (AIR). AIR uses neuro-symbolic reasoning over the DTG to perform zero-touch code maintenance and scalable logic repair. They also provide theoretical analysis and case studies to explain how DTG-based navigation avoids the 'Semantic Trap' of standard RAG systems in coding agents.

Result: Their AIR system, built on DTGs and the multi-agent architecture, achieves strong empirical performance on software engineering benchmarks, including an 87.1% resolution rate on the SWE-Verified benchmark, indicating effective repository-scale automated program repair.

Conclusion: Modeling codebases with Data Transformation Graphs and leveraging a neuro-symbolic multi-agent system enables more effective and scalable automated program repair than traditional control-centric and RAG-based approaches. This paradigm better captures data lineage, avoids semantic traps in retrieval, and lays a more robust foundation for AI-driven code maintenance tools in large, real-world software projects.

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [40] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: The paper proposes a lightweight constructive incremental transfer learning (CITL) method for fast, accurate lithium-ion battery SOH monitoring on resource-limited portable devices, achieving markedly lower estimation errors than existing TL approaches.


<details>
  <summary>Details</summary>
Motivation: Lithium-ion batteries in portable mobile devices and UAVs operate under variable conditions, making accurate state-of-health (SOH) monitoring difficult. Transfer learning can reuse knowledge from data-rich conditions to new ones, but existing TL methods are computationally expensive and unsuitable for on-device deployment, reducing device endurance. There is a need for a TL-based SOH monitoring method that is both accurate and lightweight, exploiting unlabeled target data while remaining computationally efficient.

Method: The authors design a constructive incremental transfer learning (CITL) framework that builds a compact neural network progressively. They introduce a semi-supervised transfer learning mechanism that uses unlabeled data from the target domain to iteratively add nodes to the network while minimizing SOH estimation residuals. The model’s cross-domain generalization is enforced through three principles: structural risk minimization to control model complexity, transfer mismatching minimization to reduce distributional differences between source and target domains, and manifold consistency maximization to preserve geometric structure across domains. They also provide a convergence analysis demonstrating that the incremental procedure leads to stable performance and a compact architecture.

Result: On a real UAV battery dataset gathered from many flight missions, the proposed CITL method is compared against several state-of-the-art TL-based SOH monitoring methods (SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, AS^3LSTM). Using root mean square error as the metric, CITL achieves substantially better SOH estimation accuracy, with relative improvements of 83.73%, 61.15%, 28.24%, 87.70%, and 57.34% over the respective baselines.

Conclusion: CITL provides an effective and computationally efficient transfer learning approach for battery SOH monitoring in portable and UAV applications. By incrementally constructing a compact network and exploiting unlabeled target data under semi-supervised learning, it maintains high estimation accuracy while being suitable for resource-constrained devices. Theoretical convergence guarantees and strong empirical results suggest it is a promising solution for practical on-device health monitoring under varying operating conditions.

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [41] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Introduces Principles2Plan, a human–LLM collaborative prototype that turns high-level ethical principles into operational rules to guide classical automated planning for robots.


<details>
  <summary>Details</summary>
Motivation: Robots need ethical awareness when acting in human environments, but current automated planners lack built-in support for ethics, and manually encoding context-specific ethical rules is difficult and time-consuming. The paper aims to reduce this burden and make ethical planning more practical.

Method: They design Principles2Plan, an interactive system where a domain expert provides a planning domain, problem description, and high-level ethical principles (e.g., beneficence, privacy). A Large Language Model then proposes operational, context-sensitive ethical rules consistent with these principles. The user can inspect, refine, and prioritise these rules, which are then fed into a classical planner to produce ethically informed plans.

Result: The prototype system successfully demonstrates that LLMs can help translate abstract ethical principles into concrete planning constraints and rules that integrate with classical planning. It shows that users can interactively shape and prioritise these rules for specific domains.

Conclusion: Principles2Plan illustrates a novel, principle-grounded workflow for ethical automated planning that leverages human–LLM collaboration. It is, according to the authors, the first system to support generation of principle-based ethical rules for classical planning, suggesting a feasible path toward more practical and ethically informed robotic planning systems.

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [42] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: The paper proposes CogMCTS, a cognitive-guided Monte Carlo Tree Search framework that integrates large language models for automatic heuristic design, improving solution quality, stability, and efficiency over prior LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Automatic Heuristic Design is powerful for complex optimization problems, but existing LLM-based approaches either use evolutionary population strategies that get stuck in local optima or basic MCTS integrations with limited cognitive interaction and low search diversity. There is a need for a framework that better balances exploration and exploitation while using LLMs’ cognitive capabilities more deeply and iteratively.

Method: The authors design CogMCTS, a framework that tightly couples an LLM with MCTS. It uses multi-round cognitive feedback from the LLM to integrate historical experience, node-specific information, and negative outcomes to refine heuristic generation over time. A dual-track node expansion strategy plus elite heuristic management jointly control exploration of new heuristics and exploitation of strong ones. Additionally, a strategic mutation mechanism alters heuristic structures and parameters to further diversify candidate solutions and improve optimization.

Result: Experiments show that CogMCTS achieves better performance than existing LLM-based AHD approaches, providing more stable results, higher efficiency, and better solution quality on the evaluated optimization tasks.

Conclusion: CogMCTS demonstrates that deeply integrating LLM-driven cognitive guidance with MCTS can significantly enhance automated heuristic design. By combining multi-round feedback, elite management, and strategic mutation, the framework achieves a better exploration–exploitation balance and yields more diverse and higher-quality heuristics than prior LLM-based methods.

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [43] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: Transformer-based model predicts protein secondary structure from amino acid sequences using attention and sliding-window augmentation on CB513.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of protein secondary structure (alpha helices, beta sheets, coils) from sequence helps understand and infer protein function without costly experiments.

Method: Use a transformer architecture with attention mechanisms on protein sequences, combined with a sliding-window data augmentation strategy on the CB513 dataset to increase training examples and capture local context while allowing the model to learn long-range dependencies.

Result: The transformer model generalizes well to protein sequences of variable length and effectively captures both local and long-distance residue interactions for secondary-structure prediction.

Conclusion: Attention-based transformers, aided by sliding-window augmentation, are effective for protein secondary-structure prediction and can robustly handle variable-length sequences while modeling multi-scale dependencies.

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [44] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: Introduces See-Control, a framework where a low-DoF robot physically operates smartphones guided by an MLLM, removing dependence on Android Debug Bridge and specific OS back-ends, and providing a new benchmark and dataset for embodied smartphone operation.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal LLM-based phone agents require Android Debug Bridge and direct system access, limiting them to specific platforms and preventing realistic physical-world interaction. There is a need for platform-agnostic, embodied agents that can operate smartphones through physical manipulation like a human, enabling broader real-world deployment, especially for home robots relying on phone-based services.

Method: Define the Embodied Smartphone Operation (ESO) task and create See-Control, a framework where an MLLM-based embodied agent receives multimodal input (e.g., visual observations) and outputs control commands for a low-degree-of-freedom robotic arm that physically interacts with a smartphone touchscreen. Build an ESO benchmark of 155 diverse tasks with evaluation metrics, design the control pipeline from perception to action without any ADB or OS back-end hooks, and collect a richly annotated dataset of robot-phone interaction episodes for training and evaluation.

Result: A working system where an MLLM-powered agent successfully operates a smartphone via a low-DoF robot without ADB, demonstrating platform-agnostic smartphone control. The authors provide: (1) an ESO benchmark of 155 tasks with metrics, (2) an MLLM-based control framework mapping perception to robot actions, and (3) an annotated dataset of operation episodes. Experiments show that the framework can complete smartphone-dependent tasks in realistic physical settings, validating the feasibility of embodied phone operation.

Conclusion: See-Control bridges the gap between purely digital phone agents and physical robots by enabling multimodal LLMs to control smartphones through real-world manipulation rather than software APIs. The ESO benchmark and dataset establish a foundation for future research on embodied smartphone use, and the demonstrated system suggests that home robots can, in the future, reliably perform tasks that depend on smartphone apps across different platforms.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [45] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: They propose a hierarchical multi‑agent AI framework that imitates a multidisciplinary medical team to improve multimodal clinical reasoning in GI oncology, outperforming a standard single-model baseline.


<details>
  <summary>Details</summary>
Motivation: Multimodal clinical reasoning in GI oncology requires combining information from endoscopic images, radiology, and lab markers. Existing multimodal large language models struggle with complex, heterogeneous medical histories, leading to context dilution and hallucinations. There is a need for a method that can handle this complexity more reliably and accurately, in a way that matches human multidisciplinary workflows.

Method: The authors design a hierarchical multi‑agent framework that mimics the collaboration patterns of a human Multidisciplinary Team (MDT). Different specialized agents likely focus on distinct data modalities or sub‑tasks (e.g., imaging, labs, clinical history), and their outputs are coordinated and integrated at a higher level to produce final clinical reasoning and decisions. This architecture is contrasted with a monolithic baseline MLLM handling all inputs directly.

Result: In expert evaluations, the proposed system achieved a composite score of 4.60 out of 5.00, significantly higher than the monolithic baseline. The largest performance gains were observed in the quality of reasoning logic and the medical accuracy of the system’s outputs.

Conclusion: Imitating MDT-style collaboration through a hierarchical agent-based architecture can substantially improve multimodal clinical reasoning in GI oncology compared with a single, monolithic model. This mimetic, agent-based collaboration appears scalable, more interpretable, and clinically robust, suggesting it is a promising paradigm for automated oncology decision support.

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [46] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: The paper proposes a framework to turn human expert intuition and AI decision processes into a transparent, composable system through structured human-AI interaction and a new meta-thinking network architecture.


<details>
  <summary>Details</summary>
Motivation: There is a gap between opaque human expert intuition and black-box AI models, making it hard to trust, audit, and govern AI-supported decisions, especially in high-stakes domains. The author wants a way to externalize, share, and govern expert thinking without relying on opaque model internals.

Method: The paper introduces a "human-AI collaborative cognitive enhancement" paradigm built on two key ideas: (1) a plug-and-play cognitive framework, defined as computable knowledge packages extracted from structured expert-AI dialogues, and (2) the Recursive Adversarial Meta-Thinking Network (RAMTN), an architecture that can load these cognitive modules and orchestrate them via explicit meta-interaction protocols. The method formalizes interaction protocols so that expert reasoning patterns (e.g., diagnostic logic, teaching heuristics) are captured, versioned, and reused as modular components.

Result: The paper reports that expert cognitive processes such as medical diagnosis logic or teaching intuition can be encoded into reusable packages, integrated into RAMTN, and applied as scalable public assets, effectively operationalizing expert thinking in AI systems. It also demonstrates that governance and auditing can be done at the level of transparent interaction protocols instead of model internals, and presents case validations plus an open-sourced implementation.

Conclusion: The work argues that shifting from "AI as a tool" to "AI as a thinking partner" is feasible via structured meta-interaction and RAMTN, enabling cognitive equity (sharing expert-level reasoning) and new forms of AI governance focused on verifiable, intervenable interaction protocols. By open-sourcing the framework, the author aims to support technology for good and cognitive inclusion, and positions the work as an initial engineering proof-of-concept for this paradigm.

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [47] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: The paper argues that the next major step for foundation models is to develop *native multi-agent intelligence*, and shows empirically that good single-agent performance does not automatically translate into strong multi-agent abilities.


<details>
  <summary>Details</summary>
Motivation: Foundation models are becoming the core “brain” of AI agents, and recent work has mainly focused on enhancing their capabilities as single agents (e.g., interacting with GUIs or using tools). However, many real-world scenarios involve multiple agents that must understand each other, coordinate, and adapt. There is a common assumption that if a model is strong as a single agent, sophisticated multi-agent behaviors will emerge automatically, but this has not been systematically tested. The paper aims to examine this assumption, characterize what multi-agent intelligence requires, and identify how to intentionally build such capabilities into foundation models.

Method: The authors conceptually define four core capabilities required for multi-agent intelligence in foundation models: (1) understanding other agents, (2) planning in multi-agent settings, (3) efficient communication, and (4) adaptation to others over time. Empirically, they benchmark 41 large language models on tasks designed to probe these multi-agent capabilities and compare their performance to standard single-agent benchmarks. They analyze whether single-agent strength correlates with robust multi-agent behavior and where current models fail.

Result: Across a large pool of 41 LLMs, the authors find that strong performance on single-agent tasks does not guarantee strong multi-agent intelligence. Many models that excel in standard benchmarks struggle with multi-agent understanding, coordination, communication efficiency, and adaptation. The empirical results challenge the assumption that multi-agent skills emerge automatically from scale and single-agent training alone.

Conclusion: The paper concludes that native multi-agent intelligence is a distinct capability dimension that current foundation models largely lack, even when they perform well as single agents. Achieving it will require targeted research and engineering, not just larger or better single-agent training. The authors propose research directions in four areas—dataset design, evaluation frameworks, training paradigms, and safety—to systematically develop and assess foundation models with built-in multi-agent capabilities.

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [48] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: The paper compares the performance of UAV-mounted RIS and STAR-RIS in 3D wireless networks, providing deployment guidelines for future 6G systems.


<details>
  <summary>Details</summary>
Motivation: While both aerial RIS and STAR-RIS are promising for extending coverage and improving capacity, there is a lack of comprehensive and fair performance comparison between them in realistic 3D scenarios with UAV deployment. Network designers need to know when each architecture is preferable and how altitude and orientation affect system performance.

Method: The authors build accurate 3D channel models that include directional antenna radiation patterns for both UAV-mounted RIS and STAR-RIS. They formulate system sum-rate maximization problems that jointly optimize beamforming/phase shifts and deployment parameters. To solve these non-convex problems efficiently, they adopt a weighted minimum mean square error (WMMSE) framework combined with block coordinate descent (BCD) to iteratively optimize different variable blocks.

Result: Simulations show that STAR-RIS achieves higher sum-rate than conventional RIS when the UAV operates at low altitudes, thanks to the ability to serve both sides of the surface (full-space coverage). However, at higher altitudes and near the base station, a conventional RIS can outperform STAR-RIS, indicating that its reflective-only operation is more efficient in that regime.

Conclusion: Aerial STAR-RIS is more suitable for low-altitude deployments requiring wide-area/full-space coverage, while conventional aerial RIS is preferable at higher altitudes near the base station. The paper offers practical deployment insights and design guidelines for selecting and configuring aerial intelligent surfaces in future 6G networks.

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [49] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: The paper is a practical guide to designing, building, and operating production-grade agentic AI workflows, focusing on reliability, observability, maintainability, and safety.


<details>
  <summary>Details</summary>
Motivation: As agentic AI systems that use multiple coordinated agents and tools become widely adopted, organizations struggle to build them in a way that is robust, manageable, and compliant with safety and governance requirements. There is a lack of end-to-end engineering guidance for production settings.

Method: The authors propose a structured engineering lifecycle for agentic AI systems, covering workflow decomposition, multi-agent design patterns, use of Model Context Protocol (MCP) and tools, deterministic orchestration, Responsible-AI practices, and deployment strategies. They distill this into nine best practices and illustrate them via a detailed case study of a multimodal news-analysis and media-generation workflow.

Result: The paper yields a set of nine concrete best practices and an architectural framework for production-grade agentic workflows, plus an implemented case study that shows how these guidelines can be applied in a real multimodal system using multiple agents, tools, and orchestration mechanisms.

Conclusion: Following the proposed lifecycle and nine best practices enables practitioners to build agentic AI workflows that are more robust, observable, scalable, and aligned with safety and governance requirements, providing a foundational reference for real-world deployment of agentic AI systems.

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [50] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: Introduces CARLoS, a framework to automatically characterize and retrieve LoRA generative components without relying on user-provided metadata, using a compact semantic representation derived from image-generation behavior.


<details>
  <summary>Details</summary>
Motivation: The LoRA ecosystem is large, fast-growing, and poorly organized. Discovery of useful LoRAs typically depends on noisy user-written descriptions or popularity metrics that may be biased or uninformative. There is a need for a principled, scalable way to understand what each LoRA actually does and to help users find appropriate ones based on their semantic behavior rather than unreliable metadata.

Method: The authors generate images with more than 650 LoRAs across diverse prompts and random seeds, and compare each LoRA’s outputs to those from a base model. They compute CLIP embeddings for images, then derive a compact, three-part numeric representation from the embedding differences: (1) Directions, describing the semantic shift induced by the LoRA; (2) Strength, measuring the magnitude of the effect; and (3) Consistency, measuring how reliably that effect appears across prompts/seeds. Using this representation, they build a retrieval system that maps textual queries into the same semantic space, enabling filtering by Strength and Consistency.

Result: The CARLoS representation enables a retrieval framework that semantically matches text queries with appropriate LoRAs and can filter out LoRAs whose effects are too strong or unstable. In experiments, this approach outperforms baselines that rely solely on textual metadata, as validated by both automated metrics and human studies. Additionally, the quantified Strength and Consistency metrics show meaningful connections to legal notions of substantiality and volition.

Conclusion: CARLoS provides a scalable, metadata-free method to characterize LoRAs via behavioral analysis in image generation. Its three-part representation (Direction, Strength, Consistency) not only improves semantic LoRA retrieval but also opens the door to more rigorous analysis of LoRAs’ impact, including legal and copyright-related considerations. The framework is positioned as a practical tool for organizing and understanding the growing LoRA ecosystem.

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [51] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: The paper surveys and advances methods for computing Craig and uniform interpolants for key knowledge representation formalisms, focusing on description logics and logic programming, highlighting both theory and practical algorithms.


<details>
  <summary>Details</summary>
Motivation: Craig interpolation and uniform interpolation support tasks such as explanation, forgetting, modularization, reuse, and learning in knowledge representation. However, many important formalisms lack these properties in general, and even when they exist, computing interpolants efficiently is difficult. There is thus a need to better understand when and how interpolants can be obtained in widely used logical frameworks.

Method: The authors theoretically analyze interpolation properties in two major knowledge representation formalisms—description logics and logic programming. They review and develop results on the existence of Craig and uniform interpolants and study algorithmic techniques to compute them in practice, likely including syntactic and semantic constructions, reductions, and possibly tool-supported procedures.

Result: They identify conditions under which Craig and uniform interpolation hold or fail for various fragments of description logics and logic programming. They also present or survey concrete procedures that can compute interpolants in those settings where interpolation is possible, discussing their practicality and limitations.

Conclusion: Interpolation is a powerful but nontrivial tool for knowledge representation. For description logics and logic programming, it is not universally available but can often be obtained under certain restrictions, with dedicated algorithms enabling practical computation. Understanding these boundaries and methods supports applications in explanation, forgetting, modularization, reuse, and learning.

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [52] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench is a real-world, e-commerce–focused benchmark to rigorously evaluate foundation agents on realistic tasks instead of artificial or purely academic scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing agent benchmarks mostly use synthetic, academic, or overly simplified settings and fail to capture the complexity, diversity, and decision-critical nature of real e-commerce environments. There is a need for an evaluation suite that reflects genuine user demands, dynamic markets, and practical business-relevant tasks.

Method: The authors construct EcomBench from real user demands collected from major global e-commerce platforms. Human experts curate and annotate these cases to ensure clarity and domain correctness. The benchmark spans multiple e-commerce task categories and defines three difficulty levels. These levels are designed to probe capabilities such as deep information retrieval, multi-step reasoning across steps, and integration of knowledge from multiple sources.

Result: EcomBench provides a structured set of realistic tasks where agents can be systematically evaluated on their performance at different difficulty levels across a variety of e-commerce scenarios. It empirically exposes strengths and weaknesses of foundation agents in real e-commerce contexts, though detailed quantitative results are not given in the abstract.

Conclusion: By grounding evaluation in genuine e-commerce data and diverse, multi-level tasks, EcomBench serves as a comprehensive and practical benchmark for assessing how well foundation agents perform on real-world e-commerce problems, offering a more faithful measure of their practical utility than prior synthetic or academic benchmarks.

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [53] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: They introduce REST and REST+ benchmarks to test whether multimodal LLMs give consistent answers when the same information is presented as text, image, or a mix, and show that current models are often inconsistent and sensitive to low-level visual details.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs are supposed to unify vision and language in a shared embedding space, implying they should solve the same problem regardless of whether information is presented as text, image, or mixed modalities. However, anecdotal evidence suggests that models fail to behave consistently across modalities. There is a lack of systematic benchmarks to measure this cross-modal inconsistency and understand what factors drive it.

Method: They build two benchmarks, REST and REST+ (Render-Equivalence Stress Tests), each containing parallel samples that encode the same semantic content in three modalities: pure text, pure image (rendered text or visuals), and mixed text–image formats. They then test 15 state-of-the-art MLLMs on these samples, quantify how often models give consistent answers across modalities, control for OCR errors, and run analyses varying visual properties (text color, resolution, font) and the number of vision tokens to study their influence on performance. They further compute a modality gap measure between image and text embeddings and relate it to their consistency score.

Result: Across 15 MLLMs, performance and especially cross-modal consistency vary widely. Models frequently give different answers when the same content is presented as text vs. image vs. mixed, even after controlling for OCR quality. Converting text to images or images to text does not remove the inconsistency. Visual features such as text color and resolution, as well as the number of vision tokens, significantly affect performance, whereas font type does not. Their consistency metric correlates with the embedding-space modality gap between text and images.

Conclusion: The study shows that current MLLMs have substantial cross-modal inconsistency, contradicting the expectation of modality-invariant reasoning implied by shared embeddings. Low-level visual factors and architectural tokenization choices can strongly influence outcomes, beyond OCR issues. The correlation between consistency and modality gap suggests a mechanistic link: better-aligned text–image representations may yield more consistent multimodal reasoning. REST and REST+ provide standardized tests for diagnosing and improving cross-modal consistency in future MLLMs.

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>
