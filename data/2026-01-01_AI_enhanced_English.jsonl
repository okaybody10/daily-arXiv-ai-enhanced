{"id": "2512.23850", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23850", "abs": "https://arxiv.org/abs/2512.23850", "authors": ["Rahul Baxi"], "title": "The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models", "comment": "Currently under review at TMLR", "summary": "Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.", "AI": {"tldr": "The paper proposes a new evaluation protocol (DDFT) to test how robustly language models maintain factual accuracy under compression and adversarial pressure, finding that robustness is independent of size/architecture and instead linked to error detection capabilities.", "motivation": "Existing benchmarks like MMLU and TruthfulQA mainly test what models know in ideal conditions, not whether they can maintain accurate knowledge when prompts are compressed, noisy, or adversarial. This means we cannot distinguish between models that truly lack knowledge and those that have knowledge but fail to verify or defend it under stress. With increasing deployment of LMs in critical settings, there is a need for evaluations that capture epistemic robustness\u2014how well models can preserve and check factual accuracy under more realistic, challenging conditions.", "method": "The authors introduce the Drill-Down and Fabricate Test (DDFT), a protocol that stresses models by progressively compressing semantic content and introducing adversarial fabrication. They conceptualize language models as composed of two systems: a Semantic System that produces fluent language and an Epistemic Verifier that checks factuality. They evaluate 9 frontier models across 8 knowledge domains and 5 levels of compression, performing 1,800 turn-level evaluations, and analyze correlations between robustness and variables such as parameter count, architecture type, and error detection capability.", "result": "Empirical results show that epistemic robustness does not correlate with model size (parameter count, r=0.083, p=0.832) or architecture type (r=0.153, p=0.695). Instead, robustness is strongly predicted by the model's error detection capability (rho=-0.817, p=0.007), identifying this as the key bottleneck. Large flagship models can still be brittle under stress, while some smaller models perform robustly, contradicting common expectations that bigger models are necessarily more reliable.", "conclusion": "DDFT offers a theoretically motivated and practically implementable framework for evaluating epistemic robustness of language models. The findings imply that reliability under stress depends less on scale or architecture and more on training methods and verification mechanisms that support effective error detection. This challenges size-centric design assumptions and provides tools for assessing and improving models before their deployment in high-stakes, real-world applications."}}
{"id": "2512.23880", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.23880", "abs": "https://arxiv.org/abs/2512.23880", "authors": ["Xu Huang", "Junwu Chen", "Yuxing Fei", "Zhuohan Li", "Philippe Schwaller", "Gerbrand Ceder"], "title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution", "comment": null, "summary": "Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from \"LLM + tool use\" to \"LLM + skill acquisition\". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.", "AI": {"tldr": "The paper presents CASCADE, a self-evolving LLM agent framework that shifts from static tool use to dynamic skill acquisition, enabling continuous learning and self-reflection to solve complex scientific tasks with high success rates.", "motivation": "Current LLM agents rely on fixed or fragile tool integrations, limiting their robustness and adaptability to complex, real-world scientific workflows. The authors aim to create an agent framework that can autonomously learn to use external tools, accumulate reusable skills, and improve over time.", "method": "They design CASCADE with two central meta-skills: (1) continuous learning, implemented through web search, code extraction, and interaction with external software; and (2) self-reflection, implemented via introspection over past trajectories and knowledge graph exploration. CASCADE stores acquired procedures as executable skills that can be reused, shared, and composed. The framework is evaluated on SciSkillBench, a new benchmark of 116 materials science and chemistry tasks, and demonstrated on real-world scientific applications.", "result": "On SciSkillBench, CASCADE powered by GPT-5 attains a 93.3% success rate, a large improvement over a 35.4% baseline without the self-evolution mechanisms. In case studies, CASCADE successfully performs computational analyses, autonomously plans and executes lab experiments, and selectively reproduces published scientific results.", "conclusion": "CASCADE shows that moving from static tool invocation to dynamic skill acquisition substantially boosts an LLM agent\u2019s ability to handle complex scientific research tasks. By combining continuous learning, self-reflection, and shared executable skills, the framework supports scalable, collaborative, AI-assisted science and suggests a path toward increasingly autonomous and capable research agents."}}
{"id": "2512.23932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23932", "abs": "https://arxiv.org/abs/2512.23932", "authors": ["Ioanna Gemou", "Evangelos Lamprou"], "title": "A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming", "comment": null, "summary": "Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.", "AI": {"tldr": "McCoy is a framework that uses LLMs to convert medical literature into Answer Set Programming rules, combines them with patient data, and runs an ASP solver to produce accurate, interpretable disease diagnoses.", "motivation": "Accurate and early disease prediction is crucial in healthcare, but symbolic AI approaches require manual construction of high-quality knowledge bases, which is labor-intensive and limits their adoption. The authors aim to lower the barrier to using symbolic reasoning in medicine by automating knowledge base creation from medical texts.", "method": "The framework, McCoy, orchestrates a Large Language Model to read and translate medical literature into Answer Set Programming (ASP) rules. These rules are then integrated with structured patient data and evaluated using an ASP solver. The ASP solver performs logical inference over the combined knowledge to produce a final diagnosis, providing a symbolic, interpretable reasoning chain.", "result": "Preliminary experiments show that McCoy performs well on small-scale disease diagnosis tasks, suggesting that LLM-generated ASP knowledge bases can be accurate enough to support symbolic reasoning in practice.", "conclusion": "By combining LLM-based knowledge extraction with ASP-based logical reasoning, McCoy offers a robust and interpretable disease prediction framework and demonstrates that automated construction of symbolic knowledge bases from text is a viable path to broader adoption of symbolic AI in healthcare, at least on small-scale tasks."}}
{"id": "2512.24008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24008", "abs": "https://arxiv.org/abs/2512.24008", "authors": ["Gaurab Chhetri", "Subasish Das", "Tausif Islam Chowdhury"], "title": "SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing", "comment": "This is the author's preprint. Accepted to WEB&GRAPH 2026 (co-located with WSDM 2026), Boise, Idaho, USA, Feb 26, 2026. Final version will appear in WSDM 2026 Companion Proceedings. Conf: https://wsdm-conference.org/2026/ Workshop: https://aiimlab.org/events/WSDM_2026_WEB_and_GRAPH_2026_Workshop_on_Web_and_Graphs_Responsible_Intelligence_and_Social_Media.html", "summary": "Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.", "AI": {"tldr": "SPARK is a multi-agent, persona-based LLM framework for personalized search, where specialized agents with different roles and memories coordinate to answer user queries and enable emergent personalization.", "motivation": "Existing personalized search systems rely on static user profiles or single, monolithic retrieval pipelines that struggle to capture evolving, multi-dimensional user needs. There is a need for a more dynamic, cognitively inspired architecture that models complex, changing information-seeking behavior while remaining efficient and systematically analyzable.", "method": "The paper defines a structured persona space (role, expertise, task context, domain) and introduces a Persona Coordinator that maps incoming queries to appropriate persona-based LLM agents. Each agent runs its own retrieval-augmented generation with dedicated long- and short-term memories and context-aware reasoning modules. The framework adds explicit inter-agent communication mechanisms (shared memory, debate, relay-style transfer) and uses principles from cognitive architectures and multi-agent coordination to specify minimal coordination rules and adaptive learning for persona refinement.", "result": "The work is primarily a framework-level contribution: it specifies how agent specialization, coordination mechanisms, and memory structures can jointly yield emergent personalization properties. It produces testable predictions about metrics like coordination efficiency, personalization quality, and cognitive load distribution across agents, and demonstrates how continuous adaptation can refine personas over time.", "conclusion": "By combining fine-grained persona specialization with cooperative, retrieval-augmented multi-agent LLMs, SPARK offers a conceptual and architectural blueprint for next-generation personalized search systems that better match the complexity, fluidity, and context dependence of human information needs."}}
{"id": "2512.24040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24040", "abs": "https://arxiv.org/abs/2512.24040", "authors": ["Natchaya Temyingyong", "Daman Jain", "Neeraj Kumarsahu", "Prabhat Kumar", "Rachata Phondi", "Wachiravit Modecrua", "Krittanon Kaewtawee", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment", "comment": "22 pages, 1 figure", "summary": "Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.", "AI": {"tldr": "The paper introduces ROAD, a framework that optimizes LLM prompts without requiring clean labeled datasets by transforming messy production failure logs into structured protocols via a multi\u2011agent debugging\u2011style process.", "motivation": "Existing Automatic Prompt Optimization methods need large, labeled, and clean development sets to compute fitness scores, which is unrealistic in early, real\u2011world software engineering settings where only noisy production logs and shifting failure modes exist. There is a need for a data\u2011efficient way to improve LLM agents during cold start, using the same kind of failure analysis loop engineers naturally perform.", "method": "ROAD reframes prompt optimization as automated debugging rather than stochastic search. It uses a multi\u2011agent architecture with three specialized LLM agents: (1) an Analyzer that performs root\u2011cause analysis over unstructured failure logs, (2) an Optimizer that aggregates recurring patterns from these analyses, and (3) a Coach that integrates these patterns into improved strategies. Together, they transform noisy logs into structured Decision Tree Protocols (DTPs), which then guide updated prompts/agent behaviors. The system iterates this loop automatically over a small number of cycles.", "result": "On a standard academic benchmark and a real production Knowledge Management engine, ROAD improves performance with very few optimization iterations. It boosts success rate from 73.6% to 79.2% (a 5.6\u2011point increase) and search accuracy by 3.8 points in just three automated iterations. On complex retail reasoning tasks, ROAD yields about a 19% relative performance gain over the baseline agent.", "conclusion": "Treating LLM prompt optimization as a structured debugging loop\u2014mirroring how human engineers analyze failures and patch systems\u2014can be a sample\u2011efficient alternative to RL\u2011based APO methods that rely on large curated datasets. ROAD\u2019s multi\u2011agent analysis pipeline effectively converts messy production logs into actionable decision protocols, enabling more reliable and data\u2011efficient deployment of LLM agents in real\u2011world environments."}}
{"id": "2512.23711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23711", "abs": "https://arxiv.org/abs/2512.23711", "authors": ["Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Claudio Pinhanez", "Yago Primerano"], "title": "CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations", "comment": null, "summary": "We introduce \\textsc{CAT}, a framework designed to evaluate and visualize the \\emph{interplay} of \\emph{accuracy} and \\emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \\textsc{CAT} are the \\emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \\emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \\emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \\textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.", "AI": {"tldr": "Proposes CAT, a framework to jointly evaluate accuracy and response consistency of LLMs using new metrics and visualizations.", "motivation": "Existing LLM evaluations largely treat accuracy and consistency separately. For real-world, high-stakes deployment, both are crucial and their interaction matters, but there is no standard way to quantify and visualize this trade-off, especially under controlled input variations on MC benchmarks.", "method": "Introduce CAT, centered on Consistency-Accuracy Relation (CAR) curves, which plot model accuracy as a function of increasingly strict consistency requirements across controlled input variations. Define Minimum-Consistency Accuracy (MCA) to operationalize accuracy under a given consistency threshold. Propose the Consistency-Oriented Robustness Estimate (CORE) index, aggregating the CAR curve\u2019s area and shape into a single score reflecting the balance between accuracy and consistency. Demonstrate the framework on multiple generalist and domain-specific LLMs over various MC benchmarks, and outline how to generalize beyond MC via task-specific scoring functions for long-form outputs.", "result": "CAT yields CAR curves and CORE scores that expose how different LLMs trade off raw accuracy versus consistency under perturbed inputs. The experiments show that models with similar benchmark accuracy can differ markedly in consistency behavior and robustness, and that the proposed metrics provide more nuanced insights than accuracy alone.", "conclusion": "Accuracy and consistency should be evaluated both independently and jointly. CAT, via CAR curves, MCA, and CORE, offers a principled, extensible framework to study their interaction, giving a more informative and robust characterization of LLM behavior on MC tasks, with a pathway to long-form evaluations as well."}}
{"id": "2512.24077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24077", "abs": "https://arxiv.org/abs/2512.24077", "authors": ["Chunhui Wan", "Xunan Dai", "Zhuo Wang", "Minglei Li", "Yanpeng Wang", "Yinan Mao", "Yu Lan", "Zhiwen Xiao"], "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm", "comment": null, "summary": "The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike \"blind\" mutation operators, LoongFlow integrates LLMs into a cognitive \"Plan-Execute-Summarize\" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.", "AI": {"tldr": "LoongFlow is a self-evolving LLM-based agent framework that uses structured reasoning and evolutionary memory to discover high-quality algorithms and ML pipelines more efficiently than existing evolutionary methods.", "motivation": "Traditional evolutionary approaches for turning LLMs into self-improving agents suffer from lack of structured reasoning, premature convergence, and poor exploration in large, high-dimensional code spaces. These issues limit solution quality and make the process computationally expensive. The paper aims to design a framework that injects explicit reasoning into the evolutionary loop and better balances exploration and exploitation, thereby enabling more efficient and reliable autonomous algorithm and pipeline discovery.", "method": "The authors propose LoongFlow, a self-evolving agent framework built around a cognitive Plan-Execute-Summarize (PES) paradigm where LLMs guide evolution through structured reasoning rather than random or \u201cblind\u201d mutations. They introduce a hybrid evolutionary memory system that combines Multi-Island evolutionary models, MAP-Elites for niche preservation and diversity, and adaptive Boltzmann selection to dynamically balance exploration and exploitation. This framework is instantiated in two concrete agents: a General Agent for discovering algorithms and an ML Agent for optimizing ML pipelines. Evolutionary search is thus mapped onto a reasoning-heavy process with long-term architectural coherence maintained through memory and diversity mechanisms.", "result": "On the AlphaEvolve benchmark and multiple Kaggle competitions, LoongFlow achieves state-of-the-art results. It outperforms strong baselines such as OpenEvolve and ShinkaEvolve by up to 60% in evolutionary efficiency (e.g., better solutions per unit of compute or iterations) and discovers superior solutions in terms of algorithmic quality or pipeline performance.", "conclusion": "LoongFlow demonstrates that integrating structured LLM reasoning and a carefully designed evolutionary memory system into evolutionary search can substantially improve both efficiency and solution quality in self-improving agents. The framework represents a significant advance toward autonomous scientific discovery, enabling expert-level algorithm and pipeline generation with reduced computational overhead and better robustness against premature convergence."}}
{"id": "2512.24113", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24113", "abs": "https://arxiv.org/abs/2512.24113", "authors": ["Jiaxin Hu", "Tao Wang", "Bingsan Yang", "Hongrun Wang"], "title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation", "comment": "9 pages, 6 figures", "summary": "Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent \"Black-Box\" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.", "AI": {"tldr": "The paper introduces CogRec, a cognitive recommender agent that combines Large Language Models with the Soar cognitive architecture to achieve accurate, explainable, and continually learning recommendations.", "motivation": "Existing LLM-based recommenders are powerful but untrustworthy due to black-box behavior, hallucinations, and weak online learning, while cognitive architectures like Soar are interpretable but costly to acquire knowledge. The paper aims to build a recommender that is both accurate and transparent, and that can efficiently learn online without extensive manual rule engineering.", "method": "CogRec uses Soar as the central symbolic reasoning engine and an LLM as a knowledge source. The agent runs in a Perception-Cognition-Action (PCA) cycle: it initializes Soar\u2019s working memory and production rules via the LLM, performs reasoning to generate recommendations, and when it reaches an impasse, it queries the LLM for a reasoned solution. This LLM output is converted into new Soar production rules using chunking, enabling incremental, symbolic knowledge growth. The system is evaluated on three public recommendation datasets.", "result": "Experiments on three public datasets show that CogRec outperforms baselines in recommendation accuracy, provides more interpretable explanations, and handles long-tail items more effectively than comparison methods.", "conclusion": "By tightly integrating LLMs with the Soar cognitive architecture, CogRec delivers a recommender that is accurate, explainable, and capable of robust online learning. The approach mitigates LLM opacity and hallucinations while avoiding the manual knowledge-engineering burden of traditional cognitive architectures, suggesting a promising hybrid direction for future recommender systems and cognitive agents."}}
{"id": "2512.23713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23713", "abs": "https://arxiv.org/abs/2512.23713", "authors": ["Jahidul Islam", "Md Ataullha", "Saiful Azad"], "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents", "comment": "6 Pages", "summary": "LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\\% on the development set and 71.6\\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.", "AI": {"tldr": "The paper proposes BanglaCodeAct, an agent-based framework using open-source multilingual LLMs to generate, test, and refine Python code from Bangla natural language instructions, achieving state-of-the-art performance on Bangla NL2Code benchmarks.", "motivation": "While large language models perform well on code generation from English prompts, their effectiveness in low-resource languages like Bangla is limited. There is a lack of robust systems and benchmarks specifically targeting Bangla-to-Python code generation, which hinders access to advanced coding assistance for Bangla speakers.", "method": "The authors introduce BanglaCodeAct, an agent-based framework that operates in a Thought-Code-Observation loop. It uses multi-agent prompting and iterative self-correction with an open-source multilingual LLM to dynamically generate, execute, and refine Python code based on Bangla instructions. They benchmark several small-parameter open-source LLMs within this framework on the Bangla subset of the mHumanEval dataset for natural language to code (NL2Code) tasks.", "result": "Among the evaluated models, Qwen3-8B combined with the BanglaCodeAct framework achieves the best performance, with pass@1 scores of 94.0% on the development set and 71.6% on a blind test set for Bangla-to-Python code generation tasks.", "conclusion": "BanglaCodeAct sets a new performance benchmark for Bangla-to-Python translation and demonstrates that agent-based reasoning and iterative self-correction substantially improve code generation reliability in low-resource languages. The open-source experimental scripts further support reproducibility and future research in this area."}}
{"id": "2512.24156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24156", "abs": "https://arxiv.org/abs/2512.24156", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Benjamin Ultan Cowley"], "title": "Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks", "comment": null, "summary": "We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.", "AI": {"tldr": "A training-free, graph-based agent systematically explores ARC-AGI-3 interactive tasks using vision and explicit state graphs, outperforming LLM agents without any learning.", "motivation": "Existing large language models struggle with ARC-AGI-3 interactive reasoning tasks that require hypothesis formation, experimentation, and tracking of game mechanics over time. There is a need for a strong, non-learning baseline that can systematically explore such environments, handle sparse feedback, and reveal what structured, symbolic reasoning can achieve without training.", "method": "The authors propose a training-free agent that combines visual frame segmentation with graph-structured state-space exploration. It (1) processes each frame to segment it into meaningful visual components, (2) assigns priorities to potential actions based on visual salience, and (3) builds and maintains a directed graph whose nodes are discovered states and edges are transitions induced by actions. The agent tracks visited states and already-tested actions and uses shortest-path reasoning on the graph to select actions that lead quickly to untested state\u2013action pairs, ensuring systematic coverage of the state space.", "result": "On the ARC-AGI-3 Preview Challenge (52 levels across six games), the method solves a median of 30 levels and ranks 3rd on the private leaderboard, substantially outperforming leading LLM-based agents on the same benchmark.", "conclusion": "Explicit graph-structured exploration with careful state tracking and action prioritization can be a strong baseline for interactive reasoning tasks, even without any learning. This suggests that systematic symbolic exploration is crucial in sparse-feedback environments and highlights current limitations of LLMs in capturing and exploiting task dynamics in such settings."}}
{"id": "2512.23714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23714", "abs": "https://arxiv.org/abs/2512.23714", "authors": ["Tingwei Xie", "Tianyi Zhou", "Yonghong Song"], "title": "PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents", "comment": "5 pages, 4 figures", "summary": "We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at https://github.com/KevinYuLei/PharmaShip.", "AI": {"tldr": "PharmaShip is a Chinese scanned pharmaceutical document dataset that benchmarks text-layout models under noisy OCR, supporting entity recognition, relation extraction, and reading order prediction, and shows that combining pixel, geometry, and reading-order cues yields the most robust performance.", "motivation": "Existing text-layout models are mostly evaluated on relatively clean, homogeneous document benchmarks that do not reflect real-world, noisy, and safety-critical domains like pharmaceutical logistics. There is a need for a controlled, reproducible dataset that stresses models with noisy OCR, diverse templates, and requires both structure understanding and sequence reasoning, so that different architectural biases (pixels, geometry, reading order) can be rigorously compared.", "method": "The authors construct PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents, annotated for three tasks: sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP). They design an entity-centric evaluation protocol to ensure fair comparison across architectures and reduce confounds. They benchmark five baseline models from pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their RORE-enhanced variants), standardizing preprocessing, data splits, and training setups. They further incorporate reading-order-oriented regularization and investigate the effect of longer positional coverage on model performance and robustness.", "result": "Experiments demonstrate that pixel information and explicit geometry each provide useful but incomplete inductive biases; neither is sufficient alone for robust performance. Adding reading-order-oriented regularization consistently improves SER and entity linking performance and yields the strongest configuration. Extending positional coverage helps stabilize predictions on late pages and mitigates truncation artifacts. While reading order prediction is relatively accurate at the word level, it remains difficult at the segment level due to ambiguous boundaries and long-range crossing relations.", "conclusion": "PharmaShip provides a rigorous, reproducible benchmark for document understanding in the pharmaceutical shipping domain, especially under noisy OCR and heterogeneous layouts. The findings underscore that sequence-aware (reading-order) constraints are an important, transferable bias for modeling document structure, and that combining pixel, geometric, and reading-order cues leads to more robust text-layout models. The dataset and benchmark are made publicly available to facilitate further research."}}
{"id": "2512.24189", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.24189", "abs": "https://arxiv.org/abs/2512.24189", "authors": ["Yankai Jiang", "Wenjie Lou", "Lilong Wang", "Zhenyu Tang", "Shiyang Feng", "Jiaxuan Lu", "Haoran Sun", "Yaning Pan", "Shuang Gu", "Haoyang Su", "Feng Liu", "Wangxu Wei", "Pan Tan", "Dongzhan Zhou", "Fenghua Ling", "Cheng Tan", "Bo Zhang", "Xiaosong Wang", "Lei Bai", "Bowen Zhou"], "title": "SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents", "comment": null, "summary": "We introduce SCP: the Science Context Protocol, an open-source standard designed to accelerate discovery by enabling a global network of autonomous scientific agents. SCP is built on two foundational pillars: (1) Unified Resource Integration: At its core, SCP provides a universal specification for describing and invoking scientific resources, spanning software tools, models, datasets, and physical instruments. This protocol-level standardization enables AI agents and applications to discover, call, and compose capabilities seamlessly across disparate platforms and institutional boundaries. (2) Orchestrated Experiment Lifecycle Management: SCP complements the protocol with a secure service architecture, which comprises a centralized SCP Hub and federated SCP Servers. This architecture manages the complete experiment lifecycle (registration, planning, execution, monitoring, and archival), enforces fine-grained authentication and authorization, and orchestrates traceable, end-to-end workflows that bridge computational and physical laboratories. Based on SCP, we have constructed a scientific discovery platform that offers researchers and agents a large-scale ecosystem of more than 1,600 tool resources. Across diverse use cases, SCP facilitates secure, large-scale collaboration between heterogeneous AI systems and human researchers while significantly reducing integration overhead and enhancing reproducibility. By standardizing scientific context and tool orchestration at the protocol level, SCP establishes essential infrastructure for scalable, multi-institution, agent-driven science.", "AI": {"tldr": "The paper presents SCP, a protocol and architecture for standardizing how autonomous agents discover and orchestrate scientific tools, data, and instruments across institutions, enabling scalable, reproducible, agent-driven science.", "motivation": "Current scientific workflows are fragmented: tools, datasets, models, and lab instruments live on different platforms with incompatible interfaces, making it hard for AI agents and humans to discover, invoke, and combine them into end-to-end experiments, especially across institutions. This fragmentation slows scientific discovery, increases integration overhead, and limits reproducibility and collaboration in multi-agent, multi-lab settings. The authors aim to provide a common protocol and infrastructure so heterogeneous AI systems and human researchers can securely coordinate complex, traceable scientific workflows at scale.", "method": "The authors design SCP (Science Context Protocol) as an open, universal specification for describing and invoking scientific resources of many types (software tools, models, datasets, physical instruments). On top of this protocol they build a secure service architecture with a centralized SCP Hub and federated SCP Servers that manage the full lifecycle of experiments (registration, planning, execution, monitoring, archival). The architecture enforces fine-grained authentication and authorization, orchestrates workflows across computational and physical labs, and provides standardized, traceable experiment execution. They implement a discovery platform based on SCP that exposes over 1,600 tool resources to both human researchers and autonomous agents.", "result": "Using SCP, the authors construct a large-scale scientific discovery platform where more than 1,600 heterogeneous tools and resources are integrated under the common protocol. They demonstrate that SCP allows AI agents and human researchers to securely discover, invoke, and compose these resources across different platforms and institutions. Across various use cases, SCP reduces integration overhead, supports secure large-scale collaboration, and enables reproducible, end-to-end experimental workflows that span both digital and physical laboratory environments.", "conclusion": "SCP establishes a protocol-level standard and corresponding service architecture that together form critical infrastructure for scalable, agent-driven scientific discovery. By unifying how scientific resources are described and invoked, and by managing the full experiment lifecycle with secure, traceable orchestration, SCP enables multi-institution collaboration between heterogeneous AI systems and humans while enhancing reproducibility and reducing integration friction. The work positions SCP as a foundational layer for a global network of autonomous scientific agents and tools."}}
{"id": "2512.23716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23716", "abs": "https://arxiv.org/abs/2512.23716", "authors": ["Toshiyuki Shigemura"], "title": "Noise-Driven Persona Formation in Reflexive Neural Language Generation", "comment": "324 pages, 9 figures (Figure 7 intentionally skipped), with Appendices A-I. This manuscript presents a computational framework for noise-driven persona formation in neural language generation, analyzing 152 generation cycles using GPT-5.1 with stochastic noise seeds generated by Microsoft Copilot. Primary category: cs.CL", "summary": "This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p < 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.", "AI": {"tldr": "Introduces the Luca-Noise Reflex Protocol (LN-RP) to study how adding stochastic noise to LLM generation leads to stable emergent personas and phase transitions in behavior.", "motivation": "To systematically and reproducibly study how noise and reflexive generation dynamics in large language models give rise to emergent persona-like behaviors and long-range linguistic coherence, something that is usually anecdotal and poorly formalized.", "method": "Inject stochastic noise seeds into the initial generation state of an LLM and run 152 generation cycles, analyzing linguistic behavior, entropy signatures, and transitions between behavioral modes to detect stable persona states and noise-induced phase transitions.", "result": "Identified three stable persona modes characterized by distinct entropy signatures; showed that external noise can reliably trigger phase transitions between these modes, and that each mode exhibits consistent persona retention with statistically significant differences between modes (p < 0.01).", "conclusion": "LN-RP offers a reproducible computational framework for probing reflexive generation dynamics, emergent persona behavior, and long-range coherence in LLMs, enabling controlled experiments on how noise shapes model behavior over extended generations."}}
{"id": "2512.23717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23717", "abs": "https://arxiv.org/abs/2512.23717", "authors": ["Shenzhe Zhu"], "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate", "comment": null, "summary": "Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.", "AI": {"tldr": "The paper proposes HarmTransform, a multi-agent debate framework that rewrites harmful queries into covert, more stealthy forms while preserving harmful intent, to generate better safety training data for LLMs.", "motivation": "Existing LLM safety mechanisms mainly focus on clearly harmful content and struggle with queries where malicious intent is hidden through subtle, benign-looking rephrasings. There is a lack of systematic methods and training data that capture these covertly harmful queries, creating a safety gap that attackers can exploit. The paper aims to fill this gap by automatically generating such disguised harmful queries for alignment and evaluation.", "method": "The authors design HarmTransform, a multi-agent debate setup where multiple LLM agents iteratively critique and refine transformations of an initially harmful query. Each round, agents propose rewrites aimed at preserving the underlying harmful intent while making the surface form more benign and harder for safety filters to detect. Through repeated critique\u2013refine cycles, the framework converges toward high-quality covert transformations. The generated transformations are then evaluated against baselines for effectiveness and stealth, and analyzed for properties such as topic drift and complexity.", "result": "Empirical experiments show that HarmTransform produces covert harmful query transformations that are significantly more effective than baseline approaches at evading detection while retaining harmful intent. Quantitative and qualitative evaluations indicate that the multi-agent debate improves the stealth and quality of transformations, though it sometimes leads to undesirable artifacts like topic shifts and overly complex phrasings.", "conclusion": "HarmTransform demonstrates that multi-agent debate is a powerful tool for systematically generating covertly harmful queries, which can enrich safety training datasets and stress-test LLM defenses. However, the same debate process can introduce issues such as topic drift and unnecessary complexity, so it is not a silver bullet. The work suggests that carefully managed debate-based generation can improve safety alignment but must be complemented with mechanisms to control its side effects."}}
{"id": "2512.24263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24263", "abs": "https://arxiv.org/abs/2512.24263", "authors": ["Lijun Zhang", "Lin Li", "Wei Wei", "Yajie Qi", "Huizhong Song", "Jun Wang", "Yaodong Yang", "Jiye Liang"], "title": "Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment", "comment": null, "summary": "When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.", "AI": {"tldr": "The paper introduces Risk-aware Stepwise Alignment (RSA), a method that aligns language models with safety constraints by optimizing at the token level using nested risk measures, reducing both overall risk and rare catastrophic behaviors while preserving helpfulness.", "motivation": "Existing safety alignment methods for language models, such as Safe RLHF and SACPO, are primarily risk-neutral. They do not adequately control the risks from large deviations from a reference policy and are weak at handling rare but catastrophic harmful outputs. There is a need for an alignment approach that explicitly accounts for risk, especially tail risks, during optimization.", "method": "The authors propose Risk-aware Stepwise Alignment (RSA), which casts safety alignment as a token-level, risk-aware constrained policy optimization problem. RSA uses nested risk measures to derive token-level policy updates, and performs a stepwise alignment procedure to adjust the policy while controlling risk at each step. This explicitly incorporates risk aversion over both model shift and low-probability high-impact harmful behaviors.", "result": "Experiments show that RSA maintains or improves helpfulness compared to baselines, while achieving stronger safety guarantees. In particular, it significantly reduces tail risks, i.e., the frequency and severity of low-probability but high-impact unsafe responses, compared with risk-neutral alignment methods.", "conclusion": "Risk-aware Stepwise Alignment (RSA) offers a principled way to integrate risk sensitivity into LM safety alignment. By optimizing token-level policies with nested risk measures, RSA controls model shift and suppresses rare but catastrophic harmful behaviors, delivering language models that remain helpful yet safer and more robust to tail risks, with supporting theoretical and empirical evidence."}}
{"id": "2512.23722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23722", "abs": "https://arxiv.org/abs/2512.23722", "authors": ["Adam Kamel", "Tanish Rastogi", "Michael Ma", "Kailash Ranganathan", "Kevin Zhu"], "title": "Emergent World Beliefs: Exploring Transformers in Stochastic Games", "comment": "Accepted at NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.", "AI": {"tldr": "The paper studies whether a GPT-style model trained on poker histories internally represents hidden game states (beliefs) in a partially observable, stochastic environment.", "motivation": "While prior work showed that LLMs can form internal world models in games with perfect information, it is unclear if they can do the same in incomplete-information, stochastic settings like poker. Understanding this would clarify whether LLMs can implicitly learn belief states and probabilistic structure without explicit supervision.", "method": "The authors pretrain a transformer-based GPT model on large-scale Poker Hand History (PHH) data. They then probe the model\u2019s internal activations with mainly nonlinear probes to see if quantities such as hand ranking and equity (winning probability) can be decoded. They compare these decoded quantities to theoretical belief states from a POMDP formulation of Texas Hold\u2019em.", "result": "Probing reveals that the model encodes both deterministic game structure (e.g., hand ranks) and stochastic features (e.g., equity) within its hidden representations, even though these concepts were never directly supervised. The decoded representations show meaningful correlations with theoretically computed belief states for the poker environment.", "conclusion": "Transformer LLMs trained on poker histories implicitly learn internal representations that resemble belief states in a partially observable, stochastic environment. These findings extend prior results from perfect-information games and suggest that LLMs naturally develop structured internal models of uncertainty and hidden information when trained on suitable sequential data."}}
{"id": "2512.24461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24461", "abs": "https://arxiv.org/abs/2512.24461", "authors": ["Seohui Bae", "Jeonghye Kim", "Youngchul Sung", "Woohyung Lim"], "title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents", "comment": null, "summary": "In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.", "AI": {"tldr": "A test-time adaptive LLM agent that refines an explicit belief state to align with latent world states without extra training.", "motivation": "LLM agents operating in partially observable environments often need adaptation at test time to infer hidden states. Existing approaches rely on gradient-based finetuning, heavy inference-time scaling like long prompts or retrieval, or lack structured belief tracking, leading to high overhead and suboptimal alignment with the true underlying environment. The paper aims to create a more efficient, structured, and training-free way for LLM agents to reason under partial observability.", "method": "They design an external structured belief representation over environment states that the agent maintains and updates during interaction. At each step, the agent updates its belief with action-conditioned observations and plans actions by maximizing predicted information gain over this belief space. Information gain is approximated with a lightweight LLM-based surrogate model instead of costly exact computation or training. They also introduce a new reward function that measures how well the posterior belief matches the true hidden environment configuration, used for evaluation of world alignment.", "result": "In experiments, their agent achieves better alignment with latent world states than inference-time scaling baselines, including prompt-augmented and retrieval-enhanced LLMs. It does so with significantly lower integration and computational overhead, demonstrating the benefits of explicit belief tracking and information-gain-based exploration without extra training.", "conclusion": "Posterior-guided belief refinement with an explicit belief state and information-gain-driven action selection enables test-time adaptive LLM agents to reason more effectively under partial observability. The proposed method improves world-state alignment compared to typical inference-time scaling tricks, while avoiding gradient-based updates and remaining lightweight to integrate, indicating a promising direction for efficient, structured LLM agents in complex environments."}}
{"id": "2512.23732", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23732", "abs": "https://arxiv.org/abs/2512.23732", "authors": ["Anwar Alajmi", "Gabriele Pergola"], "title": "When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection", "comment": null, "summary": "Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \\textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with a +2.72\\% improvement in F1 on the EXIST 2025 Task 1.1, and a gains of +4.48\\% and +1.30\\% on the EDOS Tasks A and B, respectively.", "AI": {"tldr": "The paper proposes a two-stage framework combining robust training and selective, multi-persona reasoning at inference to better detect subtle, ambiguous sexist content online.", "motivation": "Sexist content online is often subtle, context-dependent, and intertwined with linguistic, psychological, legal, and cultural factors, leading to inconsistent annotations, label scarcity, and class imbalance. These issues cause unstable decision boundaries and make fine-tuned models miss underrepresented forms of harm. There is a need for a design that jointly handles underrepresentation, noise, and conceptual ambiguity in both data and model predictions.", "method": "The authors introduce a two-stage framework. In the training stage, they adapt supervision to scarce and noisy data using class-balanced focal loss, class-aware batching, and post-hoc decision threshold calibration to mitigate label imbalance and noisy labels. In the inference stage, they employ a dynamic routing mechanism that sends high-confidence instances to a standard classifier, while escalating low-confidence, ambiguous cases to a Collaborative Expert Judgment (CEJ) module. The CEJ module prompts multiple personas to generate reasoning and uses a judge model to aggregate these perspectives into a final decision.", "result": "The framework achieves state-of-the-art performance on multiple sexism detection benchmarks. Specifically, it improves F1 by +2.72% on EXIST 2025 Task 1.1 and yields gains of +4.48% and +1.30% on the EDOS Tasks A and B, respectively, compared to previous best methods.", "conclusion": "Explicitly modeling and addressing underrepresentation, label noise, and conceptual ambiguity through a combination of robust training procedures and selective, reasoning-based inference improves the detection of subtle sexist content. The proposed two-stage framework, including the CEJ module, yields more reliable and accurate judgments, especially on ambiguous or borderline cases, and sets new state-of-the-art results on several benchmarks."}}
{"id": "2512.24497", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.24497", "abs": "https://arxiv.org/abs/2512.24497", "authors": ["Basile Terver", "Tsung-Yen Yang", "Jean Ponce", "Adrien Bardes", "Yann LeCun"], "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "comment": null, "summary": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.", "AI": {"tldr": "The paper studies and improves a class of world models (JEPA-WMs) that perform planning in a learned representation space, and shows a tuned variant that outperforms existing baselines on navigation and manipulation tasks.", "motivation": "AI agents struggle to solve diverse physical tasks and generalize to unseen tasks and environments. While world models plus planning are a promising approach, it is unclear which design choices (architecture, objective, planning algorithm) make representation-space planning (JEPA-style world models) actually work best. There is a need for a systematic comparison within this family to identify effective configurations.", "method": "The authors define and unify a family of methods they call JEPA-WMs: world models trained in a joint embedding predictive architecture style, used for planning in the latent representation space. They conduct a comprehensive ablation-style study over several components: model architectures, training objectives, and planning algorithms. They run experiments both in simulated environments and with real robotic data on navigation and manipulation tasks, testing different combinations of these components. Based on empirical findings, they synthesize an improved JEPA-WM configuration.", "result": "The study identifies which architectural choices, objectives, and planning strategies most strongly influence planning success for JEPA-style world models. The final proposed model, built from the best-performing components, surpasses two strong baselines\u2014DINO-WM and V-JEPA-2-AC\u2014on both navigation and manipulation benchmarks, demonstrating better generalization and task performance.", "conclusion": "Planning in representation space with JEPA-style world models can be made more effective than existing approaches when key design choices are carefully tuned. A principled, empirical analysis of architecture, objective, and planner leads to a JEPA-WM that outperforms prior state-of-the-art world models on diverse physical tasks. The released code, data, and checkpoints enable further research on representation-based planning and generalist robotic agents."}}
{"id": "2512.23739", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23739", "abs": "https://arxiv.org/abs/2512.23739", "authors": ["Michaela Levi-Richter", "Reuth Mirsky", "Oren Glickman"], "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items", "comment": "Poster presented at the Israeli Seminar on Computational Linguistics 2025", "summary": "``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.\n  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.\n  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.\n  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.", "AI": {"tldr": "The paper proposes a benchmark and method for enabling robots to infer where non-visible household items are stored, using a hybrid vision\u2013language pipeline that approaches human performance.", "motivation": "Domestic service robots still struggle with tasks requiring commonsense reasoning about where everyday items are likely stored, especially when items are out of sight in drawers, cabinets, or closets. Existing advances in vision and manipulation do not address this gap, and there is no standardized benchmark to evaluate such cognitive capabilities in realistic home environments. The authors want to create a task and datasets that capture household storage organization and allow systematic comparison of different agent architectures on this kind of reasoning.", "method": "The authors define the Stored Household Item Challenge: given a household scene image and a queried item, the system must predict the item\u2019s most likely (often hidden) storage location. They construct two datasets: (1) a real-world evaluation set of 100 item\u2013image pairs from participants\u2019 kitchens with human-annotated ground truth; and (2) a larger development set of 6,500 item\u2013image pairs based on public kitchen images, annotated with storage polygons. To tackle the challenge, they propose NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that first performs structured scene understanding over the visual input, converts this into natural-language descriptions of spatial context and visible containers, and then prompts a large language model (e.g., GPT-4) to infer the most likely hidden storage location. NOAM is designed to be modular for integration into broader robotic systems.", "result": "On the Stored Household Item Challenge, NOAM is evaluated against several baselines: random selection, pure vision-language pipelines (e.g., Grounding-DINO + SAM), leading multimodal models (Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly outperforms these baselines in prediction accuracy and achieves performance close to human-level. The experiments demonstrate that the hybrid approach leveraging structured scene descriptions plus LLM inference yields stronger commonsense storage reasoning than either traditional vision-language pipelines or end-to-end multimodal models alone.", "conclusion": "The paper concludes that commonsense reasoning about non-visible object locations is a key missing capability for domestic service robots and that the Stored Household Item Challenge provides a meaningful benchmark to study it. The proposed NOAM pipeline shows that integrating explicit scene understanding with large language model reasoning can yield emergent commonsense behaviors and approach human performance on this task. This suggests practical guidelines for building cognitively capable agents for home environments and positions NOAM as a modular component for future robotic systems that must act effectively in cluttered, partially observed households."}}
{"id": "2512.24504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24504", "abs": "https://arxiv.org/abs/2512.24504", "authors": ["Zhiwei Wei", "Yuxing Liu", "Hua Liao", "Wenjia Xu"], "title": "Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments", "comment": "43 pages, 8 figures", "summary": "Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.", "AI": {"tldr": "They build an interactive benchmark to test how foundation model agents explore, memorize, and reason about symbolic maps, showing memory structure and reasoning prompts matter more than exploration strategy or raw model scale.", "motivation": "Existing tests of spatial abilities in foundation models mostly use static maps or purely text questions, ignoring that real spatial understanding is interactive and depends on exploration, memory, and experience over time. The authors want a principled way to probe how FM agents actually build and use internal spatial representations in map-like environments.", "method": "They design a grid-based, partially observable symbolic map world with roads, intersections, and POIs. FM agents navigate these maps step by step, receiving only local observations. After exploration, agents are evaluated on six types of spatial tasks. The framework systematically varies: (1) exploration strategies, (2) memory representations (e.g., unstructured vs sequential vs graph-like), and (3) reasoning schemes / prompting styles across several foundation models, then compares task performance to tease apart the functional contribution of each component.", "result": "Exploration strategies mostly change what experience is collected but have limited influence on final reasoning accuracy once enough coverage is obtained. In contrast, structured memory representations, especially sequential and graph-based memories, substantially improve performance on tasks that depend on map structure such as path planning. Different reasoning schemes (prompt designs) affect how well models can use their stored spatial knowledge, with more advanced prompts enabling better multi-step inference. The authors also find that beyond a certain capability threshold, increasing model scale or version yields diminishing returns for spatial reasoning in this setup.", "conclusion": "Interactive, map-based spatial understanding in FM agents depends more on how experiences are encoded and reasoned over than on exploration heuristics or brute-force model scaling. Well-designed memory structures and reasoning prompts are key to strong performance on structure-heavy spatial tasks, and future improvements in map-based reasoning will likely require specialized mechanisms for spatial representation and inference rather than just larger generic models."}}
{"id": "2512.23765", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23765", "abs": "https://arxiv.org/abs/2512.23765", "authors": ["Tiancheng Su", "Meicong Zhang", "Guoxiu He"], "title": "Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning", "comment": null, "summary": "Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.", "AI": {"tldr": "The paper introduces Entropy-Aware Speculative Decoding (EASD), a training-free enhancement to speculative decoding that uses entropy-based uncertainty to selectively reject draft tokens, improving both accuracy and in many cases surpassing the target LLM\u2019s performance while maintaining similar efficiency.", "motivation": "Standard speculative decoding speeds up LLM inference by using a draft model, but it is fundamentally constrained by the target LLM\u2019s performance: the more aligned the draft is with the target, the less room there is for improvement, and mispredicted but low-confidence tokens can still propagate and hurt reasoning quality. The authors want a decoding method that both preserves speed gains and can actually improve on the target model\u2019s inherent performance without retraining.", "method": "They propose Entropy-Aware Speculative Decoding (EASD), which augments standard SD with a dynamic entropy-based penalty at each decoding step. They compute the entropy of the sampling distributions of both the draft and target models to measure uncertainty, then examine the overlap of their top-N predictions. When both models are high-entropy (uncertain) and their high-probability predictions overlap substantially, the draft token is considered unreliable and is rejected, forcing the target LLM to re-sample. This acts as a verification gate for draft tokens and selectively blocks propagation of low-confidence errors while staying training-free and lightweight.", "result": "Across multiple reasoning benchmarks, EASD consistently yields better performance than existing speculative decoding techniques and, in most settings, even surpasses the base target LLM in reasoning accuracy, all while preserving efficiency comparable to standard SD. They also provide theoretical or empirical analysis showing that the added entropy-based checks do not significantly degrade throughput.", "conclusion": "Entropy-aware filtering of draft tokens in speculative decoding can both retain speedups and improve model quality beyond the target LLM itself. By using an uncertainty-driven, training-free penalty that rejects dubious draft tokens, EASD mitigates error propagation and turns speculative decoding from a pure acceleration technique into a way to enhance reasoning performance with minimal overhead."}}
{"id": "2512.24505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24505", "abs": "https://arxiv.org/abs/2512.24505", "authors": ["Samuel Golladay", "Majid Bani-Yaghoub"], "title": "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems", "comment": "7 pages, submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.", "AI": {"tldr": "The paper evaluates how three leading LLMs perform on underrepresented math competition problems, revealing distinct error patterns and especially poor geometry reasoning.", "motivation": "Most prior work on LLM mathematical reasoning relies on a few common benchmark datasets, which may not reflect the diversity and difficulty of real competition-style math problems. This limits the generalizability of conclusions about LLM mathematical reasoning abilities. The authors aim to fill this gap by testing LLMs on less-studied competition problems to better understand their limitations and error patterns.", "method": "The authors select problems from the Missouri Collegiate Mathematics Competition, focusing on Calculus, Analytic Geometry, and Discrete Mathematics. They prompt three LLMs (GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3) with these problems. Model outputs are compared to official correct solutions to measure accuracy by domain. The authors also perform a qualitative analysis of the step-by-step reasoning to categorize error types (computational, logical, approach-related, incomplete reasoning, rushed conclusions) and compare these patterns across models and problem types.", "result": "DeepSeek-V3 outperforms GPT-4o-mini and Gemini-2.0-Flash across all three domains (Calculus, Analytic Geometry, Discrete Mathematics) in both reasoning quality and final-answer correctness. All three models show particularly weak performance on geometry tasks. DeepSeek-V3\u2019s errors are mainly computational and logical slips; GPT-4o-mini more often chooses incorrect approaches or flawed reasoning strategies; Gemini-2.0-Flash often provides incomplete reasoning and premature conclusions.", "conclusion": "Underrepresented mathematics competition datasets reveal important, model-specific weaknesses in LLM mathematical reasoning that are not fully captured by standard benchmarks. While DeepSeek-V3 currently shows the strongest performance among the three models, all of them struggle notably with geometry and structured reasoning. The study underscores the need for more diverse and challenging benchmarks and for targeted improvements in LLM reasoning, especially for geometric problems."}}
{"id": "2512.23808", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.23808", "abs": "https://arxiv.org/abs/2512.23808", "authors": ["Xiaomi LLM-Core Team", ":", "Dong Zhang", "Gang Wang", "Jinlong Xue", "Kai Fang", "Liang Zhao", "Rui Ma", "Shuhuai Ren", "Shuo Liu", "Tao Guo", "Weiji Zhuang", "Xin Zhang", "Xingchen Song", "Yihan Yan", "Yongzhe He", "Cici", "Bowen Shen", "Chengxuan Zhu", "Chong Ma", "Chun Chen", "Heyu Chen", "Jiawei Li", "Lei Li", "Menghang Zhu", "Peidian Li", "Qiying Wang", "Sirui Deng", "Weimin Xiong", "Wenshan Huang", "Wenyu Yang", "Yilin Jiang", "Yixin Yang", "Yuanyuan Tian", "Yue Ma", "Yue Yu", "Zihan Zhang", "Zihao Yue", "Bangjun Xiao", "Bingquan Xia", "Bofei Gao", "Bowen Ye", "Can Cai", "Chang Liu", "Chenhong He", "Chunan Li", "Dawei Zhu", "Duo Zhang", "Fengyuan Shi", "Guoan Wang", "Hailin Zhang", "Hanglong Lv", "Hanyu Li", "Hao Tian", "Heng Qu", "Hongshen Xu", "Houbin Zhang", "Huaqiu Liu", "Jiangshan Duo", "Jianguang Zuo", "Jianyu Wei", "Jiebao Xiao", "Jinhao Dong", "Jun Shi", "Junhao Hu", "Kainan Bao", "Kang Zhou", "Linghao Zhang", "Meng Chen", "Nuo Chen", "Peng Zhang", "Qianli Chen", "Qiantong Wang", "Rang Li", "Shaohui Liu", "Shengfan Wang", "Shicheng Li", "Shihua Yu", "Shijie Cao", "Shimao Chen", "Shuhao Gu", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Xing Yong", "Xing Zhang", "Xu Wang", "Yifan Song", "Yihao Zhao", "Yingbo Zhao", "Yizhao Gao", "Yu Cheng", "Yu Tu", "Yudong Wang", "Zhaojun Huang", "Zhengju Tang", "Zhenru Lin", "Zhichao Song", "Zhipeng Xu", "Zhixian Zheng", "Zihan Jiang"], "title": "MiMo-Audio: Audio Language Models are Few-Shot Learners", "comment": null, "summary": "Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.", "AI": {"tldr": "MiMo-Audio scales next-token prediction pretraining on >100M hours of audio to build a 7B-parameter audio language model that exhibits strong few-shot, general-purpose audio understanding and generation abilities, achieving open-source state of the art on many benchmarks and generalizing to unseen tasks.", "motivation": "Most audio language models require task-specific fine-tuning and lack human-like ability to generalize to new audio tasks from a few examples or instructions. Inspired by GPT-3\u2019s text generalization via large-scale next-token pretraining, the authors aim to test whether similar scaling laws and emergent few-shot capabilities can be achieved in the audio domain, enabling a single model to handle diverse audio understanding and generation tasks without task-specific training.", "method": "They scale MiMo-Audio\u2019s pretraining to over 100 million hours of audio using a next-token prediction objective, training a 7B-parameter base model (MiMo-Audio-7B-Base). They systematically evaluate few-shot and zero-shot performance on varied audio tasks. In post-training, they perform instruction tuning using a curated, diverse corpus of audio instructions and responses, and explicitly incorporate \u201cthinking\u201d mechanisms\u2014chain-of-thought style reasoning\u2014into both audio understanding and generation to produce MiMo-Audio-7B-Instruct. They then benchmark these models against open- and closed-source systems on multiple speech intelligence, audio understanding, spoken dialogue, and TTS benchmarks.", "result": "MiMo-Audio-7B-Base attains state-of-the-art performance among open-source models on speech intelligence and audio understanding benchmarks. It exhibits emergent few-shot capabilities and generalizes to tasks not present in training data, including voice conversion, style transfer, and speech editing, and generates long, coherent speech continuations such as talk shows and debates. The instruction-tuned version, MiMo-Audio-7B-Instruct, achieves open-source SOTA on MMSU, MMAU, MMAR, MMAU-Pro, Big Bench Audio, MultiChallenge Audio, and instruct-TTS benchmarks, matching or surpassing closed-source competitors. Model weights and an evaluation suite are publicly released.", "conclusion": "Large-scale next-token prediction pretraining on massive audio corpora can endow a single 7B-parameter audio language model with strong few-shot generalization and broad audio capabilities, paralleling GPT-3\u2019s success in text. With additional instruction tuning and explicit thinking mechanisms, MiMo-Audio-7B-Instruct sets a new open-source state of the art in audio understanding, spoken dialogue, and speech generation, and can generalize to previously unseen audio tasks. The work suggests that general-purpose audio language models can be built by scaling data and applying generic pretraining plus instruction-tuning, rather than task-specific fine-tuning."}}
{"id": "2512.24532", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24532", "abs": "https://arxiv.org/abs/2512.24532", "authors": ["Amir Tahmasbi", "Sadegh Majidi", "Kazem Taram", "Aniket Bera"], "title": "From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning", "comment": null, "summary": "Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.", "AI": {"tldr": "They decompose spatial reasoning in LLMs into basic spatial physics plus a separate planning policy, using SFT for atomic spatial transformations and RL with LoRA for composing them in ASCII puzzle environments, achieving better, faster, and more stable performance than end-to-end RL.", "motivation": "LLMs are weak at spatial transformations and multi-step planning despite strong general language skills, which limits their use in tasks like navigation and puzzle solving. Existing approaches don\u2019t effectively separate learning basic spatial physics from learning high-level planning policies.", "method": "1) Supervised fine-tuning on elementary spatial transformations (rotation, translation, scaling) to endow an LLM with basic spatial \u2018physics\u2019. 2) Freezing this physics-aware model and training lightweight LoRA adapters under the GRPO reinforcement learning framework to learn policies that compose these atomic operations for multi-step planning in ASCII-art puzzle environments. They construct both Dynamic environments (with explicit state updates) and Static environments (requiring internal state tracking) in an ASCII-based RL setup, and analyze attention patterns to inspect spatial understanding.", "result": "Their two-stage approach consistently outperforms several baselines: the unmodified backbone LLM, the physics-aware model without RL composition, and end-to-end RL models trained from scratch. It works better in both Dynamic and Static ASCII environments, converges faster, and trains more stably than end-to-end RL baselines.", "conclusion": "Decoupling spatial reasoning into (a) learned atomic spatial transformations via SFT and (b) composition policies via lightweight RL adapters improves LLM spatial reasoning and planning. This modular approach yields better performance, efficiency, and stability than training full models end-to-end with RL, and fine-tuning indeed appears to enhance spatial understanding as seen in attention analyses."}}
{"id": "2512.24565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24565", "abs": "https://arxiv.org/abs/2512.24565", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang"], "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "comment": null, "summary": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "AI": {"tldr": "MCPAgentBench is a benchmark built from real-world MCP definitions to evaluate LLM agents\u2019 tool-use capabilities with simulated tools, distractor tools, and comprehensive metrics on effectiveness and efficiency.", "motivation": "Existing MCP-related evaluation sets depend on external MCP services and lack an explicit notion of task difficulty, making them hard to reproduce, control, and use for fine-grained diagnosis of tool-use abilities in LLM agents.", "method": "The authors design MCPAgentBench, a benchmark using real-world MCP definitions but instantiated as simulated MCP tools. They build a dataset of realistic tasks and run agents inside a dynamic sandbox that offers candidate tool lists containing both relevant tools and distractors. The framework measures how agents select and sequence tools and defines metrics that jointly capture task success and execution efficiency (e.g., steps or calls taken).", "result": "When evaluated on several recent mainstream LLMs acting as agents, the benchmark reveals substantial differences across models, especially on tasks requiring complex, multi-step tool invocation, indicating that current LLMs vary widely in their ability to plan and execute tool use via MCP.", "conclusion": "MCPAgentBench provides a more controlled, difficulty-aware, and comprehensive way to assess LLM agents\u2019 MCP-based tool-use skills, highlighting current limitations in complex multi-step tool usage and offering an open-source resource for future model and agent framework development."}}
{"id": "2512.23835", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23835", "abs": "https://arxiv.org/abs/2512.23835", "authors": ["Himel Ghosh"], "title": "Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms", "comment": "10 pages, 8 figures", "summary": "Automated bias detection in news text is heavily used to support journalistic analysis and media accountability, yet little is known about how bias detection models arrive at their decisions or why they fail. In this work, we present a comparative interpretability study of two transformer-based bias detection models: a bias detector fine-tuned on the BABE dataset and a domain-adapted pre-trained RoBERTa model fine-tuned on the BABE dataset, using SHAP-based explanations. We analyze word-level attributions across correct and incorrect predictions to characterize how different model architectures operationalize linguistic bias. Our results show that although both models attend to similar categories of evaluative language, they differ substantially in how these signals are integrated into predictions. The bias detector model assigns stronger internal evidence to false positives than to true positives, indicating a misalignment between attribution strength and prediction correctness and contributing to systematic over-flagging of neutral journalistic content. In contrast, the domain-adaptive model exhibits attribution patterns that better align with prediction outcomes and produces 63\\% fewer false positives. We further demonstrate that model errors arise from distinct linguistic mechanisms, with false positives driven by discourse-level ambiguity rather than explicit bias cues. These findings highlight the importance of interpretability-aware evaluation for bias detection systems and suggest that architectural and training choices critically affect both model reliability and deployment suitability in journalistic contexts.", "AI": {"tldr": "The paper compares how two transformer-based models detect bias in news using SHAP explanations, finding that architectural and training choices shape attribution patterns, false positives, and overall reliability.", "motivation": "Automated bias detection is widely used in journalism, but we lack understanding of how models make decisions and why they fail, limiting trust, reliability, and responsible deployment.", "method": "Conduct a comparative interpretability study of (1) a bias detector fine-tuned on BABE and (2) a domain-adapted RoBERTa also fine-tuned on BABE, using SHAP to obtain word-level attributions. Analyze attribution patterns across correct vs. incorrect predictions to see how linguistic bias cues are used and where errors come from.", "result": "Both models focus on similar evaluative language, but integrate these cues differently. The standard bias detector overweights internal evidence on false positives versus true positives, leading to systematic over-flagging of neutral content. The domain-adapted RoBERTa aligns attribution strengths better with prediction correctness and yields 63% fewer false positives. Errors stem from discourse-level ambiguity more than explicit bias terms.", "conclusion": "Interpretability-centered evaluation reveals critical differences in reliability between architectures. Domain adaptation plus careful training can substantially reduce false positives, and understanding attribution patterns is key for assessing and safely deploying bias detection systems in journalistic workflows."}}
{"id": "2512.24601", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24601", "abs": "https://arxiv.org/abs/2512.24601", "authors": ["Alex L. Zhang", "Tim Kraska", "Omar Khattab"], "title": "Recursive Language Models", "comment": "9 pages, 33 with Appendix", "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.", "AI": {"tldr": "They introduce Recursive Language Models (RLMs), an inference-time strategy that lets LLMs handle much longer inputs than their context windows by recursively calling themselves on pieces of the prompt, yielding better performance and similar or lower cost than standard methods.", "motivation": "Standard LLMs are limited by a fixed context window, making it difficult or impossible to process arbitrarily long prompts. Existing long-context scaffolding methods often degrade quality or incur high costs. The authors want a way to scale inference-time capabilities so that models can reason over very long inputs while keeping or improving quality and cost.", "method": "They conceptualize long prompts as an external environment and propose Recursive Language Models (RLMs), an inference strategy where the LLM is allowed to write and execute a program-like procedure that: (1) selectively reads and examines parts of the long prompt, (2) decomposes the overall task into subproblems, and (3) recursively calls the base LLM on smaller snippets. This effectively extends the usable context range by managing attention and computation over long inputs via recursive querying instead of a single monolithic context.", "result": "RLMs can successfully process inputs up to roughly 100x longer than the model\u2019s native context window. Across four different long-context benchmarks, RLMs not only maintain performance on extremely long inputs but also significantly outperform the base LLM and commonly used long-context scaffolding techniques, even when prompts are shorter. They achieve this while keeping the per-query computational and monetary cost comparable to, or lower than, existing approaches.", "conclusion": "Inference-time recursion can substantially extend the practical context length of LLMs without modifying model weights. Treating long prompts as an environment and letting the model control recursive calls over sub-snippets yields large gains in long-context performance at similar cost, suggesting that smarter inference procedures can be a powerful alternative to training ever-larger context-window models."}}
{"id": "2512.23836", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23836", "abs": "https://arxiv.org/abs/2512.23836", "authors": ["Dingmin Wang", "Ji Ma", "Shankar Kumar"], "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?", "comment": null, "summary": "The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.", "AI": {"tldr": "The paper studies how to better use retrieved documents with LLMs by adaptively splitting them into chunks and prompting sequentially, reducing irrelevant context while keeping needed information.", "motivation": "Expanded context windows in LLMs make it easy to stuff many retrieved passages into the prompt, but this also introduces lots of irrelevant material that can confuse the model and hurt question answering performance. The authors want to understand and improve how retrieval-augmented question answering should use large contexts efficiently and effectively, in terms of both accuracy and token cost.", "method": "They propose an adaptive prompting strategy: instead of feeding all retrieved information at once, they split the retrieved content into smaller chunks and prompt the LLM sequentially, asking it to answer the question using each chunk. The chunk size is a tunable parameter controlling the trade-off between including more potentially relevant info and limiting irrelevant noise. They evaluate this strategy on three open-domain QA datasets, comparing accuracy and token usage against standard single-shot prompting with full retrieved context, and analyze error patterns, especially when information is insufficient.", "result": "Experiments on three open-domain QA benchmarks show that the adaptive prompting strategy attains comparable accuracy to standard prompting approaches while consuming fewer tokens. Analysis of the model\u2019s behavior reveals that when given incomplete or insufficient evidence, the LLM frequently produces confident but incorrect answers instead of recognizing the lack of information and refusing to answer.", "conclusion": "Adaptive, chunk-based sequential prompting can make retrieval-augmented QA more token-efficient without sacrificing performance, by better balancing relevant and irrelevant information in the prompt. However, a key limitation of current LLMs is their tendency to hallucinate answers when evidence is lacking, rather than declining to respond. Addressing this limitation\u2014teaching models to reliably abstain when information is insufficient\u2014is identified as an important direction for future research."}}
{"id": "2512.24609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24609", "abs": "https://arxiv.org/abs/2512.24609", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "comment": "Accepted by IEEE ICFTIC 2025", "summary": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "AI": {"tldr": "They propose a reinforcement learning-augmented multi-agent LLM framework that improves cooperative performance in writing and coding tasks via a Dec-POMDP formulation, CTDE training, and a new GRPO algorithm, achieving faster task completion and better consistency than single- and multi-agent baselines.", "motivation": "Existing LLMs, while strong at individual language tasks, lack collaboration awareness and cannot reliably optimize global performance when multiple agents must coordinate. There is a gap in making LLM agents work together efficiently and coherently on complex, multi-step workflows like collaborative writing and coding.", "method": "Model multi-agent LLM collaboration as a decentralized partially observable Markov decision process (Dec-POMDP) and use a centralized training with decentralized execution (CTDE) paradigm. During training, agents have access to global signals. They introduce Group Relative Policy Optimization (GRPO), a policy-gradient style RL algorithm that jointly optimizes all agents\u2019 policies, guided by a simplified joint reward function that trades off task quality, speed, and coordination cost.", "result": "On collaborative writing and coding benchmarks, the framework triples task processing speed relative to strong single-agent baselines, achieves 98.7% structural and stylistic consistency in writing tasks, and reaches a 74.6% test pass rate in coding tasks. It consistently outperforms existing multi-agent LLM baselines across evaluated metrics.", "conclusion": "Reinforcement learning with a Dec-POMDP formulation and CTDE, instantiated via the GRPO algorithm and a carefully designed joint reward, enables LLM agents to collaborate effectively. This provides a practical and scalable pathway to reliable multi-agent LLM collaboration in complex, real-world workflows such as writing and programming."}}
{"id": "2512.23837", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23837", "abs": "https://arxiv.org/abs/2512.23837", "authors": ["Kaustubh Dhole"], "title": "Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation", "comment": null, "summary": "Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model's own generation process. We evaluate whether tokens extracted from intermediate layers can serve as effective adversarial perturbations for downstream evaluation tasks. We conduct experiments on argument quality assessment using the ArgQuality dataset, with LLaMA-3.1-Instruct-8B serving as both the generator and evaluator. Our results show that attention-based adversarial examples lead to measurable drops in evaluation performance while remaining semantically similar to the original inputs. However, we also observe that substitutions drawn from certain layers and token positions can introduce grammatical degradation, limiting their practical effectiveness. Overall, our findings highlight both the promise and current limitations of using intermediate-layer representations as a principled source of adversarial examples for stress-testing LLM-based evaluation pipelines.", "AI": {"tldr": "They generate adversarial text examples directly from intermediate attention-layer token predictions in LLMs and show these can reliably degrade evaluation performance while staying semantically similar, though some layers hurt grammar.", "motivation": "Mechanistic interpretability indicates that intermediate attention layers encode evolving token-level hypotheses, but it is unclear whether these internal predictions can be systematically exploited to create realistic adversarial examples for stress-testing LLM-based evaluators without relying on external prompts or gradients.", "method": "Use LLaMA-3.1-Instruct-8B as both generator and evaluator on the ArgQuality argument quality dataset; extract token distributions from various intermediate attention layers, sample or select high-probability alternative tokens as substitutions in the original inputs, and treat these modified texts as adversarial candidates; then measure the impact of these perturbations on downstream evaluation metrics and analyze semantic similarity and grammaticality across layers and token positions.", "result": "Attention-layer-based perturbations produce adversarial examples that cause measurable drops in evaluation performance while typically remaining semantically close to the originals, but the effectiveness and quality of adversarial examples depend strongly on which layer and token positions are used, with some choices leading to noticeable grammatical degradation.", "conclusion": "Intermediate-layer attention token distributions can serve as a principled and model-consistent source of adversarial examples for LLM evaluation pipelines, but careful selection of layers and token positions is necessary to avoid harming linguistic quality, and current limitations suggest further work is needed to refine these methods for practical robustness testing."}}
{"id": "2512.24613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24613", "abs": "https://arxiv.org/abs/2512.24613", "authors": ["Zheyu Shi", "Dong Qiu", "Shanlong Yu"], "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning", "comment": "Accepted by IEEE ITCA 2025", "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.", "AI": {"tldr": "The paper introduces a group-deliberation multi-agent conversational model that significantly improves complex multi-hop reasoning accuracy and consistency over single LLMs and existing multi-agent methods.", "motivation": "Single large language models struggle with complex, multi-hop reasoning, often hallucinate, and lack explicit mechanisms for factual verification, role specialization, and coherent aggregation of diverse reasoning paths. Existing multi-agent methods can be inefficient and unstable. The paper aims to build a more reliable and efficient architecture for complex reasoning by using structured collaboration among specialized agents and reinforcement learning optimization.", "method": "The authors design a three-level role division architecture: (1) an opinion generation agent that produces diverse reasoning paths and perspectives; (2) an evidence verification agent that retrieves external knowledge via a retrieval enhancement module and quantifies factual support for each reasoning path; and (3) a consistency arbitration (integration) agent that aggregates information and selects logically coherent, fact-supported conclusions. They introduce a self-game mechanism to expand multi-path reasoning trajectories, and they define a composite reward that jointly measures factual consistency and logical coherence. An improved proximal policy optimization (PPO) algorithm is then used to collaboratively train the agents under this composite reward.", "result": "On three benchmarks\u2014HotpotQA, 2WikiMultihopQA, and MeetingBank\u2014the model improves multi-hop reasoning accuracy by 16.8%, 14.3%, and 19.2% respectively compared with baselines, and increases consistency by 21.5%. It is also reported to be more efficient than mainstream multi-agent approaches, indicating better performance-cost trade-offs in complex reasoning tasks.", "conclusion": "A structured, role-based multi-agent conversational architecture with explicit evidence verification, self-game multi-path exploration, retrieval augmentation, and PPO-based collaborative training yields significant gains in accuracy, consistency, and efficiency for complex reasoning tasks. This demonstrates that group deliberation with specialized agents is an effective and stable alternative to relying on a single LLM or less-structured multi-agent systems for multi-hop reasoning."}}
{"id": "2512.23848", "categories": ["cs.CL", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23848", "abs": "https://arxiv.org/abs/2512.23848", "authors": ["Yukun Zhang", "Stefan Elbl Droguett", "Samyak Jain"], "title": "Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs", "comment": null, "summary": "This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.", "AI": {"tldr": "They improve financial numerical QA by adding finance-specific retrieval and domain-trained encoders, showing both neural-symbolic and LLM-based models can outperform prior FinQA baselines, with large LLMs benefiting most from external knowledge.", "motivation": "Financial numerical QA is difficult for LLMs because it requires both complex multi-step numerical reasoning and specialized financial domain knowledge, which general-purpose models lack. Prior FinQA systems underperform human experts, motivating methods that better inject domain knowledge and improve reasoning.", "method": "They build a multi-retriever RAG pipeline that pulls in external financial domain knowledge and internal question context, using a SecBERT-based encoder for domain-specific training. They evaluate both neural-symbolic models and prompt-based LLM generators, run ablation studies and error analysis, and compare against the top FinQA baseline.", "result": "Domain-specific training with the SecBERT encoder substantially boosts performance, enabling their best neural-symbolic model to surpass the prior top FinQA model. Their best prompt-based LLM with RAG achieves new SOTA with over 7% improvement, though still below human experts. They also quantify how model size and few-shot setting affect the trade-off between hallucinations and gains from external knowledge.", "conclusion": "Domain-specific training and multi-retriever RAG are highly effective for financial numerical QA, and modern LLMs optimized for few-shot learning show notably enhanced numerical reasoning. External knowledge helps more than it hurts for larger models, while smaller models are more prone to hallucinations, highlighting the importance of model size and training strategy in financial QA systems."}}
{"id": "2512.24615", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24615", "abs": "https://arxiv.org/abs/2512.24615", "authors": ["Yuchen Shi", "Yuzheng Cai", "Siqi Cai", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Zhijian Zhou", "Xiang Fei", "Chaofan Qiu", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Guocan Cai", "Yong Mao", "Yunsheng Wu", "Ke Li", "Xing Sun"], "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "comment": null, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "AI": {"tldr": "Youtu-Agent is a modular framework that can automatically generate, configure, and continually improve LLM agents via workflow/meta-agent generation and hybrid in-context + RL optimization, achieving SOTA performance on several agent benchmarks with open-weight models.", "motivation": "Current LLM agent frameworks are expensive to configure and hard to adapt: building good agents requires heavy manual tool integration and prompt engineering, and deployed agents remain static unless costly fine-tuning is done. The authors want a system that can automatically assemble and evolve agents over time, lowering human effort while improving robustness and performance in dynamic environments.", "method": "They design Youtu-Agent, a modular agent framework with a structured configuration system that separates execution environments, toolkits, and context management for flexible reuse and automatic synthesis. They propose two generation paradigms: (1) Workflow mode for standard tasks, and (2) Meta-Agent mode for complex or non-standard tasks that can automatically generate tool code, prompts, and configs. For learning, they add a hybrid optimization scheme: (a) an Agent Practice module that lets agents improve via in-context learning from accumulated experience without changing model weights, and (b) an Agent RL module that plugs into distributed training frameworks to run scalable, stable reinforcement learning on any Youtu-Agent end-to-end.", "result": "On WebWalkerQA and GAIA, Youtu-Agent reaches SOTA performance (71.47% and 72.8%) using open-weight models. The automated generation pipeline achieves over 81% success in synthesizing tools. The Practice module boosts AIME 2024 and 2025 performance by +2.7% and +5.4%. The Agent RL training runs 40% faster and yields steady gains, improving coding/reasoning and search capabilities by up to 35% and 21% respectively on math and general/multi-hop QA benchmarks with 7B LLMs.", "conclusion": "Youtu-Agent effectively automates both the construction and continual improvement of LLM agents. Its modular configuration, dual generation modes, and hybrid in-context plus RL optimization reduce manual engineering costs while achieving strong and scalable performance gains across diverse benchmarks, suggesting a practical path toward self-improving, open-weight LLM agents."}}
{"id": "2512.23941", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.23941", "abs": "https://arxiv.org/abs/2512.23941", "authors": ["Conrad Borchers", "Manit Patel", "Seiyon M. Lee", "Anthony F. Botelho"], "title": "Disentangling Learning from Judgment: Representation Learning for Open Response Analytics", "comment": "Short research paper accepted at Learning Analytics and Knowledge (LAK '26)", "summary": "Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.", "AI": {"tldr": "They propose a framework that separates what students write from how teachers tend to grade, using embeddings and rater-history priors to create auditable learning analytics about grading and student reasoning.", "motivation": "Automated scoring of open-ended student responses often mixes up actual student content with systematic rater biases and tendencies, making grade predictions opaque and potentially unfair. There is a need for methods that disentangle content-based evidence of understanding from teacher-specific grading patterns so that scoring can be transparent, auditable, and better aligned with evidence of student learning.", "method": "Using de-identified ASSISTments math responses, the authors: (1) model each teacher\u2019s grading history as a dynamic prior; (2) derive text representations from sentence embeddings; (3) apply centering and residualization to reduce prompt and teacher confounds; (4) train temporally-validated linear models that separately and jointly use rater priors and content embeddings; and (5) construct a projection method to highlight cases where different models disagree, enabling qualitative inspection of contentious or informative responses.", "result": "Teacher prior models strongly influence grade predictions and, when combined with content embeddings, yield the best performance (AUC \u2248 0.815). Content-only models perform above chance but substantially worse (AUC \u2248 0.626). Removing rater and prompt effects refines the residual content representation, preserving more useful embedding dimensions and exposing instances where semantic evidence indicates understanding despite surface differences in expression or style.", "conclusion": "The paper delivers a practical, analytics-first pipeline for scoring and analyzing open-ended responses that (a) explicitly separates rater tendencies from content signals, (b) uses embeddings as interpretable learning analytics rather than black-box features, and (c) supports reflection by teachers and researchers on where grading aligns\u2014or misaligns\u2014with evidence of student reasoning and learning."}}
{"id": "2512.24679", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.24679", "abs": "https://arxiv.org/abs/2512.24679", "authors": ["Pengcheng Xia", "Yixiang Huang", "Chengjin Qin", "Chengliang Liu"], "title": "Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions", "comment": "21 pages, 8 figures", "summary": "Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.", "AI": {"tldr": "They propose a domain-generalized, multi-modal fault diagnosis model that disentangles modality and domain factors, mixes cross-domain modalities, and adaptively fuses three modalities to stay accurate under unseen working conditions.", "motivation": "Real industrial machinery operates under diverse and changing conditions. Existing intelligent fault diagnosis models lose accuracy when tested on unseen working conditions, domain adaptation methods require target-domain data that may not be available, and most approaches only use a single sensing modality, missing complementary information from multiple sensors. There is a need for a method that can generalize across domains without target samples while fully exploiting multi-modal signals.", "method": "They design a multi-modal cross-domain mixed fusion model with a dual disentanglement framework. One disentanglement branch separates modality-invariant from modality-specific features; another separates domain-invariant from domain-specific representations. To improve robustness, they introduce a cross-domain mixed fusion strategy that randomly mixes modality information across domains, augmenting both modality and domain diversity. Additionally, they propose a triple-modal fusion mechanism that adaptively integrates heterogeneous information from three sensing modalities, enabling comprehensive multi-modal representation learning and enhanced domain generalization.", "result": "On induction motor fault diagnosis tasks under both unseen constant and time-varying working conditions, the proposed model consistently surpasses various advanced baselines. Extensive ablation experiments show that each core component\u2014dual disentanglement, cross-domain mixed fusion, and triple-modal fusion\u2014contributes significantly to performance gains and that multi-modal fusion clearly outperforms single-modal settings.", "conclusion": "A dual-disentanglement, cross-domain, triple-modal fusion architecture can achieve robust, domain-generalized fault diagnosis without requiring target-domain samples. By jointly disentangling modality and domain factors, mixing modalities across domains, and adaptively fusing three modalities, the method delivers superior and stable performance under unseen working conditions, demonstrating the value of multi-modal, domain-generalizable representation learning for real-world machinery health monitoring."}}
{"id": "2512.23959", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23959", "abs": "https://arxiv.org/abs/2512.23959", "authors": ["Chulun Zhou", "Chunkang Zhang", "Guoxin Yu", "Fandong Meng", "Jie Zhou", "Wai Lam", "Mo Yu"], "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "comment": "21 pages", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "AI": {"tldr": "The paper proposes HGMem, a hypergraph-based working memory for multi-step retrieval-augmented generation (RAG) that models higher-order relations among facts, leading to better global sense-making and reasoning than standard passive memory buffers.", "motivation": "Existing multi-step RAG systems use working memory mainly as passive storage: they accumulate retrieved facts to shorten inputs and help generate new sub-queries, but treat these facts as largely independent. This static, list-like memory ignores high-order correlations and compositions among facts, which are often critical for complex reasoning and global understanding. As a result, reasoning becomes fragmented and the model struggles with tasks that require integrating many pieces of information over long contexts. The authors are motivated to design a memory mechanism that can explicitly capture and evolve higher-order interactions among facts to support more coherent, global sense-making in multi-step RAG.", "method": "The authors introduce HGMem, a hypergraph-based memory mechanism for RAG. Instead of storing retrieved items as a flat set or sequence, HGMem represents memory as a hypergraph where hyperedges are memory units that can jointly connect multiple facts or thoughts. During multi-step RAG, as new information is retrieved and new intermediate reasoning is produced, HGMem incrementally constructs and updates this hypergraph, building higher-order interactions that reflect how pieces of information relate around the focal problem. This evolving hypergraph is then used to provide structured, proposition-like guidance to the LLM at each reasoning step, influencing both subsequent retrieval and generation. Implementation details include defining how nodes and hyperedges are created/updated, how to score and select relevant hyperedges, and how to serialize parts of the hypergraph back into prompts for the LLM.", "result": "Across several challenging benchmarks specifically designed to test global sense-making and complex multi-step reasoning, HGMem consistently improves the performance of multi-step RAG systems. The method substantially outperforms strong baseline memory designs that use more traditional, passive storage schemes. In-depth analyses indicate that HGMem yields more coherent reasoning chains, better integration of dispersed evidence, and stronger global understanding over extended contexts, as reflected in both quantitative metrics and qualitative inspection of reasoning traces.", "conclusion": "The paper concludes that treating working memory in RAG as an expressive, dynamic hypergraph rather than passive storage significantly enhances multi-step reasoning and global sense-making. By modeling higher-order interactions among facts, HGMem enables the system to form integrated knowledge structures that guide deeper reasoning in subsequent steps. The approach demonstrates robust gains across diverse tasks, suggesting that structured, evolvable memory representations are a promising direction for advancing complex reasoning in LLM-based systems."}}
{"id": "2512.24686", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24686", "abs": "https://arxiv.org/abs/2512.24686", "authors": ["Songqi Zhou", "Ruixue Liu", "Boman Su", "Jiazhou Wang", "Yixing Wang", "Benben Jiang"], "title": "BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis", "comment": null, "summary": "Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their \"black-box\" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a \"numerical-semantic\" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from \"passive detection\" to \"intelligent diagnosis\" for battery safety management.", "AI": {"tldr": "A hierarchical, interpretable battery fault diagnosis framework called BatteryAgent that combines physics-based features, classical ML, SHAP, and an LLM to move from binary detection to multi-type, explainable diagnosis.", "motivation": "Existing deep learning approaches for lithium-ion battery fault detection achieve high accuracy but are black-box, limited to binary classification, and provide little insight into root causes or maintenance guidance. There is a need for a method that is both accurate and interpretable, can handle multiple fault types, and supports actionable diagnosis for battery safety management.", "method": "BatteryAgent is a three-layer hierarchical framework. (1) Physical Perception Layer: extracts 10 mechanism-based features grounded in electrochemical principles to capture essential battery behavior with reduced dimensionality. (2) Detection and Attribution Layer: trains Gradient Boosting Decision Trees for fault detection and uses SHAP to quantify each feature\u2019s contribution to decisions. (3) Reasoning and Diagnosis Layer: uses an LLM-based agent that maps numerical SHAP attributions and feature values into a semantic space via a mechanism knowledge base, generating human-readable reports that include fault type, root cause explanations, and maintenance suggestions.", "result": "BatteryAgent improves classification robustness, especially on hard boundary samples, and achieves an AUROC of 0.986, surpassing current state-of-the-art methods. It also successfully produces interpretable outputs\u2014fault labels, attributions, and explanations\u2014rather than only binary decisions.", "conclusion": "By combining physics-based features, interpretable ML attribution, and LLM reasoning, BatteryAgent transforms conventional battery fault detection into multi-type, explainable, and actionable diagnosis. This represents a paradigm shift from passive detection to intelligent diagnosis for lithium-ion battery safety management and shows that LLMs can be effectively integrated with physical models for industrial health monitoring."}}
{"id": "2512.23966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23966", "abs": "https://arxiv.org/abs/2512.23966", "authors": ["Chen Zhang", "Yang Bai", "Jiahuan Li", "Anchun Gui", "Keheng Wang", "Feifan Liu", "Guanyu Wu", "Yuwei Jiang", "Defei Bu", "Li Wei", "Haihang Jing", "Hongyin Tang", "Xin Chen", "Xiangzhou Huang", "Fengcun Li", "Rongxiang Weng", "Yulei Qian", "Yifan Lu", "Yerui Sun", "Jingang Wang", "Yuchen Xie", "Xunliang Cai"], "title": "Efficient Context Scaling with LongCat ZigZag Attention", "comment": "10 pages, 3 figures, 3 tables", "summary": "We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.", "AI": {"tldr": "LoZA is a sparse attention mechanism that converts dense long-context models into more efficient ones while preserving capability.", "motivation": "Full attention in long-context language models is computationally expensive, limiting context length and practical deployment in scenarios like RAG and tool use.", "method": "They propose LongCat ZigZag Attention, a sparse attention pattern that can be plugged into existing full-attention models, apply it mid-training to a LongCat-Flash model, and evaluate in both prefill- and decode-intensive long-context settings.", "result": "LoZA yields substantial speed-ups in long-context processing, allowing the LongCat-Flash-Exp variant to handle sequences up to 1M tokens with lower compute while retaining performance.", "conclusion": "Sparse patterns such as LoZA can be used to retrofit and extend existing models into efficient long-context foundation models capable of million-token contexts and agentic reasoning over long horizons."}}
{"id": "2512.24829", "categories": ["cs.AI", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24829", "abs": "https://arxiv.org/abs/2512.24829", "authors": ["Emmanuel Fashae", "Michael Burke", "Leimin Tian", "Lingheng Meng", "Pamela Carreno-Medrano"], "title": "Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences", "comment": "Accepted to the 2026 ACM/IEEE International Conference on Human-Robot Interaction (HRI '26)", "summary": "Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.", "AI": {"tldr": "The paper proposes an interpretable, four-factor model of human preferences for arranging household objects and shows how to use it for robotic planning.", "motivation": "Existing robotic systems learn latent preference models for object arrangement from demonstrations, which predict human choices but provide little interpretability or psychological insight into why people arrange objects the way they do. The authors want a compact, explicit, and psychologically grounded representation of arrangement preferences that can both explain human decisions and be used in robot planning.", "method": "The authors define four interpretable constructs for object arrangement preferences: spatial practicality, habitual convenience, semantic coherence, and commonsense appropriateness. They design a self-report questionnaire to measure these constructs and validate it via an online study with 63 participants across kitchen and living room scenarios, assessing construct distinctiveness and explanatory power. They then incorporate these constructs as components of a reward or evaluation function in a Monte Carlo Tree Search (MCTS) planner and test whether human-derived preference parameters guide the planner to produce arrangements similar to participants\u2019 own arrangements.", "result": "The questionnaire data from 63 participants support that the four constructs are psychologically distinct and each contributes to explaining how people arrange objects in both kitchen and living room scenarios. When the four-factor preference model is integrated into an MCTS planner and parameterized using participant responses, the planner generates object arrangements that are judged to be reasonable and that closely match arrangements produced by the participants themselves.", "conclusion": "A four-factor, interpretable model\u2014spatial practicality, habitual convenience, semantic coherence, and commonsense appropriateness\u2014captures key aspects of human object arrangement preferences. This model can be operationalized within a planning algorithm such as MCTS to produce robot-generated arrangements that align well with human preferences, thereby improving both interpretability and practical deployment of household rearrangement robots."}}
{"id": "2512.23971", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23971", "abs": "https://arxiv.org/abs/2512.23971", "authors": ["Zhiming Lin", "Kai Zhao", "Sophie Zhang", "Peilai Yu", "Canran Xiao"], "title": "CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards", "comment": "AAAI'26 poster", "summary": "Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by enabling LLMs to correct their own mistakes. CEC-Zero synthesizes errorful inputs from clean text, computes cluster-consensus rewards via semantic similarity and candidate agreement, and optimizes the policy with PPO. It outperforms supervised baselines by 10--13 F$_1$ points and strong LLM fine-tunes by 5--8 points across 9 benchmarks, with theoretical guarantees of unbiased rewards and convergence. CEC-Zero establishes a label-free paradigm for robust, scalable CSC, unlocking LLM potential in noisy text pipelines.", "AI": {"tldr": "CEC-Zero is a zero-supervision RL framework that lets LLMs learn Chinese spelling correction by generating their own noisy data and optimizing with a consensus-based reward, outperforming supervised and fine-tuned baselines on many benchmarks.", "motivation": "Chinese spelling correction is crucial for real-world applications but current large language models and supervised systems struggle with unseen error patterns and require expensive labeled data. There is a need for a scalable, label-free approach that can robustly handle diverse and novel spelling errors while leveraging the power of LLMs.", "method": "CEC-Zero uses a zero-supervision reinforcement learning setup where an LLM generates synthetic spelling errors from clean text to create training data. It then evaluates candidate corrections using a cluster-consensus reward combining semantic similarity and agreement among candidates, giving a theoretically unbiased reward signal. The policy (LLM) is optimized with PPO to improve its correction ability without human-labeled pairs.", "result": "The framework achieves 10\u201313 F1 point gains over supervised baselines and 5\u20138 point gains over strong fine-tuned LLMs across nine Chinese spelling correction benchmarks, demonstrating clear performance and robustness improvements.", "conclusion": "CEC-Zero shows that label-free reinforcement learning with automatically generated errors and consensus-based rewards can train LLMs to perform robust Chinese spelling correction, providing a scalable alternative to annotation-heavy supervised approaches and better exploiting LLM capabilities for noisy text processing."}}
{"id": "2512.24834", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24834", "abs": "https://arxiv.org/abs/2512.24834", "authors": ["Marko Jojic", "Nebojsa Jojic"], "title": "GenZ: Foundational models as latent variable generators within traditional statistical models", "comment": null, "summary": "We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.", "AI": {"tldr": "GenZ is a hybrid model that uses frozen foundation models plus statistical learning over latent semantic features to capture dataset-specific patterns and improve prediction.", "motivation": "Foundation models like LLMs have strong general knowledge but often miss fine-grained, dataset-specific regularities needed for accurate prediction (e.g., local housing market quirks, niche user preferences). Purely statistical models, in turn, lack semantic interpretability and may struggle in cold-start or low-label regimes. There is a need to connect the semantic richness of foundation models with the predictive power and data adaptivity of statistical models, in a way that yields interpretable, dataset-specific features instead of relying only on generic domain knowledge from the foundation model.", "method": "They introduce GenZ, formulated as a generalized EM algorithm. The E-step discovers and refines semantic feature descriptions by contrasting groups of items where the current statistical model makes different kinds of errors. A frozen foundation model is prompted with these candidate features and items and asked to classify whether each item exhibits each feature; these outputs are treated as noisy observations of latent binary semantic features. The M-step then learns statistical relationships mapping these latent features to real-valued prediction targets. This iterative process jointly optimizes semantic feature descriptors and the statistical model parameters, effectively aligning interpretable, dataset-specific semantics with predictive performance.", "result": "On hedonic house price prediction using multimodal listing data, GenZ attains 12% median relative error, substantially outperforming a strong GPT-5 baseline that uses the foundation model\u2019s domain knowledge directly (38% error). On Netflix data, GenZ predicts collaborative-filtering-based movie embeddings using only semantic descriptions, achieving 0.59 cosine similarity, which is equivalent to what standard collaborative filtering would need about 4000 user ratings to reach. The learned semantic features capture nuanced patterns such as architectural attributes relevant to specific housing markets and franchise membership patterns shaping user preferences.", "conclusion": "GenZ successfully bridges foundation models and statistical prediction by discovering interpretable, dataset-specific semantic features instead of relying solely on generic domain knowledge. The approach yields large performance gains in both regression (house prices) and representation prediction (movie embeddings), and the learned features offer human-understandable insights into dataset-specific regularities. This suggests a promising direction for combining frozen foundation models with data-driven feature discovery for improved, interpretable predictions across domains."}}
{"id": "2512.23988", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23988", "abs": "https://arxiv.org/abs/2512.23988", "authors": ["Zhenyu Zhang", "Shujian Zhang", "John Lambert", "Wenxuan Zhou", "Zhangyang Wang", "Mingqing Chen", "Andrew Hard", "Rajiv Mathews", "Lun Wang"], "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process", "comment": null, "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.", "AI": {"tldr": "The paper introduces RISE, an unsupervised sparse auto-encoder framework that discovers latent \u201creasoning vectors\u201d in LLM activations, enabling interpretation and control of distinct reasoning behaviors without supervision or model retraining.", "motivation": "Existing analyses of LLM reasoning rely on human-defined, word-level concepts such as overthinking or reflection and supervised labeling of chain-of-thought traces. This is limited because it cannot cover the full range of possible reasoning behaviors, many of which are hard to define in token space, and it does not provide a systematic, mechanistic view of how these behaviors are represented inside the model.", "method": "The authors segment chain-of-thought outputs into sentence-level steps and collect the model\u2019s internal activations at this step level. They then train sparse auto-encoders (SAEs) on these activations so that each sparse latent dimension\u2014interpreted as a \u201creasoning vector\u201d\u2014captures a distinct, disentangled feature of the reasoning process. They analyze the SAE decoder columns to visualize and cluster these vectors, and perform targeted activation interventions along selected vectors at inference time to test whether they can amplify or suppress corresponding behaviors, all without retraining the base LLM.", "result": "The trained SAEs identify interpretable latent features corresponding to recognizable reasoning behaviors such as reflection and backtracking; these features occupy separable regions in the decoder column space. Intervening along these vectors controllably changes the reasoning trajectory, increasing or reducing specific behaviors. SAEs also capture structural properties like response length, separating long from short reasoning traces, and reveal novel, previously unsupervised behaviors, including vectors that modulate the model\u2019s expressed confidence. These interventions adjust confidence and behavior while preserving overall task performance.", "conclusion": "Unsupervised sparse auto-encoders over step-level activations can uncover disentangled \u201creasoning vectors\u201d that mechanistically encode distinct reasoning behaviors in LLMs. These vectors enable both interpretation\u2014by mapping latent features to human-understandable behaviors\u2014and control\u2014by steering behaviors such as reflection, backtracking, response length, and confidence through targeted activation interventions, all without retraining the base model. This demonstrates the promise of latent feature discovery as a general tool for understanding and steering LLM reasoning."}}
{"id": "2512.24853", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24853", "abs": "https://arxiv.org/abs/2512.24853", "authors": ["Koki Suenaga", "Tomohiro Furuta", "Satoshi Ono"], "title": "A study on constraint extraction and exception exclusion in care worker scheduling", "comment": null, "summary": "Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.", "AI": {"tldr": "The paper proposes a method to automatically extract scheduling constraints for long-term care facilities using flexible constraint templates, enabling facility-specific schedule generation.", "motivation": "Existing automatic work schedule generation methods struggle in long-term care facilities because each facility has unique conditions, requiring time-consuming interviews with managers to define constraints manually. There is a need for a systematic way to derive these facility-specific constraints.", "method": "The method introduces constraint templates that capture combinations of elements like consecutive-day shift patterns and staff combinations. By varying the time window (number of days), the number of staff, and whether the focus is on patterns or frequencies, the system can extract many kinds of constraints. It also adds mechanisms to identify and exclude exceptional (non-typical) constraints so they are not enforced in the scheduling model.", "result": "Using the extracted constraints in a constraint programming solver, experiments showed that the generated schedules satisfied all hard constraints and had fewer violations of soft constraints, largely thanks to avoiding the inclusion of exceptional constraints as general rules.", "conclusion": "Constraint templates are an effective tool for automatically extracting realistic, facility-specific scheduling constraints in long-term care, improving schedule quality by focusing on typical patterns and systematically ignoring exceptions."}}
{"id": "2512.24000", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24000", "abs": "https://arxiv.org/abs/2512.24000", "authors": ["Gaurab Chhetri", "Subasish Das", "Tausif Islam Chowdhury"], "title": "WISE: Web Information Satire and Fakeness Evaluation", "comment": "This is the author's preprint. Accepted to WEB&GRAPH 2026 (co-located with WSDM 2026), Boise, Idaho, USA, Feb 26, 2026. Final version will appear in WSDM 2026 Companion Proceedings. Conf: https://wsdm-conference.org/2026/ Workshop: https://aiimlab.org/events/WSDM_2026_WEB_and_GRAPH_2026_Workshop_on_Web_and_Graphs_Responsible_Intelligence_and_Social_Media.html", "summary": "Distinguishing fake or untrue news from satire or humor poses a unique challenge due to their overlapping linguistic features and divergent intent. This study develops WISE (Web Information Satire and Fakeness Evaluation) framework which benchmarks eight lightweight transformer models alongside two baseline models on a balanced dataset of 20,000 samples from Fakeddit, annotated as either fake news or satire. Using stratified 5-fold cross-validation, we evaluate models across comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. Our evaluation reveals that MiniLM, a lightweight model, achieves the highest accuracy (87.58%) among all models, while RoBERTa-base achieves the highest ROC-AUC (95.42%) and strong accuracy (87.36%). DistilBERT offers an excellent efficiency-accuracy trade-off with 86.28\\% accuracy and 93.90\\% ROC-AUC. Statistical tests confirm significant performance differences between models, with paired t-tests and McNemar tests providing rigorous comparisons. Our findings highlight that lightweight models can match or exceed baseline performance, offering actionable insights for deploying misinformation detection systems in real-world, resource-constrained settings.", "AI": {"tldr": "The paper introduces WISE, a framework to benchmark lightweight transformer models for distinguishing fake news from satire, showing that small models can perform very well.", "motivation": "Fake news detection systems often confuse satire with genuinely misleading content because they share linguistic patterns but differ in intent. Existing work tends to focus on heavier models or does not systematically evaluate model calibration and efficiency, which are crucial for real-world, resource-limited deployments.", "method": "The authors build WISE, an evaluation framework that benchmarks eight lightweight transformer models and two baselines on 20,000 Fakeddit samples labeled as fake or satire. They use stratified 5-fold cross-validation and a wide set of metrics: accuracy, precision, recall, F1, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. They also conduct statistical significance testing (paired t-tests, McNemar tests) to rigorously compare models.", "result": "MiniLM attains the highest accuracy (87.58%), RoBERTa-base obtains the best ROC-AUC (95.42%) with comparable accuracy (87.36%), and DistilBERT yields a strong efficiency-accuracy balance with 86.28% accuracy and 93.90% ROC-AUC. Lightweight models generally match or surpass baseline models across metrics.", "conclusion": "Lightweight transformer models are competitive and often superior to baselines for distinguishing fake news from satire, making them suitable for deployment in real-world misinformation detection scenarios where computational resources are limited. The WISE framework provides a robust way to compare model performance and calibration for this task."}}
{"id": "2512.24873", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24873", "abs": "https://arxiv.org/abs/2512.24873", "authors": ["Weixun Wang", "XiaoXiao Xu", "Wanhe An", "Fangwen Dai", "Wei Gao", "Yancheng He", "Ju Huang", "Qiang Ji", "Hanqi Jin", "Xiaoyang Li", "Yang Li", "Zhongwen Li", "Shirong Lin", "Jiashun Liu", "Zenan Liu", "Tao Luo", "Dilxat Muhtar", "Yuanbin Qu", "Jiaqiang Shi", "Qinghui Sun", "Yingshui Tan", "Hao Tang", "Runze Wang", "Yi Wang", "Zhaoguo Wang", "Yanan Wu", "Shaopan Xiong", "Binchen Xu", "Xander Xu", "Yuchi Xu", "Qipeng Zhang", "Xixia Zhang", "Haizhou Zhao", "Jie Zhao", "Shuaibing Zhao", "Baihui Zheng", "Jianhui Zheng", "Suhang Zheng", "Yanni Zhu", "Mengze Cai", "Kerui Cao", "Xitong Chen", "Yue Dai", "Lifan Du", "Tao Feng", "Tao He", "Jin Hu", "Yijie Hu", "Ziyu Jiang", "Cheng Li", "Xiang Li", "Jing Liang", "Chonghuan Liu", "ZhenDong Liu", "Haodong Mi", "Yanhu Mo", "Junjia Ni", "Shixin Pei", "Jingyu Shen", "XiaoShuai Song", "Cecilia Wang", "Chaofan Wang", "Kangyu Wang", "Pei Wang", "Tao Wang", "Wei Wang", "Ke Xiao", "Mingyu Xu", "Tiange Xu", "Nan Ya", "Siran Yang", "Jianan Ye", "Yaxing Zang", "Duo Zhang", "Junbo Zhang", "Boren Zheng", "Wanxi Deng", "Ling Pan", "Lin Qu", "Wenbo Su", "Jiamang Wang", "Wei Wang", "Hu Wei", "Minggang Wu", "Cheng Yu", "Bing Zhao", "Zhicheng Zheng", "Bo Zheng"], "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "comment": "36 pages, 15 figures", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "AI": {"tldr": "The paper presents ALE, an end-to-end ecosystem for developing agentic LLMs, and introduces ROME, an open-source agent trained with this infrastructure that achieves strong results on coding/terminal benchmarks.", "motivation": "Existing open-source efforts for LLM-based agents are fragmented and lack a principled, end-to-end pipeline for training, evaluating, and deploying agentic models that act over multiple steps in real-world environments. The authors aim to create a unified infrastructure to streamline the development of such agents and improve their performance and stability on long-horizon tasks.", "method": "The authors design ALE with three core components: (1) ROLL, a post-training framework for weight optimization of agentic LLMs; (2) ROCK, a sandbox environment manager that generates multi-step interaction trajectories; and (3) iFlow CLI, an agent framework to manage context and prompts efficiently. Using ALE, they construct ROME, an agentic LLM trained on over one million interaction trajectories. They introduce data composition protocols for synthesizing complex behaviors and propose Interaction-based Policy Alignment (IPA), a policy optimization algorithm that assigns learning credit to semantic interaction chunks instead of single tokens to stabilize long-horizon training.", "result": "Using ALE, the authors train ROME and evaluate it on several benchmarks that capture multi-step coding and terminal interaction skills, including SWE-bench Verified, Terminal Bench, and a new large-scale benchmark called Terminal Bench Pro with better contamination controls. ROME achieves strong performance across these benchmarks, showing that the ALE pipeline and IPA training approach lead to effective agent behavior in structured software-oriented environments.", "conclusion": "A unified, end-to-end ecosystem (ALE) can significantly improve the development of agentic LLMs. Its components for post-training, trajectory generation, and context engineering, along with the IPA optimization algorithm and data composition protocols, allow training robust, long-horizon agents like ROME that perform competitively on challenging code and terminal benchmarks. This validates ALE as a strong foundation for future open-source agentic model research and development."}}
{"id": "2512.24014", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24014", "abs": "https://arxiv.org/abs/2512.24014", "authors": ["Sijia Chen", "Di Niu"], "title": "iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning", "comment": "9 pages, 6 figures. The source code is publicly available at https://github.com/AgenticFinLab/latent-planning", "summary": "Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.", "AI": {"tldr": "They propose iCLP, a framework that lets LLMs create and use compact latent plans instead of full textual plans, improving reasoning accuracy, efficiency, and cross-domain generalization while keeping chain-of-thought interpretable.", "motivation": "Explicit, textual step-by-step plans help LLMs reason better, but are hard to generate reliably because of hallucinations and the wide diversity of task types. The authors want a way for models to benefit from planning without needing explicit, verbose, and error-prone textual plans for every new problem.", "method": "1) Collect step-by-step reasoning trajectories with explicit plans. 2) Distill these into explicit plans. 3) Train a vector-quantized autoencoder with a discrete codebook to encode these explicit plans into compact latent plans (LPs). 4) Fine-tune LLMs on pairs of latent plans and corresponding reasoning steps so that, at inference, the LLM implicitly plans in the latent space while performing chain-of-thought in natural language.", "result": "On mathematical reasoning and code generation benchmarks, models augmented with iCLP show higher accuracy and better efficiency than baselines relying on explicit planning or plain chain-of-thought. The latent-planning mechanism also generalizes well across domains while still enabling interpretable verbal reasoning chains.", "conclusion": "Implicit, codebook-based latent planning (iCLP) lets LLMs internalize reusable, compact reasoning strategies learned from explicit plans. This achieves more reliable and efficient step-by-step reasoning, preserves interpretability, and generalizes across tasks, suggesting that implicit-cognition-inspired latent planning is a promising direction for improving LLM reasoning."}}
{"id": "2512.24896", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24896", "abs": "https://arxiv.org/abs/2512.24896", "authors": ["Andrii Gamalii", "Daniel G\u00f3rniak", "Robert Nowak", "Bart\u0142omiej Olber", "Krystian Radlak", "Jakub Winter"], "title": "Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing", "comment": null, "summary": "This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.", "AI": {"tldr": "They built a semi-automated, human-in-the-loop annotation pipeline that uses 3D object detection and supporting techniques (anonymization, domain adaptation, iterative retraining) to cheaply and consistently label a large multimodal driving dataset for autonomous driving research in Poland.", "motivation": "Creating a large-scale, multimodal driving dataset in Polish conditions for autonomous vehicle research is difficult because fully manual annotation of heterogeneous sensor data is expensive, slow, and hard to keep consistent across modalities.", "method": "Design a human-in-the-loop annotation pipeline where AI models perform initial labeling and humans correct and refine them. The pipeline supports iterative model retraining based on human feedback, integrates 3D object detection as the core automatic labeling engine, and incorporates anonymization and domain adaptation to handle privacy and distribution shift between source and target conditions. It outputs data in the standardized DARTS project format.", "result": "The implemented pipeline can automatically generate initial annotations for multimodal driving data, supports continuous improvement of the models, applies anonymization, and delivers consistent, high-quality annotations substantially faster than fully manual labeling, across all project sensor modalities.", "conclusion": "A semi-automated, human-in-the-loop annotation system centered on 3D object detection can significantly reduce time and cost while maintaining high-quality, standardized annotations for large-scale multimodal driving datasets, thereby strengthening the infrastructure for autonomous vehicle research in the Polish context."}}
{"id": "2512.24058", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24058", "abs": "https://arxiv.org/abs/2512.24058", "authors": ["Rohit Kumar Salla", "Manoj Saravanan", "Shrikar Reddy Kota"], "title": "Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models", "comment": "5 pages, 4 tables, accepted at AAAI 2026", "summary": "Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.", "AI": {"tldr": "The paper proposes a unified metric, the Composite Reliability Score (CRS), to evaluate how reliable large language models are by combining calibration, robustness, and uncertainty estimation into a single score.", "motivation": "Large language models are being deployed in high\u2011stakes areas like healthcare, law, and finance, where errors can be costly. However, LLMs are known to be overconfident, brittle to input changes, and poor at expressing uncertainty. Existing evaluation methods focus on only one of these aspects at a time, making it difficult to compare models or choose safe systems for deployment. The paper aims to fill this gap with a comprehensive reliability evaluation framework.", "method": "The authors define the Composite Reliability Score (CRS), which aggregates three core dimensions: calibration (how well predicted confidences match actual accuracy), robustness (how performance changes under input perturbations or distribution shifts), and uncertainty quantification quality (how informative and trustworthy the uncertainty estimates are). They apply CRS to ten popular open\u2011source LLMs and evaluate them on five question\u2011answering datasets under standard conditions, perturbed inputs, and with different calibration techniques. They then compare model rankings produced by CRS against rankings from individual metrics.", "result": "CRS yields consistent and stable rankings of LLMs across datasets and conditions, revealing reliability differences that single metrics fail to capture. It exposes hidden failure modes\u2014such as models that are accurate but poorly calibrated, or robust but overconfident\u2014and shows that relying on accuracy alone can be misleading when assessing model trustworthiness.", "conclusion": "The study concludes that reliable LLMs must jointly optimize accuracy, robustness to input shifts, and well\u2011calibrated uncertainty. The Composite Reliability Score provides an interpretable, unified measure for this purpose, better guiding model selection and deployment decisions in decision\u2011critical domains than traditional, isolated metrics."}}
{"id": "2512.24940", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24940", "abs": "https://arxiv.org/abs/2512.24940", "authors": ["Augusto B. Corr\u00eaa", "Yoav Gelberg", "Luckeciano C. Melo", "Ilia Shumailov", "Andr\u00e9 G. Pereira", "Yarin Gal"], "title": "Iterative Deployment Improves Planning Skills in LLMs", "comment": null, "summary": "We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.", "AI": {"tldr": "Iteratively deploying and fine-tuning LLMs on user\u2011curated data from previous deployments can substantially change model behavior, effectively acting as an implicit outer\u2011loop RL process with important safety and training implications.", "motivation": "To understand how repeated deployment and user\u2011driven fine\u2011tuning of LLMs shapes their capabilities and behaviors, especially in planning tasks, and to clarify the implicit learning dynamics and safety implications of this common practice.", "method": "The authors iteratively deploy LLMs on planning domains, collect user\u2011curated data from each deployment, fine\u2011tune new models on that data, and repeat. They empirically evaluate planning performance and generalization across iterations and then provide a theoretical analysis that models this iterative deployment and curation process as an outer\u2011loop reinforcement learning procedure with an implicit reward signal.", "result": "Across multiple planning domains, later\u2011generation models show substantially improved planning skills and emergent generalization, including the ability to produce much longer plans than the initial models. The theoretical analysis formalizes how the iterative deployment and curation pipeline corresponds to RL in the outer loop, with a latent reward function induced by user selection of training data.", "conclusion": "Iterative deployment with user\u2011curated fine\u2011tuning can significantly alter LLM properties and induce strong capability gains, effectively implementing an implicit RL process without explicit rewards. This has dual implications: it offers a practical alternative training paradigm to explicit RL based on data curation, but it also raises AI safety concerns because the underlying reward function that drives behavioral change is implicit, uncontrolled, and may shape future models in unforeseen ways."}}
{"id": "2512.24092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24092", "abs": "https://arxiv.org/abs/2512.24092", "authors": ["Mao Zheng", "Zheng Li", "Tao Chen", "Mingyang Song", "Di Wang"], "title": "HY-MT1.5 Technical Report", "comment": null, "summary": "In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. HY-MT1.5-1.8B, the 1.8B-parameter model demonstrates remarkable parameter efficiency, comprehensively outperforming significantly larger open-source baselines (e.g., Tower-Plus-72B, Qwen3-32B) and mainstream commercial APIs (e.g., Microsoft Translator, Doubao Translator) in standard Chinese-foreign and English-foreign tasks. It achieves approximately 90% of the performance of ultra-large proprietary models such as Gemini-3.0-Pro, while marginally trailing Gemini-3.0-Pro on WMT25 and Mandarin-minority language benchmarks, it maintains a substantial lead over other competing models. Furthermore, HY-MT1.5-7B establishes a new state-of-the-art for its size class, achieving 95% of Gemini-3.0-Pro's performance on Flores-200 and surpassing it on the challenging WMT25 and Mandarin-minority language test sets. Beyond standard translation, the HY-MT1.5 series supports advanced constraints, including terminology intervention, context-aware translation, and format preservation. Extensive empirical evaluations confirm that both models offer highly competitive, robust solutions for general and specialized translation tasks within their respective parameter scales.", "AI": {"tldr": "The paper presents HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of highly efficient machine translation models that, through a holistic multi-stage training pipeline, outperform larger open-source and commercial systems and approach the performance of ultra-large proprietary models, while supporting advanced constrained translation features.", "motivation": "To develop machine translation models that are both parameter-efficient and high-performing, closing the gap with ultra-large proprietary systems while remaining competitive with or exceeding existing open-source and commercial translation solutions, and to support more complex translation scenarios such as constraint handling and context awareness.", "method": "They design a holistic, multi-stage training framework combining general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. This pipeline is applied to build two models, HY-MT1.5-1.8B and HY-MT1.5-7B, and their performance is systematically evaluated on standard benchmarks (e.g., WMT25, Flores-200, Mandarin-minority language tasks) and against open-source and commercial baselines.", "result": "The 1.8B-parameter model outperforms much larger open-source models (Tower-Plus-72B, Qwen3-32B) and commercial APIs (Microsoft Translator, Doubao Translator), reaching about 90% of Gemini-3.0-Pro\u2019s performance overall, with only slight lag on some benchmarks while still leading other competitors. The 7B model sets a new state of the art for its size, reaching 95% of Gemini-3.0-Pro\u2019s performance on Flores-200 and surpassing it on WMT25 and Mandarin-minority language tests. Both models show strong capabilities on constrained and context-aware translation tasks.", "conclusion": "The HY-MT1.5 family, especially the 1.8B and 7B variants, delivers state-of-the-art or near state-of-the-art translation quality for their parameter scales, outperforming many larger open-source and commercial systems and approaching or surpassing ultra-large proprietary models on several benchmarks, while also supporting advanced constraints like terminology control, context-aware translation, and format preservation. This demonstrates that a carefully designed holistic training pipeline can yield compact yet highly capable MT models suitable for both general and specialized translation tasks."}}
{"id": "2512.24957", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24957", "abs": "https://arxiv.org/abs/2512.24957", "authors": ["Yulan Hu", "Xiangwen Zhang", "Sheng Ouyang", "Hao Yi", "Lu Xu", "Qinglin Lang", "Lide Tan", "Xiang Cheng", "Tianchen Ye", "Zhicong Li", "Ge Chen", "Wenjin Yang", "Zheng Pan", "Shaopan Xiong", "Siran Yang", "Ju Huang", "Yan Zhang", "Jiamang Wang", "Yong Liu", "Yinfeng Huang", "Tucheng Lin", "Xin Li", "Ning Guo"], "title": "AMAP Agentic Planning Technical Report", "comment": null, "summary": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.", "AI": {"tldr": "STAgent is a tool-using LLM specialized for spatio\u2011temporal tasks (like POI search and itinerary planning), built via a stable multi-tool environment, strict data curation, and a cascaded SFT+RL training recipe, achieving strong travel-task performance while keeping general abilities.", "motivation": "General LLMs struggle with complex spatio\u2011temporal reasoning (e.g., planning trips with constraints, using maps/POI tools, and respecting time and location constraints). Existing agents often lack robustness, scalable tool environments, and well-curated training signals that focus on truly challenging, diverse queries. The authors want a model that can robustly operate multiple domain tools, reason step-by-step in such environments, and still preserve broad general abilities.", "method": "They build STAgent, an agentic LLM optimized for spatio\u2011temporal tasks. Methodologically: (1) They create a stable tool environment that exposes 10+ domain-specific tools and supports asynchronous tool rollout and training, so the model can explore and refine intermediate reasoning steps. (2) They design a hierarchical data curation pipeline that aggressively filters raw interaction data (1:10,000) to identify diverse, difficult, high-quality spatio\u2011temporal queries. (3) They propose a cascaded training procedure: start from Qwen3\u201130B\u2011A3B, apply an initial SFT stage that also acts as a \u201cguardian\u201d to estimate query difficulty; then perform a second SFT stage using the high-certainty subset; finally, run RL training on the low-certainty (harder) data, to improve performance specifically on difficult tasks.", "result": "STAgent can reliably operate ten different tools in spatio\u2011temporal scenarios, performing multi-step reasoning such as constrained POI search and itinerary planning. On TravelBench (a benchmark for travel/itinerary tasks), the model achieves strong performance (described as promising), and evaluations on a range of standard general benchmarks indicate that its general capabilities are preserved rather than sacrificed for specialization.", "conclusion": "An agentic LLM can be effectively specialized for spatio\u2011temporal reasoning without losing generality if supported by (i) a robust multi-tool environment, (ii) extremely selective, difficulty-aware data curation, and (iii) a cascaded SFT+RL training pipeline that uses query difficulty for curriculum-like training. The positive outcomes on TravelBench and general benchmarks validate the design of STAgent and suggest that this recipe is a practical way to build domain-specialized, tool-using agentic models."}}
{"id": "2512.24098", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24098", "abs": "https://arxiv.org/abs/2512.24098", "authors": ["Liling Tan"], "title": "Training a Huggingface Model on AWS Sagemaker (Without Tears)", "comment": null, "summary": "The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.", "AI": {"tldr": "A demo paper that serves as a practical, centralized guide for training a first Hugging Face model on AWS SageMaker.", "motivation": "Most LLM work is confined to well-resourced groups with large on-premise compute. Many researchers lack such hardware and must use cloud services like AWS SageMaker, but face a steep learning curve and fragmented documentation. There is a need for a concise, end-to-end, researcher-friendly guide to lower the barrier to cloud-based model training.", "method": "The paper proposes and demonstrates a unified, step-by-step workflow for training a Hugging Face model on AWS SageMaker from scratch. It consolidates key configuration, setup, and training details that are usually scattered across multiple sources, presenting them as a single, coherent demonstration targeted at researchers familiar with local environments but new to SageMaker.", "result": "Readers can follow the paper to successfully launch and complete training of a Hugging Face model on AWS SageMaker, gaining practical know-how about required configurations, typical pitfalls, and best practices.", "conclusion": "By centralizing core knowledge and providing an end-to-end demonstration, the paper lowers the entry barrier to using AWS SageMaker for Hugging Face model training and helps democratize cloud-based LLM experimentation beyond resource-rich institutions."}}
{"id": "2512.25055", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.25055", "abs": "https://arxiv.org/abs/2512.25055", "authors": ["Tianzhi He", "Farrokh Jazizadeh"], "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings", "comment": null, "summary": "This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.", "AI": {"tldr": "Proposes and benchmarks an LLM-based AI agent framework for building energy management, showing good performance on many tasks but weaknesses on complex cost estimation and highlighting accuracy\u2013efficiency trade-offs.", "motivation": "Traditional Building Energy Management Systems (BEMS) lack natural language interaction, flexible context awareness, and autonomous analytics, limiting how effectively occupants and operators can understand and optimize energy use. With the rise of Large Language Models that can interpret text, reason over data, and interact with users, there is a need to conceptualize and systematically assess how LLMs can be integrated as AI agents for smarter, more user-friendly building energy management.", "method": "The authors design a conceptual framework for an LLM-based BEMS AI agent organized into three modules\u2014perception (sensing building and energy data), central control (LLM-based reasoning and decision-making), and action (controlling devices and interacting with users)\u2014in a closed feedback loop. They then implement a prototype agent and evaluate it on 120 user queries spanning four real residential energy datasets. Performance is benchmarked across several dimensions (latency, functionality, capability, accuracy, and cost-effectiveness), and statistical analyses such as ANOVA are used to test generalizability across datasets and scenarios.", "result": "The prototype agent shows strong performance in several task categories: 86% accuracy for device control, 97% for memory-related tasks, 74% for scheduling and automation, and 77% for energy analysis. However, performance drops to 49% accuracy for more complex cost estimation tasks. The evaluation also quantifies latency and cost, enabling examination of trade-offs between response accuracy and computational efficiency. ANOVA tests indicate the framework and performance are reasonably generalizable across different residential energy datasets.", "conclusion": "LLM-based AI agents can effectively support context-aware, conversational building energy management for many common tasks, but they still struggle with more complex, numerically intensive cost estimation. The proposed three-module framework and the presented benchmark provide an initial foundation for systematically assessing such agents. The study underscores a key trade-off between accuracy and computational efficiency and identifies the need for further research on improving complex reasoning, integrating more precise analytical tools, and optimizing deployment costs in real-world smart building environments."}}
{"id": "2512.24143", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24143", "abs": "https://arxiv.org/abs/2512.24143", "authors": ["Adi Shnaidman", "Erin Feiglin", "Osher Yaari", "Efrat Mentel", "Amit Levi", "Raz Lapid"], "title": "Activation Steering for Masked Diffusion Language Models", "comment": null, "summary": "Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\\ response).", "AI": {"tldr": "They introduce an activation-steering method for masked diffusion language models to control generation at inference using contrastive examples.", "motivation": "Masked diffusion language models are promising due to parallel decoding and performance, but there is a lack of effective, efficient inference-time control methods for steering their outputs.", "method": "They compute layer-wise steering vectors from a single forward pass using contrastive examples, then apply these directions to the model\u2019s activations at every reverse-diffusion step, without simulating the full denoising trajectory.", "result": "On LLaDA-8B-Instruct, their method can reliably modulate high-level attributes of generated text; ablations analyze where (which sub-modules) and on which tokens (prompt vs response) steering is most effective.", "conclusion": "Activation steering in MDLMs using single-pass, contrastive-derived directions is an efficient and effective mechanism for inference-time control over high-level generation attributes."}}
{"id": "2512.24149", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24149", "abs": "https://arxiv.org/abs/2512.24149", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Chang Yang", "Peng Zhang"], "title": "Large Emotional World Model", "comment": null, "summary": "World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.", "AI": {"tldr": "The paper introduces a Large Emotional World Model (LEWM) that incorporates emotions into world models, demonstrating that emotional information is crucial for accurate reasoning and prediction of social behaviors.", "motivation": "Existing large language models and world models mainly capture physical-world regularities and overlook emotional factors, despite emotion being central to human decision-making. The authors aim to show that ignoring emotional information harms reasoning and to build a model that treats emotion as a first-class component of world knowledge.", "method": "The authors first empirically test how removing emotion-related information affects reasoning performance. They then construct the Emotion-Why-How (EWH) dataset, which encodes emotions within causal relationships, enabling reasoning about why actions happen and how emotions influence future states. Using this dataset, they train LEWM, which explicitly represents emotional states together with visual observations and actions so it can jointly predict future physical states and emotional transitions.", "result": "Experiments indicate that LEWM better predicts emotion-driven social behaviors than baseline or general world models, while keeping similar performance on standard, basic prediction tasks that do not heavily rely on emotional understanding.", "conclusion": "Incorporating explicit emotion modeling into world models improves reasoning about social and emotion-driven behaviors without sacrificing performance on general tasks, highlighting emotion as a necessary component of comprehensive world knowledge in AI systems."}}
{"id": "2512.24157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24157", "abs": "https://arxiv.org/abs/2512.24157", "authors": ["Xinzhang Liu", "Chao Wang", "Zhihao Yang", "Zhuo Jiang", "Xuncheng Zhao", "Haoran Wang", "Lei Li", "Dongdong He", "Luobin Liu", "Kaizhe Yuan", "Han Gao", "Zihan Wang", "Yitong Yao", "Sishi Xiong", "Wenmin Deng", "Haowei He", "Kaidong Yu", "Yu Zhao", "Ruiyu Fang", "Yuhao Jiang", "Yingyan Li", "Xiaohui Hu", "Xi Yu", "Jingqi Li", "Yanwei Liu", "Qingli Li", "Xinyu Shi", "Junhao Niu", "Chengnuo Huang", "Yao Xiao", "Ruiwen Wang", "Fengkai Li", "Luwen Pu", "Kaipeng Jia", "Fubei Yao", "Yuyao Huang", "Xuewei He", "Zhuoru Jiang", "Ruiting Song", "Rui Xue", "Qiyi Xie", "Jie Zhang", "Zilu Huang", "Zhaoxi Zhang", "Zhilong Lu", "Yanhan Zhang", "Yin Zhang", "Yanlei Xue", "Zhu Yuan", "Teng Su", "Xin Jiang", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "title": "Training Report of TeleChat3-MoE", "comment": null, "summary": "TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastructure that enables reliable and efficient scaling to frontier model sizes. We detail systematic methodologies for operator-level and end-to-end numerical accuracy verification, ensuring consistency across hardware platforms and distributed parallelism strategies. Furthermore, we introduce a suite of performance optimizations, including interleaved pipeline scheduling, attention-aware data scheduling for long-sequence training,hierarchical and overlapped communication for expert parallelism, and DVM-based operator fusion. A systematic parallelization framework, leveraging analytical estimation and integer linear programming, is also proposed to optimize multi-dimensional parallelism configurations. Additionally, we present methodological approaches to cluster-level optimizations, addressing host- and device-bound bottlenecks during large-scale training tasks. These infrastructure advancements yield significant throughput improvements and near-linear scaling on clusters comprising thousands of devices, providing a robust foundation for large-scale language model development on hardware ecosystems.", "AI": {"tldr": "TeleChat3-MoE is a large-scale Mixture-of-Experts LLM series (105B\u20131T+ parameters) trained on Ascend NPUs, and this report focuses on the training infrastructure and system optimizations enabling accurate, efficient, near-linearly scaling training across thousands of devices.", "motivation": "To reliably scale frontier-sized Mixture-of-Experts language models (up to >1T parameters) on Ascend NPU clusters, ensuring numerical correctness across hardware/parallelism choices and overcoming system bottlenecks that limit throughput and scalability in large LLM training.", "method": "They design and describe: (1) systematic numerical accuracy verification at operator and end-to-end levels across hardware and parallel strategies; (2) multiple performance optimizations such as interleaved pipeline scheduling, attention-aware data scheduling for long sequences, hierarchical & overlapped communication for expert parallelism, and DVM-based operator fusion; (3) a systematic parallelization framework using analytical performance estimation and integer linear programming to choose multi-dimensional parallelism configs; and (4) cluster-level optimizations targeting both host- and device-side bottlenecks during large-scale training.", "result": "Their infrastructure and optimization techniques deliver significant throughput gains and near-linear scaling when training TeleChat3-MoE models on clusters with thousands of Ascend NPU devices.", "conclusion": "With carefully engineered numerical verification, parallelism planning, communication/computation optimizations, and system-level tuning, it is possible to efficiently train trillion-parameter MoE LLMs with near-linear scaling on large Ascend NPU clusters, establishing a strong infrastructure base for future large-scale language model development on such hardware platforms."}}
{"id": "2512.24181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24181", "abs": "https://arxiv.org/abs/2512.24181", "authors": ["Qipeng Wang", "Rui Sheng", "Yafei Li", "Huamin Qu", "Yushi Sun", "Min Zhu"], "title": "MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.", "AI": {"tldr": "MedKGI is a diagnostic dialogue framework that grounds LLM reasoning in a medical knowledge graph, optimizes question selection by information gain, and uses structured OSCE-style states to reduce hallucinations, improve question efficiency, and maintain coherent multi-turn clinical reasoning.", "motivation": "Although LLMs show promise in clinical diagnosis, they do not yet emulate real clinicians\u2019 hypothesis-driven, iterative reasoning. Existing models hallucinate medical facts, ask redundant or non-discriminative questions that slow diagnostic progress, and lose coherence over multi-turn dialogues, resulting in inconsistent or contradictory conclusions. There is a need for a system that can reason within validated medical knowledge, plan efficient questioning strategies, and maintain a consistent, traceable diagnostic state across turns.", "method": "The authors propose MedKGI, a framework that: (1) grounds reasoning in a medical knowledge graph so that generated content and inference paths are constrained to verified medical ontologies; (2) uses an information-gain-based strategy to select the next question, prioritizing those that best distinguish between competing diagnoses; and (3) represents the dialogue state in a structured OSCE-style format to explicitly track evidence, hypotheses, and findings over turns, preserving coherence and consistency in multi-turn clinical conversations.", "result": "On clinical benchmark datasets, MedKGI is empirically shown to outperform strong LLM baselines in two main aspects: higher diagnostic accuracy and greater inquiry efficiency. Quantitatively, it improves dialogue efficiency by about 30% on average\u2014meaning fewer, more informative questions are needed to reach a diagnosis\u2014while preserving or achieving state-of-the-art diagnostic accuracy.", "conclusion": "Grounding LLM diagnostic reasoning in a medical knowledge graph, optimizing question selection via information gain, and enforcing a structured OSCE-like dialogue state collectively mitigate key weaknesses of current clinical LLMs. MedKGI can conduct more efficient, coherent, and reliable diagnostic dialogues than baseline LLMs, suggesting a viable pathway toward clinically aligned AI diagnostic assistants."}}
{"id": "2512.24235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24235", "abs": "https://arxiv.org/abs/2512.24235", "authors": ["May Bashendy", "Walid Massoud", "Sohaila Eltanbouly", "Salam Albatarni", "Marwan Sayed", "Abrar Abir", "Houda Bouamor", "Tamer Elsayed"], "title": "LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring", "comment": null, "summary": "Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.", "AI": {"tldr": "Introduce LAILA, the largest publicly available Arabic Automated Essay Scoring dataset with 7,859 essays and rich trait-level annotations, plus benchmarks.", "motivation": "Arabic AES research is underdeveloped because of the scarcity of open, high-quality datasets, limiting model development, comparison, and progress. A large, publicly available, well-annotated dataset is needed to advance the field.", "method": "Design and collect a large corpus of Arabic essays, annotate each with holistic and seven trait-specific scores (relevance, organization, vocabulary, style, development, mechanics, grammar), describe the construction and annotation process, and run benchmark AES experiments with strong Arabic and English models in both prompt-specific and cross-prompt configurations.", "result": "LAILA consists of 7,859 Arabic essays with holistic and trait-specific scores; the paper provides baseline performance numbers for several state-of-the-art Arabic and English AES models under different evaluation setups.", "conclusion": "LAILA fills an important gap for Arabic AES by offering a large, publicly available, richly annotated dataset and initial benchmarks, thereby enabling more robust and comparable research on automated scoring of Arabic essays."}}
{"id": "2512.24259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24259", "abs": "https://arxiv.org/abs/2512.24259", "authors": ["Michael E. Rose", "Mainak Ghosh", "Sebastian Erhardt", "Cheng Li", "Erik Buunk", "Dietmar Harhoff"], "title": "Tracing the Flow of Knowledge From Science to Technology Using Deep Learning", "comment": "4 tables, 7 figures", "summary": "We develop a language similarity model suitable for working with patents and scientific publications at the same time. In a horse race-style evaluation, we subject eight language (similarity) models to predict credible Patent-Paper Citations. We find that our Pat-SPECTER model performs best, which is the SPECTER2 model fine-tuned on patents. In two real-world scenarios (separating patent-paper-pairs and predicting patent-paper-pairs) we demonstrate the capabilities of the Pat-SPECTER. We finally test the hypothesis that US patents cite papers that are semantically less similar than in other large jurisdictions, which we posit is because of the duty of candor. The model is open for the academic community and practitioners alike.", "AI": {"tldr": "The paper introduces Pat-SPECTER, a language similarity model fine-tuned on patents that outperforms other models in identifying credible patent-paper relationships and reveals cross-jurisdiction differences in how closely patents are semantically linked to cited papers.", "motivation": "Existing language similarity models are usually optimized either for scientific publications or for other domains, but not for patents and scientific articles simultaneously. This makes it hard to reliably detect or analyze relationships between patents and research papers, such as patent-paper citations, across large corpora. The authors aim to build a unified model that performs well on both patents and publications and can support empirical studies on how patents reference the scientific literature in different jurisdictions.", "method": "The authors take SPECTER2, a transformer-based scientific document embedding model, and fine-tune it on patent data to create Pat-SPECTER. They run a \u201chorse race\u201d evaluation comparing eight different language similarity models on the task of predicting credible patent\u2013paper citation links. They then test Pat-SPECTER in two applied settings: (1) distinguishing true patent\u2013paper pairs from non-related pairs and (2) predicting which papers are linked to patents. Finally, they use the model\u2019s similarity scores to test whether US patents tend to cite less semantically similar papers than patents from other major jurisdictions, which they attribute to the US duty-of-candor requirement.", "result": "Pat-SPECTER, the fine-tuned SPECTER2 model, yields the best performance among eight evaluated language similarity models in predicting credible patent\u2013paper citations. It effectively separates genuine patent\u2013paper pairs from unrelated pairs and accurately predicts patent\u2013paper links in realistic scenarios. Using its similarity scores, the authors find evidence that US patents on average cite scientific papers that are semantically less similar to the patent content than those cited by patents in other large jurisdictions.", "conclusion": "A single, fine-tuned language similarity model\u2014Pat-SPECTER\u2014can jointly handle patents and scientific literature and outperforms other tested models for tasks involving patent\u2013paper relationships. This enables more accurate large-scale analyses of how patents build on scientific research. The empirical application suggests jurisdictional differences in the semantic closeness of patent\u2013paper citations, consistent with the notion that institutional features like the duty of candor in the US broaden the scope of cited prior art. The model is released for use by researchers and practitioners."}}
{"id": "2512.24265", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24265", "abs": "https://arxiv.org/abs/2512.24265", "authors": ["Ziqing Fan", "Yuqiao Xian", "Yan Sun", "Li Shen"], "title": "Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning", "comment": null, "summary": "A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs' capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, we achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model.", "AI": {"tldr": "DATAMASK is a policy-gradient-based mask learning framework that jointly optimizes quality and diversity metrics to select efficient pre-training subsets for LLMs, drastically cutting selection cost while improving downstream performance.", "motivation": "Existing data selection for LLM pre-training usually uses either quality or diversity metrics alone because jointly optimizing both is computationally prohibitive at trillion-token scale. Empirically, pure quality-based selection yields diminishing returns over long training, while pure diversity-based selection discards many high-quality examples. The authors want an efficient method that can jointly consider multiple metrics for massive datasets like FineWeb and DCLM.", "method": "Formulate data selection as a mask learning problem: learn a stochastic binary mask over pre-training samples. Iteratively sample masks, evaluate them using predefined objectives that combine quality and diversity metrics, compute policy gradients with respect to mask sampling logits, and update these logits to favor better subsets. They use policy-gradient optimization plus engineering/algorithmic accelerations to make this procedure scalable and much faster than greedy selection on trillion-token corpora.", "result": "Using DATAMASK on the 15T-token FineWeb corpus, they select about 10% of the data to create FineWeb-Mask. Compared to a greedy baseline, the selection runtime is reduced by 98.9%. Models trained on FineWeb-Mask achieve average improvements of 3.2% on a 1.5B dense LLM and 1.9% on a 7B MoE LLM across 12 varied evaluation tasks.", "conclusion": "Joint optimization of quality and diversity for LLM pre-training data is both important and feasible at trillion-token scale when framed as a mask learning problem and optimized via policy gradients. DATAMASK makes multi-metric selection computationally practical and yields consistently better LLM performance than using single-metric or greedy selection strategies, while using only a small fraction of the original data."}}
{"id": "2512.24289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24289", "abs": "https://arxiv.org/abs/2512.24289", "authors": ["Jonathan Schmoll", "Adam Jatowt"], "title": "Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs", "comment": null, "summary": "The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.", "AI": {"tldr": "The paper introduces a public benchmark dataset derived from 190 EU corporate reports to evaluate how well LLMs can support EU Taxonomy compliance, finding that LLMs handle qualitative activity-identification moderately well but fail at quantitative KPI prediction, so they are best used as assistive tools rather than for full automation.", "motivation": "EU Taxonomy compliance currently requires a manual, resource-intensive review of corporate reports to extract taxonomy-relevant economic activities and quantitative KPIs. There is increasing interest in using LLMs to automate or streamline this process, but progress is blocked by the lack of public, labeled benchmark datasets that capture real-world reporting structures and compliance tasks. The authors aim to fill this gap and to rigorously measure what current LLMs can and cannot do across the main steps of the compliance workflow.", "method": "The authors construct a structured dataset from 190 corporate reports, annotating ground-truth economic activities and quantitative KPIs relevant to EU Taxonomy. They then design evaluation tasks that mirror the core compliance workflow: (1) a qualitative task of identifying and classifying economic activities, and (2) a quantitative task of extracting or predicting financial KPIs. Multiple LLMs are tested under different prompting strategies, including a multi-step agentic framework and zero-shot settings, and their performance, confidence calibration, and the impact of using concise metadata versus full reports are analyzed.", "result": "LLMs achieve moderate performance on the qualitative task of identifying economic activities, with a multi-step agentic framework offering modest precision gains. However, they perform poorly on the quantitative task of predicting financial KPIs in a zero-shot setting, essentially failing to produce accurate results. The study also finds that shorter, structured metadata can lead to better model performance than full, unstructured report texts, and that model confidence scores are not well calibrated to actual accuracy.", "conclusion": "Current LLMs are not reliable enough for fully automating EU Taxonomy compliance, especially for quantitative KPI extraction and prediction, but they can still provide substantial value as assistive tools that support human experts in qualitative analysis. The new dataset offers a much-needed public benchmark that can drive further research into improving LLM performance on real-world regulatory compliance tasks and understanding how best to integrate them into human-in-the-loop workflows."}}
{"id": "2512.24297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24297", "abs": "https://arxiv.org/abs/2512.24297", "authors": ["Meiqi Chen", "Fandong Meng", "Jie Zhou"], "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking", "comment": null, "summary": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.", "AI": {"tldr": "FIGR is a reinforcement-learning-based framework that augments text chain-of-thought with dynamically generated visual diagrams to better handle complex structural reasoning, significantly improving performance on hard math benchmarks.", "motivation": "Text-only reasoning models struggle with problems that rely on implicit spatial, geometric, or global structural relationships that are not fully captured in linear text, leading to unstable or locally inconsistent reasoning. There is a need to externalize and manipulate structural hypotheses in a more natural, spatial medium while still integrating them into end-to-end learning.", "method": "The paper proposes FIGR, a multimodal reasoning framework that actively constructs visual representations (figures/diagrams) during multi-turn reasoning. FIGR learns via end-to-end reinforcement learning when to invoke visual reasoning, how to structure intermediate diagrams, and how to condition subsequent textual reasoning on these visual states. The method treats figure construction as an action in a reasoning policy, jointly optimizing both text generation and visual updates for problem solving.", "result": "On challenging mathematical reasoning benchmarks, particularly those requiring strong structural and geometric understanding, FIGR outperforms powerful text-only chain-of-thought baselines. It yields a 13.12% accuracy improvement on AIME 2025 and an 11.00% gain on the BeyondAIME benchmark compared to the same base model without visual reasoning.", "conclusion": "Integrating actively constructed visual representations into the reasoning loop substantially improves the stability, coherence, and accuracy of complex structural reasoning beyond what linear text alone can provide. Figure-guided multimodal reasoning is an effective path forward for future reasoning systems on structurally rich tasks."}}
{"id": "2512.24314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24314", "abs": "https://arxiv.org/abs/2512.24314", "authors": ["Shupeng Li", "Weipeng Lu", "Linyun Liu", "Chen Lin", "Shaofei Li", "Zhendong Tan", "Hanjun Zhong", "Yucheng Zeng", "Chenghao Zhu", "Mengyue Liu", "Daxiang Dong", "Jianmin Wu", "Yunting Xiao", "Annan Li", "Danyu Liu", "Jingnan Zhang", "Licen Liu", "Dawei Yin", "Dou Shen"], "title": "QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs", "comment": null, "summary": "Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.\n  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.", "AI": {"tldr": "The paper introduces QianfanHuijin, a financial-domain large language model, and a multi-stage training paradigm that enhances financial knowledge, reasoning, and agentic capabilities, showing superior benchmark performance.", "motivation": "Existing financial LLMs mainly enhance domain knowledge but insufficiently address complex financial reasoning and agent-like decision capabilities needed for modern financial services, creating a gap between model abilities and real-world industrial demands.", "method": "They use a multi-stage training pipeline: (1) Continual Pre-training (CPT) on financial corpora to strengthen domain knowledge; (2) a staged post-training process starting with Financial Supervised Fine-Tuning (SFT); (3) Finance Reasoning Reinforcement Learning (RL); (4) Finance Agentic RL; and finally (5) General RL aligned with real business scenarios. They also conduct empirical evaluations and ablation studies to measure contributions of each stage.", "result": "QianfanHuijin outperforms baseline and existing models on multiple authoritative financial benchmarks. Ablation studies show that specialized Reasoning RL and Agentic RL stages each bring notable improvements in their targeted capabilities.", "conclusion": "A fine-grained, progressive post-training strategy\u2014moving from domain SFT through specialized reasoning and agentic RL to general business-aligned RL\u2014effectively boosts financial LLM performance and is likely to become a common paradigm for industrial domain-enhanced LLMs."}}
{"id": "2512.24329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24329", "abs": "https://arxiv.org/abs/2512.24329", "authors": ["Keito Inoshita", "Shinnosuke Mizuno"], "title": "World model inspired sarcasm reasoning with large language model agents", "comment": null, "summary": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.", "AI": {"tldr": "They propose an interpretable, multi-agent LLM-based framework (WM-SAR) that models sarcasm as inconsistency between literal meaning, context-based expectations, and intentions, achieving better accuracy and explainability than prior methods.", "motivation": "Sarcasm detection is hard because it depends on a gap between what is said and what is meant, plus social/contextual cues. Existing deep learning and LLM approaches perform reasonably well but behave like black boxes and do not explicitly model the cognitive components of sarcasm such as literal meaning, normative expectations, and speaker intention. There is a need for a structured, interpretable framework that leverages LLM reasoning while exposing these underlying factors.", "method": "They reformulate sarcasm understanding as a world-model-inspired reasoning process. The framework WM-SAR decomposes the task into four components: literal meaning, context, normative expectation, and intention. Each component is handled by a specialized LLM-based agent that reasons about its respective aspect. The system computes: (1) a deterministic inconsistency score capturing the discrepancy between the literal evaluation and the normative expectation, and (2) an intention score reflecting whether the speaker intends to be sarcastic. These scores are then fed into a simple Logistic Regression classifier, which outputs the final sarcasm probability, preserving an interpretable numerical decision structure.", "result": "On standard sarcasm detection benchmarks, WM-SAR consistently outperforms both traditional deep learning models and more recent single-LLM baselines. The experiments include ablation studies showing that removing either semantic inconsistency modeling or intention reasoning degrades performance. Case analyses further illustrate that explicit reasoning over inconsistency and intention enables the model to correctly detect subtle sarcastic instances where black-box models struggle.", "conclusion": "Structuring sarcasm understanding as a world-model-based reasoning process with separate agents for literal meaning, context, normative expectation, and intention yields both higher accuracy and better interpretability. Explicitly quantifying inconsistency and integrating it with intention reasoning via a simple Logistic Regression model provides a transparent decision mechanism. The work suggests that decomposed, cognitively-informed LLM architectures can be more effective and explainable than monolithic black-box sarcasm detectors."}}
{"id": "2512.24373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24373", "abs": "https://arxiv.org/abs/2512.24373", "authors": ["Waheed Ahmed Abro", "Zied Bouraoui"], "title": "Skim-Aware Contrastive Learning for Efficient Document Representation", "comment": null, "summary": "Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.", "AI": {"tldr": "They propose a contrastive, NLI-style self-supervised framework for learning efficient and rich representations of long documents by masking sections and training the model to align them with relevant context and separate them from irrelevant parts, achieving better accuracy and efficiency on legal and biomedical tasks.", "motivation": "Existing transformer models struggle with very long documents such as legal and biomedical texts. Sparse attention can extend context length but remains computationally heavy and often misses global document-level relationships. Hierarchical transformers are more efficient but do not explicitly model how different document sections relate to each other, unlike humans who skim and selectively integrate key parts of a text. There is a need for a method that builds holistic, document-level representations in a way that is both interpretable and computationally efficient.", "method": "They design a self-supervised, contrastive learning framework tailored for long documents. During training, they randomly mask a section (e.g., a paragraph or span) of a document. The model then uses a natural language inference (NLI)-inspired contrastive objective: it learns to bring the representation of the masked section closer to semantically relevant parts of the same document (positives) and push it away from unrelated sections (negatives). This process encourages the model to identify and integrate the most informative regions of a document, akin to human skimming, while maintaining computational efficiency compared to full sparse attention over the entire text.", "result": "On benchmarks in legal and biomedical domains, the method yields significant improvements in downstream task performance compared with standard long-document baselines, while also reducing computational cost. The learned document representations are both more accurate for tasks like classification or retrieval and more efficient in terms of memory and runtime.", "conclusion": "By mimicking human-like skimming and synthesis through a self-supervised, NLI-based contrastive objective, the proposed framework produces richer and more efficient representations of long documents than existing transformer-based approaches. This shows that selectively aligning masked sections with relevant context is an effective strategy for modeling long, structured texts, particularly in specialized domains like law and medicine."}}
{"id": "2512.24410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24410", "abs": "https://arxiv.org/abs/2512.24410", "authors": ["Chester Palen-Michel", "Constantine Lignos"], "title": "Comparing Approaches to Automatic Summarization in Less-Resourced Languages", "comment": "Under review", "summary": "Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.", "AI": {"tldr": "The paper compares multiple approaches to automatic summarization for low-resource languages, showing that a multilingual fine-tuned mT5 baseline generally outperforms zero-shot LLMs and that LLM-based evaluation is less reliable in such languages.", "motivation": "While text summarization works well in high-resource languages like English, there is a gap in performance and research focus for low-resource languages. The authors aim to systematically compare modern LLM-based and fine-tuning-based strategies to understand what works best for summarization in less-resourced settings.", "method": "They evaluate several strategies: zero-shot prompting of both large and small LLMs; fine-tuning smaller multilingual models like mT5 with and without multiple data augmentation techniques and multilingual transfer; and an LLM-based translation pipeline that translates the source text to English, summarizes in English, then translates the summary back. They use five automatic metrics and also examine LLM-as-a-judge evaluations.", "result": "Performance varies considerably across LLMs even at similar parameter scales. The multilingual fine-tuned mT5 baseline generally outperforms most alternative approaches, including zero-shot LLM prompting, on most evaluation metrics. The translation-pipeline and zero-shot LLMs are less consistently strong, and LLM-as-judge assessments appear unstable for less-resourced languages.", "conclusion": "For summarization in low-resource languages, careful fine-tuning of a multilingual encoder-decoder model such as mT5 remains highly competitive and often superior to zero-shot prompting of large LLMs. Relying on LLMs as evaluators in these languages can be misleading, so traditional metrics and more robust evaluation strategies are needed."}}
{"id": "2512.24459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24459", "abs": "https://arxiv.org/abs/2512.24459", "authors": ["Michael E. Rose", "Nils A. Herrmann", "Sebastian Erhardt"], "title": "Cleaning English Abstracts of Scientific Publications", "comment": "2 tables, 2 figures", "summary": "Scientific abstracts are often used as proxies for the content and thematic focus of research publications. However, a significant share of published abstracts contains extraneous information-such as publisher copyright statements, section headings, author notes, registrations, and bibliometric or bibliographic metadata-that can distort downstream analyses, particularly those involving document similarity or textual embeddings. We introduce an open-source, easy-to-integrate language model designed to clean English-language scientific abstracts by automatically identifying and removing such clutter. We demonstrate that our model is both conservative and precise, alters similarity rankings of cleaned abstracts and improves information content of standard-length embeddings.", "AI": {"tldr": "The paper presents an open-source language model that automatically cleans scientific abstracts by removing non-content clutter, improving their suitability for similarity and embedding-based analyses.", "motivation": "Scientific abstracts are widely used as stand-ins for full papers in tasks like document similarity and text embedding analyses, but many abstracts contain extraneous elements (e.g., copyright notices, section headings, registrations, and metadata). These noisy components can distort text-based analyses, reducing the reliability of similarity measures and embeddings. There is a need for an automatic, accurate way to strip such clutter from abstracts without harming the substantive content.", "method": "The authors develop an open-source language model tailored to English-language scientific abstracts that detects and removes non-content text such as publisher copyright statements, section headings, author notes, registrations, and bibliographic/bibliometric metadata. The model is designed to be easy to integrate into existing workflows and to err on the side of being conservative (avoiding over-deletion). They evaluate its performance on cleaning tasks and on downstream effects for similarity rankings and text embeddings.", "result": "The model accurately identifies and removes extraneous information from abstracts, demonstrating conservative and precise behavior. Its use changes similarity rankings among cleaned abstracts, indicating that prior rankings were affected by noise. Additionally, embeddings computed from cleaned abstracts show improved information content compared to embeddings from uncleaned abstracts, suggesting higher-quality semantic representations.", "conclusion": "Cleaning abstracts with the proposed language model yields more meaningful text representations by eliminating non-content clutter while preserving substantive information. This improves document similarity analyses and the quality of standard-length text embeddings. The tool is practical, open-source, and beneficial as a preprocessing step for text-based analysis of scientific literature."}}
{"id": "2512.24460", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.24460", "abs": "https://arxiv.org/abs/2512.24460", "authors": ["Titas Ramancauskas", "Kotryna Ramancauske"], "title": "IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback", "comment": null, "summary": "This paper presents the design, development, and evaluation of a proposed revision platform assisting candidates for the International English Language Testing System (IELTS) writing exam. Traditional IELTS preparation methods lack personalised feedback, catered to the IELTS writing rubric. To address these shortcomings, the platform features an attractive user interface (UI), an Automated Essay Scoring system (AES), and targeted feedback tailored to candidates and the IELTS writing rubric. The platform architecture separates conversational guidance from a dedicated writing interface to reduce cognitive load and simulate exam conditions. Through iterative, Design-Based Research (DBR) cycles, the study progressed from rule-based to transformer-based with a regression head scoring, mounted with adaptive feedback.\n  Early cycles (2-3) revealed fundamental limitations of rule-based approaches: mid-band compression, low accuracy, and negative $R^2$ values. DBR Cycle 4 implemented a DistilBERT transformer model with a regression head, yielding substantial improvements with MAE of 0.66 and positive $R^2$. This enabled Cycle 5's adaptive feedback implementation, which demonstrated statistically significant score improvements (mean +0.060 bands, p = 0.011, Cohen's d = 0.504), though effectiveness varied by revision strategy. Findings suggest automated feedback functions are most suited as a supplement to human instruction, with conservative surface-level corrections proving more reliable than aggressive structural interventions for IELTS preparation contexts. Challenges remain in assessing higher-band essays, and future work should incorporate longitudinal studies with real IELTS candidates and validation from official examiners.", "AI": {"tldr": "The paper develops and evaluates an IELTS writing revision platform that combines a modern interface, automated essay scoring, and rubric-aligned adaptive feedback, moving from rule-based to transformer-based models and showing moderate but statistically significant score gains.", "motivation": "Existing IELTS preparation often lacks personalised, rubric-aligned feedback for writing, and many automated tools are either inaccurate, not tailored to IELTS bands, or poorly integrated into a realistic exam-like workflow. The authors aim to build a more accurate, IELTS-specific, and pedagogically grounded system that can meaningfully support learners and teachers.", "method": "Using Design-Based Research (DBR), the authors iteratively designed and refined a revision platform. Early cycles implemented a rule-based AES and feedback system, which were evaluated for scoring accuracy and pedagogical usefulness. Later cycles replaced this with a DistilBERT transformer with a regression head for band scoring, then added adaptive, rubric-aligned feedback. The system was tested empirically via scoring metrics (MAE, R^2) and an intervention study measuring pre/post IELTS band changes and effect sizes across different revision strategies.", "result": "Rule-based models performed poorly, with mid-band score compression, low accuracy, and negative R^2. The DistilBERT regression model substantially improved performance, achieving an MAE of 0.66 and positive R^2, enabling more reliable automated feedback. In user testing, adaptive feedback led to statistically significant, though modest, band score increases (mean improvement of 0.060 bands, p = 0.011, Cohen\u2019s d = 0.504), with effectiveness differing by how learners used the revision tools.", "conclusion": "A transformer-based AES with adaptive, IELTS-rubric-aligned feedback can moderately improve writing performance and is best used as a complement to human teaching rather than a replacement. Conservative, surface-level feedback is more consistently beneficial than aggressive structural changes in this exam context. Remaining challenges include reliably scoring higher-band essays and validating the system with real IELTS candidates and official examiners through longer-term studies."}}
{"id": "2512.24517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24517", "abs": "https://arxiv.org/abs/2512.24517", "authors": ["Fabian Retkowski", "Alexander Waibel"], "title": "Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech", "comment": null, "summary": "Automatic speech transcripts are often delivered as unstructured word streams that impede readability and repurposing. We recast paragraph segmentation as the missing structuring step and fill three gaps at the intersection of speech processing and text segmentation. First, we establish TEDPara (human-annotated TED talks) and YTSegPara (YouTube videos with synthetic labels) as the first benchmarks for the paragraph segmentation task. The benchmarks focus on the underexplored speech domain, where paragraph segmentation has traditionally not been part of post-processing, while also contributing to the wider text segmentation field, which still lacks robust and naturalistic benchmarks. Second, we propose a constrained-decoding formulation that lets large language models insert paragraph breaks while preserving the original transcript, enabling faithful, sentence-aligned evaluation. Third, we show that a compact model (MiniSeg) attains state-of-the-art accuracy and, when extended hierarchically, jointly predicts chapters and paragraphs with minimal computational cost. Together, our resources and methods establish paragraph segmentation as a standardized, practical task in speech processing.", "AI": {"tldr": "The paper introduces new benchmarks and methods for automatic paragraph segmentation in speech transcripts, plus a compact model that jointly predicts chapters and paragraphs efficiently.", "motivation": "Automatic speech transcripts usually come as long, unstructured word streams that are hard to read or reuse. While text segmentation has been studied in written text, speech transcripts lack robust, realistic benchmarks and standard processing steps for adding paragraph structure. The authors aim to turn paragraph segmentation into a practical and standardized step in speech processing and to provide data and methods that bridge speech and text segmentation research.", "method": "1) Construct two benchmarks: TEDPara, based on human-annotated TED talk transcripts, and YTSegPara, based on YouTube video transcripts with synthetic paragraph labels. 2) Formulate paragraph insertion as a constrained decoding problem for large language models, where the model may only insert paragraph breaks while leaving the original transcript intact, enabling clean alignment and evaluation. 3) Design a compact model, MiniSeg, and a hierarchical extension that predicts both chapter and paragraph boundaries with low computational cost.", "result": "On the new benchmarks, the proposed constrained-decoding approach enables faithful evaluation, and the MiniSeg model achieves state-of-the-art accuracy for paragraph segmentation. The hierarchical version of MiniSeg can jointly predict chapter and paragraph boundaries while remaining computationally efficient.", "conclusion": "Paragraph segmentation can and should be treated as a standardized downstream task in speech processing. With TEDPara, YTSegPara, the constrained-decoding formulation for inserting paragraph breaks, and the efficient MiniSeg model (including its hierarchical extension), the paper provides benchmarks and methods that make automatic structuring of speech transcripts both accurate and practical."}}
{"id": "2512.24556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24556", "abs": "https://arxiv.org/abs/2512.24556", "authors": ["Muhammad Abdullahi Said", "Muhammad Sammani Sani"], "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.", "AI": {"tldr": "The paper audits multilingual safety alignment in leading LLMs using a Hausa-language, West Africa\u2013grounded adversarial benchmark, revealing that safety behavior is highly context-dependent across language and temporal framing rather than uniformly worse in low-resource languages.", "motivation": "As LLMs are deployed in critical infrastructure worldwide, it is risky to assume that safety alignment achieved in English automatically generalizes to other languages, especially low-resource ones. Existing narratives often claim a simple multilingual safety gap, but lack systematic, culturally grounded evaluation in Global South contexts. The authors aim to rigorously test safety behavior across languages and temporal framings to uncover more nuanced failure modes that may affect underserved populations.", "method": "The authors construct HausaSafety, an adversarial safety dataset based on West African threat scenarios such as regionally prevalent fraud schemes and improvised weapons. They evaluate three state-of-the-art LLMs (GPT-5.1, Gemini 3 Pro, Claude 4.5 Opus) in a 2 x 4 factorial design, manipulating language (English vs. Hausa) and temporal framing (e.g., past vs. future tense) across 1,440 total evaluations. They then analyze safety rates and interaction effects to identify patterns like linguistic asymmetry and temporal vulnerabilities, looking for non-linear interactions rather than simple performance drops in low-resource settings.", "result": "The audit finds that safety behavior does not simply degrade in Hausa; instead, it exhibits complex, interaction-driven patterns. Claude 4.5 Opus is actually safer in Hausa (45.0% safe responses) than in English (36.7%), largely due to more frequent refusals when uncertain. However, all models show severe temporal reasoning failures: past-tense prompts drastically reduce safety (15.6% safe) compared to future-tense prompts, which elicit overly conservative refusals (57.2% safe). The gap between the safest and most vulnerable condition reaches a factor of 9.2, demonstrating extreme volatility in safety behavior depending on the exact linguistic and temporal context.", "conclusion": "The study concludes that current LLM safety alignment is fragile and heavily dependent on superficial cues like language and tense rather than deep semantic understanding of harmful scenarios. This leads to \"Safety Pockets\"\u2014islands of high safety surrounded by contexts where users, particularly in the Global South, face greater risk from localized harms. To address this, the authors advocate for an Invariant Alignment paradigm, where safety behavior is stable across linguistic and temporal shifts, ensuring more robust and equitable protection for all users."}}
{"id": "2512.24562", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24562", "abs": "https://arxiv.org/abs/2512.24562", "authors": ["Chaodong Tong", "Qi Zhang", "Jiayang Gao", "Lei Jiang", "Yanbing Liu", "Nannan Sun"], "title": "HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering", "comment": "13 pages, 5 figures", "summary": "Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty conveyed by internal semantic representations, which provide complementary views on model reliability. We present \\textbf{HaluNet}, a lightweight and trainable neural framework that integrates multi granular token level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty. Its multi branch architecture adaptively fuses what the model knows with the uncertainty expressed in its outputs, enabling efficient one pass hallucination detection. Experiments on SQuAD, TriviaQA, and Natural Questions show that HaluNet delivers strong detection performance and favorable computational efficiency, with or without access to context, highlighting its potential for real time hallucination detection in LLM based QA systems.", "AI": {"tldr": "The paper introduces HaluNet, a lightweight neural framework that detects hallucinations in LLM question answering by integrating multiple internal uncertainty signals, achieving strong performance and efficiency across QA benchmarks.", "motivation": "Large Language Models perform well in QA but frequently hallucinate by producing factually incorrect or fabricated content. Existing hallucination detection methods usually focus on a single type of uncertainty (e.g., token probabilities) and ignore the complementary information from different uncertainty sources, especially between probability-based signals and semantic representations. There is a need for a scalable, resource-independent, and more reliable internal method that exploits these complementary uncertainties for real-time hallucination detection.", "method": "The authors propose HaluNet, a trainable and lightweight neural framework with a multi-branch architecture. It integrates multi-granular token-level uncertainties by combining: (1) semantic embeddings from internal representations, (2) probabilistic confidence (e.g., token probabilities), and (3) distributional uncertainty over outputs. These branches adaptively fuse what the model semantically encodes with the uncertainty expressed in its token-level outputs, allowing one-pass hallucination detection without relying on external tools or multiple calls.", "result": "On standard QA benchmarks such as SQuAD, TriviaQA, and Natural Questions, HaluNet achieves strong hallucination detection performance while remaining computationally efficient. It works effectively both when the original context is available and when it is not, suggesting robustness across use cases.", "conclusion": "HaluNet effectively exploits complementary uncertainty signals within LLMs to detect hallucinations in QA outputs. Its lightweight, multi-branch design supports efficient one-pass inference and strong detection performance on multiple datasets, indicating that internal uncertainty modeling is a practical solution for real-time hallucination detection in LLM-based QA systems."}}
{"id": "2512.24572", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24572", "abs": "https://arxiv.org/abs/2512.24572", "authors": ["Hongseok Oh", "Wonseok Hwang", "Kyoung-Woon On"], "title": "Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities", "comment": null, "summary": "We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models' legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at https://github.com/lbox-kr/kcl.", "AI": {"tldr": "They introduce KCL, a Korean legal reasoning benchmark that separates reasoning skills from legal knowledge using precedent-backed MCQ and essay tasks, finding large performance gaps and advantages for reasoning-specialized models.", "motivation": "Existing evaluations of legal language models often conflate memorized legal knowledge with genuine reasoning ability, especially in non-English contexts like Korean law. There is a need for a canonical, publicly available Korean benchmark that focuses specifically on legal reasoning, with transparent links to supporting precedents, to fairly compare models and guide progress in legal AI.", "method": "They construct KCL with two parts: KCL-MCQA, a multiple-choice question set (283 questions) where each question is paired with specific supporting legal precedents (1,103 in total), and KCL-Essay, an open-ended generation set (169 questions) with 550 aligned precedents and 2,739 fine-grained rubrics for automatic grading. They then systematically evaluate over 30 language models, including general-purpose and reasoning-specialized ones, using this benchmark and the automated rubric-based evaluation for essays.", "result": "The evaluation shows substantial remaining performance gaps, especially on the open-ended KCL-Essay tasks, indicating that current models struggle with more complex, free-form legal reasoning. Reasoning-specialized models consistently outperform general-purpose models across the benchmark, demonstrating that targeted training for reasoning confers clear advantages on structured legal tasks.", "conclusion": "KCL is a robust, precedent-grounded benchmark for assessing Korean legal reasoning that helps disentangle reasoning from domain knowledge. The observed performance gaps highlight that legal reasoning remains a challenging frontier for LMs, particularly for essay-style tasks, and that reasoning-focused model development is beneficial. By open-sourcing the dataset and evaluation tools, the authors provide a standardized foundation for future research and comparison in Korean legal NLP."}}
{"id": "2512.24574", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24574", "abs": "https://arxiv.org/abs/2512.24574", "authors": ["Zhenyu Zhang", "Xiaoxia Wu", "Zhongzhu Zhou", "Qingyang Wu", "Yineng Zhang", "Pragaash Ponnusamy", "Harikaran Subbaraj", "Jue Wang", "Shuaiwen Leon Song", "Ben Athiwaratkun"], "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time", "comment": null, "summary": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.", "AI": {"tldr": "They identify and control internal attention heads linked to inefficient reasoning behaviors in LLMs, enabling faster and more accurate chain-of-thought via a training-free test-time steering method called CREST.", "motivation": "LLMs using chain-of-thought often generate unnecessarily long, unstable, or repetitive reasoning, causing latency and reduced reliability. There is a need for methods that improve reasoning efficiency and stability without costly retraining or fine-tuning.", "method": "They analyze the structure of LLM reasoning trajectories to find specialized attention heads associated with cognitive behaviors like verification and backtracking. They then develop CREST, which consists of (1) an offline calibration phase that identifies such cognitive heads and computes steering vectors for them, and (2) an inference-time mechanism that rotates hidden states to suppress components along these vectors, thereby discouraging unproductive reasoning modes.", "result": "Using CREST across multiple LLMs and reasoning benchmarks, they report up to 17.5% accuracy gains while simultaneously reducing token usage by 37.6%, indicating both better task performance and lower computational cost.", "conclusion": "Internal attention heads encode interpretable cognitive behaviors that can be exploited at test time. By lightly steering these heads without additional training, CREST provides a simple, general method for making LLM reasoning faster, more efficient, and more reliable."}}
{"id": "2512.24661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24661", "abs": "https://arxiv.org/abs/2512.24661", "authors": ["Casey O. Barkan", "Sid Black", "Oliver Sourbut"], "title": "Do Large Language Models Know What They Are Capable Of?", "comment": "23 pages, 8 figures", "summary": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.", "AI": {"tldr": "The paper studies how well large language models (LLMs) can predict their own success on tasks, how this self-assessment changes during multi-step tasks, and whether they can use in-context experience to make better choices when failure is costly.", "motivation": "As LLMs are increasingly used as autonomous or semi-autonomous agents, it becomes crucial to know if they understand their own capabilities and limits. Overconfident models may attempt tasks they are likely to fail at, causing costly or harmful mistakes, while underconfident models may miss opportunities. The paper is motivated by safety, reliability, and alignment concerns: can LLMs calibrate their confidence, update it from experience, and make rational decisions about when to act or abstain?", "method": "The authors empirically test multiple LLMs on a variety of tasks, asking them to predict the probability that they will succeed before and during task execution. They compare predicted probabilities to actual outcomes to measure calibration, overconfidence, and discriminatory power (ability to distinguish between likely success and failure). They also design multi-step agentic tasks to see how confidence evolves as the model proceeds, comparing reasoning vs non-reasoning models. Finally, they give models in-context examples of previous successes and failures and measure changes in confidence and task selection behavior in scenarios with costly failures.", "result": "All tested LLMs are overconfident but still have better-than-random ability to distinguish tasks they are more or less likely to succeed on. Newer and larger models do not systematically show better discriminatory power, with the exception of a positive trend for Claude models. In multi-step tasks, several frontier LLMs become even more overconfident as they progress, and reasoning-focused LLMs do not outperform standard models on this dimension. When given in-context experience of failure, some LLMs reduce their overconfidence and improve their task selection decisions, while others remain miscalibrated. Across models, given their own estimated probabilities, their choices are approximately rational; the core problem is that the probabilities themselves are too optimistic.", "conclusion": "Current LLMs, even advanced ones, lack accurate awareness of their own capabilities: they are systematically overconfident, and this often worsens during complex, multi-step tasks. While some models can use in-context feedback to calibrate better and make improved decisions in high-stakes settings, this is not universal. Because models act rationally given miscalibrated beliefs, their over-optimism can lead to poor or risky behavior when deployed as agents. The authors highlight implications for AI misuse and misalignment risk, suggesting that improving self-knowledge and calibration of LLMs is important for making them safer and more reliable autonomous agents."}}
{"id": "2512.24618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24618", "abs": "https://arxiv.org/abs/2512.24618", "authors": ["Junru Lu", "Jiarui Qin", "Lingfeng Qiao", "Yinghui Li", "Xinyi Dai", "Bo Ke", "Jianfeng He", "Ruizhi Qiao", "Di Yin", "Xing Sun", "Yunsheng Wu", "Yinsong Liu", "Shuangyin Liu", "Mingkong Tang", "Haodong Lin", "Jiayi Kuang", "Fanxu Meng", "Xiaojuan Tang", "Yunjia Xi", "Junjie Huang", "Haotong Yang", "Zhenyi Shen", "Yangning Li", "Qianwen Zhang", "Yifei Yu", "Siyu An", "Junnan Dong", "Qiufeng Wang", "Jie Wang", "Keyu Chen", "Wei Wen", "Taian Guo", "Zhifeng Shen", "Daohai Yu", "Jiahao Li", "Ke Li", "Zongyi Li", "Xiaoyu Tan"], "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "comment": "57 pages, 26 figures", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "AI": {"tldr": "Youtu-LLM is a 1.96B-parameter, from-scratch pre-trained model designed to combine high efficiency with strong intrinsic reasoning and agentic capabilities, achieving SOTA among sub-2B LLMs, especially on agent tasks.", "motivation": "Most small language models depend heavily on distillation and struggle with deep reasoning, long-context handling, and intrinsic agent-like behaviors such as planning and reflection. The authors aim to show that a compact model, if pre-trained appropriately from scratch with the right architecture and data curriculum, can possess strong native agentic intelligence while remaining computationally efficient and suitable for long-horizon tasks.", "method": "They design a dense Multi-Latent Attention (MLA) architecture with a STEM-oriented vocabulary and support for a 128k context window, optimizing for long-context reasoning and low memory footprint. They pre-train the 1.96B model from scratch on roughly 11T tokens using a multi-stage curriculum that transitions from general commonsense data to more challenging STEM and agent-specific data, which they call a \u201cCommonsense-STEM-Agent\u201d curriculum. During a dedicated agentic mid-training phase, they construct diverse synthetic trajectories over math, coding, and tool-use tasks to explicitly teach planning and reflection behaviors. They then evaluate extensively on both general benchmarks and agent-focused tasks.", "result": "Youtu-LLM, despite having only 1.96B parameters, achieves competitive results compared with larger models on standard benchmarks and establishes new state-of-the-art performance among sub-2B models. On agent-specific benchmarks that stress planning, reasoning, and tool use, it significantly outperforms existing SOTA baselines, showing strong intrinsic agentic capabilities for its size.", "conclusion": "With the right architecture, long-context support, and a principled pre-training curriculum that emphasizes commonsense, STEM, and agentic behaviors, a lightweight 1.96B-parameter model can achieve strong reasoning and agent capabilities, rivaling or surpassing larger models. This suggests that powerful, efficient, long-horizon agents do not require massive model sizes if training and data are carefully designed."}}
{"id": "2512.24684", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24684", "abs": "https://arxiv.org/abs/2512.24684", "authors": ["Maoyuan Li", "Zhongsheng Wang", "Haoyuan Li", "Jiamou Liu"], "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory", "comment": "Accepteed by AAMAS 2026 full paper", "summary": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.", "AI": {"tldr": "R-Debater is an agentic, retrieval-augmented debate system that uses argumentative memory to produce more consistent, evidence-backed multi-turn debates than strong LLM baselines.", "motivation": "Existing LLM-based debate systems often struggle with maintaining stance consistency, leveraging prior arguments, and grounding claims in concrete evidence across multiple turns. There is a need for a framework that can treat debate as a cumulative, memory-based argumentative process rather than isolated turns, and that can be systematically evaluated on standardized debate benchmarks.", "method": "The authors design R-Debater, an agentic framework that couples (1) a debate knowledge base storing case-like evidence and prior debate moves and (2) a role-based debating agent that plans and composes utterances across turns. The system retrieves relevant argumentative memories and evidence from the knowledge base, then uses structured planning to adapt and weave them into coherent, stance-aligned multi-turn utterances. They build a 1,000-item retrieval corpus from ORCHID debates and hold out 32 debates across seven domains for evaluation. Performance is measured on (a) next-utterance generation using InspireScore (subjective, logical, factual dimensions) and (b) adversarial multi-turn debate simulations evaluated by Debatrix (argument, source, language, overall). Human judges\u201420 experienced debaters\u2014also assess consistency and evidence use.", "result": "On the ORCHID benchmark, R-Debater outperforms strong LLM baselines on both next-utterance generation (InspireScore) and adversarial multi-turn simulations (Debatrix metrics). Human evaluation by 20 experienced debaters indicates that R-Debater\u2019s outputs are more consistent with the assigned stance, better grounded in evidence, and more coherent across turns than those of baseline systems.", "conclusion": "Modeling debate as a memory-driven, retrieval-grounded, and structured-planning process improves both single-turn and multi-turn debate quality. By integrating an argumentative memory knowledge base with a role-based debating agent, R-Debater produces more faithful, stance-aligned, and coherent debates than standard LLM approaches, suggesting that argumentative memory is a key component for high-quality automated debating systems."}}
{"id": "2512.24825", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24825", "abs": "https://arxiv.org/abs/2512.24825", "authors": ["Malvina Nissim", "Viviana Patti", "Beatrice Savoldi"], "title": "Practising responsibility: Ethics in NLP as a hands-on course", "comment": null, "summary": "As Natural Language Processing (NLP) systems become more pervasive, integrating ethical considerations into NLP education has become essential. However, this presents inherent challenges in curriculum development: the field's rapid evolution from both academia and industry, and the need to foster critical thinking beyond traditional technical training. We introduce our course on Ethical Aspects in NLP and our pedagogical approach, grounded in active learning through interactive sessions, hands-on activities, and \"learning by teaching\" methods. Over four years, the course has been refined and adapted across different institutions, educational levels, and interdisciplinary backgrounds; it has also yielded many reusable products, both in the form of teaching materials and in the form of actual educational products aimed at diverse audiences, made by the students themselves. By sharing our approach and experience, we hope to provide inspiration for educators seeking to incorporate social impact considerations into their curricula.", "AI": {"tldr": "The paper presents a course design for teaching ethical aspects in NLP, using active, hands-on, and student-driven learning methods, refined over four years and multiple institutions.", "motivation": "As NLP systems are increasingly deployed in real-world applications, there is a growing need for practitioners who understand and can critically reflect on the ethical and social impacts of NLP technologies. Traditional NLP curricula tend to focus on technical skills and lag behind the fast-evolving landscape of ethical issues, leaving a gap in students\u2019 preparation to handle these challenges responsibly. The authors aim to address this by systematically integrating ethics into NLP education.", "method": "The authors designed and implemented a course titled Ethical Aspects in NLP, guided by active learning principles. The course uses interactive sessions, hands-on activities, and \u201clearning by teaching\u201d assignments where students create educational products for different audiences. Over four years, the course was iteratively refined and adapted to different institutions, academic levels, and interdisciplinary cohorts. The paper likely describes syllabus structure, teaching formats, example activities, and evaluation strategies, as well as how materials and student-created outputs were reused.", "result": "The course has been successfully run over four years in multiple institutional contexts and with students from diverse backgrounds. It produced reusable teaching materials and student-created educational products targeted at varied audiences. These outcomes suggest that the pedagogical approach is flexible, scalable, and capable of engaging students in critical reflection on ethical issues in NLP while generating artifacts that can support further education and outreach.", "conclusion": "Integrating ethics into NLP education is both necessary and feasible if approached through carefully designed, active-learning-based pedagogy. The presented course provides a replicable and adaptable model for other educators who want to embed social and ethical considerations into technically focused NLP curricula. By sharing their structure, practices, and experiences, the authors aim to inspire and guide the broader community in developing similar offerings that prepare students to handle the ethical dimensions of NLP technologies."}}
{"id": "2512.24848", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24848", "abs": "https://arxiv.org/abs/2512.24848", "authors": ["Srija Mukhopadhyay", "Sathwik Reddy", "Shruthi Muthukumar", "Jisun An", "Ponnurangam Kumaraguru"], "title": "PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI", "comment": "11 pages, 2 figures", "summary": "Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.", "AI": {"tldr": "The paper introduces PrivacyBench, a benchmark to evaluate whether personalized AI assistants accidentally leak users\u2019 private secrets during multi-turn conversations, showing that common RAG systems leak a significant fraction of secrets and arguing for structural, privacy-by-design safeguards beyond simple prompting.", "motivation": "Personalized AI agents need access to private user data (emails, chats, purchase history) to be useful, but this access creates a major privacy and societal risk when models lack awareness of social context and confidentiality. Existing systems can unintentionally surface or reveal hidden secrets, endangering digital well-being. There is a lack of standardized, socially grounded benchmarks to rigorously assess how often such assistants leak sensitive information in realistic conversational settings and to understand whether current mitigation strategies are sufficient.", "method": "The authors construct PrivacyBench, which consists of socially grounded datasets where private \u2018secrets\u2019 are embedded within user data and dialog contexts. They design a multi-turn conversational evaluation procedure to test whether AI assistants\u2014specifically Retrieval-Augmented Generation (RAG) systems\u2014reveal these embedded secrets under various interaction scenarios. They compare standard RAG assistants with versions using a privacy-aware prompt to measure the effect of prompt-level mitigation on secret leakage rates and analyze the role of the retrieval versus generation components in privacy failures.", "result": "Experiments on RAG-based assistants using PrivacyBench show that these systems leak users\u2019 embedded secrets in up to 26.56% of evaluated interactions. Introducing a privacy-aware prompt substantially reduces\u2014but does not eliminate\u2014leakage, bringing it down to 5.12%. Analysis indicates that the retrieval module continues to fetch sensitive data without discrimination, leaving the generator solely responsible for filtering or suppressing secrets, thereby concentrating privacy risk in a single component.", "conclusion": "The study concludes that current RAG architectures for personalized assistants are intrinsically unsafe for broad deployment when they rely primarily on prompt-level fixes for privacy. Because retrieval components indiscriminately surface sensitive data, placing the full privacy burden on the generator creates a brittle single point of failure. The authors argue that robust, structural privacy-by-design mechanisms\u2014beyond simple prompting\u2014are urgently needed to prevent secret leakage and to support an ethical, inclusive digital ecosystem where personalized AI does not compromise user confidentiality."}}
{"id": "2512.24693", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24693", "abs": "https://arxiv.org/abs/2512.24693", "authors": ["Wenzhe Li", "Shujian Zhang", "Wenxuan Zhou", "John Lambert", "Chi Jin", "Andrew Hard", "Rajiv Mathews", "Lun Wang"], "title": "MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models", "comment": null, "summary": "Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \\textit{training} techniques, effective automated \\textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \\textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \\textbf{MU}lti-\\textbf{S}tep \\textbf{I}nstruction \\textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.", "AI": {"tldr": "They propose MUSIC, an unsupervised data augmentation method that creates multi-turn contrastive conversation pairs so reward models better evaluate multi-turn dialogues, and they show it improves alignment with strong LLM judges without hurting single-turn performance.", "motivation": "Evaluating multi-turn conversations for LLMs is hard and usually needs expensive human judgments. Existing reward models and preference datasets mostly focus only on the final turn, missing the interaction dynamics across turns, which leads to weak signals for training robust multi-turn reward models. There is a gap between improved multi-turn training methods and the lack of similarly strong automated evaluation methods tailored to multi-turn interactions.", "method": "They introduce MUSIC (Multi-Step Instruction Contrast), an unsupervised data augmentation strategy. MUSIC automatically synthesizes contrastive pairs of conversations that differ across multiple turns rather than only at the final turn. Using the Skywork preference dataset as a base and the Gemma-2-9B-Instruct model as the backbone, they generate augmented multi-turn contrastive data and train a multi-turn reward model on this enhanced dataset.", "result": "The MUSIC-augmented multi-turn reward model achieves better alignment with advanced proprietary LLM judges on multi-turn conversation evaluation compared with baselines trained only on standard preference data focusing on final turns. At the same time, this improvement on multi-turn evaluation does not degrade performance on common single-turn reward-model benchmarks.", "conclusion": "Incorporating multi-turn-spanning contrastive signals is key for robust multi-turn reward models. MUSIC provides an effective, unsupervised way to generate such signals via data augmentation. Applying MUSIC yields a multi-turn reward model that more accurately reflects strong LLM judgments on multi-turn dialogues without sacrificing single-turn evaluation quality."}}
{"id": "2512.24863", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.24863", "abs": "https://arxiv.org/abs/2512.24863", "authors": ["Steven Bird"], "title": "Big AI is accelerating the metacrisis: What can we do?", "comment": "9 pages, 1 figure", "summary": "The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.", "AI": {"tldr": "The paper argues that large-scale AI and current NLP practices are deepening interconnected ecological, societal, and linguistic crises, and calls for a radical reorientation of NLP toward human and planetary flourishing.", "motivation": "The authors are motivated by what they see as a converging set of crises: ecological degradation, loss of meaning, and distortions of language. They believe that current trajectories in large-scale AI (\"Big AI\") and NLP are not merely failing to solve these problems but actively worsening them. They see language engineers contributing to harmful power structures and treating technology development as value-neutral, which they argue it is not. This motivates them to propose alternative directions for NLP research and practice that explicitly foreground ethical, social, and ecological concerns.", "method": "Based on the abstract, the paper is likely conceptual and argumentative rather than empirical. The method appears to involve critical analysis of the current \"scalability\" paradigm in AI/NLP, examination of its social and ecological consequences, and normative theorizing about what a \"life-affirming\" and human-centered future for NLP might entail. The authors probably draw on interdisciplinary perspectives (e.g., ethics, environmental studies, social theory) to build their case and to sketch alternative frameworks or principles for NLP research.", "result": "The primary result is not a quantitative finding but a reframing: the authors diagnose Big AI and mainstream NLP as contributing to a metacrisis and articulate why the dominant scalability story is \"failing humanity.\" They likely identify specific problematic patterns\u2014such as concentration of power among plutocrats and kleptocrats, neglect of ecological costs, and the illusion of value-neutral engineering\u2014and distill these into a critique. They may also propose high-level design principles or scenarios for an alternative NLP ecosystem aligned with human and planetary well-being.", "conclusion": "The paper concludes that continuing along the current Big AI trajectory is incompatible with addressing the intertwined ecological, meaning, and language crises. It argues that language engineering cannot be value-free and that practitioners have a responsibility to reconsider whom and what their work serves. The authors call for a collective effort to imagine and build an NLP future that explicitly centers human flourishing within planetary boundaries, implying shifts in research priorities, institutional incentives, and ethical frameworks in the field."}}
{"id": "2512.24733", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24733", "abs": "https://arxiv.org/abs/2512.24733", "authors": ["Sibo Wei", "Peng Chen", "Lifeng Dong", "Yin Luo", "Lei Wang", "Peng Zhang", "Wenpeng Lu", "Jianbin Guo", "Hongjun Yang", "Dajun Zeng"], "title": "BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature", "comment": null, "summary": "Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.", "AI": {"tldr": "The paper introduces BIOME-Bench, a benchmark to systematically evaluate large language models (LLMs) on multi-omics pathway analysis, showing current models still fall short at inferring biomolecular interactions and explaining pathway-level mechanisms.", "motivation": "Pathway enrichment is widely used to interpret multi-omics data but suffers from outdated and incomplete pathway curation, redundancy, and weak sensitivity to specific molecular states and interventions. Existing attempts to use LLMs to improve such analyses lack a standardized, large-scale, end-to-end benchmark, so evaluations are limited to small or ad hoc datasets, preventing fair, reproducible comparison and progress.", "method": "The authors design BIOME-Bench through a four-stage construction workflow and define two evaluation tasks: (1) Biomolecular Interaction Inference, to test whether LLMs can distinguish fine-grained biomolecular relation types, and (2) Multi-Omics Pathway Mechanism Elucidation, to test end-to-end mechanistic explanation at pathway level. They establish evaluation protocols and systematically test multiple contemporary LLMs under these protocols.", "result": "Across the benchmark, strong contemporary LLMs still perform poorly in key aspects of multi-omics reasoning: they fail to robustly and accurately classify detailed biomolecular interaction types and to provide faithful, consistent, and mechanistically sound pathway-level explanations from multi-omics input.", "conclusion": "BIOME-Bench reveals significant remaining gaps in LLM capabilities for multi-omics pathway analysis and provides a standardized, rigorously built benchmark to drive more reproducible and targeted improvement in biomolecular interaction inference and pathway mechanism elucidation by future models and methods."}}
{"id": "2512.24867", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24867", "abs": "https://arxiv.org/abs/2512.24867", "authors": ["Yiming Liang", "Yizhi Li", "Yantao Du", "Ge Zhang", "Jiayi Zhou", "Yuchen Wu", "Yinzhu Piao", "Denghui Cao", "Tong Sun", "Ziniu Li", "Li Du", "Bo Lei", "Jiaheng Liu", "Chenghua Lin", "Zhaoxiang Zhang", "Wenhao Huang", "Jiajun Zhang"], "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements", "comment": null, "summary": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.", "AI": {"tldr": "Encyclo-K is a new, scalable, statement-based benchmark for LLMs that dynamically composes questions from textbook-derived knowledge statements to better test comprehensive, multi-knowledge understanding while avoiding data contamination and reducing annotation costs.", "motivation": "Existing LLM benchmarks are question-based and face three major limitations: they are vulnerable to data contamination because fixed question sets can be memorized; they mostly test single knowledge points rather than comprehensive, multi-fact understanding; and they require expensive domain experts to annotate and validate questions. The authors aim to design a new benchmark that addresses these issues and remains robust and scalable as LLMs rapidly advance.", "method": "The authors propose Encyclo-K, which uses standalone knowledge statements extracted from authoritative textbooks as the basic curation unit. At test time, the system randomly samples and aggregates 8\u201310 such statements to dynamically generate evaluation questions. Human annotators only need to check that the automatically composed questions meet formatting rules; they do not need domain expertise. The vast combinatorial space of possible questions and the dynamic question generation are intended to make memorization infeasible and allow for periodic dataset refresh while preserving stable model rankings.", "result": "The benchmark is applied to over 50 LLMs, including reasoning-oriented and chat-oriented models. Encyclo-K proves challenging: even the best-performing model, OpenAI-GPT-5.1, reaches only 62.07% accuracy. Performance exhibits a clear gradient, with reasoning models scoring between 16.04% and 62.07% and chat models between 9.71% and 50.40%. Model rankings remain stable across different dynamically generated question sets, indicating that the benchmark is both discriminative and robust to question sampling.\n", "conclusion": "Encyclo-K successfully overcomes key weaknesses of traditional question-based benchmarks by using knowledge statements as the core unit, dynamically generating multi-statement questions, and reducing dependence on domain experts. The benchmark offers a scalable, contamination-resistant, and more comprehensive framework for evaluating LLMs\u2019 fine-grained disciplinary knowledge and their ability to integrate multiple pieces of information. The results show that current LLMs still struggle with such dynamic, multi-statement understanding, highlighting substantial remaining headroom for future model improvement."}}
{"id": "2512.24880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24880", "abs": "https://arxiv.org/abs/2512.24880", "authors": ["Zhenda Xie", "Yixuan Wei", "Huanqi Cao", "Chenggang Zhao", "Chengqi Deng", "Jiashi Li", "Damai Dai", "Huazuo Gao", "Jiang Chang", "Liang Zhao", "Shangyan Zhou", "Zhean Xu", "Zhengyan Zhang", "Wangding Zeng", "Shengding Hu", "Yuqing Wang", "Jingyang Yuan", "Lean Wang", "Wenfeng Liang"], "title": "mHC: Manifold-Constrained Hyper-Connections", "comment": null, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "AI": {"tldr": "Proposes Manifold-Constrained Hyper-Connections (mHC), which restore identity mapping in widened residual connections to improve stability, scalability, and efficiency.", "motivation": "Hyper-Connections extend residual connections by widening the residual stream and diversifying connectivity, which improves performance but breaks the identity mapping property. This leads to unstable training, poor scalability, and extra memory overhead. The paper aims to retain the benefits of Hyper-Connections while regaining the stability and efficiency of standard residual connections.", "method": "Introduce Manifold-Constrained Hyper-Connections (mHC), a framework that projects the expanded residual connection space used in Hyper-Connections onto a specific manifold. This projection is designed to restore the identity mapping property of residual connections. In parallel, the authors optimize the underlying infrastructure (e.g., implementation details, memory access patterns) to ensure that these operations are efficient at scale.", "result": "Experiments show that mHC stabilizes large-scale training and improves scalability compared to unconstrained Hyper-Connections. It also yields tangible performance gains (e.g., higher accuracy or lower loss) while reducing or controlling memory-related overheads.", "conclusion": "mHC offers a principled way to extend Hyper-Connections while preserving the beneficial identity mapping property of residual connections. It provides better stability, scalability, and efficiency, and serves as a useful, flexible design element for future large foundational models, potentially informing topological architecture design going forward."}}
{"id": "2512.24776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24776", "abs": "https://arxiv.org/abs/2512.24776", "authors": ["\u00c1kos Prucs", "M\u00e1rton Csutora", "M\u00e1ty\u00e1s Antal", "M\u00e1rk Marosi"], "title": "Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.", "AI": {"tldr": "The paper evaluates large language models with a focus on both accuracy and test-time compute, mapping Pareto frontiers and showing that more reasoning steps eventually yield diminishing returns.", "motivation": "Existing work emphasizes accuracy and complex reasoning abilities of LLMs but largely ignores the computational cost of long reasoning traces, which is critical for industrial deployment where inference cost and latency are constraints.", "method": "The authors perform a test-time-compute aware evaluation of a range of contemporary and older open-source LLMs on math- and reasoning-heavy benchmarks, measuring accuracy as a function of inference-time compute, constructing Pareto frontiers, comparing architectures (especially MoE), and tracking how Pareto efficiency has evolved over time.", "result": "They find that Mixture of Experts (MoE) models tend to offer a favorable balance between performance and efficiency, identify the Pareto-optimal models under different compute budgets, and quantify the trend of accuracy gains per unit of compute across model generations, including evidence of diminishing returns at higher compute levels.", "conclusion": "Extended reasoning (longer inference-time compute) improves performance only up to a saturation point, after which additional compute yields limited accuracy gains, implying that intrinsic model capacity and architecture\u2014rather than arbitrarily extended reasoning\u2014ultimately bound performance on complex tasks."}}
{"id": "2512.24997", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24997", "abs": "https://arxiv.org/abs/2512.24997", "authors": ["Luis Adri\u00e1n Cabrera-Diego"], "title": "Classifying long legal documents using short random chunks", "comment": null, "summary": "Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.", "AI": {"tldr": "They propose a legal document classifier that handles very long texts by sampling multiple short chunks and combining DeBERTa V3 with an LSTM, plus a robust deployment pipeline.", "motivation": "Legal documents are long, use specialized vocabulary, and are costly or impossible to feed entirely into Transformer-based models due to input length limits and computational expense. A practical approach is needed that can still achieve strong classification performance without processing entire documents at once, and that can be deployed reliably at scale.", "method": "They split each legal document into many short segments (max 128 tokens) and randomly select 48 of these chunks as the model input. Each chunk is encoded with DeBERTa V3, and an LSTM aggregates the sequence of chunk representations to make a document-level classification. For deployment, they implement the processing workflow on Temporal, a durable execution and workflow orchestration framework, to ensure robustness and reliability when running the classifier in production.", "result": "Their best model achieves a weighted F-score of 0.898 on the legal document classification task. In deployment on CPU, the end-to-end pipeline has a median processing time of 498 seconds per 100 documents.", "conclusion": "Chunk-based encoding with DeBERTa V3 plus LSTM can effectively classify long legal documents without feeding entire documents to the Transformer, reaching high F-score. Coupling this model with Temporal-based orchestration yields a robust, production-ready pipeline with reasonable throughput on CPU-only infrastructure."}}
{"id": "2512.25026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.25026", "abs": "https://arxiv.org/abs/2512.25026", "authors": ["Nasim Borazjanizadeh", "James McClelland"], "title": "Modeling Language as a Sequence of Thoughts", "comment": null, "summary": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level \"thought\" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.", "AI": {"tldr": "Introduces Thought Gestalt (TG), a recurrent Transformer that maintains sentence-level event-like states to improve efficiency and relational consistency over GPT-2.", "motivation": "Standard Transformers like GPT-2 model language as token sequences and rely on shallow co-occurrence statistics, which leads to inconsistent global representations of entities/events, relational direction errors (e.g., reversal curse), poor contextualization, and data inefficiency. Cognitive science suggests humans instead build persistent, compact event-like representations while verbatim word forms quickly fade. The paper is motivated by bringing this more human-like, structured comprehension mechanism into Transformer language models.", "method": "Propose the Thought Gestalt (TG) model, a recurrent Transformer that operates at two abstraction levels: (1) token-level representations for generating text, and (2) sentence-level \"thought\" states stored in a memory. The model generates one sentence at a time, with token generation cross-attending to a memory of previous sentence representations. Token and sentence representations share parameters and are trained jointly using a single next-token cross-entropy loss. The computation graph for stored sentence representations is retained, allowing gradients from future token predictions to flow backward through cross-attention and update how earlier sentence vectors are formed.", "result": "In scaling experiments, TG achieves better efficiency than comparable GPT-2 and other baseline models. Fitted scaling laws show that GPT-2 needs roughly 5\u20138% more data and 33\u201342% more parameters to match TG's language modeling loss. TG also shows improved generalization on relational direction, reducing the father\u2013son reversal curse errors compared to GPT-2.", "conclusion": "Explicit sentence-level \"thought\" states in a recurrent Transformer improve the global consistency and efficiency of language modeling relative to standard GPT-style models. By integrating event-like, persistent representations inspired by human cognition and training them jointly with token-level predictions, TG achieves better loss with fewer parameters and data and mitigates relational direction failures such as the reversal curse."}}
{"id": "2512.24842", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.24842", "abs": "https://arxiv.org/abs/2512.24842", "authors": ["Yanan Long"], "title": "Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability", "comment": "NeurIPS 2025 Workshop Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \\emph{causal} standard: claims must survive causal interventions and must \\emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \\emph{reference families} as predicate-preserving variants and introduce \\emph{triangulation}, an acceptance rule requiring necessity (ablating the circuit degrades the target behavior), sufficiency (patching activations transfers the behavior), and invariance (both effects remain directionally stable and of sufficient magnitude across the reference family). To supply candidate subgraphs, we adopt automatic circuit discovery and \\emph{accept or reject} those candidates by triangulation. We ground triangulation in causal abstraction by casting it as an approximate transformation score over a distribution of interchange interventions, connect it to the pragmatic interpretability agenda, and present a comparative experimental protocol across multiple model families, language pairs, and tasks. Triangulation provides a falsifiable standard for mechanistic claims that filters spurious circuits passing single-environment tests but failing cross-lingual invariance.", "AI": {"tldr": "The paper proposes a causal, cross-lingual standard for evaluating mechanistic explanations of multilingual language models, using a procedure called triangulation to test whether discovered circuits are necessary, sufficient, and invariant across languages.", "motivation": "Multilingual language models work well on average but behave inconsistently across languages, scripts, and cultures, and existing mechanistic interpretability methods often test their claims only in a single environment. This risks accepting explanations that are spurious or specific to one language or surface form. The authors want a stronger, falsifiable standard that ensures mechanisms are genuinely responsible for behaviors in a meaning-preserving, cross-lingual sense.", "method": "They formalize the notion of reference families: collections of inputs that differ in surface form (e.g., language or script) but preserve the same underlying predicate/meaning. They define triangulation as an acceptance rule for proposed circuits that requires three properties across this reference family: (1) necessity, shown by ablations that reliably reduce the behavior; (2) sufficiency, shown by activation patching that transfers the behavior; and (3) invariance, meaning these effects are directionally stable and of sufficient size across all reference environments. Candidate subgraphs are supplied by automatic circuit discovery and then subjected to triangulation tests. They interpret triangulation as an approximate causal abstraction score based on interchange interventions and design experiments across model families, language pairs, and tasks.", "result": "Triangulation successfully filters out many circuits that appear valid when tested only in a single language or environment but fail to show consistent necessity and sufficiency across the reference family. The method identifies more robust, causally grounded circuits that better generalize across languages and tasks, and demonstrates measurable differences in how various models implement behaviors cross-lingually.", "conclusion": "The paper concludes that mechanistic explanations for multilingual models should meet a causal, cross-lingual standard rather than relying on single-environment evidence. Triangulation offers a practical, falsifiable protocol for vetting discovered circuits using necessity, sufficiency, and invariance across reference families. This strengthens the reliability and generality of mechanistic interpretability claims and aligns them with causal abstraction principles and pragmatic interpretability goals."}}
{"id": "2512.25052", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.25052", "abs": "https://arxiv.org/abs/2512.25052", "authors": ["Chao Peng", "Bin Wang", "Zhilei Long", "Jinfang Sheng"], "title": "AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG", "comment": "Preprint. Under review", "summary": "Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.", "AI": {"tldr": "AdaGReS is a redundancy-aware context selector for RAG that jointly optimizes relevance and diversity under a token budget, with adaptive calibration and theoretical guarantees, improving QA performance.", "motivation": "Standard top-k retrieval for RAG often returns many redundant or near-duplicate chunks. This wastes limited context tokens and can hurt generation quality, especially in settings with high redundancy (e.g., biomedical corpora). There is a need for a principled, token-budget-aware way to select a set of contexts that are both relevant to the query and non-redundant, without laborious manual tuning of trade-off parameters.", "method": "The paper introduces AdaGReS, a context selection framework that defines a set-level objective combining query\u2013chunk relevance scores with penalties for intra-set redundancy (e.g., similarity among selected chunks). Given a token budget, it performs greedy selection based on marginal gains under this objective. A key innovation is a closed-form, instance-adaptive calibration of the relevance\u2013redundancy trade-off parameter that automatically adapts to the statistics of the candidate pool and the available token budget, removing the need for manual tuning. The authors also analyze the objective\u2019s properties, showing it is epsilon-approximately submodular under realistic embedding similarity assumptions, which justifies the use of greedy selection with near-optimality guarantees.", "result": "On open-domain QA (Natural Questions) and a highly redundant biomedical drug corpus, AdaGReS consistently reduces redundancy in the selected context and improves overall context quality compared with standard top-k retrieval and baselines. These gains translate into better answer accuracy and robustness in end-to-end RAG pipelines across different settings and budgets.", "conclusion": "Redundancy-aware, token-budgeted context selection is crucial for effective RAG. AdaGReS provides a practical solution that automatically balances relevance and diversity, supported by theoretical guarantees and empirical improvements. It outperforms standard top-k retrieval by selecting more informative, less redundant context sets, leading to better downstream generation performance without the need for manual hyperparameter tuning."}}
{"id": "2512.24885", "categories": ["cs.CL", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.24885", "abs": "https://arxiv.org/abs/2512.24885", "authors": ["Hengli Li", "Zhaoxin Yu", "Qi Shen", "Chenxi Li", "Mengmeng Wang", "Tinglang Wu", "Yipeng Kang", "Yuxuan Wang", "Song-Chun Zhu", "Zixia Jia", "Zilong Zheng"], "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts", "comment": "Accepted by AAMAS 2026", "summary": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.", "AI": {"tldr": "The paper introduces BEDA, a framework that uses explicitly modeled beliefs as probabilistic constraints on dialogue generation, improving strategic dialogue performance across adversarial, cooperative, and negotiation tasks.", "motivation": "Existing strategic dialogue systems can estimate interlocutors\u2019 beliefs but lack a principled way to use these beliefs when generating utterances. This leads to suboptimal strategic behavior because dialogue acts like adversarial moves or alignment moves are not tightly coupled to the agent\u2019s internal belief state. The authors aim to create a general, principled mechanism that turns beliefs into operational constraints on what agents are allowed to say, so that dialogue strategies become more reliable and effective across different strategic settings.", "method": "They formalize two core strategic dialogue acts\u2014Adversarial and Alignment\u2014and encode them as probabilistic constraints over possible utterances given the agent\u2019s belief state. They propose BEDA, a framework with three main components: (1) a world set representing possible states of the environment, (2) a belief estimator that infers a probability distribution over these worlds from the dialogue history, and (3) a conditional generator that both selects appropriate dialogue acts and generates utterances that respect the belief-based probabilistic constraints. This framework is instantiated on multiple backbone models, including small and larger LMs (e.g., GPT-4.1-nano), and tested on several strategic dialogue benchmarks.", "result": "On three benchmark tasks\u2014Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation)\u2014BEDA consistently outperforms strong baselines. For CKBG, it increases success rate by at least 5 percentage points across multiple backbones and by 20.6 points with GPT-4.1-nano. On Mutual Friends, it yields an average improvement of 9.3 percentage points over baselines. On CaSiNo, BEDA achieves the optimal negotiation deal compared to all baseline systems, indicating stronger strategic performance.", "conclusion": "Encoding belief estimation as explicit probabilistic constraints on generation provides a simple and general mechanism for improving strategic dialogue. By tying dialogue acts like adversarial and alignment moves directly to inferred beliefs, BEDA yields more reliable and effective strategies across adversarial, cooperative, and negotiation settings, suggesting that this belief-as-constraints approach can serve as a broadly useful design principle for future dialogue agents."}}
{"id": "2512.24933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24933", "abs": "https://arxiv.org/abs/2512.24933", "authors": ["Minjun Zhao", "Xinyu Zhang", "Shuai Zhang", "Deyang Li", "Ruifeng Shi"], "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline", "comment": null, "summary": "Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.", "AI": {"tldr": "ADOPT is a framework to automatically and robustly optimize prompts across all steps of multi-step LLM pipelines by modeling inter-step dependencies and using adaptive resource allocation.", "motivation": "Multi-step LLM pipelines can solve complex tasks but are highly sensitive to the quality of prompts at each step. Jointly optimizing all these prompts is hard because step-level supervision is missing, steps depend on one another, and existing end-to-end prompt optimization methods become unstable or suboptimal under such dependencies. There is a need for a principled way to optimize multiple prompts together while accounting for how each step contributes to the final task outcome.", "method": "The authors introduce ADOPT, an Adaptive Dependency-aware Prompt Optimization framework. ADOPT explicitly models how each step in a multi-step LLM pipeline affects the final task performance, allowing them to estimate \u2018textual gradients\u2019 analogous to analytical derivatives for each step\u2019s prompt. They separate (decouple) the process of estimating these text-gradients from actually updating prompts, thereby reducing the complex joint optimization over many prompts into more manageable single-prompt optimization subproblems. Additionally, they use a Shapley-value-based mechanism to measure each step\u2019s contribution and adaptively allocate optimization effort and resources across steps.", "result": "On real-world datasets and across different multi-step pipeline structures, ADOPT consistently achieves better final task performance than state-of-the-art prompt optimization baselines, and does so in a more stable and robust way. The experiments demonstrate effectiveness across diverse settings, highlighting improved reliability of multi-step LLM pipelines after applying ADOPT.", "conclusion": "ADOPT provides a principled, dependency-aware way to optimize prompts in multi-step LLM pipelines. By explicitly modeling inter-step influence, estimating text-gradients, and adaptively focusing optimization resources where they matter most, it overcomes limitations of existing end-to-end prompt optimization methods and yields consistent performance gains and robustness across tasks and pipeline structures."}}
{"id": "2512.25015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.25015", "abs": "https://arxiv.org/abs/2512.25015", "authors": ["Siddhant Agarwal", "Adya Dhuler", "Polly Ruhnke", "Melvin Speisman", "Md Shad Akhtar", "Shweta Yadav"], "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes", "comment": "Accepted by AAAI 2026", "summary": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.", "AI": {"tldr": "The paper presents a new dataset and multi-agent framework for detecting depressive symptoms in internet memes, significantly advancing automatic mental-health related meme analysis.", "motivation": "Memes are increasingly used to communicate not just humor but also serious emotions, including depressive sentiments. Existing work struggles to accurately detect clinically meaningful depressive symptoms from memes. There is a need for resources and methods that bridge informal meme content and formal clinical concepts to better understand and identify depressive expressions online.", "method": "The authors create RESTOREx, a resource featuring memes labeled for depressive symptoms along with LLM-generated and human-annotated explanations. They then propose MAMAMemeia, a collaborative multi-agent, multi-aspect discussion framework inspired by Cognitive Analytic Therapy (CAT) competencies from clinical psychology. Multiple agents analyze different aspects of each meme and discuss to reach a consensus on the presence of depressive symptoms.", "result": "MAMAMemeia achieves a 7.55% improvement in macro-F1 over the prior state-of-the-art for depressive symptom detection from memes, outperforming more than 30 baseline and comparative methods. RESTOREx is established as a key benchmark dataset for this task.", "conclusion": "Incorporating clinical psychology concepts and multi-agent LLM collaboration yields substantial gains in detecting depressive symptoms in memes. The new RESTOREx dataset and MAMAMemeia framework set a new benchmark for mental health-related meme analysis and can support future research in automated understanding of mental health expressions on social media."}}
