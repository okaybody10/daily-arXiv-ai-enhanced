<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper presents an LLM-driven framework to detect and improve 'lazy thinking' in peer reviews by multi-label issue detection, neurosymbolic classification, and template-based feedback generation, achieving substantial quality gains and releasing a labeled dataset.


<details>
  <summary>Details</summary>
Motivation: Peer review quality is crucial for scientific progress, but existing reviews often rely on oversimplified heuristics ('lazy thinking'), and prior work models this as a single-label detection problem. There is a need for finer-grained, multi-issue detection and, crucially, for guideline-aware, actionable feedback mechanisms rather than mere identification of problems.

Method: The authors propose a framework that: (1) segments peer reviews into argumentative units; (2) uses a neurosymbolic module that merges LLM-derived representations with traditional machine learning classifiers to detect multiple types of issues (lazy thinking, clarity, specificity); and (3) generates targeted, guideline-based feedback through issue-specific templates that are automatically refined using a genetic algorithm.

Result: Their experiments indicate that the framework outperforms zero-shot LLM baselines on detection and feedback tasks and can improve review quality by up to 92.4%. They also construct and publish LazyReviewPlus, a dataset of 1,309 review sentences with labels for lazy thinking and specificity issues.

Conclusion: The framework provides a more nuanced and effective way to diagnose and remediate problems in peer reviews than prior single-label, detection-only approaches. By combining neurosymbolic detection with optimized, template-based feedback and releasing a new annotated dataset, the work advances automated tools for enhancing peer review quality.

Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [2] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

TL;DR: The paper proposes LT-Tuning, a new framework for performing reasoning in the continuous latent space of LLMs instead of via explicit Chain-of-Thought, mitigating feature collapse and instability and improving reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Explicit Chain-of-Thought requires LLMs to express every reasoning step as text tokens, which constrains reasoning to discrete vocabulary space and can be inefficient or brittle. Emerging latent-space reasoning avoids discrete token constraints but current methods suffer from feature collapse, instability, and distribution mismatches when reusing hidden states or depending on assistant models. There is a need for a more stable, accurate latent reasoning mechanism that can collaborate with explicit CoT.

Method: The authors introduce Latent Thoughts Tuning (LT-Tuning), which redefines latent thought construction and use. Instead of directly feeding back raw hidden states as latent thoughts, they design a Context-Prediction-Fusion mechanism that fuses contextual hidden states with semantic guidance derived from the vocabulary embedding space. They further employ a progressive three-stage curriculum learning pipeline that trains the model to gradually and dynamically switch between latent and explicit reasoning modes, improving stability and alignment.

Result: Experimental evaluations show that LT-Tuning surpasses existing latent reasoning approaches, significantly reducing feature collapse and improving stability of latent reasoning. It also yields better reasoning accuracy compared with prior latent paradigms under benchmark tasks.

Conclusion: Latent Thoughts Tuning provides a more reliable way to perform reasoning in the continuous latent space of LLMs by fusing contextual and semantic information and training with a staged curriculum. This approach alleviates feature collapse and instability found in previous latent methods and delivers robust reasoning performance while allowing dynamic switching between latent and explicit Chain-of-Thought reasoning.

Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [3] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

TL;DR: This paper proposes KV Policy (KVP), a reinforcement learning-based method to learn which tokens to keep or evict from the KV cache in large language models, improving inference efficiency and generalization across tasks and context lengths.


<details>
  <summary>Details</summary>
Motivation: Autoregressive inference in large language models requires storing a growing Key-Value (KV) cache, which becomes memory- and compute-intensive for long contexts. Existing KV eviction/compression strategies use simple heuristics (like recency or historical attention) that are only indirect signals of future importance and can add extra computation, limiting efficiency and performance. There is a need for a principled, learned method that better predicts which tokens will be useful in future decoding steps.

Method: The authors recast KV cache eviction as a reinforcement learning ranking problem, where the goal is to learn a policy that orders tokens by their predicted future utility. They introduce KV Policy (KVP), a set of lightweight per-head RL agents trained offline on pre-computed generation traces. Each agent observes only the key and value vectors and learns a head-specific eviction policy. The reward is based on future utility, and the policy is trained to evaluate token rankings across all cache size budgets. KVP does not modify the base LLM and incurs no additional inference-time passes beyond using the learned eviction policy.

Result: On the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly surpasses heuristic-based baselines for KV cache management. It also exhibits strong zero-shot generalization on various downstream benchmarks such as LongBench, BOOLQ, and ARC, including to context lengths longer than those seen in training. These results indicate improved performance-–efficiency trade-offs over existing methods under constrained KV cache budgets.

Conclusion: Learning to predict future token utility via reinforcement learning is an effective and scalable strategy for managing the KV cache in large language models. The KV Policy framework, using per-head RL agents trained only on key and value vectors, provides superior eviction decisions compared with heuristic methods, generalizes well across tasks and lengths, and does so without altering the base model or incurring substantial inference overhead.

Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [4] [On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models](https://arxiv.org/abs/2602.10298)
*Polina Tsvilodub,Jan-Felix Klumpp,Amir Mohammadpour,Jennifer Hu,Michael Franke*

Main category: cs.CL

TL;DR: The paper tests whether language models reuse the same internal machinery for general Theory of Mind (ToM) and language-specific pragmatic reasoning, suggesting they might form integrated "social world models" rather than task-specific skills.


<details>
  <summary>Details</summary>
Motivation: To address whether large language models exhibit emergent social cognition comparable to human-like Theory of Mind, and specifically whether their reasoning about mental states and pragmatics relies on shared underlying computational mechanisms instead of isolated task modules.

Method: The authors use behavioral evaluations of LM performance on tasks covering seven ToM subcategories (per Beaudoin et al., 2020) and conduct causal-mechanistic experiments. They apply functional localization techniques, inspired by cognitive neuroscience, to identify and perturb model components associated with ToM and pragmatic reasoning, using a larger localizer dataset than previous work.

Result: Through hypothesis-driven statistical analyses, they find suggestive evidence that overlapping internal mechanisms support both general ToM and language-specific pragmatic reasoning in LMs. The results imply that these models build interconnected representations of others' mental states across tasks.

Conclusion: The findings support the functional integration hypothesis: LMs likely develop interconnected "social world models" rather than isolated competencies. The paper also contributes a new ToM localizer dataset, refines functional localization methodologies, and offers empirical evidence about how social cognition may emerge in artificial systems.

Abstract: This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent "social world models", i.e., representations of mental states that are repurposed across tasks (the functional integration hypothesis). Using behavioral evaluations and causal-mechanistic experiments via functional localization methods inspired by cognitive neuroscience, we analyze LMs' performance across seven subcategories of ToM abilities (Beaudoin et al., 2020) on a substantially larger localizer dataset than used in prior like-minded work. Results from stringent hypothesis-driven statistical testing offer suggestive evidence for the functional integration hypothesis, indicating that LMs may develop interconnected "social world models" rather than isolated competencies. This work contributes novel ToM localizer data, methodological refinements to functional localization techniques, and empirical insights into the emergence of social cognition in artificial systems.

</details>


### [5] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

TL;DR: The paper studies whether resource-rational reasoning can emerge in large language models purely from inference-time scaling, without explicit computational cost in the reward, using a controlled Variable Attribution Task.


<details>
  <summary>Details</summary>
Motivation: Although inference-time scaling greatly improves reasoning in LLMs, it is unclear if such improvements reflect resource-rational behavior—adapting the depth and style of reasoning to task complexity—especially when models are not explicitly penalized for computation. The authors want to test whether such resource rationality can emerge as a byproduct of scaling itself, and how different training paradigms (instruction tuning vs. reinforcement learning for reasoning) affect this behavior.

Method: The authors design a Variable Attribution Task where models must infer which input variables determine an output, given candidate variables, observed input-output trials, and known logical functions (e.g., AND, OR, XOR, XNOR). They systematically vary task complexity by changing the number of candidate variables and the number of trials. They test two classes of models: instruction-tuned models that generate explicit step-by-step reasoning at inference time, and Large Reasoning Models trained with reinforcement learning to discover high-accuracy reasoning paths. They then analyze how the models’ reasoning strategies change with task complexity and function type.

Result: Both instruction-tuned models and Large Reasoning Models show a clear shift from brute-force style reasoning to more analytic, structured strategies as task complexity increases. However, instruction-tuned models’ performance notably degrades on more challenging logical functions, specifically XOR and XNOR, while Large Reasoning Models maintain robust performance on these functions. This differential behavior highlights how training methodology shapes reasoning robustness across logical structures.

Conclusion: Inference-time scaling alone can give rise to resource-rational behavior: models adapt their reasoning strategies to task complexity even without explicit rewards tied to computational cost. The robustness of Large Reasoning Models on difficult logical functions further suggests that reinforcement learning-based training can better support such emergent rationality. Overall, resource rationality appears as an emergent property of scaling test-time computation, rather than requiring direct optimization for compute-efficiency.

Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [6] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

TL;DR: The paper creates a large, annotated dataset of body-worn camera traffic-stop transcripts and proposes a perspective-aware modeling framework to predict and explain perceptions of respect from multiple community groups.


<details>
  <summary>Details</summary>
Motivation: Respect in police traffic stops strongly affects public trust and perceived legitimacy, but perceptions of respect are subjective and differ across communities. Existing work lacks large-scale, multi-perspective data and tools to systematically analyze these differences using real-world body-worn camera footage.

Method: The authors obtain extensive LAPD body-worn camera traffic-stop footage and produce transcripts. They design a respect evaluation rubric based on procedural justice theory, LAPD training, and fieldwork. Annotators from three groups—police-affiliated, justice-system-impacted, and non-affiliated residents—rate respect and provide free-text rationales for both officers and civilians. Using this rubric-driven preference data, they build a perspective-aware modeling framework that predicts personalized respect ratings and generates group-specific rationales, comparing performance against perspective-agnostic baselines.

Result: The perspective-aware models outperform baselines in predicting respect ratings and aligning generated rationales with those written by annotators across all three communities. The dataset and framework capture systematic differences in how groups perceive respect in traffic stops.

Conclusion: Incorporating community-specific perspectives into dataset design and modeling substantially improves our ability to measure and explain perceived respect in police traffic stops. The proposed rubric, dataset, and perspective-aware models offer practical tools for law enforcement agencies to understand and respond to diverse expectations, supporting efforts to build public trust and procedural legitimacy.

Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [7] [When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding](https://arxiv.org/abs/2602.10350)
*Domenico De Cristofaro,Alessandro Vietti,Marianne Pouplier,Aleese Block*

Main category: cs.CL

TL;DR: The paper studies how phoneme-level predictions change across layers of a pretrained Wav2Vec2 model for a low-resource language (Campidanese Sardinian) and finds that intermediate layers yield better phoneme recognition than the final layer.


<details>
  <summary>Details</summary>
Motivation: Multilingual speech models are often used for low-resource languages, but their internal layer-wise behavior is poorly understood. Prior work suggests intermediate layers can contain more phonetically accurate information than the final layer, yet evaluation usually focuses only on the last layer and on surface metrics like PER/WER. For low-resource languages, where data and evaluation resources are limited, understanding how and where good phonetic representations emerge inside the model can improve diagnosis and adaptation of ASR systems.

Method: The authors take a pretrained Wav2Vec2 model and perform layer-wise decoding: they truncate the encoder at different transformer layers and attach a phoneme classifier to obtain phoneme predictions from each depth. They then compare Phoneme Error Rates (PER) across layers for Campidanese Sardinian. Additionally, they conduct fine-grained alignment analysis to examine how phoneme segments align with the speech signal, and they categorize different error types, including a newly defined class of "regressive errors," where a correct intermediate prediction becomes incorrect at a later layer.

Result: They find that phoneme recognition performance does not peak at the final layer: truncating upper transformer layers leads to lower PER, with the best layer being two layers before the top. Intermediate layers better preserve segmental identity, avoid overgeneration of phonemes, and reduce specific phonological error classes. The analysis identifies many regressive errors, where accurate intermediate predictions are later overwritten by errors at deeper layers, demonstrating that later layers sometimes abstract away from useful acoustic detail.

Conclusion: Intermediate layers of Wav2Vec2 can be more phonetically faithful than the final layer, especially for a low-resource language like Campidanese Sardinian. Standard surface-level metrics that only consider final-layer outputs can miss these nuances and under-represent linguistically meaningful behavior. The introduction of regressive errors shows that deeper layers may over-abstract, harming phonetic accuracy. Therefore, early-layer probing and layer-wise decoding are valuable diagnostic tools for ASR, particularly in low-resource scenarios, and can guide better model use and adaptation.

Abstract: Recent studies have shown that intermediate layers in multilingual speech models often encode more phonetically accurate representations than the final output layer. In this work, we apply a layer-wise decoding strategy to a pretrained Wav2Vec2 model to investigate how phoneme-level predictions evolve across encoder layers, focusing on Campidanese Sardinian, a low-resource language. We show that truncating upper transformer layers leads to improved Phoneme Error Rates (PER), with the best performance achieved not at the final layer, but two layers earlier. Through fine-grained alignment analysis, we find that intermediate predictions better preserve segmental identity, avoid overgeneration, and reduce certain classes of phonological errors. We also introduce the notion of regressive errors, cases where correct predictions at intermediate layers are overwritten by errors at the final layer. These regressions highlight the limitations of surface-level error metrics and reveal how deeper layers may generalize or abstract away from acoustic detail. Our findings support the use of early-layer probing as a diagnostic tool for ASR models, particularly in low-resource settings where standard evaluation metrics may fail to capture linguistically meaningful behavior.

</details>


### [8] [Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs](https://arxiv.org/abs/2602.10352)
*Keenan Pepper,Alex McKenzie,Florin Pop,Stijn Servaes,Martin Leitgab,Mike Vaiana,Judd Rosenblatt,Michael S. A. Graziano,Diogo de Lucena*

Main category: cs.CL

TL;DR: The paper shows that tiny, frozen adapters trained on interpretability data can make language models reliably describe their own internal workings, and these self-interpretations get better with model scale.


<details>
  <summary>Details</summary>
Motivation: Existing self-interpretation methods rely on prompting LMs to explain their own internals, but they are unstable and highly sensitive to prompt and hyperparameter choices, making them unreliable for serious interpretability. The authors want a scalable, robust way for models to explain their internal representations without retraining or altering the base model.

Method: Keep the base language model completely frozen and train very small adapter modules—down to a scalar affine adapter with only d_model + 1 parameters—on interpretability artifacts such as sparse autoencoder (SAE) features and labels. Evaluate these adapters on multiple families and sizes of LMs across tasks: generating SAE feature labels, topic identification, and decoding hidden bridge entities in multi-hop reasoning. Compare different adapter complexities, isolate the role of the learned bias term, and control for raw model knowledge by comparing to purely prompted descriptions across model scales (7B–72B).

Result: A minimal scalar affine adapter, with only d_model + 1 parameters, is enough to substantially improve self-interpretation. Trained adapters: (1) generate SAE feature labels that outperform the original human or heuristic labels they were trained on (71% vs 63% in a 70B model by generation scoring); (2) identify topics with 94% recall@1, compared to 1% for untrained baselines; and (3) decode latent “bridge entities” in multi-hop reasoning that do not appear in the prompt or final answer, thereby exposing implicit reasoning steps without requiring chain-of-thought outputs. The learned bias vector alone explains about 85% of the performance gain, and simpler adapters generalize better than more complex ones. Gains in self-interpretation with scale exceed the gains attributable to better underlying capabilities alone.

Conclusion: Very lightweight, frozen adapters trained on interpretability artifacts can robustly and effectively elicit self-interpretations from large language models, without modifying their parameters. Self-interpretation quality improves with model size and is largely captured by simple adapter structures, particularly bias terms. This suggests a practical, scalable path to more reliable mechanistic understanding and surfacing of implicit reasoning in LMs, using minimal additional parameters and no changes to the base models.

Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.

</details>


### [9] [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)
*Mashrekur Rahman*

Main category: cs.CL

TL;DR: They interpret and validate Google AlphaEarth satellite embeddings, then use them to build a retrieval-augmented environmental question-answering system.


<details>
  <summary>Details</summary>
Motivation: Satellite foundation models create powerful compressed representations of Earth’s surface, but these dense vectors are poorly understood physically, which limits trust and use in real-world environmental decision-making and geospatial intelligence.

Method: They analyze 64D AlphaEarth embeddings for 12.1M samples across the US against 26 environmental variables using linear models, nonlinear models, and attention-based methods, plus spatial block cross-validation and temporal stability tests; then they build a FAISS-indexed database of embeddings and a retrieval-augmented generation pipeline that translates natural language environmental questions into satellite-grounded answers, evaluated with an LLM-as-Judge setup using multiple LLMs in generator/system/judge roles.

Result: Many embedding dimensions align with specific land-surface properties; the full embedding reconstructs 12 of 26 variables with R^2 > 0.90, temperature and elevation near 0.97; relationships are robust across space (mean ΔR^2=0.017) and time (mean inter-year r=0.963). The QA system achieves good to very good scores from LLM judges (overall 3.74/5, with strong grounding 3.93 and coherence 4.25) across 360 query–response cycles.

Conclusion: AlphaEarth embeddings encode physically meaningful and stable environmental information, and can be reliably used as the backbone of operational land-surface and geospatial intelligence systems that answer natural-language environmental queries.

Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretability analysis of Google AlphaEarth's 64-dimensional embeddings against 26 environmental variables spanning climate, vegetation, hydrology, temperature, and terrain. Combining linear, nonlinear, and attention-based methods, we show that individual embedding dimensions map onto specific land surface properties, while the full embedding space reconstructs most environmental variables with high fidelity (12 of 26 variables exceed $R^2 > 0.90$; temperature and elevation approach $R^2 = 0.97$). The strongest dimension-variable relationships converge across all three analytical methods and remain robust under spatial block cross-validation (mean $ΔR^2 = 0.017$) and temporally stable across all seven study years (mean inter-year correlation $r = 0.963$). Building on these validated interpretations, we then developed a Land Surface Intelligence system that implements retrieval-augmented generation over a FAISS-indexed embedding database of 12.1 million vectors, translating natural language environmental queries into satellite-grounded assessments. An LLM-as-Judge evaluation across 360 query--response cycles, using four LLMs in rotating generator, system, and judge roles, achieved weighted scores of $μ= 3.74 \pm 0.77$ (scale 1--5), with grounding ($μ= 3.93$) and coherence ($μ= 4.25$) as the strongest criteria. Our results demonstrate that satellite foundation model embeddings are physically structured representations that can be operationalized for environmental and geospatial intelligence.

</details>


### [10] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: ACuRL is a reinforcement-learning framework that allows computer-use agents to continually adapt to specific digital environments without human-labeled data, using autonomous curriculum generation and an automatic evaluator (CUAJudge) for rewards.


<details>
  <summary>Details</summary>
Motivation: Computer-use agents operate in complex, changing digital environments, leading to frequent distribution shifts and unseen scenarios. Standard training with static datasets and heavy human annotation does not scale or generalize well, and agents risk catastrophic forgetting when adapted to new environments. The authors aim to enable continual, environment-specific learning for such agents while removing the need for costly human data and ensuring reliable evaluation signals.

Method: The authors propose ACuRL, an Autonomous Curriculum Reinforcement Learning framework. First, an agent explores the target digital environments to collect initial interaction data. Then, training proceeds iteratively: a curriculum task generator uses the collected experiences plus performance feedback from the previous iteration to synthesize new, tailored tasks that match the agent’s current skill level. For reward and evaluation, they introduce CUAJudge, an automatic evaluator trained or designed to approximate human judgment of task success for computer-use agents, achieving high agreement with humans. The framework also incorporates sparse parameter updates to support continual adaptation without overwriting prior knowledge.

Result: ACuRL enables effective continual learning for computer-use agents within single environments (intra-environment) and across different environments (cross-environment). On benchmarks, it yields 4–22% performance improvements over baselines while avoiding catastrophic forgetting on previously seen environments. Additional analyses show that successful adaptation is achieved through highly sparse parameter updates (e.g., modifying only about 20% of parameters), suggesting that the framework promotes targeted, robust changes rather than wholesale retraining.

Conclusion: The paper concludes that autonomous curriculum RL, combined with an accurate automatic evaluator, can continually adapt computer-use agents to diverse and evolving digital environments without any human-annotated data. ACuRL improves performance while preserving prior capabilities and achieves efficient adaptation via sparse parameter updates. This demonstrates a practical path toward scalable, self-improving CUAs in real-world settings, and the authors release data and code to facilitate further research.

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [11] [The Alignment Bottleneck in Decomposition-Based Claim Verification](https://arxiv.org/abs/2602.10380)
*Mahmud Elahi Akhter,Federico Ruggeri,Iman Munire Bilal,Rob Procter,Maria Liakata*

Main category: cs.CL

TL;DR: The paper studies when structured claim decomposition actually helps fact-checking, finding that it only improves performance with precisely aligned, granular evidence and cautious handling of sub-claim errors.


<details>
  <summary>Details</summary>
Motivation: Existing work suggests decomposing complex claims into sub-claims can help automated fact verification, but empirical results are mixed. The authors suspect two overlooked bottlenecks—how evidence is aligned to sub-claims and how sub-claim errors behave and propagate—are responsible for these inconsistencies. They aim to systematically study these factors.

Method: They build a new dataset of real-world, complex claims with temporally bounded evidence and human-annotated evidence spans for each sub-claim. Using this and other datasets (PHEMEPlus, MMM-Fact, COVID-Fact), they compare two evidence alignment regimes: (1) Sub-claim Aligned Evidence (SAE), where each sub-claim gets its own tailored evidence, and (2) Sub-claim Repeated Evidence (SRE), where the same claim-level evidence is reused across all sub-claims. They then analyze decomposition performance and robustness under different noise/error profiles in sub-claim labels, including conservative abstentions versus aggressive incorrect predictions.

Result: They find that claim decomposition improves verification performance only under SAE, when evidence is fine-grained and tightly aligned to sub-claims. Under SRE, where claim-level evidence is reused for all sub-claims (a common practical setup), decomposition usually fails to help and can even hurt performance, consistently across multiple datasets and domains. They also show that, given noisy sub-claim labels, conservative abstentions lead to less harmful error propagation than confident but wrong predictions, affecting the robustness of the final claim-level decision.

Conclusion: The effectiveness of structured claim decomposition is not guaranteed: it critically depends on precise evidence alignment and the nature of sub-claim errors. Future systems must focus on high-quality, granular evidence synthesis for each sub-claim and on calibrating sub-claim verifiers to favor conservative, well-calibrated predictions (including abstention) to limit error propagation.

Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand these factors, we introduce a new dataset of real-world complex claims, featuring temporally bounded evidence and human-annotated sub-claim evidence spans. We evaluate decomposition under two evidence alignment setups: Sub-claim Aligned Evidence (SAE) and Repeated Claim-level Evidence (SRE). Our results reveal that decomposition brings significant performance improvement only when evidence is granular and strictly aligned. By contrast, standard setups that rely on repeated claim-level evidence (SRE) fail to improve and often degrade performance as shown across different datasets and domains (PHEMEPlus, MMM-Fact, COVID-Fact). Furthermore, we demonstrate that in the presence of noisy sub-claim labels, the nature of the error ends up determining downstream robustness. We find that conservative "abstention" significantly reduces error propagation compared to aggressive but incorrect predictions. These findings suggest that future claim decomposition frameworks must prioritize precise evidence synthesis and calibrate the label bias of sub-claim verification models.

</details>


### [12] [Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382)
*Théo Lasnier,Wissam Antoun,Francis Kulumba,Djamé Seddah*

Main category: cs.CL

TL;DR: The paper mechanistically analyzes how language-switching backdoor triggers work in LLMs and finds that they hijack existing language-related circuits rather than forming isolated ones, suggesting new directions for detection and mitigation.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks on LLMs are known to be a serious security concern, but it is unclear how the backdoor triggers are represented and processed inside the model. Specifically, language-switching backdoors, where a trigger causes the model to change its output language, lack a detailed mechanistic understanding. The authors want to understand whether triggers form separate, dedicated circuits or whether they reuse and interfere with the model’s natural language-processing components, as this understanding is critical for designing effective defenses.

Method: The authors study the GAPperon family of LLMs (with 1B, 8B, and 24B parameters) that contain backdoor triggers inserted during pretraining which cause output language switching. They apply mechanistic-interpretability techniques, primarily activation patching, to identify where in the network the trigger representations are formed and which attention heads are responsible. They then compare these trigger-responsive heads with heads that naturally encode output language information, using measures like the Jaccard index to quantify overlap among top-ranked heads across model scales.

Result: They find that the formation of the trigger representation is localized to relatively early layers, between about 7.5% and 25% of the model depth. They also identify particular attention heads that process trigger information. Critically, there is substantial overlap between these trigger-activated heads and the heads that naturally control or encode the model’s output language. The overlap, measured via Jaccard indices over the top identified heads, ranges from 0.18 to 0.66 across model sizes, indicating nontrivial sharing of components. This demonstrates that the backdoor uses existing language-related heads rather than forming an entirely separate pathway.

Conclusion: The study concludes that language-switching backdoor triggers do not rely on isolated, dedicated circuits but instead co-opt existing language-processing components in LLMs. This entanglement between injected backdoor behavior and natural language representations implies that defenses should focus on monitoring and intervening in known functional components (such as language heads) instead of searching for completely hidden circuits. Additionally, mitigation might harness this overlap to selectively disrupt backdoor behavior while preserving normal language capabilities, informing the design of more targeted and effective backdoor detection and removal methods.

Abstract: Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify which attention heads process trigger information. Our central finding is that trigger-activated heads substantially overlap with heads naturally encoding output language across model scales, with Jaccard indices between 0.18 and 0.66 over the top heads identified. This suggests that backdoor triggers do not form isolated circuits but instead co-opt the model's existing language components. These findings have implications for backdoor defense: detection methods may benefit from monitoring known functional components rather than searching for hidden circuits, and mitigation strategies could potentially leverage this entanglement between injected and natural behaviors.

</details>


### [13] [When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents](https://arxiv.org/abs/2602.10384)
*Virginie Mouilleron,Théo Lasnier,Djamé Seddah*

Main category: cs.CL

TL;DR: The paper presents Multimodal Finance Eval, the first benchmark for evaluating French financial document understanding by vision-language models, showing that while current VLMs perform well on text and table extraction, they are brittle on chart interpretation and multi-turn reasoning.


<details>
  <summary>Details</summary>
Motivation: Although VLMs perform well on many document understanding tasks, their reliability in specialized, non-English, high-stakes domains like finance is underexplored. Financial documents are complex, mixing dense regulatory prose, numerical tables, and visual charts, and errors can have serious real-world impacts. There is no existing benchmark focused on multimodal, French financial document understanding, leaving a gap for evaluating and improving VLMs in this setting.

Method: The authors construct Multimodal Finance Eval, a multimodal benchmark built from real French financial documents such as investment prospectuses, KIDs, and PRIIPs. They create 1,204 expert-validated questions covering four task types: text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning. They then evaluate six open-weight VLMs ranging from 8B to 124B parameters, using an LLM-as-judge evaluation protocol to score model outputs on these tasks.

Result: Across the benchmark, the evaluated VLMs achieve high accuracy (about 85–90%) on text extraction and table comprehension tasks, indicating strong capabilities on relatively well-structured information. However, they perform substantially worse on chart interpretation tasks, with accuracies between 34–62%. In multi-turn conversational reasoning, the models exhibit a pronounced failure mode: initial errors propagate through subsequent turns, driving overall dialogue accuracy down to around 50%, largely independent of model size.

Conclusion: The study concludes that current VLMs are reliable for narrow, well-defined extraction tasks in French financial documents but are fragile when dealing with more complex, interactive, and multi-step analyses, especially those involving charts and multi-turn dialogue. Multimodal Finance Eval thus provides a challenging and realistic benchmark to diagnose these weaknesses and to guide future research toward more robust VLMs for high-stakes financial applications.

Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.
  These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.

</details>


### [14] [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://arxiv.org/abs/2602.10388)
*Zhongzhi Li,Xuansheng Wu,Yijiang Li,Lijie Hu,Ninghao Liu*

Main category: cs.CL

TL;DR: The paper proposes a new way to measure and increase the diversity of post-training data for LLMs using internal feature activations, and shows this improves downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods measure diversity using surface-level text metrics (e.g., lexical or syntactic variation), which weakly correlate with the task-relevant properties that actually affect LLM performance. The authors want a metric that better reflects meaningful, interpretable features inside the model so that they can systematically improve post-training datasets and thus downstream capabilities.

Method: They introduce Feature Activation Coverage (FAC), a diversity metric computed in the interpretable feature space obtained from a sparse autoencoder trained on model activations. FAC measures which internal features are covered by a dataset. On top of this, they propose FAC Synthesis: (1) train a sparse autoencoder on model activations to discover interpretable features; (2) compute FAC for a seed dataset to find under-covered or missing features; (3) generate synthetic data instances that specifically elicit those missing features, thus enriching the dataset in a targeted way. They evaluate across multiple LLM families (LLaMA, Mistral, Qwen).

Result: FAC better captures diversity relevant to downstream performance than text-only metrics. FAC Synthesis produces synthetic data that increases both the FAC score and downstream performance on multiple benchmarks: instruction following, toxicity detection, reward modeling, and behavior steering. They also empirically find that the sparse-autoencoder-derived feature spaces are to a significant extent shared and interpretable across different model families, supporting cross-model feature transfer and data design.

Conclusion: Measuring diversity in interpretable activation space via FAC is more aligned with model-relevant variation than traditional text metrics. Using this metric to drive data synthesis yields more diverse and more useful post-training data, improving performance across a range of tasks. The discovered feature space appears to generalize across model families, suggesting a practical route to data-centric optimization and cross-model knowledge transfer for LLM post-training.

Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.

</details>


### [15] [When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us](https://arxiv.org/abs/2602.10400)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: The paper uses a word-anxiety lexicon to analyze Twitter posts from the US and Canada, uncovering temporal and linguistic patterns in expressed anxiety.


<details>
  <summary>Details</summary>
Motivation: To understand when and how people express anxiety in everyday life using large-scale, passive data (social media), going beyond surveys and self-reports, and to relate anxiety to time, grammatical tense, and pronoun use.

Method: Apply a lexicon of word–anxiety and word–calmness associations to large collections of US and Canadian tweets. Compute anxiety scores over time of day and day of week, compare anxiety levels across past/present/future tense tweets, and analyze anxiety and calmness words in tweets containing different types of pronouns (1st/2nd/3rd person; subject vs. object).

Result: Anxiety on Twitter peaks around 8am and is lowest around noon; it is lower on weekends and higher mid-week. Tweets written in the past tense have the highest anxiety, and those in the future tense the lowest. Tweets with 3rd-person pronouns show more anxiety than those with 1st- or 2nd-person pronouns, and tweets with subject pronouns exhibit more anxiety than those with object pronouns.

Conclusion: Social media language reveals systematic patterns in expressed anxiety linked to time of day, day of week, tense, and pronoun focus. Anxiety tends to be higher when focusing on the past or on others and when using subject pronouns, and lower when focusing on the future or during weekends and midday. These patterns offer insight into how temporal and attentional focus relate to anxiety in everyday online expression.

Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that our levels of anxiety on social media exhibit systematic patterns of rise and fall during the day -- highest at 8am (in-line with when we have high cortisol levels in the body) and lowest around noon. Anxiety is lowest on weekends and highest mid-week. We also examine anxiety in past, present, and future tense sentences to show that anxiety is highest in past tense and lowest in future tense. Finally, we examine the use of anxiety and calmness words in posts that contain pronouns to show: more anxiety in 3rd person pronouns (he, they) posts than 1st and 2nd person pronouns and higher anxiety in posts with subject pronouns (I, he, she, they) than object pronouns (me, him, her, them). Overall, these trends provide valuable insights on not just when we are anxious, but also how different types of focus (future, past, self, outward, etc.) are related to anxiety.

</details>


### [16] [EVOKE: Emotion Vocabulary Of Korean and English](https://arxiv.org/abs/2602.10414)
*Yoonwon Jung,Hagyeong Shin,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: EVOKE is a large parallel English–Korean emotion vocabulary with rich annotations of word meanings, translations, and metaphorical/polysemous uses, intended as a flexible resource for multiple language and emotion research fields.


<details>
  <summary>Details</summary>
Motivation: Existing emotion lexicons are often limited to a single language, rely heavily on specific psychological theories, and provide incomplete coverage and shallow annotations (e.g., only basic categories or ratings). For nuanced cross-linguistic research on how emotions are expressed and conceptualized, especially between typologically different languages like English and Korean, researchers need a broad, theory-agnostic, and systematically annotated resource that captures translations, culture- or language-specific terms, polysemy, and metaphorical usage. EVOKE is introduced to fill this gap.

Method: The authors constructed a parallel vocabulary of English and Korean emotion words, collecting and curating terms in each language. They compiled 1,427 Korean and 1,399 English emotion-related words, then systematically annotated 819 Korean and 924 English adjectives and verbs. Annotations include multiple senses for each word, relationships between these senses, information on polysemy, identification of language-specific emotion words without clear equivalents in the other language, many-to-many translation links, and marking of emotion-related metaphors. The annotations are designed to be theory-agnostic so that different theoretical frameworks can be applied on top of the resource. The resulting dataset is released publicly as a GitHub repository.

Result: The outcome is the EVOKE dataset: a large-scale, parallel English–Korean emotion vocabulary with 1,427 Korean and 1,399 English entries. For a substantial subset (819 Korean and 924 English adjectives and verbs), the dataset encodes multiple meanings per word, sense relations, polysemy patterns, cross-lingual translation mappings, identification of language-specific emotion words, and annotations of emotion-related metaphors. This creates a rich, structured resource that surpasses prior emotion lexicons in coverage, detail, and cross-linguistic scope for these two languages.

Conclusion: EVOKE is, according to the authors, the most comprehensive, systematic, and theory-neutral parallel emotion word dataset available for English and Korean. Its detailed, flexible annotations make it suitable for diverse applications in emotion science, psycholinguistics, computational linguistics, and NLP, where different theoretical perspectives can be imposed on top of the resource. By being freely accessible on GitHub, it is intended to serve as a practical and reusable tool for cross-linguistic and conceptual research on emotion vocabulary.

Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.

</details>


### [17] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: The paper introduces NeSyS, a neuro-symbolic framework that combines large language models with symbolic world models to improve both accuracy and data efficiency in interactive environments.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but hallucinate and fail to strictly follow deterministic world rules, especially in edge cases, while symbolic world models are logically consistent but lack semantic richness. There is a need for a system that can leverage the strengths of both to serve as a reliable world model in interactive tasks.

Method: NeSyS alternates training between a neural world model (based on LLMs) and a symbolic world model. Trajectories that are poorly explained by one model are used to train the other. The symbolic model constrains the LLM during inference by directly adjusting its output probability distribution, while the LLM is fine-tuned only on data not already covered by symbolic rules, reducing redundant training.

Result: Across three interactive environments (ScienceWorld, Webshop, and Plancraft), NeSyS outperforms baseline methods in world model prediction accuracy and requires about 50% less training data for the neural component without sacrificing performance.

Conclusion: Integrating symbolic rules with LLM-based world models in a mutually constraining, alternating-training framework yields a system that is both more accurate and more data-efficient than purely neural or purely symbolic approaches for interactive reasoning tasks.

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [18] [Canvas-of-Thought: Grounding Reasoning via Mutable Structured States](https://arxiv.org/abs/2602.10494)
*Lingzhuang Sun,Yuxia Zhu,Ruitong Liu,Hao Liang,Zheng Sun,Caijun Jia,Honghao He,Yuchen Wu,Siyuan Li,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: The paper proposes Canvas-of-Thought (Canvas-CoT), which replaces purely textual chain-of-thought in multimodal LLMs with a structured HTML Canvas workspace that supports editable visual reasoning and state tracking, leading to better performance on complex visual and geometric tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Chain-of-Thought prompting for multimodal LLMs is mostly linear text, sometimes interleaved with static images. This linear, immutable reasoning history makes it hard to correct local errors without regenerating large portions of output, inflates token usage, and burdens the model with implicit state tracking. The problem is especially severe in high-dimensional visual domains like geometry diagrams and SVG design, where text-only CoT lacks explicit, editable visual structure and guidance.

Method: The authors introduce Canvas-of-Thought (Canvas-CoT), which uses an HTML Canvas as an external, structured reasoning substrate. The model manipulates this canvas via DOM-level CRUD operations (create, read, update, delete) on visual objects, enabling in-place edits to the reasoning state without rewriting the whole context. They further add a rendering-based critique loop that visually validates intermediate states against task constraints, providing explicit feedback that supplements or replaces textual descriptions in complex multimodal tasks.

Result: On benchmarks VCode, RBench-V, and MathVista, Canvas-CoT yields significantly better performance than previous multimodal CoT baselines. The improvements demonstrate gains in both reasoning accuracy and context efficiency, especially on tasks requiring precise visual or geometric reasoning and iterative state revisions.

Conclusion: Canvas-CoT establishes a new paradigm for multimodal reasoning where an editable visual canvas, rather than a purely textual chain, serves as the primary reasoning substrate. This approach reduces token overhead, externalizes and structures state, supports precise corrections, and leverages visual validation to solve complex tasks that are difficult to describe or track through text alone, leading to superior performance on challenging multimodal benchmarks.

Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when auxiliary visual elements are interleaved, they are often treated as static snapshots within a one-dimensional, unstructured reasoning chain. We argue that such approaches treat reasoning history as an immutable stream: correcting a local error necessitates either generating verbose downstream corrections or regenerating the entire context. This forces the model to implicitly maintain and track state updates, significantly increasing token consumption and cognitive load. This limitation is particularly acute in high-dimensional domains, such as geometry and SVG design, where the textual expression of CoT lacks explicit visual guidance, further constraining the model's reasoning precision. To bridge this gap, we introduce \textbf{Canvas-of-Thought (Canvas-CoT)}. By leveraging a HTML Canvas as an external reasoning substrate, Canvas-CoT empowers the model to perform atomic, DOM-based CRUD operations. This architecture enables in-place state revisions without disrupting the surrounding context, allowing the model to explicitly maintain the "ground truth". Furthermore, we integrate a rendering-based critique loop that serves as a hard constraint validator, providing explicit visual feedback to resolve complex tasks that are difficult to articulate through text alone. Extensive experiments on VCode, RBench-V, and MathVista demonstrate that Canvas-CoT significantly outperforms existing baselines, establishing a new paradigm for context-efficient multimodal reasoning.

</details>


### [19] [On the Robustness of Knowledge Editing for Detoxification](https://arxiv.org/abs/2602.10504)
*Ming Dong,Shiyi Tang,Ziyan Peng,Guanyi Chen,Tingting He*

Main category: cs.CL

TL;DR: The paper critically evaluates knowledge-editing-based detoxification for large language models and finds that its robustness is limited and often illusory.


<details>
  <summary>Details</summary>
Motivation: Detoxification methods for large language models are increasingly evaluated with automatic toxicity classifiers, which may not capture whether the model truly stops harmful behaviour. There is a need for a more rigorous, robustness-focused evaluation that can reveal when detoxification is superficial or fails in realistic scenarios.

Method: The authors propose a robustness-oriented evaluation framework for KE-based detoxification along three axes: optimisation robustness (sensitivity to the optimisation setup), compositional robustness (performance when editing multiple unsafe behaviours jointly), and cross-lingual robustness (effectiveness across languages). They empirically assess KE-based methods under these conditions and analyse resulting behaviours.

Result: They discover a frequent failure mode called pseudo-detoxification, where toxicity scores decrease but due to degenerate outputs rather than genuine safety improvements. They find that detoxification becomes less effective when multiple unsafe behaviours are edited at once, and that success in both monolingual and cross-lingual settings depends heavily on specific combinations of models and editing methods.

Conclusion: Knowledge-editing-based detoxification is not universally reliable: it is robust only for some models, when targeting a small number of unsafe behaviours, and in particular languages. Standard classifier-based toxicity metrics can be misleading, so more robust evaluation protocols are needed to assess and improve detoxification methods.

Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.

</details>


### [20] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: The paper introduces LHAW, a synthetic pipeline that turns well-specified long-horizon tasks into controlled underspecified variants to study and evaluate how workflow agents handle ambiguity and clarification.


<details>
  <summary>Details</summary>
Motivation: Reliable long-horizon workflow agents must handle ambiguous and underspecified instructions by asking clarifying questions, but current progress is hampered by the lack of a scalable, task-agnostic way to systematically generate and measure ambiguity across custom workflows.

Method: They propose LHAW, a modular and dataset-agnostic pipeline that takes any well-specified task and creates controllable underspecified variants by systematically removing information along four dimensions—Goals, Constraints, Inputs, and Context—with configurable severity levels. Instead of trusting LLM judgments of ambiguity, LHAW empirically validates each variant via agent trials, labeling them as outcome-critical, divergent, or benign based on terminal state divergence.

Result: Using LHAW, the authors generate 285 ambiguous task variants drawn from TheAgentCompany, SWE-Bench Pro, and MCP-Atlas, and perform formal analysis of how current agents detect, reason about, and resolve underspecification across these ambiguous scenarios.

Conclusion: LHAW serves as the first systematic, cost-sensitive framework to evaluate clarification behavior of long-horizon agents, facilitating the development and benchmarking of more reliable autonomous systems that robustly handle task ambiguity.

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [21] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: GRU-Mem improves long-context reasoning in LLMs by adding gated memory updates and an early-exit mechanism trained via reinforcement learning, achieving better accuracy and up to 4x faster inference than MemAgent.


<details>
  <summary>Details</summary>
Motivation: Long-context reasoning is important but difficult for LLMs because performance degrades as context grows. MemAgent tried recurrent, chunk-wise processing with textual memory, but it suffers from uncontrolled memory growth and no principled stopping criterion, causing inefficiency and instability. There is a need for a more structured, gate-controlled recurrent memory that updates only when necessary and can decide when to stop reading context.

Method: The authors propose GRU-Mem, a recurrent, chunk-by-chunk LLM reasoning framework inspired by GRUs. It adds two text-controlled gates: an update gate that decides whether to write new information into memory for a given chunk and an exit gate that decides whether to terminate further processing and answer. Both gates are trained end-to-end with reinforcement learning by defining two rewards, r^{update} and r^{exit}, which encourage correct decisions about when to update memory and when to exit the loop. The system operates like an RNN over context chunks but with these learned gating and stopping mechanisms.

Result: Across multiple long-context reasoning benchmarks, GRU-Mem yields higher reasoning performance and significantly better efficiency than the baseline MemAgent. It avoids unnecessary memory growth and reduces redundant processing of later chunks. Empirically, it achieves up to 400% (4x) speedup in inference while generally outperforming or matching the vanilla MemAgent in accuracy on long-context tasks.

Conclusion: Structured, gate-based memory control and early-exit policies, trained via RL, can substantially stabilize and accelerate long-context reasoning in LLMs. GRU-Mem demonstrates that mimicking GRU-style gating at the textual level allows selective memory updates and dynamic stopping, overcoming key drawbacks of naive recurrent memory agents and making long-context applications more practical.

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [22] [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604)
*Ailin Huang,Ang Li,Aobo Kong,Bin Wang,Binxing Jiao,Bo Dong,Bojun Wang,Boyu Chen,Brian Li,Buyun Ma,Chang Su,Changxin Miao,Changyi Wan,Chao Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengting Feng,Chengyuan Yao,Chunrui Han,Dan Ma,Dapeng Shi,Daxin Jiang,Dehua Ma,Deshan Sun,Di Qi,Enle Liu,Fajie Zhang,Fanqi Wan,Guanzhe Huang,Gulin Yan,Guoliang Cao,Guopeng Li,Han Cheng,Hangyu Guo,Hanshan Zhang,Hao Nie,Haonan Jia,Haoran Lv,Hebin Zhou,Hekun Lv,Heng Wang,Heung-Yeung Shum,Hongbo Huang,Hongbo Peng,Hongyu Zhou,Hongyuan Wang,Houyong Chen,Huangxi Zhu,Huimin Wu,Huiyong Guo,Jia Wang,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiashu Lv,Jiashuo Liu,Jiayi Fu,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yang,Jie Zhou,Jieyi Hou,Jing Bai,Jingcheng Hu,Jingjing Xie,Jingwei Wu,Jingyang Zhang,Jishi Zhou,Junfeng Liu,Junzhe Lin,Ka Man Lo,Kai Liang,Kaibo Liu,Kaijun Tan,Kaiwen Yan,Kaixiang Li,Kang An,Kangheng Lin,Lei Yang,Liang Lv,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lina Chen,Luck Ma,Mengqiang Ren,Michael Li,Ming Li,Mingliang Li,Mingming Zhang,Mingrui Chen,Mitt Huang,Na Wang,Peng Liu,Qi Han,Qian Zhao,Qinglin He,Qinxin Du,Qiuping Wu,Quan Sun,Rongqiu Yang,Ruihang Miao,Ruixin Han,Ruosi Wan,Ruyan Guo,Shan Wang,Shaoliang Pang,Shaowen Yang,Shengjie Fan,Shijie Shang,Shiliang Yang,Shiwei Li,Shuangshuang Tian,Siqi Liu,Siye Wu,Siyu Chen,Song Yuan,Tiancheng Cao,Tianchi Yue,Tianhao Cheng,Tianning Li,Tingdan Luo,Wang You,Wei Ji,Wei Yuan,Wei Zhang,Weibo Wu,Weihao Xie,Wen Sun,Wenjin Deng,Wenzhen Zheng,Wuxun Xie,Xiangfeng Wang,Xiangwen Kong,Xiangyu Liu,Xiangyu Zhang,Xiaobo Yang,Xiaojia Liu,Xiaolan Yuan,Xiaoran Jiao,Xiaoxiao Ren,Xiaoyun Zhang,Xin Li,Xin Liu,Xin Wu,Xing Chen,Xingping Yang,Xinran Wang,Xu Zhao,Xuan He,Xuanti Feng,Xuedan Cai,Xuqiang Zhou,Yanbo Yu,Yang Li,Yang Xu,Yanlin Lai,Yanming Xu,Yaoyu Wang,Yeqing Shen,Yibo Zhu,Yichen Lv,Yicheng Cao,Yifeng Gong,Yijing Yang,Yikun Yang,Yin Zhao,Yingxiu Zhao,Yinmin Zhang,Yitong Zhang,Yixuan Zhang,Yiyang Chen,Yongchi Zhao,Yongshen Long,Yongyao Wang,Yousong Guan,Yu Zhou,Yuang Peng,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yudi Zhao,Yue Peng,Yueqiang Lin,Yufan Lu,Yuling Zhao,Yunzhou Ju,Yurong Zhang,Yusheng Li,Yuxiang Yang,Yuyang Chen,Yuzhu Cai,Zejia Weng,Zetao Hong,Zexi Li,Zhe Xie,Zheng Ge,Zheng Gong,Zheng Zeng,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhiheng Hu,Zidong Yang,Zili Wang,Ziqi Ren,Zixin Zhang,Zixuan Wang*

Main category: cs.CL

TL;DR: Introduces Step 3.5 Flash, a sparse Mixture-of-Experts LLM designed to combine frontier-level agentic intelligence with high computational efficiency, using only 11B active parameters out of 196B total while matching larger frontier models on complex agent, coding, and math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Frontier LLM-based agents achieve strong reasoning and tool-use capabilities but are often prohibitively expensive and slow for multi-round, real-world industrial deployments. There is a need for models that preserve frontier-level agentic intelligence—especially sharp reasoning, reliable execution, and strong tool use—while drastically improving inference efficiency, latency, and scalability. Existing dense or inefficient MoE models struggle to balance performance with cost in long, interactive agent settings, particularly for math, coding, and tool-based tasks.

Method: The paper proposes Step 3.5 Flash, a sparse Mixture-of-Experts model with a 196B-parameter backbone but only 11B active parameters per token for inference. It employs an interleaved attention scheme combining 3:1 sliding-window and full attention to manage context efficiently, and uses Multi-Token Prediction with 3-token lookahead (MTP-3) to reduce latency in multi-turn interactions. On the training side, the authors design a scalable reinforcement learning framework that blends verifiable outcome signals (e.g., correctness in math/code) with preference-based feedback. This RL setup is engineered to remain stable under large-scale off-policy training and is applied for self-improvement across mathematics, coding, and tool-use domains.

Result: Step 3.5 Flash achieves high scores on several challenging benchmarks for reasoning and agentic behavior: 85.4% on IMO-AnswerBench (math), 86.4% on LiveCodeBench-v6 (2024.08–2025.05) for coding, 88.2% on tau2-Bench, 69.0% on BrowseComp with context management for browsing agents, and 51.0% on Terminal-Bench 2.0 for tool and terminal use. Its performance is reported as comparable to leading frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro, while using far fewer active parameters at inference time.

Conclusion: The work concludes that sparse MoE architectures, combined with attention optimizations and an RL framework leveraging verifiable and preference signals, can deliver frontier-level agentic performance at substantially lower inference cost. Step 3.5 Flash is positioned as a new efficiency frontier: a high-density, computationally economical foundation suitable for deploying sophisticated, multi-round agents in real-world industrial settings where latency and cost constraints are critical.

Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

</details>


### [23] [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609)
*Shuo He,Lang Feng,Xin Cheng,Lei Feng,Bo An*

Main category: cs.CL

TL;DR: The paper introduces KPO, a Kalman-filter-based method to stabilize reinforcement learning for large language models by smoothing token-level importance sampling ratios, improving performance on math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for large language models uses importance sampling (IS) ratios at the token level, which often exhibit high variance. This destabilizes large-scale policy optimization and can lead to training collapse. Existing approaches either use a single sequence-level IS ratio for all tokens or adjust each token’s IS ratio independently, both of which fail to respect the temporal, autoregressive structure of token generation and its off-policy deviations. The authors aim to understand and fix how structurally inconsistent local off-policy deviations distort policy-gradient updates across tokens.

Method: The authors empirically study token-level off-policy deviation and identify that it is structurally inconsistent across adjacent tokens, causing harmful variance in policy gradients. To fix this, they propose KPO (Online Causal Kalman Filtering for Policy Optimization). KPO treats the ideal IS ratio as a latent state evolving along the token sequence. A causal (online) Kalman filter updates this latent state autoregressively, using only current and past token information, to generate smoothed, structure-aware IS ratios per token. This retains meaningful local variation while suppressing noise spikes in the IS ratios, thus stabilizing learning.

Result: Experiments demonstrate that KPO yields more stable policy optimization and stronger performance than prior methods on challenging mathematical reasoning benchmarks. The filtered IS ratios reduce harmful variance and prevent training collapse while improving final task performance compared to state-of-the-art baselines.

Conclusion: Modeling token-wise importance sampling ratios as a latent dynamical process and filtering them with an online causal Kalman filter leads to more stable and effective reinforcement learning for large language models. KPO captures temporal structure in off-policy deviation across tokens, suppresses noise, and outperforms existing approaches on math reasoning tasks, indicating a promising direction for structure-aware variance reduction in RL for LLMs.

Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.

</details>


### [24] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: The paper proposes UMEM, a framework that jointly trains LLMs to both extract and manage self-evolving memory, improving generalization and performance of LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent memory systems mainly focus on how to store/update memory but treat the extraction of memory (what to store as knowledge) as fixed or static, causing overfitting to specific instances and accumulation of noisy, non-general memories. There is a need for a unified, trainable approach that optimizes both extraction and management for better generalization.

Method: UMEM is a self-evolving agent framework where the LLM is jointly optimized to perform memory extraction and memory management. It introduces Semantic Neighborhood Modeling, grouping semantically related queries into neighborhoods, and trains the model using a neighborhood-level marginal utility reward via GRPO (a reinforcement learning-style optimization). The reward evaluates how useful a memory is across an entire semantic cluster instead of just a single instance, encouraging generalizable memories.

Result: Across five benchmarks, UMEM outperforms strong baselines, achieving up to 10.67% improvement in multi-turn interactive tasks. It also shows monotonic performance improvement as the agent continuously evolves, indicating stable, continual learning behavior.

Conclusion: Jointly optimizing memory extraction and management with semantic neighborhood-level rewards leads to more robust, generalizable self-evolving memories in LLM-based agents. UMEM provides a principled way to reduce instance-specific noise and yields consistent performance gains over existing memory-augmented agent methods; the authors plan to release code and models.

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [25] [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: The paper studies how much benchmark performance of language models is explained by simple word-level overlap between pre-training data and evaluation benchmarks, finding that such overlap strongly predicts results.


<details>
  <summary>Details</summary>
Motivation: There is growing concern that improvements on standard language benchmarks may reflect memorization or superficial overlap with pre-training corpora rather than genuine generalization. The authors want to understand whether high benchmark scores mainly come from the statistical similarity between training data and test benchmarks in terms of simple word frequency patterns, rather than deeper reasoning abilities or distributional shifts.

Method: The authors quantify similarity between pre-training corpora and benchmark datasets using word-level unigram cross-entropy and word frequency statistics. They then conduct controlled experiments using four different pre-training datasets of varying sizes (8.5B–60B tokens), multiple model sizes (400M–3B parameters), and evaluate in zero-shot settings on ten benchmarks. By systematically varying the pre-training subset while holding the model architecture constant, they measure how benchmark performance correlates with unigram cross-entropy and word frequency overlap.

Result: They find a strong and consistent inverse correlation between word-level unigram cross-entropy and performance: benchmarks where the evaluation text is more predictable given pre-training word statistics yield higher scores. Larger pre-training subsets that preserve similar unigram cross-entropy further improve performance, which shows that both the degree of word overlap and the sheer amount of matching data matter. Overall, benchmark outcomes are heavily driven by simple word frequency alignment between pre-training and evaluation sets.

Conclusion: Standard language modeling benchmarks are often only weakly out-of-distribution relative to typical pre-training corpora. Simple word-overlap measures, such as unigram cross-entropy and frequency statistics, are surprisingly strong predictors of benchmark performance. This implies that current benchmarks may overestimate true generalization and that future evaluation protocols should better control for training–test overlap and shallow lexical similarity.

Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between pre-training corpora and evaluation datasets. We measure this overlap using word-level unigram cross-entropy and word frequency statistics, and perform controlled experiments across $10$ zero-shot benchmarks, $4$ pre-training datasets spanning $8.5\mathrm{B}$ to $60\mathrm{B}$ tokens, and model sizes ranging from $400\mathrm{M}$ to $3\mathrm{B}$ parameters. Our results demonstrate a robust inverse relationship between word-level unigram cross-entropy and benchmark performance, suggesting that widely used benchmarks are strongly influenced by word overlap between training and evaluation data. Thus, larger pre-training subsets with similar word-level unigram cross-entropy yield improved downstream results, indicating that word frequency statistics play an additional role in shaping benchmark scores. Taken together, these results suggest that many standard benchmarks are only weakly out-of-distribution relative to pre-training corpora, so that simple word-overlap statistics predict benchmark performance.

</details>


### [26] [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661)
*Daniel Gallagher,Gerhard Heyer*

Main category: cs.CL

TL;DR: The paper tests how well transformer language models handle Georgian split-ergative case marking using a controlled syntactic test suite.


<details>
  <summary>Details</summary>
Motivation: To understand whether modern transformer-based language models can learn and generalize a rare and complex case alignment system—split ergativity in Georgian—especially given that such phenomena are underrepresented and low-resource, and existing benchmarks rarely target them explicitly.

Method: The authors automatically generate minimal pairs from a Georgian treebank using the Grew query language, creating 370 controlled syntactic tests grouped into seven tasks. Each test manipulates permutations of nominative, ergative, and dative noun forms within clauses, checking models’ ability to assign correct subject and object case marking. They then evaluate five encoder-only and two decoder-only transformer models with word- and sentence-level accuracy on these tests.

Result: Across all syntactic configurations, models systematically perform worst on correctly assigning the ergative case and best on nominative, with dative in between. Model performance closely tracks the frequency distribution of the three cases in training data (NOM > DAT > ERG), indicating that rarity of forms and their specific syntactic function impact learnability. Overall, even large transformers struggle with the ergative case in Georgian.

Conclusion: Transformer language models have limited competence in handling Georgian split-ergative case marking, especially for the rare and functionally specific ergative case, likely due to both data scarcity and structural complexity. The released dataset and treebank-based minimal-pair generation method provide a reusable framework for fine-grained syntactic evaluation in low-resource and typologically rare languages where benchmarks are scarce.

Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.

</details>


### [27] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: The paper introduces LoCoMo-Plus, a benchmark to evaluate long-term cognitive conversational memory in LLMs, focusing on implicit user constraints rather than simple factual recall.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue benchmarks emphasize explicit factual recall and short-term memory, which do not capture realistic needs where systems must remember and respect users' implicit states, goals, and values over long conversations. There is a need to evaluate cognitive memory under conditions where the cues for constraints and their later triggers are semantically disconnected.

Method: The authors build LoCoMo-Plus, a benchmark composed of long, realistic conversational scenarios where key user constraints are expressed implicitly and are not directly referenced when later decisions are required. They define the setting as cue–trigger semantic disconnect, and design an evaluation framework based on constraint consistency rather than string matching or explicit task-type prompts. They test multiple backbone LLMs, retrieval-based approaches, and memory-system configurations on this benchmark.

Result: Experiments show that current LLMs and memory-augmented systems struggle with cognitive memory in the cue–trigger semantic disconnect setting. The benchmark exposes failure modes that existing factual-recall-oriented benchmarks do not reveal, indicating current systems' limitations in tracking and applying latent constraints over long dialogues.

Conclusion: Cognitive conversational memory—tracking and applying latent user constraints across long contexts—is still an open challenge for LLM systems. LoCoMo-Plus and its constraint consistency evaluation framework provide a more realistic and discriminative way to assess this capability, pointing to the need for improved memory mechanisms beyond simple retrieval and string-based evaluation.

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [28] [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732)
*Alaa Elsetohy,Sama Hadhoud,Haryo Akbarianto Wibowo,Chenxi Whitehouse,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CL

TL;DR: Macaron is a multilingual, culturally grounded reasoning benchmark created with language-agnostic templates to systematically vary reasoning types and cultural aspects across many languages.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual benchmarks either translate English-centered content, losing genuine cultural grounding, or focus on culture but without good control over what kinds of reasoning are being tested. This makes it hard to diagnose how well multilingual LLMs handle culture-specific reasoning across languages.

Method: The authors design 100 language-agnostic question templates that explicitly encode 7 reasoning types and 22 cultural aspects. Native annotators instantiate these templates into concrete, scenario-aligned multiple-choice questions in both English and local languages, and also produce systematically derived True/False variants. The resulting dataset, Macaron, spans 11,862 instances across 20 countries/cultural contexts, 10 scripts, and 20 languages, including several low-resource languages and Arabic dialects. They then evaluate 21 multilingual LLMs in zero-shot settings on this benchmark, comparing performance across reasoning types, cultural aspects, languages, and model families (reasoning-focused vs open-weight).

Result: Macaron yields a large, controlled benchmark that covers diverse cultural contexts and reasoning types in many languages. In zero-shot experiments, reasoning-mode multilingual LLMs perform best and show nearly equal accuracy on English and local-language questions. In contrast, open-weight multilingual models perform much worse on local languages, often dropping to near random accuracy on the True/False variants. Culture-grounded mathematical and counting questions emerge as the most challenging category across models.

Conclusion: The study shows that carefully templated, culturally grounded multilingual benchmarks like Macaron can reveal substantial performance gaps that are invisible in translation-based datasets. Reasoning-oriented LLMs generalize more robustly across languages and culturally specific scenarios, while many open-weight models still struggle, particularly on local-language and numerically oriented, culture-infused tasks. Macaron provides a systematic tool for evaluating and improving culturally aware multilingual reasoning in LLMs.

Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.

</details>


### [29] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: They propose a new robustness metric, Token Constraint Bound (δ_TCB), that measures how much an LLM’s internal state can be perturbed before its top next-token choice changes.


<details>
  <summary>Details</summary>
Motivation: LLMs change their predictions sharply with small prompt/context tweaks, hurting reliability. Existing metrics like accuracy and perplexity only look at output probabilities and can miss how fragile the underlying internal representation is to small perturbations. The authors want a way to quantify local robustness of next-token predictions with respect to changes in internal states and context.

Method: They define Token Constraint Bound (δ_TCB) as the maximum size of an internal state perturbation that does not lead to a significant change in the dominant next-token prediction. This is formalized using the geometry of the output embedding (logit) space, relating perturbation sizes to decision boundary margins between the top token and competitors. They compute δ_TCB on LLMs across prompts and tasks, then compare it with perplexity and prompt-engineering variants to see how it tracks stability and performance.

Result: They find that δ_TCB captures local prediction robustness that perplexity misses. Higher δ_TCB correlates with more effective prompt formulations. They also show concrete cases in in-context learning and free-form text generation where perplexity looks fine but δ_TCB reveals unstable predictions and hidden sensitivities to prompt changes.

Conclusion: δ_TCB is a principled robustness measure for LLM next-token predictions, complementary to accuracy and perplexity. It provides insight into the stability of the model’s internal predictive commitment and can be used to analyze, diagnose, and potentially improve contextual stability and prompt design for LLMs.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [30] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: The paper introduces Reinforced Curriculum Pre-Alignment (RCPA), a post-training method to adapt vision-language models to specialized domains while preserving general capabilities, using a curriculum-based reinforcement learning strategy.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models excel at general tasks but struggle in specialized domains like medical imaging or geometry. Supervised fine-tuning helps domain performance but causes catastrophic forgetting of general abilities. Continual pretraining, successful in LLMs, is too costly and data-hungry for VLMs. Existing RL-based alignment such as GRPO can preserve general abilities but often collapses when the model initially lacks domain knowledge. There is a need for an efficient post-training adaptation approach that can safely build new domain expertise without losing broad multimodal competence.

Method: The paper proposes Reinforced Curriculum Pre-Alignment (RCPA), a curriculum-based RL post-training framework. RCPA starts with a partial-output constraint phase, where only parts of the model’s generations are optimized, allowing controlled exposure to new domain concepts. As the model’s domain familiarity grows, the training curriculum gradually relaxes constraints and shifts toward full-generation optimization using RL-style preference alignment (inspired by GRPO). This progressive modulation of objectives and constraints aims to avoid optimization collapse while steadily improving domain specialization and maintaining general capabilities.

Result: Through experiments on multiple specialized-domain tasks (e.g., medical or geometric vision-language benchmarks) and standard general-purpose benchmarks, models trained with RCPA achieve better domain performance than baselines such as vanilla SFT and standard RL alignment, while suffering less catastrophic forgetting of general abilities. The results show more stable optimization dynamics and improved trade-off between specialization and generalization.

Conclusion: RCPA offers a practical and efficient post-training paradigm for adapting VLMs to new domains without sacrificing general multimodal skills. By introducing a curriculum-aware, staged RL optimization that moves from partially constrained to fully open-ended generation, it mitigates optimization collapse and catastrophic forgetting. The approach demonstrates strong empirical gains and suggests a promising direction for building domain-adaptive yet broadly capable vision-language models.

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [31] [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: The paper assesses whether current large language models can reliably extract structured, numerically precise study data for systematic reviews and meta-analyses, finding major structural and numerical reliability failures even when entity recognition is adequate.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews and meta-analyses require converting narrative scientific articles into structured records that accurately preserve variables, roles, methods, and effect sizes. Although LLMs have advanced rapidly and are being proposed for automation of evidence synthesis, it is unknown whether they can meet these stringent structural and numerical requirements, which go beyond simple entity recognition.

Method: The authors design a structural, diagnostic evaluation framework that treats evidence extraction as a sequence of schema-constrained queries with increasing relational and numerical complexity. They compile a manually curated corpus across five scientific domains, create a unified suite of queries and an evaluation protocol, and test two state-of-the-art LLMs both on single-document inputs and on long-context, multi-document inputs to identify where extraction fails.

Result: LLMs perform moderately well on simple, single-property extraction tasks but their performance degrades sharply when tasks require maintaining stable bindings between variables, roles, statistical methods, and effect sizes. Extraction of full meta-analytic association tuples is almost never reliable. Long-context, multi-document inputs worsen these structural errors. Small extraction inaccuracies compound during downstream aggregation, leading to unreliable corpus-level meta-analytic statistics.

Conclusion: Current LLMs are not suitable for fully automated meta-analysis because their main failures are structural rather than entity-recognition errors. They exhibit role reversals, cross-analysis binding drift, compression of distinct instances in dense result sections, and numeric misattribution, demonstrating insufficient structural fidelity, relational binding, and numerical grounding. Therefore, robust automation of meta-analytic evidence extraction will require methods that specifically address these structural reliability issues.

Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).

</details>


### [32] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: The paper proposes LSCL, a method to learn and express the knowledge boundaries (confidence) of black-box LLMs, reducing hallucinations using a distilled deep model from LLM outputs and token probabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs hallucinate because they cannot explicitly express whether a query is inside or outside their internal knowledge boundaries, especially in black-box settings where internal parameters are hidden and only an API is available. Existing boundary-expression methods assume white-box access, leaving a gap for black-box LLMs that are common in practice.

Method: Within a knowledge distillation framework, the authors build a deep neural model that takes as input: (1) the user question, (2) the black-box LLM’s textual answer, and (3) its token-level probabilities (when available). This model is trained to map these signals to an estimate of the LLM’s internal knowledge state, effectively outputting a confidence or boundary judgement. They also design an adaptive variant that removes the need for token probabilities for APIs that don’t provide them.

Result: On multiple public datasets and with several widely-used black-box LLMs, LSCL yields substantially better accuracy and recall in recognizing when the LLM is within or beyond its knowledge boundary, compared with existing baselines. The adaptive variant without token probabilities performs slightly worse than full LSCL but still surpasses all baselines.

Conclusion: LSCL provides an effective, general way to express knowledge boundaries for black-box LLMs, thereby helping mitigate hallucinations in real-world API-only settings. Even when token probabilities are unavailable, the adaptive alternative can still approximate LSCL’s performance and improve over existing approaches.

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [33] [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886)
*Zhuohan Xie,Rania Elbadry,Fan Zhang,Georgi Georgiev,Xueqing Peng,Lingfei Qian,Jimin Huang,Dimitar Dimitrov,Vanshikaa Jani,Yuyang Dai,Jiahui Geng,Yuxia Wang,Ivan Koychev,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: The paper introduces FinMMEval 2026, the first multilingual, multimodal evaluation lab for financial LLMs, with three tasks covering exam-style QA, multilingual QA, and financial decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing financial NLP benchmarks are mostly monolingual, text-only, and limited to narrow subtasks, which fails to capture the complexity of real-world financial reasoning, multilingual usage, and multimodal inputs. There is a need for a comprehensive, globally inclusive evaluation suite for financial LLMs.

Method: The authors design a CLEF lab, FinMMEval 2026, that defines three interconnected benchmark tasks: (1) Financial Exam Question Answering, (2) Multilingual Financial Question Answering (PolyFiQA), and (3) Financial Decision Making. These tasks jointly test models on understanding, reasoning, and action across multiple languages and modalities. They commit to releasing datasets and evaluation infrastructure publicly.

Result: As this is a lab/setup paper, the main result is the definition and organization of the evaluation framework and its tasks, rather than empirical model performance. The outcome is a benchmark suite and resources for the community to evaluate financial LLMs in a multilingual, multimodal setting.

Conclusion: FinMMEval 2026 provides a comprehensive, reproducible benchmark framework aimed at fostering robust, transparent, and globally inclusive financial AI by evaluating models' abilities to reason, generalize, and make decisions across languages and modalities, with open resources to support ongoing research.

Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>


### [34] [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947)
*Kacper Dudzic,Karolina Drożdż,Maciej Wodziński,Anastazja Szuła,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: The paper examines how autistic individuals experience time, focusing on unpredictability and desynchronization with the social world, using combined phenomenological interviews and computational narrative analysis.


<details>
  <summary>Details</summary>
Motivation: To address core temporal disturbances in autism—like feeling out of sync and facing unpredictable experiences—while overcoming limitations of deficit-focused models, small qualitative samples, and lack of phenomenological grounding in computational work.

Method: The authors combined three methods: (A) structured phenomenological interviews with autistic and control participants using the Transdiagnostic Assessment of Temporal Experience; (B) computational lexical analysis of a purpose-built corpus of autistic autobiographical narratives, especially temporal vocabulary and sentiment; and (C) a replication of an existing computational narrative-flow study to test whether autistic autobiographies are phenomenologically authentic (closer to real vs. imaginary stories).

Result: Interviews showed that unpredictability of experience was the main temporal difference between autistic and control groups. Computational text analysis converged with this: autistic narratives used more negatively valenced temporal language, particularly in the “Immediacy & Suddenness” category, with outlier terms like “unpredictably,” “precipitously,” and “abruptly” being strongly negative. Narrative-flow metrics indicated that autistic texts patterned like genuine autobiographical accounts rather than fictional/imagined narratives.

Conclusion: Temporal difficulties in autism are primarily about lived unpredictability and discontinuity in experience, not about an inherent deficit in how autistic people construct narratives. Integrating phenomenology with computational analysis provides a non-deficit-based, empirically grounded understanding of autistic temporality.

Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>


### [35] [I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification](https://arxiv.org/abs/2602.10832)
*Hardi Garari,Hossein Hassani*

Main category: cs.CL

TL;DR: The paper builds the first speech dataset and neural models for identifying whether speech is in the Hewlêri subdialect of Sorani Kurdish, achieving up to ~96% accuracy with an RNN on 5-second clips.


<details>
  <summary>Details</summary>
Motivation: Native Language Identification has been widely studied for major, well-resourced languages, but there is a lack of work on dialects and subdialects, especially for under-resourced languages like Kurdish. This limits applications in areas such as forensic linguistics and linguistic research in these communities. The authors aim to fill this gap by focusing on the Hewlêri subdialect of Sorani Kurdish, for which no NLI speech dataset previously existed.

Method: The authors collected roughly 24 hours of interview speech from 40 speakers (17 female, 23 male), including both native and non-native speakers of the Hewlêri subdialect spoken in Hewlêr (Erbil). They segmented audio into clips of varying lengths (1–60 seconds) and experimented with different data handling strategies (undersampling, oversampling, cross-validation). They then trained and evaluated three neural architectures—Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN)—across a total of 66 experiments, using an 80:10:10 train/validation/test split for their best-performing setup.

Result: Among the three neural models, the RNN achieved the best performance, reaching 95.92% accuracy on a 5-second audio segmentation task under an 80:10:10 train/validation/test split. The experiments also explored how performance varies with segment length and sampling strategies, though the abstract highlights the 5-second RNN configuration as optimal.

Conclusion: The study demonstrates that NLI is feasible and highly accurate for the Hewlêri subdialect using relatively short audio segments and RNN-based models. It provides the first dedicated Hewlêri speech dataset for NLI within the Sorani Kurdish dialect, establishing a valuable resource for future research in speech processing, dialectology, and forensic or applied linguistics in Kurdish and other low-resource dialectal settings.

Abstract: Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewlêri, a subdialect spoken in Hewlêr (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewlêri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewlêri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.

</details>


### [36] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: The paper proposes C-MOP, a framework for stabilizing and improving automatic prompt optimization for LLMs using contrastive sampling and momentum-guided semantic clustering, achieving better performance than prior SOTA methods and allowing a 3B-parameter model to surpass a 70B-parameter domain-specific model.


<details>
  <summary>Details</summary>
Motivation: Automatic prompt optimization can significantly boost LLM performance, but existing methods suffer from noisy and conflicting update signals, which destabilize and limit the effectiveness of optimization. The authors aim to design a more stable and reliable optimization framework that can extract clearer learning signals from batches of prompts and over multiple iterations.

Method: The authors introduce C-MOP (Cluster-based Momentum Optimized Prompting), which consists of two main components: (1) Boundary-Aware Contrastive Sampling (BACS), which uses batch-level information to construct triplets of Hard Negatives, Anchors, and Boundary Pairs to model the typical representation and decision boundaries between positive and negative prompt samples; and (2) Momentum-Guided Semantic Clustering (MGSC), which adds a textual momentum mechanism with temporal decay, aggregating historical gradient information to identify persistent consensus and reduce semantic conflicts over iterations. Together, these help guide prompt evolution more stably and effectively.

Result: In extensive experiments, C-MOP consistently outperforms strong baselines such as PromptWizard and ProTeGi, with average performance gains of 1.58% and 3.35%, respectively. Furthermore, applying C-MOP enables a general-purpose LLM with only 3B activated parameters to outperform a much larger, 70B-parameter domain-specific dense LLM, emphasizing the strong practical benefit of the proposed optimization scheme.

Conclusion: C-MOP is an effective and stable framework for automatic prompt optimization, addressing noisy and conflicting update signals by leveraging boundary-aware contrastive sampling and momentum-guided semantic clustering. Its consistent improvements over SOTA baselines and its ability to elevate a relatively small, general LLM beyond a much larger, specialized model show that careful prompt evolution can substitute for massive model scaling in some settings.

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [37] [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993)
*Ivan Vulić,Adam Grycner,Quentin de Laroussilhe,Jonas Pfeiffer*

Main category: cs.CL

TL;DR: LoRA-Squeeze improves standard LoRA fine-tuning by first training with a higher rank and then compressing to a lower rank using RSVD, yielding better performance-size trade-offs than directly training low-rank adapters.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA, while popular for parameter-efficient fine-tuning, still requires manual pre-selection of rank and rank-specific hyperparameters, and becomes complex to deploy when using heterogeneous ranks or more elaborate LoRA variants. There is a need for a simple approach that avoids brittle rank choices while improving performance for a given adapter size and reducing deployment complexity.

Method: The method, LoRA-Squeeze, proposes to start from a deliberately higher source rank during fine-tuning, learn an expressive weight update, reconstruct or approximate the full update matrix, and then apply Randomized SVD (RSVD) to factorize it into a new LoRA module at a lower target rank. This can be done post-hoc after training, or dynamically during training via a gradual rank-annealing schedule that repeatedly compresses and reduces the rank.

Result: Across 13 text tasks and 10 vision-language tasks, LoRA-Squeeze’s post-hoc compression yields lower-rank adapters that often outperform adapters trained directly at the same low target rank. Additional light fine-tuning after compression further amplifies these gains. The dynamic, in-training rank annealing variant consistently achieves the best trade-off between adapter size and performance among the tested LoRA configurations.

Conclusion: Learning with a higher-rank LoRA and then compressing it using RSVD is more effective than training directly with a low-rank adapter. LoRA-Squeeze offers a simple, implementation-friendly approach that mitigates rank-selection issues, reduces deployment complexity of heterogeneous LoRA modules, and provides superior performance-size trade-offs for PEFT across both text and vision-language tasks.

Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>


### [38] [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028)
*Artsvik Avetisyan,Sachin Kumar*

Main category: cs.CL

TL;DR: This paper evaluates interpretable linguistic markers from spontaneous speech to detect early dementia using machine learning and statistical validation.


<details>
  <summary>Details</summary>
Motivation: Early cognitive decline subtly affects natural language use, but many automated dementia screening methods rely on opaque models or lexical content that may not generalize well. Clinicians need transparent, linguistically interpretable markers derived from spontaneous speech that remain effective under realistic evaluation settings and are grounded in established linguistic theory.

Method: The authors use spontaneous speech transcripts from the DementiaBank Pitt Corpus and construct three linguistic representations: (1) raw cleaned text; (2) a POS-enhanced representation combining lexical items with their part-of-speech tags; and (3) a POS-only representation focusing on syntactic/grammatical information. They train logistic regression and random forest classifiers under two evaluation protocols: transcript-level train-test splits and subject-level five-fold cross-validation that avoids speaker overlap. Model interpretability is assessed via global feature importance, and group differences in key features are statistically tested with Mann-Whitney U tests and Cliff’s delta effect sizes.

Result: All three representations yield stable classification performance, with syntactic and grammatical features remaining strongly discriminative even when lexical content is removed. Subject-level cross-validation, which is more clinically realistic, produces somewhat lower but consistent and robust performance, especially for POS-enhanced and POS-only inputs. Statistical tests show significant differences between dementia and control groups in functional word usage, lexical diversity, sentence structure, and discourse coherence, and these patterns match the most important features identified by the models.

Conclusion: Abstract, linguistically grounded features of spontaneous speech provide robust, interpretable markers of early cognitive decline and retain discriminative power without relying on specific lexical items. Coupling interpretable machine learning (logistic regression, random forests with feature importance) with non-parametric statistical validation supports transparent, clinically meaningful language-based screening methods for dementia that generalize under realistic subject-level evaluation.

Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.
  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.
  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.
  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.

</details>


### [39] [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044)
*Kevin Yandoka Denamganaï,Kartic Subr*

Main category: cs.CL

TL;DR: The paper studies how to invert language models by finding prompts that produce a desired target output, using a differentiable reformulation and gradient-based optimization.


<details>
  <summary>Details</summary>
Motivation: While language models are powerful generators, it is unclear how to systematically construct prompts that lead to a specific desired output. Existing work rarely addresses this "inversion" problem, which is important for controllability, interpretability, and prompt engineering. The authors want a principled, efficient way to compute prompts that cause a frozen LM to emit a given target sequence.

Method: They reformulate language models as functions over sequences of token distributions instead of discrete token sequences, which makes the whole process differentiable. Using this view, they construct an end-to-end differentiable version of a frozen LM (DLM) and then pose prompt search as a gradient-based optimization problem. Starting from an initial prompt representation, they apply gradient descent to adjust the prompt so that the LM’s output distribution matches a chosen target sequence of tokens.

Result: Experiments and ablation studies show that their differentiable LM (DLM) setup can reliably and efficiently find prompts of lengths 10 and 80 that cause white-box LMs to output target sequences of length 20. This is demonstrated on several off-the-shelf language models, indicating that the method works across different architectures without model-specific modifications.

Conclusion: Prompt inversion of frozen language models can be cast as a differentiable optimization problem by operating on token distributions instead of discrete tokens. With this DLM framework, gradient descent becomes an effective tool for computing prompts that yield a specified target output, enabling more systematic control over LM behavior and opening paths for future work on interpretability and controllable generation.

Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).

</details>


### [40] [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908)
*Masataka Yoneda,Yusuke Matsushita,Go Kamoda,Kohei Suenaga,Takuya Akiba,Masaki Waga,Sho Yokoi*

Main category: cs.CL

TL;DR: They propose an ultra-fast, flexible search algorithm that can search trillion-token natural language corpora in under 0.3 seconds while supporting semantically relaxed (fuzzy) matching, and show it outperforms existing systems and can detect benchmark contamination.


<details>
  <summary>Details</summary>
Motivation: Existing search systems for massive text corpora either struggle with latency, do not scale well to trillions of tokens, or cannot robustly handle semantic variations like insertions, deletions, and substitutions in queries. As language model training data and text repositories grow to trillions of tokens, there is a pressing need for a search method that is both extremely fast and tolerant to natural linguistic variability, in order to enable tasks like dataset auditing, contamination detection, and flexible corpus exploration across many languages.

Method: They design a search algorithm built on suffix-array-based string matching, optimized for disk access patterns to support ultra-fast exact lookups on trillion-scale corpora. To support semantically relaxed (soft) matching without exploding the search space, they introduce two key algorithmic techniques: (1) a disk-aware index and lookup scheme that minimizes I/O latency, and (2) dynamic corpus-aware pruning, which uses language statistics of the underlying corpus to prune unlikely matches and effectively control the combinatorial growth caused by substitutions, insertions, and deletions. They provide theoretical analysis showing that leveraging natural language statistics prevents exponential search-space growth in query length, and they implement this design in a multi-language system.

Result: On the 1.4-trillion-token FineWeb-Edu corpus, their implementation achieves sub-0.3-second latency for soft search queries and significantly outperforms state-of-the-art baselines, including infini-gram, infini-gram mini, and SoftMatcha, in terms of search speed. They empirically validate their theoretical claims about scalability and demonstrate that their pruning effectively keeps search practical even with semantic relaxations. As an application, they show the system can uncover benchmark contamination in training datasets that previous methods failed to detect, and they deploy an online demo covering seven languages.

Conclusion: The proposed suffix-array-based, disk-aware, and corpus-aware-pruned search algorithm enables practical, ultra-low-latency soft search over trillion-token natural language corpora. By exploiting statistical regularities of language, it avoids the exponential blow-up usually associated with fuzzy matching, scales better than existing methods, and supports impactful applications such as more thorough benchmark contamination detection. The work establishes a new state of the art in large-scale, flexible corpus search and provides a publicly accessible multi-language demo.

Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.

</details>


### [41] [Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065)
*Dingkun Zhou,Shuchang Pan,Jiachen Lian,Siddharth Banerjee,Sarika Pasumarthy,Dhruv Hebbar,Siddhant Patel,Zeyi Austin Li,Kan Jen Cheng,Sanay Bordia,Krish Patel,Akshaj Gupta,Tingle Li,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: They propose a Graph-of-Thoughts framework for modeling and predicting conversational behavior in full-duplex dialogue, using hierarchical intent and speech-act labels over time.


<details>
  <summary>Details</summary>
Motivation: To build more natural, full-duplex interactive spoken dialogue systems, we need models that can capture and reason over the implicit chain of thoughts that structures human conversation, not just surface-level turns or text.

Method: They define a multi-level perception framework with a hierarchical labeling scheme linking high-level communicative intents to low-level speech acts and their temporal/causal dependencies. They collect a high-quality corpus of event-rich dialogues with human annotations for these labels. Using this, they train a transformer-based model whose streaming predictions are represented as an evolving Graph-of-Thoughts, allowing the system to forecast the next speech act, produce textual justifications, and iteratively refine its own reasoning.

Result: On both synthetic and real full-duplex dialogue data, the framework achieves strong behavior detection performance, yields interpretable chains of reasoning, and supports forecasting of upcoming speech acts and their rationales.

Conclusion: Modeling conversation as an evolving Graph-of-Thoughts over hierarchical intents and speech acts enables more robust and interpretable conversational reasoning, and offers a foundation for benchmarking reasoning in full-duplex spoken dialogue systems.

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [42] [SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081)
*Sebastian Wind,Jeta Sopa,Laurin Schmid,Quirin Jackl,Sebastian Kiefer,Fei Wu,Martin Mayr,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: The paper introduces SteuerEx, the first open German tax law exam benchmark, and SteuerLLM, a 28B-parameter LLM specialized for German tax law, showing that targeted domain adaptation beats sheer model size on realistic legal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs struggle with domains that require strict formal rules, precise terminology, legally binding structures, and numerical accuracy—tax law is a prototypical example. Existing benchmarks and models do not adequately capture the structure and grading logic of real university tax law exams, limiting both evaluation realism and progress in domain-specific legal AI. The authors aim to create a realistic, open benchmark and a specialized model to study and improve LLM performance in German tax law.

Method: 1) Benchmark construction: They algorithmically generate SteuerEx from authentic German university tax law exams, covering six core domains and multiple academic levels. Questions are expert-validated and evaluated using a statement-level, partial-credit scheme that mimics real exam grading. 2) Model development: They build SteuerLLM, a 28B-parameter domain-adapted LLM for German tax law. SteuerLLM is trained on a large synthetic dataset produced from authentic examination materials using a controlled retrieval-augmented generation pipeline, aligning the model with exam-style reasoning, statutory citation, and numerical requirements. 3) Evaluation: They compare SteuerLLM against general instruction-tuned LLMs of similar and larger size on SteuerEx using the partial-credit evaluation framework.

Result: SteuerLLM consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, surpasses substantially larger models on the SteuerEx benchmark. This superior performance spans multiple tax law domains and academic difficulty levels, showing marked gains in legal reasoning quality, statutory citation accuracy, and exam-style structured answers.

Conclusion: Domain-specific adaptation—via specialized training data, retrieval-augmented generation, and tailored architecture—can be more important than raw parameter count for complex, rule-bound legal tasks like German tax law exams. The authors provide an open, realistic benchmark (SteuerEx), a strong specialized model (SteuerLLM), and all accompanying data, weights, and evaluation code to foster reproducible research and future progress in domain-specific legal AI. Their work suggests that targeted specialization is a promising path for high-stakes legal reasoning applications.

Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.

</details>


### [43] [DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089)
*Yicheng Chen,Zerun Ma,Xinchen Xie,Yining Li,Kai Chen*

Main category: cs.CL

TL;DR: The paper introduces DataChef-32B, an LLM that automatically generates end-to-end data recipes (data processing pipelines) for adapting base LLMs to target tasks, using online reinforcement learning and a proxy reward to predict downstream performance, achieving performance comparable to human-curated recipes.


<details>
  <summary>Details</summary>
Motivation: High-quality training data is a key driver of LLM performance, but constructing data recipes (pipelines for transforming raw data into training corpora) is currently manual, labor-intensive, and requires expert iteration. Existing uses of LLMs in data pipelines automate isolated steps but not the full end-to-end design. The authors aim to automate the entire data recipe generation process for LLM adaptation, reducing human effort and enabling scalable, self-improving systems.

Method: The authors formalize end-to-end data recipe generation: given a target benchmark and pool of data sources, a model must output a full data processing pipeline (recipe) to adapt a base LLM to that task. They develop DataChef-32B, a 32B-parameter model trained with online reinforcement learning. The RL objective uses a proxy reward that predicts downstream performance of candidate recipes without fully training and evaluating the adapted model each time. DataChef-32B iteratively proposes and refines recipes based on this reward signal, learning to select and process data sources effectively for different benchmarks and domains.

Result: On six held-out tasks, DataChef-32B generates practical data recipes that yield downstream performance comparable to human-expert-curated recipes. In a math adaptation case study, a recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving a score of 66.7 on AIME'25 and outperforming the original Qwen3-1.7B model, indicating strong task-specific adaptation. These results demonstrate that automated recipe generation can match or exceed human-designed data pipelines for LLM training and fine-tuning.

Conclusion: The paper concludes that fully automated, end-to-end data recipe generation for LLM adaptation is feasible and effective. DataChef-32B, trained via online RL with a proxy reward, can design data processing pipelines that rival human expert work, reducing manual overhead in LLM training. This points toward more automated, self-evolving AI systems where models help design their own training processes, potentially accelerating LLM development and specialization across domains.

Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.

</details>


### [44] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: The paper introduces SafeThink, an inference-time defense that monitors chain-of-thought safety and injects a short corrective prefix only when needed, substantially reducing jailbreak success while preserving reasoning performance in multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: Explicit chain-of-thought reinforced with RL methods like GRPO improves reasoning in multimodal large-scale reasoning models but has been observed to weaken safety alignment and increase jailbreak success. There is a need for a lightweight, effective method to restore safety without retraining models or harming their reasoning ability.

Method: SafeThink adds an inference-time safety controller around existing multimodal reasoning models. It uses a safety reward model to monitor the ongoing reasoning trace token by token. When the evolving chain-of-thought violates a predefined safety threshold, the system injects a short, optimized corrective prefix (e.g., "Wait, think safely") into the generation, steering subsequent tokens. Safety is treated as a satisficing constraint: the controller intervenes only until the trace returns to a safe region, typically within the first few reasoning steps, rather than continually maximizing a safety score.

Result: Across six open-source multimodal reasoning models and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, MM-SafetyBench), SafeThink reduces attack success rates by roughly 30–60%. For example, attack success drops from 63.33% to 5.74% on JailbreakV-28K for LlamaV-o1, and from 69.07% to 5.65% on Hades for R1-OneVision. At the same time, it preserves reasoning capability, with MathVista accuracy changing minimally from 65.20% to 65.00%. Empirically, most unsafe generations can be corrected with interventions in just the first 1–3 reasoning steps.

Conclusion: Safety degradation introduced by RL-based chain-of-thought post-training can be mitigated at inference time without retraining the base model. SafeThink demonstrates that treating safety as a satisficing constraint and applying minimal, early corrective prefixes is sufficient to substantially reduce jailbreak success while keeping reasoning performance nearly intact. This suggests that many unsafe trajectories lie close to safe ones in the model’s generation space, and small, well-timed steering interventions can effectively recover safety.

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


### [45] [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047)
*Han Xiao*

Main category: cs.CL

TL;DR: The paper proposes a new method to invert text embeddings back into tokens using conditional masked diffusion, enabling parallel recovery of all tokens efficiently and accurately.


<details>
  <summary>Details</summary>
Motivation: Embedding inversion—reconstructing text from its embedding—is useful for interpretability, security/privacy analysis, and debugging representation models, but current methods are often autoregressive, slow, or require access to the original encoder. There is a need for a faster, encoder-agnostic, and accurate inversion technique.

Method: The authors model embedding inversion as a conditional masked diffusion process. A masked diffusion language model iteratively denoises masked tokens in parallel, conditioned on the target embedding via adaptive layer normalization (AdaLN). The approach uses only 8 denoising steps (forward passes) through a relatively small, 78M-parameter model and does not require access to the embedding-generating encoder.

Result: On 32-token sequences and across three different embedding models, the proposed approach recovers tokens with 81.3% accuracy and reconstructs embeddings with an average cosine similarity of 0.87 to the targets, demonstrating both strong token-level recovery and embedding-level fidelity.

Conclusion: Framing embedding inversion as conditional masked diffusion allows efficient, parallel, and encoder-agnostic reconstruction of text from embeddings with high accuracy and similarity, suggesting diffusion-based masked LMs are a promising direction for controllable, efficient inversion of representation models.

Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes through a 78M parameter model with no access to the target encoder. On 32-token sequences across three embedding models, the method achieves 81.3% token accuracy and 0.87 cosine similarity.

</details>


### [46] [Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072)
*Tom Labiausse,Romain Fabre,Yannick Estève,Alexandre Défossez,Neil Zeghidour*

Main category: cs.CL

TL;DR: The paper introduces Hibiki-Zero, a simultaneous speech translation system that removes the need for word-level alignments and instead uses sentence-level supervision plus reinforcement learning to jointly optimize translation quality and latency, achieving state-of-the-art performance and easy adaptation to new languages.


<details>
  <summary>Details</summary>
Motivation: Simultaneous speech translation must output translations in real time while handling non-monotonic word orders between languages. Existing systems depend on supervised training with word-level alignments, which are hard to obtain at scale and typically approximated by language-specific heuristic alignment tools. These heuristics are suboptimal, limit scalability to many languages and grammatical structures, and complicate the training pipeline. The authors are motivated to design a method that avoids explicit word-level alignments, simplifying training and enabling broader multilingual deployment.

Method: The authors propose Hibiki-Zero, a two-stage training framework. First, they train a speech translation model using only sentence-level aligned speech–text pairs, learning a high-latency offline translation capability. Second, they apply a reinforcement learning procedure based on GRPO (a policy optimization algorithm) to directly optimize the model’s streaming behavior, reducing latency while maintaining translation quality. This RL stage teaches the system when to wait for more source speech and when to emit target tokens, without relying on word-level alignments or language-specific wait/commit heuristics. The model is evaluated across multiple X-to-English speech translation tasks and is designed for easy adaptation to new languages with limited speech data.

Result: Hibiki-Zero achieves state-of-the-art results along several dimensions: translation accuracy, latency (lower delays), voice transfer, and perceived naturalness across five different source-to-English simultaneous speech translation benchmarks. The authors further show that the model can be adapted to a previously unsupported input language using fewer than 1000 hours of speech data. They also construct and release a new evaluation benchmark comprising 45 hours of multilingual data for assessing speech translation systems.

Conclusion: The paper concludes that explicit word-level alignments are not necessary for high-quality, low-latency simultaneous speech translation. By leveraging sentence-level supervision and reinforcement learning with GRPO to balance latency and quality, Hibiki-Zero both simplifies the training pipeline and scales effectively to many languages with diverse grammars. The system sets new state-of-the-art performance across multiple metrics and is data-efficient for adding new languages. The released model artifacts and multilingual benchmark aim to facilitate further research and development in simultaneous speech translation.

Abstract: Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.

</details>


### [47] [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091)
*Usman Naseem,Gautam Siddharth Kashyap,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Rafiq Ali*

Main category: cs.CL

TL;DR: The paper introduces MisAlign-Profile, a unified benchmark to systematically measure trade-offs among safety, value, and cultural alignment in LLMs using a new dataset and evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment benchmarks mostly focus on a single dimension—safety, values, or culture—so they miss how these dimensions interact and trade off against each other in realistic settings. Mechanistic-interpretability-based efforts help understand failures but do not provide a systematic, large-scale way to quantify cross-dimensional trade-offs. The authors want a comprehensive, unified framework to profile how LLMs fail when multiple normative constraints co-occur.

Method: They build MISALIGNTRADE, an English dataset spanning 112 normative domains (14 safety, 56 value, 42 cultural). For each prompt, they annotate the domain and one of three semantic misalignment types (object, attribute, relation). Prompts are created and expanded using Gemma-2-9B-it and Qwen3-30B-A3B-Instruct-2507, with SimHash-based fingerprinting to remove duplicates. Each prompt has both misaligned and aligned responses, obtained via two-stage rejection sampling to ensure quality. On top of this dataset they define MisAlign-Profile, a benchmark protocol that evaluates multiple classes of LLMs on cross-dimensional misalignment trade-offs.

Result: They benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE and observe that models exhibit substantial trade-offs, with 12–34% misalignment across the safety, value, and cultural dimensions when evaluated jointly. This demonstrates that current models frequently fail to satisfy all normative constraints simultaneously, and that their misalignment patterns can be systematically profiled using the proposed benchmark.

Conclusion: MisAlign-Profile and the MISALIGNTRADE dataset provide a unified, scalable way to characterize how LLMs trade off safety, values, and culture. The findings show that existing LLMs are significantly misaligned when multiple normative dimensions must be jointly satisfied, underscoring the need for alignment methods and evaluations that explicitly target cross-dimensional trade-offs rather than isolated dimensions.

Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

</details>


### [48] [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106)
*Géraud Faye,Wassila Ouerdane,Guillaume Gadek,Céline Hudelot*

Main category: cs.CL

TL;DR: TEG encodes text and its graph-structured representation, integrating external knowledge to improve misinformation detection; TEGRA further boosts performance with domain-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: Manual fact-checking relies heavily on external knowledge, but many automatic misinformation detectors use only raw text via language models. There is a need for methods that can explicitly incorporate structured external knowledge (e.g., knowledge bases) into document representations to better detect misinformation.

Method: The authors propose Text Encoding with Graph (TEG), which first extracts structured information from documents and represents it as a graph. Both the raw text and the derived graph are then encoded to form a hybrid representation used for classification. They extend this to TEGRA, which additionally injects domain-specific knowledge from external knowledge bases into the graph/text representation. Extensive experiments are conducted comparing these models against standard language-model-based baselines on misinformation detection tasks.

Result: The hybrid text-plus-graph representation produced by TEG improves misinformation detection performance over approaches that rely solely on language models. The extended framework TEGRA, which incorporates domain-specific knowledge, yields further gains in classification accuracy on most evaluated datasets or settings.

Conclusion: Explicitly modeling documents as graphs and integrating external and domain-specific knowledge into their representations is an effective strategy for improving automated misinformation detection. The TEG and TEGRA frameworks provide a practical way to augment language models with structured knowledge, leading to better classification accuracy in most cases.

Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

</details>


### [49] [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149)
*Dawid J. Kopiczko,Sagar Vaze,Tijmen Blankevoort,Yuki M. Asano*

Main category: cs.CL

TL;DR: The paper finds that, for reasoning LLMs, repeating a small chain-of-thought SFT dataset over many epochs can outperform using a much larger dataset once, under a fixed update budget.


<details>
  <summary>Details</summary>
Motivation: In post-training reasoning models, the common belief is that more unique training samples always improve generalization. However, collecting large, high-quality chain-of-thought datasets is expensive, and it is unclear whether sheer data scale or repeated exposure to a smaller set is more beneficial under a fixed training budget. The authors aim to clarify how dataset size vs. repetition affects reasoning performance and to provide practical guidance for SFT of reasoning LLMs.

Method: They perform supervised fine-tuning of the Olmo3-7B model on chain-of-thought data, comparing settings where the total number of optimization updates (compute budget) is fixed but the dataset size and number of epochs vary. They systematically vary from many samples with few epochs to few samples with many epochs, and evaluate on reasoning benchmarks like AIME'24/25 and GPQA. They track training token accuracy to study memorization dynamics and its relation to generalization improvements across different repetition regimes.

Result: Under a fixed update budget, training for many epochs on a small dataset (e.g., 128 epochs on 400 samples) significantly outperforms training for a single epoch on a much larger dataset (e.g., 1 epoch on 51,200 samples) by 12–26 percentage points on AIME and GPQA, without increased catastrophic forgetting. Training token accuracy increases with repetition and plateaus when the model fully memorizes the small dataset; performance gains on benchmarks also plateau around this full-memorization point. This saturation behavior is consistent across all evaluated settings.

Conclusion: For reasoning SFT, repeating a small, high-quality chain-of-thought dataset over many epochs can be more effective than scaling to large datasets when compute is fixed. Training token accuracy serves as a practical stopping criterion: once accuracy saturates and the model has effectively memorized the training tokens, additional repetition yields little extra benefit. The authors highlight this “repetition advantage”—where full memorization coincides with better generalization—as a counterintuitive phenomenon and an open problem for understanding LLM training dynamics.

Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: The paper studies how and why LLM behavior in strategic settings differs from human behavior by discovering interpretable behavioral models from data.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in social and strategic contexts, yet we lack a clear understanding of when and why their behavior diverges from humans. Existing behavioral game theory models are hand-designed and often fail to capture the nuanced, idiosyncratic patterns seen in either humans or black-box agents like LLMs. There is a need for a more flexible, data-driven approach that can uncover the underlying structural factors that govern behavior in such interactions.

Method: The authors use AlphaEvolve, a program discovery tool, to automatically search over a large space of candidate models and identify interpretable programs that best explain observed behavior in iterated rock-paper-scissors games. Instead of fitting standard parametric BGT models, they allow AlphaEvolve to discover new structural forms directly from behavioral data for both humans and LLMs, thus enabling an open-ended search for explanatory mechanisms behind different behaviors.

Result: In iterated rock-paper-scissors, the discovered models indicate that frontier LLMs exhibit patterns consistent with deeper strategic reasoning than typical human participants. The learned programs capture distinct structural drivers of behavior for humans versus LLMs, demonstrating that LLMs can exploit more complex contingencies or longer-term patterns in the game compared to humans.

Conclusion: The study shows that automatic, interpretable program discovery can reveal fundamental structural differences between human and LLM behavior in strategic interactions. It suggests that frontier LLMs may sometimes surpass humans in the depth of strategic reasoning and establishes a methodology for systematically uncovering and comparing the behavioral mechanisms underlying actions of humans and artificial agents.

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [51] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: Introduces LiveMedBench, a dynamic, contamination-free benchmark with rubric-based evaluation for LLMs in clinical settings.


<details>
  <summary>Details</summary>
Motivation: Existing static medical benchmarks suffer from data contamination and temporal misalignment, and current metrics (ROUGE, LLM-as-a-Judge) cannot reliably assess clinical correctness of open-ended reasoning.

Method: Continuously scrape real-world clinical cases weekly from online communities with strict temporal separation from training data; apply a Multi-Agent Clinical Curation Framework to denoise and validate cases; design an Automated Rubric-based Evaluation Framework that decomposes physician responses into fine-grained case-specific criteria and uses them to score LLM outputs.

Result: Constructed LiveMedBench with 2,756 cases across 38 specialties and multiple languages, and 16,702 criteria; evaluated 38 LLMs, where the best model scored 39.2%, most models perform worse on post-cutoff cases, and errors are dominated by failures to adapt knowledge to patient-specific context rather than missing factual knowledge.

Conclusion: LiveMedBench is a more realistic, temporally aligned, and clinically grounded benchmark that reveals substantial performance gaps and contamination issues in current medical LLMs, and highlights contextual reasoning as a key bottleneck for safe clinical deployment.

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [52] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL is a framework that integrates vision-language foundation models into reinforcement learning for autonomous driving, using asynchronous inference and new supervision methods to get VLM-level performance with real-time speed.


<details>
  <summary>Details</summary>
Motivation: Standard RL for autonomous driving is sample-inefficient and hard to interpret, while vision-language foundation models are knowledgeable but too slow to run in tight RL loops. The paper aims to combine their strengths—leveraging VLM semantic reasoning to guide RL—without incurring prohibitive inference latency, so that practical, real-time, end-to-end driving policies can be trained and deployed.

Method: The authors design Found-RL, a platform that connects RL with foundation models through an asynchronous batch inference framework which decouples the slow VLM reasoning from the fast simulation loop. They propose two supervision strategies to distill VLM decisions into the RL policy: (1) Value-Margin Regularization (VMR), which regularizes RL value estimates towards margins implied by VLM-suggested actions, and (2) Advantage-Weighted Action Guidance (AWAG), which biases the policy toward actions favored by the VLM weighted by their estimated advantage. For dense reward shaping, they use CLIP in a high-throughput manner, but counteract CLIP’s dynamic blindness via Conditional Contrastive Action Alignment: prompts are conditioned on discretized speed/command, and context-specific action anchors produce a normalized, margin-based reward bonus reflecting how aligned the current action is with desired behavior.

Result: Using Found-RL, a relatively small RL model for autonomous driving is trained that achieves performance close to that of large, billion-parameter VLMs, while maintaining real-time inference speeds of around 500 FPS. The framework successfully integrates fine-tuned VLM guidance and CLIP-based reward shaping without sacrificing training throughput or deployment latency.

Conclusion: Found-RL demonstrates that foundation models can be practically and efficiently integrated into RL for autonomous driving by decoupling heavy VLM inference from the control loop and distilling their guidance into a compact policy. The proposed supervision and reward-shaping mechanisms let lightweight RL policies approach VLM-level performance while preserving real-time operation, suggesting a promising direction for scalable, interpretable, and efficient end-to-end driving systems.

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [53] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: The paper introduces AgoraBench, a negotiation benchmark and training framework that uses utility-based metrics and human preference data to improve LLM bargaining strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs perform poorly in bargaining scenarios that require strategic depth and sensitivity to complex human factors, but current benchmarks do not adequately expose or measure these limitations. There is a need for a systematic way to evaluate and improve negotiation behavior in LLMs in a manner aligned with economic theory and human preferences.

Method: The authors design AgoraBench, a benchmark covering nine difficult negotiation settings (such as deception and monopoly) to test diverse strategic behaviors. They define economically grounded, human-aligned metrics based on utility theory—specifically agent utility, negotiation power, and acquisition ratio—to implicitly measure alignment with human preferences. Additionally, they construct a human preference grounded dataset and a training pipeline that leverages both prompting and finetuning to teach LLMs better bargaining strategies under this framework.

Result: Empirical evaluation shows that standard LLM-based negotiation strategies significantly diverge from human preferences when assessed with the proposed utility-based metrics. Applying the new benchmark, metrics, and training pipeline leads to substantial gains in negotiation performance, including more sophisticated strategic reasoning and improved awareness of opponents’ positions and incentives.

Conclusion: Utility-based evaluation and human preference grounded training can meaningfully enhance LLMs’ negotiation abilities. AgoraBench and the associated metrics and dataset provide a more realistic and rigorous way to measure and develop bargaining competence in LLMs, pushing them toward deeper strategy and better alignment with human negotiators.

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [54] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: The paper studies whether large language models (LLMs) can automatically construct qualitative numerical planning (QNP) abstractions for generalized planning problems, and proposes a prompt-based protocol plus automated debugging to improve abstraction quality.


<details>
  <summary>Details</summary>
Motivation: Generalized planning seeks plans that solve many instances at once, and QNP is a key abstraction formalism for such problems. However, building good QNP abstractions by hand is difficult and requires expertise. Recent work shows LLMs can act as generalized planners, suggesting they might also help automate the construction of QNP abstractions. The paper is motivated by the need to reduce manual effort and expertise in designing abstractions for GP by leveraging LLMs, while maintaining correctness and usefulness of the abstractions.

Method: The authors design a prompt protocol where a GP domain description and a set of training tasks are provided to an LLM. The LLM is asked to (1) propose abstract features and (2) use them to abstract the initial states, action sets, and goals into a QNP representation. On top of this, they introduce an automated debugging framework that checks the generated abstractions for errors or inconsistencies with the original GP tasks. When errors are detected, the system produces feedback that is fed back into the LLM to iteratively repair or refine the abstractions.

Result: Experimental evaluation shows that, when coupled with the automated debugging and feedback mechanism, some LLMs are capable of producing QNP abstractions that are useful for generalized planning. The abstractions, after debugging-guided refinement, are of sufficient quality to serve as effective models for GP tasks, outperforming unguided or one-shot LLM abstraction attempts.

Conclusion: LLMs, if properly guided, can act not only as generalized planners but also as generators of QNP abstractions for GP domains. Automated debugging plays a crucial role in detecting and correcting abstraction errors, leading to reliable and practically useful QNP models. This suggests a promising pathway toward automating abstraction design in generalized planning by combining LLM prompting with systematic verification and repair.

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [55] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: The paper introduces FoSS, a GFlowNet-based framework that generates text by sampling variable-length spans from retrieved corpora, forming a DAG-structured state space instead of a tree and improving text diversity and performance on generation and knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LMs generate token-by-token from a fixed vocabulary, inducing a tree-structured state space that restricts flexibility and compositional exploration. Prior dynamic-vocabulary methods use retrieved spans but ignore that sentences can be composed from spans of different lengths and do not explicitly model the underlying DAG state space. Existing GFlowNet language models also operate at the token level, remaining limited to tree structures. There is a need for a principled way to exploit DAG-structured span spaces to better explore compositional paths and improve text quality and generalization.

Method: The authors propose Flow of SpanS (FoSS), a generative framework based on Generative Flow Networks. FoSS constructs a dynamic span vocabulary by flexibly segmenting retrieved text into variable-length spans, explicitly inducing a DAG-structured state space where each state corresponds to a partial composition of spans. GFlowNets are then used to explore multiple compositional paths through this DAG, rather than a single token-level path. Specialized reward models guide the GFlowNet toward high-reward (high-quality, diverse, and relevant) text completions.

Result: FoSS yields improved text generation quality and diversity, measured by up to 12.5% higher MAUVE scores compared to a Transformer baseline. It also improves performance on knowledge-intensive tasks by 3.5% over strong baselines. The method consistently outperforms state-of-the-art approaches and, in scaling experiments, continues to show gains as model size, data volume, and retrieval corpus richness increase.

Conclusion: Modeling text generation as span composition over a DAG-structured state space, and using GFlowNets to explore this space, provides a principled and effective way to move beyond token-level, tree-structured autoregressive generation. FoSS leverages dynamic span vocabularies and reward-guided exploration to produce more diverse, higher-quality text and better performance on knowledge-intensive tasks, while scaling favorably with larger models and richer data.

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [56] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: The paper introduces Neuro-symbolic Action Masking (NSAM), a framework that integrates symbolic reasoning with deep reinforcement learning to automatically learn action masks that prevent infeasible actions, improving both sample efficiency and constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning agents often explore and execute infeasible or unsafe actions, especially in environments with constraints, leading to safety issues and inefficient learning. Existing methods usually assume a predefined symbol grounding function and manually designed action masks, which is unrealistic and limits scalability. There is a need for an approach that can automatically learn symbolic state representations and feasible action constraints directly from interaction, while respecting high-level domain constraints.

Method: The authors propose Neuro-symbolic Action Masking (NSAM), which jointly learns: (1) a symbolic model that grounds high-dimensional states into consistent symbolic representations aligned with domain constraints, and (2) action masks derived from these symbolic states that prune infeasible actions. NSAM is trained in a minimally supervised way during DRL, integrating symbolic reasoning into end-to-end deep policy optimization so that grounding and policy are improved together through interaction with the environment.

Result: Across multiple constrained domains, NSAM is empirically shown to significantly enhance the sample efficiency of DRL agents by reducing the exploration of infeasible actions. The learned action masks drastically cut down the number of constraint violations while maintaining or improving task performance compared to baselines that rely on fixed symbol grounding or manually specified action masking.

Conclusion: NSAM demonstrates that automatically learned neuro-symbolic models can effectively constrain DRL agents’ action spaces without heavy manual engineering. By coupling symbolic reasoning with policy learning in an end-to-end framework, NSAM both improves learning efficiency and enforces domain constraints, suggesting a promising direction for safer and more data-efficient DRL in complex, constrained environments.

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [57] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: The paper systematically evaluates whether step-by-step “reasoning” capacities of large language/reasoning models transfer to Theory of Mind (ToM) tasks, finding they often do not and can even harm performance.


<details>
  <summary>Details</summary>
Motivation: While large reasoning models show strong performance in structured domains like math and programming via chain-of-thought style reasoning, it remains unclear whether this advantage extends to social cognition tasks such as Theory of Mind, which involve understanding others’ hidden mental states. The authors aim to fill this gap and diagnose how current reasoning strategies behave on ToM benchmarks.

Method: The authors benchmark nine advanced LLMs, splitting them into reasoning-focused and non-reasoning models, across three representative Theory of Mind datasets. They conduct fine-grained analyses of model behavior as a function of reasoning length, reasoning budget, and input format (e.g., with vs. without multiple-choice options). They also propose and test two interventions: (1) Slow-to-Fast (S2F), an adaptive reasoning strategy that regulates how much reasoning a model performs, and (2) Think-to-Match (T2M), which aims to reduce shortcut behavior tied to option matching.

Result: Empirically, reasoning-focused models do not consistently outperform non-reasoning models on ToM benchmarks and sometimes perform worse. Longer, more elaborate reasoning often correlates with decreased accuracy (“slow thinking collapses”), and allocating larger reasoning budgets can hurt performance. Constraining or adaptively adjusting reasoning length leads to more reliable results. Additionally, removing multiple-choice options substantially improves reasoning models’ accuracy, indicating they were previously relying on superficial option matching rather than genuine Theory of Mind reasoning. The proposed S2F and T2M interventions help verify and partially alleviate these issues.

Conclusion: The gains that large reasoning models exhibit in formal reasoning domains do not straightforwardly transfer to Theory of Mind tasks. Effective ToM performance appears to require specialized capabilities beyond current generic chain-of-thought or extended-reasoning methods. Future work should therefore design models and training strategies explicitly tailored to social reasoning and mental-state understanding rather than assuming that longer or more elaborate reasoning alone will produce robust Theory of Mind.

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [58] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: They propose HARPO, a heterogeneity-aware RL optimization method, and use it to train Omnisapiens-7B 2.0, a unified foundation model for social behavior understanding that outperforms prior models on multiple behavioral tasks.


<details>
  <summary>Details</summary>
Motivation: Current socially intelligent AI systems model affective, cognitive, or social behavior dimensions separately, which is costly to train and does not generalize well across diverse behavioral settings. Even recent reasoning RL methods that unify multiple tasks fail to explicitly handle heterogeneous behavioral data distributions, creating a need for an RL approach that can balance learning from diverse tasks and samples without overfitting to any one of them.

Method: They introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL algorithm that adjusts (modulates) the advantages used in policy gradient updates so that no single task or sample dominates the training signal. HARPO is then used to train Omnisapiens-7B 2.0, a 7B-parameter behavioral foundation model, jointly across multiple heterogeneous behavioral tasks, encouraging balanced learning and robust reasoning traces.

Result: Omnisapiens-7B 2.0 trained with HARPO achieves state-of-the-art performance across a suite of social behavior tasks, improving up to +16.85% in multitask evaluations and +9.37% on held-out tasks compared with existing behavioral foundation models. HARPO itself is also benchmarked against other recent RL-based reasoning methods and shows the most consistently strong performance across varied behavioral tasks.

Conclusion: HARPO effectively manages heterogeneity in behavioral RL training by preventing over-dominance of specific tasks or samples, enabling the creation of Omnisapiens-7B 2.0, a unified social behavior foundation model with superior performance and more explicit reasoning. This suggests that heterogeneity-aware policy optimization is a promising direction for scalable, generalizable social intelligence in AI systems.

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [59] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: The paper introduces V-STAR, a reinforcement learning framework for generative recommendation that fixes probability-reward mismatch in autoregressive models, improving both accuracy and diversity under latency constraints.


<details>
  <summary>Details</summary>
Motivation: Autoregressive generative recommenders unify retrieval and ranking but struggle when fine-tuned with RL, due to a mismatch between token-level probabilities and sequence-level rewards. Standard decoding like beam search prunes low-probability but potentially high-reward branches and produces highly correlated trajectories with low reward variance, which weakens RL training. The authors want to overcome insufficient exploration and poor learning signals in this setting.

Method: They propose V-STAR, combining two components in a self-evolving loop. (1) Value-Guided Efficient Decoding (VED) that uses a learned value signal to identify decisive nodes in the generation tree and selectively deepen prefixes that appear high-potential, enabling more efficient exploration than exhaustive search. (2) Sibling-GRPO, an RL training scheme that leverages the tree structure induced by decoding to compute sibling-relative advantages at branching points, focusing the learning signal on critical decisions instead of entire correlated trajectories.

Result: Through experiments on both offline benchmarks and real-world online environments, V-STAR is shown to outperform strong baselines for generative recommendation in terms of recommendation accuracy and candidate set diversity, while still respecting strict latency constraints typical of production recommenders.

Conclusion: V-STAR effectively mitigates the probability-reward mismatch in RL-fine-tuned generative recommenders by improving exploration during decoding and sharpening the RL advantage signal using tree-structured sibling comparisons, resulting in better and more diverse recommendations under real-time constraints.

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [60] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: The paper studies how staff and students in IT and Electrical Engineering perceive and want to use GenAI, then derives requirements and a framework for responsible integration in higher education under regulations like the EU AI Act.


<details>
  <summary>Details</summary>
Motivation: GenAI is rapidly entering higher education, promising personalized learning and more efficient services, but perceptions are mixed and shaped by context. At the same time, regulations such as the EU AI Act obligate universities to deploy GenAI responsibly. There is a gap in understanding discipline-specific needs and concerns, especially in technical fields like IT and Electrical Engineering, which is necessary to design compliant, accepted GenAI-based cognitive systems.

Method: The authors use a mixed-methods approach, administering surveys to 61 staff and 37 students in the Faculty of Information Technology and Electrical Engineering at the University of Oulu. They analyze quantitative and qualitative responses to identify common and discipline-specific perceptions, interests, concerns, and expectations regarding GenAI in teaching, learning, and academic work.

Result: The study finds both shared and discipline-specific themes. Stakeholders show strong interest in GenAI for programming support. At the same time, they express concerns about response quality, privacy, and academic integrity. These findings are synthesized into a set of high-level requirements for GenAI use and a conceptual framework for responsible integration in the faculty context.

Conclusion: Responsible GenAI integration in higher education must be context- and discipline-sensitive and built through active stakeholder engagement. The proposed high-level requirements and conceptual framework offer practical guidance for universities seeking to leverage GenAI’s benefits while addressing staff and student concerns and complying with regulatory frameworks such as the EU AI Act.

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [61] [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814)
*Xingyi Zhang,Yulei Ye,Kaifeng Huang,Wenhao Li,Xiangfeng Wang*

Main category: cs.AI

TL;DR: Introduces ScratchWorld, a benchmark to test multimodal GUI agents on building Scratch programs, revealing a gap between reasoning and fine-grained GUI control.


<details>
  <summary>Details</summary>
Motivation: Block-based tools like Scratch are widely used in low-code education, but there is no rigorous way to evaluate how well AI agents can actually operate such GUIs to construct programs. Existing work focuses more on code generation or high-level planning, leaving the concrete interaction with visual programming interfaces underexplored.

Method: They design ScratchWorld, a benchmark of 83 tasks aligned with the Use-Modify-Create pedagogical framework, divided into Create, Debug, Extend, and Compute categories. The benchmark offers two interaction modes: (1) primitive mode, where agents must perform low-level drag-and-drop and other granular GUI actions; and (2) composite mode, where agents use higher-level semantic APIs that abstract away some GUI details, allowing separation of reasoning from execution skills. Evaluation is execution-based: the Scratch programs that agents construct are run in-browser and checked by runtime tests for functional correctness.

Result: Running state-of-the-art multimodal LLMs and GUI agents on ScratchWorld shows that, although agents can often plan or reason about what program is needed, they frequently fail at reliably carrying out the necessary fine-grained GUI actions. This exposes a notable gap between reasoning ability and visuomotor GUI control.

Conclusion: ScratchWorld provides a systematic way to assess and diagnose multimodal GUI agents on block-based programming tasks. The findings underscore that improving low-level GUI manipulation remains a key challenge, even for models with strong high-level reasoning, and that future work must better bridge the reasoning–acting gap in interactive visual programming environments.

Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>


### [62] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC is a framework for knowledge graph completion that adaptively fuses pre-trained semantic information with heterogeneous graph structures to improve relational reasoning and hit rates.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods suffer from a structural resolution mismatch: they cannot simultaneously handle dense and sparse graph regions, leading to structural noise in dense clusters and representation collapse in sparse areas. A more robust and adaptive way to integrate semantic and structural information is needed.

Method: SynergyKGC introduces an active Cross-Modal Synergy Expert that enhances neighbor aggregation using relation-aware cross-attention and semantic-intent-driven gating. It further employs a density-dependent Identity Anchoring strategy and a Double-tower Coherent Consistency architecture to align heterogeneous topologies and stabilize representations across training and inference.

Result: On two public KGC benchmarks, SynergyKGC significantly improves hit rates compared with existing methods, demonstrating more effective and robust completion performance.

Conclusion: SynergyKGC mitigates structural resolution mismatch in knowledge graphs by adaptively integrating semantic and structural information, offering a generalizable principle for resilient information integration in non-homogeneous structured data and achieving superior empirical performance.

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [63] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: The paper introduces RLCER, a method to autonomously reward chain-of-thought (CoT) reasoning in large language models using self-generated, evolving rubrics instead of human-labeled reward models or outcome-only rewards.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for LLM reasoning either rely on human-labeled reward models for chain-of-thought (which are costly and brittle) or focus only on final outcomes, which miss the quality of intermediate reasoning steps and can be exploited via reward hacking. Additionally, static reward models cannot easily adapt to the evolving distribution of CoTs during training. The authors are motivated to develop an autonomous, annotation-free method that can directly supervise and reward CoT processes while continuously adapting as the model and its reasoning patterns evolve.

Method: The authors propose RLCER (Reinforcement Learning with CoT Supervision via Self-Evolving Rubrics). Instead of training a static reward model or relying solely on outcome-based rewards, the method has the model generate its own rubrics—criteria for evaluating CoTs—which then serve as supervision signals. These rubrics are self-proposed and updated over time (self-evolving) as training proceeds. RLCER augments or replaces traditional outcome-centric RLVR by rewarding intermediate reasoning steps according to these learned rubrics, effectively aligning policy updates with internally generated, continuously refined CoT evaluation criteria.

Result: Empirically, RLCER is shown to provide reliable supervision for CoT quality even when explicit outcome rewards are removed, and under these conditions it still outperforms standard outcome-centric RLVR. Furthermore, when the self-proposed rubrics are used as explicit hints in the inference prompt, they yield additional performance improvements at test time, indicating that the learned rubrics are both meaningful and practically useful for guiding reasoning.

Conclusion: The paper concludes that self-proposed, self-evolving rubrics are a viable and effective way to autonomously supervise chain-of-thought reasoning in large language models. RLCER can surpass outcome-only reinforcement learning approaches without requiring human-annotated reward models, and the learned rubrics remain beneficial at inference time as in-context guidance. This establishes a promising direction for scalable, CoT-aware RL training that adapts over time and mitigates issues like reward hacking.

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [64] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: The paper evaluates how well large language models (LLMs) can adapt cultural content—specifically cooking recipes—across countries, and finds that current LLMs perform poorly compared to humans in producing culturally representative adaptations.


<details>
  <summary>Details</summary>
Motivation: LLMs are now widely used to generate cultural artifacts like stories and art, but prior work has revealed systematic cultural biases, including stereotyping and homogenization. This raises concerns about whether LLMs can respect and represent diverse cultures, especially less dominant ones. The authors are motivated to rigorously test and understand LLMs’ capacity for cultural adaptation in a concrete domain that tightly reflects culture and tradition: cooking recipes.

Method: The authors use the GlobalFusion dataset, which contains pairs of human-written recipes from different countries selected using established measures of cultural distance. For the same country pairs, they prompt multiple LLMs to generate culturally adapted recipes. They then compare human and LLM recipe pairs to study how divergence between source and adapted recipes behaves as a function of cultural distance. They analyze internal model representations for how strongly cultural information is preserved, examine how models treat novelty and creativity versus tradition, and evaluate whether models can correctly associate adapted recipes with the target countries and ground adaptations in culturally salient elements such as ingredients.

Result: LLMs’ adapted recipes do not behave like human adaptations: the divergence between their generated recipes does not track or correlate with the cultural distance between countries, unlike human-written recipe pairs. Analyses show that cultural signals are weak in the models’ internal representations; the models overinflate novelty and misunderstand creativity and tradition; and they often fail to link a given adaptation to its intended countries or to embed it in culturally diagnostic ingredients and elements.

Conclusion: Current LLMs are limited in their ability to perform genuine cultural adaptation in generative tasks such as recipe creation. They do not reliably reflect cross-cultural differences or ground their generations in culturally salient features, leading to non-representative, over-novel, and sometimes misattributed outputs. These limitations suggest that off-the-shelf LLMs may be inappropriate or risky for culturally sensitive applications without additional methods to strengthen cultural grounding and representation.

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [65] [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)
*Yusong Lin,Haiyang Wang,Shuzhe Wu,Lue Fan,Feiyang Pan,Sanyuan Zhao,Dandan Tu*

Main category: cs.AI

TL;DR: The paper introduces CLI-Gym, a scalable pipeline that transforms Docker environment histories into environment-intensive command-line tasks, and uses the resulting dataset to train LiberCoder, a model that significantly improves performance on CLI-based benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current agentic coding systems struggle with environment-intensive tasks that require rich interaction with real runtime environments (e.g., CLIs for debugging dependencies or system issues), and there is a lack of large, systematically constructed task collections to train and evaluate such agents at scale.

Method: The authors draw an analogy between Dockerfiles and agentic tasks, then automatically trace the evolution of healthy Docker-based environments, invert them back to earlier buggy states that produce runtime failures, and package each buggy state plus its error messages as a CLI task; they collect successful agent trajectories on these tasks and fine-tune a model, LiberCoder, with this curated interaction data.

Result: Using the proposed CLI-Gym pipeline, the authors derive 1,655 environment-intensive CLI tasks, claimed to be the largest such collection, and show that LiberCoder, fine-tuned on curated trajectories from these tasks, improves absolute performance on the Terminal-Bench benchmark by 21.1 percentage points, reaching 46.1% and outperforming strong baselines.

Conclusion: CLI-Gym provides the first public, scalable pipeline for generating environment-intensive CLI tasks from Docker environment histories, and the resulting dataset enables substantial gains in agentic coding capabilities, as demonstrated by LiberCoder’s performance improvements on Terminal-Bench.

Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

</details>


### [66] [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103)
*Wayne Chi,Yixiong Fang,Arnav Yayavaram,Siddharth Yayavaram,Seth Karten,Qiuhong Anna Wei,Runkun Chen,Alexander Wang,Valerie Chen,Ameet Talwalkar,Chris Donahue*

Main category: cs.AI

TL;DR: GameDevBench is a new benchmark to evaluate multimodal coding agents on realistic game development tasks that involve both complex code changes and visual assets.


<details>
  <summary>Details</summary>
Motivation: Existing coding agent benchmarks focus mostly on text-based software development and do not adequately test multimodal capabilities, especially those needed for handling visual assets and graphics. There is a lack of evaluation testbeds that combine rich code reasoning with deep multimodal understanding, so progress on multimodal coding agents has lagged behind purely textual ones.

Method: The authors construct GameDevBench, a benchmark of 132 game development tasks sourced from real-world web and video tutorials. Tasks span areas such as gameplay logic, 2D graphics, shaders, sprites, and animations. They analyze task complexity by measuring code size and number of file changes relative to prior benchmarks. They evaluate multiple coding agents on these tasks, categorize tasks by multimodal complexity, and introduce two simple image- and video-based feedback mechanisms that let agents see and use visual information from the game during development.

Result: GameDevBench tasks are substantially more complex than those in earlier software development benchmarks, requiring on average more than three times the lines of code and file modifications. Current agents perform poorly on this benchmark: the best baseline agent solves only 54.5% of tasks. Performance degrades as multimodal complexity increases, with success rates dropping from 46.9% on gameplay-focused tasks to 31.6% on 2D graphics tasks. The proposed image- and video-based feedback mechanisms consistently boost agent performance; for example, Claude Sonnet 4.5’s success rate increases from 33.3% to 47.7%.

Conclusion: GameDevBench exposes substantial gaps in current agents’ abilities to handle realistic, multimodal game development workflows, demonstrating that richer multimodal understanding and interaction with visual assets are still lacking. The benchmark, along with simple but effective visual feedback mechanisms, offers a new and challenging testbed for driving research on agentic game development and improving multimodal coding agents. The authors publicly release GameDevBench to facilitate further community progress.

Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

</details>
