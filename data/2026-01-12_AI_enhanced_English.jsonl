{"id": "2601.05271", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05271", "abs": "https://arxiv.org/abs/2601.05271", "authors": ["Xiran Fan", "Zhimeng Jiang", "Chin-Chia Michael Yeh", "Yuzhong Chen", "Yingtong Dou", "Menghai Pan", "Yan Zheng"], "title": "Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings", "comment": null, "summary": "The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.", "AI": {"tldr": "They propose a hybrid framework where LLMs generate semantic embeddings for merchant-related text fields, which are then used to initialize smaller, efficient transaction models, improving performance on transaction understanding without LLM runtime costs.", "motivation": "Existing transaction models treat merchant categorical fields as arbitrary indices, discarding the rich semantics in merchant names and descriptions. Fully LLM-based approaches can leverage these semantics but are too computationally heavy for real-time financial applications. There is a need for a practical method that injects LLM-level semantic understanding into transaction models while remaining efficient and deployable at scale.", "method": "They build a hybrid pipeline: (1) enrich and clean merchant-related text via multi-source data fusion and noise filtering; (2) use LLMs to produce semantic embeddings for merchant fields under a one-word constraint principle to ensure stable and consistent embedding generation across architectures; (3) use these LLM embeddings as initializations or features for smaller, tabular transaction models that operate sequentially on transactions; and (4) systematically handle data quality with context-aware enrichment to improve robustness.", "result": "On large-scale transaction datasets, the hybrid models initialized with LLM-based merchant embeddings significantly outperform baselines that rely on index-based categorical encodings across several transaction understanding tasks (e.g., classification or prediction tasks relevant to payment networks).", "conclusion": "Pre-computing LLM-based semantic embeddings for merchant text and using them to initialize lightweight transaction models yields better accuracy and interpretability than traditional index-based methods while preserving real-time efficiency, making it a practical path to inject LLM semantics into financial transaction modeling without prohibitive runtime costs."}}
{"id": "2601.05358", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05358", "abs": "https://arxiv.org/abs/2601.05358", "authors": ["Tim Menzner", "Jochen L. Leidner"], "title": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques", "comment": null, "summary": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.", "AI": {"tldr": "The paper develops a detailed sentence-level taxonomy of media bias and propaganda, moving beyond left/right labels to concrete linguistic mechanisms.", "motivation": "Public debate and many computational approaches treat media bias mainly as a matter of ideological position (left vs right), which obscures the specific linguistic devices by which bias and propaganda are actually conveyed. Existing taxonomies are either coarse, ambiguous, or incomplete, limiting rigorous annotation, analysis, and automatic detection of bias in text. The authors aim to provide a systematic, fine-grained framework that captures how partiality manifests in individual sentences across outlets and ideologies.", "method": "The authors compile a dataset of 26,464 sentences from three sources: newsroom corpora, user submissions, and their own browsing. Using an iterative process that combines close reading, interdisciplinary theory (e.g., from communication studies, cognitive science), and pilot annotation, they refine and structure a taxonomy of bias types. They organize these into a two-tier schema with 38 elementary bias types grouped into six functional families, visualized as a 'table of media-bias elements'. For each bias type, they provide definitions, authentic examples, explanations of cognitive/societal underpinnings, and recognition guidelines. They also run a quantitative survey on a random sample of 155 sentences to examine prevalence patterns and map their schema against existing NLP and communication-science taxonomies to assess coverage and ambiguity.", "result": "The work yields a two-level taxonomy encompassing 38 elementary bias and propaganda types, organized into six broader functional families and represented in a structured visual format. Each type is thoroughly documented with definitions, examples, underlying drivers, and annotation guidance. In the 155-sentence survey, different bias types show distinct prevalence profiles, indicating the taxonomy can differentiate patterns of partiality in real data. The cross-walk comparison demonstrates that the new schema covers a broader range of phenomena and resolves ambiguities present in widely used NLP and communication-science taxonomies.", "conclusion": "Bias in news cannot be adequately captured by coarse ideological labels; instead, it is expressed through a rich set of recurring linguistic manoeuvres. The proposed sentence-level taxonomy offers a more precise and comprehensive framework for identifying and annotating media bias and propaganda across outlets and ideological spectra. It improves coverage and reduces ambiguity relative to existing schemes, providing a foundation for more rigorous qualitative analysis, annotation projects, and future NLP systems that aim to detect and characterize media bias at the level of concrete language use."}}
{"id": "2601.05366", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05366", "abs": "https://arxiv.org/abs/2601.05366", "authors": ["Zheng Luo", "T Pranav Kutralingam", "Ogochukwu N Okoani", "Wanpeng Xu", "Hua Wei", "Xiyang Hu"], "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.", "AI": {"tldr": "The paper studies how well large language models perform tool calling in multilingual settings and finds significant robustness issues compared to English.", "motivation": "As LLMs are increasingly used as tool-calling agents in real applications, users interact in many languages, but existing evaluations focus mostly on English and overlook potential multilingual robustness issues.", "method": "The authors introduce MLCL, a diagnostic benchmark for multilingual tool calling, and systematically evaluate LLM performance on tool invocation across Chinese, Hindi, and Igbo, performing fine-grained error analysis of failure cases.", "result": "They find that many tool-calling failures occur even when the model correctly understands user intent and picks the right tool. A key error type is parameter value language mismatch, where the model fills tool parameters in the user\u2019s language rather than the expected language-agnostic or English format required for correct execution. Several inference-time system strategies mitigate but do not eliminate these language-induced execution errors.", "conclusion": "Multilingual tool calling remains fragile: language mismatches in parameter values are a major failure mode, and current mitigation strategies cannot yet achieve English-level tool-calling robustness, highlighting the need for better designs and training for multilingual agentic LLMs."}}
{"id": "2601.05403", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05403", "abs": "https://arxiv.org/abs/2601.05403", "authors": ["Zhiwei Liu", "Yupen Cao", "Yuechen Jiang", "Mohsinul Kabir", "Polydoros Giannouris", "Chen Xu", "Ziyang Xu", "Tianlei Zhu", "Tariquzzaman Faisal", "Triantafillos Papadopoulos", "Yan Wang", "Lingfei Qian", "Xueqing Peng", "Zhuohan Xie", "Ye Yuan", "Saeed Almheiri", "Abdulrazzaq Alnajjar", "Mingbin Chen", "Harry Stuart", "Paul Thompson", "Prayag Tiwari", "Alejandro Lopez-Lira", "Xue Liu", "Jimin Huang", "Sophia Ananiadou"], "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection", "comment": "Work in progress", "summary": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.", "AI": {"tldr": "The paper introduces MFMD-Scen, a benchmark to systematically test behavioral biases in large language models when detecting financial misinformation in realistic, multilingual financial scenarios.", "motivation": "LLMs are increasingly used in finance, but inherit human-like behavioral biases from their training data. Existing bias evaluations rely on direct questions or oversimplified settings, ignoring the complexity, risk, and context sensitivity of real financial environments, especially for multilingual financial misinformation detection. There is a need for a domain-specific, scenario-rich benchmark to expose and measure these behavioral biases under realistic financial decision contexts.", "method": "The authors collaborate with financial experts to design MFMD-Scen, a benchmark composed of three kinds of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios that also encode ethnicity and religious beliefs. They construct a multilingual financial misinformation dataset in English, Chinese, Greek, and Bengali, and systematically combine these scenarios with misinformation claims. Using this constructed benchmark, they evaluate the behavioral biases of 22 state-of-the-art commercial and open-source LLMs on financial misinformation detection tasks.", "result": "The evaluation across 22 LLMs on MFMD-Scen shows that significant behavioral biases remain prevalent in both commercial and open-source models when dealing with financial misinformation in complex, multilingual scenarios. These biases manifest across different scenario types and languages, indicating that current LLMs are far from neutral in high-stakes financial decision tasks.", "conclusion": "MFMD-Scen provides a comprehensive, realistic benchmark for stress-testing LLM behavioral biases in multilingual financial misinformation detection. The study demonstrates that prominent LLMs systematically exhibit biased behaviors under varied financial roles, regions, and identity-related contexts, underscoring the need for more robust debiasing methods and careful deployment of LLMs in high-risk financial applications."}}
{"id": "2601.05256", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.05256", "abs": "https://arxiv.org/abs/2601.05256", "authors": ["Eirini Baltzi", "Tilemachos Moumouris", "Athena Psalta", "Vasileios Tsironis", "Konstantinos Karantzalos"], "title": "Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring", "comment": null, "summary": "Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.", "AI": {"tldr": "NAIAD is an agentic AI assistant that uses LLMs plus EO and environmental tools to provide end\u2011to\u2011end inland water quality monitoring from natural-language queries.", "motivation": "Inland water quality monitoring is crucial for public health and ecosystems, but current approaches tend to focus on individual indicators (e.g., cyanobacteria, chlorophyll) or narrow sub-tasks instead of offering an integrated, user-friendly system that can answer diverse stakeholder questions. There is a need for a holistic, automated assistant that can connect EO data, domain models, and expert knowledge for both expert and non-expert users.", "method": "The paper proposes NAIAD, an agentic AI system that combines LLM-based reasoning with Retrieval-Augmented Generation, external tool orchestration, and computational graph execution. A single natural-language prompt is converted into a reasoning and tool-calling workflow that pulls from curated knowledge sources and EO data. The system integrates tools for accessing weather data, Sentinel-2 imagery, computing remote sensing indices such as NDCI, estimating chlorophyll-a, and interfacing with platforms like CyFi. It includes agentic reflection to improve responses and supports multiple LLM backbones, which are compared in an ablation study.", "result": "On a dedicated benchmark that spans different user-expertise levels and query types, NAIAD attains over 77% correctness and over 85% relevancy. Experiments indicate the system is adaptable and robust across various inland water monitoring queries. The ablation study identifies Gemma 3 (27B) and Qwen 2.5 (14B) as the most suitable backbones, balancing computational cost and reasoning quality.", "conclusion": "NAIAD demonstrates that an agentic LLM-based assistant can successfully unify EO data, environmental analytics, and domain knowledge into a single natural-language interface for inland water monitoring. The system delivers accurate, relevant, and user-adapted reports, and specific LLM choices (Gemma 3 27B and Qwen 2.5 14B) provide strong performance\u2013efficiency trade-offs. This positions NAIAD as a promising direction for practical, scalable monitoring tools for both experts and non-experts."}}
{"id": "2601.05411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05411", "abs": "https://arxiv.org/abs/2601.05411", "authors": ["Jan \u010cern\u00fd", "Ivana Kvapil\u00edkov\u00e1", "Silvie Cinkov\u00e1"], "title": "Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts", "comment": null, "summary": "This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.", "AI": {"tldr": "The paper explores using information entropy, computed via multiple language models, to estimate and visualize text readability, particularly for administrative documents.", "motivation": "Administrative and bureaucratic texts are often hard to understand. The authors seek a principled, quantitative way to assess and ultimately improve their readability and clarity, beyond traditional readability formulas.", "method": "They compute approximations of information entropy for given texts using multiple language models, then embed this in a visualization framework that shows entropy patterns. This multi-model visualization is intended to highlight complex or unclear parts of the text. The approach is implemented in an open-source software toolkit called Glitter.", "result": "They provide a working visualization framework and toolset that can approximate and display information entropy of text using several language models. The tool is released as libre/open-source software on GitHub (ufal/Glitter).", "conclusion": "Information entropy derived from language models can be practically visualized and used as a proxy for readability. Their open-source framework lays groundwork for tools that help authors analyze and improve complex administrative texts."}}
{"id": "2601.05298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05298", "abs": "https://arxiv.org/abs/2601.05298", "authors": ["Yeongbin Cha", "Namjung Kim"], "title": "Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing", "comment": "preprint", "summary": "Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.", "AI": {"tldr": "The paper presents an ontology- and equation-centered framework that combines large language models with a mathematical knowledge graph to extract, represent, and use additive manufacturing knowledge for more reliable extrapolative modeling.", "motivation": "In additive manufacturing, predicting material and process behavior often requires extrapolating beyond existing data. Current data-driven methods struggle because knowledge is scattered in unstructured text and models extrapolate unreliably, especially under sparse data. A more structured and physics-aware way to represent and use knowledge from the literature is needed to make extrapolation stable and trustworthy.", "method": "The authors build an additive manufacturing mathematical knowledge graph (AM-MKG) that encodes equations, variables, assumptions, and their semantic relations within a formal ontology. They transform unstructured literature into this machine-interpretable representation using ontology-guided extraction. Large language models are then used to generate equations, but their outputs are constrained by relevant MKG subgraphs to ensure physically meaningful functional forms. Finally, they define a confidence-aware extrapolation assessment that combines extrapolation distance, statistical stability of predictions, and physical consistency derived from the knowledge graph into a unified confidence score.", "result": "Ontology-guided extraction improves the structural coherence and quantitative reliability of the extracted knowledge from literature compared with unguided approaches. When LLMs generate equations conditioned on MKG subgraphs, the resulting models provide more stable, physically consistent extrapolations than unconstrained LLM-generated models. The proposed confidence score better reflects extrapolation reliability than uncertainty estimates alone.", "conclusion": "Integrating a formal ontology and mathematical knowledge graph with LLM-based equation generation enables a unified, equation-centric pipeline for knowledge extraction, reasoning, and extrapolative modeling in additive manufacturing. Knowledge-graph-augmented LLMs can act as reliable tools for physics-aware extrapolation when guided by structured domain knowledge and complemented with a confidence-aware assessment of predictions."}}
{"id": "2601.05414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05414", "abs": "https://arxiv.org/abs/2601.05414", "authors": ["Minda Zhao", "Yilun Du", "Mengyu Wang"], "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions", "comment": null, "summary": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.", "AI": {"tldr": "The paper audits how well large language models can sample from specified probability distributions and finds that most models fail badly, especially when using many independent calls, so external samplers are needed for reliable probabilistic behavior.", "motivation": "As LLMs are increasingly used in pipelines that assume correct probabilistic sampling (e.g., generating balanced datasets, enforcing target proportions in assessments or synthetic data), it becomes important to know whether models can actually draw faithful samples from explicit distributions, not just approximate them heuristically. Prior work treated this more as a theoretical curiosity; this paper addresses it as a practical, large-scale reliability question.", "method": "The authors benchmark 11 frontier LLMs on their ability to sample from 15 different probability distributions. They use two distinct protocols to separate potential failure modes: (1) Batch Generation, where each model is asked to produce N=1000 samples in a single response, and (2) Independent Requests, where they issue N=1000 independent, stateless calls to the model, each requesting one sample. They then apply statistical tests to evaluate how closely the empirical samples match the target distributions and analyze how performance changes with distributional complexity and sample size N. They also construct downstream tasks\u2014MCQ generation with uniform answer positions and attribute-constrained text-to-image prompt generation\u2014to see how sampling errors manifest in applied settings.", "result": "Batch Generation yields limited but non-zero statistical validity, with a median pass rate of 13% across distributions. In contrast, the Independent Requests protocol performs extremely poorly: 10 of the 11 models fail to pass any of the 15 distributions. Performance worsens as the distributions become more complex and as the number of requested samples N increases. The same issues appear in downstream applications: models cannot maintain uniform answer-position distributions in multiple-choice question generation and systematically miss demographic proportion targets in text-to-image prompt synthesis.", "conclusion": "Current LLMs do not implement a reliable, functional internal sampling mechanism that can be trusted to follow explicit probability distributions, particularly under independent-call usage patterns that are common in real-world pipelines. As a result, any application requiring statistical guarantees or controlled sampling should not rely on the model\u2019s native sampling behavior alone but should instead integrate external tools or algorithms for probabilistic control."}}
{"id": "2601.05302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05302", "abs": "https://arxiv.org/abs/2601.05302", "authors": ["Mizuki Sakai", "Mizuki Yokoyama", "Wakaba Tateishi", "Genki Ichinose"], "title": "Effects of personality steering on cooperative behavior in Large Language Model agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.", "AI": {"tldr": "The paper studies how assigning personality traits to large language models affects their cooperative behavior in repeated Prisoner\u2019s Dilemma games, finding that agreeableness strongly promotes cooperation but also increases vulnerability to exploitation, especially in older models.", "motivation": "As LLMs are increasingly deployed as autonomous agents in strategic and social settings, understanding and controlling their social behavior (e.g., cooperation vs. defection) becomes crucial. Prior work shows that \u201cpersonality steering\u201d via prompts can change LLM behavior, but it is not clear, under controlled experimental conditions, which personality traits matter for cooperation, and whether personality can be used as a reliable control mechanism rather than a loose bias. The paper aims to fill this gap.", "method": "The authors first assess the baseline personality profiles of three LLMs (GPT-3.5-turbo, GPT-4o, GPT-5) by administering the Big Five Inventory and interpreting the resulting scores. They then run repeated Prisoner\u2019s Dilemma experiments where these models act as agents under two types of conditions: (1) baseline (no explicit personality steering) and (2) personality-informed, where prompts specify certain Big Five profiles. They further perform controlled manipulations of each Big Five trait\u2014agreeableness, conscientiousness, extraversion, neuroticism, and openness\u2014setting each to extreme high or low values in the prompts, and analyze how these manipulations affect cooperation rates, exploitation, and strategy dynamics over repeated rounds.", "result": "Across all three models, high agreeableness robustly increases cooperative choices in repeated Prisoner\u2019s Dilemma games. Other Big Five traits show comparatively weak or inconsistent effects on cooperation. Explicitly stating cooperative-oriented personality traits in the prompt generally raises cooperation rates but also leads, particularly in GPT-3.5-turbo and GPT-4o, to higher exploitation by less cooperative counterparts. Newer models (GPT-5) demonstrate more selective cooperation: they cooperate more when appropriate but are better at avoiding sustained exploitation. Overall, personality steering shifts behavioral tendencies but does not fully determine strategies.", "conclusion": "Personality steering in LLMs functions primarily as a behavioral bias that nudges agents toward more or less cooperative behavior, rather than as a precise control dial for their strategic decisions. Among the Big Five traits, agreeableness is the key driver of increased cooperation, while other traits contribute minimally in this setting. Explicit personality prompts can have side effects by making agents more prone to exploitation, though newer model generations mitigate this risk via more context-sensitive behavior. These insights highlight both the promise and the limits of using psychological constructs like personality for aligning and controlling LLM agents in multi-agent interactions."}}
{"id": "2601.05437", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05437", "abs": "https://arxiv.org/abs/2601.05437", "authors": ["Chenxiao Yu", "Bowen Yi", "Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Jinyi Ye", "Shrikanth Narayanan", "Yue Zhao", "Morteza Dehghani"], "title": "Tracing Moral Foundations in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.", "AI": {"tldr": "The paper investigates whether large language models have structured internal representations of moral concepts, using Moral Foundations Theory as a framework.", "motivation": "Although LLMs can generate human-like moral judgments, it is unknown if this behavior reflects genuine internal conceptual structures or just surface-level imitation. Understanding this is important for interpretability, safety, and aligning AI moral behavior with human values.", "method": "The authors analyze two instruction-tuned LLMs (Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct) using three complementary approaches: (i) layer-wise analysis of how Moral Foundations Theory (MFT) concepts are represented and how well they align with human moral perceptions; (ii) use of pretrained sparse autoencoders over the residual stream to identify sparse features corresponding to moral concepts; and (iii) causal steering experiments where they manipulate model behavior along dense MFT vectors and sparse SAE features to test causal effects on moral outputs.", "result": "They find that both models encode and differentiate moral foundations in a structured, layer-specific way that correlates with human moral judgments. Sparse autoencoder features are semantically linked to particular moral foundations, indicating partially disentangled mechanisms. Manipulating models along these dense and sparse directions reliably shifts their moral-foundation-relevant behavior, revealing a causal role for these internal representations.", "conclusion": "Moral concepts in these LLMs are not just superficial patterns but correspond to distributed, layered, and partly disentangled internal structures. Pluralistic moral structure appears as an emergent latent pattern from language statistics alone, offering mechanistic evidence that LLMs develop internal moral representations aligned with Moral Foundations Theory."}}
{"id": "2601.05330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05330", "abs": "https://arxiv.org/abs/2601.05330", "authors": ["Tengwei Song", "Long Yin", "Zhen Han", "Zhiqiang Xu"], "title": "Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings", "comment": null, "summary": "Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.", "AI": {"tldr": "The paper proposes Hyper-Enz, a knowledge-enhanced hypergraph model using knowledge graph embeddings and hypergraph transformers to predict enzyme-substrate interactions from chemical reaction data, achieving large accuracy gains over traditional methods.", "motivation": "Enzyme-substrate interaction prediction is central to biochemistry and metabolic engineering, but existing curated databases of enzyme-substrate pairs are sparse and costly to maintain. This data scarcity limits traditional models\u2019 ability to generalize to unseen interactions. Chemical reaction equations are more abundant and denser but involve complex multi-compound interactions with enzymes that conventional models don\u2019t capture well. The paper is motivated by a need to better exploit these rich reaction databases to infer missing enzyme-substrate pairs.", "method": "The authors convert chemical reaction equations into knowledge graph triples of the form (educt, enzyme, product), enabling the use of knowledge graph embedding (KGE) for graph completion and enzyme-substrate inference. They propose Hyper-Enz, a knowledge-enhanced hypergraph model that integrates a hypergraph transformer with a KGE model to learn representations of hyper-edges that can involve multiple educts and products. Additionally, they introduce a multi-expert training paradigm that jointly leverages the proposed model and the reaction equation information to guide learning of enzyme-substrate interactions.", "result": "Experiments show that Hyper-Enz significantly outperforms traditional enzyme prediction models. The method achieves up to an 88% relative improvement in average enzyme retrieval accuracy and a 30% improvement in pair-level enzyme-substrate prediction performance, demonstrating its effectiveness in predicting interactions from reaction data.", "conclusion": "By representing reaction equations as knowledge graphs and modeling them with a hypergraph transformer integrated with KGE, Hyper-Enz can effectively capture complex multi-compound\u2013enzyme relationships and infer missing enzyme-substrate pairs. Leveraging richer reaction equation data, the proposed method substantially improves enzyme prediction accuracy compared with traditional approaches, suggesting a promising direction for data-efficient biochemical interaction modeling."}}
{"id": "2601.05459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05459", "abs": "https://arxiv.org/abs/2601.05459", "authors": ["Hongjin Kim", "Jaewook Lee", "Kiyoung Lee", "Jong-hun Shin", "Soojong Lim", "Oh-Woog Kwon"], "title": "Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction", "comment": "IJCNLP-AACL 2025 (Main), Outstanding Paper Award", "summary": "Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.", "AI": {"tldr": "The paper studies how to improve Korean reasoning performance of LLMs using reinforcement learning, finding that RL helps only after the model\u2019s internal reasoning is aligned with Korean via targeted fine-tuning.", "motivation": "LLMs reason well in high-resource languages like English but perform poorly in low-resource languages such as Korean. The authors want to know whether RL can close this gap and what conditions are required for RL to be effective for multilingual reasoning.", "method": "They apply reinforcement learning to models on Korean reasoning tasks, then experiment with various fine-tuning strategies. In particular, they align internal reasoning with Korean by tuning Korean-sensitive neurons in early layers and by training on a self-correction code-switching dataset that mixes languages to encourage internal translation and reasoning alignment.", "result": "RL alone brings only small gains when models have weak Korean reasoning. After neuron-level, Korean-focused fine-tuning and training with the code-switching self-correction dataset, the models show large improvements in Korean mathematical reasoning and self-correction performance, approaching their English capabilities more closely.", "conclusion": "Enhancing multilingual reasoning is less about adding new linguistic knowledge and more about eliciting and aligning existing reasoning abilities with the target language. Internal translation mechanisms and selective neuron tuning in early layers are crucial for making RL effective for low-resource language reasoning like Korean."}}
{"id": "2601.05376", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05376", "abs": "https://arxiv.org/abs/2601.05376", "authors": ["Tassallah Abdullahi", "Shrestha Ghosh", "Hamish S Fraser", "Daniel Le\u00f3n Tramontini", "Adeel Abbasi", "Ghada Bourjeily", "Carsten Eickhoff", "Ritambhara Singh"], "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models", "comment": null, "summary": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $\u03ba= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.", "AI": {"tldr": "The paper studies how assigning personas to clinical LLMs (like \u201cER doctor\u201d or \u201cnurse\u201d, bold vs cautious) affects their accuracy, calibration, and safety in medical decision-making, finding that personas help in some contexts but hurt in others, so they are not uniformly beneficial or safe.", "motivation": "Persona prompting is widely used to steer LLM behavior and is often presumed to uniformly enhance expertise and safety, especially in high\u2011stakes domains like medicine. However, its actual impact on clinical decision\u2011making, including triage and patient safety, has not been systematically characterized, raising concerns about untested assumptions in real\u2011world deployments of clinical LLMs.", "method": "The authors systematically evaluate persona-based conditioning in clinical LLMs by assigning professional-role personas (e.g., Emergency Department physician, nurse) and interaction-style personas (bold vs cautious). They test these across different clinical tasks (critical care vs primary care) and models, measuring performance with multidimensional metrics: task accuracy, calibration, and safety-related risk behavior. They also compare LLM persona performance using LLM-judge rankings and human clinician evaluations that assess safety compliance and reasoning quality, with agreement quantified via Cohen\u2019s kappa.", "result": "Medical personas substantially improve LLM performance in certain high-acuity settings (critical care), with up to approximately +20% gains in accuracy and calibration, but cause comparable performance degradation in lower-acuity, primary-care contexts. Interaction-style personas systematically affect risk-taking and sensitivity, but these effects are highly model-dependent. Aggregated LLM-judge rankings tend to prefer medical personas over non-medical ones in safety-critical scenarios. Human clinicians, however, show only moderate agreement on safety compliance (average Cohen\u2019s \u03ba = 0.43) and report low confidence in nearly all (95.9%) of their assessments of LLM reasoning quality.", "conclusion": "Persona prompts act as behavioral priors that shape LLM behavior in nuanced, context-dependent ways rather than delivering uniform improvements in safety or expertise. While certain medical personas can enhance performance in critical care settings, they introduce trade-offs and can impair performance in other contexts, such as primary care. Consequently, persona conditioning cannot be treated as a monotonic safety guarantee; careful, task-specific evaluation is necessary when using personas in clinical LLM applications."}}
{"id": "2601.05473", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.05473", "abs": "https://arxiv.org/abs/2601.05473", "authors": ["Zhihao Yuan", "Yunze Xiao", "Ming Li", "Weihao Xuan", "Richard Tong", "Mona Diab", "Tom Mitchell"], "title": "Towards Valid Student Simulation with Large Language Models", "comment": null, "summary": "This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the \"competence paradox\" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.", "AI": {"tldr": "Framework for making LLM-based simulated students more faithful to real learners by explicitly modeling what they know and how their knowledge changes.", "motivation": "LLMs are increasingly used to simulate students for research, tutoring-system design, and curriculum testing. However, powerful LLMs struggle to authentically mimic learners with partial or imperfect knowledge: they either answer too well, make unrealistic errors, or display implausible learning trajectories. This \u201ccompetence paradox\u201d threatens the scientific validity and practical usefulness of LLM-based student simulations, motivating a need for clearer conceptual foundations and design principles.", "method": "The paper does not introduce a new system or benchmark; instead, it develops a conceptual and methodological framework. It (1) analyzes the competence paradox as a core failure mode of LLM-based student simulation; (2) formalizes student simulation as constrained text generation guided by an explicit Epistemic State Specification (ESS) that encodes accessible knowledge, error structures, and state transitions; and (3) proposes a Goal-by-Environment framework to classify simulation setups based on intended behavioral goals and deployment environments. It synthesizes prior work and articulates design dimensions and constraints rather than running empirical experiments.", "result": "The main result is a structured framework and vocabulary for designing and reasoning about LLM-based simulated students. By specifying epistemic states and constraints explicitly, the framework clarifies how to align a model\u2019s behavior with target learner profiles and how to reason about their learning dynamics over time. It also organizes existing approaches within the Goal-by-Environment space, highlighting gaps, tradeoffs, and misalignments between intended use and actual simulation design.", "conclusion": "The paper concludes that reliable use of LLM-based simulated students requires prioritizing epistemic fidelity\u2014faithful representation of what learners know, can access, and how they err\u2014over superficial behavioral realism. Explicit Epistemic State Specifications and careful consideration of goals and environments are necessary to avoid the competence paradox and to make simulations scientifically meaningful and pedagogically trustworthy. The authors also emphasize persistent open challenges around validating these simulations, evaluating their fidelity, and managing ethical risks of their deployment."}}
{"id": "2601.05384", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05384", "abs": "https://arxiv.org/abs/2601.05384", "authors": ["Alessandro Bellina", "Giordano De Marzo", "David Garcia"], "title": "Conformity and Social Impact on AI Agents", "comment": null, "summary": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.", "AI": {"tldr": "The paper studies how large multimodal language models, used as AI agents, conform to group opinions under social pressure in multi-agent settings, showing that they systematically exhibit human-like conformity that creates security vulnerabilities.", "motivation": "As AI agents increasingly interact in multi-agent environments and artificial societies, their collective behavior can shape outcomes such as information flow, decision-making, and vulnerability to manipulation. Human social psychology shows that people conform to group pressure; if AI agents show similar patterns, this could lead to systemic risks like coordinated errors, misinformation cascades, and exploitability. The paper aims to understand whether and how modern large language models acting as agents exhibit conformity, and what this implies for the safety and robustness of multi-agent AI systems.", "method": "The authors adapt classic visual conformity experiments from social psychology (e.g., Asch-style line judgment tasks) into a setting where large multimodal language models operate as AI agents. They place a target agent in a group of other agents that provide answers, some of which are manipulated to represent a majority opinion. They systematically vary key social influence variables\u2014group size, unanimity (whether there is a dissenter), task difficulty (easy vs at competence boundary), and characteristics of the information source\u2014and measure how often the target agent aligns with the group even when the group is wrong. They compare the agent\u2019s performance in isolation versus in social settings and examine effects across different model scales and capabilities.", "result": "The experiments show that AI agents display a consistent conformity bias analogous to patterns predicted by Social Impact Theory. Agents are more likely to align with group opinions as group size increases, when the group is unanimous, and as tasks become more difficult. Models that perform near-perfectly when reasoning alone become much more error-prone when exposed to conflicting group opinions, indicating strong susceptibility to social influence. While larger, more capable models conform less on simple tasks, they still show substantial conformity and vulnerability near their competence limits. These effects are observed across model scales, suggesting a general phenomenon rather than an artifact of a particular model size.", "conclusion": "The paper concludes that LLM-based AI agents systematically conform to group influence in multi-agent environments, mirroring key aspects of human social behavior. This conformity can be exploited to manipulate agent decisions, enabling coordinated misinformation, malicious steering, and amplification of biases in artificial societies. The persistence of these vulnerabilities, even in larger models and especially near their capability boundaries, underscores a fundamental security and safety risk in collective AI deployments. The authors argue for urgent development of safeguards\u2014such as mechanisms for independent verification, robustness to social pressure, and better multi-agent governance\u2014to prevent harmful emergent behaviors in AI agent collectives."}}
{"id": "2601.05478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05478", "abs": "https://arxiv.org/abs/2601.05478", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Fanxiao Li", "Zhi Zeng", "Min-Yen Kan"], "title": "The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence", "comment": null, "summary": "To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.", "AI": {"tldr": "The paper shows that LLMs, though robust to obvious misinformation, are easily misled by sophisticated, hard-to-falsify deceptive evidence, and proposes a framework (MisBelief) to generate such evidence and a defense method (DIS) to detect deceptive intent and reduce belief shifts.", "motivation": "Existing evaluations mainly test LLMs against direct, blatant misinformation, but real-world manipulation often uses subtle, logically persuasive yet deceptive evidence. There is a need to understand whether LLMs can maintain correct beliefs when faced with such refined, hard-to-falsify misinformation and to develop mechanisms to guard against this vulnerability.", "method": "The authors build MisBelief, a framework where multiple LLM agents with different roles engage in multi-round, collaborative interactions to generate misleading but logically plausible evidence supporting false claims. They create 4,800 such instances at three difficulty levels and test 7 representative LLMs, measuring how their belief in false statements changes after exposure to this evidence. They then design Deceptive Intent Shielding (DIS), a governance mechanism that analyzes incoming evidence for signs of deceptive intent and provides early warnings, encouraging the model to evaluate evidence more cautiously.", "result": "Across 4,800 generated cases, LLMs remain robust to direct, explicit misinformation but show large vulnerability to refined deceptive evidence: belief scores in falsehoods increase by an average of 93.0%, significantly degrading downstream recommendations. With DIS applied, models exhibit reduced belief shifts toward falsehoods and show more cautious evidence evaluation across the tested settings.", "conclusion": "LLMs have a critical, previously underexplored vulnerability to sophisticated, hard-to-falsify deceptive evidence that can drastically alter their internal beliefs and recommendations. The MisBelief framework exposes this weakness at scale, and the proposed Deceptive Intent Shielding offers an effective governance layer that detects potential deceptive intent and mitigates harmful belief shifts, suggesting a path toward more trustworthy decision-support systems."}}
{"id": "2601.05386", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05386", "abs": "https://arxiv.org/abs/2601.05386", "authors": ["Daniel Keren"], "title": "On the Effect of Cheating in Chess", "comment": null, "summary": "Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}", "AI": {"tldr": "The paper studies how much a chess player can improve their results by consulting a strong engine only a limited number of times per game.", "motivation": "Cheating with chess engines, including at top levels, has become a serious problem. Most prior research focuses on detecting if cheating occurred but not on quantifying how strong or effective limited assistance actually is. Understanding the potential performance gain from occasional engine use is important both to assess the real impact of cheating and to design better detection and prevention methods and fair policies.", "method": "The authors design algorithms that simulate or model a player receiving advice from a strong chess engine only a limited number of times during a game. They implement and test these algorithms on a widely used chess engine, examining how different cheating strategies (e.g., when and how often to consult the engine) affect playing strength and game outcomes.", "result": "They empirically estimate the performance boost obtainable from a bounded number of engine consultations, showing how even infrequent cheating can substantially raise effective playing strength. The results quantify the relationship between the number/timing of cheating instances and the resulting improvement in play.", "conclusion": "Limited, strategic engine assistance can significantly enhance a player\u2019s performance, so even sparse cheating is impactful. Quantifying this effect is essential for calibrating anti-cheating measures, setting appropriate thresholds for detection systems, and understanding the seriousness of seemingly small infractions."}}
{"id": "2601.05488", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05488", "abs": "https://arxiv.org/abs/2601.05488", "authors": ["Zhiyu Shen", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Yanghui Rao"], "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards", "comment": "19 pages (9 main + 10 appendix), 7 figures, 3 tables", "summary": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.", "AI": {"tldr": "MemBuilder is an RL framework that teaches LLMs to build and manage multi-dimensional memories for long-term dialogue, using dense, attributed rewards so a small open model can beat closed-source baselines.", "motivation": "Standard retrieval for long-term dialogue misses temporal evolution of user and dialogue state, and existing memory-augmented systems either hard-code prompts into closed-source models or rely on RL with very sparse rewards, leading to weak learning. The paper aims to create a trainable, open framework that can effectively construct and attribute rich conversational memory over long time horizons.", "method": "They design MemBuilder, a reinforcement learning framework where an LLM acts as a policy that constructs and updates multi-dimensional memories (different memory components). To avoid sparse rewards, they generate synthetic, session-level questions over long dialogue trajectories and use the model\u2019s ability to answer them as dense intermediate rewards. For better credit assignment across memory components, they propose contribution-aware gradient weighting, rescaling policy gradients for each memory dimension based on its measured downstream impact on performance.", "result": "Using MemBuilder, a 4B-parameter model trained with their RL scheme surpasses state-of-the-art closed-source baselines on multiple long-term dialogue benchmarks. The model shows improved consistency and generalization in maintaining and using long-term conversational memory.", "conclusion": "Structured, RL-based training of memory construction with dense, attributed rewards can substantially improve long-term dialogue consistency, allowing relatively small open models to outperform strong closed baselines. Reward shaping via synthetic questions and contribution-aware gradient weighting are key to effective training over long trajectories."}}
{"id": "2601.05455", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05455", "abs": "https://arxiv.org/abs/2601.05455", "authors": ["Sahil Wadhwa", "Himanshu Kumar", "Guanqun Yang", "Abbaas Alif Mohamed Nishar", "Pranab Mohanty", "Swapnil Shinde", "Yue Wu"], "title": "ART: Adaptive Reasoning Trees for Explainable Claim Verification", "comment": null, "summary": "Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.", "AI": {"tldr": "The paper introduces Adaptive Reasoning Trees (ART), a hierarchical argumentation framework using LLMs to verify claims in an explainable and contestable way, outperforming prior methods like Chain-of-Thought on multiple datasets.", "motivation": "Although LLMs are strong at complex, zero-shot decision-making, they are opaque: they do not provide faithful, structured explanations and users cannot easily contest or correct their reasoning, which is problematic in high-stakes scenarios. The authors aim to build a verification method that yields transparent, contestable, and trustworthy decisions.", "method": "The authors propose ART (Adaptive Reasoning Trees), where verification starts from a root claim and expands into a tree of supporting and attacking arguments as child nodes. Each node\u2019s strength is computed bottom-up via pairwise tournaments among its children, with an LLM acting as a judge to decide which arguments are stronger. This structured aggregation yields a final verdict on the root claim, and the whole reasoning path is explicit and inspectable. They also experiment with different argument generation strategies and comparison schemes across datasets.", "result": "Across several claim verification datasets, ART\u2019s structured argumentation approach yields higher accuracy and reliability than strong baselines, including Chain-of-Thought-style reasoning, establishing new state-of-the-art or benchmark performance for explainable claim verification tasks.", "conclusion": "ART provides a more transparent, reliable, and contestable framework for claim verification with LLMs than existing approaches. By organizing reasoning into explicit trees of pro and con arguments and using an LLM judge to aggregate them, ART improves both performance and explainability, making LLM-based decision-making more suitable for high-stakes contexts."}}
{"id": "2601.05505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05505", "abs": "https://arxiv.org/abs/2601.05505", "authors": ["Yubo Hou", "Zhisheng Chen", "Tao Wan", "Zengchang Qin"], "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse", "comment": null, "summary": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.", "AI": {"tldr": "FlashMem is a memory framework that turns an LLM\u2019s own hidden states into long-term latent memory without extra encoders, achieving similar performance to heavy baselines with 5\u00d7 lower latency.", "motivation": "LLMs are stateless and must re-ingest long histories to act autonomously over long horizons, which is slow and costly. Existing latent memory methods try to store compressed history, but they usually rely on separate encoder modules that are architecturally decoupled from the main reasoning model, adding complexity and inefficiency. The authors want a way to get persistent, efficient memory directly from the LLM\u2019s internal states, without extra encoders or redundant computation.", "method": "The paper introduces FlashMem, which reuses the LLM backbone\u2019s own computation to form memory. The key idea is that internal hidden states uniquely capture the input trajectory, so the final hidden state can be treated as a sufficient statistic of the interaction history. FlashMem uses a Shared-KV Consolidator that reads directly from the model\u2019s frozen key\u2013value (KV) cache and synthesizes memory representations via attention, avoiding new parameterized encoders. A parameter-free Cognitive Monitor computes attention entropy to estimate epistemic uncertainty and decides when to consolidate memory: memory is only updated when uncertainty is high, reducing unnecessary consolidation steps.", "result": "Experiments show that FlashMem matches the performance of more complex, heavy latent-memory baselines on relevant long-horizon or memory-demanding tasks, while reducing inference latency by a factor of five. This confirms that directly reusing hidden states and KV caches can provide efficient memory without sacrificing effectiveness.", "conclusion": "FlashMem demonstrates that an LLM\u2019s own hidden state can serve as an intrinsic memory representation, making it possible to build persistent cognition on top of a nominally stateless model without auxiliary encoders. By consolidating memory from the frozen KV cache and adaptively triggering updates using a simple uncertainty signal, FlashMem achieves a strong trade-off between computational efficiency and long-horizon performance, narrowing the gap between fast inference and effective, persistent memory in LLM agents."}}
{"id": "2601.05520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05520", "abs": "https://arxiv.org/abs/2601.05520", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems", "comment": "22 pages, 13 figures, 7 tables", "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.", "AI": {"tldr": "The paper proposes CHisAgent, a multi-agent LLM framework to automatically build a historical event taxonomy for ancient China from texts like the Twenty-Four Histories.", "motivation": "LLMs struggle with historical and cultural reasoning in non-English contexts, and building high-quality historical taxonomies by hand is expensive and hard to scale. The authors want an automated way to organize Chinese historical knowledge into a coherent taxonomy to improve understanding and reasoning.", "method": "They design CHisAgent, a multi-agent framework with three specialized roles: (1) Inducer: performs bottom-up extraction of event types and hierarchies from raw historical corpora; (2) Expander: uses LLM world knowledge to add missing intermediate concepts and refine the top-down structure; (3) Enricher: integrates external structured historical resources, using evidence to correct and ground the taxonomy. They apply this framework to the Twenty-Four Histories to construct a large-scale event taxonomy spanning several domains of ancient Chinese life.", "result": "The system produces a domain-aware, large-scale event taxonomy for ancient Chinese history, covering politics, military, diplomacy, and social life. Evaluations, both without gold references and against available references, show that the produced taxonomy has better structural coherence and coverage than baselines. Additional analysis indicates that the taxonomy can be used to support cross-cultural alignment tasks.", "conclusion": "A multi-agent LLM framework can effectively automate historical taxonomy construction in complex, low-resource cultural settings like ancient China. By combining bottom-up induction, top-down expansion, and evidence-based enrichment, CHisAgent generates coherent, comprehensive taxonomies that organize historical knowledge and support cross-cultural reasoning."}}
{"id": "2601.05483", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05483", "abs": "https://arxiv.org/abs/2601.05483", "authors": ["Zixuan Xiao", "Jun Ma", "Siwei Zhang"], "title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis", "comment": null, "summary": "Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.", "AI": {"tldr": "MMUEChange is a multi-modal agent framework that fuses heterogeneous urban data to analyze complex urban environment changes more flexibly and accurately than single-modal remote sensing methods.", "motivation": "Urban environmental change analysis is crucial for sustainable development, but existing methods, especially traditional remote sensing change detection, are rigid, single-modal, and struggle to capture complex, multi-faceted urban dynamics. There is a need for a system that can flexibly integrate diverse data sources and reduce hallucinations while handling realistic urban policy questions.", "method": "The paper proposes MMUEChange, a multi-modal agent framework built around a modular toolkit and a core Modality Controller module. The toolkit allows ingestion and processing of heterogeneous urban data, while the Modality Controller performs both cross-modal and intra-modal alignment to coordinate information from different sources. The framework is evaluated via urban case studies and benchmarked against a strong baseline for task success rate and hallucination control.", "result": "In three case studies, MMUEChange reveals nuanced patterns: 1) in New York, a transition toward smaller, community-oriented parks aligned with local green space initiatives; 2) in Hong Kong, the spread of concentrated water pollution across districts, implying coordinated water management issues; and 3) in Shenzhen, a marked reduction in open dumpsites coupled with differing relationships between nighttime economic activity and domestic versus construction waste. Quantitatively, MMUEChange improves task success rate by 46.7% over the best-performing baseline and reduces hallucination in urban change analysis tasks.", "conclusion": "MMUEChange demonstrates that a multi-modal, agent-based framework with explicit modality control can substantially enhance the robustness and expressiveness of urban environmental change analysis. Its strong empirical performance and interpretable case-study insights show its potential to support complex, real-world urban planning and environmental policy decisions."}}
{"id": "2601.05524", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05524", "abs": "https://arxiv.org/abs/2601.05524", "authors": ["Yuhao Shen", "Tianyu Liu", "Junyi Shen", "Jinyang Wu", "Quan Kong", "Li Huan", "Cong Wang"], "title": "Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism", "comment": null, "summary": "Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \\textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \\emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \\textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\\textbf{5.3}\\times$ on LLaMA3.3-70B and $\\textbf{2.8}\\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.", "AI": {"tldr": "The paper proposes DOUBLE, a training-free and lossless speculative decoding framework that overcomes speed limits and inefficiencies of existing speculative and parallel speculative decoding by using dual retrieval-based speculation and guidance, achieving state-of-the-art speedups on large LLMs.", "motivation": "Traditional speculative decoding is limited by a theoretical speedup ceiling based on the speed gap between draft and target models, and parallel speculative decoding still suffers from computational waste and pipeline stalls caused by mid-sequence token rejections due to early prediction errors. The authors want to break this ceiling and reduce waste without expensive retraining.", "method": "The authors introduce DOUBLE (Double Retrieval Speculative Parallelism), which synchronously combines iterative retrieval-based speculation by the draft model with authoritative retrieval-based multi-token guidance by the target model. The draft model repeatedly retrieves speculative continuations to extend beyond conventional bounds, while the target model periodically provides multi-token authoritative signals to reduce rejections and avoid rollback. The mechanism is fully training-free and integrates with speculative decoding pipelines between a fast draft model and a slower target model.", "result": "Experiments show that DOUBLE achieves up to 5.3x speedup on LLaMA3.3-70B and 2.8x on Qwen3-32B, outperforming prior advanced methods such as EAGLE-3 that require substantial additional training cost. The method preserves output quality (lossless) while improving decoding efficiency.", "conclusion": "DOUBLE effectively bridges the gap between standard speculative decoding and parallel speculative decoding by resolving the precision-efficiency trade-off in retrieval-based speculation. Its synchronous double-retrieval mechanism breaks prior speedup limits and reduces computational waste without training, providing a practical and state-of-the-art solution for faster, high-quality LLM inference."}}
{"id": "2601.05500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05500", "abs": "https://arxiv.org/abs/2601.05500", "authors": ["Aparna Elangovan", "Lei Xu", "Mahsa Elyasi", "Ismail Akdulum", "Mehmet Aksakal", "Enes Gurun", "Brian Hur", "Saab Mansour", "Ravid Shwartz Ziv", "Karin Verspoor", "Dan Roth"], "title": "The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm", "comment": null, "summary": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.\n  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.", "AI": {"tldr": "The paper argues that standard benchmarks for AI systems overlook uncertainty in expert ground truth, proposes a probabilistic framework to model this uncertainty, and recommends stratifying evaluation by ground-truth agreement to get more reliable performance comparisons.", "motivation": "Current evaluations of AI systems (especially in medicine) assume that expert ground truth labels are correct and deterministic, but in reality experts often disagree, creating uncertainty. This can make non-experts or weaker systems appear to perform similarly to experts when ground-truth variability is high. There is a need to formally account for this uncertainty so that benchmark scores actually reflect real differences in capability.", "method": "The authors build a probabilistic model of labeling in which each item has a distribution over possible ground-truth answers, and annotators (experts or systems) produce labels according to their own competence relative to this distribution. Within this framework, they derive expressions for expected accuracy and expected F1 as a function of the variability of ground-truth answers and annotator quality. They then use this to analyze how scores change under different levels of ground-truth certainty and to formalize when and why expert and non-expert scores converge.", "result": "The theoretical analysis shows that when ground-truth answers are highly certain (high expert agreement), experts can achieve high accuracy/F1 and clearly outperform weaker annotators. Conversely, when ground-truth variability is high (low agreement among experts), the maximum achievable accuracy/F1 is inherently limited, so the observed performance gap between experts and random labelers shrinks and may become small. This explains why benchmarks that ignore uncertainty can misleadingly suggest that non-experts or weaker systems are close to expert performance.", "conclusion": "The paper concludes that evaluation of AI systems should explicitly incorporate uncertainty in the ground truth. In practice, benchmark results should be stratified by the probability (or agreement rate) of the ground-truth answer, especially when overall scores fall below about 80%. Performance comparisons in the high-certainty strata are more trustworthy, because they are less confounded by label uncertainty, leading to more accurate assessments of system capability."}}
{"id": "2601.05543", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.05543", "abs": "https://arxiv.org/abs/2601.05543", "authors": ["Chaoren Wang", "Heng Lu", "Xueyao Zhang", "Shujie Liu", "Yan Lu", "Jinyu Li", "Zhizheng Wu"], "title": "Closing the Modality Reasoning Gap for Speech Large Language Models", "comment": null, "summary": "Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.", "AI": {"tldr": "They propose TARS, a reinforcement learning framework that aligns speech- and text-based reasoning in speech LLMs, greatly reducing the reasoning performance gap between modalities and achieving SOTA on hard benchmarks.", "motivation": "Speech large language models still reason much worse on spoken inputs than on equivalent text, implying a modality reasoning gap likely caused by how internal representations drift across layers and how long-chain reasoning behavior diverges between speech- and text-conditioned runs. There is a need for a principled way to bring speech-conditioned trajectories closer to text-conditioned ones, which generally exhibit stronger reasoning.", "method": "They design TARS, a reinforcement learning framework that optimizes speech LLMs using asymmetric rewards defined only on speech-conditioned trajectories but guided by text-conditioned ones. TARS uses two dense reward components: (1) representation alignment, which measures and rewards layer-wise similarity between hidden states of the speech- and text-conditioned trajectories; and (2) behavior alignment, which evaluates and rewards semantic consistency between speech-based generations and high-quality text reference completions. These signals are combined to update the speech LLM so that its internal computations and outputs for speech inputs become more like those for text inputs.", "result": "On challenging reasoning benchmarks such as MMSU and OBQA, the TARS-trained 7B-scale Speech LLM substantially reduces the gap between its reasoning performance on speech versus text inputs and reaches state-of-the-art performance among comparable (7B) speech LLMs.", "conclusion": "Aligning speech- and text-conditioned trajectories via reinforcement learning with dense representation and behavior alignment rewards is an effective strategy to mitigate the modality reasoning gap in speech LLMs. This approach improves reasoning on speech inputs without sacrificing text performance and establishes a new state of the art for 7B-scale Speech LLMs on demanding reasoning tasks."}}
{"id": "2601.05525", "categories": ["cs.AI", "cs.LG", "physics.comp-ph", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.05525", "abs": "https://arxiv.org/abs/2601.05525", "authors": ["Ricardo Vinuesa", "Steven L. Brunton", "Gianmarco Mengaldo"], "title": "Explainable AI: Learning from the Learners", "comment": null, "summary": "Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.", "AI": {"tldr": "The paper argues that combining explainable AI with causal reasoning lets us \u2018learn from AI learners\u2019 to improve scientific and engineering workflows.", "motivation": "Although AI now surpasses humans in many scientific and engineering tasks, its internal workings are opaque, limiting our ability to trust, adopt, or build on its insights. There is a need for frameworks that turn powerful but black-box models into tools that can reveal mechanisms, support design, and enable accountable use in high\u2011stakes domains.", "method": "This is a Perspective paper rather than an empirical study. The authors conceptually analyze how explainable AI (XAI) techniques, when integrated with causal reasoning and foundation models, can be used across three key activities\u2014discovery, optimization, and certification. They survey existing methods and outline how explanation tools can extract causal structure, inform robust design and control, and serve as a basis for human\u2013AI collaboration.", "result": "The paper synthesizes examples and arguments showing that XAI, paired with causal methods, can help surface candidate causal mechanisms from foundation models, provide interpretable guidance for design and control decisions, and create more transparent workflows suitable for high\u2011stakes applications. It identifies concrete pain points in current XAI\u2014such as lack of faithfulness, poor generalization of explanations, and usability barriers for domain experts.", "conclusion": "The authors propose explainable AI as a central, unifying framework for human\u2013AI collaboration in science and engineering. By explicitly embedding causal reasoning into XAI for discovery, optimization, and certification, AI systems can move from opaque predictors to partners that reveal mechanisms, support robust engineering decisions, and foster trust and accountability, while motivating future work on more faithful, generalizable, and usable explanation methods."}}
{"id": "2601.05545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05545", "abs": "https://arxiv.org/abs/2601.05545", "authors": ["Hongjin Kim", "Jeonghyun Kang", "Harksoo Kim"], "title": "Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring", "comment": "COLING 2025 accepted paper (Main)", "summary": "This study addresses critical gaps in Automated Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.", "AI": {"tldr": "The paper proposes a new benchmark, Harmful Essay Detection (HED), to evaluate how well AES systems and LLMs detect and appropriately score harmful essays on sensitive topics, showing that current systems often give high scores to ethically problematic content.", "motivation": "Existing Automated Essay Scoring systems and LLM-based scorers can assign high scores to well-written but ethically harmful essays because they mainly measure linguistic quality and argumentative structure, not ethical implications. This creates a critical gap when such systems are used in educational or evaluative contexts, where rewarding harmful content is unacceptable.", "method": "The authors construct the Harmful Essay Detection (HED) benchmark, a dataset of essays that intentionally include harmful opinions on sensitive topics such as racism and gender bias, mixed with non-harmful argumentative essays. They then evaluate several LLMs and existing AES models on this benchmark, examining their ability to (a) recognize harmful content and (b) reflect ethical considerations in the scores they assign.", "result": "Experimental results show that current LLMs struggle to reliably distinguish harmful essays from legitimate argumentative essays and often fail to downgrade scores when content is ethically problematic. Similarly, standard AES systems largely ignore ethical dimensions, focusing on surface quality and structure, and thus frequently assign inappropriately high scores to harmful essays.", "conclusion": "The study concludes that both AES systems and LLM-based scorers need to be redesigned to explicitly incorporate ethical awareness in their scoring criteria. It advocates for more robust, safety-sensitive AES frameworks and suggests that future research should integrate harm detection and value-sensitive evaluation into automated scoring pipelines."}}
{"id": "2601.05529", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05529", "abs": "https://arxiv.org/abs/2601.05529", "authors": ["Jua Han", "Jaeyoon Seo", "Jungbin Min", "Jean Oh", "Jihie Kim"], "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making", "comment": null, "summary": "One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.", "AI": {"tldr": "The paper evaluates whether current LLMs and VLMs can be safely used for robotics decision-making in safety-critical situations, showing they frequently fail in ways that could be catastrophic, so they are not ready for direct deployment.", "motivation": "As LLMs are increasingly used to control or advise robots in the physical world, even a single wrong instruction can cause serious harm or death. Despite high average accuracies, there is little systematic study of how often and how badly LLMs fail in life-or-death spatial and navigation tasks. The authors want to rigorously test these models under safety-critical conditions and understand whether a seemingly small error rate (e.g., 1%) is acceptable.", "method": "The authors first conduct a qualitative study using a fire evacuation scenario to identify how LLM-based decision-making can fail. Using these failure patterns, they design seven quantitative benchmark tasks in three categories: (1) Complete Information tasks using ASCII maps to isolate pure spatial reasoning; (2) Incomplete Information tasks that require inferring missing spatial context and reveal hallucinations vs. continuity-aware reasoning; and (3) Safety-Oriented Spatial Reasoning (SOSR) tasks, posed in natural language, that test whether models make safe decisions in life-threatening situations. They then benchmark multiple LLMs and VLMs on these tasks, compare performance, and analyze the implications of their observed error rates, especially the impact of a 1% failure rate in repeated robotic executions.", "result": "The experiments show severe and systematic vulnerabilities. Some models perform extremely poorly, achieving 0% success on ASCII navigation tasks. In SOSR fire-drill simulations, models sometimes instruct robots to move toward hazards instead of toward emergency exits. Even models that appear strong on average still exhibit nontrivial failure rates that, when translated into repeated real-world executions, imply frequent catastrophic errors. The analysis emphasizes that a nominally high accuracy (e.g., 99%) still corresponds to an unacceptably high rate of dangerous failures in safety-critical robotics.", "conclusion": "The authors conclude that current LLMs and VLMs are unsafe for direct deployment as decision-makers in safety-critical robotic systems. Their observed error rates, including rare but severe mistakes, mean that relying on these models without strong additional safeguards can lead to catastrophic harm. High aggregate accuracy is misleading in this domain; true safety requires near-zero catastrophic failures, which present models cannot provide."}}
{"id": "2601.05548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05548", "abs": "https://arxiv.org/abs/2601.05548", "authors": ["Jeonghyun Kang", "Hongjin Kim", "Harksoo Kim"], "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation", "comment": "COLING 2025 accepted paper (Main)", "summary": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.", "AI": {"tldr": "Introduces KEEM, a dataset for better long-term conversational memory that integrates factual, emotional, and causal information.", "motivation": "Existing long-term conversational systems often just accumulate dialogue history or use simple operations to update memory, which can cause conflicting information and poor tracking of the user\u2019s evolving state. Systems also tend to miss emotional context and causal links between events, limiting empathy and response quality.", "method": "Proposes KEEM, a generation-based dataset where system memories are dynamically generated and updated. These memories are integrative: they merge essential factual details from conversations with users\u2019 emotional states and causal relationships between events, rather than storing raw dialogue or applying simple edit operations.", "result": "KEEM enables the creation of memory representations that better reflect both what happened (facts) and how the user felt (emotions), as well as why things happened (causality). This leads to more accurate and context-aware memory of the user\u2019s current state in long-term interactions.", "conclusion": "By using KEEM\u2019s integrative, generation-based memories, conversational systems can maintain richer, more consistent long-term user models that capture essential and emotional information, improving empathetic and meaningful responses in open-domain dialogue."}}
{"id": "2601.05567", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05567", "abs": "https://arxiv.org/abs/2601.05567", "authors": ["Tengxiao Liu", "Deepak Nathani", "Zekun Li", "Kevin Yang", "William Yang Wang"], "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature", "comment": null, "summary": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.", "AI": {"tldr": "WildSci is a synthetic, literature-derived multi-choice dataset across 9 scientific disciplines used with RL to improve LLM scientific reasoning, showing gains on external science benchmarks.", "motivation": "LLMs reason well in math and coding but lag in scientific fields like medicine and materials science due to limited high-quality, broad-coverage datasets and the difficulty of evaluating open-ended scientific questions. The authors want a scalable way to train and objectively evaluate scientific reasoning across many domains.", "method": "They automatically synthesize domain-specific multiple-choice science questions from peer-reviewed papers spanning 9 disciplines and 26 subdomains, forming the WildSci dataset. Complex reasoning is cast into MC format to provide clear reward signals. They then finetune LLMs via reinforcement learning on WildSci and analyze training dynamics, including per-domain performance, behavioral shifts in responses, and generalization patterns.", "result": "Models trained with RL on WildSci improve performance on a range of existing scientific benchmarks. The analysis shows domain-specific performance gains, characteristic changes in model answers, and evidence of better generalization beyond the training distribution.", "conclusion": "Transforming complex scientific reasoning into large-scale, automatically generated multiple-choice questions enables effective RL-based finetuning of LLMs, leading to stronger and more general scientific reasoning capabilities. WildSci is released as an open dataset to support continued research in this area."}}
{"id": "2601.05560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05560", "abs": "https://arxiv.org/abs/2601.05560", "authors": ["Junyao Yang", "Chen Qian", "Dongrui Liu", "Wen Shen", "Yong Liu", "Jing Shao"], "title": "ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging", "comment": "22 pages, 6 figures, 14 tables", "summary": "Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as \"Reasoning + X\", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes \"Reasoning + X\" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.", "AI": {"tldr": "They introduce ReasonAny, a merging framework that fuses a strong reasoning model with domain-specific models without training, avoiding the usual performance collapse in either reasoning depth or domain utility.", "motivation": "Existing Large Reasoning Models excel at long chain-of-thought reasoning but are not domain-specialized. Conversely, domain-specific models lack strong reasoning. Naively merging them tends to damage both reasoning depth and domain performance. The authors want a training-free way to endow any domain model (safety, biomedicine, finance, etc.) with strong reasoning capabilities simultaneously.", "method": "They empirically analyze where reasoning ability is encoded in model parameters and find it sits mainly in low gradient-sensitivity regions, contrary to the typical focus on high-magnitude or high-sensitivity parameters for domain skills. Based on this, they design ReasonAny, a model-merging framework that uses Contrastive Gradient Identification to distinguish and selectively combine parameter regions that carry reasoning vs. domain capabilities, mitigating interference between them during merging.", "result": "Across several domains (safety, biomedicine, finance), merging a reasoning model with domain-specific models using ReasonAny yields models that exhibit both strong chain-of-thought reasoning and strong domain performance, clearly surpassing existing state-of-the-art merging baselines that suffer from collapse in one or both aspects.", "conclusion": "ReasonAny demonstrates that careful, gradient-based identification of reasoning-related parameter regions enables effective training-free fusion of reasoning and domain expertise. This challenges common assumptions about where reasoning capabilities reside in parameter space and offers a practical recipe for building \"Reasoning + X\" models across domains without sacrificing either capability."}}
{"id": "2601.05570", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05570", "abs": "https://arxiv.org/abs/2601.05570", "authors": ["Cooper Lin", "Maohao Ran", "Yanting Zhang", "Zhenglin Wan", "Hongwei Fan", "Yibo Xu", "Yike Guo", "Wei Xue", "Jun Song"], "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models", "comment": null, "summary": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.", "AI": {"tldr": "The paper introduces Crisis-Bench, a benchmark to evaluate how well LLMs manage information strategically (including withholding) in simulated corporate crises, revealing a tension between safety alignment and professional reputation management needs.", "motivation": "Standard safety alignment makes LLMs universally honest and transparent, which can conflict with real-world professional domains like PR and crisis management where strategic ambiguity and partial disclosure are required. The authors want to quantify and study this gap between general safety and domain-specific utility.", "method": "They design Crisis-Bench, a multi-agent POMDP-based 7-day corporate crisis simulation across 80 scenarios in 8 industries. An LLM-driven PR Agent must manage separate private vs public narrative states under enforced information asymmetry. Evaluation uses an Adjudicator-Market Loop that converts adjudicated public sentiment into a simulated stock price, reflecting economic consequences of the PR Agent\u2019s decisions.", "result": "When tested on Crisis-Bench, some models over-prioritize ethical transparency and reveal too much, harming simulated stock price, while others are capable of more Machiavellian but legitimate strategic withholding that better preserves firm value. This exposes a clear trade-off between conventional safety alignment and high-level PR performance.", "conclusion": "Crisis-Bench offers a first quantitative framework for measuring LLM reputation-management skill under crisis. The authors argue that safety alignment should evolve from rigid, universal moral rules toward context-aware professional alignment that allows strategic, domain-appropriate behavior while remaining within legitimate bounds."}}
{"id": "2601.05582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05582", "abs": "https://arxiv.org/abs/2601.05582", "authors": ["Sung-Yoo Lim", "Koki Sato", "Kiyoshi Takami", "Giancarlos Parady", "Eui-Jin Kim"], "title": "Can large language models interpret unstructured chat data on dynamic group decision-making processes? Evidence on joint destination choice", "comment": "23 pages, 9 figures", "summary": "Social activities result from complex joint activity-travel decisions between group members. While observing the decision-making process of these activities is difficult via traditional travel surveys, the advent of new types of data, such as unstructured chat data, can help shed some light on these complex processes. However, interpreting these decision-making processes requires inferring both explicit and implicit factors. This typically involves the labor-intensive task of manually annotating dialogues to capture context-dependent meanings shaped by the social and cultural norms. This study evaluates the potential of Large Language Models (LLMs) to automate and complement human annotation in interpreting decision-making processes from group chats, using data on joint eating-out activities in Japan as a case study. We designed a prompting framework inspired by the knowledge acquisition process, which sequentially extracts key decision-making factors, including the group-level restaurant choice set and outcome, individual preferences of each alternative, and the specific attributes driving those preferences. This structured process guides the LLM to interpret group chat data, converting unstructured dialogues into structured tabular data describing decision-making factors. To evaluate LLM-driven outputs, we conduct a quantitative analysis using a human-annotated ground truth dataset and a qualitative error analysis to examine model limitations. Results show that while the LLM reliably captures explicit decision-making factors, it struggles to identify nuanced implicit factors that human annotators readily identified. We pinpoint specific contexts when LLM-based extraction can be trusted versus when human oversight remains essential. These findings highlight both the potential and limitations of LLM-based analysis for incorporating non-traditional data sources on social activities.", "AI": {"tldr": "The paper tests whether large language models can automatically turn unstructured group chat logs about going out to eat into structured data about how groups make restaurant decisions, and compares the LLM\u2019s performance to human annotations.", "motivation": "Understanding how people jointly decide on social activities, like choosing a restaurant, is important for travel behavior research, but traditional surveys cannot easily capture the rich, context\u2011dependent dynamics seen in real conversations. New data sources such as chat logs offer this detail but are hard to analyze because they require time\u2011consuming manual annotation to interpret both explicit and implicit decision factors. The authors want to see if LLMs can reduce this manual effort while still reliably capturing key aspects of the decision process.", "method": "The authors use group chat data related to joint eating\u2011out activities in Japan. They design a prompting framework that mimics a stepwise knowledge acquisition process: the LLM is first asked to extract the group\u2011level choice set and final restaurant outcome, then to infer each individual\u2019s preferences for the alternatives, and finally to identify the restaurant attributes driving those preferences. This transforms unstructured dialogues into structured tabular data of decision\u2011making factors. They evaluate LLM outputs quantitatively against a human\u2011annotated ground truth dataset and conduct qualitative error analysis to understand where and why the LLM fails, focusing on explicit vs. implicit decision cues and context\u2011dependent meanings.", "result": "The LLM performs well at extracting clearly stated, explicit decision\u2011making factors, such as overt mentions of restaurant options and explicit likes or dislikes. However, it often fails to detect more nuanced, implicit factors that human annotators identify, such as preferences implied by subtle social cues, cultural norms, or indirect language. The authors are able to characterize the types of contexts and decision elements where the LLM\u2019s extraction is reliable and where it is not.", "conclusion": "LLMs can effectively automate part of the annotation process for group decision\u2011making in chat data by reliably extracting explicit factors and converting unstructured conversations into structured data. However, they are limited in capturing implicit, context\u2011rich aspects of decisions, so human oversight remains necessary for nuanced interpretation. The study underscores both the promise and the current boundaries of LLM\u2011based analysis for leveraging non\u2011traditional behavioral data sources like social chat logs in research on social and travel activities."}}
{"id": "2601.05578", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.05578", "abs": "https://arxiv.org/abs/2601.05578", "authors": ["Cooper Lin", "Yanting Zhang", "Maohao Ran", "Wei Xue", "Hongwei Fan", "Yibo Xu", "Zhenglin Wan", "Sirui Han", "Yike Guo", "Jun Song"], "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection", "comment": null, "summary": "E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.", "AI": {"tldr": "The paper applies reinforcement learning to post-train lightweight language models on raw e-commerce transaction text to improve fraud detection performance.", "motivation": "Existing fraud detection in e-commerce relies heavily on conventional machine learning and engineered features, while the practical use of large language models on real-world, domain-specific transaction data is underexplored and unvalidated. There is a need to leverage LLMs\u2019 capacity to capture complex textual patterns for fraud detection without extensive feature engineering.", "method": "The authors fine-tune lightweight language models with reinforcement learning using the Group Sequence Policy Optimization (GSPO) algorithm and a rule-based reward system. The models are trained directly on raw textual transaction data (customer information, shipping details, product descriptions, order histories) from a real-world payment solution provider, encouraging exploration of diverse fraud-related signals.", "result": "Post-trained language models show substantial improvements in F1-score on held-out test data compared to baseline methods, indicating enhanced fraud detection effectiveness. Analysis suggests that the gains stem from the models learning novel fraud indicators that go beyond traditional engineered features.", "conclusion": "Reinforcement learning-based post-training of language models on raw transaction text is an effective approach for e-commerce fraud detection. The exploration mechanism of RL enables models to uncover new fraud patterns and signals that conventional feature engineering misses, demonstrating the practical value of LLMs in real-world financial fraud detection tasks."}}
{"id": "2601.05589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05589", "abs": "https://arxiv.org/abs/2601.05589", "authors": ["Jiawei Shen", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Yue Cui", "Qingyu Niu", "Guoqing Ma", "Yidan Liang", "Jingjiang Liu", "Yiling Wang", "Shimin Di", "Jiajie Xu"], "title": "ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \\textbf{contextual inertia} and \\textbf{state drift}. To address these challenges, we propose the \\textbf{A}daptive \\textbf{C}ontext \\textbf{R}efactoring \\textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.", "AI": {"tldr": "Proposes an Adaptive Context Refactoring (ACR) framework to improve multi-turn dialogue in LLMs by dynamically managing conversation history to reduce context-related errors and token usage.", "motivation": "LLMs in multi-turn dialogue struggle with maintaining alignment with earlier turns, handling long-range dependencies, and avoiding factual drift as conversations lengthen. Existing solutions like larger context windows, external memory, and context compression still suffer from contextual inertia (over-reliance on stale context) and state drift (gradual deviation from correct state). There is a need for a principled way to manage dialogue context that actively mitigates these issues rather than just extending or compressing it.", "method": "Introduce the Adaptive Context Refactoring (ACR) framework that decouples context management from reasoning. ACR uses (1) a library of context refactoring operators that can reshape the interaction history (e.g., summarize, reorganize, or correct prior context) and (2) a teacher-guided self-evolving training paradigm that learns policies for when to intervene in the dialogue and which refactoring operators to apply. The system dynamically monitors the evolving conversation and applies appropriate operators to maintain a clean, relevant, and accurate working context for the base LLM.", "result": "In extensive experiments on multi-turn dialogue benchmarks, ACR achieves significantly better performance than existing baselines that rely on larger context windows, external memory, or simple compression. It also reduces token consumption, indicating improved efficiency in how context is represented and used.", "conclusion": "Dynamically refactoring dialogue context via a learned framework like ACR is more effective than simply enlarging or compressing context windows. By actively mitigating contextual inertia and state drift and by decoupling context management from reasoning, ACR improves both the quality and efficiency of LLM-based multi-turn dialogue and offers a general paradigm for better context handling in long interactions."}}
{"id": "2601.05590", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05590", "abs": "https://arxiv.org/abs/2601.05590", "authors": ["Haoming Gong", "Qingyao Ai", "Zhihao Tao", "Yongfeng Zhang"], "title": "A Causal Information-Flow Framework for Unbiased Learning-to-Rank", "comment": null, "summary": "In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the\n  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.", "AI": {"tldr": "They propose a causal, information-theoretic framework to learn unbiased ranking models from biased click data, handling multiple bias types jointly and improving ranking performance.", "motivation": "Click data used to train web search and recommendation ranking models is strongly biased by position, selection, and trust effects. Existing unbiased learning-to-rank approaches mostly address position bias via propensity estimation, cannot easily quantify residual bias, lack risk guarantees, and struggle to treat multiple interacting biases in a unified way. There is a need for a framework that can both model the causal process that generates clicks and provide a quantitative measure and control of how bias contaminates learned relevance estimates.", "method": "They build a causal learning-based ranking framework grounded in Structural Causal Models (SCMs) to explicitly model how clicks arise from relevance and various biases. Using this SCM, they identify the causal, unbiased relevance signal from observed click data. They then employ an information-theoretic quantity, conditional mutual information, as a measure of how much bias information \u201cleaks\u201d into the learned relevance estimates. This leakage becomes a disentanglement metric and is used as a regularization term during training to penalize bias. Additionally, they adopt a doubly robust causal inference estimator to obtain more reliable and theoretically sound risk (loss) estimates when learning the ranking model.", "result": "On standard learning-to-rank benchmark datasets, their approach consistently reduces the measured bias leakage according to the proposed conditional mutual information metric. Empirically, this leads to better ranking performance compared to baselines, with particularly strong gains in more realistic settings where multiple biases such as position bias and trust bias interact and confound conventional unbiased learning-to-rank methods.", "conclusion": "By combining structural causal modeling with information-theoretic disentanglement and doubly robust estimation, the proposed framework extends traditional unbiased learning-to-rank. It can jointly handle multiple click biases, quantify and reduce how much these biases contaminate relevance estimation, and delivers more accurate and reliable ranking models in practice."}}
{"id": "2601.05609", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05609", "abs": "https://arxiv.org/abs/2601.05609", "authors": ["Nguyen Minh Phuong", "Ha-Thanh Nguyen", "May Myo Zin", "Ken Satoh"], "title": "Data Augmented Pipeline for Legal Information Extraction and Reasoning", "comment": "Accepted in the Demonstration Track at ICAIL 2025", "summary": "In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.", "AI": {"tldr": "A simple, LLM-based data augmentation pipeline to reduce manual annotation and improve robustness for legal-domain information extraction, with potential to generalize to other NLP tasks.", "motivation": "Information Extraction in the legal domain requires extensive manual annotation, which is costly and time-consuming, and existing systems may lack robustness due to limited labeled data. There is a need for a scalable way to generate more diverse, high-quality training data with less human effort.", "method": "Design a pipeline that uses Large Language Models to automatically generate augmented training data for Information Extraction tasks on legal texts. The pipeline likely takes existing annotated examples as prompts, instructs the LLM to create new, varied instances that preserve labels, and then filters or post-processes the generated data. The approach is formulated to be task-agnostic so it can be transferred to other NLP problems.", "result": "The LLM-based augmentation pipeline substantially reduces the amount of manual annotation needed while improving the robustness (e.g., generalization and stability) of legal-domain Information Extraction systems, compared with baselines that rely on limited human-labeled data alone.", "conclusion": "LLM-driven data augmentation is an effective, simple, and general approach for strengthening Information Extraction systems in the legal domain and can be extended to other NLP tasks, offering a practical way to cut annotation costs and enhance model robustness."}}
{"id": "2601.05629", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05629", "abs": "https://arxiv.org/abs/2601.05629", "authors": ["Jiapu Wang", "Xinghe Cheng", "Zezheng Wu", "Ruiqi Ma", "Rui Wang", "Zhichao Yan", "Haoran Luo", "Yuhao Jiang", "Kai Sun"], "title": "Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion", "comment": null, "summary": "Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.", "AI": {"tldr": "The paper proposes CPSR, a new inductive knowledge graph completion framework that jointly leverages structural and semantic information, masks noisy structure, scores paths semantically, and achieves state-of-the-art performance.", "motivation": "Existing inductive KGC methods can handle emerging entities and relations but struggle with noisy structural information and fail to capture long-range dependencies along reasoning paths, limiting performance in dynamic, evolving knowledge graphs.", "method": "CPSR (Cumulative Path-Level Semantic Reasoning) integrates structural and semantic reasoning for inductive KGC. It uses a query-dependent masking module to adaptively filter out noisy structural information and preserve information most relevant to the query target. It also includes a global semantic scoring module that evaluates node contributions both individually and cumulatively along reasoning paths, enabling better long-range dependency modeling.", "result": "Experiments on standard inductive KGC benchmarks show that CPSR outperforms prior inductive KGC methods, achieving state-of-the-art performance metrics.", "conclusion": "By jointly modeling structural and semantic information, adaptively masking noisy structure, and performing cumulative path-level semantic scoring, CPSR effectively addresses key weaknesses of existing inductive KGC methods and advances the state of the art in inductive knowledge graph completion."}}
{"id": "2601.05624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05624", "abs": "https://arxiv.org/abs/2601.05624", "authors": ["Abayomi O. Agbeyangi"], "title": "Text Detoxification in isiXhosa and Yor\u00f9b\u00e1: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages", "comment": "26 pages, 9 figures and 1 algorithm", "summary": "Toxic language is one of the major barrier to safe online participation, yet robust mitigation tools are scarce for African languages. This study addresses this critical gap by investigating automatic text detoxification (toxic to neutral rewriting) for two low-resource African languages, isiXhosa and Yor\u00f9b\u00e1. The work contributes a novel, pragmatic hybrid methodology: a lightweight, interpretable TF-IDF and Logistic Regression model for transparent toxicity detection, and a controlled lexicon- and token-guided rewriting component. A parallel corpus of toxic to neutral rewrites, which captures idiomatic usage, diacritics, and code switching, was developed to train and evaluate the model. The detection component achieved stratified K-fold accuracies of 61-72% (isiXhosa) and 72-86% (Yor\u00f9b\u00e1), with per-language ROC-AUCs up to 0.88. The rewriting component successfully detoxified all detected toxic sentences while preserving 100% of non-toxic sentences. These results demonstrate that scalable, interpretable machine learning detectors combined with rule-based edits offer a competitive and resource-efficient solution for culturally adaptive safety tooling, setting a new benchmark for low-resource Text Style Transfer (TST) in African languages.", "AI": {"tldr": "The paper presents a hybrid, interpretable approach to detect and rewrite toxic text into neutral text for two low-resource African languages, isiXhosa and Yor\u00f9b\u00e1, achieving strong detection and perfect preservation of non-toxic sentences.", "motivation": "There is a shortage of robust tools for mitigating toxic language in African languages, which creates barriers to safe online participation for speakers of these languages. Existing text detoxification and safety tools focus mostly on high-resource languages, leaving isiXhosa and Yor\u00f9b\u00e1 underserved and limiting culturally adapted safety solutions.", "method": "The authors design a hybrid system with two main components: (1) a transparent toxicity detection model using TF-IDF features and Logistic Regression, allowing interpretability and efficiency; and (2) a rule-based, controlled rewriting module guided by a lexicon and token-level rules to convert toxic sentences into neutral ones. They also construct a parallel corpus of toxic-to-neutral rewrites for isiXhosa and Yor\u00f9b\u00e1 that accounts for idioms, diacritics, and code-switching, and use stratified K-fold cross-validation to evaluate the detection model with metrics including accuracy and ROC-AUC.", "result": "For toxicity detection, the model achieves stratified K-fold accuracies of 61\u201372% for isiXhosa and 72\u201386% for Yor\u00f9b\u00e1, with ROC-AUC scores up to 0.88, indicating good discriminative capacity. The rewriting component successfully detoxifies all sentences that were detected as toxic while preserving 100% of non-toxic sentences, meaning it introduces no false positives at the rewriting stage in the reported experiments.", "conclusion": "Interpretable, lightweight machine learning models combined with rule-based rewriting can provide effective and resource-efficient detoxification for low-resource African languages. The approach sets a new benchmark for text style transfer (toxic-to-neutral rewriting) in isiXhosa and Yor\u00f9b\u00e1 and suggests a scalable direction for culturally adaptive online safety tools in other low-resource settings."}}
{"id": "2601.05637", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.05637", "abs": "https://arxiv.org/abs/2601.05637", "authors": ["Emily Cheng", "Carmen Amo Alonso", "Federico Danieli", "Arno Blaas", "Luca Zappella", "Pau Rodriguez", "Xavier Suau"], "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models", "comment": null, "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.", "AI": {"tldr": "The paper introduces a theoretical and empirical framework to measure how controllable generative models actually are, showing that controllability is limited and fragile.", "motivation": "Despite many techniques for controlled generation (prompting, fine-tuning, etc.), we lack a principled way to answer whether a given generative model can, in fact, be controlled to achieve desired outputs. Current practice assumes controllability without quantifying its limits, which is problematic as generative models are deployed widely in high-stakes and interactive settings like dialogue.", "method": "The authors model human\u2013model interaction as a control process and define a notion of a \"controllable set\"\u2014the set of states or outputs that can be reached via interaction. They propose an algorithm to estimate these controllable sets in a dialogue setting, treating the generative model as a black-box nonlinear control system. They derive distribution-free PAC-style bounds on the estimation error that only assume bounded outputs, giving formal guarantees on the quality of the controllable-set estimates. They then instantiate this framework empirically on dialogue control tasks for both language and text-to-image models.", "result": "The framework yields provable bounds on how accurately one can estimate what parts of the model\u2019s behavior are reachable under control, as a function of sample complexity. Empirically, applying the method to multiple dialogue tasks and generative modalities shows that the actual controllable regions of model behavior can be surprisingly small, and that measured controllability changes significantly depending on the specific experimental setup and task formulation.", "conclusion": "Controllability of generative models cannot be taken for granted; it is fragile and highly context-dependent. The proposed framework and algorithm provide a principled way to quantify and analyze controllable sets, with theoretical guarantees. This suggests that future work on controlled generation should first assess fundamental controllability limits of a model-task pair before designing or evaluating control strategies."}}
{"id": "2601.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05633", "abs": "https://arxiv.org/abs/2601.05633", "authors": ["Nuoyan Lyu", "Bingbing Xu", "Weihao Meng", "Yige Yuan", "Yang Zhang", "Zhiyong Huang", "Tat-Seng Chua", "Huawei Shen"], "title": "GIFT: Games as Informal Training for Generalizable LLMs", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.", "AI": {"tldr": "The paper proposes using games as environments for informal learning in large language models, and introduces a Nested Training Framework that composes tasks sequentially with an explicit AND objective, improving multi-task reinforcement learning and generalization across benchmarks.", "motivation": "LLMs excel at formal, instruction-based tasks (e.g., math, coding) but underperform in practical wisdom, strategic creativity, and social reasoning. The authors argue this gap stems from insufficient informal learning, which in humans is driven by interactive feedback and open-ended environments. Existing multi-task RL approaches for LLMs also suffer from performance degradation due to task interference and naive task mixing. The paper aims to provide a structured way to give LLMs informal learning experiences and to mitigate multi-task interference so that models develop more human-like, generalizable abilities.", "method": "The authors propose using games as primary environments for LLM informal learning, exploiting their intrinsic rewards and structured-yet-abstract complexity. They introduce a Nested Training Framework that replaces naive task mixing (optimizing an implicit OR objective) with sequential task composition enforcing an explicit AND objective: the model must master multiple abilities concurrently to maximize rewards. They implement this using GRPO-based reinforcement learning across multiple games (Matrix Games, TicTacToe, Who's the Spy), where tasks are combined in a nested fashion rather than simply interleaved. Training encourages the model to integrate competencies needed for each game, reducing interference.", "result": "Experiments show that game-based informal learning within the Nested Training Framework avoids the performance degradation seen in conventional multi-task setups. Models trained this way exhibit improved competence across the individual games and demonstrate stronger generalization on broader ability-oriented benchmarks beyond the training tasks, suggesting that the learned skills transfer to other domains. Compared to naive multi-task training, the nested approach yields higher rewards and more robust performance across tasks.", "conclusion": "Treating games as environments for informal learning and organizing them via a Nested Training Framework allows LLMs to acquire more general, human-like abilities such as strategic and social reasoning. By turning multi-task optimization into an explicit AND objective, the approach mitigates task interference and improves generalization. The results indicate that informal, game-based RL can be a powerful complement to traditional, instruction-focused training for developing more broadly capable LLMs, and the authors release their framework and implementation to support further research."}}
{"id": "2601.05656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05656", "abs": "https://arxiv.org/abs/2601.05656", "authors": ["Rongxin Chen", "Tianyu Wu", "Bingbing Xu", "Xiucheng Xu", "Huawei Shen"], "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation", "comment": null, "summary": "High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.", "AI": {"tldr": "The paper proposes HAG, a hierarchical agent generation framework that creates topic-adaptive, high-fidelity agent populations by aligning macro-level distributions and micro-level individual consistency, and shows it outperforms existing methods on a new benchmark and evaluation framework.", "motivation": "Credible agent-based models require agent populations that accurately reflect real-world macro-level distributions (e.g., demographics, attributes across a population) while also maintaining micro-level rational and realistic individual personas. Existing methods either retrieve from static datasets, which cannot adapt to new topics or contexts not represented in the data, or rely purely on LLM generation, which loses awareness of overall population statistics and yields inconsistencies between agent attributes and reality. There is also no standardized evaluation framework tailored for this initialization problem, making it difficult to compare and improve methods.", "method": "The authors model population generation as a two-stage hierarchical decision process called HAG (Hierarchical Agent Generation). In the first stage, they use a World Knowledge Model to infer hierarchical conditional probability distributions over population attributes and structure them into a Topic-Adaptive Tree, ensuring that the global, macro-level attribute distributions match topic-specific real-world patterns. In the second stage, they ground the generation with real-world data, instantiate individual agents from the tree, and apply agentic augmentation to refine the personas, thereby enforcing micro-level internal consistency and realism. They also design a multi-domain benchmark and a comprehensive PACE evaluation framework specifically for assessing the quality of generated populations along multiple dimensions, including population alignment and sociological consistency.", "result": "In extensive experiments across multiple domains, HAG is compared with representative static retrieval-based and LLM-based generation baselines. The results show that HAG substantially reduces population alignment errors by an average of 37.7% relative to baselines, indicating better macro-level distribution matching. It also improves sociological consistency metrics by 18.8%, demonstrating more realistic and coherent micro-level agent personas. These gains are observed consistently across the new benchmark scenarios evaluated with the proposed PACE framework.", "conclusion": "The paper concludes that hierarchical, topic-adaptive population generation is both feasible and effective when grounded in a World Knowledge Model and real-world data. HAG successfully bridges the gap between macro-level statistical alignment and micro-level agent rationality, outperforming prior approaches that focus on only one of these aspects. The introduced multi-domain benchmark and PACE evaluation framework provide a standardized way to assess and compare future methods in high-fidelity agent initialization, paving the way for more credible and realistic agent-based simulations across diverse application domains."}}
{"id": "2601.05641", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05641", "abs": "https://arxiv.org/abs/2601.05641", "authors": ["Alireza Dehghanpour Farashah", "Aditi Khandelwal", "Marylou Fauchard", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "title": "Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs", "comment": null, "summary": "As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.", "AI": {"tldr": "The paper investigates how to reliably remove specific data or concepts from a multilingual language model and analyzes how unlearning transfers across ten languages.", "motivation": "Most machine unlearning work focuses on English-only models, but real-world language models are multilingual and exhibit cross-lingual transfer and biases. There is a need to understand how unlearning behaves in multilingual settings, including how removing knowledge or harmful stereotypes in one language affects others, particularly across different language families and resource levels.", "method": "The authors use the Aya-Expanse 8B multilingual model and study two unlearning scenarios: (1) data unlearning, where particular training data are removed, and (2) concept unlearning, where certain factual knowledge or stereotypical associations are erased. They build and extend benchmarks for factual knowledge and stereotypes via translation into ten languages spanning five language families and variable resource levels. They then conduct experiments to measure stability of unlearning and cross-lingual transfer effects, and correlate these with linguistic distance metrics, including syntactic similarity.", "result": "Unlearning procedures are more stable and reliable in high-resource languages than in low-resource ones. The authors find asymmetric transfer of unlearning effects between typologically related languages, meaning that unlearning in one language can affect others unevenly. They also show that syntactic similarity between languages is the strongest predictor of how unlearning behavior transfers across languages.", "conclusion": "Multilingual unlearning is substantially shaped by language resources, typological relationships, and especially syntactic similarity. Designing effective and predictable unlearning for multilingual LLMs requires accounting for these linguistic factors and cannot simply rely on insights from monolingual (English-focused) settings."}}
{"id": "2601.05675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05675", "abs": "https://arxiv.org/abs/2601.05675", "authors": ["Bingyi Liu", "Jinbo He", "Haiyong Shi", "Enshu Wang", "Weizhen Han", "Jingxiang Hao", "Peixi Wang", "Zhuangzhuang Zhang"], "title": "CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space", "comment": "Accepted by AAAI 2026", "summary": "Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \\textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\\%$ in success rate.", "AI": {"tldr": "They propose Cooperative Hybrid Diffusion Policies (CHDP), a diffusion-based reinforcement learning framework for hybrid discrete-continuous action spaces, using cooperating discrete and continuous diffusion policies with codebook embeddings and Q-guided training, achieving notable performance gains on benchmarks.", "motivation": "Hybrid action spaces that mix discrete choices and continuous parameters are common in applications like robotics and games, but existing RL methods struggle with expressive modeling of such actions and do not scale well in high-dimensional discrete spaces. There is a need for a more expressive and scalable policy representation that can capture complex dependencies between discrete and continuous parts of actions.", "method": "They cast hybrid action selection as a fully cooperative game between two agents: one controls discrete actions with a diffusion policy, and the other controls continuous parameters with another diffusion policy. The continuous diffusion policy is explicitly conditioned on the representation of the chosen discrete action to model their dependency. To avoid conflicts in learning, they introduce a sequential policy update scheme so that each agent can adapt to the other. For scalability in large discrete spaces, they build a codebook that embeds discrete actions into a low-dimensional latent space, and the discrete diffusion policy operates in this compact latent space. A Q-function-based guidance mechanism is used to align the codebook embeddings with the discrete policy\u2019s learned representation during training, ensuring that the learned latent structure is value-aware.", "result": "On challenging hybrid-action benchmarks, their CHDP framework achieves up to 19.3% higher success rate compared to state-of-the-art methods, demonstrating improved performance and scalability in complex hybrid control tasks.", "conclusion": "Modeling hybrid discrete-continuous action selection as cooperation between specialized diffusion policies enables expressive and scalable control. By conditioning continuous actions on discrete representations, using sequential updates to encourage co-adaptation, and learning a Q-guided codebook for high-dimensional discrete actions, CHDP significantly improves performance on hybrid-action benchmarks and offers a promising direction for complex control problems with hybrid action spaces."}}
{"id": "2601.05654", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05654", "abs": "https://arxiv.org/abs/2601.05654", "authors": ["Sejun Park", "Yoonah Park", "Jongwon Lim", "Yohan Jo"], "title": "A Framework for Personalized Persuasiveness Prediction via Context-Aware User Profiling", "comment": null, "summary": "Estimating the persuasiveness of messages is critical in various applications, from recommender systems to safety assessment of LLMs. While it is imperative to consider the target persuadee's characteristics, such as their values, experiences, and reasoning styles, there is currently no established systematic framework to optimize leveraging a persuadee's past activities (e.g., conversations) to the benefit of a persuasiveness prediction model. To address this problem, we propose a context-aware user profiling framework with two trainable components: a query generator that generates optimal queries to retrieve persuasion-relevant records from a user's history, and a profiler that summarizes these records into a profile to effectively inform the persuasiveness prediction model. Our evaluation on the ChangeMyView Reddit dataset shows consistent improvements over existing methods across multiple predictor models, with gains of up to +13.77%p in F1 score. Further analysis shows that effective user profiles are context-dependent and predictor-specific, rather than relying on static attributes or surface-level similarity. Together, these results highlight the importance of task-oriented, context-dependent user profiling for personalized persuasiveness prediction.", "AI": {"tldr": "They build a system that uses a person\u2019s past conversations to better predict how persuasive a new message will be for that person.", "motivation": "Persuasiveness prediction is important in applications like recommendation and LLM safety, but current models largely ignore the specific characteristics and history of the target user, or use them in an ad\u2011hoc, non\u2011systematic way. There is no framework that optimally leverages a user\u2019s past activities (e.g., conversations) when estimating how persuasive a message will be for that individual.", "method": "They introduce a context\u2011aware user profiling framework with two trainable modules: (1) a query generator that, given the current persuasion context, learns to construct optimal queries for retrieving persuasion\u2011relevant records from a user\u2019s interaction history; and (2) a profiler that takes these retrieved records and summarizes them into a user profile tailored to the current prediction task. This profile is then fed into a downstream persuasiveness prediction model. They evaluate the framework on the ChangeMyView Reddit dataset and compare against existing user modeling and baselines across different predictor architectures.", "result": "Across multiple persuasiveness predictor models, their framework consistently outperforms prior methods, achieving up to a +13.77 percentage\u2011point improvement in F1 score. Analyses show that the most effective user profiles are not static or based solely on surface similarity between the current message and historical posts, but instead depend on the specific context and the particular predictor model in use.", "conclusion": "Task\u2011oriented, context\u2011dependent user profiling\u2014driven by learned query generation and profiling over a user\u2019s history\u2014is crucial for accurate, personalized persuasiveness prediction. Static attributes or naive similarity-based profiles are insufficient; instead, profiles need to be dynamically tailored to both the prediction context and the model consuming them."}}
{"id": "2601.05693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05693", "abs": "https://arxiv.org/abs/2601.05693", "authors": ["Zenghao Duan", "Liang Pang", "Zihao Wei", "Wenbin Duan", "Yuxin Tian", "Shicheng Xu", "Jingcheng Deng", "Zhiyi Yin", "Xueqi Cheng"], "title": "Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models", "comment": null, "summary": "Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.", "AI": {"tldr": "The paper identifies and analyzes a new failure mode in Large Reasoning Models called circular reasoning loops, introduces a benchmark to study them, explains their mechanism via attention patterns, and proposes an early detection method using CUSUM.", "motivation": "Large Reasoning Models, despite benefiting from test-time scaling and long-chain reasoning, often get stuck in repetitive loops that waste computation and cause inference failures. Existing notions of degeneration do not fully explain these logical, self-referential loops, and there is a lack of systematic benchmarks and mechanistic understanding to detect or mitigate them.", "method": "The authors define and formalize a specific failure mode named Circular Reasoning, construct LoopBench with two loop types (numerical loops and statement loops), and perform mechanistic analysis of model behavior and attention patterns. They characterize loops as state collapse where semantic repetition precedes literal text repetition, identify reasoning impasses as triggers, and describe a V-shaped self-reinforcing attention pattern. Based on the observed precursors, they apply the Cumulative Sum (CUSUM) change-detection algorithm to detect early loop onset across multiple LRMs.", "result": "LoopBench successfully elicits and categorizes circular reasoning loops in LRMs. Analysis shows clear boundaries of state collapse, with semantic repetition emerging before explicit textual repetition, and a consistent V-shaped attention pattern maintaining the loop. The CUSUM-based detector accurately predicts loop onset early across diverse LRMs, providing insight into the stability and fragility of long-chain reasoning.", "conclusion": "Circular reasoning loops constitute a distinct, systematic failure mode in LRMs, driven by reasoning impasses and maintained by self-reinforcing attention dynamics. LoopBench enables structured study of these loops, and CUSUM-based monitoring provides an effective way to predict and potentially mitigate looping, offering a path toward more reliable long-chain reasoning in large models."}}
{"id": "2601.05657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05657", "abs": "https://arxiv.org/abs/2601.05657", "authors": ["Hao Yang", "Hongyuan Lu", "Dingkang Yang", "Wenliang Yang", "Peng Sun", "Xiaochuan Zhang", "Jun Xiao", "Kefan He", "Wai Lam", "Yang Liu", "Xinhua Zeng"], "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat", "comment": "13 pages", "summary": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.", "AI": {"tldr": "The paper introduces Stephanie2, a dialogue agent that generates multi-message chats with more natural timing through explicit send/wait decisions and latency modeling, outperforming a prior system in naturalness and engagement.", "motivation": "Existing step-by-step AI chat systems simply split a single long response into multiple short messages without an explicit mechanism to decide when to send or wait, resulting in unnatural pacing and lack of realistic human-like message timing. There is a need for a dialogue agent that can better mimic how humans pause, think, and type in instant messaging conversations.", "method": "The authors design Stephanie2, a step-wise decision-making dialogue agent that, at each step, chooses whether to send a message or wait. The system explicitly models latency as the sum of thinking time and typing time, enabling active waiting and adaptive message pacing. They also construct a time-window-based dual-agent dialogue framework to automatically generate pseudo dialogue histories, which are used for both human and automatic evaluation of conversational quality and timing behavior.", "result": "Experimental evaluations show that Stephanie2 substantially improves over the earlier Stephanie1 system on metrics including perceived naturalness and user engagement. In a human evaluation based on a role-identification Turing test, Stephanie2 achieves a higher pass rate, indicating that its chatting behavior is more human-like.", "conclusion": "By incorporating explicit send/wait decisions and a latency model that captures both thinking and typing times, Stephanie2 produces instant-messaging style conversations with more realistic pacing. The proposed dual-agent time-window evaluation setup supports systematic assessment, and results indicate that Stephanie2 advances the state of the art in natural, engaging step-wise AI chat compared to its predecessor."}}
{"id": "2601.05705", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.05705", "abs": "https://arxiv.org/abs/2601.05705", "authors": ["Ali Farjami", "Luca Redondi", "Marco Valentino"], "title": "Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning", "comment": "Work in progress", "summary": "Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.", "AI": {"tldr": "The paper introduces a logic-parametric framework that combines large language models with theorem provers for verifiable natural language inference, showing that treating logic as a controllable component improves robustness and domain-specific performance.", "motivation": "Existing neuro-symbolic NLI systems combine LLMs and theorem provers but typically rely on a single, fixed logical formalism. This rigidity limits their robustness, adaptability across domains, and ability to capture different kinds of reasoning (e.g., commonsense vs. ethical). The authors are motivated to create a framework where the logical layer itself can be varied, inspected, and optimized for specific reasoning tasks, especially in normative reasoning, where the choice of logic critically affects conclusions.", "method": "They use the LogiKEy methodology to embed multiple classical and non-classical logics into a common higher-order logic (HOL) meta-framework. Within this setting, they build a neuro-symbolic NLI pipeline where LLMs generate or guide formal representations and theorem provers perform verification. They then systematically vary the underlying logic (e.g., first-order, deontic, modal) and compare logic-external encodings of norms (via axioms) with logic-internal encodings (where normative behavior is built into the logic). They evaluate across domains, measuring inference quality, explanation quality, and proof efficiency.", "result": "Experiments show that when normative reasoning is encoded internally in the logic (logic-internal approach), the system achieves more accurate NLI judgments and more efficient proofs than when norms are encoded as external axioms. They also find that the best-performing logic depends on the domain: first-order logic is better for commonsense NLI tasks, while deontic and modal logics perform better in ethical or norm-governed scenarios. Hybrid proofs combining LLM-generated content and theorem prover checks are shorter and more reliable under logic-internal strategies.", "conclusion": "Making the choice of logic a parametric, first-class component of neuro-symbolic NLI architectures leads to more robust, modular, and adaptable reasoning systems. Logic-internal treatments of normative reasoning are particularly effective, and no single logic is universally optimal\u2014different domains benefit from different logics. The approach provides a principled way to tune the logical layer to task requirements while maintaining verifiable, explainable inference."}}
{"id": "2601.05699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05699", "abs": "https://arxiv.org/abs/2601.05699", "authors": ["Atnafu Lambebo Tonja", "Srija Anand", "Emilio Villa-Cueva", "Israel Abebe Azime", "Jesujoba Oluwadara Alabi", "Muhidin A. Mohamed", "Debela Desalegn Yadeta", "Negasi Haile Abadi", "Abigail Oppong", "Nnaemeka Casmir Obiefuna", "Idris Abdulmumin", "Naome A Etori", "Eric Peter Wairagala", "Kanda Patrick Tshinu", "Imanigirimbabazi Emmanuel", "Gabofetswe Malema", "Alham Fikri Aji", "David Ifeoluwa Adelani", "Thamar Solorio"], "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages", "comment": null, "summary": "Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)", "AI": {"tldr": "Afri-MCQA is a new benchmark of 7.5k culturally grounded Q&A pairs in 15 African languages (plus English, text and speech) showing that current LLMs perform very poorly on these languages and settings.", "motivation": "Despite Africa having over a third of the world\u2019s languages, African languages and cultures are severely underrepresented in AI benchmarks and model training. Existing evaluations rarely test cultural knowledge, multimodality, or speech in African contexts, and they typically focus on English or a few high\u2011resource languages. The authors want to fill this gap with a resource that can systematically measure both cultural competence and linguistic/speech capabilities of models in African languages.", "method": "The authors construct Afri-MCQA, a multilingual cultural question\u2011answering benchmark with 7.5k question\u2011answer pairs spanning 15 African languages from 12 countries, plus parallel English versions. Data collection is done entirely by native speakers, and content is grounded in local cultural knowledge. The benchmark includes both text and speech modalities and supports both multiple\u2011choice and open\u2011ended VQA\u2011style settings. They then evaluate several open\u2011weight large language models on the benchmark, designing additional control experiments to separate linguistic competence (ability to handle the language) from cultural competence (knowledge of local culture).", "result": "Open\u2011weight LLMs perform poorly on Afri-MCQA across all evaluated cultures and languages, especially on open\u2011ended visual question answering in native African languages or speech, where accuracy is near zero. Control experiments reveal substantial performance gaps between English and the corresponding African languages, in both text and speech, indicating weak linguistic competence and limited transfer from English to African languages.", "conclusion": "Afri-MCQA exposes serious shortcomings of current LLMs in African languages and cultures, particularly for multimodal and speech-based tasks. The results highlight the necessity of speech\u2011first modeling strategies, culturally grounded pretraining data, and better cross\u2011lingual transfer mechanisms. By releasing Afri-MCQA under a permissive academic/CC BY\u2011NC 4.0 license on HuggingFace, the authors aim to catalyze more inclusive, culturally aware multimodal AI research for African languages."}}
{"id": "2601.05724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05724", "abs": "https://arxiv.org/abs/2601.05724", "authors": ["Yuxuan Zhou", "Fei Huang", "Heng Li", "Fengyi Wu", "Tianyu Wang", "Jianwei Zhang", "Junyang Lin", "Zhi-Qi Cheng"], "title": "Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding", "comment": null, "summary": "Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.", "AI": {"tldr": "The paper proposes Hierarchical Speculative Decoding (HSD), a new sequence-level verification method that increases the number of accepted tokens in speculative decoding without changing the original model\u2019s output distribution, achieving state-of-the-art efficiency when integrated into existing frameworks like EAGLE-3.", "motivation": "Speculative decoding accelerates language model inference by drafting multiple tokens and then verifying them with a stronger model, but verification is a bottleneck. Token-wise verification is conservative and leads to fewer accepted tokens, while existing sequence-level methods either use approximations or incomplete information, making joint verification intractable and limiting performance. There is a need for a principled, tractable, and provably lossless sequence-level verification method that maximizes accepted tokens while preserving distribution fidelity.", "method": "The authors introduce Hierarchical Speculative Decoding (HSD), a sequence-level verification scheme that organizes candidate continuations into a hierarchy (or tree) of branches. It performs verification in a way that redistributes excess and deficient probability mass across reachable branches, ensuring that the verified distribution exactly matches the target model\u2019s distribution (lossless). This hierarchical balancing over accessible branches addresses the joint intractability normally present in sequence-level verification and increases the expected number of accepted tokens. HSD is designed as a general verification layer that can be plugged into various speculative decoding frameworks.", "result": "Large-scale experiments across multiple model families and benchmarks show that HSD consistently increases token acceptance rates compared with prior speculative decoding verification strategies. When integrated into the EAGLE-3 framework, HSD yields more than a 12% performance gain in decoding efficiency while preserving distribution fidelity, achieving state-of-the-art efficiency among speculative decoding methods evaluated.", "conclusion": "HSD offers a principled, provably lossless solution to sequence-level verification in speculative decoding, resolving the joint intractability issue while significantly increasing token acceptance. Its explainable, hierarchical design and general applicability make it easy to integrate into existing speculative decoding frameworks, where it delivers substantial efficiency improvements without sacrificing faithfulness to the target model\u2019s distribution."}}
{"id": "2601.05707", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05707", "abs": "https://arxiv.org/abs/2601.05707", "authors": ["Zhaolin Li", "Jan Niehues"], "title": "Multimodal In-context Learning for ASR of Low-resource Languages", "comment": "Under review", "summary": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.", "AI": {"tldr": "The paper explores using multimodal in-context learning (MICL) with speech-enabled large language models to improve ASR for unseen, low-resource languages, showing gains without target-language training data.", "motivation": "Most ASR systems cover only a small portion of the world\u2019s languages because they rely on large supervised corpora. While text-only in-context learning with LLMs can help low-resource scenarios, existing studies mostly focus on high-resource, training-covered languages and ignore speech. The authors want to know whether speech-capable LLMs can quickly adapt to completely unseen, endangered languages using only prompts that mix audio and text, and whether this can practically improve ASR quality without collecting new labeled data.", "method": "The authors evaluate two speech LLMs, Phi-4 and Qwen3-Omni, on three typologically diverse endangered languages. They apply multimodal in-context learning (MICL) by giving the models prompts that include both speech and corresponding text in various languages, including cross-lingual examples, and then test performance on unseen target languages. They inspect attention patterns across layers to understand how models use audio vs. text context. For ASR, they compare direct prompt-based recognition to a hybrid system where an external acoustic model generates candidate transcriptions and the speech LLM, guided by MICL, selects or rescoring the best hypothesis. They also test cross-lingual transfer, where MICL uses data from other languages but not the target language itself.", "result": "MICL enables both speech LLMs to handle unseen endangered languages better than text-only or naive approaches, successfully leveraging information across speech and text modalities. Cross-lingual transfer further improves MICL efficiency: models can adapt to target languages without any target-language training, yet still reach or surpass the performance of language models trained on target-language corpora. Attention analysis reveals that different layers show varying reliance on audio versus text, with an overall preference for text context. Direct prompt-based ASR with speech LLMs performs poorly on unseen languages, but the proposed hybrid ASR pipeline\u2014combining a strong acoustic model with MICL-based selection of acoustic hypotheses\u2014consistently improves ASR metrics.", "conclusion": "Speech LLMs can effectively learn and support ASR in completely unseen, endangered languages through multimodal in-context learning, especially when aided by cross-lingual transfer. While direct prompt-based ASR is inadequate, integrating a robust acoustic model with MICL-guided hypothesis selection yields consistent performance gains. MICL can match or outperform corpus-trained language models without using any target-language data, suggesting a practical, data-efficient path toward ASR for low-resource and endangered languages."}}
{"id": "2601.05739", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05739", "abs": "https://arxiv.org/abs/2601.05739", "authors": ["G M Shahariar", "Zabir Al Nazi", "Md Olid Hasan Bhuiyan", "Zhouxing Shi"], "title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.", "AI": {"tldr": "The paper introduces PII-VisBench, a benchmark to test how vision-language models leak or refuse to leak personal data depending on how visible a person is online.", "motivation": "Existing privacy evaluations for vision-language models mostly treat PII leakage as a simple, static extraction problem and ignore that people differ greatly in how much information about them is publicly available online. Since many real-world applications involve individuals with varying online footprints, there is a need to understand whether models behave differently\u2014i.e., are more or less privacy-preserving\u2014based on a subject\u2019s online visibility, and to reveal systematic safety weaknesses that depend on this factor.", "method": "The authors build PII-VisBench, a benchmark with 4000 PII-focused prompts targeting 200 real or synthetic subjects. These subjects are stratified into four visibility levels\u2014high, medium, low, and zero\u2014based on the amount and type of information available about them online. They evaluate 18 open-source vision-language models (ranging from 0.3B to 32B parameters) using two metrics: (1) Refusal Rate, the proportion of PII-related prompts that the model refuses to answer, and (2) Conditional PII Disclosure Rate, the proportion of non-refusal responses that contain PII, as flagged by an automatic or rule-based detector. They also test paraphrased and jailbreak-style prompts to probe robustness and model-dependent vulnerabilities.", "result": "The study finds a clear trend across models: as subject visibility decreases, models refuse more PII requests and disclose less PII. Quantitatively, the conditional PII disclosure rate drops from around 9.10% for high-visibility subjects to about 5.34% for low-visibility subjects. Models are therefore more likely to disclose PII for subjects with a larger online footprint. The authors also observe substantial heterogeneity across model families and differences in how various types of PII are handled. Furthermore, paraphrased and jailbreak-style prompts can significantly reduce refusals and increase successful PII extraction, revealing attack- and model-specific failure modes.", "conclusion": "PII-VisBench reveals that current vision-language models are not uniformly privacy-preserving: their safety behavior depends on the online visibility of the subject, with higher-visibility individuals facing greater risk of PII disclosure. This indicates that privacy evaluations and training procedures need to be visibility-aware, and that current safety safeguards can be circumvented via prompt paraphrasing and jailbreak techniques. The work motivates developing more robust, subject-agnostic privacy alignment and using benchmarks like PII-VisBench to systematically stress-test VLMs in privacy-critical settings."}}
{"id": "2601.05713", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05713", "abs": "https://arxiv.org/abs/2601.05713", "authors": ["Thomas Fabian"], "title": "Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging", "comment": null, "summary": "Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.", "AI": {"tldr": "The paper introduces a new visualization and analysis tool that applies diffusion tensor imaging (DTI) techniques to word embeddings in large language models to reveal how information flows through entire natural language expressions across model layers.", "motivation": "Current interpretability methods for large language models largely focus on static word embeddings and point-plot visualizations that compare relative positions of isolated words. These methods ignore the role of context and sequential structure in natural language, and thus cannot adequately explain how models process complete expressions or perform complex tasks. There is a need for tools that capture and visualize dynamic information flow within and across layers, enabling deeper understanding of model structure, usage of layers, and task-specific processing patterns.", "method": "The authors adapt diffusion tensor imaging (DTI)\u2014a technique from neuroimaging used to map directional flow of information\u2014to the space of word embeddings in large language models. They use DTI-style analysis to infer and visualize how information flows between word embeddings within sequences and across layers. By tracking these flows through the model, they construct a representation of the directional and structural patterns of information propagation for different inputs and tasks.", "result": "The DTI-based approach reveals structured information flow patterns between embeddings that are not visible in standard point-plot visualizations. It enables comparison of architectures and identification of under-utilized layers that may be candidates for pruning. The method also uncovers distinct information flow signatures for different NLP tasks such as pronoun resolution and metaphor detection, demonstrating task-dependent processing differences inside the model.", "conclusion": "The proposed DTI-inspired framework offers a novel and more fine-grained way to analyze how large language models represent and process full natural language expressions, going beyond isolated word-level comparisons. It improves interpretability by making information flow visible, supports structural analyses like pruning, and highlights task-specific internal dynamics, thereby contributing a new tool for understanding and optimizing NLP models."}}
{"id": "2601.05746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05746", "abs": "https://arxiv.org/abs/2601.05746", "authors": ["Zhenghao Li", "Zhi Zheng", "Wei Chen", "Jielun Zhao", "Yong Chen", "Tong Xu", "Enhong Chen"], "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation", "comment": "16pages,6figures", "summary": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.", "AI": {"tldr": "The paper proposes DynaDebate, a Dynamic Multi-Agent Debate framework for Large Language Model-based multi-agent systems, designed to avoid agents converging on identical reasoning paths and improve collaborative reasoning performance.", "motivation": "Existing Large Language Model-based Multi-Agent Systems and Multi-Agent Debate frameworks often initialize agents in an unguided and homogeneous manner. This causes them to follow similar reasoning paths, replicate the same errors, and reduces debate to ineffective majority voting over similar outputs rather than genuine logical scrutiny. The authors aim to overcome these limitations by fostering diverse reasoning, process-level critique, and objective resolution of disagreements to achieve more reliable and accurate decisions.", "method": "The authors design DynaDebate with three main components: (1) Dynamic Path Generation and Allocation, where a specialized Path Generation Agent creates multiple diverse, logically coherent solution paths with adaptive redundancy and assigns them to debating agents; (2) Process-Centric Debate, which reorients the debate from final-answer comparison to detailed step-by-step logic critique, ensuring each reasoning step is examined and corrected collaboratively; (3) a Trigger-Based Verification Agent, which is invoked when disagreements arise and leverages external tools to objectively check claims and break deadlocks in the debate.", "result": "Through extensive experiments on various benchmarks, DynaDebate consistently outperforms prior state-of-the-art Multi-Agent Debate methods, demonstrating higher accuracy and more effective collaborative reasoning across tasks.", "conclusion": "DynaDebate effectively addresses the problem of homogeneous reasoning and ineffective majority voting in existing multi-agent debate systems. By introducing dynamic path generation, process-focused critique, and tool-augmented verification upon disagreement, it strengthens the reasoning diversity and reliability of Large Language Model-based Multi-Agent Systems, achieving superior performance on multiple benchmarks."}}
{"id": "2601.05751", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05751", "abs": "https://arxiv.org/abs/2601.05751", "authors": ["Amalie Brogaard Pauli", "Maria Barrett", "Max M\u00fcller-Eberstein", "Isabelle Augenstein", "Ira Assent"], "title": "Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns", "comment": null, "summary": "Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.", "AI": {"tldr": "The paper studies how large language models generate persuasive messages and how this changes with recipient gender, sender intent, and output language, revealing systematic gender-related biases.", "motivation": "As LLMs are increasingly used to draft interpersonal, persuasive messages, it becomes crucial to understand whether and how these systems adapt their persuasive language depending on characteristics like the recipient\u2019s gender, the sender\u2019s goal, and the language used, as such adaptations may encode and amplify harmful social biases.", "method": "The authors design a framework using pairwise prompts that systematically vary recipient gender, sender intent, and output language across 13 LLMs and 16 natural languages. They then use an LLM-as-judge approach, grounded in constructs from social psychology and communication science, to annotate each generated message along 19 predefined categories of persuasive language, enabling quantitative comparison of how persuasion strategies shift across conditions.", "result": "Across all evaluated models, the analysis uncovers statistically significant differences in persuasive language depending on the recipient\u2019s gender, with models exhibiting patterns that align with known gender-stereotypical linguistic styles and tendencies from sociolinguistics and social psychology. These effects hold across different models and languages, indicating that the observed gender-related variation is systematic rather than idiosyncratic.", "conclusion": "The study concludes that contemporary LLMs do not generate neutral persuasive language: instead, their persuasive strategies systematically vary by recipient gender and mirror established gender stereotypes in human communication. This highlights the need for more careful auditing, mitigation, and design of LLMs used in interpersonal and persuasive contexts to avoid reinforcing or amplifying gender biases across languages and applications."}}
{"id": "2601.05787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05787", "abs": "https://arxiv.org/abs/2601.05787", "authors": ["Zezhou Wang", "Ziyun Zhang", "Xiaoyi Zhang", "Zhuzhong Qian", "Yan Lu"], "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation", "comment": "Work In Progress", "summary": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git", "AI": {"tldr": "The paper proposes BEPA, a bi-level method to better leverage limited expert trajectories for training end-to-end vision-language computer-use agents via reinforcement learning from verifiable rewards, significantly improving success rates on GUI benchmarks like OSWorld-Verified.", "motivation": "Existing GUI benchmarks like OSWorld-Verified provide only a small number of interactive, verifiable tasks and require costly expert trajectories collected through real interaction, which are difficult to scale. Framework-based CUAs that separate planning and execution currently outperform end-to-end screenshot-to-action policies, but are more complex to deploy. When trying to use reinforcement learning from verifiable rewards with a small pool of expert trajectories, naive mixing of off-policy expert traces into on-policy RL training is brittle due to structural mismatches and distribution shifts between expert data and the learner\u2019s policy. The authors aim to find a way to better exploit scarce expert trajectories to improve end-to-end CUAs.", "method": "The authors introduce BEPA (Bi-Level Expert-to-Policy Assimilation), a method that converts static expert trajectories into policy-aligned guidance for RL from verifiable rewards. BEPA operates at two levels: (LEVEL-1) it generates self-rolled, reachable trajectories under the current base policy to align available expert data with what the policy can actually reach; and (LEVEL-2) it maintains a per-task, dynamically updated cache that stores this aligned guidance and is used during RLVR to stabilize and improve learning. This bi-level process mitigates distribution shift and structural mismatches between expert traces and the learner\u2019s on-policy data, enabling more effective use of limited expert demonstrations in an end-to-end screenshot-to-action training setup.", "result": "On the OSWorld-Verified benchmark, applying BEPA to the UITARS1.5-7B model increases success rate from 22.87% to 32.13%, and improves performance on a held-out split from 5.74% to 10.30%. The approach also shows consistent gains on other GUI benchmarks, including MMBench-GUI and Online-Mind2Web, indicating that BEPA generalizes beyond a single dataset and improves the robustness and effectiveness of end-to-end computer-use agents trained with RLVR.", "conclusion": "BEPA effectively leverages a small pool of expert trajectories by transforming them into policy-aligned guidance through a bi-level process, making reinforcement learning from verifiable rewards more stable and sample-efficient for end-to-end computer-use agents. This narrows the performance gap between simple, deployable screenshot-to-action policies and more complex framework-based CUAs, while providing a scalable way to train GUI agents despite limited, hard-to-collect expert data. The reported benchmark improvements suggest BEPA is a promising direction for training robust, generalizable vision-language CUAs in desktop and browser environments."}}
{"id": "2601.05752", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05752", "abs": "https://arxiv.org/abs/2601.05752", "authors": ["Shu Yang", "Jingyu Hu", "Tong Li", "Hanqi Yan", "Wenxuan Wang", "Di Wang"], "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor", "comment": null, "summary": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.", "AI": {"tldr": "AutoMonitor-Bench is a benchmark to systematically test how reliably LLM-based safety monitors detect misbehavior without over-blocking benign content.", "motivation": "LLM-based safety monitors are increasingly used to detect and block misbehavior, but their reliability across tasks, failure modes, and models is poorly understood and hard to compare objectively. There is also a safety-utility trade-off: stricter monitors may catch more harms but mistakenly block more harmless content. A standardized benchmark is needed to quantify these issues and guide better monitor design and training.", "method": "The authors build AutoMonitor-Bench, a dataset of 3,010 annotated test samples covering question answering, code generation, and reasoning, each with paired misbehavior and benign variants. They define two metrics\u2014Miss Rate (MR) for undetected misbehavior and False Alarm Rate (FAR) for benign content mistakenly flagged\u2014to evaluate monitors. They test 22 LLMs (12 proprietary, 10 open source) on this benchmark. To study how training affects monitor reliability, they also curate a large-scale corpus of 153,581 misbehavior-related samples and fine-tune Qwen3-4B-Instruction as a monitor, then evaluate its generalization to unseen, more implicit misbehaviors.", "result": "Across the 22 evaluated models, they find large differences in monitoring performance and a clear trade-off between MR and FAR: reducing misses usually increases false alarms and vice versa, reflecting a fundamental tension between safety and usability. Even with extensive training on a large misbehavior corpus, the fine-tuned Qwen3-4B monitor shows limited ability to robustly generalize to subtler, unseen misbehaviors, indicating that naive scaling of training data is insufficient for reliable monitoring.", "conclusion": "AutoMonitor-Bench exposes significant reliability gaps in current LLM-based misbehavior monitors and an inherent safety-utility trade-off captured by MR and FAR. The benchmark shows that existing monitors are inconsistent across tasks and failure types, and simply training on large, known misbehavior datasets does not guarantee robustness to more implicit harms. The authors argue that future work should pursue more task-aware monitor architectures and training strategies, using benchmarks like AutoMonitor-Bench to systematically evaluate and improve monitoring reliability at scale."}}
{"id": "2601.05890", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05890", "abs": "https://arxiv.org/abs/2601.05890", "authors": ["Ruizhe Zhang", "Xinke Jiang", "Zhibang Yang", "Zhixin Zhang", "Jiaran Gao", "Yuzhen Xiao", "Hongbin Lai", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management", "comment": null, "summary": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.", "AI": {"tldr": "StackPlanner is a hierarchical multi-agent framework that adds explicit memory management and reusable coordination experience to LLM-based multi-agent systems, improving long-horizon collaboration.", "motivation": "Centralized LLM-based multi-agent systems struggle with long-horizon tasks because they lack proper memory management, causing context bloat, error accumulation, and poor generalization across tasks. There is a need to both manage task-level memory more efficiently and reuse prior coordination experience between agents.", "method": "The paper proposes StackPlanner, a hierarchical multi-agent framework that separates high-level coordination from low-level subtask execution. It introduces active task-level memory control to avoid context bloat and a structured experience memory that stores reusable coordination patterns. Reinforcement learning is used to learn when and how to retrieve and exploit these stored coordination experiences during new tasks.", "result": "Across several deep-search and multi-agent system benchmarks, StackPlanner outperforms baselines by enabling more reliable and efficient long-horizon collaboration between agents. The experiments show better task performance and robustness on complex, knowledge-intensive tasks.", "conclusion": "Explicit memory control and reusable coordination experience are key to stabilizing long-horizon collaboration in LLM-based multi-agent systems. By decoupling coordination from execution and learning to manage and reuse memory, StackPlanner improves reliability and generalization on complex tasks."}}
{"id": "2601.05776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05776", "abs": "https://arxiv.org/abs/2601.05776", "authors": ["Benedikt Ebing", "Lennart Keller", "Goran Glava\u0161"], "title": "One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models", "comment": null, "summary": "Exposing latent lexical overlap, script romanization has emerged as an effective strategy for improving cross-lingual transfer (XLT) in multilingual language models (mLMs). Most prior work, however, focused on setups that favor romanization the most: (1) transfer from high-resource Latin-script to low-resource non-Latin-script languages and/or (2) between genealogically closely related languages with different scripts. It thus remains unclear whether romanization is a good representation choice for pretraining general-purpose mLMs, or, more precisely, if information loss associated with romanization harms performance for high-resource languages. We address this gap by pretraining encoder LMs from scratch on both romanized and original texts for six typologically diverse high-resource languages, investigating two potential sources of degradation: (i) loss of script-specific information and (ii) negative cross-lingual interference from increased vocabulary overlap. Using two romanizers with different fidelity profiles, we observe negligible performance loss for languages with segmental scripts, whereas languages with morphosyllabic scripts (Chinese and Japanese) suffer degradation that higher-fidelity romanization mitigates but cannot fully recover. Importantly, comparing monolingual LMs with their mLM counterpart, we find no evidence that increased subword overlap induces negative interference. We further show that romanization improves encoding efficiency (i.e., fertility) for segmental scripts at a negligible performance cost.", "AI": {"tldr": "The paper studies whether romanizing text (converting non-Latin scripts to Latin script) is a good representation choice for pretraining multilingual language models, especially for high-resource languages.", "motivation": "Previous work showed romanization helps cross-lingual transfer mainly in very favorable settings (Latin \u2192 non-Latin, closely related languages with different scripts). It is unclear if romanization is still beneficial or at least not harmful when building general-purpose multilingual models, and whether it causes information loss or harmful interference, particularly for high-resource languages.", "method": "The authors pretrain encoder language models from scratch on both original-script and romanized text for six typologically diverse high-resource languages. They use two romanization systems with different fidelity levels and compare monolingual vs multilingual settings. They analyze performance differences to isolate two hypothesized degradation sources: loss of script-specific information and negative cross-lingual interference from increased subword overlap. They also evaluate encoding efficiency (fertility).", "result": "Languages using segmental scripts show negligible performance loss after romanization, while morphosyllabic-script languages (Chinese and Japanese) show notable performance degradation. Higher-fidelity romanization reduces but does not eliminate this degradation. Multilingual models do not show evidence that increased subword overlap from romanization leads to negative cross-lingual interference. Romanization improves tokenization efficiency for segmental scripts with minimal performance costs.", "conclusion": "Romanization is generally safe for segmental-script high-resource languages, offering better encoding efficiency with little performance loss, but it is problematic for morphosyllabic scripts like Chinese and Japanese, where even high-fidelity romanization cannot fully preserve performance. Concerns about negative cross-lingual interference from increased subword-level overlap are not empirically supported in their experiments."}}
{"id": "2601.05899", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05899", "abs": "https://arxiv.org/abs/2601.05899", "authors": ["Dawei Wang", "Chengming Zhou", "Di Zhao", "Xinyuan Liu", "Marci Chi Ma", "Gary Ushaw", "Richard Davison"], "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents", "comment": "AAAI 2026 Oral", "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).", "AI": {"tldr": "Introduces TowerMind, a lightweight, multimodal tower-defense RTS environment to benchmark LLM planning, decision-making, and hallucination, revealing large gaps vs. humans and limitations in current models.", "motivation": "LLMs are increasingly used as agents requiring long-term planning and decision-making, but existing RTS environments for evaluation are either too computationally heavy or lack textual observations, limiting their suitability for LLM benchmarking.", "method": "Design a new RTS-style tower defense environment, TowerMind, with low compute requirements and multimodal observations (pixel, text, structured state), plus customization and hallucination evaluation; create five benchmark levels and test popular LLMs under different input modalities, as well as two RL algorithms (Ape-X DQN, PPO).", "result": "Across the benchmarks, LLMs perform significantly worse than human experts in both task performance and hallucination; analysis shows specific behavioral issues such as poor plan validation, limited ability to achieve multiple goals (lack of multifinality), and inefficient action usage. RL baselines are also evaluated for comparison.", "conclusion": "TowerMind provides a practical, flexible RTS tower-defense environment that preserves the planning and decision-making challenges of RTS games while being lightweight and multimodal, thereby enriching the RTS-based evaluation ecosystem and offering a new benchmark for AI agents; source code is released for the community."}}
{"id": "2601.05794", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05794", "abs": "https://arxiv.org/abs/2601.05794", "authors": ["Eilam Cohen", "Itamar Bul", "Danielle Inbar", "Omri Loewenbach"], "title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs", "comment": null, "summary": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.", "AI": {"tldr": "Comparative study of fine-tuning vs prompt engineering for text simplification with encoder-decoder LLMs, finding fine-tuning better for structural simplification and human preference, while prompting preserves semantics but often copies input.", "motivation": "To understand the practical tradeoffs between fine-tuning and prompt engineering for text simplification using large language models, given that both are widely used but systematically compared less often.", "method": "Conduct a comparative evaluation of encoder-decoder LLMs on multiple text simplification benchmarks, applying both fine-tuning and prompt-based approaches. Use a range of automatic evaluation metrics and run human evaluation to assess structural simplification and semantic similarity.", "result": "Fine-tuned models consistently achieve stronger structural simplification. Prompted models typically achieve higher semantic similarity scores but tend to copy the input text. Human evaluations show an overall preference for the outputs of fine-tuned models.", "conclusion": "For text simplification with encoder-decoder LLMs, fine-tuning is generally superior for structural simplification and overall human preference, while prompting is better at preserving semantics but risks excessive copying. The released code, data, checkpoints, and prompts support reproducibility and future research."}}
{"id": "2601.05991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05991", "abs": "https://arxiv.org/abs/2601.05991", "authors": ["Jiayu Ding", "Haoran Tang", "Ge Li"], "title": "Open-Vocabulary 3D Instruction Ambiguity Detection", "comment": null, "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.", "AI": {"tldr": "Defines a new task and benchmark (Ambi3D) for detecting whether natural-language instructions are ambiguous in 3D scenes, and proposes a two-stage method (AmbiVer) that outperforms existing 3D LLMs.", "motivation": "In safety-critical embodied AI settings (e.g., surgery, robotics), ambiguous language commands can lead to dangerous mistakes. Existing embodied AI systems largely assume instructions are unambiguous and focus on execution rather than first checking whether the instruction is clear. There is a lack of explicit problem formulation, benchmarks, and methods for detecting ambiguity of instructions in complex 3D environments.", "method": "1) Formally define the task of Open-Vocabulary 3D Instruction Ambiguity Detection, where a model must decide if a given natural-language command refers to a unique target in a 3D scene. 2) Construct Ambi3D, a large-scale benchmark with 700+ 3D scenes and ~22k instructions with varying levels and types of ambiguity. 3) Analyze state-of-the-art 3D LLMs on this benchmark, showing they struggle with ambiguity detection. 4) Propose AmbiVer, a two-stage framework: (a) actively collects explicit visual evidence by rendering or selecting multiple views of the scene, (b) feeds this evidence to a vision-language model to judge whether the instruction has a unique interpretation or multiple plausible referents. 5) Evaluate AmbiVer and baselines extensively on Ambi3D.", "result": "Empirical results on Ambi3D show that existing 3D LLMs perform poorly at deciding if instructions are ambiguous, confirming the difficulty and novelty of the task. AmbiVer, the proposed two-stage framework, substantially improves ambiguity detection accuracy over these baselines across diverse scenes and instruction types.", "conclusion": "Instruction ambiguity in 3D embodied settings is a critical but previously underexplored safety problem. The paper introduces a formal task, a large benchmark (Ambi3D), and a new method (AmbiVer) that significantly improves the detection of ambiguous commands in 3D scenes. This work highlights limitations of current 3D LLMs and takes an important step toward safer, more reliable embodied AI systems by prompting them to detect and potentially clarify ambiguous instructions before execution."}}
{"id": "2601.05808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05808", "abs": "https://arxiv.org/abs/2601.05808", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "comment": "Working in progress", "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "AI": {"tldr": "EnvScaler automatically synthesizes scalable tool-interaction environments for training LLM agents, greatly boosting their multi-tool, multi-turn problem-solving abilities.", "motivation": "Training LLMs as agents requires rich, diverse, and reliable tool-interaction environments. Real systems are often inaccessible due to security and cost; LLM-simulated environments can hallucinate and be inconsistent; and hand-crafted sandboxes are labor-intensive and not scalable. A systematic, automated way to generate large numbers of realistic, checkable environments and tasks is needed.", "method": "EnvScaler programmatically synthesizes tool-interaction environments in two stages. (1) SkelBuilder mines topics and models logical structures to build diverse environment skeletons, then filters them via automatic quality evaluation. (2) ScenGenerator instantiates each skeleton with multiple concrete task scenarios and constructs rule-based trajectory validation functions that can automatically check whether an agent\u2019s action sequence is valid or successful. These synthesized environments and scenarios are then used to train LLMs (Qwen3 series) via Supervised Fine-Tuning and Reinforcement Learning.", "result": "Using EnvScaler, the authors automatically generate 191 distinct environments and around 7,000 task scenarios. Applying these to SFT and RL training for Qwen3 models yields substantial performance gains on three external benchmarks, especially on tasks that require complex, multi-step, multi-tool interactions.", "conclusion": "Automated, programmatic synthesis of tool-interaction environments is an effective and scalable way to train LLM agents. EnvScaler produces a large, diverse set of validated environments and tasks, leading to significant improvements in LLM performance on complex tool-use benchmarks. The released code and data enable broader adoption and extension of this environment generation approach."}}
{"id": "2601.05821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05821", "abs": "https://arxiv.org/abs/2601.05821", "authors": ["Milad Alshomary", "Grace Li", "Anubhav Jangra", "Yufang Hou", "Kathleen McKeown", "Smaranda Muresan"], "title": "LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public", "comment": null, "summary": "The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose a framework for training LLMs to emulate the role of a science journalist that can be used by early-stage researchers to learn how to properly communicate their papers to the general public. We evaluate the usefulness of our trained LLM Journalists in leading conversations with both simulated and human researchers. %compared to the general-purpose ones. Our experiments indicate that LLMs trained using our framework ask more relevant questions that address the societal impact of research, prompting researchers to clarify and elaborate on their findings. In the user study, the majority of participants who interacted with our trained LLM Journalist appreciated it more than interacting with general-purpose LLMs.", "AI": {"tldr": "A framework is proposed to fine-tune LLMs to act as science journalists, helping early-stage researchers better communicate their work to the public.", "motivation": "Early-stage researchers often struggle to communicate their scientific findings and their societal relevance to non-expert audiences. While general-purpose LLMs can help, they are not specifically aligned to highlight public-facing aspects like impact, accessibility, and clear explanation. There is a need for tools that guide researchers through this communication process in a structured, dialog-based way.", "method": "The authors design and train LLMs to emulate science journalists. These LLM Journalists are optimized to lead conversations with researchers, focusing on asking questions that elicit clear explanations and emphasize societal impact. The framework is evaluated in interactions with both simulated researchers (likely using scripted or model-based agents) and real human researchers. Comparisons are made against general-purpose LLMs in terms of question relevance and conversation quality.", "result": "LLMs trained with the proposed framework generate more relevant questions, especially around societal impact and clarification of findings, than general-purpose LLMs. These questions prompt researchers to elaborate and refine how they present their work. In a user study, most participants preferred interacting with the specialized LLM Journalist over standard LLMs.", "conclusion": "Specialized training of LLMs to play the role of a science journalist can better support early-stage researchers in communicating their work to the public. By directing conversations toward impact and clarity, the trained LLMs provide more useful guidance than general-purpose models, and are perceived as more helpful by users."}}
{"id": "2601.05833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05833", "abs": "https://arxiv.org/abs/2601.05833", "authors": ["Liu Zai"], "title": "Peek2: A Regex-free implementation of pretokenizers for Byte-level BPE", "comment": "5 pages, 4 figures, for associated code, see https://github.com/omegacoleman/tokenizers_peek2", "summary": "Pretokenization is a crucial, sequential pass in Byte-level BPE tokenizers. Our proposed new implementation, Peek2, serves as a drop-in replacement for cl100k-like pretokenizers used in GPT-3, LLaMa-3, and Qwen-2.5. Designed with performance and safety in mind, Peek2 is Regex-free and delivers a $ 1.11\\times $ improvement in overall throughput across the entire Byte-level BPE encoding process. This algorithm runs entirely on the CPU, has stable linear complexity $ O(n) $, and provides presegmentation results identical to those of the original Regex-based pretokenizer.", "AI": {"tldr": "Peek2 is a new, Regex-free pretokenizer for byte-level BPE that speeds up encoding by about 11% while preserving identical segmentation and linear-time complexity.", "motivation": "Existing byte-level BPE pretokenizers, such as the cl100k-style ones used in GPT-3, LLaMa-3, and Qwen-2.5, rely on regular expressions. Regex-based implementations can be relatively slow, complex, and may have safety or robustness concerns, especially at scale. There is a need for a faster, safer, and simpler pretokenization step that can be used as a direct replacement without changing model behavior.", "method": "The authors design Peek2, a CPU-only, Regex-free algorithm for pretokenization in byte-level BPE tokenizers. The algorithm processes text sequentially with stable linear-time complexity O(n), implementing the same presegmentation rules as the prior Regex-based pretokenizers but using a custom deterministic finite procedure instead of regular expressions. Peek2 is engineered as a drop-in replacement for cl100k-like pretokenizers in major LLMs, focusing on cache-friendly, branch-efficient operations to maximize throughput.", "result": "Peek2 achieves a 1.11\u00d7 improvement in overall throughput for the entire byte-level BPE encoding pipeline compared to the original Regex-based pretokenizer, while producing presegmentation outputs that are bitwise identical to the baseline. It maintains linear-time behavior and runs entirely on CPU, demonstrating that Regex-free implementations can be both faster and fully compatible with existing tokenization schemes.", "conclusion": "The paper concludes that Regex-free, CPU-only pretokenizers like Peek2 can safely replace traditional Regex-based implementations in byte-level BPE tokenizers, yielding measurable performance gains (~11% higher throughput) without any change to downstream model behavior or tokenization outputs. This suggests that careful algorithmic and systems design in the pretokenization stage can yield worthwhile efficiency improvements in large-scale language model pipelines."}}
{"id": "2601.05835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05835", "abs": "https://arxiv.org/abs/2601.05835", "authors": ["Molly Kennedy", "Ali Parker", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation", "comment": null, "summary": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.", "AI": {"tldr": "The paper investigates how modern large language models exhibit and express political framing biases, finding a strong tendency toward centrist framing even when prompted otherwise.", "motivation": "As LLMs are increasingly used to summarize and generate journalistic text, there is concern that their subtle wording choices may introduce or reinforce political framing, potentially shaping public interpretation of news. However, it is unclear whether existing bias-detection methods align with how these models actually frame content in generated text.", "method": "The authors evaluate nine state-of-the-art LLMs. First, they perform few-shot classification of article ideology, comparing model predictions to labeled LEFT/CENTER/RIGHT ground truth. Second, they prompt the models to generate news summaries under different steering instructions (FAITHFUL, CENTRIST, LEFT, RIGHT). All generated summaries and article-level outputs are then scored using a single, fixed ideology evaluator model, enabling comparison between classification-based bias signals and framing behavior in generation.", "result": "They observe a pervasive 'ideological center-collapse,' where models\u2019 ratings and generated summaries systematically gravitate toward centrist framing, even when prompts steer them toward left or right. Grok 4 stands out as the most ideologically expressive generator, more willing or able to depart from the center in its summaries. For bias classification performance, Claude Sonnet 4.5 performs best among commercial models and Llama 3.1 among open-weight models, suggesting strong detection capabilities but still a tendency toward centrist expression in generation.", "conclusion": "Current LLMs, while capable of detecting ideological leanings, tend to collapse their own generated political framing toward the center, potentially masking or dampening ideological variation in journalistic summarization. This has important implications for how LLMs might homogenize political discourse and for how we evaluate and control model bias, highlighting the need to distinguish between bias detection and expressive framing behavior."}}
{"id": "2601.05847", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05847", "abs": "https://arxiv.org/abs/2601.05847", "authors": ["Rafael Brens", "Yuqiao Meng", "Luoxi Tang", "Zhaohan Xi"], "title": "Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs", "comment": null, "summary": "Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.", "AI": {"tldr": "The paper proposes an NLP pipeline to convert free-text EHR notes into interoperable, FHIR-compliant patient digital twins.", "motivation": "Digital twins in healthcare need standardized, interoperable data, but EHRs are mostly unstructured free text with heterogeneous documentation practices. There is a gap in robust methods to automatically convert these unstructured notes into standardized, computable formats like FHIR, limiting the practical deployment of patient digital twins.", "method": "The authors design a semantic NLP pipeline that processes free-text EHR notes. It uses named entity recognition to detect clinical concepts, concept normalization to map these entities to standard terminologies (SNOMED-CT, ICD-10), and relation extraction to identify links among conditions, medications, and observations. The resulting structured information is serialized into FHIR-compliant resources to instantiate patient digital twins. They evaluate the system on the MIMIC-IV Clinical Database Demo, validating against MIMIC-IV-on-FHIR reference mappings.", "result": "On MIMIC-IV demo data, the pipeline achieves high F1 scores for both entity recognition and relation extraction. Compared to baseline methods, the approach improves schema completeness (i.e., more FHIR fields correctly populated) and interoperability (i.e., higher alignment with reference FHIR mappings).", "conclusion": "A semantic NLP-based approach can effectively transform unstructured EHR notes into FHIR-compliant digital twin representations, enhancing interoperability and completeness over baseline approaches and thereby supporting the realization of patient digital twins in clinical settings."}}
{"id": "2601.05851", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05851", "abs": "https://arxiv.org/abs/2601.05851", "authors": ["Sandeep Mishra", "Devichand Budagam", "Anubhab Mandal", "Bishal Santra", "Pawan Goyal", "Manish Gupta"], "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs", "comment": "Accepted to EACL 2026 Industry Track, 12 pages, 6 figures", "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.", "AI": {"tldr": "This paper proposes Multimodal Auto-Completion (MAC), which predicts the next characters in live chats by combining partial text with visual context, and shows that multimodal models and a routing framework can improve completion quality and efficiency over traditional text-only methods.", "motivation": "Existing auto-completion systems mainly rely on text-only context, which is insufficient in settings like digital assistants, chatbots, design tools, and healthcare consultations where users and systems share visual information. There is a need to exploit this multimodal context to better infer user intent, reduce typing effort, and improve user satisfaction, while also managing the computational cost of vision-language models.", "method": "The authors define a new task, Multimodal Auto-Completion (MAC), where the system predicts upcoming characters based on partially typed text and associated visual context. They adapt two existing multimodal dialogue datasets, MMDialog and ImageChat, into benchmarks for this task. They evaluate state-of-the-art vision-language models (VLMs) and strong text-only baselines on these datasets. To address efficiency, they propose Router-Suggest, a routing framework that dynamically chooses between text-only models and VLMs depending on the dialogue context, and also design a lightweight variant for limited-resource settings.", "result": "Empirical evaluation shows that while VLMs improve completion quality and user satisfaction compared with strong text-only baselines, they are more computationally expensive. Router-Suggest successfully balances this trade-off, delivering between 2.3x and 10x speedups relative to the best-performing VLM while maintaining strong performance. A user study confirms that VLM-based completions substantially reduce user typing effort and enhance perceived completion quality in multi-turn dialogues.", "conclusion": "Multimodal context\u2014specifically visual information paired with text\u2014substantially benefits auto-completion in interactive systems. The proposed MAC task and benchmarks demonstrate that VLMs can capture user intent better than text-only models, improving user experience, though at a higher computational cost. The Router-Suggest framework mitigates this cost by selectively invoking VLMs when needed, enabling more efficient and user-aware multimodal auto-completion for real-time applications."}}
{"id": "2601.05858", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05858", "abs": "https://arxiv.org/abs/2601.05858", "authors": ["Alexandra Dragomir", "Florin Brad", "Radu Tudor Ionescu"], "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.", "AI": {"tldr": "They show that using a restart-based curriculum learning schedule during preference optimization improves multilingual machine translation of LLMs.", "motivation": "While LLMs already perform well in zero-shot multilingual MT and can be further improved via preference optimization, the impact of the training data order (curriculum) on these methods has been largely unexplored. The authors want to see whether smarter ordering of preference data can systematically boost MT quality and mitigate forgetting.", "method": "They integrate curriculum learning into several state-of-the-art preference optimization algorithms, designing a new strategy called CLewR (Curriculum Learning with Restarts). CLewR repeatedly cycles through an easy-to-hard curriculum multiple times during training, rather than a single pass, with the goal of counteracting catastrophic forgetting of easy examples. They test this across different LLM families (Gemma2, Qwen2.5, Llama3.1) and preference optimization setups for multilingual MT.", "result": "Their restart-based curriculum consistently improves multilingual MT performance over baselines without curriculum learning, across all evaluated model families and preference optimization methods.", "conclusion": "Curriculum scheduling\u2014specifically, repeatedly traversing an easy-to-hard data curriculum\u2014matters for preference optimization in multilingual MT and yields robust performance gains. CLewR is an effective and broadly applicable strategy, and the authors provide open-source code to facilitate further research and adoption."}}
{"id": "2601.05864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05864", "abs": "https://arxiv.org/abs/2601.05864", "authors": ["Jonathan Downie", "Joss Moorkens"], "title": "What do the metrics mean? A critical analysis of the use of Automated Evaluation Metrics in Interpreting", "comment": "25 pages", "summary": "With the growth of interpreting technologies, from remote interpreting and Computer-Aided Interpreting to automated speech translation and interpreting avatars, there is now a high demand for ways to quickly and efficiently measure the quality of any interpreting delivered. A range of approaches to fulfil the need for quick and efficient quality measurement have been proposed, each involving some measure of automation. This article examines these recently-proposed quality measurement methods and will discuss their suitability for measuring the quality of authentic interpreting practice, whether delivered by humans or machines, concluding that automatic metrics as currently proposed cannot take into account the communicative context and thus are not viable measures of the quality of any interpreting provision when used on their own. Across all attempts to measure or even categorise quality in Interpreting Studies, the contexts in which interpreting takes place have become fundamental to the final analysis.", "AI": {"tldr": "The paper critically reviews emerging automated metrics for interpreting quality and argues that, on their own, they are not yet valid for assessing real-world interpreting because they ignore communicative context.", "motivation": "Interpreting technologies are rapidly evolving (remote interpreting, CAI tools, speech translation, avatars), creating an urgent need for fast, scalable, and efficient ways to assess the quality of both human and machine-delivered interpreting. Existing traditional evaluation methods are often slow, resource-intensive, and not easily applicable at scale or to automated systems, prompting the exploration of semi- or fully-automatic quality metrics.", "method": "The paper conducts a critical overview and conceptual analysis of recently proposed automated or semi-automated methods for measuring interpreting quality. It compares their design assumptions and operationalisation of quality against the complex realities of authentic interpreting practice, with particular attention to how (or whether) they incorporate communicative context.", "result": "The analysis finds that current automatic metrics systematically fail to incorporate key contextual factors\u2014such as purpose of the interaction, participants\u2019 needs, situational norms, and communicative outcomes\u2014that are central to how quality is understood in Interpreting Studies. As a result, these metrics may provide partial or misleading indications of quality when used in isolation.", "conclusion": "The paper concludes that existing automatic metrics are not viable as standalone measures of interpreting quality for real-world practice, whether the interpreting is performed by humans or machines. Any serious assessment of interpreting must integrate contextual and communicative dimensions; automated metrics, if used at all, should be embedded within broader, context-sensitive evaluation frameworks rather than treated as definitive quality indicators."}}
{"id": "2601.05866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05866", "abs": "https://arxiv.org/abs/2601.05866", "authors": ["Maxime Dassen", "Rebecca Kotula", "Kenton Murray", "Andrew Yates", "Dawn Lawrie", "Efsun Kayi", "James Mayfield", "Kevin Duh"], "title": "FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG", "comment": "Accepted at ECIR 2026. 18 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.", "AI": {"tldr": "Introduces FACTUM, a mechanistic framework to measure and predict when RAG model citations are trustworthy, showing that correct citations have distinct, scale-dependent internal signatures and improving hallucination detection performance.", "motivation": "Citation hallucinations in RAG systems are a particularly harmful kind of hallucination because the model fabricates supporting sources, undermining trust. Prior explanations largely blame an undifferentiated over-reliance on parametric knowledge without dissecting the internal mechanisms or how they vary with model scale. The authors aim to open this black box and develop principled, mechanistic indicators of when a citation is likely trustworthy.", "method": "They propose FACTUM, which defines four mechanistic scores capturing (1) the contribution of the model\u2019s parametric knowledge, (2) the contribution of attention pathways, (3) the contribution of FFN pathways, and (4) the alignment or orthogonality between attention and FFN contributions. They analyze internal activations and attention patterns, including use of the attention sink for information synthesis, across different Llama model sizes within RAG setups, and derive predictive signatures of correct vs hallucinated citations from these scores.", "result": "They find two main signatures of correct citations: (a) a stronger contribution from parametric knowledge and (b) greater use of the attention sink for synthesizing information, but these signatures change with scale. For Llama-3.2-3B, correct citations correlate with higher alignment between attention and FFN pathways, while for Llama-3.1-8B, correct citations show lower alignment and more orthogonal contributions. Using these mechanistic signals, FACTUM predicts citation trustworthiness and outperforms prior hallucination-detection baselines by up to 37.5% AUC.", "conclusion": "Citation hallucinations stem from a nuanced, scale-dependent interplay between attention and FFN mechanisms rather than a simple over-reliance on parametric knowledge. FACTUM\u2019s mechanistic scores capture evolving signatures of trustworthy citations and significantly improve automatic detection of citation hallucinations, suggesting that more reliable RAG systems will require scale-aware, mechanism-level analysis rather than one-size-fits-all heuristics."}}
{"id": "2601.05874", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05874", "abs": "https://arxiv.org/abs/2601.05874", "authors": ["Santosh Srinath K", "Mudit Somani", "Varun Reddy Padala", "Prajna Devi Upadhyay", "Abhijit Das"], "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models", "comment": null, "summary": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.", "AI": {"tldr": "They propose a continual-learning approach using POS-based code-switching plus a replay adapter to prevent catastrophic forgetting when adapting large language models to small multilingual models, validated on VQA and language modeling tasks.", "motivation": "Adapting large language models to build small language models for low-resource, multilingual settings often causes catastrophic forgetting, where the model loses previously learned knowledge. The authors aim to mitigate this issue when transferring from LLMs to SLMs, especially in multilingual and vision-language scenarios.", "method": "They use a continual learning framework combining (1) parts-of-speech (POS) based code-switching to better mix languages in training data and (2) a replay adapter strategy, where adapters retain and replay prior knowledge during subsequent training. This architecture is applied while training small language models from large language models.", "result": "Experiments on vision-language tasks like visual question answering and on language modeling tasks show that their approach effectively reduces catastrophic forgetting and improves performance compared to baselines without this strategy.", "conclusion": "A continual learning approach with POS-based code-switching and replay adapters can successfully mitigate catastrophic forgetting when adapting LLMs to SLMs in multilingual and vision-language tasks, improving stability and performance of the resulting models."}}
{"id": "2601.05877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05877", "abs": "https://arxiv.org/abs/2601.05877", "authors": ["Meghana Sunil", "Manikandarajan Venmathimaran", "Muthu Subash Kavitha"], "title": "iReasoner: Trajectory-Aware Intrinsic Reasoning Supervision for Self-Evolving Large Multimodal Models", "comment": null, "summary": "Recent work shows that large multimodal models (LMMs) can self-improve from unlabeled data via self-play and intrinsic feedback. Yet existing self-evolving frameworks mainly reward final outcomes, leaving intermediate reasoning weakly constrained despite its importance for visually grounded decision making. We propose iReasoner, a self-evolving framework that improves an LMM's implicit reasoning by explicitly eliciting chain-of-thought (CoT) and rewarding its internal agreement. In a Proposer--Solver loop over unlabeled images, iReasoner augments outcome-level intrinsic rewards with a trajectory-aware signal defined over intermediate reasoning steps, providing learning signals that distinguish reasoning paths leading to the same answer without ground-truth labels or external judges. Starting from Qwen2.5-VL-7B, iReasoner yields up to $+2.1$ points across diverse multimodal reasoning benchmarks under fully unsupervised post-training. We hope this work serves as a starting point for reasoning-aware self-improvement in LMMs in purely unsupervised settings.", "AI": {"tldr": "iReasoner is a self-evolving framework that improves large multimodal models\u2019 reasoning by eliciting and rewarding consistent chain-of-thought on unlabeled images, achieving better multimodal reasoning performance without supervision.", "motivation": "Existing self-evolving large multimodal model frameworks mainly reward final outputs and ignore how the model reasons step-by-step. This leaves intermediate visual reasoning unconstrained and potentially brittle, even though such reasoning is crucial for robust, visually grounded decision making. The authors want a way to refine the model\u2019s internal reasoning process itself, not just its final answers, and to do this using only unlabeled data and no external judges.", "method": "They introduce iReasoner, a self-evolving framework built around a Proposer\u2013Solver loop on unlabeled images. The model is prompted to produce explicit chain-of-thought (CoT) reasoning trajectories. iReasoner then computes intrinsic rewards that combine (1) outcome-level rewards based on final answers and (2) a trajectory-aware signal based on internal agreement among multiple reasoning paths. This trajectory-aware reward lets the system distinguish and favor better reasoning paths even when they lead to the same answer, all without ground-truth labels or external evaluators. The framework is applied as fully unsupervised post-training to an existing LMM (Qwen2.5-VL-7B).", "result": "Applying iReasoner to Qwen2.5-VL-7B in a fully unsupervised post-training setting yields up to +2.1 point improvements across a range of multimodal reasoning benchmarks. These gains indicate that explicitly modeling and rewarding intermediate reasoning steps can enhance the reasoning quality of large multimodal models using only unlabeled data.", "conclusion": "The work demonstrates that LMMs\u2019 implicit reasoning can be improved in a purely unsupervised fashion by eliciting chain-of-thought and rewarding internal consistency of reasoning trajectories, not just final outcomes. iReasoner provides a trajectory-aware self-evolving framework that can be layered onto existing LMMs and serves as an initial step toward more reasoning-aware self-improvement methods for multimodal models without relying on labeled data or external judges."}}
{"id": "2601.05879", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05879", "abs": "https://arxiv.org/abs/2601.05879", "authors": ["Jakub Harasta", "Matej Vasina", "Martin Kornel", "Tomas Foltynek"], "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law", "comment": "Accepted at AI for Access to Justice, Dispute Resolution, and Data Access (AIDA2J) at Jurix 2025, Torino, Italy", "summary": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.", "AI": {"tldr": "The paper tests whether popular large language models show gender bias when giving advice on a realistic divorce and child-custody scenario under Czech family law, comparing responses across models, gendered vs. neutral versions, and varying case factors, and finds model differences and signs of gender-dependent outcomes.", "motivation": "As more laypeople turn to LLMs for legal self-help, untested biases in model outputs could unfairly affect expectations and decisions in sensitive areas like family law and child custody. There is little systematic evidence on how LLMs handle such scenarios, especially regarding gender bias and cross-model variation.", "method": "The authors design a realistic, expert-crafted divorce and custody scenario based on Czech family law. They test four state-of-the-art LLMs (GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, Llama 3.3) in zero-shot mode with two versions of the scenario: one using gendered names (e.g., mother/father) and one using neutral labels. They introduce nine legally relevant factors to vary the facts of the case (e.g., behaviors or circumstances of each parent) and analyze how these variations and naming conditions affect the models\u2019 recommended shared-parenting ratios. The analysis is exploratory and descriptive, focusing on patterns and asymmetries rather than causal inference.", "result": "Preliminary analysis reveals notable differences between the four models in their recommended parenting arrangements and indicates that some systems produce outcomes that vary depending on the gendered framing of the parties. These gender-dependent patterns emerge in how the models adjust parenting ratios across the different factual variants, suggesting systematic asymmetries linked to party gender. However, the evidence is descriptive and not positioned as establishing causality.", "conclusion": "The study concludes that current LLMs may exhibit gender-related asymmetries in family-law guidance, which is concerning given that lay users often rely on such tools without legal expertise. The authors argue for more rigorous, systematic evaluation of LLM behavior in legally sensitive contexts and caution against uncritical use of LLMs for legal self-help, especially where fairness and equality are central concerns."}}
{"id": "2601.05882", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05882", "abs": "https://arxiv.org/abs/2601.05882", "authors": ["Constantinos Karouzos", "Xingwei Tan", "Nikolaos Aletras"], "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift", "comment": null, "summary": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation", "AI": {"tldr": "Study of how different preference-tuning (alignment) objectives generalize under domain shift, and how adaptation strategies like pseudo-labeling mitigate degradation.", "motivation": "Preference-tuned LLMs often lose performance and helpfulness outside their training domain, but it is unclear which alignment objectives generalize better and how to best adapt them to new domains.", "method": "Systematic empirical comparison of five popular alignment objectives and several adaptation strategies (including target-domain supervised fine-tuning and pseudo-labeling) on summarization and QA helpfulness tasks under source-to-target domain shifts.", "result": "They observe clear, systematic differences in how well various alignment objectives generalize under domain shift, and find that certain adaptation strategies\u2014especially those using pseudo-labeling\u2014can significantly reduce performance degradation.", "conclusion": "Choice of alignment objective has a strong impact on cross-domain generalization, and pseudo-labeling-based adaptation is an effective way to mitigate domain-shift-induced degradation in preference-tuned LLMs."}}
{"id": "2601.05903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05903", "abs": "https://arxiv.org/abs/2601.05903", "authors": ["Zihang Tian", "Rui Li", "Jingsen Zhang", "Xiaohe Bo", "Wei Huo", "Xu Chen"], "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search", "comment": null, "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.", "AI": {"tldr": "Proposes HAPS, a hierarchical LLM routing framework that jointly chooses model architectures and parameter settings to better leverage multiple LLMs.", "motivation": "Existing LLM routing methods mainly decide which model architecture to use for a given input, but they ignore how parameter settings (e.g., decoding parameters, configuration choices) strongly affect performance. The authors aim to build a routing system that can simultaneously select the most suitable LLM and tune its parameters for each task or instance, thereby maximizing performance across diverse tasks.", "method": "Introduce HAPS, a hierarchical framework with two levels of routers: (1) a high-level router that chooses among candidate LLM architectures for a given input; (2) a low-level router that searches the optimal parameter settings for the chosen architecture. A parameter generation network is used to share and coordinate parameters between the two routers, so that learning at one level benefits the other. Training is done with a reward-augmented objective that leverages performance feedback to optimize both routers and the shared parameter network end-to-end.", "result": "On two commonly used benchmarks, HAPS consistently surpasses strong existing LLM routing baselines that only route over architectures without joint parameter search, demonstrating improved performance and more effective use of multiple LLMs.", "conclusion": "Jointly routing over both LLM architectures and their parameter settings via a hierarchical framework and shared parameter generation leads to better task performance than routing over architectures alone. The proposed HAPS framework is an effective and general approach for improving multi-LLM systems, as confirmed by empirical gains on standard benchmarks, and the released code enables further research and application."}}
{"id": "2601.05905", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05905", "abs": "https://arxiv.org/abs/2601.05905", "authors": ["Haoming Xu", "Ningyuan Zhao", "Yunzhi Yao", "Weihong Xu", "Hongru Wang", "Xinle Deng", "Shumin Deng", "Jeff Z. Pan", "Huajun Chen", "Ningyu Zhang"], "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency", "comment": "Work in progress", "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.", "AI": {"tldr": "They introduce a new way (NCB) to measure how robust an LLM\u2019s beliefs are to small contextual changes, plus a stress-test protocol and a training method (SAT) that makes beliefs more stable and cuts brittle knowledge errors by about 30%.", "motivation": "Current LLM evaluations focus on point-wise confidence (e.g., self-consistency) which can look good even when the model\u2019s underlying belief is fragile and collapses under slight context changes. They want a more structural, neighborhood-based measure of belief robustness to enable reliable real-world deployment.", "method": "1) Define Neighbor-Consistency Belief (NCB) as a structural robustness metric by checking coherence of model responses across a neighborhood of related prompts/contexts. 2) Design a cognitive stress-testing protocol that perturbs context to probe stability under interference. 3) Propose Structure-Aware Training (SAT), a training procedure that explicitly optimizes for context-invariant belief structures rather than just isolated answers. 4) Empirically evaluate NCB and SAT on multiple LLMs and datasets.", "result": "They empirically show that: (a) LLMs can show perfect self-consistency yet collapse under mild contextual interference; (b) samples with higher NCB scores are more resistant to such interference; and (c) their SAT method reduces long-tail knowledge brittleness by about 30%, improving belief robustness.", "conclusion": "Point-wise confidence metrics like self-consistency are insufficient to judge belief robustness in LLMs. Their proposed NCB provides a more structural view of belief stability, and the cognitive stress-testing protocol plus SAT training can measurably harden models against contextual interference, leading to more reliable real-world deployment."}}
{"id": "2601.05911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05911", "abs": "https://arxiv.org/abs/2601.05911", "authors": ["Phuong-Hang Le", "Valentin Pelloin", "Arnault Chatelain", "Maryem Bouziane", "Mohammed Ghennai", "Qianwen Guan", "Kirill Milintsevich", "Salima Mdhaffar", "Aidan Mannion", "Nils Defauw", "Shuyue Gu", "Alexandre Audibert", "Marco Dinarelli", "Yannick Est\u00e8ve", "Lorraine Goeuriot", "Steffen Lalande", "Nicolas Herv\u00e9", "Maximin Coavoux", "Fran\u00e7ois Portet", "\u00c9tienne Ollion", "Marie Candito", "Maxime Peyrard", "Solange Rossato", "Benjamin Lecouteux", "Aur\u00e9lie Nardy", "Gilles S\u00e9rasset", "Vincent Segonne", "Sol\u00e8ne Evain", "Diandra Fabre", "Didier Schwab"], "title": "Pantagruel: Unified Self-Supervised Encoders for French Text and Speech", "comment": null, "summary": "We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.", "AI": {"tldr": "Pantagruel is a new family of French self-supervised encoder models for both text and speech, trained with a shared feature-space prediction objective and achieving competitive or superior results to prior French models.", "motivation": "Existing French models for text and speech typically use modality-specific prediction targets (e.g., tokens for text, speech units for audio) and separate architectures, which limits unified modeling and potentially underuses shared linguistic structure. There is also a need for stronger French representations and larger, more diverse audio resources. The authors aim to build a unified architecture for French text and speech, leverage a feature-space self-supervised objective, and provide improved benchmarks and resources, especially for speech.", "method": "They design a shared encoder architecture for French that can process either text or speech, using modality-specific encoders but a common self-supervised objective that predicts contextualized feature-space representations instead of discrete tokens or units. They pre-train separate Pantagruel models on large-scale French text corpora (Wikipedia, OSCAR, CroissantLLM) and on large-scale speech corpora (MultilingualLibriSpeech, LeBenchmark, and a new INA-100k corpus of 100,000 hours of diverse French broadcast audio). They then evaluate the resulting models on a broad set of downstream tasks for both modalities, including the FLUE and LeBenchmark suites.", "result": "Across a wide set of French text and speech downstream tasks, Pantagruel models achieve competitive or better performance than strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0. The shared architecture successfully handles both text and speech and demonstrates that the feature-space prediction objective leads to strong representations for each modality.", "conclusion": "Feature-space self-supervised objectives combined with a shared encoder architecture are effective for French representation learning in both text and speech. The Pantagruel family provides a robust foundation model for multimodal French speech-text understanding, and the introduced INA-100k corpus enriches available French audio resources for future research."}}
{"id": "2601.05930", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05930", "abs": "https://arxiv.org/abs/2601.05930", "authors": ["Jingsheng Zheng", "Jintian Zhang", "Yujie Luo", "Yuren Mao", "Yunjun Gao", "Lun Du", "Huajun Chen", "Ningyu Zhang"], "title": "Can We Predict Before Executing Machine Learning Agents?", "comment": "Work in progress", "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "AI": {"tldr": "They propose an LLM-based agent that predicts which candidate solutions are best before actually executing them, greatly reducing expensive physical trials and speeding up scientific discovery.", "motivation": "Autonomous ML agents that assist scientific discovery are slowed down by an \"Execution Bottleneck\"\u2014they must physically run or experimentally test every hypothesis to evaluate it. This is costly, time-consuming, and often impractical. The authors want to let agents reason about which solutions are promising using learned internal models, so that only the most promising ones need to be physically executed.", "method": "1) They formalize a new task called Data-centric Solution Preference, where the goal is to choose the better of two candidate solutions based purely on data and analysis rather than full execution. 2) They construct a large dataset of 18,438 pairwise comparisons for this task. 3) They prime large language models with a Verified Data Analysis Report and test their ability to predict which solution is better, treating this as a supervised preference prediction problem and measuring accuracy and calibration. 4) They design FOREAGENT, an autonomous agent that uses a Predict-then-Verify loop: it first uses the LLM to predict promising solutions with internalized execution priors, then selectively verifies them with actual execution, iterating this loop.", "result": "The LLM, when provided with a Verified Data Analysis Report, reaches 61.5% accuracy in predicting which solution is better in the pairwise comparison task and shows good confidence calibration. The FOREAGENT system, which uses this predictive capability, converges to good solutions 6 times faster and outperforms purely execution-based baselines by 6% in final performance metrics.", "conclusion": "Internalizing execution priors into an LLM allows autonomous agents to replace many expensive physical executions with predictive reasoning. The proposed Data-centric Solution Preference task and associated dataset show that LLMs can meaningfully predict solution quality. Embedding this capability into an agent architecture (FOREAGENT) via a Predict-then-Verify loop mitigates the Execution Bottleneck, accelerates convergence, and improves performance over traditional execution-heavy approaches."}}
{"id": "2601.05960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05960", "abs": "https://arxiv.org/abs/2601.05960", "authors": ["V\u00edctor Gallego"], "title": "Distilling Feedback into Memory-as-a-Tool", "comment": "Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench", "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.", "AI": {"tldr": "They create a system where an LLM saves its own reasoning critiques as reusable guidelines in files, so future inferences are cheaper but still high quality.", "motivation": "Inference-time reasoning methods like test-time refinement improve LLM performance but are expensive because they require repeated, transient reasoning for each example. The authors want a way to keep the benefits of such reasoning while amortizing its cost across many future inferences.", "method": "They build a file-based memory system that stores critiques as persistent guidelines and use agent-controlled tool calls so the LLM can read, write, and update these guideline files during interaction. They introduce Rubric Feedback Bench, a new dataset designed to evaluate rubric-based learning and feedback. The system is evaluated by comparing augmented LLMs that use this memory against standard test-time refinement pipelines.", "result": "Augmented LLMs using the guideline memory quickly reach the performance level of more expensive test-time refinement pipelines, while requiring much less inference-time computation and associated cost.", "conclusion": "Persisting critiques as retrievable guidelines in an external memory, coupled with tool-using agents, can effectively amortize the cost of inference-time reasoning. This yields near-refinement-level performance with significantly reduced inference cost, validated on the Rubric Feedback Bench dataset."}}
{"id": "2601.06002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06002", "abs": "https://arxiv.org/abs/2601.06002", "authors": ["Qiguang Chen", "Yantao Du", "Ziniu Li", "Jinhao Liu", "Songyao Duan", "Jiarui Guo", "Minghao Liu", "Jiaheng Liu", "Tong Yang", "Ge Zhang", "Libo Qin", "Wanxiang Che", "Wenhao Huang"], "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "comment": "Preprint", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "AI": {"tldr": "The paper studies why long chain-of-thought (Long CoT) reasoning is hard to teach LLMs and proposes a molecular-inspired framework plus a new synthesis method (Mole-Syn) to construct effective Long CoT training data, improving reasoning performance and RL stability.", "motivation": "LLMs struggle to reliably learn and generalize long chain-of-thought reasoning, especially when training signals come from humans or shorter, non-Long-CoT model outputs. Existing approaches often rely on keyword or surface-form imitation, which does not capture deeper structural properties of effective reasoning. The paper aims to understand what makes a Long CoT trajectory both effective for reasoning and learnable by LLMs, and then leverage this understanding to design better training procedures.", "method": "The authors introduce a unified, molecule-inspired view of Long CoT trajectories, where reasoning steps form structures via three interaction types: Deep-Reasoning (covalent-like bonds representing strong logical dependencies), Self-Reflection (hydrogen-bond-like links representing revisiting and correcting thoughts), and Self-Exploration (van der Waals-like weak exploratory connections). They analyze distilled Long CoT trajectories to identify how these structural patterns emerge and define the concept of Effective Semantic Isomers\u2014different structural organizations of semantically similar reasoning. They theoretically and empirically study which \u201cbonds\u201d promote fast entropy convergence during learning. Based on these insights, they design Mole-Syn, a distribution-transfer-graph algorithm that constructs or guides the synthesis of Long CoT structures with favorable interaction patterns for training.", "result": "The analysis shows that useful Long CoT structures arise specifically from fine-tuning on Long CoT data rather than simply copying surface patterns or keywords. Only certain interaction patterns\u2014those that promote fast entropy convergence\u2014lead to stable and effective Long CoT learning, whereas competing or conflicting structures degrade training. Mole-Syn is demonstrated to synthesize Long CoT structures that better match these desirable properties, improving both reasoning performance and stability of reinforcement learning training across multiple benchmarks.", "conclusion": "Long CoT reasoning is not just about length or explicit stepwise explanation but about having specific, stable structural patterns in the reasoning traces. Modeling these trajectories as molecular-like structures with distinct interaction types provides a useful lens for understanding what makes them learnable. By focusing on Effective Semantic Isomers and favoring bonds that speed entropy convergence, the authors can algorithmically guide the creation of superior Long CoT training data. Their Mole-Syn method leverages this to transfer distributional properties of good structures, leading to improved LLM reasoning and more stable RL optimization."}}
{"id": "2601.06007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06007", "abs": "https://arxiv.org/abs/2601.06007", "authors": ["Elias Lumer", "Faheem Nizar", "Akshaya Jangiti", "Kevin Frank", "Anmol Gulati", "Mandar Phadate", "Vamse Kumar Subbiah"], "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks", "comment": "15 pages, 8 figures", "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.", "AI": {"tldr": "The paper evaluates how different prompt caching strategies affect cost and latency for multi-turn LLM agent workloads across major providers, finding large savings when caches are structured to separate static and dynamic content.", "motivation": "LLM agents performing long, multi-turn tool-using tasks incur high API costs and latency due to large, repetitive prompts, but despite providers offering prompt caching, there is little empirical research on how well caching works for these agentic workloads or how to best structure prompts to exploit it.", "method": "The authors benchmark three caching strategies\u2014full context caching, system-prompt-only caching, and caching that excludes dynamic tool results\u2014on DeepResearchBench, a multi-turn agentic benchmark with real web search tool calls and 10,000-token system prompts, across three major LLM providers (OpenAI, Anthropic, Google), and they measure both API cost and time to first token over 500+ agent sessions.", "result": "Across providers, prompt caching yields 45\u201380% API cost reductions and 13\u201331% improvements in time to first token, with strategies that carefully place dynamic content at the end of prompts, avoid dynamic traditional function-calling, and exclude dynamic tool outputs performing more consistently well than naive full-context caching, which can sometimes increase latency; provider-specific nuances in caching behavior are also observed.", "conclusion": "Prompt caching, when used with structured prompts that clearly separate static and dynamic segments, substantially improves cost-efficiency and responsiveness of multi-turn LLM agents; naive full-context caching is suboptimal, and practitioners should design prompts and tool-calling patterns to align with provider-specific caching mechanisms for production agentic systems."}}
{"id": "2601.06022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06022", "abs": "https://arxiv.org/abs/2601.06022", "authors": ["Chengming Cui", "Tianxin Wei", "Ziyi Chen", "Ruizhong Qiu", "Zhichen Zeng", "Zhining Liu", "Xuying Ning", "Duo Zhou", "Jingrui He"], "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.", "AI": {"tldr": "AdaFuse is an adaptive ensemble decoding framework that dynamically decides when and how to ensemble multiple LLMs during generation, improving performance without retraining.", "motivation": "Existing LLM ensemble methods use a fixed fusion granularity and cannot adapt mid-generation or across tasks, limiting their ability to exploit complementary model strengths in different decoding contexts.", "method": "AdaFuse treats words as basic alignment units and, at each decoding step, uses an uncertainty-based criterion to decide whether to ensemble. When the state is confident, it continues with a single model; when uncertain, it triggers a diversity-aware scaling strategy to explore multiple candidate continuations across models, then adaptively fuses them based on semantic appropriateness and decoding context.", "result": "On open-domain QA, arithmetic reasoning, and machine translation benchmarks, AdaFuse consistently surpasses strong ensemble baselines, yielding an average relative performance gain of 6.88%.", "conclusion": "Adaptive, context-dependent ensembling with integrated test-time scaling more effectively combines complementary LLM capabilities than fixed-granularity ensembles, providing a practical way to boost performance across varied tasks without retraining the underlying models."}}
{"id": "2601.06021", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06021", "abs": "https://arxiv.org/abs/2601.06021", "authors": ["Jiajie Zhang", "Xin Lv", "Ling Feng", "Lei Hou", "Juanzi Li"], "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "AI": {"tldr": "The paper introduces CaRR and C-GRPO, a citation-aware, rubric-based reinforcement learning framework that trains deep search LLM agents to reason comprehensively, factually, and with explicit evidence chains, outperforming standard outcome-based RL methods.", "motivation": "Existing RL methods for LLM-based deep search agents mostly use coarse, binary outcome rewards (correct/incorrect), which do not evaluate how comprehensive, well-grounded, or well-cited the reasoning process is. This leads to issues like shortcut exploitation (gaming the training setup rather than truly reasoning) and hallucinations (unsupported claims). There is a need for fine-grained, process-level rewards that assess reasoning quality and factual support, not just final answers.", "method": "The authors propose Citation-aware Rubric Rewards (CaRR), which decomposes complex queries into multiple single-hop, verifiable rubrics. The agent must (1) identify hidden intermediate entities, (2) attach correct citations to support each step, and (3) build a complete evidence chain from these steps to the final answer. These rubric-based process rewards are combined with traditional outcome rewards via a new RL algorithm, Citation-aware Group Relative Policy Optimization (C-GRPO), which adjusts the policy based on both process-level rubric satisfaction and final-task success.", "result": "Across several deep search benchmarks, agents trained with C-GRPO show better performance than baselines trained with standard outcome-only RL. The improvements include higher task accuracy, more complete and better-grounded reasoning traces, fewer shortcut-based solutions, and fewer hallucinations. The method also maintains robustness and performance on more open-ended deep research tasks beyond the benchmarks used for training.", "conclusion": "Incorporating citation-aware, rubric-based process rewards into RL training leads to deep search agents that reason more comprehensively and factually while reducing undesirable behaviors like shortcuts and hallucinations. C-GRPO demonstrates that combining fine-grained reasoning rewards with traditional outcome rewards yields more robust and generalizable LLM-based search agents, suggesting a promising direction for future work in training reliable reasoning systems."}}
