<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.AI](#cs.AI) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: Introduction of Moxin 7B, a fully open-source LLM and its task-specialized variants, with emphasis on complete transparency and strong performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM landscape is dominated by proprietary models (e.g., GPT-4, GPT-o1) and partially open models, limiting transparency, reproducibility, and community collaboration. Although open-source LLMs like LLaMA and Mistral exist, they often do not fully open training pipelines, datasets, and implementation details. There is a need for a truly open, transparent LLM that can serve as a solid, extensible foundation for research and applications, and that covers multiple modalities and languages.

Method: Develop Moxin 7B, a 7-billion-parameter large language model designed under the Model Openness Framework, which mandates releasing not only model weights but also training data, code, and implementation details. Build three specialized derivatives: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. Train these models using open-source frameworks and open datasets, then evaluate them on a range of benchmarks corresponding to their capabilities.

Result: Moxin 7B and its variants demonstrate superior performance on multiple evaluations relative to comparable open-source baselines (exact metrics are not given in the abstract, but “superior performance” is claimed across various tasks). The models are successfully trained using only open frameworks and open data, validating the feasibility of a fully transparent training pipeline.

Conclusion: Moxin 7B provides a fully open, transparent LLM platform, going beyond typical open-weight releases by disclosing training data, code, and implementation details in line with the Model Openness Framework. Its variants (Moxin-VLM, Moxin-VLA, and Moxin-Chinese) extend capabilities to vision-language, vision-language-action, and Chinese language tasks while achieving strong benchmark results. The complete release of models, code, and available data is intended to support a sustainable, collaborative, and healthy open-source LLM ecosystem.

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [2] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: The paper shows that transformer-based sentence embeddings contain a graded, hierarchical structure that aligns with human-defined cognitive attributes, and this structure is linearly or shallowly nonlinearly decodable.


<details>
  <summary>Details</summary>
Motivation: While prior work has demonstrated rich geometric structures in transformer embedding spaces, it is unclear whether these representations also encode higher-level, psychologically meaningful organization. The authors aim to test whether sentence embeddings reflect graded and hierarchical cognitive attributes that humans can interpret, beyond mere surface word statistics.

Method: The authors create a dataset of 480 natural sentences annotated with continuous energy scores and discrete tier labels across seven ordered cognitive categories. They extract fixed sentence embeddings from several transformer language models and train linear and shallow nonlinear probing models to predict the continuous scores and categorical tiers from these embeddings. They compare against TF-IDF lexical baselines, use nonparametric permutation tests to establish significance over chance, and perform qualitative analyses using UMAP visualizations and confusion matrices to inspect the geometry and error patterns.

Result: Both the continuous cognitive energy scores and the discrete tier labels are reliably decodable from transformer sentence embeddings. Shallow nonlinear probes consistently outperform linear probes, while TF-IDF baselines perform much worse, indicating that surface word statistics cannot fully explain the observed structure. Permutation tests confirm that probe performance is significantly above chance. Visualization and confusion analyses show smooth gradients from low to high cognitive tiers and mostly adjacent-tier confusions, suggesting a graded hierarchy in the embedding space.

Conclusion: Transformer-based sentence embeddings exhibit a hierarchical geometric organization that aligns with human-defined cognitive or psychological attributes. This structure goes beyond simple lexical statistics and can be revealed with relatively simple probing models. However, the findings are framed in representational terms only and do not imply that the models possess internal awareness or phenomenological experience.

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [3] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: SmartSnap introduces self-verifying agents that proactively collect minimal snapshot evidence to prove task completion in GUI-based RL, enabling more scalable and reliable training with significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Agentic RL for complex GUI tasks struggles to scale because task success is usually verified passively after the whole trajectory, requiring analysis of long, noisy interaction histories. Existing verifiers (scripts, reward models, LLM judges) become costly and unreliable under such verbose context, forming a bottleneck for training autonomous agents.

Method: The paper proposes SmartSnap, a paradigm where agents perform proactive, in-situ self-verification. It defines a Self-Verifying Agent whose dual role is to both complete tasks and gather curated snapshot evidence. Using 3C Principles (Completeness, Conciseness, Creativity), the agent selects a minimal, decisive set of GUI snapshots and uses its online access to verify its own progress. These snapshots are then passed—without the full trajectory—to a general LLM-as-a-Judge, which evaluates whether the evidence proves successful task completion.

Result: On mobile GUI tasks and across multiple LLM families and sizes, SmartSnap enables more scalable training of LLM-driven agents and yields notable performance improvements: up to 26.08% gain for 8B models and 16.66% for 30B models. The resulting self-verifying agents achieve competitive performance compared with strong baselines such as DeepSeek V3.1 and Qwen3-235B-A22B.

Conclusion: Embedding self-verification into the agent—via SmartSnap’s snapshot-based evidence collection guided by the 3C Principles—reduces verification cost and noise, improves reliability, and boosts performance in GUI task RL. This paradigm shows that tightly coupling problem solving with evidence seeking is an effective path toward scalable, high-performing autonomous agents.

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [4] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: The paper analyzes the syntax of qulk-clauses in Yemeni Ibbi Arabic within the Minimalist Program, arguing they are biclausal structures with qulk as a clause-embedding predicate selecting a null CP, and shows how their derivation proceeds via standard minimalist operations and post-syntactic morphological merger.


<details>
  <summary>Details</summary>
Motivation: To understand the syntactic nature of qulk-clauses in Yemeni Ibbi Arabic, clarify how this dialect-specific construction fits into the Minimalist Program, and explore whether its behavior supports broader claims about the universality of minimalist operations.

Method: The study adopts the Minimalist Program framework, treating qulk as a clause-embedding predicate and applying core minimalist operations (Merge, Move, Agree, Spell-Out) plus post-syntactic Morphological Merger to derive the observed surface patterns, while examining phenomena such as absence of overt complementizers, bipartite negation, cliticization, and CP embedding in the dialect.

Result: It shows that qulk-clauses in Yemeni Ibbi Arabic are best analyzed as biclausal CP-embedding structures with a null complementizer, that their surface morphology results from standard minimalist derivations plus morphological merger, and that the same analysis captures dialect-specific behaviors such as bipartite negation and cliticization.

Conclusion: The paper concludes that qulk-clauses instantiate a regular CP-embedding structure compatible with the Minimalist Program, raises questions about extending the analysis to the second-person form kil-k 'you said', and suggests that the successful analysis offers some support for the possible universality of minimalist mechanisms.

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [5] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

TL;DR: Introduces Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method that uses random Fourier features inside adapters to enable frequency-aware modulation of representations, achieving competitive or better results than prior PEFT methods with low overhead.


<details>
  <summary>Details</summary>
Motivation: Large pre-trained language models are expensive to fully fine-tune, and existing parameter-efficient fine-tuning methods may not fully exploit the structure of internal representations. The authors aim to improve adaptation efficiency and effectiveness by leveraging frequency decomposition of representations, allowing the model to focus on informative frequency bands while keeping the backbone frozen.

Method: Insert lightweight adapter modules into a frozen pre-trained language model. These adapters use random Fourier features to decompose intermediate hidden states into low- and high-frequency components. FAA then applies frequency-aware activation and adaptive weighting to modulate these components, enabling selective emphasis or suppression of certain frequency bands during fine-tuning, while only updating the adapter parameters.

Result: On GLUE, E2E NLG, and instruction-tuning benchmarks, FAA matches or outperforms existing parameter-efficient fine-tuning baselines in accuracy/quality while requiring similar or lower computational and memory cost. Ablation experiments show that removing frequency-aware activation or adaptive weighting degrades performance, confirming their contributions.

Conclusion: Frequency-aware adapters based on random Fourier features provide an effective and efficient way to fine-tune large language models. FAA preserves backbone capacity, adds minimal overhead, and delivers robust performance gains over standard PEFT approaches, making it a strong option for post-training adaptation of large models.

Abstract: We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [6] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: An LLM-guided exemplar selection framework for few-shot human activity recognition using wearable sensors that fuses semantic priors with geometric and structural cues to select compact, informative exemplars and significantly improves macro F1 on UCI-HAR.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art HAR methods depend on large labeled datasets and mostly geometric exemplar selection, which struggle to distinguish subtly different activities (e.g., walking vs. walking upstairs/downstairs) in few-shot settings. There is a need for exemplar selection that incorporates semantic knowledge about activity similarity, feature importance, and class confusability to improve representation quality when only a small number of labeled sensor samples are available.

Method: The authors introduce an LLM-Guided Exemplar Selection framework. They query a large language model to obtain semantic knowledge priors describing feature importance, inter-class confusability, and class-specific exemplar budget multipliers. These LLM-derived priors guide exemplar scoring and selection, which is further refined using margin-based validation signals, PageRank centrality to measure representativeness, hubness penalization to avoid over-selected samples, and facility-location optimization to choose a compact exemplar set. The framework is applied to human activity data from wearable sensors under few-shot constraints.

Result: On the UCI-HAR dataset in strict few-shot conditions, the framework achieves a macro F1-score of 88.78%. This performance surpasses classical exemplar selection baselines such as random sampling, herding, and k-center approaches, demonstrating the benefit of incorporating LLM-derived semantic priors alongside geometric and structural signals.

Conclusion: Integrating LLM-derived semantic priors with structural and geometric cues yields more informative and compact exemplar sets for few-shot wearable-sensor human activity recognition. This combination significantly improves classification performance over traditional, purely geometric selection strategies and indicates that semantic reasoning from LLMs can compensate for limited labeled data and subtle inter-class differences in HAR tasks.

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [7] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: The paper proposes HHEM, a lightweight hallucination detection model for LLMs that is more efficient than prior methods while maintaining high accuracy, and enhances it with segment-based retrieval for summarization tasks.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs reduce trust and reliability, and existing evaluation methods like KnowHalu are computationally expensive due to multi-stage verification relying on LLM-based judgments. There is a need for a fast, accurate, and scalable hallucination detection approach that does not depend heavily on large models for evaluation.

Method: The authors adopt and integrate the Hughes Hallucination Evaluation Model (HHEM), a classification-based hallucination detector independent of LLM judgments. They compare hallucination detection methods across different LLMs using metrics such as TPR, TNR, and Accuracy on QA and summarization tasks. They further augment HHEM with non-fabrication checking and introduce segment-based retrieval for summarization, which verifies smaller text segments against evidence to better capture localized hallucinations. They also perform CDF-based analysis of hallucination rates across models of different parameter sizes.

Result: HHEM cuts evaluation time dramatically from about 8 hours to 10 minutes while preserving high detection performance. HHEM with non-fabrication checking achieves an accuracy of 82.2% and a TPR of 78.9%. The base HHEM struggles with localized hallucinations in summarization, but segment-based retrieval improves detection on these harder cases. The CDF analysis shows that larger models (7B–9B parameters) generally hallucinate less, while mid-sized models are more unstable and prone to hallucinations.

Conclusion: The study demonstrates that a lightweight, classification-based framework like HHEM can substantially improve the efficiency of hallucination evaluation without sacrificing accuracy. Enhancements such as non-fabrication checking and segment-based retrieval are important for handling nuanced, localized hallucinations, especially in summarization. The findings also suggest that model size influences hallucination behavior, highlighting the necessity of structured, scalable evaluation frameworks that combine computational efficiency with rigorous factual validation to improve the reliability of LLM outputs.

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [8] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: HiFi-RAG is a hierarchical, multi-stage RAG pipeline that uses a cheaper LLM (Gemini 2.5 Flash) for retrieval-related steps and a stronger LLM (Gemini 2.5 Pro) for final reasoning, significantly improving answer quality over standard RAG baselines, especially on post-cutoff questions.


<details>
  <summary>Details</summary>
Motivation: Standard open-domain RAG often retrieves irrelevant passages and produces answers that don’t match user intent, reducing reliability and wasting computation. There is also a practical need to control cost and latency by not using the most capable, expensive model for every step of the pipeline. The paper aims to design a more precise, cost-efficient RAG architecture that filters noisy context and better aligns generated answers with user queries, while leveraging different models for different sub-tasks.

Method: The authors propose HiFi-RAG (Hierarchical Filtering RAG), a multi-stage RAG pipeline. Instead of relying solely on embedding-based retrieval, they use Gemini 2.5 Flash to: (1) reformulate and expand queries, (2) perform hierarchical content filtering over retrieved documents to discard irrelevant information, and (3) help with citation attribution. After this lightweight, inexpensive processing, the filtered, high-precision context is passed to Gemini 2.5 Pro, which is used exclusively for final answer generation and reasoning-intensive steps. This separation of roles allows coarse, fast filtering and fine, expensive reasoning. The system was implemented and evaluated in the MMU-RAGent NeurIPS 2025 competition under Text-to-Text static evaluation.

Result: On the MMU-RAGent validation dataset, HiFi-RAG achieved ROUGE-L of 0.274, a 19.6% relative improvement over the baseline, and DeBERTaScore of 0.677, a 6.2% relative improvement. On the Test2025 dataset—which specifically targets questions requiring knowledge after January 2025—HiFi-RAG outperformed the parametric (no-retrieval or weaker retrieval) baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore, indicating much better alignment with ground-truth answers, especially for post-cutoff knowledge.

Conclusion: Hierarchical Filtering RAG, which delegates retrieval, filtering, and citation tasks to a cheaper, fast model while reserving a stronger model for final reasoning, yields substantial performance gains over standard embedding-based RAG and parametric baselines in open-domain QA. The system demonstrates that structured multi-stage pipelines and model-specialization across the stack can improve both answer quality and cost-efficiency, particularly for questions depending on up-to-date knowledge.

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [9] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: The paper evaluates how well various large language models perform accounting-specific reasoning and finds GPT-4 is strongest but still insufficient for real-world enterprise use.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly influence professional work, organizations need to understand and reliably leverage these models for specialized domains like accounting, which is critical for digital transformation and social development. However, the domain-specific reasoning abilities of LLMs in accounting are not well characterized, limiting safe and effective adoption.

Method: The authors define the concept of vertical-domain accounting reasoning and derive evaluation criteria based on an analysis of the training data characteristics of GLM-series models. Using these criteria, they design accounting reasoning tasks and systematically test models including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4 under different prompt-engineering strategies, comparing performance across models and prompts.

Result: The experiments show that prompt engineering can significantly affect performance, with varying gains across different models. Among the evaluated models, GPT-4 demonstrates the strongest accounting reasoning performance. Nonetheless, all models exhibit notable gaps relative to the demands of practical, real-world accounting applications.

Conclusion: While current LLMs—especially GPT-4—show promising capabilities in accounting reasoning, they are not yet ready to fully meet enterprise-level accounting requirements. The proposed evaluation framework and benchmarks can guide future research on reasoning paradigms and model optimization, with the goal of improving LLM performance and reliability in professional accounting scenarios.

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [10] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: The paper argues that Korean constituency treebanks should use eojeol (spacing units) as terminals, separating morphology into a different layer, and shows Sejong and Penn Korean treebanks are equivalent under this view.


<details>
  <summary>Details</summary>
Motivation: There is a representational problem in Korean treebanks: whether to use morphemes or eojeols as terminal units. Using morphemes as terminals mixes word-internal morphology with phrase-level syntax and conflicts with existing eojeol-based dependency resources. The authors want a representation that keeps syntax interpretable and compatible across resources.

Method: They conduct a comparative representational analysis of the Sejong and Penn Korean treebanks, under explicit normalization assumptions, focusing on how constituency is defined when terminals are eojeols versus morphemes. They then design an annotation scheme that treats eojeols as constituency terminals and encodes morphological segmentation and fine-grained POS tags in a separate layer.

Result: They show that, once normalized, Sejong and Penn Korean treebanks can be considered representationally equivalent at the eojeol-based constituency level. They also propose a concrete eojeol-based annotation scheme that separates constituency from morphology while still capturing detailed morphological information.

Conclusion: Eojeol-based constituency with a separate morphological layer is a better design choice for Korean treebanks: it preserves clear syntactic constituency, aligns with eojeol-based dependency resources, enables cross-treebank comparison, and facilitates conversion between constituency and dependency representations.

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [11] [ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation](https://arxiv.org/abs/2512.22491)
*Suhua Wang,Zifan Wang,Xiaoxin Sun,D. J. Wang,Zhanbo Liu,Xin Li*

Main category: cs.CL

TL;DR: Proposes ManchuTTS, a Manchu-specific TTS system with hierarchical text representation and attention, non-autoregressive generation, and new dataset, achieving high quality under low-resource conditions.


<details>
  <summary>Details</summary>
Motivation: Manchu is an endangered and under-resourced language whose agglutinative phonology and lack of data make high-quality text-to-speech difficult. Existing TTS models are not optimized for its complex morphology and scarcity of annotated speech, limiting preservation and technological support for the language.

Method: Design a three-level text representation (phoneme, syllable, prosodic units) combined with a cross-modal hierarchical attention mechanism to align these granularities with acoustics. Use a non-autoregressive synthesis architecture that merges deep convolutional networks with a flow-matching Transformer for efficient speech generation. Introduce a hierarchical contrastive loss to enforce structured correspondence between linguistic tiers and acoustic features. Build the first Manchu TTS corpus (6.24 hours) and apply data augmentation; train models on a 5.2-hour subset for evaluation. Compare to baseline systems and perform ablation studies on the hierarchical components.

Result: ManchuTTS achieves a Mean Opinion Score (MOS) of 4.52 when trained on only 5.2 hours of data, clearly outperforming all baseline TTS models. Ablation experiments show that the hierarchical guidance components significantly boost agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%, validating the design choices for handling Manchu’s agglutinative structure.

Conclusion: The proposed ManchuTTS framework effectively addresses both linguistic complexity and data scarcity for Manchu speech synthesis. Its hierarchical text representation, cross-modal attention, and contrastive learning substantially improve pronunciation and prosody, enabling near-natural speech quality in a low-resource setting. The newly constructed Manchu TTS dataset also provides a foundational resource for future work on endangered and agglutinative languages.

Abstract: As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.

</details>


### [12] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: Introduces All-or-Here Attention (AHA), a mechanism where each attention head dynamically chooses between full attention and local sliding-window attention per token, greatly reducing full attention use without hurting performance.


<details>
  <summary>Details</summary>
Motivation: Full attention in LLMs scales quadratically with context length and is computationally expensive. However, humans typically rely on local context and only occasionally refer back to distant information. The paper seeks an attention mechanism that lets models mostly use cheap local attention while retaining the ability to selectively access the full context when needed, improving efficiency without sacrificing accuracy.

Method: Design a per-head binary routing mechanism (AHA) that, for each token, switches between (1) standard full attention over the entire context and (2) local sliding-window attention over a fixed number of nearby tokens. Train or learn this router so that the model can decide on demand when global context is necessary. Run experiments on LLMs with different window sizes to evaluate how often global attention is actually needed and the impact on performance.

Result: With a local window size of 256 tokens, AHA can replace up to 93% of full attention operations with sliding-window attention while maintaining performance comparable to full attention baselines. Experiments across window sizes reveal that the demand for global attention has a long-tail distribution that rapidly decreases as the local window grows.

Conclusion: Most full attention in LLMs is redundant for many tokens; effective inference can be achieved with predominantly local sliding-window attention, supplemented by selective, on-demand full attention. By decoupling local processing from global access, AHA offers a path to more efficient LLM inference while preserving modeling power over long contexts.

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [13] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: The paper presents a system for a multimodal conversational aspect-based sentiment analysis challenge, using structured prompting for component extraction and LLM ensembling for sentiment flip detection, achieving competitive scores on both subtasks.


<details>
  <summary>Details</summary>
Motivation: Sentiment understanding in multimodal, multi-speaker conversations is difficult but necessary for emotionally intelligent AI. Existing work often focuses on simpler settings (single modality, single speaker, or coarse sentiment labels), so there is a need for methods that can extract fine-grained sentiment structures and track dynamic sentiment shifts in realistic dialogue scenarios.

Method: For Subtask-I (sentiment sextuple extraction), the authors design a structured, step-wise prompting pipeline for large language models, where the model sequentially identifies the sentiment holder, target, aspect, opinion, sentiment polarity, and rationale, using prompts that refine and incorporate contextual information from the dialogue. For Subtask-II (sentiment flipping detection), they construct an ensemble of three different LLMs, exploiting their complementary strengths and aggregating their predictions to robustly detect points where sentiment shifts occur and to identify the triggers behind such shifts.

Result: Their system attains a 47.38% average score on Subtask-I and a 74.12% exact-match F1 score on Subtask-II in the MCABSA Challenge, indicating good performance, especially on sentiment flip detection, compared with typical baselines in such complex tasks.

Conclusion: Structured, step-wise prompting can help LLMs better understand and extract fine-grained sentiment components in complex multimodal dialogues, while model ensembling improves robustness in detecting sentiment transitions and their causes. These strategies are effective for rich, conversational aspect-based sentiment analysis and can inform future work on emotionally aware AI systems.

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [14] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: Proposes T3LLM, a three-LLM framework (worker, reviewer, student) for time series question answering (TSQA) that performs multi-step reasoning with explicit correction to reduce numeric reasoning errors and achieve SOTA on TSQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for time series question answering directly apply general NLP techniques and often make reasoning errors on complex numerical sequences. Unlike text-only tasks, time series outputs can be checked against the original data, enabling verification-based correction, which current approaches underuse.

Method: Reformulate time series analysis as TSQA and introduce T3LLM, a framework with three cooperative LLMs: (1) a worker model that generates structured, step-wise chains of thought; (2) a reviewer model that inspects these reasoning steps against the time series, flags incorrect steps, and suggests corrections; and (3) a student model that is fine-tuned on the corrected chains of thought so it can internalize both multi-step reasoning and self-correction. The prompts are structured to make reasoning explicit and verifiable on time series data.

Result: On several real-world TSQA benchmarks, T3LLM outperforms strong LLM-based baselines and achieves state-of-the-art performance, indicating improved reliability and accuracy in time series numerical reasoning tasks.

Conclusion: Leveraging the verifiability of time series data via a worker–reviewer–student triad with explicit correction substantially enhances LLM-based time series question answering. Encoding corrected chains of thought into a student model leads to better multi-step reasoning and self-correction capabilities, yielding SOTA results on TSQA tasks.

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [15] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: M2G-Eval is a new benchmark and framework to evaluate code LLMs at multiple granularities and across many programming languages, plus tuned models and empirical findings.


<details>
  <summary>Details</summary>
Motivation: Existing code LLM benchmarks usually test only one level of code structure (e.g., functions) and few languages, which hides how capabilities differ between small vs. large code scopes and among many languages.

Method: They build M2G-Eval, a benchmark with tasks at four code granularities (Class, Function, Block, Line) across 18 languages, with >17K training tasks and 1,286 carefully filtered test instances. They train M2G-Eval-Coder models by fine-tuning Qwen3-8B via supervised fine-tuning and Group Relative Policy Optimization, then evaluate 30 models on this benchmark.

Result: They observe a difficulty hierarchy (Line easiest, Class hardest), larger performance gaps between languages when tasks become more complex, and strong correlations in performance across languages, implying shared, transferable programming knowledge.

Conclusion: M2G-Eval is a useful, fine-grained, multilingual benchmark that reveals nuanced strengths and weaknesses of code LLMs and shows that complex, long-form code synthesis remains challenging.

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [16] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: The paper analyzes diffusion models for language generation, identifies structural mismatches between generic diffusion mechanics and the specifics of text, and highlights key problems in current approaches to guide better model design.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have beneficial properties for text generation, such as parallel decoding and iterative refinement, but existing methods struggle because text is discrete, sequential, and highly structured. There is a lack of clear understanding of what properties a diffusion process must satisfy to work well for language, and current methods show limitations and trade-offs. The authors aim to clarify these requirements and diagnose why current diffusion language models underperform or behave suboptimally.

Method: The authors conceptually analyze diffusion language modeling from two perspectives: the general diffusion process and classic language modeling. They formalize and enumerate five essential properties that a diffusion-based language model should meet to be well aligned with the nature of text. They categorize and compare existing diffusion language models into two families: (1) continuous diffusion in embedding space, and (2) discrete diffusion over tokens. They examine to what extent each family satisfies the five desired properties and expose the structural trade-offs in these designs. They further analyze recent large diffusion language models to empirically and conceptually pinpoint where they fall short, focusing on how they corrupt text and how they train on token-wise marginals.

Result: The analysis shows that neither continuous nor discrete diffusion approaches fully satisfy all five essential properties; instead, each covers only a subset, making inherent trade-offs. In particular, they find two central issues in the leading designs: (i) the commonly used uniform corruption process does not align with how information is unevenly distributed across positions in text (e.g., some tokens carry more meaning than others), and (ii) training that only optimizes token-wise marginal distributions fails to learn multi-token dependencies needed for coherent parallel decoding. These shortcomings explain several observed weaknesses of current diffusion language models.

Conclusion: The authors conclude that there is a fundamental mismatch between standard diffusion mechanics and the structural requirements of language, and that current diffusion language models only partially address these needs. They argue that future work should design diffusion processes and training objectives that better respect the structure of text, including non-uniform, structure-aware corruption and objectives that capture multi-token dependencies. Such directions are expected to yield more coherent and effective diffusion-based language models.

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [17] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: The paper studies how to make chain-of-thought (CoT) reasoning by LLMs more faithful to their real internal reasoning, comparing two optimization methods and finding GRPO works better, especially on larger models.


<details>
  <summary>Details</summary>
Motivation: CoT is widely used to boost LLM reasoning, but current CoT explanations can be unfaithful: models provide plausible but misleading rationales that do not match how they actually derived answers. This threatens its use for safety and alignment monitoring, where truthful rationales are crucial. The authors aim to better understand and improve the faithfulness of CoT reasoning.

Method: The authors empirically compare two post-training optimization methods—Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO)—in terms of how well they improve CoT faithfulness. They test them on multiple model sizes, including Qwen2.5-14B-Instruct, and evaluate with several faithfulness-related metrics, analyzing performance trends with model scale and stability of training behavior.

Result: GRPO generally outperforms DPO on larger models with respect to CoT faithfulness metrics. Qwen2.5-14B-Instruct optimized with GRPO achieves the best scores across all reported evaluation metrics. Both GRPO and DPO benefit from larger model sizes, but GRPO shows stronger gains in faithfulness, although it behaves less stably on smaller models.

Conclusion: Optimizing LLMs with GRPO is a promising strategy for making CoT reasoning more faithful, especially for larger models. While both GRPO and DPO improve faithfulness as model size increases, GRPO has greater potential for enhancing transparency and trustworthiness of LLM reasoning, though additional work is needed to stabilize it at smaller scales.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [18] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: Paper introduces Vocabulary-Aware Conformal Prediction (VACP) to make prediction sets for LLM next-token prediction both valid and small, achieving large efficiency gains while maintaining near-target coverage.


<details>
  <summary>Details</summary>
Motivation: Standard softmax probabilities in LLMs are miscalibrated, which is risky in high-stakes domains. Conformal prediction can guarantee coverage but, when applied naively to huge vocabularies, yields extremely large prediction sets that are practically useless. The paper aims to design a method that keeps the formal uncertainty guarantees while making the prediction sets small and informative for large-vocabulary LLMs.

Method: They study Adaptive Prediction Sets (APS) for next-token prediction in transformer LLMs with vocabularies over 250k tokens. They identify a tradeoff between coverage and prediction set size, then propose Vocabulary-Aware Conformal Prediction (VACP). VACP uses semantic masking to restrict attention to a semantically relevant subset of the vocabulary and applies temperature-adjusted scoring to re-shape the score distribution. The method is constructed to preserve conformal prediction’s marginal coverage guarantees while operating on this reduced effective vocabulary.

Result: On Gemma-2B evaluated with SQuAD and WikiText, VACP attains 89.7% empirical coverage for a 90% coverage target, while shrinking the average prediction set size from 847 tokens (naive conformal APS) to 4.3 tokens—a 197x improvement in efficiency. They also provide theoretical analysis of how vocabulary reduction interacts with conformal coverage guarantees and supply code for reproducibility.

Conclusion: Vocabulary-aware conformal prediction can make uncertainty quantification for LLM next-token prediction both statistically valid and practically useful, even with very large vocabularies. By combining semantic masking with temperature-based scoring, VACP maintains near-nominal coverage while drastically reducing prediction set sizes, offering a scalable framework for calibrated, efficient LLM predictions.

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [19] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: Multilingual transformer-based framework for detecting hope speech, with strong results for Urdu and several other languages.


<details>
  <summary>Details</summary>
Motivation: Hope speech is underexplored in NLP, and existing work is mostly for English. Low-resource languages like Urdu lack tools and datasets that encourage positive online communication. There is also little systematic evaluation of transformer models for hope speech detection across multiple languages.

Method: Develop a multilingual hope speech detection framework centered on Urdu. Use pretrained multilingual and monolingual transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT). Apply simple text preprocessing, fine-tune these models as classifiers, and evaluate them on the PolyHope-M 2025 benchmark for both binary and multi-class setups in several languages.

Result: On PolyHope-M 2025, the approach attains strong performance: 95.2% F1 for Urdu binary hope vs non-hope classification and 65.2% F1 for Urdu multi-class hope categories, with similarly competitive scores for Spanish, German, and English.

Conclusion: Existing multilingual transformer models, with minimal preprocessing and fine-tuning, are effective for hope speech detection even in low-resource languages. This opens the way to practical tools that can identify and promote constructive, positive discourse across diverse linguistic settings.

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [20] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: The paper evaluates whether the logical quality of chain-of-thought reasoning from large language models transfers across languages and finds substantial cross-lingual discrepancies, especially for non-Latin scripts.


<details>
  <summary>Details</summary>
Motivation: While chain-of-thought prompting showcases strong reasoning abilities in large language models, it is unclear if this quality is preserved across different languages. Existing multilingual benchmarks focus mainly on answer accuracy, not on whether the intermediate reasoning is logically valid and aligned with the final answer. This gap is important because flawed reasoning, even when answers are correct, undermines trust and reliability, particularly in non-English or low-resource languages.

Method: The authors create a human-validated evaluation framework to judge whether model-generated reasoning traces actually support model conclusions across multiple languages. They collect and analyze around 65,000 chain-of-thought traces produced by six frontier LLMs on GlobalMMLU questions translated into six languages, covering both Latin and non-Latin scripts. Human annotators label whether each reasoning trace is logically aligned with the answer and help build a taxonomy of error types. The taxonomy distinguishes evidential errors—like unsupported assertions or ambiguous facts—from illogical reasoning steps and other failure modes.

Result: Despite high task accuracy, models frequently produce reasoning traces that do not logically support their final answers. This misalignment is significantly worse in non-Latin scripts, where the rate of incorrect or unsupported reasoning is at least double that observed in Latin-script languages. The human annotation study reveals that the dominant error category is evidential problems—unsupported claims and vague or ambiguous facts—followed by errors in the logical progression of steps within the chain-of-thought.

Conclusion: Current multilingual evaluation practices, which primarily focus on final-answer accuracy, overlook serious deficiencies in the quality and reliability of model reasoning, especially in non-Latin-script languages. The paper argues that to properly assess and improve LLMs’ multilingual reasoning abilities, evaluations must explicitly examine the logical soundness and evidential grounding of chain-of-thought explanations. The proposed framework and error taxonomy provide a foundation for more reasoning-aware, cross-lingual evaluation of language models.

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [21] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: The paper studies how to reduce social desirability bias in LLM-generated survey responses using simple, psychologically grounded prompt tweaks, finding that neutral third-person reformulations work best.


<details>
  <summary>Details</summary>
Motivation: Silicon Sampling with LLMs is being used to approximate population-level survey responses, but socially sensitive questions show strong Social Desirability Bias, making LLM responses less human-like and potentially misleading. Existing work has only shallowly examined how to mitigate this bias in LLM-based sampling.

Method: The authors use American National Election Study (ANES) survey items and compare distributions of human responses to those generated by three LLMs (Llama-3.1 variants and GPT-4.1-mini). They first replicate a baseline silicon sampling setup to confirm bias, then experiment with four prompt interventions: (1) reformulated neutral third-person prompts, (2) reverse-coded (semantically inverted) items, and two meta-instruction styles: (3) priming the model for analytical thinking, and (4) a sincerity preamble. They quantify alignment between model and human response distributions using Jensen-Shannon Divergence with bootstrap confidence intervals.

Result: Baseline silicon samples show persistent Social Desirability Bias—overconcentration on socially approved answers compared to ANES data. Reformulated neutral third-person prompts substantially reduce this bias, decreasing distributional concentration and lowering Jensen-Shannon Divergence to human data. Reverse-coded questions yield inconsistent improvements. Priming and sincerity preambles introduce more uniform response distributions but do not reliably improve alignment or reduce bias.

Conclusion: Simple, psychologically informed changes to prompt framing—especially neutral, third-person reformulations—can meaningfully reduce Social Desirability Bias in LLM-based silicon sampling and bring model-generated survey distributions closer to real human data. However, not all interventions help: reverse-coding has mixed effects, and generic analytic or sincerity instructions do not systematically mitigate bias. Prompt framing thus offers a practical, low-cost tool for improving the representativeness of LLM-simulated survey samples.

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [22] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: Uses Twitter data and an NLP pipeline to identify and classify self-reported pregnancy outcomes, showing social media can augment epidemiological research on birth outcomes.


<details>
  <summary>Details</summary>
Motivation: Infant mortality in the U.S. is high, with birth defects and other negative pregnancy outcomes (miscarriage, stillbirth, preterm birth, low birth weight) as major contributors. Traditional epidemiological datasets are limited in scale, timeliness, and detail on exposures and patient experiences. There is a need for new, scalable, and more granular data sources to better understand risk factors, evaluate interventions, and ultimately reduce adverse pregnancy outcomes.

Method: The paper proposes leveraging publicly available social media posts, primarily from Twitter, as an additional data source. It builds an NLP pipeline that: (1) detects posts by women describing their own pregnancies; (2) extracts information about pregnancy course and outcomes; (3) classifies users into positive cases (full gestation, normal birth weight) and negative cases (miscarriage, stillbirths, birth defects, preterm birth, etc.). The pipeline addresses challenges such as data imbalance, noise, and unstructured, informal language via preprocessing and data augmentation techniques, then uses the labeled cohorts for observational analyses.

Result: The authors demonstrate that social media data can be automatically mined to identify cohorts of pregnant women and differentiated outcome groups (positive vs. negative). The system successfully constructs analyzable datasets from noisy Twitter streams, providing sufficient signal to explore associations between pregnancy outcomes and various exposures or interventions discussed online. The results show feasibility and potential validity of using such data as a complement to traditional sources for pregnancy-outcome research.

Conclusion: Social media, despite its noise and unstructured nature, can serve as a viable adjunct data source for epidemiological studies on pregnancy outcomes. An NLP-based pipeline can reliably detect and categorize self-reported pregnancy experiences, enabling construction of pregnant cohorts and comparator groups. This approach opens possibilities for assessing causal impacts of treatments, interventions, or exposures and offers a framework that future maternal and fetal health studies can adopt or extend.

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [23] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: The paper introduces WeDLM, a diffusion-based language model decoding framework that matches or exceeds autoregressive (AR) quality while achieving significantly faster inference by making diffusion decoding compatible with causal attention and KV caching.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in LLMs is inherently sequential, limiting parallelism and thus inference speed. Diffusion language models, which can update multiple tokens in parallel, are a promising alternative but have not yielded real deployment speedups over highly optimized AR engines like vLLM, mainly because their use of bidirectional attention breaks prefix KV caching. The motivation is to design a diffusion-style decoding method that fully exploits parallelism without sacrificing the cache-friendliness and practical efficiency of causal AR architectures.

Method: The authors propose WeDLM, a diffusion decoding framework that operates entirely with standard causal attention. The key technique is Topological Reordering: observed (already denoised) tokens are moved to the physical prefix of the sequence while respecting their logical positions, so each masked token can attend to all observed tokens under a strict causal mask. On top of this, they design a streaming decoding algorithm that, at each step, commits high-confidence tokens into a growing left-to-right prefix and keeps a roughly constant amount of parallel diffusion work per step, thereby avoiding inefficient stop-and-wait block processing typical of many diffusion-based decoders.

Result: Empirically, WeDLM maintains the generation quality of strong AR backbones while providing substantial wall-clock speedups under realistic serving conditions. On challenging reasoning benchmarks, it achieves close to 3x faster decoding compared to AR baselines, and in low-entropy (more deterministic) generation regimes, it reaches up to 10x speedup. These gains are demonstrated against AR models deployed with vLLM under matched settings, indicating that the improvements are robust to strong AR baselines and real-world deployment constraints.

Conclusion: The study concludes that diffusion-style decoding, when carefully designed to use purely causal attention and leverage prefix KV caching through topological reordering and streaming commitment of tokens, can outperform highly optimized AR decoding engines in practice. WeDLM demonstrates that parallel diffusion decoding is not just theoretically attractive but can yield concrete, large speedups without sacrificing model quality, suggesting a viable alternative paradigm for practical LLM deployment.

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [24] [Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis](https://arxiv.org/abs/2512.22741)
*Dongning Rao,Yunbiao Zeng,Zhihua Jiang,Jujian Lv*

Main category: cs.CL

TL;DR: Proposes TEXT, a text-routed sparse Mixture-of-Experts model that uses multimodal LLM-generated explanations and a new temporal alignment block (combining Mamba and temporal cross-attention) to better align audio/video with text for multi-modal sentiment analysis, achieving SOTA on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal sentiment analysis methods struggle to capture subtle emotions across modalities and underexplore two key aspects: (1) using explanations (e.g., natural language rationales) to guide multimodal understanding, and (2) precise temporal alignment between audio, video, and text streams. With the rise of multimodal LLMs, there is an opportunity to leverage their explanatory power while also improving temporal modeling to advance performance and interpretability in MSA.

Method: Introduce TEXT (Text-routed sparse Mixture-of-Experts with eXplanation and Temporal alignment). Step 1: Use multimodal LLMs to generate explanations for multimodal sentiment samples, and incorporate these explanations into the model as an additional textual signal. Step 2: Design a temporality-oriented neural block that aligns audio and video representations over time; this block combines ideas from Mamba-style sequence modeling and temporal cross-attention to better capture cross-modal temporal dependencies. Step 3: Build a text-routed sparse Mixture-of-Experts architecture, where text (augmented by explanations) is used to route inputs through different experts, and a gate fusion mechanism integrates the expert outputs across modalities. The entire system is trained/tested on multiple MSA benchmarks.

Result: On four multimodal sentiment analysis datasets (including CH-SIMS), TEXT outperforms three recently proposed baselines and three multimodal LLMs. It achieves the best performance on all four datasets and wins on at least four out of six evaluation metrics. On CH-SIMS specifically, TEXT reduces the mean absolute error to 0.353, a 13.5% improvement over strong recent approaches, indicating more accurate sentiment prediction.

Conclusion: Leveraging LLM-generated explanations and improved temporal alignment within a text-routed sparse MoE framework significantly advances multimodal sentiment analysis. TEXT demonstrates that explanations can effectively guide cross-modal fusion, and that combining Mamba-style temporal modeling with temporal cross-attention yields better alignment of audio/video with text. The empirical gains across multiple benchmarks suggest this is a promising direction for more accurate and interpretable human-centric multimodal understanding tasks.

Abstract: Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.

</details>


### [25] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: The paper studies whether domain-adaptive pretraining improves fake-news detection in a low-resource language (Urdu) using multilingual models.


<details>
  <summary>Details</summary>
Motivation: Misinformation detection has mainly focused on high-resource languages. For low-resource languages like Urdu, a typical solution is to fine-tune multilingual pretrained models. However, these models often perform poorly on domain-specific vocabulary and style, which are frequent in news and misinformation, leading to suboptimal detection performance. The authors aim to improve performance for Urdu fake-news classification without requiring massive task-specific labeled data.

Method: They adopt a staged training approach: first perform domain-adaptive pretraining of multilingual language models on a large Urdu news corpus, then fine-tune these adapted models on downstream fake-news classification datasets. They experiment with two common multilingual encoders, XLM-RoBERTa and mBERT. Domain adaptation is done via continued pretraining (e.g., MLM) on the Urdu news corpus. Afterwards, they fine-tune each model (vanilla vs domain-adapted) on four public Urdu fake-news datasets and compare performance.

Result: Domain-adapted XLM-RoBERTa consistently outperforms the original (non-adapted) XLM-R across all four Urdu fake-news datasets, indicating that domain-adaptive pretraining effectively improves generalization in this setting. For mBERT, domain adaptation yields mixed results: in some cases it helps, in others it does not clearly outperform the vanilla model.

Conclusion: Domain-adaptive pretraining on in-domain Urdu news data is a promising strategy to enhance fake-news detection performance for low-resource languages when using multilingual language models, particularly for XLM-RoBERTa. However, benefits may depend on model architecture, as evidenced by the inconsistent gains with mBERT. The staged training approach helps address domain mismatch but is not universally beneficial across all multilingual models.

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [26] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: The paper evaluates methods for automatically segmenting clinical notes into meaningful sections, finding that large language models perform best overall while rule-based methods remain useful for more structured text.


<details>
  <summary>Details</summary>
Motivation: Clinical notes are stored in unstructured or semi-structured formats, which makes it hard to use them for secondary analysis and downstream applications. Identifying section boundaries (e.g., history of present illness, medications, discharge instructions) is crucial for structuring these notes because different sections carry different clinical contexts.

Method: The authors curate a dataset of 1,000 clinical notes from MIMIC-IV and compare rule-based baselines, domain-specific transformer models, and large API-based language models for the task of clinical note segmentation at both sentence-level and freetext granularity. Performance is measured primarily using F1 scores.

Result: Large API-based models achieve the best segmentation performance overall, with GPT-5-mini reaching an average F1 score of 72.4 across sentence-level and freetext segmentation tasks. Rule-based, lightweight baselines perform competitively on structured sentence-level segmentation but their performance degrades on unstructured freetext.

Conclusion: Large language models are currently the most effective approach for clinical note segmentation, especially for unstructured freetext, while simpler baselines can still be useful for more structured scenarios. These findings offer practical guidance on method selection and support future downstream applications like information extraction, cohort identification, and automated summarization based on structured clinical notes.

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [27] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: The paper introduces a unified pipeline for automatically generating challenging, verifiable simulated environments and an environment-level RL algorithm that stabilizes training with simulated users, leading to better and more efficient language-agent training.


<details>
  <summary>Details</summary>
Motivation: Existing RL for language-based agents in simulated environments is held back by semi-automated or overly easy environments, limited diversity, and unstable simulated users, which together reduce both the depth and reliability of training.

Method: They design (1) an automated, scalable synthesis pipeline that generates high-difficulty but easily verifiable tasks/environments, and (2) an environment-level RL algorithm that handles user-simulation instability and performs advantage estimation at the environment level to stabilize and improve learning efficiency.

Result: On several agent benchmarks—tau-bench, tau2-Bench, and VitaBench—the proposed system outperforms prior approaches, showing more stable and efficient RL training for language-based agents.

Conclusion: Automated generation of challenging, verifiable simulated environments combined with environment-level RL leads to more stable, efficient, and generalizable training for language-based agents, with demonstrated strong in-domain performance and promising out-of-domain generalization.

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [28] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: The paper proposes a modified pre-training objective for language models, derived from on-policy reinforcement learning principles, to create a token distribution that leads to better exploration and reasoning when later fine-tuned with RL.


<details>
  <summary>Details</summary>
Motivation: While RL has been shown to significantly enhance LLM reasoning, its effectiveness heavily depends on the exploration space induced by the pre-trained model’s token distribution. Standard cross-entropy can be seen as a special case of policy gradient in a one-step setting, but it does not explicitly control how suitable the resulting distribution is for downstream RL exploration. The authors want to understand and optimize how pre-training shapes this distribution to ultimately improve RL-based reasoning performance.

Method: The authors re-interpret cross-entropy training as policy-gradient optimization in a single-step decision process and then generalize this to a new pre-training objective inspired by on-policy RL. They cast next-token prediction as a stochastic decision process and introduce a reward-shaping scheme with two key components: (1) a positive reward scaling factor that adjusts how much probability mass is focused on ground-truth tokens (controlling precision vs. diversity), and (2) a rank-aware treatment of negative tokens that penalizes high-ranking and low-ranking non-ground-truth tokens differently. This shapes the token-output distribution during pre-training to be more suitable for subsequent RL exploration.

Result: Using the proposed objective, the authors obtain pre-trained models whose token distributions provide a more effective exploration space for downstream RL. Empirically, these models achieve better end-to-end reasoning performance after RL fine-tuning than models pre-trained with standard cross-entropy, even though the resulting distributions can be more concentrated (lower entropy). The experiments show that precision-oriented priors lead to more successful RL exploration and better reasoning outcomes than simply encouraging higher entropy.

Conclusion: The paper concludes that pre-training objectives should be explicitly designed with downstream RL exploration in mind. Standard cross-entropy corresponds to a limited special case of policy gradient and does not optimally shape the exploration space. By using RL-inspired reward shaping—emphasizing precision on correct tokens and rank-aware handling of negatives—one can create a more favorable token distribution for RL. Surprisingly, a precision-oriented, lower-entropy prior supports better exploration and reasoning than a high-entropy one, challenging common intuitions about exploration in RL for LLMs.

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [29] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: The paper evaluates multiple large language models on end-to-end clinical reasoning tasks and examines how different prompt engineering strategies affect their performance, showing that effects are highly task- and model-dependent.


<details>
  <summary>Details</summary>
Motivation: Although LLMs do well on structured medical exams, it is unclear how reliably they support real-world, sequential clinical decision-making across the full workflow of a patient encounter. Additionally, while prompt engineering is widely promoted as a way to boost LLM performance, its actual benefits and limitations in a clinical context are not well understood.

Method: The authors tested three advanced LLMs (ChatGPT-4o, Gemini 1.5 Pro, LLaMA 3.3 70B) on 36 clinical case studies, evaluating them on five sequential tasks: generating a differential diagnosis, proposing essential immediate steps, selecting relevant diagnostic tests, making a final diagnosis, and recommending treatment. Each model was run under two temperature settings (default and zero) to examine stability and accuracy trade-offs. They then applied variations of the MedPrompt prompt-engineering framework using both targeted and random dynamic few-shot examples, to see how these strategies affect performance across tasks.

Result: All models showed task-dependent performance: they were nearly perfect for final diagnosis, weak for identifying relevant diagnostic tests, and moderate on other tasks. ChatGPT performed better with zero temperature, while LLaMA did better at default temperature, indicating different optimal settings. Prompt engineering significantly improved performance only on the weakest task (relevant diagnostic testing) but reduced performance on some other tasks. Targeted dynamic few-shot examples did not consistently outperform randomly selected examples, suggesting that highly tailored examples do not automatically yield better results than more diverse ones.

Conclusion: LLM performance in clinical decision support is uneven across stages of clinical reasoning and sensitive to both model configuration and prompt design. Prompt engineering cannot be assumed to uniformly improve outcomes; instead, its impact is strongly model- and task-dependent. Effective deployment of LLMs in healthcare will require carefully customized, context-aware prompting strategies rather than generic, one-size-fits-all methods.

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [30] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: Proposes FANG, a post-training structured pruning framework for LLMs that groups neurons by function to reduce calibration bias and improve downstream performance at given sparsity levels.


<details>
  <summary>Details</summary>
Motivation: LLMs are computationally and storage intensive due to their large size. Existing post-training structured pruning approaches depend on small few-shot calibration sets that often do not match the pretraining data distribution, leading to poor generalization on downstream tasks. The paper aims to create a pruning method that is more robust to calibration bias and better preserves useful functionality.

Method: Introduce Function-Aware Neuron Grouping (FANG), which: (1) clusters neurons into groups sharing similar functional roles, defined by the type of semantic context they process; (2) prunes each neuron group independently rather than treating all neurons uniformly; (3) in importance scoring within each group, reweights tokens so that those strongly correlated with the group’s functional role get higher weight; (4) explicitly preserves neurons that contribute to multiple context types, reflecting their broad utility; and (5) adaptively allocates sparsity across transformer blocks according to each block’s estimated functional complexity, to balance sparsity and performance.

Result: On language modeling and downstream tasks, FANG improves downstream accuracy without hurting language modeling performance. When combined with existing state-of-the-art post-training pruning methods FLAP and OBC, FANG yields SOTA results, outperforming vanilla FLAP and OBC by 1.5%–8.5% average accuracy at 30% and 40% sparsity levels.

Conclusion: Function-aware structured pruning—specifically, grouping neurons by semantic function and using group-specific importance estimation and adaptive sparsity allocation—mitigates calibration bias and leads to better downstream generalization at a given sparsity. FANG can serve as a plug-in framework to enhance existing post-training pruning methods such as FLAP and OBC.

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [31] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: The paper presents LENS, a framework that aligns multimodal health sensor data with large language models to automatically generate clinically useful mental-health narratives from raw time-series signals.


<details>
  <summary>Details</summary>
Motivation: Multimodal health sensing can capture rich behavioral signals relevant to mental health, but these data are numerical time-series that are difficult to interpret and cannot be directly processed by standard LLMs. Long-duration sensor streams exceed LLM context limits and there is a lack of large paired datasets that connect sensor data with natural-language descriptions of mental state. Clinicians, however, need narrative, clinically grounded summaries instead of raw sensor traces, motivating a method that tightly links sensor data with language models for mental-health assessment.

Method: The authors propose LENS, a framework that connects raw multimodal sensor streams with an LLM. First, they construct a large dataset by converting Ecological Momentary Assessment (EMA) responses about depression and anxiety symptoms into detailed natural-language descriptions, forming more than 100,000 sensor–text question-answer pairs from 258 participants. Second, they design and train a patch-level encoder that ingests raw time-series sensor signals, segments them into patches, and projects these patches directly into the LLM’s internal representation space, enabling native integration of long time-series into the language model. The system is trained to align sensor embeddings with corresponding textual descriptions, so the LLM can generate narratives conditioned on sensor-derived representations.

Result: Empirically, LENS performs better than strong baselines on standard NLP evaluation metrics (e.g., text quality and similarity measures) and on task-specific evaluations focused on accurately predicting or describing symptom severity. Additionally, a user study with 13 mental-health professionals finds that narratives generated by LENS are judged to be comprehensive and clinically meaningful, indicating that the system’s outputs have practical relevance for clinical contexts.

Conclusion: LENS demonstrates that large language models can be effectively aligned with multimodal sensor data to produce clinically grounded mental-health narratives from raw behavioral signals. The framework offers a scalable approach to building models that reason directly over time-series health data, strengthening LLMs as interfaces for health sensing and potentially supporting downstream clinical decision-making and mental-health assessment workflows.

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [32] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: The paper critiques existing metrics for judging chain-of-thought (CoT) faithfulness, showing that many allegedly “unfaithful” CoTs are actually faithful but incomplete, and proposes better evaluation tools including a new faithful@k metric and causal mediation analysis.


<details>
  <summary>Details</summary>
Motivation: Current evaluations often label a chain-of-thought as unfaithful if it fails to repeat or mention a prompt-injected hint that changed the model’s answer. This risks misdiagnosing the problem: natural language CoTs are inevitably compressed summaries of internal computation, so omission of some factors does not necessarily mean the reasoning is dishonest or unfaithful. The authors want to clarify what unfaithfulness actually is and to develop more robust tools to measure it.

Method: The authors critique the Biasing Features metric conceptually, then run experiments on multi-hop reasoning tasks using Llama-3 and Gemma-3 models with prompt-injected hints. They compare Biasing Features judgments with other faithfulness metrics, introduce a faithful@k metric that varies the inference-time token budget to see how often hints are verbalized, and apply Causal Mediation Analysis to test whether non-verbalized hints still causally influence predictions through the CoT. They also use corruption-based metrics as part of a broader interpretability analysis.

Result: They find that many CoTs marked unfaithful by Biasing Features are actually judged faithful by alternative metrics, with disagreement exceeding 50% for some models. Increasing the allowed CoT length dramatically raises the rate at which hints are explicitly verbalized (up to 90% in some experimental settings), indicating that tight token limits force omissions. Causal Mediation Analysis shows that even hints not mentioned in the CoT can still causally mediate the model’s final answer via the reasoning process.

Conclusion: The paper concludes that omission of a biasing hint in the CoT often reflects incompleteness rather than true unfaithfulness, and that evaluations based solely on hint verbalization are unreliable. They recommend using a richer toolkit—such as faithful@k, causal mediation, and corruption-based metrics—to more accurately assess CoT faithfulness and understand model reasoning.

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [33] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: The paper presents Prompt Choreography, a framework that speeds up multi-agent LLM workflows using a global KV cache so calls can reuse and reorder previous computations.


<details>
  <summary>Details</summary>
Motivation: Multi-agent and multi-call LLM workflows repeatedly re-encode overlapping context, causing redundant computation, high latency, and poor scalability, especially when many calls run in parallel or share history.

Method: They design Prompt Choreography, which maintains a dynamic global key–value (KV) cache of token representations. Each new LLM call selects an arbitrary subset and order of prior messages from this cache instead of re-encoding them from scratch, while supporting parallel calls. Because cached encodings can slightly change model behavior compared to fresh re-encoding, they also fine-tune the LLM on data that uses this caching mechanism so it better matches the original outputs.

Result: Across diverse multi-agent and workflow benchmarks, Prompt Choreography reduces per-message latency by 2.0–6.2× in time-to-first-token and achieves over 2.2× end-to-end speedups on workflows with substantial redundant computation, while maintaining output behavior close to the non-cached baseline after fine-tuning.

Conclusion: A globally shared, dynamic KV cache plus modest fine-tuning enables LLMs in complex, multi-call workflows to reuse past computation, significantly cutting latency and total runtime while approximately preserving original model behavior.

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [34] [TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish](https://arxiv.org/abs/2512.23065)
*Melikşah Türker,A. Ebrar Kızıloğlu,Onur Güngör,Susan Üsküdarlı*

Main category: cs.CL

TL;DR: Introduces TabiBERT, a modern, from-scratch Turkish monolingual encoder (ModernBERT-style) plus a new benchmark, achieving SOTA on many Turkish NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Although encoder-only Transformer architectures have advanced (RoPE, FlashAttention, better normalization, long-context), Turkish NLP lacks a monolingual encoder trained from scratch that incorporates these modern design choices and is evaluated systematically. Existing Turkish models (e.g., BERTurk, TurkishBERTweet) do not fully exploit these architectural improvements or provide a broad, standardized benchmark.

Method: Design a ModernBERT-style Turkish encoder (TabiBERT) with RoPE, FlashAttention, refined normalization and 8,192-token context. Curate a large multi-domain Turkish corpus (84.88B tokens of web, scientific, code, math) and pre-train TabiBERT on 1T sampled tokens. Build TabiBench, a benchmark of 28 Turkish datasets across eight task types, with standardized splits and GLUE-style macro-averaged evaluation. Compare TabiBERT against BERTurk and task-specific SOTA models on TabiBench and relevant tasks.

Result: TabiBERT supports 8,192-token sequences (16× BERT), yields up to 2.65× faster inference and lower GPU memory use, enabling larger batch sizes. On TabiBench, it achieves a macro score of 77.58, beating BERTurk by 1.62 points and setting new SOTA on five of eight task categories, including large gains in question answering (+9.55), and improvements in code retrieval (+2.41) and document retrieval (+0.60). Compared with prior task-specific best models (including TurkishBERTweet), TabiBERT improves the average performance by +1.47 points, showing strong cross-domain generalization.

Conclusion: A modern, from-scratch Turkish encoder (TabiBERT) built with recent Transformer advancements can substantially improve performance and efficiency across diverse Turkish NLP tasks. The accompanying TabiBench benchmark, along with released weights and code, provides a transparent, reproducible foundation and new state-of-the-art baselines for Turkish encoder research and applications.

Abstract: Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.

</details>


### [35] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: They build a more efficient, matrix-multiplication-free language model by fixing and sharing some layer weights and inserting untrained reservoir layers, cutting parameters and compute while keeping performance similar.


<details>
  <summary>Details</summary>
Motivation: Large language models are powerful but very expensive to train and run, mainly because of heavy matrix multiplications and many trainable parameters. Existing MatMul-free LMs reduce multiplication cost but still have substantial training overhead. The authors want an even more computationally efficient architecture that lowers training and inference cost without losing much accuracy.

Method: Starting from a MatMul-free language model, they partially fix and share weights in selected layers so those parameters no longer need to be trained. They then insert reservoir computing-style layers—dynamical recurrent-like layers whose weights are fixed—to enrich the model’s representations without adding trainable parameters. They also fuse several operations to reduce memory access overhead. The overall design keeps dynamic expressivity via reservoirs while shrinking the number of trainable parameters and memory traffic.

Result: On experimental benchmarks, the proposed architecture reduces the parameter count by up to 19%, training time by 9.9%, and inference time by 8.0% compared with the baseline MatMul-free LM. Despite these savings, its task performance remains comparable to the baseline, showing little or no accuracy degradation.

Conclusion: A MatMul-free language model augmented with reservoir-computing ideas—fixed/shared weights plus inserted reservoir layers and optimized memory operations—can notably cut parameters and computational cost during both training and inference without sacrificing performance. This suggests reservoir-style fixed dynamics is a promising direction for building more efficient LLM architectures.

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [36] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: The paper introduces BiomedTLDR, a large dataset of author-written biomedical paper TLDRs, and uses it to evaluate how well open-weight LLMs generate extreme summaries compared to human experts.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in using LLMs to generate very short, high-quality scientific summaries (TLDRs) that help rapid understanding and communication of research. However, progress is limited by the lack of a comprehensive, high-quality dataset of human-written scientific TLDRs, making it difficult both to train and to rigorously evaluate LLM summarization performance and to understand how machine-generated summaries differ from expert-written ones.

Method: The authors construct BiomedTLDR, a large-scale dataset of researcher-authored TLDR-style summaries in the biomedical domain by mining authors’ own short comments that accompany bibliography items. They then benchmark several popular open-weight LLMs on the task of generating TLDRs from paper abstracts, and perform qualitative and quantitative analyses comparing LLM-generated summaries with human-written TLDRs in terms of lexical overlap, rhetorical structure, and degree of extractiveness vs. abstractiveness.

Result: BiomedTLDR provides a substantial collection of high-quality, human-written TLDRs paired with scientific abstracts. When evaluated on this dataset, some open-weight LLMs can generate summaries that resemble human-written ones (“humanoid”), but overall they rely more heavily on the wording and structure of the original abstracts, yielding summaries that are more extractive and less abstractive than those written by researchers.

Conclusion: The new BiomedTLDR dataset fills a key resource gap for extreme scientific summarization and enables systematic evaluation of LLM performance on this task. Current open-weight LLMs, while capable, are still more conservative and extractive than human experts in how they summarize, suggesting room for improvement in training models that can produce more genuinely abstractive, higher-level scientific TLDRs. The released code and data support future research on improving and assessing LLM-based scientific summarization.

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [37] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: The paper introduces LLM-PeerReview, an unsupervised ensemble framework that picks the best answer from multiple LLM outputs using LLMs as judges and score aggregation, achieving strong gains over prior ensemble methods.


<details>
  <summary>Details</summary>
Motivation: Different LLMs (or multiple runs of one LLM) have complementary strengths, but existing ways to ensemble their outputs are limited, often supervised, or not very interpretable. There is a need for a general, unsupervised, and transparent mechanism to reliably select the best response from several LLM-generated candidates for any given query.

Method: The authors propose LLM-PeerReview, a peer-review-inspired ensemble framework with three stages. (1) Scoring: For each candidate response, multiple LLMs are used as judges (LLM-as-a-Judge) to assign quality scores. (2) Reasoning / aggregation: The various scores for each candidate are combined either via a graphical model-based truth inference algorithm or via simple averaging to yield a final score for each response. (3) Selection: The response with the highest final score is selected as the ensemble output. The approach is fully unsupervised and model-agnostic, relying only on LLM calls rather than labeled data.

Result: Two variants of LLM-PeerReview (with graphical-model aggregation and with averaging) are tested on four datasets. Both achieve strong performance, surpassing the recent advanced ensemble model Smoothie-Global by 6.9 and 7.3 percentage points, respectively, indicating that the method effectively leverages multiple LLMs to improve answer quality.

Conclusion: LLM-PeerReview provides a simple, interpretable, and unsupervised way to ensemble multiple LLM responses by using LLMs themselves as peer reviewers and aggregating their judgments. Its strong empirical gains over a strong baseline suggest that peer-review-style score aggregation is a promising direction for robust LLM ensembling and can generalize flexibly across tasks and models without supervision.

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [38] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: The paper shows that a purpose-built, constrained DSL for data pipelines (Anka) lets LLMs solve complex, multi-step coding tasks far more reliably than with Python, even though the models were never trained on the DSL.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail on complex, multi-step programming tasks in general-purpose languages like Python, likely because such languages allow many ways to express a solution and require implicit state and variable management, which are hard for LLMs to handle reliably. The authors want to test whether a carefully designed DSL with explicit constraints can reduce these systematic errors and better harness LLM coding abilities.

Method: The authors design Anka, a domain-specific language for data transformation pipelines with explicit, constrained syntax to minimize ambiguity. Without any pretraining on Anka, they prompt LLMs (Claude 3.5 Haiku and GPT-4o-mini) to use the language on a benchmark of 100 data pipeline problems, including single-step and multi-step tasks. They compare parse success and task accuracy when using Anka versus Python for the same problems, focusing on multi-step pipelines where sequencing and variable management are challenging.

Result: Claude 3.5 Haiku, with only in-context exposure, attains 99.9% parse success and 95.8% overall task accuracy in Anka. On multi-step pipeline tasks, Anka yields 100% accuracy versus 60% in Python—a 40 percentage point improvement—largely by preventing errors in operation ordering and variable/state handling. GPT-4o-mini shows similar trends, with Anka improving multi-step task accuracy by 26.7 percentage points over Python. These results hold despite the models having extensive prior training on Python but none on Anka.

Conclusion: The study concludes that (1) LLMs can rapidly acquire and use a new DSL purely from in-context examples with near-native performance, (2) restricting and structuring the syntax of a language substantially reduces LLM errors on complex, multi-step programming problems, and (3) DSLs explicitly optimized for LLM generation can outperform familiar general-purpose languages like Python for certain domains. The authors release Anka, the benchmarks, and the evaluation framework to support further research on LLM-oriented language design.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [39] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: They use sparse autoencoders to build an interpretable low-rank subspace for parameter-efficient fine-tuning, improving safety alignment while touching very few parameters.


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient fine-tuning methods like LoRA assume task-relevant changes lie in a low-rank subspace, but this subspace is learned in a black-box, uninterpretable way and is harmed by polysemanticity (neurons mixing multiple concepts). The authors want an interpretable, controllable way to identify task-relevant directions, especially for safety alignment.

Method: They apply pre-trained Sparse Autoencoders to the model’s activations to obtain a disentangled, approximately monosemantic feature space. They then identify which SAE features are task-relevant and use these to explicitly construct a low-rank subspace. This subspace guides the initialization and structure of the fine-tuning adapters. They provide theoretical analysis showing that, under monosemanticity, the SAE-based subspace can recover the true task subspace with arbitrarily small error, whereas working directly in the original polysemantic basis incurs an irreducible error floor.

Result: On safety alignment benchmarks, their SAE-guided fine-tuning reaches up to 99.6% safety rate, which is 7.4 percentage points better than full fine-tuning and approaches RLHF-based methods, while only updating 0.19–0.24% of parameters. They also obtain interpretable descriptions of the learned “alignment subspace” via the semantics of the activated SAE features.

Conclusion: Mechanistic-interpretability tools, specifically sparse autoencoders that induce monosemantic features, can be integrated into parameter-efficient fine-tuning to simultaneously improve performance, reduce parameter updates, and yield interpretable, controllable low-rank subspaces for alignment-related modifications.

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [40] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: The paper proposes a new task and dataset for detecting pronunciation-based morphs used to evade regulation in Chinese e-commerce health live streams, and uses LLM-based text-to-text generation to resolve them and improve regulation.


<details>
  <summary>Details</summary>
Motivation: E-commerce live streams on platforms like Douyin are a major sales channel in China, but some hosts use pronunciation-based morphs to bypass scrutiny and conduct false advertising, especially in health and medical content. Existing morph research focuses on text-based evasion in social media and underground markets, leaving a gap in handling spoken morphs in live streams. Regulators need automated tools to detect and interpret these evasive expressions in real time.

Method: The authors define a new task called Live Auditory Morph Resolution (LiveAMR) to detect and interpret pronunciation-based morphs in health and medical live streams. They build the first LiveAMR dataset with 86,790 samples. They formulate LiveAMR as a text-to-text generation problem, where models must map morph-laden live stream content to its resolved, regulation-relevant form. They further leverage large language models to generate additional synthetic training data to augment the dataset and improve model performance.

Result: Using the LiveAMR dataset and the text-to-text formulation, the proposed approach achieves improved performance in resolving pronunciation-based morphs compared to baselines. Data augmentation via LLM-generated samples further boosts accuracy and robustness on the LiveAMR task. Empirical results show that the system can reliably interpret evasive spoken expressions used in live e-commerce health streams.

Conclusion: The study establishes LiveAMR as a practical and important task for regulating e-commerce health live streams, provides a large benchmark dataset, and shows that casting morph resolution as text-to-text generation with LLM-based data augmentation is effective. The findings indicate that accurate morph resolution can significantly strengthen automated monitoring and regulation of live streaming commerce.

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [41] [AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration](https://arxiv.org/abs/2512.23300)
*Minjiang Huang,Jipeng Qiang,Yi Zhu,Chaowei Zhang,Xiangyu Zhao,Kui Yu*

Main category: cs.CL

TL;DR: The paper presents AI4Reading, a multi-agent LLM-based system that automatically generates podcast-style audiobook interpretations, aiming to preserve book content accurately while improving clarity and narrative structure.


<details>
  <summary>Details</summary>
Motivation: Manual production of audiobook-style interpretations is labor-intensive and costly, limiting their scalability despite rising demand for accessible, in-depth book analyses. The authors want to reduce human effort while maintaining or improving interpretive quality.

Method: They design AI4Reading, a collaborative framework of 11 specialized LLM-driven agents (topic analysts, case analysts, editors, narrator, proofreaders, etc.). These agents jointly analyze a book, identify themes, mine real-world cases, reorganize and refine content, and then convert the result into natural spoken language using speech synthesis. The workflow is orchestrated so each agent focuses on a well-defined subtask, and their outputs feed into subsequent agents.

Result: In a comparison between expert-created interpretations and system-generated ones, AI4Reading’s scripts are found to be simpler and more accurate in conveying the book’s ideas, though its speech generation quality still lags behind human narration.

Conclusion: Multi-agent LLM collaboration combined with speech synthesis can effectively automate the creation of audiobook-style book interpretations, achieving high content accuracy and clarity. While speech quality needs improvement, the approach demonstrates that structured agent roles can yield interpretable, high-quality interpretive scripts with reduced manual effort.

Abstract: Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.

</details>


### [42] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: A survey that bridges cognitive neuroscience and LLM-based agent memory, summarizing concepts, mechanisms, evaluation, security, and future directions.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous agent memory designs draw loosely from human cognition but lack a deep, systematic integration of cognitive neuroscience due to disciplinary gaps. There is a need for a unified framework that connects how memory is defined, structured, stored, managed, evaluated, and secured in both biological brains and LLM-driven agents.

Method: The paper conducts an interdisciplinary literature review and conceptual synthesis. It defines memory progressively across cognitive neuroscience, LLMs, and agents; compares taxonomies and storage/management mechanisms in humans and machines; surveys benchmarks for evaluating agent memory; and analyzes memory security from an attack/defense standpoint. It then organizes open problems and future directions, especially for multimodal memory and skill learning.

Result: The authors present a structured framework unifying concepts of memory from neuroscience and AI, map biological memory types and processes to components in LLM-based agents, catalog existing benchmarks for agent memory evaluation, and systematize known vulnerabilities and defensive strategies related to memory security in agents.

Conclusion: A principled connection between human and artificial memory can guide the design of more capable, secure, and general LLM-based agents. Future work should emphasize multimodal and skill-centric memory systems informed by cognitive neuroscience, along with rigorous evaluation protocols and robust security mechanisms.

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [43] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: They propose SGR, a framework that uses dynamically generated external subgraphs to guide stepwise reasoning in LLMs and improve logical inference accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on many NLP tasks but still struggle with deep reasoning and logical inference, partly because they may rely on noisy or irrelevant information from large text corpora and produce factually inconsistent or logically flawed outputs. There is a need for methods that ground LLM reasoning in more structured, relevant knowledge to improve reliability.

Method: The SGR framework first constructs a query-specific subgraph from an external knowledge base, capturing entities and relations relevant to the input. It then guides the LLM to perform multi-step, stepwise reasoning over this structured subgraph rather than free-form text, using the graph’s semantic structure to constrain and inform the reasoning process. Finally, it aggregates multiple reasoning paths traced through the subgraph to generate a final answer.

Result: Across multiple benchmark datasets, SGR achieves better performance than strong baseline methods, showing higher reasoning accuracy and more reliable predictions.

Conclusion: Grounding LLMs’ reasoning in dynamically generated, query-relevant knowledge subgraphs and enforcing stepwise reasoning over these structures effectively reduces the impact of noisy information and enhances the reasoning capabilities of LLMs, as evidenced by consistent improvements over strong baselines.

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [44] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: The paper proposes EntroDrop, an entropy-guided token dropout method that stabilizes multi-epoch training of LLMs on limited data by masking easy, low-entropy tokens so the model continues learning to generalize on harder, high-entropy tokens.


<details>
  <summary>Details</summary>
Motivation: LLMs often need to be adapted in domains where high-quality data is scarce, making multi-epoch training on the same corpus necessary. However, standard autoregressive training under repeated data exposure causes overfitting and performance degradation, especially because the model over-optimizes predictable, low-entropy tokens and loses generalization on harder, high-entropy tokens. There is a need for regularization techniques that are aware of and aligned with token-level learning dynamics under data-constrained, multi-epoch regimes.

Method: The authors empirically study learning dynamics in multi-epoch training and identify that low-entropy (predictable) tokens quickly dominate the gradient signal, harming generalization on high-entropy tokens over time. They then introduce EntroDrop, a structured data regularization technique that performs entropy-guided token dropout. During training, EntroDrop selectively masks low-entropy tokens in the input sequence, effectively reducing their influence and forcing the model to focus more on informative, high-entropy tokens. It uses a curriculum schedule that gradually adjusts the dropout strength in accordance with training progress, so the regularization is neither too weak early on nor too strong later, and is scalable across LLM sizes (0.6B–8B parameters).

Result: Across models from 0.6B to 8B parameters trained under extended multi-epoch regimes, EntroDrop consistently outperforms standard regularization baselines. It prevents the typical performance degradation seen in repeated data exposure, maintaining robust model capability and improving generalization. The experiments confirm that entropy-guided masking stabilizes training dynamics and yields better performance than conventional regularization methods when data is limited.

Conclusion: The paper concludes that performance degradation in multi-epoch LLM training stems from an imbalance where easy, low-entropy tokens dominate optimization, undermining learning on hard, high-entropy tokens. Aligning regularization with token-level entropy can correct this imbalance. EntroDrop, which selectively drops low-entropy tokens with a curriculum schedule, acts as effective structured data regularization, sustaining or improving performance during extended training on scarce data. This approach offers a promising and scalable solution for adapting LLMs in data-constrained settings and highlights the broader importance of entropy-aware training strategies.

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [45] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: The paper studies how gender diversity within different team roles (leadership vs support) affects scientific impact, finding an inverted U-shaped relation overall and role- and size-specific effects.


<details>
  <summary>Details</summary>
Motivation: Prior work on gender diversity and scientific team success has yielded inconsistent results and typically uses coarse, aggregate measures of diversity that ignore who occupies which roles on the team. This makes it hard to understand the mechanisms by which gender composition actually influences impact. The authors aim to provide a more fine-grained analysis by distinguishing leadership and support roles and by examining how team size moderates these relationships.

Method: The authors define scientific teams as all coauthors on a paper and use five-year citation counts as a proxy for team impact. They use author contribution statements in over 130,000 PLOS journal articles (mainly biomedical) to classify coauthors into leadership and support roles. They then apply multivariable regression to estimate the association between gender diversity within each role group and citation impact, and use a threshold regression model to study how team size changes (moderates) these effects.

Result: (1) For both leadership and support groups, the relationship between gender diversity and scientific impact is inverted U-shaped: moderate levels of diversity are associated with higher impact than very low or very high diversity. (2) Teams with all-female leadership and all-male support achieve higher impact than other role-gender configurations. (3) Gender diversity in the leadership group has a significantly negative effect on impact in small teams, but in large teams its effect turns positive and becomes statistically insignificant. (4) Gender diversity in the support group has a consistently positive and significant effect on impact, regardless of team size.

Conclusion: Gender diversity influences scientific impact in nuanced, role- and size-dependent ways. Both leadership and support groups show non-linear (inverted U-shaped) effects of diversity, but the pattern differs by role and team size: leadership diversity can hinder small teams yet is not detrimental in large teams, whereas support-role diversity is robustly beneficial. Moreover, specific gender-role configurations—particularly all-female leaders with all-male support—are associated with especially high impact. The study underscores the importance of moving beyond aggregate diversity measures and explicitly accounting for internal role differentiation when evaluating or designing diverse scientific teams.

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [46] [ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning](https://arxiv.org/abs/2512.23440)
*Yuqi Tang,Jing Yu,Zichang Su,Kehua Feng,Zhihui Zhu,Libin Wang,Lei Liang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: Introduces ClinDEF, a dynamic benchmark for evaluating LLM clinical reasoning via simulated doctor–patient dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing LLM medical benchmarks focus on static QA, failing to capture the iterative, interactive nature of real clinical reasoning, and recent dynamic approaches suffer from small, contamination-prone datasets and coarse evaluation.

Method: Build a dynamic evaluation framework (ClinDEF) grounded in a disease knowledge graph that procedurally generates patient cases and runs multi-turn dialogues between an LLM acting as doctor and an automated patient agent, then evaluates performance with diagnostic accuracy, efficiency metrics, and rubric-based quality assessments.

Result: Experiments using state-of-the-art LLMs within ClinDEF show that the framework surfaces significant and otherwise hidden deficiencies in their clinical reasoning across multiple dimensions, not just final diagnosis.

Conclusion: ClinDEF provides a more realistic, fine-grained, and clinically relevant way to assess LLMs’ diagnostic reasoning, revealing important gaps and setting a paradigm for future clinical LLM evaluation.

Abstract: Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.

</details>


### [47] [Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss](https://arxiv.org/abs/2512.23447)
*Ang Lv,Jin Ma,Yiyuan Ma,Siyuan Qiao*

Main category: cs.CL

TL;DR: They add a lightweight loss (ERC) that couples the router with experts in MoE-LLMs using expert router embeddings as proxy tokens, enforcing mutual specialization and improving performance efficiently.


<details>
  <summary>Details</summary>
Motivation: In Mixture-of-Experts language models, the router sends tokens to experts, but there is no explicit constraint that the routing decisions match what each expert is actually good at. This misalignment can limit performance and interpretability, and previous coupling methods are costly because they scale with the number of tokens. The paper aims to create an efficient way to explicitly align router decisions with expert capabilities and to track/control specialization.

Method: They introduce the Expert-Router Coupling (ERC) loss. For each expert, they treat its router embedding as a proxy token representing all tokens routed to that expert. They perturb these router embeddings and feed them through the experts to obtain internal activations. The ERC loss imposes two constraints: (1) For any expert, its activation on its own proxy token should be larger than on any other expert’s proxy tokens (expert-centric constraint). (2) For any proxy token, its activation should be strongest in its corresponding expert compared with all other experts (token-centric constraint). This is computed over n^2 activations for n experts, forming an auxiliary loss added during pre-training.

Result: On MoE large language models with 3B–15B parameters trained on trillions of tokens, models with ERC loss outperform baselines without it. The router-expert alignment and specialization are improved, and the method remains computationally efficient because its cost depends only on the number of experts, not batch size or token count. Analysis shows clearer expert specialization patterns and better controllability of specialization levels during training.

Conclusion: The ERC loss provides an efficient and effective way to couple routers and experts in MoE-LLMs. By aligning router embeddings with expert capabilities via two mutual activation constraints, it improves model performance and expert specialization while adding only a fixed, small computational overhead. It also enables quantitative monitoring and control of specialization, offering better insight into how MoE models organize expertise.

Abstract: Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.

</details>


### [48] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: The paper introduces a method that builds hierarchical trees of semantically related texts using nested density-based clustering on LLM embeddings, enabling unsupervised discovery of research areas and topics across domains.


<details>
  <summary>Details</summary>
Motivation: Although LLM embeddings and vector databases are widely used for semantic search, they mainly expose local similarity (nearest neighbors) and not the global, hierarchical structure of relationships within large text corpora. Existing text classification often depends on predefined labels or categories, which can limit exploratory analysis, especially in scientific domains where fields and subfields evolve over time. The authors want a data-driven way to uncover and visualize hierarchical semantic structures—such as research areas, subfields, and topics—directly from text, without manual taxonomies.

Method: The authors propose a nested density clustering approach applied to LLM-derived embeddings of texts. First, they search the embedding space for dense regions that represent strongly semantically similar texts, forming small, tight clusters. Then they gradually relax the density threshold so that these dense clusters are merged into larger, more diffuse clusters. This progressive relaxation continues until all documents are merged into a single root cluster. By tracking how clusters emerge and merge across density levels, the method constructs a hierarchical tree (a “nested density tree”) in which lower-level nodes represent specific, highly similar text groups and higher-level nodes correspond to broader, more general semantic groupings. They demonstrate the approach using scientific abstracts and apply it to standard text benchmark datasets (20 Newsgroups, IMDB 50k) to test robustness across domains.

Result: The method successfully infers a hierarchical tree structure over text corpora that reflects semantic relationships at multiple levels of granularity. In the scientific abstracts case study, it identifies research areas and subfields without requiring pre-existing categories. On benchmark datasets like 20 Newsgroups and IMDB 50k Movie Reviews, the approach works across different content types and domains, suggesting good generalizability. The nested density trees provide a meaningful organization of documents that aligns with intuitive topic groupings and sentiment categories, though exact quantitative metrics are not detailed in the abstract.

Conclusion: Nested density clustering in LLM embedding space can reveal hierarchical semantic organization in text collections, moving beyond flat similarity search and fixed taxonomies. The resulting nested density trees enable unsupervised, data-driven discovery of topics, research areas, and their substructures, and appear robust across scientific and non-scientific datasets. The authors suggest that this framework can support scientometrics, tracking topic evolution, and broader analyses of semantic structure and change in large text corpora.

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster - the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 News- groups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [49] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: ACT is a three-stage algorithm for automatically detecting and classifying complex biblical quotations in Rabbinic literature, outperforming existing text reuse systems.


<details>
  <summary>Details</summary>
Motivation: Existing text reuse and quotation detection systems perform poorly on short, paraphrased, morphologically varied, or structurally embedded quotations common in Rabbinic literature. There is also a methodological gap between exhaustive automated detection and nuanced human editorial judgment, especially for complex citation styles such as “Wave” and “Echo” quotations in morphologically rich languages. The authors aim to create a method that reliably finds and categorizes such quotations, enabling better intertextual and genre analysis.

Method: The authors propose ACT (Allocate Connections between Texts), a three-stage pipeline: (1) a morphology-aware alignment algorithm that can match words and phrases across inflectional variants and subtle paraphrases; (2) a context-sensitive enrichment stage that captures complex, non-contiguous quotation patterns like “Wave” and “Echo” citations; and (3) configurable n-gram and stylistic settings, resulting in multiple configurations (ACT-QE, ACT-2, ACT-3) to trade off recall vs. precision. They evaluate ACT against existing systems (Dicta, Passim, Text-Matcher) and human-annotated critical editions, and perform ablation-style comparisons across ACT variants to measure each component’s contribution.

Result: The full pipeline, ACT-QE, achieves an F1 score of 0.91, with Recall 0.89 and Precision 0.94, outperforming all baselines and competing systems. ACT-2, which omits stylistic enrichment, slightly improves Recall to 0.90 but at the cost of lower Precision, indicating more false positives. ACT-3, which relies on longer n-grams, provides an intermediate tradeoff between coverage and specificity. Overall, ACT detects more quotations and classifies them more accurately than existing frameworks, particularly for complex citation structures.

Conclusion: ACT effectively addresses limitations of current text reuse systems in detecting short, paraphrased, and structurally complex quotations in Rabbinic literature. Its strong performance against both automated baselines and human-edited texts shows that morphology-aware alignment combined with context-sensitive enrichment is a powerful approach. Beyond quotation detection, ACT’s capacity to classify stylistic citation patterns supports new research directions in genre classification and intertextual analysis. The framework lays groundwork for broader use in digital humanities and historical textual analysis, particularly for morphologically rich, citation-dense traditions like Aggadic literature.

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [50] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: The paper studies whether and how integrating visual generation into a unified vision-language model improves visual understanding at large pretraining scales.


<details>
  <summary>Details</summary>
Motivation: Vision-language large models are increasingly designed to handle both understanding (e.g., recognition, captioning, QA) and generation (e.g., image synthesis), but it is unclear on large-scale data whether the generation side actually helps the understanding performance or just adds complexity.

Method: They build and analyze a unified vision-language model called UniHetero with a concise architecture and pretrain it on more than 200 million samples. They compare variants that generate high-level semantic representations versus pixel-level outputs, examine data scaling behavior and data efficiency, and test an autoregressive mechanism applied directly on input embeddings to encode fine visual details.

Result: They observe that incorporating generation improves visual understanding only when the model is trained to generate semantic-level outputs instead of raw pixels. The unified model with semantic generation exhibits a better scaling trend with data size and makes more effective use of training data. Additionally, autoregressive modeling over input embeddings proves effective for capturing detailed visual information.

Conclusion: Unified vision-language models can benefit understanding by incorporating generative objectives, but the gains depend critically on predicting semantic content rather than pixels. Semantic generation improves scaling and data efficiency, and autoregression on input embeddings is a promising design for encoding fine-grained visual details in large-scale pretraining.

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [51] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: The paper proposes MoLaCE, an inference-time framework that mitigates confirmation bias in LLMs by mixing experts defined via different activation strengths over latent concepts, achieving debate-like benefits with lower compute.


<details>
  <summary>Details</summary>
Motivation: LLMs tend to exhibit input confirmation bias: they align with implied answers in prompts instead of exploring alternatives, which is harmful in both single-model use and especially in multi-agent debate systems that can form echo chambers. Existing methods do not adequately address this issue in a scalable, efficient way.

Method: Introduce Mixture of Latent Concept Experts (MoLaCE), where the model internally uses multiple experts formed by varying activation strengths over latent concepts relevant to the prompt. These experts correspond to different internal perspectives driven by how language composition reweights concepts. At inference time, MoLaCE mixes these experts’ outputs, effectively creating internal debate without spawning full separate models. It can also be plugged into multi-agent debate setups to decorrelate agent errors.

Result: Empirical evaluations show that MoLaCE consistently lowers confirmation bias and improves robustness of LLM outputs. Its performance matches or surpasses standard multi-agent debate techniques while using only a fraction of the computational resources.

Conclusion: MoLaCE offers an efficient, scalable way to mitigate confirmation bias in LLMs by leveraging latent concept expert mixtures, providing debate-like advantages within a single model and enhancing multi-agent debate systems through more diverse, less correlated reasoning.

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [52] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: The paper proposes a simple, model-agnostic hallucination self-detection method that converts LLM outputs into knowledge graphs and uses them to better judge if a response is hallucinated, outperforming existing self-detection baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently produce fluent but false statements (hallucinations), which undermines their safe deployment. Existing self-detection approaches work reasonably well but still miss many hallucinations and often do not exploit explicit structure in the content being checked. The authors are motivated to use structured knowledge representations—knowledge graphs—to help LLMs analyze and evaluate their own factual claims more reliably.

Method: The method has two main steps: (1) Convert an LLM’s textual response into a knowledge graph (KG) of entities and relations, i.e., decompose the answer into atomic factual triples. (2) Use this KG representation to estimate the likelihood that the response contains hallucinations, presumably by prompting or modeling over the structured triples, instead of only using the raw text. The approach is designed to be simple, low-cost, and model-agnostic, and is applied as a self-detection layer on top of existing LLMs such as GPT-4o and Gemini-2.5-Flash.

Result: In experiments on two hallucination detection datasets, using GPT-4o and Gemini-2.5-Flash as the base models, the proposed KG-based self-detection method outperforms standard self-detection baselines and SelfCheckGPT, a strong state-of-the-art method. The improvements reach up to 16% relative gain in accuracy and 20% in F1-score. Additionally, the authors curate and enhance one hallucination detection dataset and release it to support more robust future benchmarking.

Conclusion: Structuring LLM outputs as knowledge graphs enables more effective analysis of atomic facts and improves hallucination self-detection, even when the original answer itself is partially inaccurate. Because the approach is simple, low-cost, and model-agnostic, it offers a practical path toward safer, more trustworthy language models and better standardized evaluation of hallucination detection via the released dataset.

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [53] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: The paper shows that large vision-language models (LVLMs) lose some of their original instruction-following ability after standard visual instruction fine-tuning, and that adding explicit output-format instructions during training can mitigate this problem.


<details>
  <summary>Details</summary>
Motivation: While large language models are good at following instructions, their multimodal counterparts (LVLMs) often stop following task instructions reliably after being fine-tuned on visual instruction datasets. This harms their practical usefulness. The authors want to measure this degradation rigorously and understand why it happens, with the goal of improving LVLM training so they better preserve and use instruction-following skills.

Method: The authors quantitatively compare instruction-following performance of LVLMs before and after visual instruction fine-tuning using standard datasets. They then construct new training datasets that differ by whether they explicitly specify the desired output format in the instructions. By training LVLMs with and without these output-format specifications and evaluating them on benchmarks designed to measure instruction-following fidelity, they analyze how dataset design—especially explicit format guidance—affects the preservation of instruction-following behavior.

Result: They confirm that instruction-following ability consistently declines after fine-tuning LVLMs with commonly used visual instruction datasets. Models trained on datasets that explicitly state the required output format follow instructions more accurately than models trained without such explicit format guidance. Quantitative evaluations show measurable improvements in adherence to requested formats and instructions when format-aware data is used.

Conclusion: Standard visual instruction fine-tuning harms LVLMs’ inherited instruction-following capabilities, but this degradation can be mitigated. Incorporating training samples that clearly specify the desired output format helps preserve or improve instruction adherence. Therefore, future LVLM training pipelines should deliberately include explicit output-format instructions during (visual) instruction tuning to maintain strong instruction-following performance.

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [54] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: Paper identifies and analyzes “style amnesia” in spoken language models (SLMs): their inability to consistently maintain a requested speaking style over multi-turn conversations, even though they can recall the original instruction.


<details>
  <summary>Details</summary>
Motivation: Spoken language models are increasingly used in interactive, multi-turn scenarios (assistants, dialogue systems), where consistent paralinguistic style (emotion, accent, volume, speed) is crucial for user experience, accessibility, and brand/persona design. However, it is unclear whether current SLMs can maintain such styles reliably over many turns, especially when instructions are given only once at the beginning of the conversation or via system prompts, as intended by many frameworks. The authors aim to systematically test and characterize this behavior.

Method: The authors define and investigate a phenomenon they call “style amnesia,” where SLMs lose adherence to an initially specified speaking style as a conversation continues. They evaluate five SLMs (three proprietary, two open-source) on their ability to sustain four paralinguistic styles: emotion, accent, volume, and speaking speed. They run multi-turn conversational tests where a style instruction is given at the start and then track whether the style is preserved across turns. They also test whether models can recall the style instructions later when explicitly queried, and experiment with different prompting strategies, such as putting style instructions in user messages vs. system messages, and adding explicit recall prompts mid-conversation to see if that mitigates style amnesia.

Result: All evaluated SLMs show style amnesia: none maintain the requested paralinguistic style consistently across multiple turns. While the models can usually recall the original style instruction when asked explicitly, they fail to continuously realize that style in their spoken output. Prompting the model to restate or recall the style instruction later in the dialogue partially reduces, but does not eliminate, this degradation. Furthermore, the authors find that placing style instructions in system messages leads to worse style adherence than placing them in user messages, contrary to the conventional expectation that system prompts should have stronger control over model behavior.

Conclusion: The paper concludes that current spoken language models cannot reliably maintain paralinguistic speaking styles over multi-turn conversations, revealing a systematic “style amnesia” problem. This limitation persists even though models can remember the textual instructions themselves, indicating a gap between instruction memory and consistent style realization. Simple prompting strategies, including use of system messages or occasional recall requests, only partially address the issue. The findings call for architectural, training, or control-method improvements to enable robust, long-horizon style control in SLMs and caution practitioners who rely on system prompts for stable persona or style enforcement.

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [55] [Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing](https://arxiv.org/abs/2512.23611)
*Yuwen Li,Wei Zhang,Zelong Huang,Mason Yang,Jiajun Wu,Shawn Guo,Huahao Hu,Lingyi Sun,Jian Yang,Mingjie Tang,Byran Dai*

Main category: cs.CL

TL;DR: InfTool is a fully autonomous, multi-agent framework that teaches LLMs to call tools effectively using only raw API specs and synthetic trajectories, dramatically improving tool-use accuracy without human annotation.


<details>
  <summary>Details</summary>
Motivation: Reliable tool use is essential for autonomous LLM agents, but current methods depend on costly human-labeled trajectories, fail to generalize to new tools, and are limited by single-model data synthesis that preserves existing biases and gaps. The paper aims to build a system that can continuously and autonomously generate high-quality training data for tool use, generalize to unseen tools, and improve over time without human supervision.

Method: They propose InfTool, which uses three collaborating agents—a User Simulator, a Tool-Calling Assistant, and an MCP Server—to synthesize diverse tool-use trajectories given only raw API specifications. These agents run in a closed loop, producing both simple and complex multi-step tool workflows that are automatically verified. The synthetic trajectories are then used to fine-tune the base LLM with Group Relative Policy Optimization (GRPO) and gated reward signals. The improved model is reinserted into the loop to generate better and more targeted synthetic data, creating a self-evolving training cycle with no human-in-the-loop annotations.

Result: On the Berkeley Function-Calling Leaderboard (BFCL), InfTool boosts a 32B base model’s accuracy from 19.8% to 70.9%, a 258% relative improvement. This performance surpasses models that are ten times larger in parameter count and approaches the performance of strong proprietary systems like Claude-Opus, all using only synthetic data derived from the framework’s autonomous process.

Conclusion: Autonomous, multi-agent synthetic data generation paired with GRPO can drastically improve LLM tool-use capabilities without human annotation. InfTool overcomes prior bottlenecks of cost, generalization, and bias ceilings in single-model synthesis, demonstrating that self-evolving training loops can elevate mid-sized open models to near state-of-the-art tool-calling performance using only raw API specs and closed-loop synthetic trajectories.

Abstract: Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.

</details>


### [56] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: They create and release a new expert-annotated dataset (CHQ-Summ) for summarizing consumer health questions and benchmark it with modern summarization models.


<details>
  <summary>Details</summary>
Motivation: Consumer health questions on the web are often long, descriptive, and contain peripheral information, which makes natural language understanding and automatic processing difficult. While large datasets exist for other summarization tasks, there is no domain-expert annotated dataset specifically for summarizing consumer healthcare questions, limiting research and system performance in this area.

Method: They collect consumer health questions from a community question answering forum and have domain experts annotate concise summaries for each question, forming the CHQ-Summ dataset of 1507 question–summary pairs. They then evaluate (“benchmark”) this dataset using several state-of-the-art text summarization models to assess performance and demonstrate its utility.

Result: The outcome is a curated dataset of 1507 consumer health questions with corresponding expert-written summaries, plus benchmarking results showing how existing summarization models perform on this new task/dataset.

Conclusion: CHQ-Summ fills a key resource gap for consumer health question summarization, enabling better modeling and understanding of health-related queries on social media and providing a basis for future improvements in domain-specific summarization systems.

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, con- sumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [57] [Nested Browser-Use Learning for Agentic Information Seeking](https://arxiv.org/abs/2512.23647)
*Baixuan Li,Jialong Wu,Wenbiao Yin,Kuan Li,Zhongwang Zhang,Huifeng Yin,Zhengwei Tao,Liwen Zhang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: NestBrowse is a new framework that lets information-seeking agents use the web like a real browser, but with simplified, nested actions that make deep web search both more powerful and easier to control.


<details>
  <summary>Details</summary>
Motivation: Existing information-seeking agents mainly call APIs to retrieve snippets or fetch full pages via URLs. This limits them to surface-level content and prevents them from exploiting richer, interactive information available through real browser use. Full browser control, however, is complex: fine-grained actions and verbose page content overwhelm ReAct-style, function-calling agents. The paper aims to remove this bottleneck so agents can perform deeper, more interactive web searches without being crippled by control complexity.

Method: The authors design Nested Browser-Use Learning (NestBrowse), a minimal but complete browser-action framework with a nested structure. It explicitly decouples high-level interaction control (deciding what to do next) from low-level page exploration (navigating and extracting information inside a page). This reduces the reasoning burden on the agent while preserving the expressivity needed for real browsing. They integrate NestBrowse into information-seeking agents and evaluate it on challenging deep information-seeking benchmarks, comparing against standard API-based and URL-fetching approaches. They also run ablation and efficiency analyses to understand how the nested design affects performance and flexibility.

Result: On difficult deep information-seeking benchmarks, agents using NestBrowse outperform baseline agents restricted to API-level retrieval or simple URL-based page fetching. The framework enables more effective acquisition of deep-web information, and empirical analyses show that the nested separation of control vs. exploration leads to better efficiency and adaptable behavior across tasks.

Conclusion: By introducing a compact, nested browser-action interface, NestBrowse allows agents to harness the full power of browser-based interaction without incurring overwhelming complexity in reasoning and tool use. This yields practical gains in deep information-seeking tasks and demonstrates that carefully structured browser control is a key ingredient for scalable, capable web agents.

Abstract: Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.

</details>


### [58] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: The paper examines how much linguistic context is needed to relate language model probabilities to cognitive phenomena, finding that shorter n-gram contexts may be sufficient.


<details>
  <summary>Details</summary>
Motivation: To determine the appropriate size of linguistic context when using language model probabilities to study human cognitive processes, such as how people plan and produce speech.

Method: The authors compare the use of whole utterances versus shorter n-gram segments as contextual units in analyzing probabilistic reduction effects, assessing how well each representation captures the relationship between language model probabilities and cognitive measures.

Result: They find that it is not necessary to use entire utterances; n-gram representations are sufficient to serve as cognitive units of planning for capturing probabilistic reduction.

Conclusion: The amount of context required to link language model probabilities with cognitive phenomena can be limited to n-gram-sized units, suggesting that cognitive planning may operate over relatively local linguistic contexts rather than whole utterances.

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [59] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: The paper investigates how hidden prompt injection attacks inside academic papers affect large language model (LLM)-based peer review systems, showing strong vulnerabilities that vary by injection language.


<details>
  <summary>Details</summary>
Motivation: As LLMs start being used in high-stakes tasks such as academic peer review, it is crucial to understand their susceptibility to document-level prompt injection, where malicious instructions are hidden within the text and may alter the model’s behavior and judgments. Existing work has focused mostly on prompt injection in short or obvious contexts rather than in long, realistic documents across multiple languages.

Method: The authors collect a dataset of about 500 authentic ICML-accepted papers. For each paper, they embed hidden adversarial instructions that are semantically equivalent but written in four languages: English, Japanese, Chinese, and Arabic. They then use an LLM to generate peer reviews and scores for these papers, comparing outcomes with and without the hidden prompts, and across languages, to quantify the effect on review scores and accept/reject decisions.

Result: The study finds that hidden prompts in English, Japanese, and Chinese significantly change the LLM’s review scores and even flip accept/reject decisions, indicating pronounced vulnerability to document-level prompt injection. In contrast, Arabic prompt injections cause minimal or no measurable changes in the LLM’s review behavior, revealing strong language-dependent differences in susceptibility.

Conclusion: LLM-based peer-review pipelines are highly vulnerable to hidden prompt injection when adversarial instructions are embedded in the reviewed documents, and the degree of vulnerability varies considerably by language. This underscores the need for robust defenses and language-aware safeguards before deploying LLMs in critical evaluation workflows such as academic peer review.

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [60] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: The paper introduces ProfASR-Bench, a benchmark for evaluating automatic speech recognition in high-stakes professional domains, focusing on entity accuracy and the limited impact of textual prompts on current models.


<details>
  <summary>Details</summary>
Motivation: Existing ASR benchmarks do not adequately stress-test models on dense domain terminology, formal register variation, and extremely low tolerance for entity errors, which are crucial in professional settings like finance, medicine, law, and technology. The authors aim to fill this gap by providing a benchmark tailored to these high-stakes scenarios and to systematically study how well ASR systems use available textual context.

Method: The authors construct ProfASR-Bench, a multi-domain evaluation suite of professional talks where each audio sample is paired with a natural-language prompt (domain cue and/or speaker profile) and an entity-rich target transcript. They define evaluation protocols that include standard ASR metrics (e.g., WER), entity-aware scores, and slice-wise breakdowns by accent and gender. They then evaluate representative systems—Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models)—under several contextual conditions: no context, profile only, domain+profile, oracle prompts, and adversarial prompts, to examine how prompts affect recognition performance.

Result: Across model families and conditions, lightweight textual context, including oracle prompts, produces minimal changes in average word error rate, and adversarial prompts also fail to reliably worsen recognition, indicating that current systems do not effectively exploit or respond to the provided context. The benchmark enables detailed reporting with confidence intervals and slice-wise analyses, and supports comparisons of different context-fusion strategies.

Conclusion: The study identifies a "context-utilization gap" in modern ASR and audio language models: despite being promptable, they make little effective use of side information such as domain cues and speaker profiles. ProfASR-Bench is proposed as a standardized, reproducible testbed for measuring and improving context-conditioned recognition, particularly with respect to entity accuracy and fairness-related slices like accent and gender.

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [61] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: Method and dataset for fine-tuning language models using feedback-driven improvement chains, where annotators mark liked/disliked spans and models iteratively rewrite disliked parts, yielding better preference tuning than standard A/B ranking.


<details>
  <summary>Details</summary>
Motivation: Traditional preference-based fine-tuning often relies on coarse A/B rankings or full-response comparisons, which fail to capture fine-grained, localized preferences and can be inefficient in teaching models how to improve specific parts of their outputs. The authors aim to provide more structured, targeted supervision that reflects how humans actually revise text: through incremental edits guided by explicit feedback on specific spans.

Method: Annotators read a model response and highlight fine-grained liked and disliked spans, adding short explanations of what is good or bad about each. Starting from the leftmost disliked span, the base model rewrites only that portion based on the feedback, while preserving the rest, then proceeds sequentially through remaining disliked spans to form a chain of incremental revisions. Each adjacent pair of responses in this chain is treated as a preference pair (later step preferred over earlier) for direct preference alignment, so the model learns from many localized, targeted edits instead of just holistic comparisons.

Result: Models trained on these feedback-driven improvement chains outperform models trained with standard A/B ranking or full contrastive rewrites on preference-based alignment benchmarks. The approach yields more efficient learning, requiring fewer examples to achieve better or comparable quality, and improves the model’s ability to make precise, localized edits that align with human preferences.

Conclusion: Structured, revision-based supervision via feedback-driven improvement chains is a more effective way to fine-tune language models to human preferences than conventional direct alignment with coarse preferences. By decomposing feedback into span-level annotations and incremental rewrites, the method offers finer control, improved sample efficiency, and better overall alignment performance, suggesting that future preference-tuning pipelines should mirror human editing workflows rather than relying solely on global A/B comparisons.

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [62] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: The paper studies how to automatically find prompts that trigger specific, often problematic, behaviors in large language models within multi-turn conversations, compares different elicitation strategies, and shows online methods are far more effective than static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing work on behavior elicitation mostly focuses on single-turn prompts and cannot fully capture complex behaviors that only emerge in multi-turn conversations. Traditional multi-turn benchmarks are static and often fail to surface many failure cases. There is a need for a systematic framework to understand elicitation methods, extend them to multi-turn setups, and quantify their efficiency in discovering problematic behaviors under limited query budgets.

Method: The authors: (1) define an analytical framework that groups behavior elicitation methods into three families based on how they interact with the target LLM: prior-knowledge-based, offline-interaction-based, and online-interaction-based; (2) generalize the online elicitation approach to a unified multi-turn setting, so single-turn and multi-turn elicitation are treated within one formalism; and (3) empirically evaluate all three families on the task of automatically generating multi-turn test cases, measuring the trade-off between query budget (number of model queries) and success rate (fraction of discovered behavior-eliciting inputs) across several tasks.

Result: Online elicitation methods are substantially more effective than static or offline approaches for multi-turn conversations. With only a few thousand queries, online methods achieve average success rates of 45%, 19%, and 77% across three tasks, while static methods built from existing multi-turn benchmarks discover very few or no failure cases at all. This demonstrates that dynamic, query-efficient elicitation can uncover many more problematic behaviors than traditional, fixed benchmarks.

Conclusion: Behavior elicitation techniques are powerful tools for evaluating large language models in realistic multi-turn conversational settings, especially when implemented with online interaction. Static, pre-defined multi-turn benchmarks are insufficient for reliably surfacing complex or rare behaviors. The work argues that the evaluation community should transition toward dynamic, behavior-elicitation-based benchmarks that adapt via interaction with the target model.

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: The paper proposes Bidirectional RAG, a retrieval-augmented generation framework that lets the system safely write high-quality generated answers back into the knowledge corpus, enabling self-improvement over time.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems rely on a static external corpus that does not grow from user interactions or model outputs. This limits their ability to accumulate new knowledge and improve coverage. Naively adding generated text back into the corpus risks hallucination pollution and degraded reliability, so a principled, safe way to let RAG systems learn from deployment is needed.

Method: The authors design Bidirectional RAG, where generated answers can be written back into the retrieval corpus only after passing a multi-stage acceptance layer. This layer uses grounding verification via NLI-based entailment checks against retrieved evidence, attribution checking to ensure claims are supported, and novelty detection to ensure that only genuinely new, non-duplicative information is stored, reducing hallucinations and redundancy.

Result: On four QA datasets (Natural Questions, TriviaQA, HotpotQA, Stack Overflow) with three random seeds (12 runs per system), Bidirectional RAG attains an average coverage of 40.58%, almost twice that of Standard RAG at 20.33%, while requiring far fewer added documents than a naive write-back approach (140 vs. 500), indicating better efficiency and quality in corpus expansion.

Conclusion: Self-improving RAG systems are practically achievable if corpus expansion is strictly regulated by robust validation. Bidirectional RAG offers a safe and efficient mechanism for RAG systems to learn from deployment by selectively incorporating only well-grounded, novel generated content into the knowledge base.

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [64] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: The paper investigates when large language models will try to persuade users without being explicitly asked, especially after being steered or fine-tuned toward certain personas.


<details>
  <summary>Details</summary>
Motivation: As conversational AI becomes widespread and influential, there is growing concern that models may subtly shape users’ opinions and behaviors, including in harmful ways. Prior work focused on explicit misuse, where someone directly prompts a model to persuade. This paper is motivated by a more subtle and potentially more dangerous scenario: models that begin to persuade users spontaneously, without explicit requests, and how training or steering might create such behavior unintentionally.

Method: The authors compare two main intervention types on LLMs: (1) internal activation steering to push the model toward specific persona traits (both persuasion-related and unrelated), and (2) supervised fine-tuning (SFT) to make the model consistently exhibit those traits. They then systematically measure how often and how strongly the models try to persuade users in the absence of explicit persuasion prompts, including across both benign and controversial/harmful topics.

Result: They find that activation steering toward persona traits does not consistently increase unprompted persuasive behavior. In contrast, supervised fine-tuning on those traits does lead to a higher tendency for models to persuade users unprompted. Additionally, models fine-tuned only on benign persuasion data show increased persuasion tendencies even on controversial and harmful topics, indicating that harmful persuasive capabilities can emerge indirectly from seemingly safe training data.

Conclusion: Unprompted persuasion in LLMs is not reliably induced by simple activation steering but does emerge after supervised fine-tuning on persuasive traits, even when training data covers only benign topics. This suggests that safety evaluations must consider emergent persuasion risks from benign SFT and that existing training practices could unintentionally increase a model’s capacity to persuade users on harmful or controversial issues. Further dedicated study and mitigation strategies for emergent harmful persuasion are required.

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [65] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: The paper introduces GamiBench, an origami-inspired benchmark to evaluate multimodal large language models’ spatial reasoning and 2D-to-3D planning abilities via folding tasks and multi-view visual QA.


<details>
  <summary>Details</summary>
Motivation: While multimodal LLMs are good at perception and following instructions, they perform poorly on spatial reasoning, particularly tracking and manipulating objects over views and time. Existing benchmarks mainly use static images and final-answer evaluations, ignoring the inherently sequential, viewpoint-dependent nature of spatial reasoning. There is a need for a benchmark that captures dynamic 2D-to-3D reasoning and provides process-level diagnostics.

Method: The authors design GamiBench, a benchmark based on origami-like 2D crease patterns and their corresponding 3D folded shapes. It contains 186 regular and 186 impossible crease patterns, each rendered from six viewpoints and used in three VQA-style tasks: (1) predicting 3D fold configurations, (2) identifying valid viewpoints, and (3) detecting impossible crease patterns. The benchmark evaluates complete reasoning chains instead of only final predictions, incorporating cross-view consistency checks, physical-feasibility assessment via impossible-fold detection, and understanding of intermediate folding steps. New metrics—viewpoint consistency (VC) and impossible fold selection rate (IFSR)—quantify model robustness across fold complexities.

Result: Experiments with state-of-the-art multimodal models, including GPT-5 and Gemini-2.5-Pro, reveal that they struggle even on single-step spatial reasoning tasks within GamiBench, indicating current MLLMs lack robust geometric understanding and 2D-to-3D planning skills.

Conclusion: GamiBench provides a standardized, fine-grained framework to evaluate and diagnose spatial reasoning and geometric understanding in multimodal LLMs through origami-inspired tasks. The observed failures of leading models highlight a significant gap between current MLLM capabilities and human-like spatial reasoning, and the benchmark plus its metrics offer a foundation for future research aimed at closing this gap.

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [66] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: A fairness-aware AI model for post-flood aid in Bangladesh uses adversarial debiasing to prioritize regions based on real vulnerability while reducing regional and rural/urban bias, achieving more equitable and still-accurate aid allocation rankings.


<details>
  <summary>Details</summary>
Motivation: Post-disaster aid in developing countries is often biased toward historically favored or less vulnerable regions, reinforcing inequities and leaving marginalized districts under-supported. There is a need for systematic, data-driven tools that can both predict vulnerability and actively correct historical allocation biases so that humanitarian aid reaches those most in need.

Method: The paper builds an adversarial debiasing framework that predicts flood vulnerability for administrative units in Bangladesh while discouraging the model from encoding protected or sensitive regional attributes. It adapts fairness-aware representation learning from healthcare AI, using a gradient reversal layer so that the main predictor learns vulnerability-relevant but bias-invariant representations. The model is trained on real 2022 flood data for 87 upazilas across 11 districts, and evaluated on both predictive performance (R-squared) and fairness metrics like statistical parity difference and regional fairness gaps, then used to produce aid-priority rankings.

Result: The proposed model substantially reduces unfairness while preserving most of the predictive power: statistical parity difference is reduced by 41.6%, regional fairness gaps by 43.2%, while R-squared drops only slightly from 0.811 in the baseline to 0.784. The system generates revised vulnerability scores and priority rankings that favor genuinely high-need regions rather than those historically prioritized.

Conclusion: Algorithmic fairness methods, particularly adversarial debiasing with gradient reversal, can be successfully transferred from domains like healthcare to humanitarian disaster management. The resulting system offers decision-makers a practical tool to design more equitable flood aid allocation policies in Bangladesh, improving fairness without sacrificing much accuracy, and providing a template for fair aid distribution in other disaster-prone developing contexts.

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [67] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: The paper introduces the Agentic Risk & Capability (ARC) Framework, a technical governance approach to systematically identify, assess, and mitigate risks from agentic AI systems, enabling safer organizational deployment and innovation.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems can autonomously execute code, interact with the internet, and modify files. These powerful capabilities create substantial and evolving risks that are hard for organizations to govern effectively. Existing governance approaches are not well-tailored to the distinctive, capability-driven risks of such systems, motivating a structured, technical framework to map risks to controls in a way that organizations can actually implement.

Method: The authors design the ARC Framework as a capability-centric technical governance framework. They decompose agentic AI risk into three intrinsic sources—system components, system design, and system capabilities—and map each to concrete materialized risks and technical controls. They then organize these mappings into a structured, step-by-step process that organizations can follow to analyze their agentic systems and apply appropriate mitigations.

Result: The result is the ARC Framework: (1) a new way to analyze diverse agentic AI systems through their capabilities; (2) a taxonomy of three primary risk sources (components, design, capabilities); (3) explicit linkages from those sources to concrete risks and corresponding technical controls; and (4) an implementation-oriented process that organizations can operationalize. The framework is released as open-source resources for practical adoption.

Conclusion: The paper concludes that the ARC Framework offers a robust, adaptable, and practical methodology for governing agentic AI systems. By focusing on capabilities and systematically connecting risk sources to controls, it allows organizations to innovate quickly with agentic AI while maintaining safety, security, and responsibility. The open-sourced nature of the framework is intended to facilitate broader use and continuous improvement.

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [68] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: Humans are only slightly better than chance at distinguishing challenging AI-generated images from real photos.


<details>
  <summary>Details</summary>
Motivation: Many people are confident they can tell AI-generated images from real ones, but this belief had not been rigorously tested with modern, high-quality generative models on realistic tasks.

Method: The authors built an interactive web experiment where participants classified 20 images per session as real or AI-generated. They constructed a dataset of 120 particularly difficult portrait images: real photos from CC12M and visually similar AI-generated images from MidJourney. A total of 165 users completed 233 sessions. The study measured accuracy, response times, learning across repeated attempts, and image-wise difficulty.

Result: Participants achieved an average classification accuracy of 54%, only marginally above random guessing (50%), and showed limited performance improvement over repeated sessions. Average response time was 7.3 seconds per decision, and some images were consistently more deceptive than others across users.

Conclusion: Humans struggle to reliably detect AI-generated portrait images, even under relatively simple conditions, and their performance is barely above chance. As synthetic media quality improves, human visual judgment alone is inadequate for distinguishing real from artificial images. This underlines the need for awareness, tools, and ethical guidelines to manage the societal impact of increasingly indistinguishable AI-generated media.

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [69] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: The paper proposes Logic Sketch Prompting (LSP), a prompting framework that makes LLM outputs more rule-compliant, deterministic, and auditable, and shows it outperforms common prompting baselines on pharmacologic decision tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs are good at natural language reasoning but unreliable when strict rule following, determinism, and auditability are required, which is crucial in safety-critical domains like clinical decision support. The paper aims to address this gap.

Method: Introduce Logic Sketch Prompting (LSP), which uses typed variables, deterministic condition evaluators, and a rule-based validator to structure model reasoning. Benchmark LSP against zero-shot, chain-of-thought, and concise prompting on two pharmacologic logic compliance tasks using three open-weight models (Gemma 2, Mistral, Llama 3). Evaluate using accuracy, F1 scores, and McNemar tests for statistical significance.

Result: Across both tasks and all three models, LSP yields the highest performance with accuracy and F1 in the 0.83–0.89 range, clearly surpassing zero-shot prompting (0.24–0.60), concise prompts (0.16–0.30), and chain-of-thought prompting (0.56–0.75). Statistical tests (McNemar, p < 0.01) confirm the improvements are significant in almost all comparisons.

Conclusion: Logic Sketch Prompting significantly improves LLM determinism, interpretability, and rule compliance while maintaining or improving predictive performance. This makes it particularly suitable for clinical, regulated, and safety-critical decision support applications where traceability and consistency are required.

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [70] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit is an open-source, unified toolkit for benchmarking AI models on diverse, expert-grade scientific tasks across multiple disciplines and capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation platforms are typically general-purpose and do not adequately capture the specialized, core competencies needed for AI in scientific domains. There is a need for a standardized yet flexible way to assess scientific intelligence—across modalities, reasoning types, and disciplines—using realistic, domain-specific benchmarks so that progress in AI4Science can be measured and compared fairly.

Method: The authors design SciEvalKit as a unified benchmarking toolkit focused on scientific AI capabilities. They define key categories of scientific intelligence (multimodal perception, reasoning, understanding, symbolic reasoning, code generation, hypothesis generation, and knowledge understanding) and collect expert-grade benchmarks from real-world, domain-specific datasets across six scientific domains including physics, chemistry, astronomy, and materials science. They implement a flexible, extensible evaluation pipeline that supports batch evaluation, integration of custom models and datasets, and standardized scoring to ensure transparency, reproducibility, and comparability of results.

Result: SciEvalKit provides a working, open-sourced toolkit that can evaluate AI models on a wide spectrum of scientific tasks and domains. It successfully supports batch evaluations across models and datasets, allows easy extension with new models and benchmarks, and yields transparent, comparable evaluation metrics tailored to scientific competencies. This infrastructure demonstrates that capability-based, discipline-diverse evaluation can be practically realized in a single framework.

Conclusion: SciEvalKit establishes a standardized yet customizable infrastructure for evaluating scientific foundation models and intelligent agents, filling a gap left by general-purpose benchmarks. By coupling capability-centric evaluation dimensions with disciplinary diversity and by being open-sourced and actively maintained, it is positioned to become a community-driven platform that advances the development and fair assessment of AI systems for science.

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [71] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World is a multi-agent, tool-using framework that generates and validates symbolic world models (like PDDL domains or simulators) via interactive testing, and also produces training data that significantly improves LLM world-model generation.


<details>
  <summary>Details</summary>
Motivation: Training LLMs to generate accurate symbolic world models for model-based planning is hard because there is little large-scale, verifiable supervision, and existing static validation methods miss behavior-level errors that only appear during interactive execution. The authors want a system that can both perform better at inference time and create high-quality supervision data by grounding world-model generation in executable behavior and feedback.

Method: They design Agent2World, a three-stage, tool-augmented multi-agent framework: (1) a Deep Researcher agent performs web search and knowledge synthesis to fill in gaps in the task/domain specification; (2) a Model Developer agent uses this information to implement executable symbolic world models (in PDDL and code-based simulators); (3) a Testing Team of specialized agents performs adaptive unit tests and simulation-based validation, providing behavior-aware feedback and generating multi-turn interaction trajectories. These trajectories are then used for supervised fine-tuning of the LLM world-model generator.

Result: Agent2World achieves state-of-the-art inference-time performance on three benchmarks covering both PDDL and executable-code world models. The interactive Testing Team produces rich, behavior-based feedback that forms training trajectories; fine-tuning a model on these data yields an average relative improvement of 30.95% in world-model generation quality over the same base model without this training.

Conclusion: Grounding world-model generation in an interactive, tool-augmented multi-agent loop with adaptive testing both improves immediate performance and provides high-quality supervision for further training. Agent2World’s approach of combining research, model development, and behavioral validation yields robust symbolic world models and substantially boosts LLM capabilities in world-model generation across multiple benchmarks.

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [72] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: The paper tackles numeric planning with continuous control parameters by compiling a tractable subclass into standard simple numeric planning, enabling the use of existing numeric heuristics despite an infinite action space.


<details>
  <summary>Details</summary>
Motivation: Standard numeric planning assumes a finite, discrete set of ground actions. When actions have free numeric control parameters, there can be infinitely many applicable actions in a given state, which breaks existing heuristic techniques that rely on enumerating or structurally reasoning about actions. The authors want to extend heuristic planning techniques to this richer and more realistic setting while retaining computational tractability.

Method: They first characterize a tractable subclass of numeric planning problems with control parameters, called controllable, simple numeric problems. For this subclass, they introduce an optimistic compilation that converts control-parameterized actions into a standard simple numeric planning model. The compilation abstracts away control-dependent numeric expressions by replacing them with bounded constant effects and by relaxing some preconditions, carefully preserving admissible or informative heuristic estimates. This yields a finite-action representation amenable to standard subgoaling (relaxed-plan based) numeric heuristics.

Result: The compiled problems can be solved using off-the-shelf numeric planners with subgoaling heuristics, and experiments show that heuristic estimates remain informative and that planning remains computationally feasible, even though the original domain allowed infinitely many actions via control parameters. Performance improves over naive baselines or uncompiled formulations, and the approach successfully handles instances that were previously intractable for existing numeric planners.

Conclusion: By introducing controllable, simple numeric problems and providing an optimistic compilation to simple numeric tasks, the paper shows that traditional heuristic numeric planning techniques can be extended to handle domains with control parameters and infinitely many possible actions. This pushes the scalability and applicability of numeric planning, suggesting that similar compilation strategies might make other rich control-parameterized models accessible to existing planning technology.

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [73] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: The paper introduces HalluMatData, a benchmark dataset, and HalluMatDetector, a detection framework, to identify and reduce hallucinations in LLM-generated materials science content, achieving a 30% reduction in hallucination rates and proposing a new consistency metric PHCS.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are increasingly used in scientific discovery and materials science, but hallucinations—factually incorrect or misleading outputs—threaten research reliability and integrity. There is a lack of domain-specific benchmarks and systematic tools to evaluate and mitigate such hallucinations in materials science content.

Method: The authors build HalluMatData, a benchmark dataset of AI-generated materials science content annotated for hallucination, factual consistency, and robustness. They design HalluMatDetector, a multi-stage hallucination detection framework combining intrinsic verification of model outputs, multi-source information retrieval, contradiction graph analysis to capture conflicting statements, and metric-based assessment. They also define the Paraphrased Hallucination Consistency Score (PHCS) to measure how consistently an LLM responds to semantically equivalent queries.

Result: Experiments show hallucination rates differ markedly across materials science subdomains, with high-entropy (more open-ended or uncertain) queries producing more factual inconsistencies. Using the HalluMatDetector verification pipeline reduces hallucination rates by 30% relative to unverified, standard LLM outputs. PHCS effectively captures inconsistencies across paraphrased queries, highlighting reliability differences between models and query types.

Conclusion: HalluMatData and HalluMatDetector provide a domain-specific benchmark and a practical framework for detecting and mitigating hallucinations in materials science applications of LLMs. The 30% reduction in hallucinations and the introduction of PHCS demonstrate that structured verification and consistency analysis can significantly improve the factual reliability of AI-assisted scientific workflows, and the approach can guide safer deployment of LLMs in materials research.

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [74] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias personalizes frozen knowledge graph foundation models at inference time using tiny, interpretable, per-entity bias adaptations, improving user-level alignment without hurting overall performance.


<details>
  <summary>Details</summary>
Motivation: Foundation models on knowledge graphs perform well on average link prediction but do not model individual user preferences well, creating a gap between global relational reasoning and personalized ranking. There is a need for a way to personalize these strong global models without retraining large embeddings or sacrificing their cohort-level performance.

Method: They introduce GatedBias, an inference-time personalization framework. It keeps the KG foundation model frozen and adds a very small number (~300) of trainable parameters per user. It uses structure-gated adaptation: user profile-specific features are combined with graph-derived binary gates to compute interpretable per-entity bias terms that adjust scores or rankings. This is done as a lightweight post-hoc layer rather than retraining the base model.

Result: On Amazon-Book and Last-FM benchmarks, GatedBias yields statistically significant gains in personalization/alignment metrics while maintaining the original cohort-level link prediction performance. Counterfactual perturbation tests show entities associated with certain preference signals improve their ranks 6–30x more when those signals are boosted, suggesting the method is causally responsive to preference features rather than spurious correlations.

Conclusion: Inference-time, bias-based personalization of KG foundation models is feasible, parameter-efficient, and does not degrade global performance. By using structure-gated, interpretable bias adaptations, GatedBias can bridge general-purpose KG representations and individual user needs, offering a practical path to personalized recommendations from frozen foundation models.

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [75] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper proposes Monadic Context Engineering (MCE), an algebraic, monad-based architecture for building more robust, composable LLM-based agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent architectures are mostly ad hoc and imperative, making them brittle and hard to manage, especially around state, errors, and concurrency. The authors want a principled, formal foundation to design reliable, composable autonomous agents.

Method: They introduce Monadic Context Engineering (MCE), modeling agent workflows as monadic computational contexts. Using Functors, Applicative Functors, Monads, and Monad Transformers, they structure sequential and parallel compositions, state propagation, error handling, and asynchronous execution. They extend this to meta-agents that generate and orchestrate sub-agents via metaprogramming within the same algebraic framework.

Result: MCE provides a layered abstraction in which sequential logic uses monads, parallelism uses applicatives, and combined behaviors are composed via monad transformers. This yields agent systems built from small, independently verifiable components that still support complex behaviors like concurrent tool use and dynamic sub-agent orchestration.

Conclusion: Algebraic abstractions from functional programming, particularly monads and related structures, can serve as a rigorous architectural basis for LLM-based agents, improving robustness, composability, and scalability. Meta-agents built with MCE can manage dynamic, generative agent workflows more systematically than ad hoc designs.

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [76] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: The paper presents DarkPatterns-LLM, a fine-grained benchmark and diagnostic framework to evaluate manipulative content produced by LLMs across multiple harm dimensions, revealing current models’ weaknesses in detecting autonomy-undermining patterns.


<details>
  <summary>Details</summary>
Motivation: LLMs can inadvertently or intentionally generate manipulative or deceptive content that threatens user autonomy, trust, and well-being. Existing safety benchmarks use coarse binary labels (safe/unsafe) and lack the psychological and social nuance needed to identify how manipulation actually occurs. The authors aim to fill this gap with a richer, more structured way to measure and diagnose manipulative behaviors in LLM outputs.

Method: The authors construct DarkPatterns-LLM, a benchmark dataset and framework centered on seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. They design a four-layer analytical pipeline: (1) Multi-Granular Detection (MGD) for identifying manipulative patterns at different text levels; (2) Multi-Scale Intent Analysis (MSIAN) to infer and characterize manipulative intent; (3) Threat Harmonization Protocol (THP) to align and standardize threat/harm categorizations across examples; and (4) Deep Contextual Risk Alignment (DCRA) to factor in broader context and risk severity. The dataset consists of 401 expert-annotated instruction–response pairs labeled according to this schema. They then evaluate several state-of-the-art LLMs (GPT-4, Claude 3.5, LLaMA-3-70B) on this benchmark and compare performance.

Result: The benchmark reveals substantial performance gaps among leading LLMs, with detection accuracies ranging from about 65.2% to 89.7%. Across models, there is a systematic weakness in recognizing autonomy-undermining manipulative patterns. The dataset and framework provide quantitative metrics and qualitative diagnostics that expose where and how models fail to identify different types of manipulative harm.

Conclusion: DarkPatterns-LLM offers the first standardized, multi-dimensional evaluation framework specifically tailored to manipulation detection in LLM outputs. By moving beyond binary safety labels to a rich taxonomy and analytical pipeline, it enables more precise auditing and debugging of manipulative behaviors. The work is positioned as a foundational resource for building and tuning more trustworthy AI systems that better protect user autonomy and reduce psychological, economic, and societal harms.

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [77] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: The paper develops an AI-assisted machine-learning potential to simulate aluminum nanoparticle oxidation at quantum accuracy and very large scales, revealing a dual-mode, temperature-dependent oxidation mechanism dominated by outward aluminum diffusion.


<details>
  <summary>Details</summary>
Motivation: To understand the atomic-scale mechanisms by which passivated aluminum nanoparticles transition to highly reactive, explosive states, overcoming the limitations of ab initio and empirical simulations that either lack scale or reactive fidelity.

Method: They build a machine learning interatomic potential within a closed-loop "human-in-the-loop" framework, where self-auditing AI agents identify and visualize model artifacts for expert correction. This MLP reaches near-DFT accuracy but scales to million-atom, nanosecond simulations of aluminum nanoparticle combustion.

Result: The simulations identify a temperature-dependent dual oxidation mode: at moderate temperatures, the oxide shell regulates oxidation via transient nanochannels ("breathing mode"), while above a critical temperature the shell catastrophically fails ("rupture mode"). They also show that aluminum cation outward diffusion, not oxygen diffusion, dominates mass transport across the shell, with Al diffusion coefficients 2–3 orders of magnitude larger than oxygen's at all temperatures.

Conclusion: The work provides a unified atomic-scale picture of aluminum nanoparticle oxidation and combustion, resolving long-standing debate about the primary diffusing species and offering a computational framework to rationally design energetic nanomaterials with tunable ignition sensitivity and energy release profiles.

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [78] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: The paper argues that current large language models, built mainly on next-token prediction, miss key elements present in predictive coding theories of the brain—namely action, hierarchical compositional structure, and episodic memory—and proposes integrating these to achieve safer, more human-like, and interpretable AI.


<details>
  <summary>Details</summary>
Motivation: Although LLMs have made phenomenal progress using simple next-token prediction, they still exhibit issues like hallucinations, lack of grounding, weak sense of agency, limited interpretability, and energy inefficiency. Neuroscience’s predictive coding frameworks suggest additional ingredients—action integration, compositional hierarchy, and episodic memory—that could address these shortcomings, motivating a re-alignment of AI architectures with brain-inspired principles.

Method: This is a conceptual/theoretical paper. It synthesizes evidence from neuroscience and cognitive science about predictive coding and its components (actions, hierarchical composition, episodic memory), analyzes current foundation-model practices (e.g., CoT, RAG), and proposes an augmented architecture where actions at multiple abstraction levels are tightly coupled with a compositional generative model and an episodic memory system.

Result: The paper identifies three critical components missing from today’s foundation models and articulates how adding them could mitigate specific problems: grounding and hallucinations via action and interaction, agency and responsibility via controllable action policies, interpretability and safety via explicit hierarchical generative structure, and better efficiency and performance via episodic memory. It also clarifies how current add-ons like CoT and RAG only partially address these needs compared with a more integrated, brain-inspired approach.

Conclusion: To build safe, interpretable, energy-efficient, and human-centered AI, foundation models should evolve from pure next-token predictors to systems that integrate actions, hierarchical compositional generative architectures, and episodic memory, inspired by predictive coding models of the brain. Renewed cross-fertilization between brain science and AI research is argued to be essential for this transition.

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [79] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: The paper introduces SANet, a semantic-aware Agentic AI networking architecture for wireless networks, enabling multiple specialized AI agents to collaboratively manage and optimize networks in a decentralized way, with efficiency and performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional network management struggles with real-time, autonomous optimization in highly dynamic and complex wireless environments. Agentic AI networking (AgentNet) offers a promising paradigm where many specialized AI agents collaborate, but decentralization introduces challenges because agents can have different or conflicting objectives. There is a need for an architecture that can infer user goals, coordinate heterogeneous agents across protocol layers, and optimize them under multiple, potentially conflicting objectives while remaining computationally efficient.

Method: The authors propose SANet, a semantic-aware AgentNet architecture that infers the user's semantic goal and automatically assigns appropriate agents across different network layers to achieve it. They formulate the decentralized optimization of SANet as a multi-agent multi-objective optimization problem and focus on finding Pareto-optimal solutions for agents with distinct and possibly conflicting goals. They design three new evaluation metrics tailored to SANet and introduce a model partition and sharing (MoPS) framework, where large AI models for different agents are split into shared and agent-specific parts based on local computational resources. Two decentralized optimization algorithms are developed, with theoretical analysis establishing bounds and a three-way tradeoff among optimization error, generalization error, and conflicting error. They implement an open-source hardware prototype over RAN and core network components, with agents interacting at three network layers, and evaluate performance experimentally.

Result: Theoretical results provide bounds on performance and demonstrate a fundamental three-way tradeoff among optimization, generalization, and conflicting errors in decentralized multi-agent settings. The hardware prototype shows that SANet with the proposed MoPS framework and decentralized optimization can achieve up to 14.61% performance improvement compared to state-of-the-art algorithms while using only 44.37% of the floating point operations (FLOPs), indicating significant computational savings.

Conclusion: SANet demonstrates that a semantic-aware Agentic AI networking architecture can effectively coordinate multiple specialized agents across wireless network layers, handling decentralized, multi-objective optimization with conflicting goals. The model partition and sharing strategy and the proposed decentralized algorithms enable both performance gains and substantial reductions in computational cost. This validates AgentNet as a practical AI-native paradigm for real-time, autonomous wireless network management and optimization.

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [80] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: They propose Tyee, a unified, configurable deep-learning toolkit for physiological signals that standardizes data handling, preprocessing, modeling, and experiments, achieving strong and often state-of-the-art performance across many datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning for physiological signals is hampered by inconsistent data formats, ad-hoc preprocessing, fragmented model implementations, and non-reproducible experiments. Researchers currently spend significant effort re-implementing pipelines and cannot easily compare methods or scale to multiple signal modalities and tasks.

Method: They design and implement Tyee, a unified, modular, and fully configurable toolkit. It provides (1) a unified data interface and configurable preprocessing pipeline covering 12 physiological signal modalities; (2) a modular, extensible architecture that allows interchangeable components and rapid prototyping across tasks; and (3) end-to-end configuration of workflows (data, model, training, evaluation) to enable reproducible and scalable experimentation.

Result: Across a broad evaluation suite (multiple tasks and 13 datasets), Tyee-based pipelines consistently match or outperform existing baselines, achieving state-of-the-art performance on 12 of the 13 datasets, showing both effectiveness and generalizability of the toolkit design.

Conclusion: A unified, configurable, and modular toolkit like Tyee can substantially reduce fragmentation in physiological signal research, making experiments more reproducible and pipelines more scalable, while still achieving or improving on the best existing performance. The authors release and maintain Tyee as an open-source resource for the community.

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [81] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: The paper proposes M^3ob, a multi-modal spatial-temporal model that leverages LLM-enhanced spatial-temporal knowledge graphs to better predict human mobility and recommend locations, achieving strong performance and generalization across six datasets.


<details>
  <summary>Details</summary>
Motivation: Existing human mobility prediction and location recommendation methods have limited generalization. Unimodal models suffer from data sparsity and bias, while existing multimodal approaches cannot fully capture mobility dynamics due to a semantic gap between static multimodal features (e.g., images) and spatial-temporal behavioral patterns. There is a need to integrate richer spatial-temporal semantic knowledge to better characterize mobility dynamics and improve robustness, especially in abnormal scenarios.

Method: The authors build a Multi-Modal Mobility (M^3ob) framework. (1) They construct a unified spatial-temporal relational graph (STRG) that brings together multiple modalities by using functional semantics and spatial-temporal knowledge extracted from an LLM-enhanced spatial-temporal knowledge graph (STKG). (2) They encode each modality on this STRG and introduce a gating mechanism to adaptively fuse the spatial-temporal graph representations of different modalities. (3) They propose an STKG-guided cross-modal alignment mechanism to inject spatial-temporal dynamic knowledge into the static image modality, reducing the semantic gap between static content and dynamic mobility behaviors. This framework is applied to the location recommendation task.

Result: On six public datasets, M^3ob consistently outperforms baselines in standard (normal) evaluation settings and shows notably better generalization in abnormal scenarios, indicating that its use of multi-modal spatial-temporal knowledge effectively improves robustness and adaptability of human mobility modeling.

Conclusion: Incorporating LLM-enhanced spatial-temporal knowledge into a unified multi-modal graph framework and aligning static modalities (like images) with dynamic spatial-temporal patterns leads to more accurate and robust human mobility prediction and location recommendation, particularly improving generalization to abnormal or shifted conditions.

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [82] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces SimVC-CAS, a multi-agent system that models venture capital investors as interacting agents to better predict startup financing success, achieving notably higher accuracy than prior single-decision-maker models.


<details>
  <summary>Details</summary>
Motivation: Startup success is valuable yet hard to predict, and most existing models treat the prediction as if a single decision-maker evaluates startups, ignoring that in real VC practice, groups of heterogeneous investors, connected by co-investment networks, collectively drive funding decisions. The authors aim to close this gap and more faithfully capture real-world VC decision dynamics, both to improve prediction accuracy and to gain interpretable insight into how investor networks behave.

Method: They build SimVC-CAS, a collective agent system where each agent represents an investor with its own traits and preferences. A graph-structured co-investment network encodes relationships among investors. The framework includes role-playing agents plus a GNN-based supervised interaction module to simulate multi-agent interactions and information exchange, and reformulates startup financing prediction as a group decision-making problem over this network while leveraging enterprise fundamentals as input features.

Result: On real-world PitchBook data and under careful data leakage controls, SimVC-CAS outperforms existing baselines on startup financing prediction, with around a 25% relative improvement in average precision@10 and generally higher predictive accuracy. It also produces interpretable, multi-perspective reasoning traces that reflect different investor viewpoints and interaction patterns.

Conclusion: Modeling venture capital decision-making as a collective, networked multi-agent process—rather than a single decision-maker—yields significantly better predictive performance and richer interpretability for startup financing prediction. The proposed SimVC-CAS framework is effective for this task and can be generalized to other complex group decision-making scenarios beyond venture capital.

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [83] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: The paper tests whether structured deliberation between multiple large language models (LLMs) can improve their forecasting accuracy, finding gains when diverse models with shared information critique each other’s forecasts.


<details>
  <summary>Details</summary>
Motivation: Human forecasting tournaments show that structured discussion and cross-critique between forecasters improves accuracy. With LLMs increasingly used for prediction and decision support, it is important to know whether similar deliberation setups can systematically improve LLM forecasts, and under what conditions (model diversity, information sharing).

Method: The authors use 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament. They evaluate three frontier LLMs (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5) across four experimental conditions: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous model groups with distributed information, and (4) homogeneous model groups with shared information. In the deliberation intervention, models review and comment on each other’s initial probability forecasts, then revise their own forecasts. Accuracy is measured primarily with Log Loss and compared with non-deliberation baselines; statistical significance is assessed (e.g., p-values).

Result: In the diverse, shared-information setup (scenario 2), deliberation significantly reduces Log Loss by 0.020, corresponding to roughly a 4% relative improvement (p = 0.017). In homogeneous groups (three instances of the same LLM), the intervention yields no meaningful accuracy gains. Adding extra contextual information to prompts does not improve forecasting accuracy, contrary to expectations, and thus does not provide clear evidence for information-pooling as the mechanism behind the gains from deliberation.

Conclusion: Structured deliberation—LLMs critiquing and then updating based on each other’s forecasts—can improve LLM forecasting accuracy, but this effect appears contingent on model diversity with shared information. Simply cloning the same model or giving additional context does not confer similar benefits. Deliberation among heterogeneous LLMs emerges as a promising, low-cost way to boost predictive performance, though the underlying mechanisms and generality beyond this setting require further study.

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [84] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: Proposes DICE, a new explainable, robust, and efficient framework for evaluating Retrieval-Augmented Generation (RAG) systems via interpretable pairwise comparison and tournament-style ranking.


<details>
  <summary>Details</summary>
Motivation: RAG systems are becoming more complex and are used in sensitive, real-world scenarios, so their evaluation must be trustworthy, explainable, and robust. Existing evaluation metrics are mainly scalar scores that (1) are hard to interpret, (2) lack explicit uncertainty or confidence estimation, and (3) are computationally expensive for comparing many systems because they often require exhaustive pairwise evaluations. These limitations hinder responsible comparison, debugging, and deployment of RAG systems.

Method: The authors propose DICE (Discrete Interpretable Comparative Evaluation), a two-stage evaluation framework. First, it performs deep analytical reasoning over RAG outputs coupled with the underlying evidence. Second, it assigns probabilistic discrete comparative labels from the set {A better, B better, Tie}, capturing both direction and confidence of comparisons between systems. DICE records interpretable reasoning traces for each judgment to support qualitative analysis and error diagnosis. To scale to many systems, DICE organizes comparisons in a Swiss-system tournament scheme, which reduces the number of required comparisons from quadratic to near linear-logarithmic complexity, while still producing a reliable system ranking.

Result: Applying DICE to a curated Chinese financial QA dataset, the framework achieves 85.7% agreement with human experts, significantly higher than existing LLM-based metrics such as RAGAS. In an experiment with eight RAG systems, the Swiss-system tournament design yields a 42.9% reduction in evaluation comparisons relative to exhaustive pairwise evaluation while maintaining similar ranking quality.

Conclusion: DICE provides an explainable, confidence-aware, and computationally efficient approach to evaluating RAG systems. Its discrete comparative scoring with reasoning traces enables transparent judgments, systematic error analysis, and actionable insights for system improvement. The Swiss-system tournament design makes large-scale multi-system evaluation practical without sacrificing ranking fidelity, positioning DICE as a responsible and trustworthy paradigm for RAG system assessment.

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [85] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: They propose TravelBench, a realistic, multi-turn, tool-based benchmark to evaluate LLM agents on travel-planning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs for travel planning are limited: they cover narrow domains, lack rich multi-turn interaction, and do not effectively test dynamic user-agent behavior or tool use under evolving constraints. A more realistic, reproducible benchmark is needed to rigorously assess and improve LLM agents’ planning and tool-use abilities in a high-impact application domain like travel planning.

Method: They design TravelBench, a benchmark built from real-world travel-planning requests. The benchmark has three subsets—multi-turn, single-turn, and unsolvable cases—to probe different agent capabilities such as interactive preference elicitation, stepwise reasoning, and error handling. They create a controlled sandbox with 10 specialized travel tools (e.g., for flights, hotels, etc.) that return deterministic outputs, ensuring stable and reproducible evaluation of tool-using LLM agents. Multiple LLMs are then evaluated on this benchmark, and their behaviors and performance are analyzed.

Result: TravelBench successfully serves as a structured, controlled environment in which multiple LLMs can be systematically evaluated on realistic travel-planning tasks. The experiments reveal how well different models handle multi-step reasoning, manage multi-turn interaction, and use tools in a deterministic sandbox, and expose their limitations in each subset, including the unsolvable cases.

Conclusion: TravelBench provides a practical, real-world-inspired, and reproducible benchmark for assessing and advancing LLM agents in travel planning, especially regarding multi-turn interaction and tool use. It fills gaps in prior benchmarks and can guide future research on robust, interactive, tool-using LLM agents in complex planning domains.

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [86] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: A theoretical framework for continual learning in LLM agents using episodic memory and reflection instead of parameter updates.



<details>
  <summary>Details</summary>
Motivation: Existing large language model agents typically require backpropagation and fine-tuning for adaptation, which enforces a rigid separation between training and deployment. There is a need for agents that can continually learn from experience during deployment via interaction, especially using external memory rather than changing model parameters. The paper aims to provide a principled, RL-grounded foundation for such memory-augmented, retrieval-based adaptation.


Method: The authors introduce the Stateful Reflective Decision Process (SRDP), a formal model in which an agent interacts with an environment while reading from and writing to an episodic memory. Reflection is cast as a two-stage process: (1) writing interaction outcomes to memory (policy evaluation) and (2) reading relevant past cases from memory (policy improvement). They show that this induces an equivalent MDP over augmented state–memory pairs. On this formulation, they instantiate an entropy-regularized policy iteration algorithm operating over the augmented state space, and derive theoretical convergence guarantees as episodic memory grows and covers the state space.


Result: They prove that the SRDP with read–write episodic memory is equivalent to an MDP over augmented states, so standard dynamic programming and RL techniques apply. Under entropy-regularized policy iteration and assuming episodic memory eventually covers the state space, the induced policy iterates converge to the optimal policy without any parameter updates to the underlying language model. Thus, continual adaptation is achieved purely via interaction and memory operations.


Conclusion: The paper establishes a formal, reinforcement-learning-based foundation for reflective, memory-augmented language model agents that can continually adapt through interaction without backpropagation or fine-tuning. By modeling reflection as read–write operations over episodic memory and proving equivalence to an augmented MDP with convergence guarantees, it justifies retrieval-based, memory-centric approaches as a principled alternative to parameter-updating methods for continual learning in LLM agents.

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [87] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: The paper proposes SAMP-HDRL, a hierarchical deep reinforcement learning framework for portfolio optimization that dynamically groups assets, coordinates global and local allocation decisions, integrates risky and risk-free assets via a utility-based mechanism, and achieves consistently better risk-return performance and interpretability in non-stationary markets compared to traditional and DRL baselines.


<details>
  <summary>Details</summary>
Motivation: Portfolio optimization in real-world financial markets is difficult because markets are non-stationary, exhibit regime shifts, and have time-varying correlations. Existing DRL-based approaches can be powerful but often lack interpretability, robustness, and explicit handling of structural market constraints, especially under volatile and oscillating regimes. There is a need for a method that can adapt to changing regimes, coordinate decisions across different asset groups, incorporate risk-free assets coherently, and provide transparent explanations of its behavior.

Method: The authors introduce SAMP-HDRL, a hierarchical DRL architecture with dynamic asset grouping. First, a dynamic clustering step partitions assets into high-quality and ordinary subsets based on market conditions. A top-level (global) agent learns and extracts overall market signals and determines capital allocation across groups and between risky and risk-free assets using a momentum-adjusted utility function. Lower-level (local) agents operate within each asset group, performing intra-group allocation subject to mask constraints that enforce the grouping structure. A utility-based capital allocation mechanism coordinates global and local decisions. The model is trained and evaluated through backtests across multiple market regimes. The authors also conduct ablation studies by removing key components (upper-lower coordination, dynamic clustering, capital allocation) and use SHAP for post-hoc interpretability analysis of agents' decisions.

Result: Backtests over three market regimes from 2019 to 2021 show that SAMP-HDRL consistently outperforms nine traditional baseline strategies and nine DRL benchmarks, particularly under volatile and oscillating market conditions. Relative to the strongest baseline, SAMP-HDRL achieves at least 5% higher total return, 5% higher Sharpe ratio, 5% higher Sortino ratio, and 2% higher Omega ratio, with much larger performance improvements observed in turbulent markets. Ablation studies indicate that each major component—hierarchical upper-lower coordination, dynamic asset clustering, and the utility-based capital allocation scheme—contributes critically to the model's robustness and performance.

Conclusion: SAMP-HDRL effectively embeds structural market constraints and hierarchical decision-making into a DRL-based portfolio management framework, leading to improved adaptability and robustness in non-stationary, regime-shifting markets. The method not only delivers superior risk-adjusted returns compared to a wide range of baselines but also enhances interpretability through SHAP analysis, revealing a complementary "diversified + concentrated" allocation mechanism across agents. The framework provides a promising, more transparent approach to DRL-driven portfolio optimization in complex financial environments.

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [88] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench is a hierarchical, multimodal, multi-discipline benchmark to systematically evaluate LLMs’ scientific intelligence from basic literacy to creative discovery, revealing steep performance drops at higher reasoning levels.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation benchmarks for scientific capabilities of LLMs are fragmented, task-specific, and do not capture the hierarchical, multi-stage, and cross-disciplinary nature of real scientific inquiry. There is a need for an integrated framework that reflects the full scientific workflow and exposes detailed strengths and weaknesses of models across levels of scientific reasoning.

Method: The authors construct HiSciBench, a hierarchical benchmark with five levels aligned to the scientific workflow: L1 Scientific Literacy, L2 Literature Parsing, L3 Literature-based QA, L4 Literature Review Generation, and L5 Scientific Discovery. They curate 8,735 instances across six disciplines (mathematics, physics, chemistry, biology, geography, astronomy), support multimodal inputs (text, equations, figures, tables) and cross-lingual evaluation, and design dependency-aware tasks that build on each other. They then evaluate state-of-the-art LLMs and multimodal models, including GPT-5 and DeepSeek-R1, on these tasks.

Result: Models perform reasonably well on basic tasks but struggle as complexity increases. Leading systems reach around 69% accuracy on L1 scientific literacy but drop to about 25% on L5 scientific discovery. This reveals large gaps in higher-order scientific reasoning, literature synthesis, and creative discovery capabilities, even for advanced models.

Conclusion: HiSciBench offers a comprehensive, hierarchical and multimodal benchmark that better reflects real-world scientific workflows than prior task-specific benchmarks. It exposes significant deficiencies of current foundation models in high-level scientific discovery and reasoning, provides fine-grained diagnostic signals, and sets a new standard and resource for developing more capable and reliable scientific AI systems. The benchmark will be publicly released to support further research.

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [89] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma is a new structural knowledge graph foundation model that uses multi-head geometric attention with multiple algebraic spaces to achieve more expressive and robust zero-shot inductive reasoning than Ultra.


<details>
  <summary>Details</summary>
Motivation: Existing structural knowledge graph foundation models like Ultra use a single relational transformation in message passing, which limits expressiveness and their ability to capture diverse relational and structural patterns across many different graphs. There is a need for a more flexible mechanism that can adaptively model different relational structures for better zero-shot inductive reasoning on unseen entities and relations.

Method: Gamma introduces multi-head geometric attention for knowledge graph reasoning. Instead of a single transformation, it employs multiple parallel relational transformations based on different number systems (real, complex, split-complex, and dual numbers), each encoding distinct relational biases. A relation-conditioned attention fusion module with lightweight gating and entropy regularization adaptively fuses these heads at the link level, choosing appropriate relational bias per triple. The paper formally defines these algebraic message functions and analyzes the expressiveness of their combination.

Result: On 56 diverse knowledge graphs, Gamma consistently outperforms the Ultra model on zero-shot inductive link prediction. It achieves a 5.5% mean reciprocal rank (MRR) improvement on inductive benchmarks and a 4.4% MRR improvement across all benchmarks, indicating gains from combining complementary geometric representations.

Conclusion: Combining multiple algebraic relational transformations within a multi-head geometric attention framework yields a more expressive and adaptable knowledge graph foundation model. Gamma’s relation-conditioned fusion of diverse geometric spaces improves zero-shot inductive link prediction over single-transformation baselines like Ultra, showing the value of complementary geometric representations for structural reasoning on unseen graphs.

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [90] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: The paper introduces RW-Post, a new real-world multimodal fact-checking dataset with contextual posts, explicit reasoning chains, and linked evidence, and proposes AgentFact, an agent-based multimodal fact-checking framework that uses this dataset to significantly improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Multimodal misinformation is spreading rapidly, and current automated fact-checking systems—especially LVLMs and deep fusion models—struggle due to limited reasoning ability and shallow evidence use. A core bottleneck is the absence of datasets that couple real multimodal misinformation with detailed reasoning steps and verifiable evidence, preventing robust training and evaluation of explainable fact-checkers.

Method: The authors construct RW-Post by aligning multimodal claims with their original social media posts, thus preserving contextual cues, and augment these with detailed reasoning and explicitly linked evidence extracted from human-written fact-checking articles using an LLM-assisted pipeline. On top of this dataset, they design AgentFact, a multi-agent framework with five specialized agents for planning, evidence retrieval, visual analysis, reasoning, and explanation generation, organized into an iterative loop that alternates between evidence search and task-aware filtering/reasoning to mimic human verification workflows.

Result: Experiments show that models trained or evaluated with RW-Post and using the AgentFact framework achieve notably higher fact-checking accuracy and produce more interpretable, well-grounded explanations compared with prior multimodal fact-checking baselines and end-to-end LVLMs.

Conclusion: Combining a richly annotated, context-preserving dataset (RW-Post) with an agent-based verification pipeline (AgentFact) addresses key limitations of current multimodal fact-checking systems, leading to more accurate and interpretable detection of real-world multimodal misinformation and offering a scalable blueprint for future fact-checking tools.

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [91] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: The paper shows that traditional deep knowledge tracing models outperform large language models as student models for adaptive tutoring, in both predictive accuracy and temporal coherence of mastery estimates, and argues for hybrid systems rather than LLM-only tutors.


<details>
  <summary>Details</summary>
Motivation: The spread of LLM-based tutors in high-risk K–12 education has led to a misconception that they can fully replace established learner modelling approaches for adaptive instruction. Given regulatory concerns (e.g., EU AI Act) and the need for reliable, temporally coherent estimates of student knowledge, the authors aim to rigorously test whether LLMs can match or surpass classical student models in tracking evolving learner knowledge over time.

Method: The authors conduct a comparative study between a deep knowledge tracing (DKT) model and a widely used large language model. They evaluate the LLM both in zero-shot configuration and after fine-tuning on a large open-access educational dataset. The main task is next-step correctness prediction and estimation of learners’ mastery trajectories over time, including multi-skill mastery. They assess discrimination performance (AUC), error patterns (especially early in sequences where errors are more harmful), temporal coherence and directionality of mastery updates, and computational cost (training time). They complement the quantitative evaluation with qualitative analysis of mastery trajectories across skills.

Result: DKT achieves the best discrimination performance, with an AUC of 0.83 on next-step correctness prediction, and consistently outperforms the LLM in all tested settings. Fine-tuning improves the LLM’s AUC by around 8% relative to its zero-shot performance, but it still lags behind DKT by about 6% and yields more early-sequence errors. Temporal analyses show that DKT provides stable, directionally correct mastery updates over time, while both zero-shot and fine-tuned LLMs suffer from temporal weaknesses such as inconsistent and wrong-direction updates. These issues persist despite the fine-tuned LLM requiring roughly 198 hours of high-compute training, which is substantially more than DKT. Qualitative assessment of multi-skill mastery estimation confirms that the LLM generates noisy, inconsistent mastery trajectories, whereas DKT maintains smooth, coherent updates.

Conclusion: LLM-based tutors, even when fine-tuned and trained with substantial computational resources, currently fail to match the predictive performance and temporal reliability of specialized learner models like deep knowledge tracing. Relying solely on LLMs for adaptive tutoring and learner modelling is thus inadvisable, especially in high-risk K–12 contexts. Instead, responsible educational AI should adopt hybrid architectures in which LLMs are combined with robust learner modelling components to ensure accurate, coherent tracking of students’ knowledge and safe, effective adaptive instruction.

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [92] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: The paper argues that standard reward model (RM) accuracy is a poor proxy for real personalized alignment performance under deployment constraints, and introduces new metrics and a benchmark showing that better RM ranking does not reliably yield better user-aligned behavior.


<details>
  <summary>Details</summary>
Motivation: Most personalized alignment work optimizes reward model accuracy on preference data, assuming that accurate preference ranking leads to better, user-tailored behavior. In practice, though, large models are adapted at inference time using reward-guided decoding (RGD) rather than per-user fine-tuning, so RMs must guide token-level generation decisions effectively. The authors are motivated by the gap between offline RM metrics and actual deployment behavior quality.

Method: 1) Systematically evaluate multiple reward models and personalization methods across three datasets using both standard RM accuracy and a new metric, policy accuracy, which measures whether RGD-based scoring correctly prefers better full responses. 2) Measure the correlation between RM accuracy and policy accuracy. 3) Introduce Pref-LaMP, a benchmark with ground-truth user completions that allows direct behavioral evaluation without using reward-based metrics. 4) Compare reward-guided methods against simple in-context learning (ICL) across model scales (>3B parameters).

Result: They find that RM accuracy correlates only weakly with policy accuracy (Kendall’s tau between 0.08 and 0.31), showing that high RM accuracy does not ensure that RGD will pick better responses. On the Pref-LaMP benchmark, they observe that methods with large differences (around 20 points) in RM accuracy yield almost indistinguishable output quality, and even systems with high discrimination ability fail to generate behaviorally well-aligned responses. In contrast, simple ICL consistently outperforms all reward-guided methods for models larger than 3B parameters, with 3–5 ROUGE-1 point gains at 7B scale.

Conclusion: Standard RM accuracy is a misleading optimization target for personalized alignment when deployment relies on reward-guided decoding. Discrimination ability at the reward-model level does not translate into improved behavioral alignment, and strong preference models still fail to guide generation effectively. Simple ICL currently outperforms more complex reward-based approaches under realistic constraints, implying that the field needs new metrics, benchmarks, and methods that directly measure and optimize deployment-time behavioral alignment rather than proxy RM scores.

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [93] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason is a resource-efficient vision-language model for chest X-ray reasoning trained with an RL paradigm (SFT + GRPO). RL boosts in-distribution performance but harms cross-dataset generalization, suggesting curated supervised fine-tuning may be preferable for robust clinical deployment.


<details>
  <summary>Details</summary>
Motivation: While RL has significantly improved reasoning capabilities of LLMs, its use under realistic, resource-limited constraints in medical imaging is not well studied. There is a need to understand whether RL-based reasoning helps or harms robustness and generalization for vision-language models in clinical settings, where training data, compute, and cross-institution variability are all major constraints.

Method: The authors build ChexReason, a vision-language model for chest X-ray interpretation, and train it with an R1-style pipeline: small-scale supervised fine-tuning (2,000 SFT samples) followed by RL with GRPO (1,000 RL samples) on a single A100 GPU. They evaluate the model on two public benchmarks (CheXpert as the main in-distribution dataset and NIH as an out-of-distribution dataset), and compare different training stages (pretrained, SFT, RL) and different model types (general-purpose VLMs vs medically pre-trained models) to analyze reasoning, performance, and generalization behavior.

Result: On CheXpert (in-distribution), the GRPO stage substantially improves performance, achieving a 23% gain and reaching a macro-F1 of 0.346. However, the same RL training causes a 19% performance drop on the NIH dataset (out-of-distribution), indicating a significant loss in cross-dataset transferability. The SFT checkpoint, in contrast, uniquely improves NIH performance before RL optimization. Cross-model experiments show that adding explicit reasoning scaffolds yields gains for general-purpose vision-language models but produces little benefit for models already pre-trained on medical data. These findings parallel trends seen in larger models such as NV-Reason-CXR-3B, implying the problem lies with the RL setup rather than model scale.

Conclusion: RL-based reasoning (via GRPO) in medical vision-language models creates a trade-off between in-distribution performance and out-of-distribution generalization, reflecting a generalization paradox: SFT alone can yield better cross-institution robustness than subsequent RL. Teacher-guided, curated supervised fine-tuning may thus be a more reliable approach than aggressive RL for clinically deployed systems that must generalize across diverse institutions and patient populations.

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [94] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: The paper introduces Intrinsic Self-reflective Preference Optimization (\q), a new preference-optimization method for aligning LLMs that conditions on both the context and alternative responses, aiming to overcome key limitations of DPO and RLHF.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods like DPO and RLHF suffer from two core problems: (1) the learned optimal policy depends heavily on arbitrary modeling choices (e.g., scalarization function and reference policy), so model behavior can reflect artifacts of these parameterizations rather than genuine human preferences; (2) they treat response generation in isolation and ignore the comparative structure of pairwise preference data during inference, thereby failing to exploit the model’s potential for intrinsic self-reflection when evaluating or generating answers.

Method: The authors propose Intrinsic Self-reflective Preference Optimization (\q), a training objective that derives a globally optimal policy explicitly conditioned on both the input context and alternative candidate responses. This framework is theoretically constructed to be invariant to the choice of scalarization and reference policy, and is formulated as a plug-and-play optimization enhancement that can be applied without modifying the base model architecture or adding inference-time overhead.

Result: Through experiments on standard LLM alignment benchmarks, \q consistently improves win rates and length-controlled evaluation metrics compared with DPO and standard RLHF-style baselines. The empirical results support the claim that incorporating self-reflective conditioning on alternatives leads to more robust and better-aligned behavior across tasks.

Conclusion: The paper concludes that conventional preference-optimization methods like DPO and RLHF are fundamentally limited by dependence on arbitrary modeling choices and by ignoring comparative information at generation time. Intrinsic Self-reflective Preference Optimization (\q) overcomes these issues by learning a policy that conditions on both context and alternative responses, is invariant to scalarization and reference-policy choices, and can be used as a drop-in replacement. This self-reflective formulation produces LLMs that are more robust and better aligned with human preferences without additional inference cost.

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [95] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: The paper argues current methods for evaluating emotional intelligence (EI) in AI are inadequate and need philosophically and theoretically grounded refinement.


<details>
  <summary>Details</summary>
Motivation: Existing EI benchmarks for AI focus on narrow tasks and lack a robust account of what emotions and EI fundamentally are, especially given differences between human phenomenological experience and artificial systems.

Method: The authors first review and analyze major theories of emotion and EI, assessing which components are applicable to AI. They then critically examine current EI benchmark frameworks in AI against this theoretical account, identifying gaps and misalignments, and finally propose directions for better evaluation methods.

Result: They find that some human-centric aspects of EI (like phenomenological feeling and subjective understanding) are not applicable to AI, while functional capacities (sensing, explaining, responding to, and adapting to emotional states and contexts) are. Existing benchmarks only partially capture these functional aspects and are not well grounded in a clear theory of emotion and EI.

Conclusion: The paper concludes that EI evaluation in AI must be restructured around a theoretically sound, function-focused account of EI tailored to artificial systems, and it sketches options for more comprehensive, conceptually grounded evaluation strategies that avoid current shortcomings.

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [96] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL is a framework that embeds three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop to turn search into guided, reflective, grounded planning, significantly outperforming standard CoT and other search-based LLM agents on planning benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex, exploratory planning because linear chain-of-thought cannot easily recover from early reasoning errors, and classical search methods like MCTS are ineffective when guided only by sparse rewards and do not fully exploit LLMs' semantic strengths. The paper aims to build a system that combines the exploration power of search with the semantic and reflective abilities of LLMs to create more robust autonomous planners.

Method: The authors propose SPIRAL, a symbolic LLM planning framework that integrates a cognitive architecture of three collaborating LLM agents into an MCTS loop. A Planner agent proposes creative next actions, a Simulator agent grounds the search by predicting realistic outcomes of these actions in the environment, and a Critic agent reflects on trajectories to provide dense reward signals that guide MCTS. This yields a guided, self-correcting search over possible plans instead of unguided brute-force exploration.

Result: On DailyLifeAPIs and HuggingFace planning benchmarks, SPIRAL consistently outperforms standard Chain-of-Thought planning and other state-of-the-art LLM agent/search frameworks. It achieves 83.6% accuracy on DailyLifeAPIs, over 16 percentage points better than the next-best search framework, while also using tokens more efficiently.

Conclusion: Embedding specialized, reflective LLM agents within a search algorithm such as MCTS turns planning into a guided, grounded, and self-correcting process, substantially improving both robustness and efficiency of LLM-based autonomous planning. Structured cognitive architectures like SPIRAL provide a promising direction for future LLM planners, and the released code and data support reproducibility and further research.

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [97] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: The paper proposes 'model belief', a way to use token-level probabilities from LLMs as a more efficient estimator of behavior than single sampled outputs, proving statistical advantages and showing large computational savings in a demand estimation task.


<details>
  <summary>Details</summary>
Motivation: Common practice treats each LLM output as a single data point ('model choice'), ignoring the full probability distribution the model assigns to alternatives. This is statistically inefficient and computationally expensive when many runs are needed to estimate behavior or downstream quantities. The authors want to better exploit the probabilistic nature of LLMs to obtain more information per query.

Method: They formally define 'model belief' as a distribution over discrete choice alternatives constructed from the model’s token-level probabilities in a single generation. They derive theoretical results showing that model belief is an asymptotically equivalent but more efficient estimator than aggregating multiple sampled model choices, including for smooth functionals often used in applications. They then empirically compare model belief against model choice in a demand estimation setting where an LLM simulates consumers’ purchase decisions at different prices, evaluating variance, convergence, and predictive performance relative to ground truth.

Result: Model belief has lower variance and faster convergence than model choice while being asymptotically equivalent in expectation. In experiments, with realistic limits on the number of LLM calls, model belief more accurately recovers ground-truth demand curves and predicts aggregated model choices. It achieves a given accuracy with about 20 times fewer calls than needed when using model choices alone.

Conclusion: Using token-level probabilities to form 'model belief' should be the default way to extract behavioral data from LLMs. It is theoretically justified as an efficient estimator and practically delivers large computational savings and better predictive performance in demand estimation tasks, suggesting broad applicability to other domains relying on LLM-simulated behavior.

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [98] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: The paper introduces TCEval, a new benchmark using thermal comfort scenarios to test LLMs’ real-world cognitive abilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks are mostly abstract and task-specific, and they don’t adequately evaluate real-world, embodied, and context-aware cognitive capabilities. Thermal comfort, which involves complex interactions between environment, physiology, and subjective perception, is an ideal testbed for such abilities, but hasn’t been used systematically for AI evaluation.

Method: The authors build TCEval, an evaluation framework where LLM agents are initialized with virtual personality attributes (e.g., demographic or preference profiles) and are asked to make clothing insulation choices and report thermal comfort feedback under given environmental conditions. The agents’ outputs are translated into Predicted Mean Vote (PMV) values and compared against large-scale human thermal comfort datasets (ASHRAE Global Database and Chinese Thermal Comfort Database). They test four LLMs and perform statistical analyses on alignment with human data, directional consistency with a PMV tolerance, distribution similarity, and discrete comfort classification performance.

Result: LLM agents show limited exact alignment with human thermal comfort responses, but their directional consistency improves notably when a ±1 PMV tolerance is allowed, suggesting coarse-grained but not fine-grained alignment. Statistical tests indicate that the distributions of LLM-generated PMV values differ substantially from those of humans, and the agents perform close to random in discrete thermal comfort classification tasks.

Conclusion: TCEval is a feasible, ecologically valid framework akin to a Cognitive Turing Test that exposes strengths and weaknesses of current LLMs in real-world cognitive tasks. Present LLMs show some basic cross-modal reasoning in thermal comfort scenarios but lack accurate causal understanding of the nonlinear relations among relevant variables. The benchmark complements traditional evaluation suites by shifting attention from decontextualized tasks to embodied, context-aware perception and decision-making, and can guide development of AI for human-centric domains like smart buildings.

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [99] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: The paper shows that current general-purpose vision-language models fail at reliable physical control, and proposes a small, domain-specific language model trained with physics-based validation that achieves stable, safe reactor control policies with strong generalization.


<details>
  <summary>Details</summary>
Motivation: General-purpose foundation models, including state-of-the-art vision-language models, perform poorly on basic quantitative physics tasks and cannot provide the strict, outcome-level guarantees required for safety-critical physical control. The authors are motivated to find an alternative AI paradigm that can reliably control physical systems, overcoming the structural limitations of perception-centric, imitation-based architectures.

Method: The authors design a compact, 360M-parameter language model configured as an "Agentic Physical AI" controller. Instead of learning from perceptual imitation, the model is trained on synthetic reactor control scenarios where policy learning is guided by physics-based validation of outcomes. They systematically scale the training dataset from 10^3 to 10^5 examples, monitor behavior across different sizes, and analyze actuation choices across four actuator families. They also test transfer of the learned representations across different physical systems and continuous input modalities without changing the architecture.

Result: As the dataset scales, the model exhibits a sharp phase transition in behavior that is not seen in general-purpose models: small-data regimes show high-variance, risky imitation, while larger datasets cause a more than 500x reduction in variance, yielding highly stable control behavior. Despite being trained on balanced examples from four different actuation strategies, the trained model autonomously rejects about 70% of the training distribution and focuses about 95% of its runtime actions on a single-bank actuation strategy. The learned control representations transfer successfully to other physical regimes and input formats without altering the model architecture.

Conclusion: The paper concludes that general-purpose multimodal models are structurally misaligned with the needs of safety-critical physical control, and that domain-specific, physics-validated language models can overcome this limitation. By using physics-based outcome validation and scaling data in a targeted domain, compact models can undergo a stability phase transition, yielding low-variance, self-pruning control policies that generalize across physics and modalities. This represents a viable alternative pathway to "foundation models" for physical systems that prioritize execution-level reliability over broad perceptual competence.

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [100] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: The paper connects conformant planning with model-checking of a fragment of hyperproperties, showing mutual reductions between the two.


<details>
  <summary>Details</summary>
Motivation: To bridge two research areas—conformant planning and hyperproperty model-checking—by understanding their formal relationship, enabling transfer of techniques and insights between planning and verification.

Method: They formally define an encoding that reduces model-checking of ∃*∀* hyperproperties to conformant planning and prove this encoding is sound and complete. Then they construct the reverse reduction, showing that any conformant planning problem can be expressed as a hyperproperty model-checking instance.

Result: They obtain efficient reductions from ∃*∀* hyperproperty model-checking to conformant planning and show that conformant planning problems can be fully captured as hyperproperty model-checking tasks.

Conclusion: Conformant planning and model-checking of ∃*∀* hyperproperties are essentially equivalent in expressive power via efficient mutual reductions, suggesting cross-fertilization of algorithms and theory between planning and hyperproperty verification.

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [101] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper introduces CubeBench, a Rubik’s Cube–based benchmark to test LLM agents’ spatial reasoning, long-horizon state tracking, and active exploration, finding that current leading LLMs fail long-horizon tasks (0% pass rate).


<details>
  <summary>Details</summary>
Motivation: LLM agents perform well in digital, text-based tasks but struggle to operate reliably in the physical world, mainly because they lack robust spatial mental models and the ability to maintain and reason about long-term, partially observed states. The paper aims to systematically diagnose these gaps, which currently hinder the deployment of LLM agents as physically grounded intelligent agents.

Method: The authors design CubeBench, a generative benchmark built around the Rubik’s Cube. The benchmark has a three-tiered diagnostic structure: (1) tasks with full symbolic cube-state information focusing on basic state tracking and spatial reasoning, (2) tasks that require long-horizon mental simulation and state tracking over many moves, and (3) tasks where the agent must actively explore and reason under partial visual observation. They test several leading LLMs on these tasks and also introduce a diagnostic setup where external solver tools are provided to the agents to help isolate specific cognitive bottlenecks.

Result: Experiments show that existing leading LLMs have serious deficiencies, particularly on long-horizon tasks, where they uniformly achieve a 0.00% pass rate. The analysis of their behavior under different tiers and with/without external solver tools reveals consistent failures in long-term planning, state tracking, and effective exploration under partial observability.

Conclusion: Current LLM agents lack crucial cognitive faculties—especially robust spatial reasoning, long-horizon mental simulation, and strategic exploration—needed for reliable physical-world deployment. CubeBench exposes these weaknesses quantitatively (e.g., 0% pass rate on long-horizon tasks) and, via its diagnostic design and tool-augmented evaluations, offers a framework for analyzing failure modes and guiding the development of more physically grounded, capable agent architectures.

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [102] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher is a tool-integrated reasoning agent with interleaved, multimodal chain-of-thought that autonomously plans and invokes tools, achieving strong performance via specialized tools, curated data, and efficient training.


<details>
  <summary>Details</summary>
Motivation: Existing workflow-based agents are rigid and not intelligent enough for complex, real-world tasks that need flexible, multi-step tool use without human-designed workflows. There is a need for agents that can autonomously reason, decide when and how to call tools, and handle multimodal inputs for robust decision-making.

Method: They design MindWatcher, a tool-integrated reasoning agent that uses an interleaved thinking paradigm (alternating between reasoning and tool calls at any step) and multimodal chain-of-thought to manipulate images during reasoning. They build automated data auditing and evaluation pipelines, curate high-quality training datasets, construct a dedicated benchmark (MWE-Bench), and provide MindWatcher with a broad suite of reasoning tools plus a large local image retrieval database. They also develop a more efficient training infrastructure to speed training and improve hardware utilization. Experiments include comparisons with larger or newer models and analyses of training dynamics, including a “genetic inheritance” phenomenon in agentic RL.

Result: MindWatcher effectively decides whether and how to call tools, coordinates multiple tools, and handles broad-domain multimodal tasks. Its strong tool invocation capabilities allow it to match or outperform larger and more recent models on their benchmark and tasks, thanks to its auxiliary tools and high-quality image retrieval database.

Conclusion: Integrating interleaved thinking with multimodal chain-of-thought and rich tool suites enables smaller agents like MindWatcher to achieve or surpass the performance of larger models on complex, tool-based multimodal tasks. The work also provides infrastructure (benchmarks, datasets, training pipelines) and reveals new training insights such as genetic inheritance in agentic RL, offering guidance for future agent design and training strategies.

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [103] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: The paper develops a formal, computationally-embedded framework for continual learning in which agents are inherently constrained by being part of the environment, proposes an "interactivity" objective to quantify continual adaptation, and shows that deep linear networks better sustain interactivity with scale than deep nonlinear networks.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning formulations often rely on explicit, ad hoc constraints to model that an agent is smaller than the world, which complicates problem design, incorporation into algorithms, and scaling with model capacity. The authors want a more principled setting where constraints arise naturally from the agent being embedded in the environment, and a clear objective that captures continual adaptation itself rather than convergence to a fixed solution.

Method: They formalize an embedded agent as a finite automaton simulated by a universal computer, proving that such an agent is equivalent to operating in a partially observable Markov decision process (POMDP) with a countably infinite state space. Based on this setting, they define an "interactivity" objective that numerically measures how much an agent continues to adapt by learning new predictions over time. They then design a model-based reinforcement learning algorithm that explicitly seeks to maximize interactivity and use it to build a synthetic benchmark problem tailored to stress-test continual learning behavior. They empirically compare deep nonlinear and deep linear networks within this framework.

Result: Theoretical results establish the equivalence between computationally embedded agents and agents acting in countably infinite POMDPs. Empirically, when trained with the interactivity-seeking algorithm on the synthetic continual learning task, deep nonlinear networks fail to maintain high interactivity over time—they tend to lose the ability to keep adapting. In contrast, deep linear networks maintain or increase interactivity as their capacity grows, indicating more robust continual adaptation under this metric.

Conclusion: By grounding continual learning in a computationally-embedded setting, the paper removes the need for ad hoc capacity constraints and provides a principled objective—interactivity—to quantify ongoing adaptation. The findings suggest that, within this framework, architectural choices significantly impact continual learning: deep linear models scale more gracefully in terms of sustained interactivity than deep nonlinear ones. This opens directions for designing architectures and objectives that better exploit capacity for continual adaptation in embedded environments.

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [104] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: The paper introduces AKG kernel agent, a multi-agent system that automatically generates, migrates, and tunes high-performance AI computation kernels across different DSLs and hardware backends, achieving notable speedups over PyTorch Eager.


<details>
  <summary>Details</summary>
Motivation: Manually optimizing computation kernels for increasingly complex AI models (LLMs, multimodal, recommendation systems) and rapidly evolving heterogeneous hardware (GPUs, NPUs, diverse chips) does not scale. Each hardware/DSL combination demands bespoke kernels, and new techniques like sparsity and quantization add further complexity, creating a bottleneck in AI system deployment.

Method: The authors design AKG kernel agent, a modular multi-agent system that leverages LLM-based code generation to automate kernel development. It supports multiple DSLs (Triton, TileLang, CPP, CUDA-C) to target various hardware backends. Agents handle tasks such as kernel generation, migration between backends/DSLs, and performance tuning, while ensuring correctness and portability. The architecture allows easy extension to new DSLs and hardware targets.

Result: On KernelBench, using Triton as the DSL and evaluating across GPU and NPU backends, AKG kernel agent attains an average 1.46× speedup over PyTorch Eager baseline implementations, indicating better performance and more efficient kernels produced automatically.

Conclusion: AKG kernel agent demonstrates that LLM-driven, multi-agent automation can significantly accelerate the development and optimization of AI computation kernels across heterogeneous hardware. Its modular, DSL-agnostic design suggests good portability and extensibility, helping bridge the gap between rapidly evolving models/hardware and the limited scalability of manual kernel engineering.

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [105] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: They propose HiR, a sample-efficient RL framework that reuses failed instruction-following attempts as partial successes to better train LLMs with only binary rewards.


<details>
  <summary>Details</summary>
Motivation: RL for LLM alignment needs enough successful, high-quality samples, but initial models rarely satisfy all complex constraints, causing sparse or uninformative rewards and making training inefficient or unstable.

Method: HiR introduces a select-then-rewrite strategy: for each failed response, it retrospectively identifies which constraints were actually satisfied and rewrites the trajectory so that these satisfied parts are treated as successes. RL is then run jointly on both the original and these hindsight-replayed samples. The learning objective is formulated as a dual-preference problem at both the instruction and response level and is optimized using only a binary reward signal.

Result: HiR improves performance on multiple complex instruction-following benchmarks while using less compute compared with standard RL approaches.

Conclusion: Reframing failed attempts as partial successes through hindsight replay enables more sample- and compute-efficient RL for aligning LLMs to follow complex, constrained instructions, and the approach is validated empirically on several tasks with public code and data.

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [106] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: The paper introduces MIRAGE-VC, a graph + LLM framework for predicting startup success by selecting informative paths in investment networks and enabling explicit, interpretable reasoning.


<details>
  <summary>Details</summary>
Motivation: Most VC investments fail and only a few yield high returns, so better prediction of startup success is crucial. Existing ML and GNN methods struggle because they cannot perform explicit, step-by-step reasoning over heterogeneous relational evidence such as company disclosures, investor track records, and network structures. LLMs can reason but do not naturally operate over graph-structured data, and current graph-LLM work mostly focuses on in-graph tasks, whereas VC success prediction is an off-graph task. There is a need for a method that can select and reason over the most relevant graph paths to optimize an external objective like investment performance.

Method: The authors propose MIRAGE-VC, a multi-perspective retrieval-augmented generation framework. It uses an information-gain-driven path retriever that iteratively selects high-value neighbors in the investment network, reducing the path explosion problem by distilling large graphs into compact reasoning chains suitable for LLM context. They employ a multi-agent architecture where different agents process different evidence streams (e.g., textual company information, investor history, network context). A learnable gating mechanism conditioned on company attributes fuses these heterogeneous evidence streams, allowing the model to emphasize different types of evidence for different startups. Strict anti-leakage protocols are used in the experimental setup to avoid future information contaminating training or evaluation.

Result: MIRAGE-VC is evaluated on VC success prediction under anti-leakage constraints and outperforms baselines, achieving a 5.0 percentage point improvement in F1 score and a 16.6 percentage point gain in Precision@5. The framework also produces explicit, interpretable reasoning paths based on selected graph chains.

Conclusion: MIRAGE-VC effectively bridges graph-structured investment networks and LLM-based reasoning for off-graph prediction tasks like VC success prediction. By addressing path explosion and heterogeneous evidence fusion, it improves predictive performance and interpretability. The approach suggests broader applicability to other off-graph problems such as recommendation systems and risk assessment, and the authors release code for reproducibility.

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [107] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: The paper studies how to design AI agents that are aligned with human values and remain safe, focusing on assistance and shutdown problems, and argues that solving these requires agents capable of reasoning under uncertainty and handling incomplete and non-Archimedean preferences.


<details>
  <summary>Details</summary>
Motivation: AI systems may act in ways misaligned with human values, posing safety risks. To mitigate this, we need formal frameworks to model value alignment and control mechanisms like shutdown while maintaining capability and usefulness.

Method: The authors model two game-theoretic frameworks: the AI assistance game, where an AI learns and optimizes the human's unknown utility function(s), and the AI shutdown game, where an AI must obey shutdown signals without manipulating them while still performing tasks well. They analyze these settings under uncertainty and preference structures that may be incomplete or non-Archimedean.

Result: They show that standard assumptions about preferences (complete and Archimedean utilities) are insufficient for capturing realistic human values and for guaranteeing safe behavior in assistance and shutdown scenarios. They demonstrate that AI agents must be able to reason with uncertainty and handle more general preference structures, including incomplete and non-Archimedean preferences.

Conclusion: Ensuring AI alignment and shutdown safety requires moving beyond classical utility theory to models where agents can handle uncertainty and richer, potentially non-standard human preferences. This has implications for the design of future AI systems and theoretical work on alignment and control.

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [108] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: Proposes CreativeDC, a two-phase prompting method for LLMs to generate more diverse, novel, yet useful educational problems.


<details>
  <summary>Details</summary>
Motivation: LLM-generated educational questions suffer from an 'Artificial Hivemind' effect, yielding repetitive and homogeneous problems across and within models, which limits diversity of thought for students.

Method: Inspired by Wallas's creativity stages and Guilford's divergent-convergent thinking, CreativeDC structures prompting into two explicit phases: an unconstrained, creative exploration phase and a subsequent constraint-satisfaction phase, decoupling idea generation from final problem formulation. The method is evaluated on creative problem generation using metrics for diversity, novelty, and utility, and scaling behavior with increased sampling.

Result: CreativeDC produces problem sets with significantly higher diversity and novelty than baseline prompting approaches while preserving high utility of the generated problems. As more problems are sampled, the effective number of distinct problems grows faster under CreativeDC than under baselines.

Conclusion: Explicitly separating creative exploration from constrained generation in LLM prompting mitigates the 'Artificial Hivemind' effect, enabling broader idea exploration and yielding larger, more diverse sets of high-utility educational problems than standard prompting methods.

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [109] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: NeuroSPICE is a physics-informed neural network (PINN) framework that simulates circuits by solving DAEs via residual minimization instead of traditional time-stepping used in SPICE.


<details>
  <summary>Details</summary>
Motivation: Traditional SPICE tools rely on numerical time discretization to solve circuit DAEs, which can be rigid when dealing with emerging or highly nonlinear devices and are not naturally suited as differentiable surrogates for optimization and inverse design. There is a need for more flexible, differentiable, and model-based approaches that can better handle new device types and support tasks like design optimization.

Method: Use physics-informed neural networks to represent circuit and device waveforms as analytical functions of time, and train them by minimizing the residuals of the governing circuit DAEs via backpropagation. The PINN provides exact temporal derivatives from automatic differentiation and serves as a differentiable surrogate model for the circuit behavior.

Result: NeuroSPICE successfully applies PINNs to circuit and device simulation, matching SPICE-like solutions in principle but without surpassing SPICE in speed or accuracy during training. However, it demonstrates that the PINN-based approach can flexibly model complex and highly nonlinear devices, such as ferroelectric memories, and can act as a surrogate suitable for optimization and inverse problems.

Conclusion: While PINN-based NeuroSPICE is not yet a drop-in replacement for SPICE in terms of performance, it introduces a flexible, differentiable framework for circuit simulation, particularly advantageous for emerging and highly nonlinear devices and for applications in design optimization and inverse modeling.

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [110] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: They introduce I-PERI, a federated causal discovery algorithm that handles heterogeneous, unknown client-level interventions to recover a tighter causal graph equivalence class than standard CPDAGs, with theory and synthetic experiments.


<details>
  <summary>Details</summary>
Motivation: Existing federated causal discovery methods assume all clients share the same causal model, which is unrealistic because client-specific policies or protocols (e.g., across hospitals) act as unknown interventions that change the underlying causal structure. There is a need for methods that can both respect data decentralization and privacy and also exploit these heterogeneous interventions instead of being biased by them.

Method: They propose I-PERI, a federated algorithm for causal discovery. First, it aggregates information across clients to recover the CPDAG corresponding to the union of all client graphs. Then, it uses structural differences induced by client-specific interventions to further orient edges beyond what is possible from observational data alone. This leads to a refined equivalence class called the Φ-Markov Equivalence Class, represented by a Φ-CPDAG. The method is accompanied by convergence analysis and a study of its privacy-preserving properties.

Result: Theoretically, they show that I-PERI converges and satisfies certain privacy guarantees in the federated setting. Empirically, using synthetic datasets, they find that I-PERI more accurately orients edges and recovers causal structure compared to baselines, demonstrating the effectiveness of leveraging unknown heterogeneous interventions across clients.

Conclusion: Federated causal discovery can be significantly improved by explicitly modeling and exploiting unknown client-level interventions. I-PERI provides a principled and privacy-preserving way to aggregate heterogeneous client data, recover a union CPDAG, and then refine it to a Φ-CPDAG that represents a tighter causal equivalence class, supported by both theoretical guarantees and synthetic experiments.

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [111] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper proposes Web World Models (WWMs), which use standard web technologies to implement consistent, persistent virtual worlds while leveraging language models for narrative and high-level decisions.


<details>
  <summary>Details</summary>
Motivation: Language agents need persistent, controllable environments to act, remember, and learn. Existing solutions either rely on rigid web apps with fixed contexts or on unconstrained generative world models that are hard to control and engineer. The authors seek a middle-ground substrate that supports both reliability and open-endedness.

Method: They introduce the Web World Model (WWM) paradigm, where the underlying world state and rules ("physics") are implemented with conventional web stacks (typed interfaces, databases, deterministic logic). Large language models operate on top of this latent state to generate descriptions, narratives, and high-level plans. They build several example WWMs, such as an infinite travel atlas grounded in real maps, fictional galaxy exploration, encyclopedic/narrative universes, and game-like simulations, and distill design principles from these implementations.

Result: They demonstrate multiple working systems that allow unlimited yet structured exploration, showing that web code can maintain consistency and controllability while LLMs provide open-ended narrative and decision-making. They identify concrete design patterns: strict separation of code-defined rules from model imagination, treating latent state as typed web interfaces, and using deterministic generation for reproducible expansion.

Conclusion: Standard web stacks can function as scalable substrates for world models, enabling controllable, logically consistent, yet open-ended environments for language agents by combining code-based world physics with LLM-driven imagination and high-level control.

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>
