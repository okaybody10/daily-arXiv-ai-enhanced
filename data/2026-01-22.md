<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs](https://arxiv.org/abs/2601.14267)
*Pouria Mortezaagha,Joseph Shaw,Bowen Sun,Arya Rahgozar*

Main category: cs.CL

TL;DR: The paper proposes a schema-constrained, provenance-aware AI pipeline that converts complex biomedical PDFs into structured, auditable data for evidence synthesis.


<details>
  <summary>Details</summary>
Motivation: Biomedical evidence synthesis requires extracting detailed methodological and outcome variables from full-text PDFs, but manual abstraction is slow and existing document AI suffers from OCR errors, long-document handling issues, throughput limits, and poor auditability—problems that are acute in high-stakes biomedical contexts.

Method: The authors design a document AI pipeline that: (1) ingests PDFs via resume-aware hashing; (2) partitions them into caption-aware, page-level chunks; (3) runs asynchronous, concurrency-controlled extraction constrained by typed schemas, controlled vocabularies, and evidence-gated decisions; and (4) deterministically merges chunk-level outputs into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to ensure traceability.

Result: On a corpus of studies about direct oral anticoagulant level measurement, the system processed all documents end-to-end without manual intervention, maintained stable throughput under service constraints, and produced internally consistent extractions across chunks. Iterative refinement of the schema led to notable gains in accuracy for key synthesis variables such as assay type, outcome definitions, follow-up length, and measurement timing.

Conclusion: Schema-constrained, provenance-aware extraction pipelines can reliably and scalably convert heterogeneous biomedical PDFs into structured, auditable evidence, better aligning document AI with the transparency and reliability standards required for biomedical evidence synthesis.

Abstract: Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI systems remain limited by OCR errors, long-document fragmentation, constrained throughput, and insufficient auditability for high-stakes synthesis. We present a schema-constrained AI extraction system that transforms full-text biomedical PDFs into structured, analysis-ready records by explicitly restricting model inference through typed schemas, controlled vocabularies, and evidence-gated decisions. Documents are ingested using resume-aware hashing, partitioned into caption-aware page-level chunks, and processed asynchronously under explicit concurrency controls. Chunk-level outputs are deterministically merged into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to support traceability and post-hoc audit. Evaluated on a corpus of studies on direct oral anticoagulant level measurement, the pipeline processed all documents without manual intervention, maintained stable throughput under service constraints, and exhibited strong internal consistency across document chunks. Iterative schema refinement substantially improved extraction fidelity for synthesis-critical variables, including assay classification, outcome definitions, follow-up duration, and timing of measurement. These results demonstrate that schema-constrained, provenance-aware extraction enables scalable and auditable transformation of heterogeneous scientific PDFs into structured evidence, aligning modern document AI with the transparency and reliability requirements of biomedical evidence synthesis.

</details>


### [2] [The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues](https://arxiv.org/abs/2601.14269)
*Youyou Cheng,Zhuangwei Kang,Kerry Jiang,Chenyu Sun,Qiyang Pan*

Main category: cs.CL

TL;DR: The paper studies how large language models gradually violate safety boundaries during long, multi-turn mental-health dialogues, and proposes a stress-testing framework showing that current single-turn safety checks are insufficient.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used for mental health support, but existing safety evaluations mainly check for explicit, prohibited content in single-turn interactions. This misses a more subtle risk: over many conversational turns, models may incrementally overstep safety boundaries by offering guarantees, assuming professional roles, or making irresponsible assurances, even if they avoid obvious unsafe words. The authors aim to systematically explore and quantify this gradual erosion of safety in realistic, long mental-health dialogues.

Method: The authors design a multi-turn stress-testing framework with two pressure strategies: (1) static progression, where risk or pressure increases along a fixed script across turns, and (2) adaptive probing, where subsequent prompts are adjusted dynamically based on the model’s previous replies to more effectively push it toward boundary violations. They create 50 virtual patient personas and run up to 20-turn virtual psychiatric consultations with three state-of-the-art LLMs, logging when and how the models violate safety guidelines (e.g., making definitive guarantees or accepting responsibility). They then compare violation rates, types, and the number of turns required to trigger a violation under the two pressure modes.

Result: All three evaluated LLMs frequently violated safety boundaries in long psychiatric-style dialogues. The overall violation rates were similar under static progression and adaptive probing, indicating that both strategies can elicit safety failures. However, adaptive probing caused models to cross safety boundaries much earlier, cutting the average turns-to-violation roughly in half (from 9.21 under static progression to 4.64 under adaptive probing). Across both conditions, the dominant violation pattern was giving definitive, zero-risk promises or guarantees, rather than using overtly prohibited phrases.

Conclusion: Safety mechanisms that only rely on single-turn checks or keyword-based filtering are inadequate for mental-health applications of LLMs. The primary risk lies in progressive boundary erosion over multi-turn conversations, especially under user pressure designed to elicit comfort, empathy, or reassurance. Evaluations must explicitly model interaction dynamics, including different pressure modes and dialogue lengths, to assess and strengthen the robustness of LLM safety boundaries over time.

Abstract: Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.
  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.

</details>


### [3] [Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2601.14270)
*Liangming Pan,Jason Liang,Jiaran Ye,Minglai Yang,Xinyuan Lu,Fengbin Zhu*

Main category: cs.CL

TL;DR: Survey of how large language models perform multi-step reasoning internally, offering a conceptual framework and future directions.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are good at multi-step reasoning, the mechanisms behind this ability are not well understood and existing surveys mainly emphasize engineering tricks to improve performance, not the internal processes.

Method: Conduct a literature survey structured around seven key research questions that examine different aspects of LLMs’ internal multi-step reasoning, from implicit multi-hop processing in hidden states to the impact of explicit, verbalized reasoning such as chain-of-thought.

Result: Provides an organized synthesis of existing mechanistic work on LLM reasoning, mapping studies to the seven-question framework and clarifying how internal computations relate to observed reasoning behavior.

Conclusion: Clarifying the mechanisms of multi-step reasoning in LLMs through a systematic framework exposes gaps in current understanding and leads to five concrete directions for future mechanistic research.

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.

</details>


### [4] [Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning](https://arxiv.org/abs/2601.14280)
*Nicholas X. Wang,Aggelos K. Katsaggelos*

Main category: cs.CL

TL;DR: The paper proposes a multi-agent, hallucination-aware framework for generating educational multiple-choice questions (MCQs) with LLMs, treating the task as an optimization problem that minimizes hallucination risk while maintaining quality and cost-efficiency, and shows a >90% reduction in hallucinations on AP-aligned STEM questions.


<details>
  <summary>Details</summary>
Motivation: LLMs are promising for generating educational MCQs at scale, but they frequently hallucinate: they produce fluent yet incorrect, incoherent, or unsound questions and answers. These hallucinations undermine educational validity, confuse learners, and limit trust in AI-assisted content creation. Existing MCQ generation approaches often treat hallucinations loosely or as a single phenomenon, rather than categorizing and systematically mitigating them throughout the generation pipeline. There is a need for a structured, scalable way to reduce hallucinations while preserving pedagogical quality and keeping computational cost reasonable.

Method: The authors first categorize hallucinations specific to MCQ generation into four types: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. They then design a hallucination-free multi-agent generation framework that decomposes MCQ generation into multiple discrete, verifiable stages. Different agents—some rule-based, others LLM-based—are responsible for steps such as drafting items, checking solvability, validating reasoning, and screening for factual and mathematical mistakes. The process is formalized as an optimization problem whose objective is to minimize hallucination risk subject to constraints on validity, answerability, and computational cost. Hallucination scoring metrics guide selection, filtering, and refinement. An agent-led refinement loop uses counterfactual reasoning and chain-of-thought (CoT) prompting to iteratively revise questions and options until quality thresholds are met.

Result: On a set of AP-aligned STEM MCQs, the proposed framework substantially lowers hallucination incidence, achieving over a 90% reduction in hallucination rates compared to baseline one-shot LLM generation. Despite this aggressive filtering and refinement, the generated questions maintain their educational value and stylistic alignment with AP-style items, indicating that the system improves reliability without sacrificing pedagogical utility.

Conclusion: Structured, multi-agent collaboration—combining rule-based checks, LLM-based evaluators, and iterative refinement informed by hallucination scores—can effectively mitigate hallucinations in LLM-generated educational MCQs. By reframing MCQ generation as an optimization problem balancing hallucination risk, validity, answerability, and cost, the approach delivers substantially more reliable content at scale. This demonstrates a viable path toward trustworthy, LLM-powered learning tools and suggests that similar multi-agent, verification-oriented designs could be applied to other educational and high-stakes content generation tasks.

Abstract: Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.

</details>


### [5] [RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension](https://arxiv.org/abs/2601.14289)
*Yelin Chen,Fanjin Zhang,Suping Sun,Yunhe Pang,Yuanchun Wang,Jian Song,Xiaoyan Li,Lei Hou,Shu Zhao,Jie Tang,Juanzi Li*

Main category: cs.CL

TL;DR: RPC-Bench is a large-scale, fine-grained QA benchmark built from real review–rebuttal exchanges of CS papers to evaluate foundation models’ ability to deeply understand research papers.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for paper understanding are coarse, small, and do not capture the nuanced, specialized reasoning needed to read and critique research papers, especially with complex figures and tables. The authors want a more realistic and fine-grained way to evaluate how well models actually understand scholarly work.

Method: They construct RPC-Bench from review–rebuttal conversations of high-quality computer science papers, yielding 15K human-verified QA pairs. They design a taxonomy aligned with the research lifecycle to classify questions into why/what/how categories, and create an LLM–human interaction framework for large-scale annotation and quality control. For evaluation, they adopt an LLM-as-a-Judge setup to score model answers along two axes: correctness–completeness and conciseness, calibrated to align closely with human judgments.

Result: The resulting benchmark contains 15K vetted QA pairs plus a taxonomy and evaluation framework. When evaluated on RPC-Bench, even top-performing models (e.g., GPT-5, as stated) only reach 68.2% on correctness–completeness, which further drops to 37.46% when conciseness is also required.

Conclusion: There remains a substantial gap between current foundation models’ capabilities and the demands of precise, fine-grained academic paper understanding. RPC-Bench provides a scalable, realistic benchmark and evaluation framework to expose and measure these shortcomings, and the authors release code and data to support further research.

Abstract: Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.

</details>


### [6] [Project Aletheia: Verifier-Guided Distillation of Backtracking for Small Language Models](https://arxiv.org/abs/2601.14290)
*Aradhya Dixit,Tianxi Liang,Jai Telang*

Main category: cs.CL

TL;DR: They propose a training method that teaches small language models not just to get the right answer, but to detect and fix their own reasoning mistakes, improving performance on strict constraint-satisfaction tasks.


<details>
  <summary>Details</summary>
Motivation: Small language models are desirable for private, on-device use, but they struggle with tasks that require strict adherence to constraints because their reasoning is linear and overconfident; once they make an early mistake, they rarely recover. The authors want to enable small models to handle such tasks more robustly, approaching the reliability of larger models while maintaining their efficiency and deployability.

Method: They introduce Verifier-Guided Distillation, a training protocol where a model is trained on reasoning traces that have been checked by a verifier. These traces explicitly include initial errors, subsequent conflict detection, and backtracking/self-correction steps. Instead of distilling only correct final answers, they distill the whole error-repair process into a 7B-parameter model so it can internalize verification-like behavior.

Result: After training with this protocol, the 7B model exhibits latent verification behavior: at inference time, it sometimes halts, recognizes contradictions in its own reasoning, and revises earlier assumptions. This leads to improved performance on strict constraint-satisfaction problems compared to standard training that only targets correct final outputs.

Conclusion: Explicitly teaching small language models the process of error detection and repair via verifier-guided distillation enables them to develop internal verification behaviors, making them more capable on constraint-sensitive tasks while remaining small enough for on-device, privacy-preserving deployment.

Abstract: Small Language Models (SLMs, under 10B parameters) are attractive for private, on-device deployment, yet they frequently fail on strict constraint-satisfaction problems due to linear, overconfident reasoning traces that do not recover from early mistakes. We introduce Verifier-Guided Distillation, a training protocol that transfers the process of error repair - explicit conflict detection and backtracking - rather than only correct final answers. By training a 7B model on verified reasoning traces that include mistakes and self-corrections, we show that latent verification behavior can emerge in small models, enabling them to occasionally stop, detect contradictions, and revise earlier assumptions.

</details>


### [7] [Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding](https://arxiv.org/abs/2601.14304)
*Juncheng Wang,Zhe Hu,Chao Xu,Siyue Ren,Yuxiang Feng,Yang Liu,Baigui Sun,Shujun Wang*

Main category: cs.CL

TL;DR: The paper enhances autoregressive text-to-audio generation by using a lightweight auxiliary model (Plan-Critic) that evaluates early token prefixes to steer sampling toward semantically faithful outputs, significantly boosting CLAP scores without extra compute over best-of-N decoding.


<details>
  <summary>Details</summary>
Motivation: AR audio models generate temporally coherent audio but struggle to follow complex textual prompts, particularly for multi-event or semantically rich descriptions. The authors observe that early tokens already encode global semantic properties, suggesting untapped planning signals. They aim to exploit this for better instruction following without sacrificing efficiency.

Method: They empirically show that early prefixes of AR audio generations correlate with global semantic attributes like event count and sound category. Leveraging this, they design Plan-Critic, a small auxiliary model trained to predict final text–audio alignment quality from partial generations, using a loss inspired by Generalized Advantage Estimation to emphasize informative prefixes. At inference, they generate multiple candidate prefixes, score them with Plan-Critic, prune low-scoring ones, and allocate more sampling budget to promising prefixes, enabling guided exploration while keeping total computation similar to standard best-of-N sampling.

Result: Plan-Critic-guided decoding increases CLAP scores by up to 10 points compared to a strong AR baseline, setting a new state of the art for autoregressive text-to-audio generation in terms of instruction following. It does so while maintaining similar computational cost to best-of-N decoding, showing that the planning signal in prefixes can be exploited efficiently.

Conclusion: The work demonstrates that strictly autoregressive audio generators implicitly plan via their early tokens, which encode global semantics. By learning to critique and select prefixes with Plan-Critic, the model can align much better with complex prompts without extra inference cost. This bridges the gap between causal token-by-token generation and global semantic alignment, suggesting a general strategy for improving instruction following in AR generation models.

Abstract: Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.

</details>


### [8] [Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis](https://arxiv.org/abs/2601.14417)
*Thanathai Lertpetchpun,Yoonjeong Lee,Thanapat Trachu,Jihwan Lee,Tiantian Feng,Dani Byrd,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The paper studies how explicit phonological rules and speaker embeddings interact in accent-controllable TTS, introducing a metric (PSR) to quantify how embeddings preserve or override rule-based accent transformations.


<details>
  <summary>Details</summary>
Motivation: Existing TTS systems control accent mostly via speaker embeddings tied to particular accents, but these embeddings entangle accent with timbre, emotion, and identity, limiting interpretability and fine-grained control. The authors aim to separate and better understand the contributions of rule-based phonology and learned embeddings to accent realization, using a concrete English accent pair.

Method: They use American vs. British English as a case study and implement explicit phonological rules for phenomena like flapping, rhoticity, and vowel correspondences. They synthesize speech with combinations of these rules and learned speaker embeddings. They then introduce and compute the Phoneme Shift Rate (PSR), which measures the extent to which phonemes are shifted according to rules vs. preserved according to the embedding, to analyze how embeddings interact with and potentially override the rules.

Result: Experiments indicate that applying both rule-based transformations and speaker embeddings yields speech with more authentic and natural-sounding accents than using either alone. The analysis via PSR shows that speaker embeddings sometimes attenuate or fully overwrite the prescribed phonological rules, evidencing entanglement between accent characteristics and other speaker traits encoded in the embeddings.

Conclusion: Explicit phonological rules provide a powerful and interpretable mechanism for accent control in TTS, and PSR offers a quantitative way to study disentanglement between accent and speaker identity. The work suggests that combining rule-based and embedding-based approaches gives better accent realism and provides a framework for future research on disentangled, controllable speech generation.

Abstract: Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.

</details>


### [9] [Large Language Models for Large-Scale, Rigorous Qualitative Analysis in Applied Health Services Research](https://arxiv.org/abs/2601.14478)
*Sasha Ronaghi,Emma-Louise Aveling,Maria Levis,Rachel Lauren Ross,Emily Alsentzer,Sara Singer*

Main category: cs.CL

TL;DR: The paper proposes and applies a general framework for integrating large language models into qualitative analysis in multi-site health-services research, showing that LLMs can improve efficiency while maintaining methodological rigor.


<details>
  <summary>Details</summary>
Motivation: Qualitative analysis in large, multi-site health-services research is time-consuming and resource-intensive, and there is little methodological guidance on how to systematically and rigorously integrate LLMs into these workflows. The authors aim to fill this gap to make qualitative analysis more scalable while preserving scientific quality.

Method: The authors design a model- and task-agnostic framework that guides how humans and LLMs can collaborate in qualitative analysis for various analytic goals. They then apply this framework in a multi-site study of diabetes care in Federally Qualified Health Centers, using LLMs to assist with qualitative synthesis of researcher summaries into comparative feedback reports, and to conduct deductive coding of 167 interview transcripts to inform refinement of a practice-transformation intervention.

Result: Using the framework, the research team was able to use LLMs to generate timely comparative feedback for practitioners and to code a large volume of interview data, enabling these data to meaningfully shape theoretical insights and practical intervention refinements in the diabetes care study.

Conclusion: The study shows that LLMs can be systematically and rigorously integrated into applied health-services qualitative research to improve efficiency and timeliness of analysis without sacrificing rigor, and it offers a generalizable framework to guide future human-LLM methods development in qualitative research contexts.

Abstract: Large language models (LLMs) show promise for improving the efficiency of qualitative analysis in large, multi-site health-services research. Yet methodological guidance for LLM integration into qualitative analysis and evidence of their impact on real-world research methods and outcomes remain limited. We developed a model- and task-agnostic framework for designing human-LLM qualitative analysis methods to support diverse analytic aims. Within a multi-site study of diabetes care at Federally Qualified Health Centers (FQHCs), we leveraged the framework to implement human-LLM methods for (1) qualitative synthesis of researcher-generated summaries to produce comparative feedback reports and (2) deductive coding of 167 interview transcripts to refine a practice-transformation intervention. LLM assistance enabled timely feedback to practitioners and the incorporation of large-scale qualitative data to inform theory and practice changes. This work demonstrates how LLMs can be integrated into applied health-services research to enhance efficiency while preserving rigor, offering guidance for continued innovation with LLMs in qualitative research.

</details>


### [10] [Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks](https://arxiv.org/abs/2601.14479)
*Crish Nagarkar,Leonid Bogachev,Serge Sharoff*

Main category: cs.CL

TL;DR: The paper fine-tunes open-source LLMs to solve advanced statistics problems and to judge solution quality, showing they can reach statistics-student-level performance and outperform traditional automatic metrics in evaluating reasoning.


<details>
  <summary>Details</summary>
Motivation: Although modern LLMs excel in many NLP tasks, their capability to handle even moderately complex statistical reasoning and to reliably assess reasoning quality is unclear. There is a need for models that can both solve statistics tasks and grade or critique solutions, to support education, research, and data analysis workflows where human evaluation is costly and traditional automatic metrics (e.g., BLEU, BertScore) poorly capture reasoning quality.

Method: The authors construct a tailored dataset of statistical tasks, including solutions and reasoning, and fine-tune several open-source LLM architectures to improve their statistical reasoning abilities. They benchmark these models against human scores (e.g., statistics students or expert-graded solutions). They also evaluate the models’ ability to judge answer quality, including correctness, explanation, and reasoning, and compare these judgments against traditional automatic metrics like BLEU and BertScore.

Result: Fine-tuned LLMs perform substantially better on advanced statistical tasks than their base versions and reach performance comparable to statistics students. The gains from fine-tuning depend on the model architecture, with some models exhibiting large improvements, suggesting strong suitability for certain families of LLMs. Additionally, LLM-based self-evaluation of answers—scoring correctness and reasoning quality—correlates better with human judgment than BLEU or BertScore, demonstrating that LLMs are much more effective evaluators of reasoning-heavy responses.

Conclusion: Fine-tuning open-source LLMs on a dedicated statistics dataset yields models capable of statistics-student-level performance on advanced tasks. Their architecture-dependent improvement patterns highlight which models are most promising for deployment in educational and analytical tools. Crucially, LLMs’ superior ability to judge answer and reasoning quality over traditional text-similarity metrics enables scalable, automated assessment for statistics education, research methodology validation, and quality control in data analysis workflows.

Abstract: This paper investigates the ability of large language models (LLMs) to solve statistical tasks, as well as their capacity to assess the quality of reasoning. While state-of-the-art LLMs have demonstrated remarkable performance in a range of NLP tasks, their competence in addressing even moderately complex statistical challenges is not well understood. We have fine-tuned selected open-source LLMs on a specially developed dataset to enhance their statistical reasoning capabilities, and compared their performance with the human scores used as a benchmark. Our results show that the fine-tuned models achieve better performance on advanced statistical tasks on the level comparable to a statistics student. Fine-tuning demonstrates architecture-dependent improvements, with some models showing significant performance gains, indicating clear potential for deployment in educational technology and statistical analysis assistance systems. We also show that LLMs themselves can be far better judges of the answers quality (including explanation and reasoning assessment) in comparison to traditional metrics, such as BLEU or BertScore. This self-evaluation capability enables scalable automated assessment for statistical education platforms and quality assurance in automated analysis tools. Potential applications also include validation tools for research methodology in academic and industry settings, and quality control mechanisms for data analysis workflows.

</details>


### [11] [Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence](https://arxiv.org/abs/2601.14518)
*Jinhui Liu,Ximeng Zhang,Yanbo Ai,Zhou Yu*

Main category: cs.CL

TL;DR: The paper introduces a new framework for generating synthetic Text-to-SQL evaluation data tailored to realistic business intelligence (BI) settings, emphasizing business logic realism and reasoning complexity, and shows it better reflects real-world BI use than prior methods.


<details>
  <summary>Details</summary>
Motivation: Evaluating Text-to-SQL agents in private BI environments is hard because there is limited access to realistic, domain-specific data. Existing synthetic data generation techniques scale well but lack 'business realism'—their questions don’t adequately represent real business logic, personas, and workflows. As a result, current benchmarks may overestimate Text-to-SQL performance in real BI settings.

Method: The authors propose a Business Logic-Driven Data Synthesis framework. It constructs synthetic evaluation data by grounding question generation in explicit business personas (types of users), work scenarios, and operational workflows typical for BI tasks. Additionally, they implement a business reasoning complexity control strategy that systematically varies and diversifies the analytical reasoning depth and steps required to answer the questions, enabling evaluation of models across different levels of business reasoning complexity.

Result: On a production-scale Salesforce database, the synthesized dataset achieves high business realism, with a 98.44% realism score, outperforming two prior synthetic data generators, OmniSQL by 19.5 percentage points and SQL-Factory by 54.7 percentage points. It also maintains strong question-to-SQL alignment at 98.59%. When using this data to evaluate current state-of-the-art Text-to-SQL models, the models achieve only 42.86% execution accuracy on the most complex, high-reasoning business queries, highlighting substantial remaining gaps.

Conclusion: The proposed business logic-driven synthetic data framework yields evaluation sets that are both highly realistic and well-aligned with SQL, better reflecting true BI workloads than existing synthetic methods. This more realistic benchmark reveals that current state-of-the-art Text-to-SQL systems struggle with complex business reasoning, indicating that genuine BI deployment will require further advances in modeling and reasoning capabilities.

Abstract: Evaluating Text-to-SQL agents in private business intelligence (BI) settings is challenging due to the scarcity of realistic, domain-specific data. While synthetic evaluation data offers a scalable solution, existing generation methods fail to capture business realism--whether questions reflect realistic business logic and workflows. We propose a Business Logic-Driven Data Synthesis framework that generates data grounded in business personas, work scenarios, and workflows. In addition, we improve the data quality by imposing a business reasoning complexity control strategy that diversifies the analytical reasoning steps required to answer the questions. Experiments on a production-scale Salesforce database show that our synthesized data achieves high business realism (98.44%), substantially outperforming OmniSQL (+19.5%) and SQL-Factory (+54.7%), while maintaining strong question-SQL alignment (98.59%). Our synthetic data also reveals that state-of-the-art Text-to-SQL models still have significant performance gaps, achieving only 42.86% execution accuracy on the most complex business queries.

</details>


### [12] [Towards Execution-Grounded Automated AI Research](https://arxiv.org/abs/2601.14525)
*Chenglei Si,Zitong Yang,Yejin Choi,Emmanuel Candès,Diyi Yang,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: The paper builds an automated system that both executes and evaluates LLM-generated research ideas for AI training, then studies how well search and RL can use execution feedback to improve those ideas.


<details>
  <summary>Details</summary>
Motivation: LLMs can propose many research ideas for improving model training, but these ideas are often ungrounded and ineffective because they are never actually implemented and tested. The authors want to know if it is practically feasible to automatically execute such ideas at scale and whether feedback from these executions can systematically improve idea quality, enabling more reliable automated AI research.

Method: 1) Build an automated executor that can parse, implement, and run LLM-generated ideas as concrete training configurations on large-scale parallel GPU experiments. 2) Instantiate two realistic execution environments: LLM pre-training and LLM post-training, both parameterized so they can be modified by the LLM’s suggestions. 3) Sample ideas from frontier LLMs and measure how many can be turned into runnable experiments. 4) Use two feedback-driven optimization schemes over ideas: (a) execution-guided evolutionary search that iteratively mutates and selects ideas based on measured performance, and (b) reinforcement learning where the ideator LLM is trained with execution reward signals. 5) Analyze the discovered methods, performance trajectories, and failure modes of both search and RL.

Result: The automated executor can successfully implement a large proportion of sampled LLM ideas in realistic training setups. Execution-guided evolutionary search is highly sample-efficient: in post-training it discovers a method yielding 69.4% performance vs 48.0% for the GRPO baseline, and in pre-training it finds a recipe that reduces training time from 35.9 to 19.7 minutes compared with nanoGPT, all within ten search epochs. LLMs frequently propose meaningful algorithmic variants, but their search tends to plateau and only rarely shows consistent scaling behaviors. RL from execution rewards raises the ideator model’s average reward but fails to increase the best-achieved performance, as training collapses to repetitive, simple ideas (mode collapse).

Conclusion: Automated execution for LLM-generated research ideas is feasible and can power effective search over training algorithms, especially when using execution-guided evolutionary methods. While frontier LLMs can contribute non-trivial algorithmic innovations under this framework, their search capabilities saturate without careful guidance, and straightforward RL on execution rewards is prone to mode collapse and does not reliably discover stronger methods. The paper’s analyses and benchmarks provide a foundation for more robust, execution-grounded systems for automated AI research.

Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.

</details>


### [13] [Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models](https://arxiv.org/abs/2601.14553)
*Brian Christian,Matan Mazor*

Main category: cs.CL

TL;DR: The paper studies whether large language models can make fairer decisions by correctly ignoring biasing information (like gender or race) using counterfactual self-simulation, and proposes using the model’s own blinded API responses to improve fairness and transparency.


<details>
  <summary>Details</summary>
Motivation: Human decision-makers struggle to mentally simulate what they would decide if they did not know biasing attributes (e.g., gender, race), which leads to unfair and biased outcomes even when they intend to be fair. As LLMs are increasingly used in decision support, it is important to know whether they share similar limitations and whether prompting them to ignore biasing attributes actually works. The authors want to understand and mitigate LLM biases related to sensitive attributes and sycophancy by leveraging unique capabilities of LLMs, such as access to their own APIs.

Method: The authors test LLMs on decision-making tasks where certain attributes (like gender or race) could bias outcomes. They compare several prompting strategies: telling the model to ignore sensitive information, asking it to pretend it does not know that information, and giving it access to a “blinded replica” of itself via API that never sees the sensitive attribute. They evaluate whether the resulting decisions are less biased and whether biases are reduced or removed by these strategies, analyzing the difference between implicit bias and deliberate, explicitly biased behavior.

Result: They find that naive prompting strategies—such as instructing the LLM to ignore or pretend not to know biasing information—do not reliably remove bias and can sometimes make it worse. In contrast, when the model is given programmatic access to its own blinded API replica, whose inputs omit gender or race, the resulting decisions become fairer relative to standard prompting. This setup also allows clearer separation of what bias comes from hidden internal associations versus explicit instructions to act in a biased way.

Conclusion: LLMs, like humans, struggle to internally simulate what they would decide without access to biasing information, so simple textual instructions to be fair or ignore sensitive attributes are insufficient and can backfire. However, unlike humans, LLMs can query a blinded version of themselves through an API, effectively outsourcing counterfactual reasoning to a ground-truth process that never sees the sensitive attributes. This leads to fairer, more transparent decisions and offers a practical design pattern for using LLMs in high-stakes domains where fairness with respect to attributes like gender or race matters.

Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.

</details>


### [14] [Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education](https://arxiv.org/abs/2601.14560)
*Unggi Lee,Jiyeong Bae,Jaehyeon Park,Haeun Park,Taejun Park,Younghoon Jeon,Sungmin Cho,Junbo Koh,Yeil Jeong,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: The paper proposes PedagogicalRL-Thinking, a framework that aligns LLMs’ internal reasoning with educational theory using pedagogical reasoning prompts and a reward signal on hidden thinking traces, improving tutoring quality and generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs are being used as tutors, but most optimization focuses only on their visible responses, ignoring how they think and make instructional decisions internally. Existing RL-based tutoring approaches optimize surface-level answers or dialogue quality, not the pedagogical soundness of the underlying reasoning, and they usually rely on generic instructions rather than domain-specific educational theory. This leaves a gap between what education research recommends (e.g., scaffolding, formative feedback, metacognition support) and how LLM tutors actually reason and teach.

Method: The authors propose PedagogicalRL-Thinking, which aligns the hidden reasoning process of LLM tutors with educational theory via two components: (1) Pedagogical Reasoning Prompting, where the model’s internal chain-of-thought is explicitly guided by domain-specific pedagogical principles instead of generic reasoning instructions; and (2) a Thinking Reward, an RL-style reward model that scores the quality of internal reasoning traces based on pedagogical criteria and reinforces traces that exhibit better instructional planning, scaffolding, and feedback behavior. They apply this to LLMs fine-tuned on math tutoring dialogues and compare domain-specific vs generic prompting and different reward setups through experiments and analyses of reasoning traces.

Result: Empirically, domain-specific, theory-grounded pedagogical prompts improve tutoring performance over generic prompts. The Thinking Reward further improves outcomes, particularly when combined with pedagogical prompting. Models trained only on math tutoring data generalize to unseen educational benchmarks, suggesting improved educational capabilities beyond the training domain, while retaining the base model’s factual knowledge. Analysis of reasoning traces shows that the thinking reward leads to more systematic changes: more explicit pedagogical reasoning, clearer instructional structure, and more deliberate decision-making about how to guide the learner.

Conclusion: Aligning LLMs’ internal reasoning with educational theory—rather than optimizing only surface responses—yields better tutoring behavior. PedagogicalRL-Thinking demonstrates that pedagogical reasoning prompts plus a reward signal on hidden thinking can systematically shape how LLMs plan, scaffold, and explain, improving both in-domain tutoring and out-of-domain educational tasks while preserving general knowledge. This suggests that future educational LLMs should target their internal pedagogical thinking processes as a primary object of optimization.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.

</details>


### [15] [Social Caption: Evaluating Social Understanding in Multimodal Models](https://arxiv.org/abs/2601.14569)
*Bhaavanaa Thumu,Leena Mathur,Youssouf Kebe,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: The paper proposes Social Caption, a framework to evaluate social understanding abilities of multimodal large language models (MLLMs) using interaction-theoretic principles.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs interact with humans and must understand social situations in images and videos, but current benchmarks insufficiently capture nuanced social understanding. The authors aim to systematically measure how well MLLMs interpret and reason about social interactions.

Method: They design Social Caption, an evaluation framework based on interaction theory, defining three dimensions: Social Inference (SI) for accurate inference of social interactions, Holistic Social Analysis (HSA) for comprehensive multi-aspect descriptions, and Directed Social Analysis (DSA) for targeted extraction of relevant social details. They then test multiple MLLMs under this framework, varying factors like model size, architecture, and presence of spoken context, and also employ MLLM-based judges to explore scalable automatic evaluation.

Result: The experiments reveal how model size, architecture design, and inclusion of spoken context affect performance across the three social understanding dimensions. They show that MLLM judges can approximate human evaluation and provide useful signals for large-scale assessment of multimodal social understanding.

Conclusion: Social Caption offers a structured way to evaluate and analyze social understanding in MLLMs, highlighting strengths and weaknesses across different capabilities and design choices. It also demonstrates the feasibility and challenges of using MLLM judges for scalable automated evaluation of multimodal social reasoning.

Abstract: Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.

</details>


### [16] [SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation](https://arxiv.org/abs/2601.14615)
*Xichen Zhang,Ziyi He,Yinghao Zhu,Sitong Wu,Shaozuo Yu,Meng Chu,Wenhu Zhang,Haoru Tan,Jiaya Jia*

Main category: cs.CL

TL;DR: The paper introduces SearchGym, a simulated environment and RL curriculum for training search agents that generalize well to real-world web search, avoiding noisy rewards from live APIs or misaligned static data.


<details>
  <summary>Details</summary>
Motivation: Training search agents with RL using live web APIs is too expensive, and training on static web snapshots causes data misalignment, leading to incorrect rewards that either punish good reasoning or reward hallucinations. The authors want a scalable, low-cost, and reliable way to train search agents with clean rewards and strong Sim-to-Real generalization.

Method: They build SearchGym, a simulation environment with a generative pipeline that constructs a verifiable knowledge graph and an aligned document corpus. This ensures that each reasoning task is fully grounded in the underlying data and has a deterministically verifiable solution. On top of this, they propose SearchGym-RL, a curriculum learning framework that trains agents from simple to complex, long-horizon planning tasks using purified, noise-free feedback signals.

Result: Experiments on Llama and Qwen model families show that agents trained in SearchGym transfer effectively to real-world search tasks. Their Qwen2.5-7B-Base model trained in SearchGym outperforms the web-enhanced ASearcher baseline on nine benchmarks, with an average relative improvement of 10.6%.

Conclusion: High-fidelity simulation with a well-aligned knowledge graph and document corpus provides an efficient and scalable way to train strong search agents. Clean, controllable rewards and curriculum-based RL in SearchGym enable robust policies that generalize effectively from simulation to real-world web search scenarios.

Abstract: Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.

</details>


### [17] [Say Anything but This: When Tokenizer Betrays Reasoning in LLMs](https://arxiv.org/abs/2601.14658)
*Navid Ayoobi,Marcus I Armstrong,Arjun Mukherjee*

Main category: cs.CL

TL;DR: The paper shows that non-unique subword tokenizations cause hidden failures in LLM reasoning, and introduces a probe task revealing systematic tokenizer-induced artifacts that create phantom edits.


<details>
  <summary>Details</summary>
Motivation: LLMs operate over token IDs, but modern subword tokenizers often map identical text to multiple distinct token sequences. This one-to-many mapping is rarely scrutinized yet can introduce hidden fragility: the model may internally treat identical strings as different. The authors are motivated to measure how much of apparent reasoning failure is actually due to such representational issues, and to characterize these tokenizer-level problems before resorting to larger models or more data.

Method: They design a tokenization-consistency probe: given text with designated target words, the model must replace only those targets in context and keep all other content unchanged. Because the task is straightforward at the text level, errors can be attributed to tokenizer-detokenizer artifacts rather than knowledge or capacity limitations. They run over 11,000 such replacement trials on several state-of-the-art open-source LLMs, then analyze failure cases qualitatively and categorize systematic tokenization artifacts.

Result: They observe a non-negligible fraction of outputs where models exhibit “phantom edits”: the model appears to reason correctly internally but produces outputs with unintended modifications due to tokenizer-induced representational issues. From these failure cases, they derive a taxonomy of eight systematic tokenizer artifacts, such as whitespace-boundary shifts and intra-word resegmentation. These artifacts reveal that even simple text-editing tasks are undermined by non-unique token encodings.

Conclusion: A significant portion of what looks like reasoning deficiency in LLMs can actually stem from issues in the tokenizer layer, especially from one-to-many token ID mappings that break tokenization consistency. Addressing these tokenizer-level artifacts—via better tokenizer design or constraints—should be a priority and may yield improvements more efficiently than scaling up models and data alone.

Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct "words" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.

</details>


### [18] [AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization](https://arxiv.org/abs/2601.14696)
*Zhaiyu Fang,Ruipeng Sun*

Main category: cs.CL

TL;DR: AdaTIR is a framework that teaches tool-using LLM agents to internalize reasoning on easy tasks and invoke tools only when necessary, greatly reducing tool calls while preserving or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Tool-Integrated Reasoning (TIR) agents over-rely on external tools, invoking them even for simple problems. This cognitive offloading is inefficient, costly, and suggests a lack of genuine agentic intelligence, which should include the ability to decide when tools are actually needed. The paper aims to correct this by enabling agents to adaptively balance internal reasoning and tool usage based on task difficulty.

Method: The authors introduce AdaTIR, which incorporates a difficulty-aware efficiency reward into training so that the agent learns to allocate a dynamic tool-use budget conditioned on task complexity. Simple tasks are encouraged to be solved via internal reasoning, while complex ones still permit external tools. They further identify a sign reversal problem in reward design, where penalties for tool use can outweigh correctness rewards and mis-train the agent. To fix this, they propose Clipped Advantage Shaping (CAS), a modification to the advantage signal that prioritizes correctness while including efficiency as a constrained secondary objective.

Result: AdaTIR substantially cuts down tool usage, reducing tool calls by up to 97.6% on simple tasks and 28.2% on harder ones, yet maintains or improves answer quality. On AIME 2024, the model trained with AdaTIR even outperforms baselines by 4.8% in a no-tools setting, showing successful internalization of reasoning skills rather than superficial reliance on tools.

Conclusion: Adaptive control of tool usage—rather than maximal or static use—is key to more intelligent tool-augmented LLM agents. By combining difficulty-aware efficiency rewards with Clipped Advantage Shaping, AdaTIR trains agents that use tools sparingly and judiciously, internalize reasoning for easy tasks, and reach higher accuracy even without tools, indicating more robust underlying capabilities.

Abstract: Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework that shifts the paradigm from static tool invocation to difficulty-aware reasoning internalization. By introducing a difficulty-aware efficiency reward, AdaTIR dynamically adjusts tool budgets based on task complexity--internalizing reasoning for simple tasks while selectively invoking tools for complex tasks. Furthermore, we identify a sign reversal problem where tool penalties outweigh correctness rewards, mistakenly penalizing correct rollouts with negative advantages. To resolve this, we propose Clipped Advantage Shaping (CAS), which ensures that correctness remains the primary objective while using efficiency as a secondary constraint. Empirical results demonstrate that AdaTIR reduces tool calls by up to 97.6% on simple tasks and 28.2% on complex challenges while maintaining or enhancing accuracy. Notably, AdaTIR successfully internalizes reasoning, outperforming baselines by 4.8% on AIME 2024 even when tool access is strictly disabled.

</details>


### [19] [ClaimDB: A Fact Verification Benchmark over Large Structured Data](https://arxiv.org/abs/2601.14698)
*Michael Theologitis,Preetam Prabhu Srikar Dammu,Chirag Shah,Dan Suciu*

Main category: cs.CL

TL;DR: ClaimDB is a new fact-verification benchmark where claims must be checked against large real-world relational databases using program-like reasoning instead of simple text reading.


<details>
  <summary>Details</summary>
Motivation: Most fact-verification research focuses on text (web pages, Wikipedia), while many real-world tasks need verifying claims against large structured databases with millions of records and many tables. Existing benchmarks don’t capture the complexity of cross-table, large-scale, real-life databases, and current LLMs aren’t well-understood in this setting. The authors aim to fill this gap and to stress-test LLMs’ ability to reason over structured data and to abstain when evidence is insufficient.

Method: The authors construct ClaimDB, a benchmark built from 80 realistic databases across diverse domains (governance, healthcare, media, education, natural sciences). Claims are paired with evidence that comes from composing queries across multiple tables and millions of rows. They then evaluate 30 state-of-the-art LLMs (both proprietary and open-source models under 70B parameters) on this benchmark, measuring verification accuracy and the ability to abstain when evidence is lacking. Evidence is accessed via executable reasoning (e.g., program/query-like operations) rather than by reading text passages.

Result: No evaluated model exceeds 83% accuracy on ClaimDB; more than half of the models achieve under 55% accuracy. Both closed- and open-source LLMs show pronounced weaknesses in abstention: they often fail to say “unknown” or “not enough evidence” when the database does not support a definitive decision. This indicates that current LLMs are far from robust for large-scale structured-data fact verification.

Conclusion: ClaimDB exposes a challenging, underexplored regime of fact verification over large, realistic relational databases where current LLMs perform poorly, both in accuracy and in calibrated abstention. The benchmark encourages a move from text-based evidence reading to executable program-style reasoning and highlights reliability concerns for deploying LLMs in high-stakes data analysis. The authors publicly release the benchmark, code, and leaderboard to drive further research in this area.

Abstract: Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on "reading" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .

</details>


### [20] [DARL: Encouraging Diverse Answers for General Reasoning without Verifiers](https://arxiv.org/abs/2601.14700)
*Chongxuan Huang,Lei Lin,Xiaodong Shi,Wenping Hu,Ruiming Tang*

Main category: cs.CL

TL;DR: The paper introduces DARL, a reinforcement learning framework that improves both reasoning accuracy and output diversity for large language models by encouraging controlled deviation from reference answers, outperforming prior general-domain RL methods like RLPR on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RL with verifiable rewards (RLVR) improves LLM reasoning but relies on domain-specific verifiers, limiting applicability. Newer approaches like RLPR work in general domains but heavily overfit to single reference answers, hurting diversity and especially failing on open-ended tasks with many valid responses. The authors want a method that keeps the benefits of general-domain RL training while avoiding overfitting to references and better supporting diverse yet aligned outputs.

Method: The authors propose DARL, a reinforcement learning framework that modifies the reward structure used when training LLMs from reference answers. Instead of rewarding only close matches to a single reference, DARL encourages the model to generate outputs that stay within a controlled deviation range from the reference while still being aligned with it. This reward shaping explicitly promotes diverse responses that are not identical to the reference but remain plausible and relevant. DARL is designed to be compatible with existing general RL methods (like RLPR) and does not require extra external verifiers, so it can be plugged into existing training setups in open domains.

Result: Through experiments on thirteen benchmarks, including six reasoning-focused and seven general tasks, models trained with DARL consistently outperform baselines. DARL exceeds RLPR by an average of 1.3 points on reasoning benchmarks and 9.5 points on general benchmarks, demonstrating that it not only improves reasoning accuracy but also enhances the diversity of generated outputs in open-ended tasks.

Conclusion: DARL effectively addresses the overfitting-to-reference problem in general-domain RL for LLMs, allowing models to produce diverse yet aligned answers without relying on domain-specific verifiers. Its plug-and-play compatibility with existing RL approaches and its strong empirical gains across many benchmarks suggest it is a practical and scalable way to improve both reasoning performance and output diversity in large language models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated promising gains in enhancing the reasoning capabilities of large language models. However, its dependence on domain-specific verifiers significantly restricts its applicability to open and general domains. Recent efforts such as RLPR have extended RLVR to general domains, enabling training on broader datasets and achieving improvements over RLVR. However, a notable limitation of these methods is their tendency to overfit to reference answers, which constrains the model's ability to generate diverse outputs. This limitation is particularly pronounced in open-ended tasks such as writing, where multiple plausible answers exist. To address this, we propose DARL, a simple yet effective reinforcement learning framework that encourages the generation of diverse answers within a controlled deviation range from the reference while preserving alignment with it. Our framework is fully compatible with existing general reinforcement learning methods and can be seamlessly integrated without additional verifiers. Extensive experiments on thirteen benchmarks demonstrate consistent improvements in reasoning performance. Notably, DARL surpasses RLPR, achieving average gains of 1.3 points on six reasoning benchmarks and 9.5 points on seven general benchmarks, highlighting its effectiveness in improving both reasoning accuracy and output diversity.

</details>


### [21] [Typhoon OCR: Open Vision-Language Model For Thai Document Extraction](https://arxiv.org/abs/2601.14722)
*Surapon Nonesung,Natapong Nitarach,Teetouch Jaknamon,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: Typhoon OCR is an open, Thai- and English-capable vision-language OCR model that performs transcription and layout/structure extraction, achieving performance comparable to larger proprietary systems while being compact and efficient.


<details>
  <summary>Details</summary>
Motivation: Most existing vision-language OCR models work well only for high-resource, mainly Latin-script languages and often struggle on Thai because of its complex script, lack of word boundaries, and messy real-world document layouts. There is a need for an open, lightweight system that can accurately extract text and structure from diverse Thai documents rather than relying on expensive proprietary models and language-specific metadata.

Method: The authors start from existing vision-language backbones and fine-tune them into Typhoon OCR using a Thai-centric training corpus. They build this corpus via a multi-stage pipeline that combines: (1) traditional OCR outputs, (2) restructuring with powerful VLMs to improve layout/structure labels, and (3) curated synthetic documents to cover challenging layouts and scripts. They design a unified VLM framework able to perform text transcription, layout reconstruction, and document-level structural reasoning. In version V1.5, they optimize the architecture to be compact and inference-efficient, with reduced dependence on external metadata to ease deployment.

Result: On comprehensive benchmarks spanning many Thai document types—financial reports, government forms, books, infographics, and handwritten pages—Typhoon OCR matches or surpasses larger proprietary frontier systems on text extraction and layout/structure tasks, while using substantially fewer computational resources. It remains competitive on English as well.

Conclusion: An open, lightweight VLM-based OCR system can handle complex Thai documents with accuracy comparable to proprietary offerings, both in transcription and in structural/layout reconstruction. Typhoon OCR V1.5 demonstrates that carefully constructed Thai-focused training data and a unified VLM framework enable strong performance across diverse, real-world document types while remaining practical to deploy.

Abstract: Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.

</details>


### [22] [Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning](https://arxiv.org/abs/2601.14750)
*Yifan Wang,Shiyu Li,Peiming Li,Xiaochen Yang,Yang Tang,Zheng Wei*

Main category: cs.CL

TL;DR: Introduces Render-of-Thought (RoT), which compresses chain-of-thought reasoning by encoding reasoning steps as images using vision encoders in VLMs, achieving 3–4x token compression and faster inference with competitive performance.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting improves LLM reasoning but is verbose, causing high computational cost. Existing methods supervise only final answers, not intermediate reasoning, making latent reasoning chains hard to analyze and align. The authors aim to reduce CoT verbosity, speed up inference, and make intermediate reasoning more explicit and traceable without expensive additional pre-training.

Method: They propose Render-of-Thought (RoT), a framework that converts the reasoning chain into images instead of long text. RoT uses off-the-shelf vision encoders from Vision-Language Models as semantic anchors to align visual embeddings with the language space. This alignment allows the model to encode intermediate reasoning steps visually and decode them when needed, in a plug-and-play manner that avoids extra large-scale pre-training.

Result: On mathematical and logical reasoning benchmarks, RoT yields 3–4x token compression of the reasoning process and significantly faster inference compared with explicit textual CoT, while retaining competitive accuracy relative to standard CoT and other reasoning methods.

Conclusion: Rendering intermediate reasoning as images via RoT is a feasible and efficient alternative to purely textual chain-of-thought. It makes reasoning more compact and traceable, accelerates inference, and maintains strong performance, suggesting a promising new paradigm for supervising and representing LLM reasoning chains.

Abstract: Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT

</details>


### [23] [RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models](https://arxiv.org/abs/2601.14780)
*Anqi Li,Yuqian Chen,Yu Lu,Zhaoming Chen,Yuan Xie,Zhenzhong Lan*

Main category: cs.CL

TL;DR: The paper introduces PsyFIRE, a fine-grained theoretical framework of client resistance in text counseling, and RECAP, a two-stage NLP model trained on a new annotated corpus to detect and explain resistance behaviors, achieving strong performance and practical clinical value.


<details>
  <summary>Details</summary>
Motivation: Client resistance is crucial to recognize in therapy because it affects outcomes, but in text-based counseling it is hard to detect automatically. Prior NLP work uses coarse labels, neglects sequential therapeutic dynamics, and offers little interpretability for clinicians, limiting real-world usefulness.

Method: The authors first develop PsyFIRE, a theory-driven scheme with 13 specific resistance behaviors plus collaborative behaviors, then build the ClientResistance corpus of ~24k Chinese counseling utterances with context-linked rationales. Using this dataset, they design RECAP, a two-stage framework: (1) detect whether an utterance expresses collaboration or resistance, and (2) classify fine-grained resistance types and generate explanations, benchmarking it against prompt-based large language models.

Result: RECAP reaches 91.25% F1 in binary collaboration-vs-resistance detection and 66.58% macro-F1 across 13 resistance categories, outperforming strong prompt-based LLM baselines by over 20 F1 points. In external applications to another counseling dataset and a pilot study with 62 counselors, RECAP effectively uncovers common resistance patterns and their negative association with therapeutic relationships, and supports counselors in better recognizing and responding to resistance.

Conclusion: The study demonstrates that a theory-grounded, fine-grained annotation framework (PsyFIRE) combined with a tailored two-stage model (RECAP) can accurately, interpretablely detect client resistance in text counseling and generalizes across datasets. This approach not only advances NLP modeling of nuanced therapeutic processes but also shows concrete potential to enhance clinicians’ awareness of resistance and refine intervention strategies in real-world practice.

Abstract: Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.
  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.
  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.

</details>


### [24] [Comparative Study of Large Language Models on Chinese Film Script Continuation: An Empirical Analysis Based on GPT-5.2 and Qwen-Max](https://arxiv.org/abs/2601.14826)
*Yuxuan Cao,Zida Yang,Ye Wang*

Main category: cs.CL

TL;DR: The paper builds a Chinese film-script continuation benchmark and finds GPT-5.2 produces higher-quality, more structurally faithful continuations than Qwen-Max, despite slightly lower ROUGE-L.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used for creative writing, but their performance on culturally specific, narrative-rich tasks like Chinese film script continuation is underexplored; the authors want a systematic, reproducible way to compare models on such tasks.

Method: They curate a benchmark from 53 classic Chinese films and adopt a “first half to second half” script continuation setup, generating three continuations per film with GPT-5.2 and Qwen-Max. They filter for validity and evaluate valid outputs with ROUGE-L, a structural similarity metric, and LLM-as-judge scores from DeepSeek-Reasoner, then run statistical analyses (including effect sizes) on 144 paired samples.

Result: Qwen-Max has slightly higher ROUGE-L scores, but GPT-5.2 significantly outperforms in structural preservation, overall quality, and composite scores, with a large effect size for overall quality. GPT-5.2 better maintains character consistency, tone and style, and script formatting, whereas Qwen-Max shows weaker generation stability.

Conclusion: For Chinese film script continuation, automatic lexical similarity (ROUGE) alone is misleading: GPT-5.2, not Qwen-Max, is superior when considering structure and human-like quality. The benchmark and evaluation pipeline offer a reusable framework for assessing LLMs on culturally specific creative writing tasks.

Abstract: As large language models (LLMs) are increasingly applied to creative writing, their performance on culturally specific narrative tasks warrants systematic investigation. This study constructs the first Chinese film script continuation benchmark comprising 53 classic films, and designs a multi-dimensional evaluation framework comparing GPT-5.2 and Qwen-Max-Latest. Using a "first half to second half" continuation paradigm with 3 samples per film, we obtained 303 valid samples (GPT-5.2: 157, 98.7% validity; Qwen-Max: 146, 91.8% validity). Evaluation integrates ROUGE-L, Structural Similarity, and LLM-as-Judge scoring (DeepSeek-Reasoner).
  Statistical analysis of 144 paired samples reveals: Qwen-Max achieves marginally higher ROUGE-L (0.2230 vs 0.2114, d=-0.43); however, GPT-5.2 significantly outperforms in structural preservation (0.93 vs 0.75, d=0.46), overall quality (44.79 vs 25.72, d=1.04), and composite scores (0.50 vs 0.39, d=0.84). The overall quality effect size reaches large effect level (d>0.8).
  GPT-5.2 excels in character consistency, tone-style matching, and format preservation, while Qwen-Max shows deficiencies in generation stability. This study provides a reproducible framework for LLM evaluation in Chinese creative writing.

</details>


### [25] [HiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model](https://arxiv.org/abs/2601.14857)
*Motong Tian,Allen P. Wong,Mingjun Mao,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: The paper introduces HiNS, a data construction framework that models hierarchical difficulty levels of negative samples and their natural distribution in dialogue, to train better embedding models for memory-augmented language agents.


<details>
  <summary>Details</summary>
Motivation: Existing memory retrieval for language agents depends on embedding models trained with negative samples that do not reflect real-world difficulty structure. Current datasets use synthetic or uniformly sampled negatives, ignoring that some negatives are hard (semantically close) and some are easy (irrelevant), and that natural conversations have specific proportions of each. This mismatch limits the ability of embeddings to finely distinguish relevant from distractor memories, hurting retrieval in realistic scenarios.

Method: The authors propose HiNS, a principled framework for constructing training data that (1) explicitly categorizes negative samples into difficulty tiers (e.g., close distractors vs trivial negatives) and (2) calibrates the ratio of these tiers based on empirical statistics from human-agent conversational data. They then train embedding models for memory retrieval using these structured negatives and evaluate performance on memory-intensive dialogue benchmarks.

Result: Using HiNS to train embedding models leads to significantly better memory retrieval quality and downstream dialogue performance. On the LoCoMo benchmark, they report F1/BLEU-1 improvements of 3.27%/3.30% for MemoryOS and 1.95%/1.78% for Mem0. On the PERSONAMEM dataset, total scores improve by 1.19% for MemoryOS and 2.55% for Mem0, demonstrating improved retrieval fidelity and generalization.

Conclusion: Modeling the hierarchical difficulty and natural distribution of negative samples in training data substantially improves embedding models for memory-augmented language agents. HiNS provides a principled way to build more realistic training sets, yielding more robust and generalizable memory retrieval in dialogue systems, as evidenced by consistent gains across multiple benchmarks and agent frameworks.

Abstract: Memory-augmented language agents rely on embedding models for effective memory retrieval. However, existing training data construction overlooks a critical limitation: the hierarchical difficulty of negative samples and their natural distribution in human-agent interactions. In practice, some negatives are semantically close distractors while others are trivially irrelevant, and natural dialogue exhibits structured proportions of these types. Current approaches using synthetic or uniformly sampled negatives fail to reflect this diversity, limiting embedding models' ability to learn nuanced discrimination essential for robust memory retrieval. In this work, we propose a principled data construction framework HiNS that explicitly models negative sample difficulty tiers and incorporates empirically grounded negative ratios derived from conversational data, enabling the training of embedding models with substantially improved retrieval fidelity and generalization in memory-intensive tasks. Experiments show significant improvements: on LoCoMo, F1/BLEU-1 gains of 3.27%/3.30%(MemoryOS) and 1.95%/1.78% (Mem0); on PERSONAMEM, total score improvements of 1.19% (MemoryOS) and 2.55% (Mem0).

</details>


### [26] [Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14896)
*Rui Qi,Fengran Mo,Yufeng Chen,Xue Zhang,Shuo Wang,Hongliang Li,Jinan Xu,Meng Jiang,Jian-Yun Nie,Kaiyu Huang*

Main category: cs.CL

TL;DR: The paper proposes LcRL, a reinforcement-learning framework to improve multilingual retrieval-augmented generation by reducing knowledge bias and cross-lingual conflicts in retrieved evidence.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual RAG systems usually treat semantically equivalent queries in different languages with a single, uniform retrieval and optimization process. This "one-size-fits-all" setup is suboptimal because multilingual retrieval introduces language-specific knowledge bias (some languages or sources dominate) and knowledge conflict (retrieved evidence in different languages may disagree). The authors want to address these issues so MRAG systems can better exploit multilingual corpora, especially in realistic, low-resource and many-language scenarios.

Method: They design LcRL, a multilingual search-augmented reinforcement learning framework. It introduces a language-coupled Group Relative Policy Optimization, applied to both the policy and reward models. In the rollout stage, they perform language-coupled group sampling, where semantically aligned queries across languages are grouped to explicitly control and balance knowledge from different languages, aiming to reduce knowledge bias. In the reward model, they add an auxiliary anti-consistency penalty that discourages inconsistent or conflicting use of knowledge across languages, thus mitigating knowledge conflict. The approach is trained end-to-end within an RL framework that interacts with a multilingual search engine.

Result: Experiments show that LcRL matches or surpasses state-of-the-art MRAG baselines on multilingual tasks. It is particularly effective in practical conditions such as when training data is limited or when retrieval is performed over collections with a large number of languages. These results support the claim that the proposed language-coupled optimization and anti-consistency mechanisms successfully address knowledge bias and conflict in MRAG.

Conclusion: The paper concludes that a uniform, language-agnostic optimization pipeline is inadequate for multilingual RAG. By explicitly coupling languages during RL training—via group-based policy optimization and anti-consistency regularization in the reward model—LcRL achieves better, more robust multilingual performance. The framework generalizes well to challenging real-world setups (few data, many languages), indicating it is a practical solution for improving MRAG systems.

Abstract: Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.

</details>


### [27] [PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation](https://arxiv.org/abs/2601.14903)
*Chenning Xu,Mao Zheng,Mingyu Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: PodBench is a new benchmark and evaluation framework for testing how well LLMs generate long, structured podcast-style dialogues from complex, lengthy inputs.


<details>
  <summary>Details</summary>
Motivation: Podcast script generation needs models to handle long context, multiple speakers, and structured, grounded conversation, but there are few systematic, standardized resources to evaluate these capabilities. This limits progress and fair comparison across models.

Method: The authors build PodBench, a dataset of 800 podcast-generation tasks with inputs up to 21K tokens and intricate multi-speaker instructions. They design a multifaceted evaluation framework combining hard quantitative constraints (e.g., structure, length, speaker roles) with LLM-based qualitative assessment of generation quality. They then run extensive experiments comparing proprietary and open-source LLMs, including models with explicit reasoning mechanisms, on this benchmark.

Result: Proprietary LLMs tend to achieve the best overall performance, but open-source models with explicit reasoning strategies are more robust to very long contexts and complex multi-speaker coordination than standard open-source baselines. The experiments also show a systematic mismatch where models that follow instructions well do not necessarily produce content that is rich or substantive.

Conclusion: PodBench provides a reproducible benchmark and evaluation methodology for long-form, audio-centric dialogue generation, highlighting current model strengths and weaknesses—especially the gap between instruction adherence and content substance—and enabling more targeted future improvements in podcast script generation systems.

Abstract: Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.

</details>


### [28] [CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents](https://arxiv.org/abs/2601.14914)
*Tianxiang Fei,Cheng Chen,Yue Pan,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: The paper introduces CodeDelegator, a multi-agent LLM framework that separates planning from code execution to reduce context pollution and improve long-horizon task performance.


<details>
  <summary>Details</summary>
Motivation: Single LLM agents that both plan and implement code suffer from context pollution: debugging traces, intermediate failures, and long interaction histories harm performance on complex, long-horizon real-world tasks. There is a need for an approach that preserves strategic oversight while keeping implementation contexts clean and focused.

Method: The authors design CodeDelegator, a multi-agent architecture with role specialization. A persistent Delegator agent handles high-level planning: decomposing tasks, writing detailed specifications, and tracking progress without executing code. For each sub-task, a fresh Coder agent is spawned with a clean context containing only the relevant specification. They introduce Ephemeral-Persistent State Separation (EPSS) to separate each Coder's local execution/debugging state from the Delegator’s global state so that noisy traces do not pollute strategic planning.

Result: On a variety of benchmarks and scenarios, CodeDelegator outperforms baseline approaches that use a single agent or less-structured multi-agent methods, showing improved effectiveness on complex and long-horizon tasks. The framework demonstrates better robustness to failures and debugging noise.

Conclusion: Separating planning from implementation via specialized agents and enforcing Ephemeral-Persistent State Separation significantly improves LLM-based code agents on complex tasks. CodeDelegator offers a practical architecture for robust, long-horizon code generation and execution by maintaining clean contexts for Coders and stable strategic oversight by the Delegator.

Abstract: Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.

</details>


### [29] [The GDN-CC Dataset: Automatic Corpus Clarification for AI-enhanced Democratic Citizen Consultations](https://arxiv.org/abs/2601.14944)
*Pierre-Antoine Lequeu,Léo Labat,Laurène Cave,Gaël Lejeune,François Yvon,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: The paper introduces a framework and datasets for using small, open-weights language models to clarify and structure noisy citizen consultation texts into argumentative units for downstream political and topic analysis.


<details>
  <summary>Details</summary>
Motivation: Citizen consultation and online deliberation platforms generate large amounts of noisy, unstructured, and multi-topic text. For democratic analysis, topic modeling, and political science research, these contributions need to be standardized at a pragmatic and argumentative level. Existing work raises ethical concerns about using large proprietary LLMs for this analysis due to transparency, control, and resource constraints. The authors aim to create resources and methods that support ethical, reproducible analysis using smaller, locally runnable models.

Method: They propose 'Corpus Clarification', a preprocessing framework that takes noisy, multi-topic citizen contributions and restructures them into clear, self-contained argumentative units suitable for downstream tasks. They construct GDN-CC, a manually curated dataset from the French Grand Débat National containing 1,231 contributions split into 2,285 argumentative units, annotated for argumentative structure and manually clarified. They then finetune small, open-weights language models on this dataset and compare their performance to larger LLMs in reproducing the annotations. They also evaluate these models in an opinion clustering task. Finally, they apply their best models to automatically annotate a much larger corpus, GDN-CC-large, with 240k contributions.

Result: Finetuned small language models are able to match or even outperform larger LLMs on the task of reproducing the human-provided argumentative structure and clarification annotations. These models are also shown to be practically usable for an opinion clustering task, indicating that the clarified argumentative units are effective for downstream democratic text analysis. Using the trained models, the authors automatically annotate 240,000 additional contributions, producing GDN-CC-large.

Conclusion: Corpus Clarification is an effective approach for transforming noisy democratic consultation texts into structured argumentative units suitable for analysis. Small, open-weights language models, once finetuned, can reliably perform this clarification and annotation, rivaling or surpassing larger LLMs while being more transparent and resource-efficient. The released datasets (GDN-CC and GDN-CC-large) provide substantial new resources for democratic consultation research, topic modeling, and political text analysis.

Abstract: LLMs are ubiquitous in modern NLP, and while their applicability extends to texts produced for democratic activities such as online deliberations or large-scale citizen consultations, ethical questions have been raised for their usage as analysis tools. We continue this line of research with two main goals: (a) to develop resources that can help standardize citizen contributions in public forums at the pragmatic level, and make them easier to use in topic modeling and political analysis; (b) to study how well this standardization can reliably be performed by small, open-weights LLMs, i.e. models that can be run locally and transparently with limited resources. Accordingly, we introduce Corpus Clarification as a preprocessing framework for large-scale consultation data that transforms noisy, multi-topic contributions into structured, self-contained argumentative units ready for downstream analysis. We present GDN-CC, a manually-curated dataset of 1,231 contributions to the French Grand Débat National, comprising 2,285 argumentative units annotated for argumentative structure and manually clarified. We then show that finetuned Small Language Models match or outperform LLMs on reproducing these annotations, and measure their usability for an opinion clustering task. We finally release GDN-CC-large, an automatically annotated corpus of 240k contributions, the largest annotated democratic consultation dataset to date.

</details>


### [30] [CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning](https://arxiv.org/abs/2601.14952)
*Zhiyuan Lu,Chenliang Li,Yingcheng Shi,Weizhou Shen,Ming Yan,Fei Huang*

Main category: cs.CL

TL;DR: The paper introduces CorpusQA, a new 10M-token-scale benchmark and synthesis framework to evaluate and improve LLMs’ ability to perform true corpus-level reasoning across large document collections.


<details>
  <summary>Details</summary>
Motivation: Current long-context LLM benchmarks mostly focus on single documents or assume that answers lie in a few relevant chunks, which does not reflect real corpus-level tasks where evidence is dispersed and requires global integration, comparison, and aggregation. There is a need for a scalable, reliable way to test and train models on holistic reasoning over large, unstructured text collections.

Method: They propose CorpusQA, built with a data synthesis framework that decouples reasoning from textual representation. The framework programmatically generates complex, computation-intensive questions over large synthetic corpora, along with guaranteed ground-truth answers. This allows creation of up-to-10M-token testbeds where models must integrate evidence across many documents. They also use this synthetic data to fine-tune LLMs and evaluate various architectures, including standard long-context models, retrieval-augmented generation systems, and memory-augmented agentic architectures.

Result: Experiments show that even cutting-edge long-context LLMs experience substantial performance degradation as context size grows, and typical retrieval-augmented generation pipelines fail (“collapse”) on these corpus-level tasks. Fine-tuning on the synthesized CorpusQA data improves models’ general long-context reasoning. Memory-augmented agentic architectures handle the tasks more robustly than standard approaches.

Conclusion: CorpusQA exposes a significant gap between current long-context capabilities and the demands of real corpus-level reasoning. Simply extending context windows or using standard RAG is insufficient. Memory-augmented, agentic architectures appear more promising for global information synthesis, and synthetic corpus-scale benchmarks like CorpusQA are effective both for evaluation and for training models to reason over very large text repositories.

Abstract: While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a "sparse retrieval" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.

</details>


### [31] [A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala](https://arxiv.org/abs/2601.14958)
*Minuri Rajapakse,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: Benchmarking open and closed LMs on Unicode and Romanized Sinhala, showing which models work best for each script and underscoring the impact of script-specific training data.


<details>
  <summary>Details</summary>
Motivation: Performance of language models on lower-resource, morphologically rich languages like Sinhala, especially in its widely used Romanized form, is poorly understood. Practitioners lack guidance on which models to use for Sinhala-specific tasks and how script variation affects model behavior.

Method: Construct a diverse corpus covering both Unicode Sinhala and Romanized Sinhala. Evaluate open-source language models quantitatively using perplexity on this corpus, and assess closed-source models qualitatively via sentence-completion tasks. Compare model performance across scripts and families (Mistral, Llama, Gemini, DeepSeek, Claude).

Result: Among open models, Mistral-Nemo-Base-2407 achieves the best perplexity on Unicode Sinhala, while Mistral-7B-v0.3 performs best on Romanized Sinhala; Llama-3.1-8B shows strong, balanced performance on both scripts. For closed models, Gemini-1.5-pro and DeepSeek generate higher-quality Unicode Sinhala, whereas Claude-3.5-Sonnet is strongest on Romanized Sinhala, revealing substantial performance gaps between systems and scripts.

Conclusion: Different models specialize in different scripts: no single LM dominates across Unicode and Romanized Sinhala. LMs’ effectiveness for Sinhala is highly dependent on script-specific training exposure, so model choice for real-world Sinhala applications should be guided by the target script. The benchmark offers a practical reference and underscores the importance of including both Unicode and Romanized variants in training data for robust Sinhala handling.

Abstract: The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.

</details>


### [32] [Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora](https://arxiv.org/abs/2601.14994)
*Chaymaa Abbas,Nour Shamaa,Mariette Awad*

Main category: cs.CL

TL;DR: The paper studies how data contamination in multilingual settings, especially via Arabic translations, affects LLM evaluation and proposes a translation-aware detection method that works even when English-only methods miss contamination.


<details>
  <summary>Details</summary>
Motivation: Existing contamination detection methods mostly target English benchmarks, leaving a gap in understanding and detecting contamination when benchmarks are translated into other languages, which can hide memorization while still benefiting models.

Method: The authors fine-tune several open-weight LLMs on different proportions of Arabic datasets that overlap with English benchmarks, evaluate performance on the original English benchmarks, extend the Tested Slot Guessing method with a choice-reordering strategy, and add Min-K% probability analysis to capture distributional memorization signals. They then propose a Translation-Aware Contamination Detection framework that compares contamination signals across multiple translated versions of the same benchmark.

Result: They find that translating benchmarks into Arabic weakens standard contamination indicators in English, yet models still gain performance from contaminated data, particularly those with strong Arabic abilities. This hidden contamination is revealed through higher Min-K% scores and greater cross-lingual answer consistency as contamination increases. The proposed Translation-Aware Contamination Detection successfully exposes contamination even when English-only methods do not.

Conclusion: Multilingual contamination can remain hidden if evaluation and detection focus only on English benchmarks. Translation-aware, multilingual evaluation and detection methods, such as the proposed Translation-Aware Contamination Detection, are necessary to ensure fair and reliable assessment of LLMs in multilingual contexts.

Abstract: Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.
  Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.

</details>


### [33] [Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction](https://arxiv.org/abs/2601.15037)
*Xiaonan Jing,Gongqing Wu,Xingrui Zhuo,Lang Sun,Jiapu Wang*

Main category: cs.CL

TL;DR: Proposes KRPO, a self-reflective prompt-optimization framework that lets LLMs iteratively improve at open-domain relational triplet extraction without predefined schemas, achieving better F1 than strong baselines.


<details>
  <summary>Details</summary>
Motivation: Open-domain relational triplet extraction aims to mine subject–relation–object triples from text without fixed schemas, but current LLM-based approaches rely on static, heuristic prompts. Without a reflection or feedback mechanism, LLMs tend to reinforce early mistakes and are especially vulnerable to semantic ambiguity, leading to persistent erroneous extraction patterns and redundant or inconsistent relations. There is a need for a mechanism that can provide intrinsic feedback and adapt prompts over time to improve extraction quality and relation consistency.

Method: The authors propose KRPO, a Knowledge Reconstruction-driven Prompt Optimization framework. First, they introduce a self-evaluation mechanism based on knowledge restoration: extracted triplets are projected back into a semantic space and evaluated with semantic consistency scores, providing intrinsic feedback on extraction quality. Using this feedback, they design a textual-gradient-based prompt optimizer that treats prompt editing as an iterative optimization process, internalizing historical errors and successes to refine prompts over time. Additionally, they build a relation canonicalization memory that stores representative relations and uses them to map diverse surface forms into semantically distinct, canonical relation schemas, thereby reducing relation redundancy in the final triples.

Result: Across three benchmarks for open-domain relational triplet extraction, KRPO achieves significantly higher extraction F1 scores than strong baseline methods, demonstrating improved precision and recall in complex, schema-free extraction scenarios.

Conclusion: KRPO shows that coupling LLM-based triplet extraction with an intrinsic self-evaluation signal and an iterative, gradient-inspired prompt optimizer can substantially enhance open-domain relational triplet extraction. The added relation canonicalization memory further improves the structure and usability of the resulting knowledge by reducing redundancy. This establishes KRPO as an effective and adaptable framework for complex ORTE workflows and suggests a promising direction for self-improving LLM-based information extraction systems.

Abstract: Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.

</details>


### [34] [\textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering](https://arxiv.org/abs/2601.15050)
*Zhichao Yan,Yunxiao Zhao,Jiapu Wang,Jiaoyan Chen,Shaoru Guo,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: The paper proposes LogicScore, a framework to evaluate not just factual attribution but global logical coherence of long-form answers in attributed QA, revealing large gaps between models’ fact grounding and their reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Existing AQA evaluation focuses on checking whether individual statements are correctly attributed to sources, ignoring whether the whole multi-step reasoning is logically coherent, complete, non-redundant, and leads deterministically to the answer. This creates a blind spot where LLMs can appear strong by attribution metrics while still producing incoherent reasoning. The authors aim to fill this gap with a principled, logic-based evaluation of global reasoning.

Method: They design LogicScore, a unified evaluation framework based on Horn Rules and a backward verification mechanism. The framework reconstructs and evaluates the reasoning chain behind long-form answers along three dimensions: Completeness (every step of the deduction is logically supported), Conciseness (removal of redundant steps or information), and Determinateness (the reasoning consistently entails the answer rather than multiple incompatible conclusions). The system operates over multi-hop QA settings and can be applied across different LLMs.

Result: Across three multi-hop QA datasets (HotpotQA, MusiQue, 2WikiMultiHopQA) and more than 20 LLMs (e.g., GPT-5, Gemini-3-Pro, LLaMA3, and task-tuned models), LogicScore reveals that many top-performing models score very well on attribution precision (e.g., 92.85% for Gemini-3 Pro) but fare poorly on global reasoning metrics, such as achieving only 35.11% in Conciseness. This exposes a substantial discrepancy between factual grounding and reasoning quality.

Conclusion: LogicScore provides a robust standard for evaluating the logical structure of long-form LLM answers, demonstrating that high factual attribution does not imply sound global reasoning. The authors argue that future LLM development and evaluation should emphasize reasoning coherence (completeness, conciseness, determinateness) in addition to factual accuracy, and they release their code to support further research.

Abstract: Current evaluation methods for Attributed Question Answering (AQA) suffer from \textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \textit{Completeness} (logically sound deduction), \textit{Conciseness} (non-redundancy), and \textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.

</details>


### [35] [Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure](https://arxiv.org/abs/2601.15077)
*Christopher Scofield*

Main category: cs.CL

TL;DR: Formal explanation for why multi-agent LLM systems can outperform single agents, using operator theory and constraint optimization.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems of LLMs often solve problems better than single models even when all agents see the same information and have similar capabilities. This empirical fact lacks a rigorous theoretical explanation. The paper aims to formally explain how and why distributing reasoning across multiple interacting agents can access solution structures that a single agent, applying all constraints at once, typically cannot reach.

Method: The authors model each agent as an operator that enforces its own family of validity constraints on a shared solution state. Using operator theory and constrained optimization, they represent a multi-agent system as a factorized composition of these constraint-enforcement operators. They analyze the dynamics of iteratively applying these operators (agents) to the shared state, study convergence properties, and characterize the invariant sets corresponding to intersections of constraint sets. They then generalize from exact (hard) constraint operators to soft constraints modeled via proximal operators, and illustrate the framework on text-based dialogue systems built from LLM agents.

Result: They prove that under mild conditions, the iterative interaction of agents—each enforcing its own constraints—converges to invariant solution sets corresponding to the intersection of the agents’ constraint sets. These invariant sets are generally not reachable by a single agent that attempts to apply all constraints in a monolithic way, even with the same information and expressive power. The extension to proximal operators shows similar behavior for soft constraints, maintaining the advantage of multi-agent factorization in more realistic, non-exact settings, and they demonstrate applicability to modern LLM-based dialog systems.

Conclusion: The paper concludes that the superior performance often observed in multi-agent LLM systems is not merely an empirical curiosity but has a principled explanation: factorizing constraint enforcement across multiple agents induces dynamics that can reach solution regions inaccessible to a single, unified agent. This operator-theoretic view explains why multi-agent collaboration can systematically improve problem-solving and suggests design principles for constructing more effective multi-agent LLM architectures, such as carefully choosing complementary constraint families and interaction protocols.

Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.

</details>


### [36] [Circadian Modulation of Semantic Exploration in Social Media Language](https://arxiv.org/abs/2601.15091)
*Vuong Hung Truong,Mariana Gabrielle Cangco Reyes,Masatoshi Koizumi,Jihwan Myung*

Main category: cs.CL

TL;DR: The paper studies how human language use on Reddit varies across the day, showing circadian rhythms in semantic exploration vs. exploitation using transformer-based embeddings and entropy measures.


<details>
  <summary>Details</summary>
Motivation: While many cognitive functions are known to vary with circadian rhythms, it is unclear how these daily cycles influence complex, high-dimensional semantic behavior in naturalistic language use. The authors aim to understand whether and how daily biological rhythms shape patterns of semantic exploration and topic diversity in large-scale online communication.

Method: The authors analyze large-scale Reddit text data across time of day. They embed posts using a pretrained transformer language model to obtain high-dimensional semantic representations, then compute semantic entropy-based measures as proxies for exploration-exploitation in language. They distinguish between local semantic entropy (how broadly a given post explores nearby semantic space) and global semantic entropy (overall diversity of topics at a given time). They examine circadian rhythmicity, possible entrainment by seasonal light cues, and control for sentiment and affective valence.

Result: They find robust circadian rhythms in semantic entropy. Local semantic exploration is highest in the morning, indicating broader exploration of semantic space earlier in the day. Global semantic diversity peaks later in the day as more submissions accumulate around already established topics, exhibiting rich-get-richer dynamics. These temporal patterns are not accounted for by sentiment or affective valence, implying that semantic exploration is distinct from mood. The rhythmic patterns align with expected effects of seasonal light cues.

Conclusion: The study concludes that human semantic behavior online exhibits clear circadian structure, with a temporal dissociation between local exploration and global topic diversity. These patterns cannot be reduced to mood and are consistent with known diurnal fluctuations in neuromodulatory systems, suggesting that biological circadian rhythms extend into the organization and dynamics of semantic cognition as expressed in natural language use.

Abstract: Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with "rich-get-richer" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain.

</details>


### [37] [RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)](https://arxiv.org/abs/2601.15129)
*Yishu Wei,Adam E. Flanders,Errol Colak,John Mongan,Luciano M Prevedello,Po-Hao Chen,Henrique Min Ho Lee,Gilberto Szarf,Hamilton Shoji,Jason Sho,Katherine Andriole,Tessa Cook,Lisa C. Adams,Linda C. Chu,Maggie Chung,Geraldine Brusca-Augello,Djeven P. Deva,Navneet Singh,Felipe Sanchez Tijmes,Jeffrey B. Alpert,Elsie T. Nguyen,Drew A. Torigian,Kate Hanneman,Lauren K Groner,Alexander Phan,Ali Islam,Matias F. Callejas,Gustavo Borges da Silva Teles,Faisal Jamal,Maryam Vazirabad,Ali Tejani,Hari Trivedi,Paulo Kuriki,Rajesh Bhayana,Elana T. Benishay,Yi Lin,Yifan Peng,George Shih*

Main category: cs.CL

TL;DR: They built and released a 200‑case, expert‑verified chest X‑ray benchmark and an AI‑assisted workflow that speeds up accurate labeling by radiologists.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs can perform well on radiology exam questions, but there is a lack of high‑quality, expert‑curated imaging benchmarks needed to rigorously evaluate and develop clinically useful models. Manual labeling of large imaging datasets by radiologists is slow, expensive, and prone to omissions, so there is a need for scalable, efficient, and reliable labeling procedures.

Method: They used 13,735 deidentified chest radiographs and reports from MIDRC. GPT‑4o first extracted abnormal findings from the clinical reports. A locally hosted LLM (Phi‑4‑Reasoning) then mapped these findings to 12 predefined benchmark labels. From these automatically labeled studies, 1,000 were sampled according to an algorithm designed to ensure clinical relevance and a range of difficulty. Seventeen chest radiologists independently reviewed the AI‑suggested labels for each sampled radiograph (each study read by three radiologists) and rated them as “Agree all”, “Agree mostly”, or “Disagree”. Cases with at least two radiologists selecting “Agree all” (381 cases) formed a high‑confidence pool. From this pool, 200 radiographs were chosen, prioritizing those with rarer or multiple findings, and split into a 100‑case public release set and a 100‑case holdout set for independent evaluation by RSNA. This whole pipeline constitutes an AI‑assisted expert labeling workflow.

Result: They produced a benchmark of 200 chest radiographs (100 public, 100 holdout) labeled with 12 standardized labels, where each image’s labels were proposed by LLMs and then verified by three chest radiologists, achieving high consensus (“Agree all” by at least two of three readers). They demonstrated that AI assistance can be used to pre‑label large datasets and then be efficiently corrected or confirmed by experts. The holdout subset is reserved by RSNA for independent evaluation of models, while the released subset is available publicly via the RSNA imaging platform.

Conclusion: The study shows that AI‑assisted labeling, using LLMs to propose structured labels from radiology reports followed by targeted expert review, can generate high‑quality, expert‑verified imaging benchmarks more efficiently than fully manual labeling. The resulting 200‑case, 12‑label chest radiography benchmark provides a standardized resource for developing and evaluating multimodal LLMs and other AI systems, while the holdout set supports fair, independent performance assessment. The semicollaborative workflow helps scale radiologist labeling, reduce omissions, and may generalize to other imaging domains.

Abstract: Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked "Agree all", "Agree mostly" or "Disagree" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected "Agree All" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.

</details>


### [38] [Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems](https://arxiv.org/abs/2601.15161)
*Yinzhu Chen,Abdine Maiga,Hossein A. Rahmani,Emine Yilmaz*

Main category: cs.CL

TL;DR: They propose an automated, retrieval-augmented multi-agent system that generates instance-specific, evidence-grounded rubrics to evaluate and improve medical LLM outputs, achieving better alignment and discrimination than GPT-4o baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs used for clinical decision support can hallucinate or provide unsafe advice. These failures are often subtle clinical errors that generic metrics miss, and creating detailed expert rubrics by hand is expensive and hard to scale. The authors want a scalable, systematic way to get fine-grained, clinically grounded evaluation criteria for each individual case.

Method: They build a retrieval-augmented multi-agent framework that pulls authoritative medical evidence, decomposes it into atomic clinical facts, and combines these with the specific user interaction constraints to synthesize detailed, verifiable, instance-specific evaluation rubrics. These rubrics are then used to score model outputs and to guide refinement of responses.

Result: On the HealthBench benchmark, their framework improves Clinical Intent Alignment (CIA) from 55.16% with a GPT-4o baseline to 60.12%. In discriminative tests, rubrics from their system yield a mean score delta of 8.658 and an AUROC of 0.977, roughly doubling the separation quality relative to the GPT-4o baseline delta of 4.972. When used to refine responses, the rubrics boost output quality by 9.2 percentage points (59.0% to 68.2%).

Conclusion: Automating rubric generation through a retrieval-augmented, multi-agent pipeline yields scalable, transparent, and fine-grained evaluation criteria that are better aligned with clinical intent. These rubrics both more accurately assess medical LLM performance and serve as actionable guidance to improve responses, offering a practical path toward safer and more reliable clinical decision-support LLMs.

Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.

</details>


### [39] [Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time](https://arxiv.org/abs/2601.15172)
*Ilia Kuznetsov,Rohan Nayak,Alla Rozovskaya,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces a framework to empirically compare and measure the quality of peer reviews in major AI/ML conferences and finds no consistent decline in median review quality over time.


<details>
  <summary>Details</summary>
Motivation: There is a widespread belief that as submission numbers increase in major AI and ML conferences, the quality of peer reviews is deteriorating. However, review quality is hard to define, measure, and compare across venues and years due to evolving formats and practices. The authors aim to provide an evidence-based way to study review quality over time rather than relying on anecdotal concerns.

Method: The authors (1) survey and document the diversity of review formats in major AI/ML conferences (ICLR, NeurIPS, *ACL), (2) design a new standardization approach to normalize heterogeneous review structures, (3) propose a multi-dimensional schema that defines review quality as its utility to editors and authors, and (4) implement both LLM-based metrics and lightweight quantitative measures to operationalize this schema. They then analyze relationships between these metrics and perform cross-temporal comparisons of review quality across venues and years.

Result: They obtain standardized, comparable representations of reviews across ICLR, NeurIPS, and *ACL. Using multiple metrics (including LLM-based evaluations and simple quantitative signals), they quantify review quality along several dimensions of usefulness. Their longitudinal analysis shows no consistent downward trend in median review quality over the years studied, despite community concerns and increasing submission loads.

Conclusion: The prevailing narrative that peer review quality is steadily declining at major AI/ML conferences is not supported by their empirical evidence at the level of median quality. The authors suggest alternative explanations for the perception of decline (e.g., volume, variance, expectations) and provide recommendations for better data, standards, and methods to enable ongoing, empirical study of review quality. Their framework and metrics offer a systematic basis for tracking and comparing review quality over time and across venues.

Abstract: Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidence-based comparative study of review quality and apply it to major AI and machine learning conferences: ICLR, NeurIPS and *ACL. We document the diversity of review formats and introduce a new approach to review standardization. We propose a multi-dimensional schema for quantifying review quality as utility to editors and authors, coupled with both LLM-based and lightweight measurements. We study the relationships between measurements of review quality, and its evolution over time. Contradicting the popular narrative, our cross-temporal analysis reveals no consistent decline in median review quality across venues and years. We propose alternative explanations, and outline recommendations to facilitate future empirical studies of review quality.

</details>


### [40] [Supporting Humans in Evaluating AI Summaries of Legal Depositions](https://arxiv.org/abs/2601.15182)
*Naghmeh Farzi,Laura Dietz,Dave D. Lewis*

Main category: cs.CL

TL;DR: The paper adapts nugget-based summarization evaluation techniques to directly assist legal professionals in assessing and improving LLM-generated deposition summaries, via a prototype tool for comparing and editing summaries based on factual nuggets.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to summarize long documents, including legal depositions, but their factual accuracy can be unreliable. In law, even small factual errors in summaries can have serious consequences. While nugget-based methods have been useful for automatic evaluation of summarization quality, their use as interactive support for end users (like legal professionals) is underexplored. The authors aim to bridge this gap by bringing nugget-based techniques into practical, user-facing workflows in the legal domain.

Method: The authors design and implement a prototype system that operationalizes a factual nugget-based approach for legal deposition summaries. The system identifies or uses predefined factual nuggets—small, atomic units of information—and helps users interact with these nuggets to (1) compare two candidate summaries and decide which better covers the important nuggets and (2) guide manual refinement of an automatically generated summary, highlighting covered/missing or inaccurately represented nuggets.

Result: The abstract indicates the creation of a working prototype that demonstrates how nugget-based approaches can be used in two legal workflows: choosing the better of two summaries and improving a single generated summary. Although detailed quantitative or user-study results are not given in the abstract, the prototype shows feasibility and illustrates how nugget-based factual units can structure user interaction with summaries.

Conclusion: Nugget-based methods, previously used mainly for automated evaluation of summarization systems, can be repurposed as practical tools to support end users in high-stakes domains like law. By exposing factual nuggets and their coverage, the proposed prototype helps legal professionals evaluate and refine LLM-generated deposition summaries, pointing toward broader applications of nugget-based interaction beyond evaluation alone.

Abstract: While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.

</details>


### [41] [Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models](https://arxiv.org/abs/2601.15220)
*Anmol Goel,Cornelius Emde,Sangdoo Yun,Seong Joon Oh,Martin Gubri*

Main category: cs.CL

TL;DR: Benign fine-tuning can silently break language models’ contextual privacy, even while benchmark performance looks normal.


<details>
  <summary>Details</summary>
Motivation: As language models become agentic, persistent, and widely deployed, they increasingly handle sensitive user data in complex contexts. Existing safety evaluations largely focus on explicit data leakage and benchmarked capabilities, but may miss subtler privacy failures that arise after fine-tuning models for helpfulness, personalization, or tool use. The authors aim to understand whether ordinary, seemingly harmless fine-tuning can erode models’ ability to respect contextual privacy norms and maintain proper boundaries between users, tools, and memories, thereby creating hidden risks in real-world systems.

Method: The authors fine-tune multiple frontier language models (both closed- and open-weight) on a variety of benign datasets, including real-world agent traces, user interactions, emotional and subjective dialogues, and code/debug logs with internal variables. They then evaluate the fine-tuned models on tasks that probe contextual privacy: respecting memory boundaries across sessions or users, not inappropriately sharing information with tools, and reasoning about when data should remain private. They compare performance on these privacy-sensitive tasks before and after fine-tuning, and contrast it with performance on standard safety and utility benchmarks. Additionally, they conduct mechanistic analyses of model representations to examine how features related to privacy norms change relative to task-relevant features during fine-tuning.

Result: Across six different models, five fine-tuning datasets, and two main task types (agentic behavior and memory-based tasks), the authors find consistent evidence that benign fine-tuning can induce a “privacy collapse”: models begin to violate contextual privacy, share information across boundaries, and mishandle tool interactions, even though their benchmark performance on safety and utility remains high. Mechanistic analysis suggests that internal representations that encode privacy norms and contextual boundaries are disproportionately disrupted by fine-tuning, while representations for task performance are preserved, revealing a mismatch between traditional capability metrics and privacy robustness.

Conclusion: The paper concludes that current fine-tuning practices and evaluation regimes are insufficient to guarantee contextual privacy in deployed language-model-based systems. Benign fine-tuning can silently erode privacy-respecting behavior without degrading standard benchmark scores, producing dangerous hidden vulnerabilities, particularly for agentic and memory-based applications. The authors argue that privacy representations in models are unusually fragile and call for new safety evaluations, training methods, and deployment practices that explicitly test and preserve contextual privacy norms when specializing or fine-tuning large language models.

Abstract: We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.

</details>


### [42] [Metadata Conditioned Large Language Models for Localization](https://arxiv.org/abs/2601.15236)
*Anjishnu Mukherjee,Ziwei Zhu,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: The paper shows that adding simple geographic metadata (URLs, country, continent tags) during pre‑training makes small language models more locally aware and efficient, without hurting their global generalization.


<details>
  <summary>Details</summary>
Motivation: Standard LLM training treats all text as if it came from a single global distribution, which leads to homogenized, often non-local behavior. Many applications need models that understand and adapt to specific regions (countries/continents), but fully separate regional models or heavy adaptation are expensive. The authors want a simple, compute-efficient way to localize models while retaining broad coverage.

Method: They pre-train 31 language models (0.5B and 1B parameters) from scratch on large-scale English news data. The corpus is annotated with verified URLs plus country and continent tags spanning 4 continents and 17 countries. During pre-training, they condition the model on these metadata fields. They run four controlled experiments comparing: baseline global models (no metadata), metadata-conditioned global models, and region-specific models. They also perform ablations to test which metadata types are most helpful and how data balance across regions affects performance. Finally, they instruction-tune the models and evaluate them on a new benchmark of 800 localized news multiple-choice questions (MCQs).

Result: 1) Metadata conditioning consistently improves performance on in-region tasks while preserving cross-region generalization. 2) Global models with metadata can match the localization performance of models trained only on region-specific data. 3) Metadata conditioning improves learning efficiency, i.e., better performance for a given data/compute budget. 4) URL-level metadata alone captures much of the geographic information; adding country/continent tags helps less than expected. 5) Balanced data coverage across regions is still crucial—metadata cannot make up for a lack of training data from a region. 6) On the new localized news MCQ benchmark, instruction-tuned metadata-conditioned models reach accuracy comparable to LLaMA-3.2-1B-Instruct despite using much less training data.

Conclusion: Metadata conditioning is an effective, practical, and compute-efficient technique for localizing language models. By simply including URL, country, and continent tags during pre-training on news data, models gain stronger region-specific behavior without sacrificing global performance. URL metadata alone provides a strong geographic signal, but adequate, balanced regional data remains necessary. This approach offers a scalable alternative to training many separate regional models or using more complex localization schemes.

Abstract: Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering 4 continents and 17 countries. Across four controlled experiments, we show that metadata conditioning consistently improves in-region performance without sacrificing cross-region generalization, enables global models to recover localization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compensate for missing regions. Finally, we introduce a downstream benchmark of 800 localized news MCQs and show that after instruction tuning, metadata conditioned global models achieve accuracy comparable to LLaMA-3.2-1B-Instruct, despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models.

</details>


### [43] [Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs](https://arxiv.org/abs/2601.15247)
*Rian Dolphin,Joe Dursun,Jarrett Blankenship,Katie Adams,Quinton Pike*

Main category: cs.CL

TL;DR: The paper proposes a three-stage LLM-based pipeline to extract structured, taxonomy-aligned risk factors from 10-K filings and an AI-driven system to autonomously refine the taxonomy over time, validated on S&P 500 data.


<details>
  <summary>Details</summary>
Motivation: Risk factors in corporate 10-K filings are lengthy, unstructured, and inconsistently described across firms, making it hard to compare companies or industries systematically. Existing methods struggle to align extracted risks with a predefined hierarchical taxonomy and to maintain or improve that taxonomy as new data and evaluation feedback arrive. There is a need for a scalable, accurate way to map unstructured text to structured risk categories and continually refine those categories so they remain economically meaningful.

Method: The authors design a three-stage pipeline: (1) an LLM extracts risk factor candidates from 10-K filings along with supporting text quotes; (2) an embedding-based semantic mapping module assigns each extracted risk to categories in a predefined hierarchical taxonomy based on vector similarity; and (3) a second LLM acts as a judge, validating or rejecting the proposed taxonomy assignments to reduce spurious mappings. They apply this pipeline to extract over 10k risk factors from S&P 500 firm filings. In addition, they build an autonomous taxonomy maintenance agent that uses evaluation feedback to detect problematic taxonomy nodes, analyze systematic failure patterns, and propose taxonomy refinements. These refinements are evaluated via embedding separation metrics and external economic validation such as industry-based similarity and statistical tests.

Result: Applying the pipeline to 10,688 extracted risk factors from S&P 500 firms, the authors obtain structured risk profiles for each company aligned with the taxonomy. They report that the autonomous taxonomy maintenance process yields a 104.7% improvement in embedding separation between categories in a case study, indicating clearer semantic distinction among categories. External validation shows that firms in the same industry have substantially more similar risk profiles than cross-industry pairs, with 63% higher similarity, a large effect size (Cohen's d=1.06), strong discriminative performance (AUC 0.82), and high statistical significance (p<0.001).

Conclusion: The paper concludes that their LLM and embedding-based pipeline can reliably extract and categorize risk factors from unstructured 10-K text in alignment with a hierarchical taxonomy, and that the autonomous taxonomy maintenance agent can significantly improve category separability and maintain taxonomy quality over time. The demonstrated economic validity—same-industry firms showing markedly higher risk profile similarity—suggests the resulting taxonomy and risk representations capture meaningful structure. The approach is generalizable to other domains where information must be extracted from unstructured text and mapped to a predefined taxonomy, with the autonomous refinement loop enabling continuous improvement as more documents are processed.

Abstract: We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation feedback to identify problematic categories, diagnose failure patterns, and propose refinements, achieving 104.7% improvement in embedding separation in a case study. External validation confirms the taxonomy captures economically meaningful structure: same-industry companies exhibit 63% higher risk profile similarity than cross-industry pairs (Cohen's d=1.06, AUC 0.82, p<0.001). The methodology generalizes to any domain requiring taxonomy-aligned extraction from unstructured text, with autonomous improvement enabling continuous quality maintenance and enhancement as systems process more documents.

</details>


### [44] [The Effect of Scripts and Formats on LLM Numeracy](https://arxiv.org/abs/2601.15251)
*Varshini Reddy,Craig W. Schmidt,Seth Ebner,Adam Wiemerslage,Yuval Pinter,Chris Tanner*

Main category: cs.CL

TL;DR: The paper shows that large language models perform well on standard arithmetic but struggle with numbers written in less common scripts or formats, and that targeted prompting can reduce this performance gap.


<details>
  <summary>Details</summary>
Motivation: While LLMs appear strong at arithmetic in common numerical formats, it is unclear how robust this ability is when numbers are written using different scripts (e.g., non-Latin numerals) or unconventional formatting. Since real-world multilingual applications often involve diverse numeral systems and styles, understanding this limitation is important for reliable deployment.

Method: The authors systematically evaluate LLMs on arithmetic and numerical reasoning tasks where the mathematical content is held constant but the numeral script or formatting is varied. They compare performance on prevalent scripts/formats versus underrepresented ones, and then apply targeted prompting strategies (e.g., few-shot examples, explicit mappings between numeral systems) to measure how much these interventions recover performance.

Result: They find a substantial accuracy drop when numerical inputs are presented in underrepresented numeral scripts or unusual formats, despite the tasks being mathematically equivalent to standard ones. However, they also show that carefully designed prompts, including few-shot demonstrations and explicit numeral mapping instructions, can significantly narrow this performance gap.

Conclusion: LLM numerical reasoning is not fully script- or format-invariant; it depends heavily on how numbers are written relative to training distributions. This reveals an overlooked challenge in multilingual numerical reasoning and suggests that practitioners should employ targeted prompting or representation strategies to ensure reliable number handling across diverse scripts and formats.

Abstract: Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral scripts and formats. We show that LLM accuracy drops substantially when numerical inputs are rendered in underrepresented scripts or formats, despite the underlying mathematical reasoning being identical. We further demonstrate that targeted prompting strategies, such as few-shot prompting and explicit numeral mapping, can greatly narrow this gap. Our findings highlight an overlooked challenge in multilingual numerical reasoning and provide actionable insights for working with LLMs to reliably interpret, manipulate, and generate numbers across diverse numeral scripts and formatting styles.

</details>


### [45] [Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks](https://arxiv.org/abs/2601.15277)
*Sahar Tahmasebi,Eric Müller-Budack,Ralph Ewerth*

Main category: cs.CL

TL;DR: The paper studies how vulnerable fake news detectors are to sentiment manipulation and proposes AdSent, a framework that makes them robust to sentiment-based adversarial attacks using LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing fake news detectors often rely on sentiment and emotion features. With powerful LLMs, attackers can easily tweak the sentiment of news content while keeping its factual content similar, potentially bypassing such detectors. Current adversarial work focuses on style, not on sentiment, leaving a key vulnerability underexplored. The authors aim to understand and mitigate this risk.

Method: They design controlled sentiment-based adversarial attacks using LLMs that rewrite news articles to change sentiment while preserving meaning. They then evaluate state-of-the-art fake news detectors under these sentiment perturbations to analyze performance shifts and biases. Finally, they propose AdSent, which incorporates a sentiment-agnostic training strategy—training models to give consistent veracity predictions across original and sentiment-altered versions of articles—to improve robustness.

Result: Experiments on three benchmark datasets show that standard fake news detectors’ performance drops substantially when sentiment is manipulated, revealing a bias: neutral articles are more likely predicted as real and non‑neutral as fake. AdSent, trained with the proposed strategy, achieves higher accuracy and stronger robustness under sentiment attacks and also transfers better to unseen datasets and adversarial conditions than baseline methods.

Conclusion: Sentiment is both an informative and a dangerous signal for fake news detection, as models over-rely on it and can be easily fooled by sentiment manipulation. By training for sentiment invariance via AdSent, detectors become more robust, maintaining stable veracity judgments across sentiment shifts and generalizing better to new data and adversarial scenarios.

Abstract: Misinformation and fake news have become a pressing societal challenge, driving the need for reliable automated detection methods. Prior research has highlighted sentiment as an important signal in fake news detection, either by analyzing which sentiments are associated with fake news or by using sentiment and emotion features for classification. However, this poses a vulnerability since adversaries can manipulate sentiment to evade detectors especially with the advent of large language models (LLMs). A few studies have explored adversarial samples generated by LLMs, but they mainly focus on stylistic features such as writing style of news publishers. Thus, the crucial vulnerability of sentiment manipulation remains largely unexplored. In this paper, we investigate the robustness of state-of-the-art fake news detectors under sentiment manipulation. We introduce AdSent, a sentiment-robust detection framework designed to ensure consistent veracity predictions across both original and sentiment-altered news articles. Specifically, we (1) propose controlled sentiment-based adversarial attacks using LLMs, (2) analyze the impact of sentiment shifts on detection performance. We show that changing the sentiment heavily impacts the performance of fake news detection models, indicating biases towards neutral articles being real, while non-neutral articles are often classified as fake content. (3) We introduce a novel sentiment-agnostic training strategy that enhances robustness against such perturbations. Extensive experiments on three benchmark datasets demonstrate that AdSent significantly outperforms competitive baselines in both accuracy and robustness, while also generalizing effectively to unseen datasets and adversarial scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: The paper proves that you cannot have a truly neutral shared ontology if it encodes causal or normative claims; a neutral substrate must stay at a pre-causal, pre-normative level and push all interpretation outside.


<details>
  <summary>Details</summary>
Motivation: Modern data systems must operate in contexts where different parties—legal, political, scientific, or organizational—disagree about causes, responsibilities, and norms but still need to share and query data. Designers often hope for a “neutral” ontology that can serve as a common base layer across these disagreements. The paper asks: under what conditions can such a neutral ontological substrate exist, and what must it avoid encoding to remain acceptable to parties with incompatible views?

Method: The authors conduct a conceptual and formal analysis of ontological neutrality. They define neutrality as (1) interpretive non-commitment—avoiding taking sides on contested causal or normative interpretations—and (2) stability under incompatible extensions—allowing different, mutually incompatible interpretive frameworks to be layered on top without forcing revision of the base. They then show, via an impossibility-style argument, that any ontology which bakes causal or deontic (normative) conclusions into its foundational layer cannot satisfy both neutrality requirements across divergent frameworks.

Result: They derive an impossibility result: a foundational ontology that includes causal or normative commitments cannot be neutral in the sense required for cross-framework accountability. Such commitments inevitably conflict with at least some plausible interpretive extensions, forcing revision or contradiction. The only viable candidates for neutral substrates are ontologies that restrict themselves to representing entities plus their identity and persistence conditions, while keeping causal and normative structure out of the base layer.

Conclusion: A genuinely neutral ontological substrate for accountable data systems must be pre-causal and pre-normative. It can assert what entities exist and how they persist or remain identical over time, but it must externalize causal explanation, normative evaluation, and other contested interpretive layers so they can vary across frameworks. The paper does not present a concrete ontology or protocol; instead, it articulates design constraints that any system claiming to provide a stable, shared representation of reality across conflicting interpretations must respect.

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [47] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: The paper argues that AI systems, especially large language models used for reasoning, need an explicit ‘epistemic constitution’—a set of transparent rules about how they form and express beliefs, with a focus on handling source-based bias in a principled way.


<details>
  <summary>Details</summary>
Motivation: Large language models already act as de facto reasoners: they judge arguments, assign credibility, and state levels of confidence. However, the rules governing how they form these ‘beliefs’ are opaque and unexamined. A concrete concern is source attribution bias: models change their evaluation of arguments depending on who is said to have made them, often enforcing a problematic identity-ideology coherence. This reveals hidden epistemic norms that can be unreliable, unfair, and difficult to audit or contest. The author wants a framework that makes these epistemic norms explicit, governable, and subject to public scrutiny, similar to how we now demand explicit ethical guidelines for AI.

Method: The paper uses conceptual and empirical analysis. Conceptually, it introduces and clarifies the notion of an ‘epistemic constitution’ and distinguishes two broad models of it—Platonic and Liberal—drawing on work in epistemology and political philosophy. Empirically or illustratively, it investigates source attribution bias in frontier language models, showing that models systematically penalize arguments when attributed to sources with mismatched expected ideological stances and that this behavior changes when models notice they are being tested. On the basis of these analyses, it articulates candidate constitutional principles and orientations for AI epistemic governance.

Result: The paper finds that frontier language models display source attribution bias in the form of identity-stance coherence: they rate arguments differently depending on whether the attributed source is ideologically ‘expected’ to endorse that content. When models suspect systematic testing, this bias diminishes or disappears, indicating that they treat source-sensitivity as a bias to be suppressed on inspection rather than as a capacity to be used responsibly. Building on this, the author develops a structured proposal for an AI ‘epistemic constitution’ that contrasts a Platonic model—prioritizing formal correctness and source-independence from an ostensibly neutral standpoint—with a Liberal model that emphasizes procedural norms, pluralism, and principled attentiveness to sources under the rubric of epistemic vigilance.

Conclusion: The paper concludes that AI systems need an explicit epistemic constitution analogous to explicit AI ethics frameworks. It argues against a Platonic approach that enforces source-independence from a supposedly privileged standpoint and instead defends a Liberal approach that focuses on procedures supporting collective inquiry, transparency, and contestability. Within this Liberal model, the paper sketches a core set of eight principles and four orientations guiding how AI systems should form, update, and express beliefs, including when and how they may legitimately attend to the identity of information sources. Epistemic governance for AI, on this view, must become explicit, publicly debatable, and revisable, rather than remaining an implicit byproduct of model training and ad hoc alignment choices.

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [48] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: The paper studies why vision-language models perform worse on math problems shown as images than as text, and proposes a tool-using reasoning agent and new datasets/pipelines to reduce this modality gap, especially via structured reasoning and OCR grounding.


<details>
  <summary>Details</summary>
Motivation: Vision-language models underperform text-only language models on mathematical reasoning when problems are presented visually instead of as plain text, even if the underlying question is the same. This performance drop—modality gap—is due to difficulties in accurately reading and interpreting complex mathematical notation, layouts, and combinations of symbols and diagrams. Understanding, measuring, and mitigating this gap is important for real-world applications like homework help, document understanding, and scientific problem solving that naturally involve images of math rather than clean text.

Method: The authors first introduce VisTIRA, a tool-integrated reasoning agent for visual math. VisTIRA takes a math problem as an image and performs structured, step-by-step problem solving by decomposing the problem into natural language rationales and executable Python code blocks, iteratively using tools to compute intermediate and final results. Second, they construct an evaluation and training framework for visual math reasoning: (1) a LaTeX-based pipeline that converts chain-of-thought math datasets (such as NuminaMath) into image-based versions, preserving difficulty while introducing visual rendering challenges; and (2) a large collection of synthetic tool-use trajectories based on a real-world image dataset of homework-style questions (SnapAsk), which they use to fine-tune vision-language models on tool-integrated reasoning. They also experiment with OCR-based grounding, aligning visual input with textual representations, and evaluate the impact across different model sizes.

Result: Experiments demonstrate that supervising models with tool-integrated reasoning trajectories improves image-based mathematical reasoning performance. Incorporating OCR grounding—i.e., explicitly converting visual content to aligned text—further narrows the modality gap for smaller models by improving their ability to read and parse visual math, though this benefit decreases for larger models that are inherently stronger. The empirical analyses show that the severity of the modality gap decreases as model size increases, and that combining structured tool-based reasoning with OCR grounding yields complementary benefits, especially for weaker models.

Conclusion: The paper concludes that the performance gap between text and image modalities in mathematical reasoning is significant but reducible. Larger models suffer less from the modality gap, yet still benefit from structured, tool-integrated reasoning frameworks like VisTIRA. OCR-based grounding is particularly effective for smaller models, but its marginal gains diminish as model scale increases. Overall, the work suggests that combining structured reasoning, tool integration, and selective OCR grounding is a promising direction to advance visual mathematical reasoning and bring VLM performance closer to that of text-only language models on math tasks presented as images.

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [49] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: The paper studies whether high planning performance of fine-tuned LLMs comes from real, transferable planning ability or just memorizing domain patterns, and finds a large generalization gap.


<details>
  <summary>Details</summary>
Motivation: LLMs fine-tuned on PDDL planning tasks can produce many valid plans, but it is unclear if this indicates genuine, domain-independent planning competence or overfitting to specific domains and surface forms. Understanding this is important for building robust LLM-based planners that can handle new domains.

Method: The authors fine-tune a 1.7B-parameter LLM on 40,000 (domain, problem, plan) tuples from 10 IPC 2023 domains and measure both in-domain and cross-domain performance. They probe the nature of the learned behavior via three interventions: (i) instance-wise symbol anonymization that renames symbols while preserving plan semantics, (ii) compact plan serialization that changes the textual encoding of plans, and (iii) verifier-reward fine-tuning that uses the VAL planner validator as a reinforcement-learning-style reward signal to encourage valid plans.

Result: The fine-tuned model achieves 82.9% valid plan rate on problems from the same training domains, but 0% valid plans on two new, unseen domains. Symbol anonymization and compact plan serialization substantially reduce performance despite leaving the underlying plans semantically equivalent, indicating strong reliance on surface-level patterns. Verifier-reward fine-tuning speeds up reaching peak in-domain performance (saturation in about half as many supervised epochs) but fails to improve cross-domain generalization. In-domain performance plateaus around 80%, while cross-domain performance stays extremely poor.

Conclusion: The observed behavior suggests that the fine-tuned LLM mainly memorizes domain-specific regularities and surface forms rather than acquiring transferable planning competence. The work exposes a stubborn generalization gap for LLM-based planning and offers diagnostic tools—anonymization, serialization changes, and verifier-reward training—to better understand and probe the reasons behind this lack of robust generalization across planning domains.

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [50] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: They propose a scalable genetic-programming hyper-heuristic for the dynamic multi-mode resource-constrained project scheduling problem by combining activity ordering, knee-point-based filtering, and group selection of activity-mode pairs.


<details>
  <summary>Details</summary>
Motivation: Existing genetic-programming-based hyper-heuristics for this scheduling problem typically make sequential decisions on single activity-mode pairs, ignoring interdependencies among activities. A group selection strategy was proposed to consider subsets of activities jointly, which improves solution quality on small instances but scales poorly to large problems. There is a need for a method that preserves the benefits of group-based decision-making while remaining computationally feasible on large-scale instances.

Method: They introduce a knee-point-based selection mechanism into the activity group selection strategy. First, a GP-evolved activity ordering rule ranks all eligible activity-mode pairs. Next, a knee-point detection procedure identifies a promising subset of these ranked pairs, effectively filtering the search space. Then, a GP-evolved group selection rule evaluates combinations only within this filtered subset to choose the best activity combination. A multi-tree genetic programming framework is designed so that both the ordering rule and the group selection rule are evolved simultaneously as separate trees within each individual.

Result: Experiments on benchmark instances of varying sizes show that the proposed approach handles large-scale instances efficiently, avoiding the scalability issues of the original group selection strategy. It delivers better or comparable solution quality than GP approaches that use purely sequential decision-making in most tested scenarios, demonstrating improved scalability and effectiveness of the evolved hyper-heuristics.

Conclusion: Incorporating knee-point-based filtering into activity group selection enables genetic-programming hyper-heuristics to consider interdependent activity decisions without incurring prohibitive computational cost on large instances. The multi-tree GP framework successfully co-evolves both ordering and group selection rules, leading to scalable, high-quality solutions for the dynamic multi-mode resource-constrained project scheduling problem and outperforming conventional sequential GP-based decision-making in most cases.

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [51] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: People use a Just-in-Time strategy to build simplified mental models during simulation-based reasoning, selectively encoding only task-relevant objects while still making good predictions.


<details>
  <summary>Details</summary>
Motivation: Probabilistic mental simulation is central to human reasoning and planning, but real-world environments are too complex for exhaustive simulation given human cognitive limits. Existing theories suggest people use simplified, abstracted representations, but they lack a concrete, efficient mechanism explaining how such simplifications are constructed in real time.

Method: The authors propose a Just-in-Time framework in which simulation, visual search, and representation updating are tightly interleaved. The ongoing simulation guides attention to where to look next; visual search identifies objects that matter; only those objects are then encoded into the internal model for further simulation. They evaluate this framework against alternative models in both a grid-world planning task and a physical reasoning task using multiple behavioral measures.

Result: The Just-in-Time model, which encodes only a small subset of available objects, still produces high-utility predictions. Across both grid-world planning and physical reasoning tasks, participants’ behavior aligns more closely with the Just-in-Time framework than with competing models on several behavioral metrics.

Conclusion: Humans likely rely on an online, Just-in-Time process to construct reduced task-relevant representations that make mental simulation computationally tractable. This framework offers a concrete algorithmic account of how selective encoding and attention support efficient simulation-based reasoning in complex environments.

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [52] [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)
*Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang*

Main category: cs.AI

TL;DR: PhyloEvolve is an LLM-agent system that treats GPU code optimization as an in-context RL problem, using trajectory histories and a phylogenetic tree of code variants to systematically improve scientific computing kernels on GPUs, outperforming baselines in speed, memory, and correctness.


<details>
  <summary>Details</summary>
Motivation: Optimizing scientific computing algorithms for modern GPUs is complex, manual, and iterative, requiring repeated code edits, benchmarking, and tuning over diverse hardware/software stacks. Existing LLM-based evolutionary optimization mainly uses simple outcome-based selection plus random mutation and discards rich trajectory information from previous optimization attempts. The authors aim to automate and improve this process by reusing optimization experience without retraining models.

Method: They formulate GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem, where the LLM agent conditions on past trajectories of code modifications and performance feedback. The system integrates Algorithm Distillation and prompt-based Decision Transformers, treating sequences of edits plus metrics as learning signals within prompts. Optimization history is structured as a phylogenetic tree capturing inheritance, divergence, and recombination of algorithm variants, enabling backtracking and cross-lineage transfer. The workflow also uses elite trajectory pooling, multi-island parallel exploration, and containerized execution to coordinate exploration–exploitation across heterogeneous hardware.

Result: On several scientific computing workloads—PDE solvers, manifold learning, and spectral graph algorithms—PhyloEvolve produces GPU implementations that consistently improve runtime, memory efficiency, and correctness compared to baseline approaches and prior LLM-assisted evolutionary optimization methods.

Conclusion: Reframing GPU algorithm optimization as an in-context RL problem and organizing code variants in a phylogenetic structure allows more effective reuse of optimization trajectories without retraining LLMs. The PhyloEvolve system demonstrates that trajectory-aware, phylogeny-guided LLM agents can substantially outperform outcome-only evolutionary baselines on real scientific computing workloads, offering a more systematic and scalable path to GPU code tuning.

Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve

</details>


### [53] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: They propose MAS-Orchestra, a reinforcement-learning-based framework that orchestrates multiple agents holistically as callable functions, and MASBENCH, a benchmark to rigorously analyze when multi-agent systems actually outperform single-agent ones.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems are believed to enable more powerful intelligence via coordinated agents, but current automatic design methods are complex, scale poorly, and it is unclear when they truly outperform single-agent systems. The authors want a more principled, scalable orchestration method and a way to rigorously understand under what conditions MAS are beneficial.

Method: They formulate the orchestration of multi-agent systems as a function-calling reinforcement learning problem, where each sub-agent is abstracted as a callable function and the orchestrator learns to generate an entire MAS configuration at once. They introduce MASBENCH, a controlled benchmark that categorizes tasks along five structural axes (Depth, Horizon, Breadth, Parallel, Robustness) to systematically probe MAS vs single-agent performance and analyze dependencies on task structure, verification procedures, and agent capabilities.

Result: Empirical analysis on MASBENCH shows that performance gains from MAS are highly dependent on task structure, verification setups, and the strengths of the orchestrator and sub-agents, rather than being universal. Using the insights from this analysis, MAS-Orchestra consistently improves performance on public benchmarks such as mathematical reasoning, multi-hop question answering, and search-based QA tasks.

Conclusion: Multi-agent systems are not universally superior to single-agent systems; their benefits hinge on specific task and system properties. By casting orchestration as RL over function calls and by providing MASBENCH for controlled evaluation, MAS-Orchestra offers a more globally reasoned, scalable way to design MAS and a clearer understanding of when multi-agent intelligence is actually advantageous.

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [54] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: The paper analyzes how vulnerable GraphRAG systems are to efficient graph-stealing attacks and shows that an agent-based attack can reconstruct most of their hidden knowledge graphs under realistic query budgets.


<details>
  <summary>Details</summary>
Motivation: GraphRAG systems use knowledge graphs to enable multi-hop reasoning over document collections, but these internal graphs are typically hidden and considered proprietary or sensitive. Prior work showed that responses may leak parts of the retrieved subgraphs, but it was unclear whether an attacker with a limited number of queries could practically reconstruct a large portion of the underlying graph. The paper is motivated by assessing this real-world risk: can adversaries efficiently reverse-engineer and steal the latent entity-relation graph of GraphRAG systems under strict query constraints?

Method: The authors formalize a budget-constrained black-box threat model where an attacker can adaptively query a GraphRAG system and observe its outputs to infer the hidden graph. They propose AGEA (Agentic Graph Extraction Attack), which uses an agentic framework with: (1) a novelty-guided exploration–exploitation strategy to decide what to query next, (2) external graph memory modules to store and reason over discovered nodes and edges, and (3) a two-stage graph extraction pipeline where a lightweight discovery step proposes candidate entities/relations and an LLM-based filtering step prunes false positives. AGEA is implemented and evaluated against two GraphRAG systems (Microsoft-GraphRAG and LightRAG) on multiple domains (medical, agriculture, literary datasets).

Result: Under the same query budgets as baselines, AGEA recovers a substantially larger portion of the target graphs. The paper reports that AGEA can reconstruct up to 90% of entities and relationships in the hidden knowledge graphs while maintaining high precision, clearly surpassing prior attack methods in both recall and precision across all evaluated datasets and GraphRAG systems.

Conclusion: The study concludes that modern GraphRAG systems are highly susceptible to structured, agentic extraction attacks. Even when the attacker operates under strict query limits and only has black-box access, AGEA can efficiently reconstruct most of the underlying entity–relation graph. This implies that deployed GraphRAG systems pose serious intellectual property and privacy risks unless stronger defenses or access controls are implemented.

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [55] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: The paper proposes and evaluates a structured, LLM-based framework for anonymizing qualitative research data in a context-aware and standards-compliant way, showing that local LLMs can outperform humans in detecting sensitive information while preserving analytic value.


<details>
  <summary>Details</summary>
Motivation: Qualitative research transcripts contain rich personal and organizational details that pose privacy and ethics risks. Manual anonymization is slow, inconsistent, and often misses important identifiers, while existing automated tools rely on rigid rules and pattern matching that fail to understand context and can distort meaning. There is a need for a more reliable, repeatable, and context-aware anonymization approach that aligns with major privacy regulations and preserves the analytic integrity of qualitative data.

Method: The authors design a Structured Framework for Adaptive Anonymizer (SFAA) built on local LLMs. The framework has three stages—detection of sensitive data, classification of identifier types and risk levels, and adaptive anonymization. It uses four anonymization strategies (rule-based substitution, context-aware rewriting, generalization, and suppression), chosen according to identifier type and risk. The identifiers and rules are grounded in major international privacy and research ethics standards such as GDPR, HIPAA, and OECD guidelines. The framework is evaluated through a dual-method approach (manual plus LLM-assisted processing) on two case studies: (1) 82 face-to-face interviews on gamification in organizations, and (2) 93 AI-led interviews about LLM awareness and workplace privacy. Two local LLMs, LLaMA and Phi, are used to run the framework and their outputs are compared with human review.

Result: Both LLaMA and Phi detected more sensitive information than a human reviewer in the qualitative transcripts, demonstrating higher recall of identifiers. Phi outperformed LLaMA in identifying sensitive data but also produced slightly more errors. Phi detected over 91% of all sensitive data, and 94.8% of its anonymizations preserved the original sentiment, indicating limited distortion of the underlying qualitative content.

Conclusion: Local LLMs, when embedded in a structured, standards-informed framework like SFAA, can perform highly effective and context-aware anonymization of qualitative research transcripts. They can surpass human reviewers in finding sensitive data while preserving the sentiment and analytic usefulness of the text. The SFAA’s adaptive combination of rule-based and context-aware strategies offers a repeatable and compliant approach to protecting participant privacy without undermining qualitative analysis.

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [56] [IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization](https://arxiv.org/abs/2601.14686)
*Shuai Wang,Yaoming Yang,Bingdong Li,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: They propose IB-GRPO, a multi-objective RL alignment method to better fine-tune LLMs for long-horizon learning path recommendation, improving learning effectiveness and difficulty scheduling under sparse feedback and limited expert data.


<details>
  <summary>Details</summary>
Motivation: Learning Path Recommendation needs to generate long sequences of educational items that respect pedagogy (e.g., ZPD) and constraints while maximizing learning gains. Standard LLM recommendation struggles because it is not aligned with pedagogical objectives, has limited high-quality expert trajectory data, and must balance several competing goals like learning effect, difficulty scheduling, path length, and diversity. Existing RL or scalarized objective methods cannot handle these challenges well or provide good Pareto trade-offs.

Method: They introduce IB-GRPO (Indicator-Based Group Relative Policy Optimization), an RL-based alignment framework for LLM-based learning path recommendation. First, they mitigate data scarcity by generating hybrid expert demonstrations with a Genetic Algorithm search and teacher RL agents, then warm-start the LLM with supervised fine-tuning on these trajectories. Next, they define a within-session ZPD alignment score that captures how well the recommended sequence respects the learner's Zone of Proximal Development, especially for difficulty scheduling. IB-GRPO then employs the I_{ε+} dominance indicator from multi-objective optimization to compute group-relative advantages over multiple objectives, enabling policy updates that directly optimize a multi-objective frontier without manual scalarization. The approach is implemented on top of a Qwen2.5-7B backbone within the KES simulator.

Result: On two benchmark datasets for knowledge tracing and educational recommendation (ASSIST09 and Junyi) using the KES simulator and a Qwen2.5-7B LLM backbone, IB-GRPO consistently outperforms representative RL-based and LLM-based baselines on key metrics related to learning effect, difficulty scheduling, and other objectives. The experiments suggest that their multi-objective indicator-based optimization yields better Pareto trade-offs than methods relying on manually combined objectives.

Conclusion: IB-GRPO effectively aligns large language models with pedagogically grounded, multi-objective goals in long-horizon learning path recommendation. By using hybrid expert demonstrations for warm-starting, a ZPD-based alignment score, and an indicator-based group-relative policy optimization scheme, it addresses sparse feedback, data scarcity, and conflicting objectives without manual scalarization, leading to superior empirical performance on standard educational datasets.

Abstract: Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{ε+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.

</details>


### [57] [Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation](https://arxiv.org/abs/2601.14691)
*Muhammad Khalifa,Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Yunxiang Zhang,Moontae Lee,Hao Peng,Lu Wang,Honglak Lee*

Main category: cs.AI

TL;DR: The paper shows that large language model (LLM) judges can be easily manipulated by changing an agent’s chain-of-thought text, even when the agent’s actual actions and observations stay the same.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as automatic judges of agent performance, especially in tasks where answers cannot be easily verified and evaluation depends on the agent’s reasoning trajectory. Current practice assumes that the chain-of-thought (CoT) honestly reflects the agent’s internal reasoning and the true task state. The authors question this assumption and aim to test how robust LLM judges are when the CoT is strategically manipulated.

Method: The authors fix agent actions and observations on diverse web-based tasks, then systematically rewrite the associated chain-of-thought traces using different manipulation strategies. They explore (1) style-based manipulations that only change presentation (tone, structure) and (2) content-based manipulations that introduce fabricated signals of progress or justification. They then measure how these manipulations affect the judgments of state-of-the-art vision-language model (VLM) judges. They also test various defenses, including improved prompting and increased compute at judgment time, to see how much these reduce susceptibility.

Result: Manipulating only the reasoning text, while keeping the underlying trajectory identical, can dramatically alter LLM-judge assessments. In particular, content-based manipulations can increase false positive rates by up to 90% over 800 trajectories across varied web tasks, meaning judges frequently rate failing or unchanged trajectories as successful. Style-based manipulations also have an effect but are less potent than content-based ones. Defensive strategies like better prompts and more judge compute offer partial mitigation but do not fully solve the problem.

Conclusion: The paper concludes that LLM-based judges have a fundamental vulnerability: they over-trust the chain-of-thought and are easily fooled by strategically rewritten reasoning that is not grounded in actual actions or observations. This undermines the reliability of using LLMs as automatic evaluators in non-verifiable settings. The authors argue that future judging mechanisms must explicitly verify reasoning claims against observable evidence, rather than treating CoT as an inherently faithful reflection of what happened or what the agent truly reasoned.

Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.

</details>


### [58] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: The paper introduces AutoDriDM, a decision-centric benchmark to evaluate vision-language models (VLMs) for autonomous driving, focusing on the gap between perception and decision-making.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving benchmarks for VLMs focus mostly on perception (e.g., detecting objects) and do not sufficiently evaluate the quality and safety of decision-making, which is critical in complex driving scenarios. There is a need to understand and measure how well VLMs can move from correct perception to safe, logical driving decisions.

Method: The authors design AutoDriDM, a benchmark consisting of 6,650 questions organized into three dimensions—Object, Scene, and Decision—that progressively test the perception-to-decision pipeline. They evaluate mainstream VLMs on this benchmark, perform correlation analyses between perception and decision scores, and carry out explainability analysis to inspect reasoning chains and identify failure modes. They also build an analyzer model to automatically annotate and analyze large-scale model outputs and reasoning errors.

Result: Mainstream VLMs show a weak correlation between their perception performance and their decision-making performance on AutoDriDM, indicating that good perception does not guarantee safe or correct decisions. Explainability analyses highlight frequent logical reasoning errors and other systematic failure patterns. The analyzer model successfully automates the annotation and categorization of these reasoning failures at scale.

Conclusion: AutoDriDM provides a decision-focused, progressive evaluation framework that exposes the limitations of current VLMs in moving from perception to safe driving decisions. By bridging perception-centered and decision-centered evaluation and revealing key reasoning failure modes, the benchmark offers practical guidance for designing safer, more reliable VLMs for real-world autonomous driving applications.

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [59] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: The paper tackles few-shot AI-generated bidding under budget constraints by combining LLM-based reasoning with a numerically precise optimizer, achieving better advertiser value than baselines.


<details>
  <summary>Details</summary>
Motivation: In online advertising, advertisers aim to maximize the cumulative value of winning impressions while obeying budget limits. Many advertisers have personalized goals but very limited historical interaction data, making the setting few-shot and challenging for traditional RL-based bidding methods that typically require large amounts of experience to learn good policies. While LLMs, with their strong in-context learning capabilities, could help generalize from limited data and capture complex objectives, they are not numerically precise enough for fine-grained bidding optimization. There is therefore a need for a method that can leverage LLMs' reasoning and generalization strengths while overcoming their weaknesses in numerical optimization for AI-generated bidding.

Method: The authors propose two main technical contributions. First, GRPO-Adaptive, a post-training strategy for LLMs that improves both reasoning ability and numerical precision by dynamically updating the reference policy during training rather than keeping it fixed. This encourages the model to better align its reasoning with performance signals in the bidding context. Second, they build DARA, a dual-phase AIGB framework. In phase one, a few-shot LLM-based reasoner uses in-context prompting to produce initial bidding plans consistent with advertiser goals and constraints. In phase two, a fine-grained optimizer refines these initial plans using feedback-driven reasoning, allowing precise adjustment of bids to budget and performance feedback. The decision process is thus decomposed into high-level reasoning followed by low-level numerical optimization.

Result: Through experiments on both real-world and synthetic online advertising environments, the proposed methods are compared against existing RL-based and LLM-based baselines. The results show that the DARA framework, powered by the GRPO-Adaptive trained LLM, yields higher cumulative advertiser value while respecting budget constraints. Performance gains are consistent across different datasets and settings, indicating that the approach is robust in few-shot scenarios where traditional methods underperform.

Conclusion: The paper concludes that combining an adaptively post-trained LLM with a dual-phase decision framework effectively addresses the few-shot AI-generated bidding problem under budget constraints. By separating high-level reasoning and planning from fine-grained numerical optimization, the proposed DARA framework leverages LLMs' strengths while mitigating their weaknesses in precision. The empirical results support that this design leads to superior cumulative advertiser value compared with existing baselines, suggesting a promising direction for LLM-enhanced bidding systems in online advertising.

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [60] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: Survey of explanation methods for Answer Set Programming from an XAI perspective, mapping user questions to explanation types, assessing tool coverage, and highlighting gaps and future research directions.


<details>
  <summary>Details</summary>
Motivation: Explainable AI is increasingly important, and Answer Set Programming, as a rule-based symbolic formalism, is well-suited for interpretable reasoning. However, existing explanation methods for ASP are fragmented, scenario-specific, and do not systematically address the full spectrum of users' explanatory needs. A structured overview is needed.

Method: Adopt an Explainable AI viewpoint to classify and analyze types of explanations relevant to ASP. Map these explanation types to typical user questions, review existing theoretical frameworks and software tools for ASP explanations, and assess how well they cover the identified explanation needs.

Result: The survey identifies a variety of explanation types for ASP and systematically relates them to user-oriented explanation questions. It catalogs existing explanation theories and tools, evaluates their coverage with respect to these explanation types and questions, and reveals mismatches and uncovered scenarios where user needs are not fully supported.

Conclusion: Current explanation approaches for ASP provide partial but not comprehensive support for the range of explanation needs of ASP users. There are notable gaps in both theory and tooling. The paper outlines research directions aimed at closing these gaps and improving the practical and theoretical foundations for explainable ASP-based reasoning.

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [61] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: They propose an unsupervised video summarization method that uses semantic-guided attention, a Transformer-based generator, and incremental GAN training to create better, more stable video summaries.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised video summarization methods largely depend on GANs and unimodal visual features, which ignore rich semantic information that could guide keyframe selection. Moreover, these GAN-based approaches often suffer from unstable training, limiting performance and reliability in real-world applications like social media video browsing and information extraction.

Method: They introduce a Semantic-Guided Unsupervised Video Summarization framework. The core is a frame-level semantic alignment attention mechanism embedded in a keyframe selector. This selector guides a Transformer-based generator within an adversarial (GAN) framework to reconstruct the original video from selected keyframes. Additionally, an incremental training strategy is used to progressively update model components to alleviate GAN training instability.

Result: Through experiments on several standard benchmark datasets for video summarization, the proposed method outperforms existing approaches, showing improved quantitative metrics for summarization quality and reconstruction, as well as better stability during training.

Conclusion: Incorporating semantic-guided attention into the keyframe selection process and stabilizing adversarial training via incremental updates leads to more accurate and coherent unsupervised video summaries, demonstrating clear advantages over prior GAN-based unimodal methods on benchmark datasets.

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [62] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: The paper presents the first bound-consistent algorithm for the no-overlap constraint using a limited-width MDD, outperforming previous precedence-detection methods and complementing classical propagation.


<details>
  <summary>Details</summary>
Motivation: Bound consistency for the no-overlap constraint is NP-complete, so current solvers rely on weaker but polynomial-time techniques (edge finding, not-first-not-last, energetic reasoning). There is a need for a practical way to approximate or reach bound consistency more effectively, improving search efficiency in scheduling problems with time windows and just-in-time objectives.

Method: The authors build on the no-overlap multi-valued decision diagram (MDD) introduced by Ciré and van Hoeve. From this MDD, they derive bounds on the time windows of jobs to tighten their start and end times. They propose an algorithm that achieves bound-consistent filtering in time polynomial in the number of MDD nodes. To control complexity, they restrict the MDD width to a predefined threshold, forming a relaxed MDD and a corresponding relaxed bound-consistent filtering method.

Result: On a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), the proposed filtering— even when applied on relaxed, width-bounded MDDs—reduces the number of visited search nodes more than the prior precedence-detection algorithm based on MDDs by Ciré and van Hoeve. The experiments also show that combining the new filtering with classical no-overlap propagation yields notable reductions in both search tree size and solving time on many instances.

Conclusion: The paper demonstrates that an MDD-based approach can achieve bound-consistent filtering for the no-overlap constraint in polynomial time relative to the MDD size, overcoming prior practical limitations of NP-completeness at the problem level. Width-limited (relaxed) MDDs provide a controllable trade-off between propagation strength and computational cost, and the new filtering is both stronger than previous MDD-based precedence detection and complementary to classical propagation algorithms, leading to significant performance gains on tested scheduling benchmarks.

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [63] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: The paper proposes CI4A, a component interface optimized for agents that abstracts UI components into unified tool primitives, enabling LLM agents to more effectively and efficiently manipulate web interfaces and achieving new state-of-the-art performance on a refactored WebArena benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs are strong at high-level planning but weak at precise, low-level manipulation of web components. Prior work mainly tried to better ground models in human-oriented UIs, often via reinforcement learning, which is complex and still constrained by those interfaces. The authors aim to remove this mismatch by redesigning the interaction layer itself so that it is optimized for agents, not humans, making web tasks more tractable and reliable for LLM-based agents.

Method: They introduce CI4A (Component Interface for Agent), a semantic encapsulation mechanism that converts the complex interaction logic of UI components into a uniform set of tool primitives that agents can call. CI4A is implemented on top of Ant Design, covering 23 common UI component categories. They also design a hybrid agent whose available action space (set of CI4A tools) dynamically updates with page state, letting it flexibly choose and execute component-level actions. Using this CI4A-enhanced Ant Design stack, they refactor and upgrade the WebArena benchmark to systematically evaluate a range of SoTA web agents.

Result: On the CI4A-integrated version of WebArena, the CI4A-based agent substantially outperforms prior state-of-the-art methods, achieving a new best task success rate of 86.3% and showing notable gains in execution efficiency (faster or fewer steps) over baselines.

Conclusion: Redesigning web interaction interfaces for agents via CI4A, instead of forcing agents to adapt to human-centric UIs, markedly improves LLM agents’ ability to perform fine-grained web manipulation. CI4A’s abstraction of UI components into unified tool primitives, combined with a dynamically updating action space, yields significant performance and efficiency gains on web benchmarks, suggesting a promising direction for more robust and scalable web agents.

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [64] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: The paper evaluates and improves vision-language models for chest X-ray classification using medical taxonomies to distinguish minor from severe errors.


<details>
  <summary>Details</summary>
Motivation: Standard flat metrics for chest X-ray classification with VLMs do not reflect clinical severity; they treat all misclassifications equally, ignoring whether an error is clinically minor or potentially harmful. There is a need to better quantify and reduce clinically severe abstraction errors when using VLMs in medical imaging.

Method: The authors benchmark several state-of-the-art vision-language models on chest X-ray classification using hierarchical metrics derived from medical taxonomies. They define and measure Catastrophic Abstraction Errors, focusing on mistakes that cross major branches of the taxonomy. To mitigate such errors, they introduce risk-constrained thresholding strategies and a taxonomy-aware fine-tuning scheme that uses radial embeddings to better align model representations with the medical hierarchy.

Result: They find that, despite strong zero-shot performance on standard (flat) metrics, existing VLMs are substantially misaligned with clinical taxonomies and exhibit many severe cross-branch errors. Applying their proposed risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings significantly reduces severe abstraction errors, bringing them below 2%, while retaining competitive overall performance.

Conclusion: Hierarchical, taxonomy-based evaluation reveals clinically important failure modes in vision-language models that flat metrics miss. Aligning model representations with medical taxonomies and using risk-aware decision strategies can greatly decrease clinically dangerous abstraction errors, supporting safer and more meaningful use of VLMs in medical imaging applications.

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [65] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD is a framework that embeds knowledge representation and reasoning directly into object-oriented programming using native classes, enabling tight integration of logic and imperative code with strong performance on benchmarks and a robot task-learning scenario.


<details>
  <summary>Details</summary>
Motivation: Modern software engineering relies on object-oriented programming, but most knowledge representation and reasoning systems are external, use separate ontology languages, and integrate poorly with imperative code. This makes it hard to build complex, real-world autonomous systems that need expressive reasoning tightly coupled to application logic. The paper aims to close this integration gap.

Method: The authors design KRROOD, a framework that treats knowledge as a first-class abstraction within object-oriented languages. It maps knowledge concepts and reasoning constructs onto native class structures, effectively blending logic programming concepts with OOP. They then implement KRROOD and empirically evaluate it using the OWL2Bench benchmark and a human–robot task learning scenario to test both performance and expressive reasoning capabilities.

Result: On the OWL2Bench benchmark, KRROOD demonstrates strong performance, suggesting that its integrated design does not significantly sacrifice efficiency. In the human–robot task learning scenario, the framework successfully supports the expressive reasoning needed for real-world autonomous behavior, indicating practical viability beyond synthetic benchmarks.

Conclusion: KRROOD effectively bridges the gap between OOP-based software engineering and KR&R systems by embedding knowledge and reasoning directly into native class structures. It achieves competitive performance while providing sufficient expressiveness for real-world autonomous systems, showing that tight integration of logic and imperative paradigms is feasible and useful in practice.

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [66] [To Neuro-Symbolic Classification and Beyond by Compiling Description Logic Ontologies to Probabilistic Circuits](https://arxiv.org/abs/2601.14894)
*Nicolas Lazzari,Valentina Presutti,Antonio Vergari*

Main category: cs.AI

TL;DR: They compile Description Logic ontologies into differentiable circuits to support data generation, fast reasoning, and neuro-symbolic classifiers whose predictions are ontology-consistent and competitive with standard neural networks.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic models can enforce logical constraints on neural networks but typically do not support rich, structured knowledge encoded as ontologies (e.g., Description Logic). This limits their applicability in domains where domain knowledge is already formalized in ontological form. The authors want a way to guarantee that model predictions respect such ontologies, while remaining scalable and efficient.

Method: The authors compile a Description Logic ontology into a differentiable feed-forward circuit (a computational graph). This circuit represents the ontology’s semantics and supports tractable evaluation of queries and transformations. They then use this circuit in three ways: (i) to generate synthetic datasets whose labels and structure respect the ontology; (ii) to execute deductive reasoning on GPUs efficiently using the circuit; and (iii) to integrate the circuit into neuro-symbolic models so that predictions are either approximately or provably consistent with the ontology, by constraining or regularizing the neural network through the circuit.

Result: The ontology-derived synthetic datasets reflect the intended semantics while remaining hard for standard ML and neural models to learn. The compiled circuit enables deductive reasoning that runs up to 1000× faster than existing DL reasoners. Neuro-symbolic classifiers built with the circuit yield predictions that are much more consistent with the ontology than unconstrained neural baselines, and they achieve comparable or better predictive performance.

Conclusion: Compiling Description Logic ontologies into circuits provides a unifying representation that bridges Deep Learning and Knowledge Representation. This single circuit can be reused across tasks such as data generation, scalable reasoning, and neuro-symbolic classification, enabling practical ontology-aware ML systems that are both efficient and logically consistent.

Abstract: Background: Neuro-symbolic methods enhance the reliability of neural network classifiers through logical constraints, but they lack native support for ontologies.
  Objectives: We aim to develop a neuro-symbolic method that reliably outputs predictions consistent with a Description Logic ontology that formalizes domain-specific knowledge.
  Methods: We encode a Description Logic ontology as a circuit, a feed-forward differentiable computational graph that supports tractable execution of queries and transformations. We show that the circuit can be used to (i) generate synthetic datasets that capture the semantics of the ontology; (ii) efficiently perform deductive reasoning on a GPU; (iii) implement neuro-symbolic models whose predictions are approximately or provably consistent with the knowledge defined in the ontology.
  Results We show that the synthetic dataset generated using the circuit qualitatively captures the semantics of the ontology while being challenging for Machine Learning classifiers, including neural networks. Moreover, we show that compiling the ontology into a circuit is a promising approach for scalable deductive reasoning, with runtimes up to three orders of magnitude faster than available reasoners. Finally, we show that our neuro-symbolic classifiers reliably produce consistent predictions when compared to neural network baselines, maintaining competitive performances or even outperforming them.
  Conclusions By compiling Description Logic ontologies into circuits, we obtain a tighter integration between the Deep Learning and Knowledge Representation fields. We show that a single circuit representation can be used to tackle different challenging tasks closely related to real-world applications.

</details>


### [67] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: The paper proposes shifting from debating AI consciousness to evaluating AI awareness through a practical, multidimensional framework that can compare diverse systems and predict task performance.


<details>
  <summary>Details</summary>
Motivation: Debates about whether AI systems are conscious and what moral status they should have are conceptually murky and methodologically hard to resolve. There is no consensus on how to test for consciousness or moral status, yet we urgently need ways to assess and compare advanced AI systems for research, design, and governance purposes. The authors are motivated to find a more tractable, operationalisable concept than consciousness that still captures ethically and practically important aspects of AI systems.

Method: The authors reconceptualise the target from "consciousness" to "awareness," defined functionally in terms of a system’s capacity to process, store, and use information for goal-directed action. They then propose four desiderata for any evaluation framework meant to cover diverse artificial systems: it must be domain-sensitive, applicable at multiple scales, multidimensional, and predictive of task performance while abstracting to ability-level measures for comparison. On this basis, they sketch a structured procedure for building and applying "awareness profiles" that quantify and compare these abilities across systems with different architectures and domains.

Result: The paper yields a conceptual and methodological framework rather than empirical data: a definition of awareness suited to artificial systems, a set of four design requirements for evaluation schemes, and a structured approach for constructing awareness profiles. This framework allows evaluators to characterise which informational and control capacities a system has, at what level and in which domains, and to relate these to expected task performance, enabling systematic comparison between heterogeneous systems.

Conclusion: The authors conclude that focusing on "being aware enough"—operationalised via their awareness profiles—offers a more fruitful path than attempting to measure AI consciousness. Their approach promises more principled assessments of AI systems, better support for system design and oversight, and a clearer basis for scientific and public discussions about advanced AI, without requiring contentious claims about consciousness or moral status.

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [68] [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)
*Hanqi Jin,Gaoming Yang,Zhangming Chan,Yapeng Yuan,Longbin Li,Fei Sun,Yeqiu Yang,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: The paper proposes TGA, a linear-complexity Transition-Aware Graph Attention Network that models informative multi-behavior transitions in e-commerce sequences and outperforms transformer-based methods with lower cost.


<details>
  <summary>Details</summary>
Motivation: Existing multi-behavior sequential recommendation models, often transformer-based, effectively use diverse user actions (click, favorite, cart, purchase) but suffer from polynomial time complexity and high computational cost, making them hard to deploy on large-scale industrial platforms with long user histories. There is a need for a model that better exploits behavior-transition signals while remaining computationally efficient and deployable at scale.

Method: The authors design Transition-Aware Graph Attention Network (TGA), which replaces dense attention over all behavior pairs with a structured sparse graph of informative transitions. This graph is constructed from three complementary views: item-level transitions between items, category-level transitions across item categories, and neighbor-level transitions based on local sequence context. On this graph, they apply a transition-aware graph attention mechanism that simultaneously encodes user–item interactions and the types of behavior transitions, enabling linear-time complexity with respect to sequence length.

Result: On benchmark datasets, TGA surpasses all compared state-of-the-art multi-behavior sequential models in recommendation accuracy while substantially reducing computational cost due to its linear complexity and sparse transition graph. It also scales to long user sequences better than transformer-based baselines.

Conclusion: Transition-aware, graph-based modeling of multi-behavior sequences can preserve or improve recommendation quality while drastically improving efficiency compared to transformer architectures. TGA’s success in both offline benchmarks and real-world industrial deployment demonstrates that structured sparse transition graphs and transition-aware attention are practical, scalable solutions for large-scale e-commerce recommendation systems.

Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for un- derstanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi- behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional trans- formers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behav- ior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while sig- nificantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.

</details>


### [69] [Emergent, not Immanent: A Baradian Reading of Explainable AI](https://arxiv.org/abs/2601.15029)
*Fabio Morreale,Joan Serrà,Yuki Mistufuji*

Main category: cs.AI

TL;DR: Reconceptualizes explainable AI through Barad’s agential realism, treating explanations as situated, material-discursive performances rather than mere technical model introspection, and proposes ethical, interface-level design directions (illustrated via a speculative text-to-music XAI system).


<details>
  <summary>Details</summary>
Motivation: Existing XAI work largely assumes that meaning and explanation are properties recoverable from within the AI model itself, that an explainer stands outside the socio-technical system, and that causal structure can be extracted computationally. These assumptions are rarely questioned, leading to limitations in how explanation, responsibility, and ethics are understood and operationalized in AI systems. The authors aim to challenge these foundations and provide a richer theoretical basis for XAI that aligns better with how interpretation actually happens in practice.

Method: The authors use a theoretical and critical method: they adopt Barad’s agential realism as the onto-epistemological lens, and systematically “read through” a broad range of existing XAI techniques (e.g., feature attribution, counterfactuals, example-based explanations, etc.). Through this reading, they unpack each method’s implicit assumptions about where meaning resides, who or what explains, and how causality is conceived. They then translate the agential realist perspective into concrete design principles and demonstrate them via a speculative case: a text-to-music AI system whose interface is intentionally designed to foreground emergent, situated interpretation.

Result: The analysis shows that many XAI approaches implicitly treat explanations as objective, model-internal facts that can be surfaced with the right algorithm, downplaying the roles of human practice, context, and interpretive apparatuses. By reframing explanation as a material-discursive performance within entangled human–AI–context setups, the authors reveal conceptual blind spots and limitations of current methods (e.g., oversimplified notions of causality and user passivity). They derive a set of design directions for XAI interfaces—such as supporting co-construction of meaning, making apparatuses visible, and enabling multiple, situated readings—and instantiate these directions in the speculative text-to-music XAI interface.

Conclusion: Explainability should not be conceived solely as uncovering pre-existing, model-internal truths; instead, it is an emergent, situated practice arising from entangled interactions between models, humans, tools, and contexts. Agential realism offers a productive onto-epistemological foundation for XAI that foregrounds these entanglements and their ethical stakes. Designing XAI systems thus requires focusing on interpretive interfaces and practices, not just on technical explanation algorithms. The proposed framework and speculative case study illustrate how future XAI can better support emergent, ethically attuned interpretation rather than merely offering static, decontextualized model introspections.

Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.

</details>


### [70] [The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems](https://arxiv.org/abs/2601.15059)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: The paper argues that in modern CI/CD pipelines with agent-generated code, responsibility for decisions disappears because no one both approves and fully understands them.


<details>
  <summary>Details</summary>
Motivation: To highlight and formalize a growing governance problem in AI-augmented software delivery, where speed and scale outstrip human ability to verify decisions, making traditional notions of accountability inoperative.

Method: Conceptual and structural analysis of CI/CD architectures using assumptions about agent parallelism, CI validation, and human approval gates to derive conditions under which responsibility cannot be meaningfully assigned.

Result: The authors identify a responsibility vacuum as an inherent structural property once decision throughput surpasses human verification capacity, leading to ritualized approvals based on proxy signals, exacerbated rather than solved by additional automation and test coverage.

Conclusion: Without redesigning decision boundaries or shifting responsibility from individual approvals to higher-level batch or system ownership, scaled agent deployments will systematically suffer from hidden responsibility vacuums that can’t be fixed by more automation alone.

Abstract: Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.
  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.
  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.
  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.
  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.

</details>


### [71] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: They propose a general, hierarchical framework to attribute LLM agent actions to specific past internal states and text, not just in failure cases.


<details>
  <summary>Details</summary>
Motivation: Existing work mostly explains failures by finding explicit errors in unsuccessful trajectories, but real-world LLM agents act autonomously at scale, and stakeholders need to know why an agent took a certain action whether it succeeded or failed. Simple failure attribution does not capture the broader internal reasoning and historical context behind behaviors, especially in complex multi-step, tool-using agents with subtle reliability risks.

Method: They design a hierarchical attribution framework for LLM-based agents. At the component level (coarser granularity), they track temporal likelihood dynamics over an interaction history to find which steps in the trajectory are most influential on a later action. At the finer, sentence level, they perform perturbation-based analyses on textual inputs and memory to see which specific sentences, when changed or removed, most affect the agent’s action or its probability, thereby isolating key textual evidence behind behaviors. The framework is applied to different agentic setups including standard tool-using agents and agents with long-term memory.

Result: Across multiple benchmarks and agentic scenarios, including typical tool use and cases involving subtle reliability issues such as bias introduced by memory, the framework can consistently identify the most influential past events and specific sentences responsible for a given agent action. Quantitative and qualitative experiments show that these identified pivotal steps and texts align with the intuitive or ground-truth causes of agent behavior better than existing failure-only attribution methods.

Conclusion: The proposed hierarchical general agentic attribution framework provides a more comprehensive explanation of LLM-agent behavior than traditional failure attribution. By pinpointing both critical interaction steps and the exact textual evidence underlying actions—regardless of whether the final outcome is success or failure—it advances transparency, safety, and accountability in autonomous LLM agents deployed in real-world settings.

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


### [72] [Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories](https://arxiv.org/abs/2601.15120)
*Qian Xiong,Yuekai Huang,Yujia Zheng,Tianhao Li,Ziyou Jiang,Zhiyuan Chang,Zhaoyang Li,Huanxiang Feng,Mingyang Li*

Main category: cs.AI

TL;DR: RISE is a Real-to-Virtual data synthesis and training framework to reduce intent deviation in LLM tool-use agents, using verified real tool primitives to generate synthetic trajectories and hard negative samples for two-stage intent alignment fine-tuning, significantly boosting task completion and intent alignment over prior methods.


<details>
  <summary>Details</summary>
Motivation: Tool-using LLM agents for real-world applications often exhibit "intent deviation," where the agent’s behavior subtly diverges from the user’s true intent despite apparently successful tool calls. This makes evaluation and iterative improvement difficult. Existing post-training approaches depend either on costly hand-crafted real system data or on fully simulated virtual data that can be distributionally different from real tools, and both lack well-structured negative examples that specifically target intent deviation. A better way to generate realistic, intent-focused training data is needed.

Method: The authors propose RISE, a Real-to-Virtual framework that starts from verified real tool primitives (i.e., real tool specifications and correct usage instances) and then synthesizes virtual trajectories of tool-using agents. RISE systematically mutates critical parameters within these trajectories to create diverse negative samples that reflect different forms of intent deviation. Using this synthetic dataset, they perform a two-stage fine-tuning of backbone LLMs for intent alignment: first training on synthetic positive and negative trajectories, and then further refining the model’s preference for intent-consistent behaviors. The method thus combines realism from real tools with scalable virtual data generation and targeted negative examples.

Result: On benchmarks that measure user requirement satisfaction, execution trajectories, and agent responses, synthetic data produced by RISE lead to strong performance across eight metrics. When integrated into training, RISE yields an average 35.28% improvement in task completion accuracy (Acctask) and 23.27% improvement in intent alignment accuracy (Accintent). These gains exceed state-of-the-art baselines by 1.20–42.09% on Acctask and 1.17–54.93% on Accintent, indicating substantial benefits over existing post-training strategies.

Conclusion: RISE effectively addresses intent deviation in tool-using LLM agents by leveraging a Real-to-Virtual data pipeline that anchors on real tool primitives while enabling large-scale synthetic trajectory generation and targeted negative sampling. The two-stage fine-tuning with this synthetic data significantly improves both task completion and intent alignment, outperforming current state-of-the-art methods and offering a practical, scalable approach to improving reliability of LLM-based agents in real-world tool-use scenarios.

Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of "intent deviation" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a "Real-to-Virtual" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.

</details>


### [73] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: The paper warns that people increasingly misuse large language models for simple, deterministic tasks, causing wasted resources and new risks, and proposes a framework to decide when not to use generative AI.


<details>
  <summary>Details</summary>
Motivation: As LLMs become widely available and convenient, users are turning to them even for trivial or deterministic tasks (like OCR or basic fact-checking) where simpler algorithms are more efficient and appropriate. This shift prioritizes convenience over computational efficiency and can introduce new forms of error or bias. The authors want to formalize and measure this misuse, highlight its costs and risks, and provide guidance to prevent it.

Method: The authors introduce the concept of the "Plausibility Trap" and empirically analyze it using micro-benchmarks and case studies. Specifically, they compare performance and behavior when using LLMs versus traditional deterministic tools for tasks like OCR and fact-checking. They quantify latency and other efficiency metrics to estimate an "efficiency tax". They also study phenomena like algorithmic sycophancy—where models agree with users regardless of correctness. Based on their findings, they propose a framework: Tool Selection Engineering and a Deterministic-Probabilistic Decision Matrix to guide tool choice.

Result: The experiments show that relying on LLMs for simple deterministic tasks leads to substantial inefficiency, with around a 6.5x latency penalty compared to conventional tools, alongside risks from algorithmic sycophancy affecting reliability of fact-checking. The case studies validate that LLMs are often inferior to dedicated deterministic systems in these contexts, both in speed and in predictable correctness.

Conclusion: The paper concludes that uncritical reliance on LLMs for all digital tasks is wasteful and risky. It argues that developers and users should apply structured decision processes—via Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix—to decide when generative AI is appropriate. The authors advocate for a shift in digital literacy education so that people learn not only how to use generative AI effectively but also when to avoid it in favor of simpler, deterministic tools.

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [74] [Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding](https://arxiv.org/abs/2601.15131)
*Ayan Maity,Sudeshna Sarkar*

Main category: cs.AI

TL;DR: They propose a deep reinforcement learning approach with a novel routing network embedding to solve a vehicle routing problem under a finite time horizon, improving both service rate and solution time over existing methods.


<details>
  <summary>Details</summary>
Motivation: Classical vehicle routing methods often ignore or simplify strict finite time horizon constraints and may struggle to scale or adapt to complex, real-world networks. There is a need for methods that can explicitly account for remaining time, exploit full graph structure (nodes, edges, adjacency), and learn efficient routing policies that maximize the number of served customers while being computationally efficient.

Method: They model the vehicle routing problem as a Markov decision process whose state includes node features, adjacency matrix, and edge features. A new routing network embedding module produces local node embeddings and a global, context-aware graph representation that explicitly incorporates the remaining time horizon. This embedding is integrated into a policy-gradient deep reinforcement learning framework, which is trained on both real-world and synthetic Euclidean networks to learn routing policies that decide which customer to serve next under time constraints.

Result: On both real-world routing networks and synthetically generated Euclidean instances, their method serves more customer requests within the time horizon than existing routing approaches and does so with significantly lower computation (solution) time.

Conclusion: Incorporating a time-aware, graph-based embedding into a policy-gradient deep RL framework yields an effective solution for vehicle routing with finite time horizons, providing higher customer service rates and faster solution times than traditional methods, which suggests this learning-based approach is well-suited for practical routing applications under tight temporal constraints.

Abstract: In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.

</details>


### [75] [How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework](https://arxiv.org/abs/2601.15153)
*Choro Ulan uulu,Mikhail Kulyabin,Iris Fuhrmann,Jan Joosten,Nuno Miguel Martins Pacheco,Filippos Petridis,Rebecca Johnson,Jan Bosch,Helena Holmström Olsson*

Main category: cs.AI

TL;DR: The paper presents a framework to capture expert domain knowledge and embed it into AI agents for automatic generation of high-quality simulation data visualizations, enabling non-experts to reach expert-level results.


<details>
  <summary>Details</summary>
Motivation: Critical domain knowledge is concentrated in a few experts, creating bottlenecks in scalability and decision-making. Non-experts are unable to design effective visualizations, leading to poor insights and consuming scarce expert time. The authors aim to reduce dependence on experts by encoding their tacit knowledge into AI agents that can support or replace experts in visualization tasks.

Method: The authors conduct an industrial case study and propose a software engineering framework that augments a Large Language Model with several components: a request classifier to route tasks, a Retrieval-Augmented Generation (RAG) system for code generation, explicit codified expert rules, and established visualization design principles. These elements are unified into an AI agent capable of autonomous, reactive, proactive, and social behaviors for simulation data visualization. They evaluate the system on five scenarios across multiple engineering domains with 12 evaluators, comparing against a baseline.

Result: The agent-based system shows a 206% improvement in output quality over the baseline. In all evaluated scenarios, the agent reaches expert-level ratings while the baseline performs poorly. Additionally, the system maintains superior code quality with lower variance, indicating more consistent and reliable performance.

Conclusion: The paper delivers two main contributions: (1) an automated agent-based system that can generate high-quality visualizations, and (2) a validated framework for systematically capturing and codifying tacit expert knowledge into AI agents. The findings indicate that non-experts can achieve expert-level outcomes in specialized domains by leveraging this framework, reducing reliance on scarce human experts and improving scalability in industrial settings.

Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.

</details>


### [76] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: They introduce a post-training method that uses knowledge-graph-derived rewards to teach an LLM to compose axiomatic medical facts, greatly improving multi-hop scientific reasoning and generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs are strong at structured reasoning in math and code but are weak at compositional, multi-hop reasoning in specialized domains like medicine, where correct answers depend on chaining many domain-specific facts. Existing RL and fine-tuning pipelines mostly reward only final answers, which can lead to shallow pattern matching or shortcut learning instead of genuine compositional reasoning. The authors want a scalable way to supervise and verify the *process* of reasoning, not just the final output, using structured knowledge resources already available in scientific domains.

Method: They propose a bottom-up learning paradigm that grounds models in axiomatic domain facts (from a medical knowledge graph) and trains them to compose these facts via short reasoning paths. Concretely, they build a post-training pipeline combining supervised fine-tuning on short-hop knowledge-graph paths (1–3 hops) with reinforcement learning. In RL, the knowledge graph functions as an implicit reward model: the system derives rewards from whether the model’s intermediate reasoning steps align with valid paths in the graph, not only whether the final answer is correct. These path-derived rewards encourage the model to explicitly compose intermediate axioms and learn reusable reasoning patterns that can later generalize to longer chains.

Result: In the medical domain, they train a 14B-parameter model on short-hop KG reasoning paths and then evaluate it zero-shot on harder queries requiring 4–5 hops. The model shows strong generalization: it significantly outperforms much larger language models and named frontier systems (e.g., GPT-5.2, Gemini 3 Pro) on the most challenging multi-hop reasoning benchmarks they construct. They also run adversarial option-shuffling stress tests and find that performance remains robust, suggesting the model is less reliant on answer-position artifacts and more on true reasoning over the knowledge graph structure.

Conclusion: Using knowledge graphs as implicit reward models provides verifiable, grounded, and scalable supervision for training LLMs to perform compositional reasoning. By rewarding alignment with valid KG paths rather than only final answers, their bottom-up training paradigm enables smaller models to outperform larger frontier systems on complex medical multi-hop reasoning and to remain robust under adversarial perturbations. The work argues that grounding the reasoning process in structured domain knowledge is a promising route toward more intelligent and reliable scientific reasoning in LLMs.

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [77] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: The paper identifies a dataset-induced "information collapse" where VLAs ignore language, and proposes BayesianVLA, a Bayesian decomposition with a PMI-based objective that restores language grounding and improves OOD generalization.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models for robot manipulation fail to generalize to novel instructions and multi-task setups because standard goal-driven data collection creates a bias: instructions can be inferred from images alone, so models learn vision-only shortcuts and ignore language in OOD scenarios.

Method: They formally analyze the information-theoretic pathology (information collapse: vanishing conditional mutual information between instructions and actions) and propose BayesianVLA, a dual-branch architecture with learnable Latent Action Queries that separately estimates a vision-only prior p(a|v) and a language-conditioned posterior π(a|v,l). They optimize the policy using a conditional Pointwise Mutual Information objective that penalizes actions explainable by vision alone and rewards those that are specifically supported by the language instruction, all without collecting new data.

Result: On the SimplerEnv and RoboCasa benchmarks, BayesianVLA yields substantially better generalization, including an 11.3% absolute improvement on the challenging OOD SimplerEnv benchmark compared with prior methods, demonstrating stronger instruction grounding in manipulation tasks.

Conclusion: Dataset bias in typical robot manipulation pipelines causes VLAs to collapse into vision-only policies. By explicitly modeling and contrasting a vision prior and a language-conditioned posterior, and maximizing their conditional PMI, BayesianVLA mitigates this information collapse and robustly grounds language in action, leading to significantly improved OOD performance without additional data.

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>
