<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 71]
- [cs.AI](#cs.AI) [Total: 60]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou,Jie Yang*

Main category: cs.CL

TL;DR: The paper proposes State-Centric Retrieval, a unified, efficient RAG retrieval–reranking framework that shares compact intermediate “state” representations between embedding and reranking to remove redundant computation while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional Retrieval-Augmented Generation (RAG) pipelines use a separate embedding retriever and a reranker. These two stages do not share representations, so the reranker re-processes full documents, causing high computational cost and latency, especially for long documents. The authors want to reduce this inefficiency by designing a system where retrieval and reranking share intermediate information and where reranking cost does not scale with document length.

Method: They introduce State-Centric Retrieval, where a single RWKV-based LLM is fine-tuned into EmbeddingRWKV. This unified model serves both as an embedding model and a backbone for extracting compact, reusable “states” (intermediate layer representations) for documents. These states are precomputed and stored. For reranking, they design a state-based reranker that only processes query tokens while conditioning on the precomputed document states, decoupling inference cost from document length. They also study how many intermediate layer states are needed, using a uniform layer selection strategy that keeps only a subset of layers to reduce storage and compute while preserving performance.

Result: The proposed approach achieves significant speedups in the reranking stage—between 5.4× and 44.8×—since the model no longer needs to re-encode full documents. By selecting only 25% of the RWKV layers in a uniform manner, the system retains 98.62% of the full-model performance, indicating that many layer states are redundant. Empirical evaluations across retrieval and reranking benchmarks show that the method yields high-quality results comparable to or better than standard RAG pipelines, while being much more efficient.

Conclusion: State-Centric Retrieval demonstrates that sharing compact intermediate states between embedding and reranking can drastically improve the efficiency of RAG systems with minimal loss in quality. A single RWKV-based backbone can serve as both the embedding model and the source of reusable document states, enabling query-only computation during reranking and reducing dependency on document length. This paradigm suggests a practical path toward scalable, high-performance RAG systems, and the released code facilitates further research and adoption.

Abstract: Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [2] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin,Han Ying,Jiacheng Ji,Hanhui Xu,Mengyue Wu*

Main category: cs.CL

TL;DR: The paper introduces MedES, a scenario-centric benchmark and alignment framework to adapt large language models to complex medical ethics in the Chinese healthcare context, showing that an aligned 7B model can outperform larger baselines on ethical decision tasks.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are increasingly used in healthcare, there is a gap in reliably aligning them with the subtle, context-dependent requirements of medical ethics and law, especially in realistic, complex clinical scenarios and within specific cultural and legal systems such as China’s. Existing evaluations and alignment methods are limited in domain coverage and often lack scenario-level, ethics-focused assessment and feedback mechanisms.

Method: The authors construct MedES, a benchmark built from 260 authoritative Chinese medical, ethical, and legal sources, emphasizing dynamic, real-world clinical scenarios. They then propose a guardian-in-the-loop alignment framework, which includes an automated evaluator model trained on expert-labeled data that reaches over 97% accuracy in the target domain. This evaluator generates targeted prompts and structured ethical feedback. Using this feedback pipeline, the authors align a 7B-parameter LLM via supervised fine-tuning and domain-specific preference optimization, iteratively improving adherence to medical-ethical norms.

Result: Within the Chinese medical ethics setting, the aligned 7B model surpasses significantly larger baseline LLMs on key ethical decision-making tasks, with gains observed both in response quality and in composite quantitative metrics derived from the MedES benchmark and the automated evaluator. The evaluator itself shows high reliability (97%+ accuracy) in assessing ethical consistency and correctness within this domain.

Conclusion: The work provides a practical, modular framework for aligning LLMs with medical-ethical requirements in a culturally and legally specific healthcare system, exemplified by China. By combining a scenario-centric benchmark with an automated, expert-trained evaluator in a guardian-in-the-loop pipeline, smaller models can be made to outperform larger general-purpose LLMs on domain-ethical tasks. The authors argue that this approach can be generalized to other jurisdictions by swapping in local normative corpora and re-training the evaluator, enabling adaptable, context-aware ethical alignment of LLMs in healthcare and potentially other regulated domains.

Abstract: Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [3] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang,Jiantong Qin,Xueli Qiu,Sharon Levy,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

TL;DR: The paper studies how large language models (LLMs) embody human values in realistic advice-giving settings, introducing a new dataset and comparing LLMs with humans.


<details>
  <summary>Details</summary>
Motivation: Although value alignment is seen as crucial for safe AI, we know little about how LLMs actually apply values when giving advice in messy, real-world-like situations, and how that compares to humans’ values and behavior.

Method: The authors build ValAct-15k, a dataset of 3,000 advice-seeking scenarios from Reddit mapped to ten Schwartz human values. They use these scenarios plus a standard value questionnaire to evaluate ten top LLMs (five from U.S. and five from Chinese companies) and 55 human participants. They analyze consistency across LLMs and humans, correspondence between stated and enacted values, and how LLMs behave when instructed to embody a particular value.

Result: LLMs show almost perfect agreement with each other in their decisions in these scenarios, unlike humans who vary widely. For both humans and LLMs, self-reported values only moderately match the values expressed in their actual scenario decisions, revealing a knowledge–action gap. When LLMs are told to “hold” (role-play) a given value rather than just choose it, their performance worsens by up to 6.6%, suggesting reluctance or difficulty in strong value role-play.

Conclusion: Current alignment makes different frontier LLMs converge on very similar normative patterns, but this convergence coexists with a human-like inconsistency between knowing and enacting values. Instructing LLMs to strongly embody a specific value can actually degrade their behavior, indicating that simply prompting for values is not enough to ensure value-consistent decisions.

Abstract: Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


### [4] [Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis](https://arxiv.org/abs/2601.07974)
*Yuxi Xia,Kinga Stańczak,Benjamin Roth*

Main category: cs.CL

TL;DR: The paper studies why AI-text detectors fail to generalize by analyzing how shifts in linguistic features between training and test data correlate with detector accuracy.


<details>
  <summary>Details</summary>
Motivation: Although AI-text detectors often perform well on in-domain benchmarks, they tend to fail under distribution shifts such as new prompts, LLMs, or domains. Prior work mostly reports these failures but doesn’t clearly explain the linguistic reasons behind them. The paper aims to provide a systematic, feature-based explanation of generalization behavior so that detector design and evaluation can be improved.

Method: The authors build a broad benchmark of human- and AI-generated texts covering 6 prompting strategies, 7 LLMs, and 4 domains. They fine-tune classification-based AI-text detectors on different subsets of this data and then evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain performance differences, they compute 80 linguistic features (e.g., tense usage, pronoun frequency) and measure how much these features shift between training and test conditions. They then correlate these feature shifts with changes in generalization accuracy.

Result: The study finds that generalization performance is systematically related to shifts in certain linguistic features between training and test data. In particular, detectors’ success or failure on specific evaluation conditions is strongly associated with differences in features such as verb tense distribution and pronoun frequency.

Conclusion: The paper concludes that linguistic feature shifts are a key factor underlying the generalization gaps of AI-text detectors. Certain stylistic and grammatical markers, like tense usage and pronoun patterns, strongly influence whether detectors trained in one setting will transfer to others. Understanding and accounting for these feature shifts is crucial for building more robust, generalizable AI-text detection systems.

Abstract: AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.

</details>


### [5] [Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models](https://arxiv.org/abs/2601.07984)
*Haorui Yu,Ramon Ruiz-Dolz,Xuehang Wen,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: The paper proposes and validates a three-tier framework to quantitatively evaluate how well vision-language models understand cultural meaning in art, producing calibrated cultural-understanding scores and diagnostics.


<details>
  <summary>Details</summary>
Motivation: While vision-language models perform strongly on general visual perception tasks, their capacity to interpret culturally embedded meaning in artworks is unclear and under-validated. Existing automated metrics do not capture cultural depth or nuance, and cross-judge variability in human evaluations complicates reliable benchmarking. There is a need for a systematic, calibrated evaluation framework that can assess cultural understanding across different traditions and guide model comparison and diagnosis.

Method: The authors design a tri-tier evaluation framework: Tier I computes automated coverage and risk indicators offline to characterize model outputs at scale; Tier II employs a rubric-based human scoring protocol across five dimensions of art critique, using a single primary judge for consistency; Tier III then calibrates the aggregate rubric score to human ground-truth ratings via isotonic regression, aligning the scale and improving predictive accuracy. They apply this framework to evaluate 15 vision-language models on 294 expert-annotated art prompts (anchors) drawn from six cultural traditions, and analyze the resulting scores and diagnostics.

Result: The isotonic regression calibration in Tier III reduces mean absolute error by 5.2% on a 152-sample held-out set relative to uncalibrated scores. The evaluation of 15 VLMs shows that commonly used automated metrics fail to reliably reflect cultural depth in art understanding, Western artworks receive higher scores than non-Western under the authors’ sampling and rubric, and averaging scores from multiple human judges without calibration yields inconsistent results due to scale mismatches.

Conclusion: The proposed tri-tier framework provides a more reliable, calibrated way to assess the cultural understanding of vision-language models in art critique tasks. It highlights the inadequacy of standard automated metrics for measuring cultural depth, reveals systematic performance gaps between Western and non-Western samples under the chosen setup, and shows the importance of using a single primary judge with explicit calibration to avoid cross-judge scale inconsistencies. The framework can support model selection, cultural-gap diagnosis, and more culturally aware development of VLMs.

Abstract: Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.

</details>


### [6] [Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset](https://arxiv.org/abs/2601.07985)
*Z. Melce Hüsünbeyi,Virginie Mouilleron,Leonie Uhling,Daniel Foppe,Tatjana Scheffler,Djamé Seddah*

Main category: cs.CL

TL;DR: The paper presents a pipeline to build multilingual (French, German), multimodal fact-checking datasets with structured links between claims, evidence, and verdicts, using LLMs for evidence extraction and justification generation.


<details>
  <summary>Details</summary>
Motivation: Misinformation is spreading rapidly online, and current fact-checking datasets are insufficient: they are often monolingual, text-only, lack rich structure, and do not clearly link claims to specific evidence and verdicts. This limits the training and evaluation of robust, explainable, and multilingual fact-checking models.

Method: The authors aggregate ClaimReview feeds in French and German, scrape full debunking articles, and normalize heterogeneous verdict labels. They enrich each item with structured metadata and aligned visual content. State-of-the-art (multimodal) LLMs are used to (i) extract evidence according to predefined evidence categories, and (ii) generate justifications that explicitly connect the selected evidence with the final verdict. They then evaluate the quality of this pipeline with G-Eval and human assessments.

Result: The pipeline successfully constructs multilingual, multimodal fact-checking datasets in French and German that include structured links among claims, evidence, and verdicts, plus visual content and metadata. G-Eval and human evaluations indicate that the extracted evidence and generated justifications are of sufficient quality to support fine-grained analysis of fact-checking practices and the development of interpretable models.

Conclusion: The proposed pipeline provides a practical way to build rich fact-checking datasets that go beyond simple claim–label pairs, supporting multilingual and multimodal settings. It enables more detailed comparison of fact-checking organizations, fosters the creation of more interpretable and evidence-grounded fact-checking systems, and establishes a foundation for future research in multilingual, multimodal misinformation verification.

Abstract: The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.

</details>


### [7] [VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding](https://arxiv.org/abs/2601.07986)
*Haorui Yu,Ramon Ruiz-Dolz,Diji Yang,Hang He,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: VULCA-Bench is a multicultural art-critique benchmark to test vision-language models on deep cultural understanding, not just surface visual tasks.


<details>
  <summary>Details</summary>
Motivation: Most current VLM benchmarks focus on low-level perception (object/scene recognition and factual Q&A) and largely ignore higher-order, culturally grounded interpretation of artworks. There is a need for a systematic way to evaluate whether models understand culture-specific meanings, symbols, and aesthetics beyond basic vision-language alignment.

Method: The authors construct VULCA-Bench, a dataset of 7,410 image-critique pairs across eight cultural traditions, with Chinese-English bilingual coverage. They define a five-layer framework of cultural understanding from L1 to L5 (Visual Perception → Philosophical Aesthetics) and instantiate it into 225 concrete, culture-specific evaluation dimensions. Expert annotators write bilingual art critiques aligned with this framework, and the benchmark is paired with evaluation scripts and annotation tools.

Result: Pilot experiments with current vision-language models show that performance drops as the evaluation moves from lower layers (L1-L2: visual and technical analysis) to higher layers (L3-L5: cultural, symbolic, and philosophical reasoning), indicating that models struggle with deeper cultural understanding.

Conclusion: VULCA-Bench exposes a significant gap in VLMs’ ability to perform higher-order, culture-aware art interpretation. It provides a structured, multilingual, culturally diverse benchmark and tooling under an open license to drive future research on culturally grounded vision-language understanding.

Abstract: We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.

</details>


### [8] [From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP](https://arxiv.org/abs/2601.07988)
*Adithya V Ganesan,Vasudha Varadarajan,Oscar NE Kjell,Whitney R Ringwald,Scott Feltman,Benjamin J Luft,Roman Kotov,Ryan L Boyd,H Andrew Schwartz*

Main category: cs.CL

TL;DR: The paper argues that standard NLP pipelines, which treat documents as independent and unordered, are flawed for longitudinal data and proposes a new behavior-sequence-focused modeling and evaluation framework.


<details>
  <summary>Details</summary>
Motivation: In longitudinal studies, texts are generated by the same individuals over time, making documents nested within people and ordered. Standard NLP methods ignore this structure, leading to misleading conclusions when modeling behaviors or symptoms over time. The authors want to create methods and evaluation protocols that respect the temporal and person-specific nature of such data.

Method: The authors formalize a longitudinal NLP paradigm that modifies four components of the pipeline: (1) designing evaluation splits that test generalization across people (cross-sectional) and across time (prospective); (2) defining metrics that disentangle between-person differences from within-person change; (3) using sequence-based inputs that incorporate each person’s history by default; and (4) exploring model architectures with varying granularity of latent state over time (from pooled history summaries to explicit temporal dynamics and interaction-based models). They test traditional vs proposed approaches on a large daily-diary PTSD dataset.

Result: Using 17,000 daily diary transcripts linked with PTSD symptom scores from 238 individuals, the study shows that standard document-level evaluation can produce substantially different—and sometimes opposite—conclusions about model performance and findings compared to the proposed longitudinally aware framework, which better reflects real-world use and ecological validity.

Conclusion: Longitudinal text data should be modeled as person-indexed, time-ordered behavioral sequences rather than independent documents. Evaluations must align with generalization over people and time, and models should incorporate history and appropriate temporal representations. The paper advocates moving the field from word-sequence-centric evaluation to behavior-sequence paradigms for applications that track or predict human states over time.

Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.

</details>


### [9] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi,Jonathan Zhang,Jinho D. Choi*

Main category: cs.CL

TL;DR: DyCP is a lightweight, query-time context management method that dynamically segments and retrieves relevant parts of long dialogues, improving answer quality and reducing latency across multiple benchmarks and LLMs.


<details>
  <summary>Details</summary>
Motivation: As conversations with LLMs grow longer, models suffer from slower responses and worse answer quality. Existing memory or context management approaches often need extra LLM calls or offline processing that ignores the latest user input, causing inefficiency and breaking conversational flow. There is a need for a simple, efficient method that can manage long contexts in real time while preserving dialogue structure.

Method: The paper proposes DyCP, a dynamic context processing method. At query time, DyCP segments the dialogue into sequential units without predefined topic boundaries and retrieves only the most relevant segments as memory for the current query. It focuses on preserving the natural order of the dialogue while performing efficient, adaptive context retrieval, and avoids additional LLM calls for memory construction.

Result: On three long-form dialogue benchmarks (LoCoMo, MT-Bench+, SCM4LLMs) and across multiple LLMs, DyCP consistently improves answer quality compared to baselines while also reducing response latency. The experiments show that even for models with large context windows, naive use of long context underperforms well-managed context via DyCP.

Conclusion: Effective context management remains crucial even as LLM context windows grow. DyCP offers a lightweight, real-time approach that respects dialogue structure and adaptively retrieves relevant history, leading to better and faster responses in long conversations. This demonstrates that context management algorithms are necessary to fully leverage large-context LLMs.

Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [10] [Is Sentiment Banana-Shaped? Exploring the Geometry and Portability of Sentiment Concept Vectors](https://arxiv.org/abs/2601.07995)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer L. Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: The paper evaluates Concept Vector Projections (CVP) for sentiment analysis in humanities across different domains, showing it transfers well but relies on an approximate linearity assumption.


<details>
  <summary>Details</summary>
Motivation: Humanities research often needs nuanced, contextual, and continuous sentiment scores, not just discrete labels, and these scores should ideally be multilingual and portable across different genres, times, and languages. CVP is a promising recent method, but its robustness, portability, and core assumptions have not been systematically tested.

Method: The authors represent sentiment as a direction in embedding space via Concept Vector Projections (CVP) and empirically evaluate models trained on one corpus and tested on others. They test across multiple genres, historical periods, languages, and affective dimensions. They also analyze the linearity assumption of CVP by probing how well a single linear direction in embedding space captures sentiment across these varied settings.

Result: Concept vectors trained on one corpus transfer effectively to other corpora with only minimal performance loss across genres, periods, languages, and affective dimensions. However, the analysis shows that the linearity assumption—that sentiment can be fully captured as a single direction in embedding space—is only approximately valid.

Conclusion: CVP is a notably portable and effective approach for generating continuous, multilingual sentiment scores in humanities contexts, successfully capturing generalizable affective patterns across domains. Still, because its linearity assumption holds only approximately, there is room for improvement, suggesting future work might explore richer or more flexible representations of sentiment beyond a single linear direction.

Abstract: Use cases of sentiment analysis in the humanities often require contextualized, continuous scores. Concept Vector Projections (CVP) offer a recent solution: by modeling sentiment as a direction in embedding space, they produce continuous, multilingual scores that align closely with human judgments. Yet the method's portability across domains and underlying assumptions remain underexplored. We evaluate CVP across genres, historical periods, languages, and affective dimensions, finding that concept vectors trained on one corpus transfer well to others with minimal performance loss. To understand the patterns of generalization, we further examine the linearity assumption underlying CVP. Our findings suggest that while CVP is a portable approach that effectively captures generalizable patterns, its linearity assumption is approximate, pointing to potential for further development.

</details>


### [11] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li,Mingxiao Song,Zhenda Shen,Dachuan Zhao,Yunfan Long,Yi Li,Yongce Li,Ruyi Yang,Mengyu Wang*

Main category: cs.CL

TL;DR: The paper proposes LLM Review, a blind peer-review-inspired multi-agent framework that boosts creative writing performance of LLMs, evaluated via a new SciFi-100 benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs and multi-agent systems are good at reasoning but often weak at open-ended creative writing. Standard multi-agent interaction can cause agents to converge on similar ideas, reducing diversity and originality. There is a need for interaction structures that encourage constructive feedback while preserving divergent creative directions, and for a rigorous evaluation setup focused on creative text generation.

Method: The authors design LLM Review, a framework where multiple LLM agents independently draft creative pieces and then exchange blinded, targeted peer-review-style feedback without seeing each other’s full identities or full trajectories. Each agent then revises its own work based on the received feedback, maintaining independent creative paths. To evaluate, they build SciFi-100, a science-fiction writing benchmark with standardized prompts, and assess outputs through a combined evaluation scheme: LLM-as-a-judge quality scoring, human annotations of creativity and coherence, and rule-based novelty metrics to quantify originality and divergence.

Result: LLM Review improves creative writing performance versus existing multi-agent baselines across multiple metrics on the SciFi-100 benchmark. The framework helps maintain diversity while increasing overall quality. Furthermore, smaller LLMs using LLM Review can outperform larger single-agent LLMs, indicating that structured interaction design can partially compensate for model size in creative generation tasks.

Conclusion: Interaction structure matters as much as, or more than, sheer model scale for creative generation. A blind peer-review-inspired multi-agent setup can encourage constructive critique while preserving diversity, leading to better creative outputs. The SciFi-100 benchmark and evaluation protocol provide a standardized way to measure such improvements and could support future research on creativity-enhancing LLM frameworks.

Abstract: Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [12] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He,Guangzhi Xiong,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: They investigate why Chain-of-Thought (CoT) prompting improves LLM reasoning and show that specific internal latent features, identifiable via Sparse Autoencoders, causally drive multi-step reasoning and can be directly steered without explicit CoT prompts.


<details>
  <summary>Details</summary>
Motivation: Although CoT prompting is known to enhance reasoning in LLMs, it is unclear what internal mechanisms are responsible, whether CoT is necessary, and how reasoning states are represented and triggered inside models. The paper aims to open this black box by mechanistically understanding and controlling the internal activations that underlie reasoning, going beyond surface-level prompt engineering.

Method: The authors train and apply Sparse Autoencoders (SAEs) on internal activations of LLMs during reasoning tasks to discover interpretable latent features. They then perform causal interventions by selectively steering or activating specific "reasoning-related" latent units during generation. They evaluate the effects of these interventions across several LLM families and reasoning benchmarks, and analyze when in the generation process these latents are activated and how they interact with prompts that either encourage or discourage explicit reasoning.

Result: They identify a small number of SAE latent features that are causally associated with improved reasoning behavior. By steering a single reasoning-related latent, models achieve substantial accuracy gains on reasoning benchmarks without CoT-style prompts. In larger models, this latent steering achieves accuracy comparable to standard CoT prompting while producing shorter, more efficient outputs. They also find that the reasoning-oriented internal state is activated early in the generation and can dominate over prompt instructions that tell the model not to reason explicitly.

Conclusion: Multi-step reasoning in LLMs is driven by specific latent internal activations that can be externally controlled, and Chain-of-Thought prompting is just one effective way to activate these states rather than a fundamentally unique or necessary mechanism. This suggests that mechanistic interventions on internal representations (via tools like SAEs) offer a powerful alternative to prompt-based methods for eliciting and studying reasoning behavior in LLMs.

Abstract: Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [13] [Universal computation is intrinsic to language model decoding](https://arxiv.org/abs/2601.08061)
*Alex Lewandowski,Marlos C. Machado,Dale Schuurmans*

Main category: cs.CL

TL;DR: The paper proves that language models, via chained autoregressive outputs, can perform universal computation and that even randomly initialized models are already computationally universal; training mainly improves how easily we can program them via prompts.


<details>
  <summary>Details</summary>
Motivation: There is ongoing debate about the ultimate computational power of language models because they are trained for next-token prediction rather than given an explicit algorithmic structure like a Turing machine. The authors aim to clarify whether such models are fundamentally limited or can, in principle, implement any algorithm, and to understand what role training plays in their computational expressiveness.

Method: The authors use theoretical analysis to show that the autoregressive mechanism of language models is sufficient to simulate the execution of any algorithm. They construct a formal mapping between chained next-token predictions and the step-by-step execution of a universal computational model. They then extend this reasoning to randomly initialized language models, arguing that their architecture alone can support universal computation without relying on learned weights.

Result: They prove that a language model whose outputs are chained autoregressively can simulate any algorithm on any input, establishing that such models are computationally universal. Furthermore, they show that this universality already holds for randomly initialized models, indicating that no training is required to obtain universal computational capability in principle.

Conclusion: Language models possess intrinsic universal computational power due to their autoregressive architecture. Training does not create this expressiveness but instead enhances programmability, i.e., the practical ability to elicit specific computations through suitable prompts, thereby turning the model into a more usable natural-language-programmable computer.

Abstract: Language models now provide an interface to express and often solve general problems in natural language, yet their ultimate computational capabilities remain a major topic of scientific debate. Unlike a formal computer, a language model is trained to autoregressively predict successive elements in human-generated text. We prove that chaining a language model's autoregressive output is sufficient to perform universal computation. That is, a language model can simulate the execution of any algorithm on any input. The challenge of eliciting desired computational behaviour can thus be reframed in terms of programmability: the ease of finding a suitable prompt. Strikingly, we demonstrate that even randomly initialized language models are capable of universal computation before training. This implies that training does not give rise to computational expressiveness -- rather, it improves programmability, enabling a natural language interface for accessing these intrinsic capabilities.

</details>


### [14] [Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations](https://arxiv.org/abs/2601.08064)
*Yuxi Xia,Dennis Ulmer,Terra Blevins,Yihong Liu,Hinrich Schütze,Benjamin Roth*

Main category: cs.CL

TL;DR: The paper proposes a new framework to evaluate how trustworthy large language models’ confidence scores are, focusing on robustness and semantic consistency rather than only calibration and discrimination.


<details>
  <summary>Details</summary>
Motivation: Existing confidence estimation methods for LLMs are mostly judged by how well stated confidence matches accuracy (calibration) or ranks correct over incorrect answers (discrimination). These metrics miss key behaviors needed in realistic language use: confidence should stay stable when prompts or answers are rephrased without changing meaning, and should change when the meaning changes. Without evaluating these aspects, confidence estimates can mislead users even if they look calibrated overall.

Method: The authors introduce an evaluation framework that adds three dimensions to confidence estimation assessment: (1) robustness to prompt perturbations (semantically equivalent prompt variants should yield similar confidence), (2) stability across semantically equivalent answers (different wordings of the same answer should get similar confidence), and (3) sensitivity to semantically different answers (confidence should vary appropriately when answer meaning changes). They construct tests along these axes and apply them to common LLM confidence estimation methods that were previously mainly evaluated on calibration and discrimination.

Result: Empirical evaluation shows that many popular confidence estimation methods for LLMs perform well on traditional metrics like calibration and discrimination but fail on the new ones. Some methods are highly sensitive to minor prompt rephrasings, while others do not appropriately lower confidence when the answer meaning changes, indicating inconsistency and potential unreliability in real usage scenarios.

Conclusion: Standard CE evaluation with only calibration and discrimination is insufficient for LLM settings with rich linguistic variation. The proposed framework exposes important weaknesses in current methods and offers new metrics that better reflect practical reliability requirements. This can guide practitioners and researchers in choosing and designing confidence estimation approaches that are more robust, semantically consistent, and thus safer for real-world deployment.

Abstract: Confidence estimation (CE) indicates how reliable the answers of large language models (LLMs) are, and can impact user trust and decision-making. Existing work evaluates CE methods almost exclusively through calibration, examining whether stated confidence aligns with accuracy, or discrimination, whether confidence is ranked higher for correct predictions than incorrect ones. However, these facets ignore pitfalls of CE in the context of LLMs and language variation: confidence estimates should remain consistent under semantically equivalent prompt or answer variations, and should change when the answer meaning differs. Therefore, we present a comprehensive evaluation framework for CE that measures their confidence quality on three new aspects: robustness of confidence against prompt perturbations, stability across semantic equivalent answers, and sensitivity to semantically different answers. In our work, we demonstrate that common CE methods for LLMs often fail on these metrics: methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes. Overall, our framework reveals limitations of existing CE evaluations relevant for real-world LLM use cases and provides practical guidance for selecting and designing more reliable CE methods.

</details>


### [15] [AdaJudge: Adaptive Multi-Perspective Judging for Reward Modeling](https://arxiv.org/abs/2601.08097)
*Yongliang Miao,Yangyang Liang,Mengnan Du*

Main category: cs.CL

TL;DR: AdaJudge is a reward modeling framework that adaptively refines representations and pooling, outperforming standard static pooling reward models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing reward models for aligning large language models use static pooling to map token sequences to scalar scores. This static strategy introduces a fixed inductive bias that may not match different tasks’ preference signals, and it reuses representations optimized for text generation rather than fine-grained preference discrimination, leading to suboptimal alignment quality. A more flexible, task-aware way to extract and use sequence information for reward scoring is needed.

Method: AdaJudge jointly adapts representation and aggregation in reward models. It adds gated refinement blocks on top of the backbone to transform generation-oriented representations into a discrimination-oriented space tailored for preference judgment. For aggregation, it replaces standard static readout (e.g., simple pooling) with an adaptive multi-view pooling module that dynamically routes and combines evidence from different parts or views of the sequence. This creates a more expressive, task-adaptive mapping from token-level features to a final scalar reward.

Result: On RM-Bench and JudgeBench, two benchmarks for reward modeling and LLM-as-a-judge evaluation, AdaJudge consistently outperforms strong off-the-shelf reward models and baselines that use traditional static pooling strategies. The gains show up across various tasks that require fine-grained preference discrimination.

Conclusion: Adapting both the internal representations and the pooling/readout mechanism yields more accurate reward models than conventional static pooling approaches. By refining backbone features into a discrimination-focused space and using adaptive multi-view pooling, AdaJudge better captures task-dependent preference signals and improves alignment performance on established reward modeling benchmarks.

Abstract: Reward modeling is essential for aligning large language models with human preferences, yet predominant architectures rely on a static pooling strategy to condense sequences into scalar scores. This paradigm, however, suffers from two key limitations: a static inductive bias that misaligns with task-dependent preference signals, and a representational mismatch, as the backbone is optimized for generation rather than fine-grained discrimination. To address this, we propose AdaJudge, a unified framework that jointly adapts representation and aggregation. AdaJudge first refines backbone representations into a discrimination-oriented space via gated refinement blocks. It then replaces the static readout with an adaptive multi-view pooling module that dynamically routes and combines evidence. Extensive experiments on RM-Bench and JudgeBench show that AdaJudge outperforms strong off-the-shelf reward models and traditional pooling baselines.

</details>


### [16] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh,Tianyi Chen,Chen-Hao Chiang,Bin Shen*

Main category: cs.CL

TL;DR: The paper studies how to suggest alternative, answerable queries when a user’s question is outside the knowledge or tool scope of an agentic RAG system.


<details>
  <summary>Details</summary>
Motivation: Agentic RAG systems are powerful but constrained by the coverage of their tools and underlying knowledge. When user queries fall outside this scope, current guardrails focus on blocking or rejecting them, which ends the interaction and frustrates users. There is no established method for proactively suggesting similar, answerable questions that respect the system’s limitations while still helping the user. The paper aims to fill this gap and improve user interaction and safety.

Method: The authors formalize the task of query suggestion in agentic RAG under the condition that the original user question is unanswerable. They propose a robust dynamic few-shot learning approach that, at run time, retrieves examples from prior relevant RAG workflows (e.g., previous user queries and their handling) and uses them as context to generate query suggestions. This method is self-learned from historical, unlabeled real-world queries and is designed to understand the multi-step nature of RAG pipelines better than static prompts or simple retrieval alone.

Result: On three benchmark datasets created from two real-world, unlabeled user query collections, the proposed system generates suggested queries that are both more relevant to the user’s original intent and more likely to be answerable by the RAG system. It outperforms standard few-shot baselines and retrieval-only methods in terms of relevance and answerability metrics.

Conclusion: Query suggestion for agentic RAG is a distinct and important problem that existing guardrail or search-style recommendation methods do not solve. By leveraging robust dynamic few-shot learning over past workflows, a RAG agent can automatically learn to propose similar, answerable alternatives when the user’s question is out of scope. This leads to safer, more effective, and more user-friendly interactions with tool-calling RAG agents.

Abstract: Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [17] [Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought](https://arxiv.org/abs/2601.08108)
*Bowen Li,Ziqi Xu,Jing Ren,Renqiang Luo,Xikun Zhang,Xiuzhen Zhang,Yongli Ren,Feng Xia*

Main category: cs.CL

TL;DR: They present ACPS, a prompting framework for LLMs that uses causal modeling and concise “Sketch-of-Thought” reasoning to improve accuracy and efficiency over Chain-of-Thought while reducing token usage and working across diverse reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing prompting methods like Chain-of-Thought often require long, verbose reasoning traces, which increases token usage and inference cost. Moreover, these approaches tend to be less generalisable across heterogeneous reasoning tasks, limiting their practical applicability. The authors want a prompting method that is both more efficient and more robustly general across tasks, especially for causal and logical reasoning.

Method: They introduce Adaptive Causal Prompting with Sketch-of-Thought (ACPS). ACPS uses structural causal models to model the relationship between a query and its answer. Based on this model, it estimates the causal effect of the query and adaptively chooses an intervention strategy, such as standard front-door or conditional front-door adjustment. Instead of long Chain-of-Thought explanations, ACPS employs a shorter, more abstract “Sketch-of-Thought” reasoning pattern to guide the LLM, thereby cutting down tokens while still structuring the reasoning. The framework is designed to be plug-and-play across tasks without retraining.

Result: Across multiple reasoning benchmarks and different LLM backbones, ACPS achieves higher accuracy and robustness compared to existing prompting baselines. It also uses fewer tokens and lowers inference costs, indicating better computational efficiency while maintaining or improving performance.

Conclusion: ACPS provides a general, causally-grounded prompting framework that improves LLM reasoning quality and robustness while significantly reducing token and compute overhead. By combining adaptive causal interventions with concise Sketch-of-Thought prompting, the method generalises across diverse reasoning tasks without task-specific retraining and outperforms standard CoT-based methods.

Abstract: Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.

</details>


### [18] [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)
*Jonathan Su*

Main category: cs.CL

TL;DR: The paper introduces ExoFormer, a Transformer architecture that uses external anchor projections instead of reusing early-layer attention as residuals, improving performance, data efficiency, and reducing attention sinks while revealing a new offloading-based explanation for representation collapse.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers and recent variants often reuse early-layer attention projections as residual anchors for all deeper layers. This couples two conflicting roles: (1) providing a stable reference representation for later computation, and (2) performing substantial computation at the first layer itself. This tension may limit model expressivity, stability, and efficiency. The authors want to decouple these roles to improve performance, understand attention dynamics, and explain phenomena like attention sinks and representation collapse.

Method: The authors propose ExoFormer, a Transformer architecture that introduces exogenous (external) anchor projections that live outside the normal sequential stack of layers. These anchors serve as stable reference representations for all layers, while the in-stack layers focus on computational refinement. They design a normalized mixing framework that blends anchor and layer-generated representations along different attention pathways (queries, keys, values, and gating logits). They systematically compare different mixing granularities (elementwise, headwise, scalar coefficients) and contrast ExoFormer with internal-anchor baselines that reuse early-layer outputs. They study performance, data efficiency, attention sink behavior, and analyze representation geometry to formulate the Offloading Hypothesis.

Result: Across multiple configurations, ExoFormer variants outperform baselines that rely on internal anchors, including improved downstream accuracy, better validation loss for the same or fewer tokens, and reduced attention sink. The dynamic ExoFormer variant yields a 2.13-point accuracy improvement and matches baseline validation loss with 1.84x fewer tokens, evidencing improved data efficiency. ExoFormer also halves the attention sink phenomenon relative to standard Gated Attention models. Despite these gains, all ExoFormer variants show representation collapse, where in-layer representations become less distinguishable, motivating a new explanatory hypothesis.

Conclusion: Decoupling anchoring from computation via external exogenous projections leads to more effective Transformers: ExoFormer improves accuracy, data efficiency, and mitigates attention sinks compared to internal-anchor architectures. The observed representation collapse is not necessarily harmful; the authors propose an Offloading Hypothesis where external anchors maintain token identity while internal layers specialize in sophisticated transformations. This reframes our understanding of how residual and attention structures support both stability and expressivity in deep Transformers and opens avenues for further architectural and theoretical work. Code and models are released to encourage follow-up research.

Abstract: Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.

</details>


### [19] [How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains](https://arxiv.org/abs/2601.08134)
*Reza Khanmohammadi,Erfan Miahi,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: The paper introduces RMCB, a large benchmark of reasoning traces from multiple Large Reasoning Models to study and evaluate confidence estimation methods for long-form, multi-step reasoning outputs. It finds a fundamental trade-off between discrimination and calibration across methods and suggests a performance ceiling for current representation-based approaches.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models are miscalibrated, especially in high-stakes domains such as clinical, financial, legal, and mathematical reasoning, where knowing when the model is likely correct is crucial. Existing work lacks a comprehensive, standardized benchmark and systematic analysis of confidence estimation methods for long-form reasoning traces, making it difficult to assess and improve model reliability.

Method: The authors construct RMCB, a benchmark containing 347,496 reasoning traces generated by six popular LRMs from different architectural families, covering multiple high-stakes and complex reasoning datasets. Each sample is annotated for correctness. Using this benchmark, they run a large-scale empirical comparison of more than ten representation-based confidence estimation approaches, including sequential models, graph-based models, and text-based encoders that operate on chunk-level hidden states from the LRMs.

Result: On RMCB, text-based encoders achieve the highest discrimination between correct and incorrect reasoning traces (best AUROC of 0.672), while structurally-aware models (e.g., graph-based or models using explicit structure of the reasoning trace) achieve the best calibration (lowest ECE of 0.148). No single method performs best on both metrics, and more complex architectures do not consistently outperform simpler sequential baselines.

Conclusion: There is a persistent trade-off between discrimination and calibration in current representation-based confidence estimation methods for LRMs, and increasing architectural complexity does not reliably overcome it. RMCB offers a comprehensive benchmark and rigorous baselines, and the results suggest that methods relying solely on chunk-level hidden states may have reached a performance ceiling, indicating that new paradigms are needed for robust confidence estimation in long-form reasoning.

Abstract: The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.

</details>


### [20] [Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training](https://arxiv.org/abs/2601.08141)
*Muhammad Taimoor Hassan,Jawad Ahmed,Muhammad Awais*

Main category: cs.CL

TL;DR: The paper presents Qalb, an Urdu-specialized language model derived from LLaMA 3.1 8B, which achieves state-of-the-art performance on multiple Urdu NLP tasks through continued pre-training and instruction fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Urdu, despite being spoken by over 230 million people, is underrepresented in NLP, and existing multilingual and base models (e.g., LLaMA-3.1 8B-Instruct) perform poorly on Urdu due to its complex morphology, script, and literary richness. There is a need for a high-performance, Urdu-focused language model that can handle diverse Urdu tasks and domains.

Method: The authors start from LLaMA 3.1 8B and apply a two-stage process. First, they perform continued pre-training on 1.97B tokens: 1.84B tokens of diverse Urdu sources (news, classical and contemporary literature, government documents, social media) plus 140M English Wikipedia tokens to mitigate catastrophic forgetting. Second, they apply supervised instruction fine-tuning using the Alif Urdu-instruct dataset. They then evaluate the resulting model, Qalb, on multiple Urdu-specific benchmarks across seven task categories.

Result: Qalb significantly outperforms both the previous Urdu-focused Alif-1.0-Instruct model and the base LLaMA-3.1 8B-Instruct model on Urdu benchmarks. It achieves a weighted average score of 90.34, surpassing Alif-1.0-Instruct (87.1) by 3.24 points and LLaMA-3.1 8B-Instruct by 44.64 points. It attains state-of-the-art performance across seven diverse Urdu tasks, including Classification, Sentiment Analysis, and Reasoning.

Conclusion: The study shows that continued pre-training on large-scale, diverse, high-quality Urdu text, combined with focused instruction fine-tuning, is an effective strategy for adapting general-purpose foundation models to low-resource languages. This approach yields substantial performance gains and sets a new state-of-the-art for Urdu NLP across multiple task types.

Abstract: Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.

</details>


### [21] [Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning](https://arxiv.org/abs/2601.08146)
*Khumaisa Nur'aini,Ayu Purwarianti,Alham Fikri Aji,Derry Wijaya*

Main category: cs.CL

TL;DR: They introduce CT-SFT, a method that adapts large language models to low-resource languages by only fine-tuning a small, automatically identified set of task-relevant attention heads (plus LayerNorm), improving performance and reducing catastrophic forgetting compared to full-model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to low-resource languages is challenging due to limited labeled data, instability of full-model fine-tuning, and catastrophic forgetting when doing continued cross-lingual tuning. The authors want a more parameter-efficient and stable way to transfer task abilities from a high-resource proxy language to a low-resource target language while retaining performance in the proxy/source language.

Method: They build on CD-T (Contextual Decomposition Transformer) and propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT). CT-SFT (1) uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint; (2) transfers to the target language by updating only those identified heads and LayerNorm parameters; and (3) enforces this via head-level gradient masking so only selected components are trained during transfer.

Result: On NusaX-Senti and XNLI benchmarks, CT-SFT yields higher cross-lingual accuracy than continued full-model fine-tuning, despite updating only a small fraction of parameters. It also shows an “editing-preserving” trade-off, where hard transfer settings benefit from editing task-relevant circuit heads, while easier transfers are better served by leaving important heads mostly unchanged and instead adjusting low-relevance heads. Additionally, CT-SFT significantly mitigates catastrophic forgetting of the proxy/source language.

Conclusion: CT-SFT is an effective, parameter-efficient strategy for adapting LLMs to low-resource languages. By targeting and selectively updating task-relevant attention heads and LayerNorms, it improves cross-lingual task performance, reduces catastrophic forgetting, and reveals a nuanced trade-off between modifying core task circuits versus preserving them and editing less relevant components, depending on transfer difficulty.

Abstract: Adapting LLMs to low-resource languages is difficult: labeled data is scarce, full-model fine-tuning is unstable, and continued cross-lingual tuning can cause catastrophic forgetting. We propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT): a counterfactual-free adaptation of CD-T (Contextual Decomposition Transformer) that uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint, then transfer learns to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Across NusaX-Senti and XNLI, CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. We find an editing-preserving trade-off: harder transfers favor editing circuit heads, while easier transfers often favor near-zero (i.e., low-relevance heads) updates, preserving the source mechanism. CT-SFT also substantially reduces catastrophic forgetting, preserving proxy/source-language competence during transfer.

</details>


### [22] [WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents](https://arxiv.org/abs/2601.08158)
*Yuqing Zhou,Zhuoer Wang,Jie Yuan,Hong Wang,Samson Koelle,Ziwei Zhu,Wei Niu*

Main category: cs.CL

TL;DR: WISE-Flow is a framework that turns logs of LLM-agent service interactions into reusable workflows so the agent can self-improve over time without costly retraining.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents in user-facing services frequently fail on new tasks, repeat similar mistakes, and behave inconsistently across runs. Fixing these issues through environment-specific training or manual patching is expensive and does not scale. The authors want an agent that can systematically learn from its own past interactions in a structured, reusable way, enabling more reliable performance in production settings.

Method: The authors introduce WISE-Flow, a workflow-centric framework that (1) mines historical service interaction logs of LLM agents and converts them into structured workflows composed of prerequisite-augmented action blocks, capturing procedural knowledge; and (2) at deployment time, retrieves relevant workflows for a new task, aligns the current execution trajectory with these workflows, and performs prerequisite-aware feasibility reasoning to decide the next action grounded in the current environment state. This allows the agent to follow and adapt existing workflows instead of improvising from scratch.

Result: On ToolSandbox and τ^2-bench benchmarks, integrating WISE-Flow with different base LLMs yields consistent performance improvements, showing better reliability and task success compared to baseline agent setups without workflow induction and alignment.

Conclusion: WISE-Flow enables LLM-based agents to self-improve in user-facing environments by converting unstructured past interactions into reusable workflows and using them to guide future action selection with state-aware reasoning. This reduces reliance on expensive retraining or manual fixes while improving stability and accuracy across tasks and models.

Abstract: Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.

</details>


### [23] [SwiftMem: Fast Agentic Memory via Query-aware Indexing](https://arxiv.org/abs/2601.08160)
*Anxin Tian,Yiming Li,Xing Li,Hui-Ling Zhen,Lei Chen,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: SwiftMem is a query-aware memory system for LLM agents that uses temporal and semantic indexing plus consolidation to achieve much faster, sub-linear retrieval without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing agentic memory systems for LLMs typically scan or retrieve over the entire memory store regardless of the query. As the memory size grows, this exhaustive approach creates serious latency and scalability issues, limiting real-time use of memory-augmented agents even though accurate retrieval is central to their performance.

Method: They design SwiftMem, which introduces: (1) a temporal index that supports logarithmic-time range queries for time-related information; (2) a semantic DAG-Tag index, where memories and queries are organized using hierarchical semantic tags so that only relevant topical regions are searched; and (3) an embedding-tag co-consolidation mechanism that periodically reorganizes memory based on semantic clusters to reduce fragmentation and improve cache locality during growth.

Result: On the LoCoMo and LongMemEval benchmarks, SwiftMem delivers up to 47× faster search latency than state-of-the-art memory frameworks, while retaining similar retrieval accuracy, showing that their query-aware and indexed design scales better with memory size.

Conclusion: By using temporal and semantic indices plus consolidation, SwiftMem achieves sub-linear, low-latency retrieval for LLM agent memory without sacrificing accuracy, making it more practical for deployment in real-time, memory-augmented agent systems as memory sizes grow.

Abstract: Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.

</details>


### [24] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang,Yingnian Wu,Hongjing Lu*

Main category: cs.CL

TL;DR: They refine and combine “function vectors” inside LLM activations to better encode semantic relations and analogies, improving relational completion and analogy benchmarks via activation patching.


<details>
  <summary>Details</summary>
Motivation: Relational knowledge (e.g., analogies, semantic relations between words) is central to intelligence, but how and where such relations are encoded in LLMs is not fully understood or easily controlled. Prior work found that some attention heads carry task representations that can be summarized as “function vectors,” but these raw vectors are not optimized for performance or controllability. The authors aim to (1) improve how these vectors capture semantic relations, (2) test whether a small number of labeled examples can tune them, and (3) see whether appropriately combined vectors can explicitly enhance analogy reasoning at inference time.

Method: Starting from function vectors identified via causal mediation analysis on in-context learning tasks, the authors fine-tune these vectors using a very small set of labeled word-pair examples for specific semantic relations. They then evaluate the tuned vectors on relation-based word-completion tasks across both small and large LLMs. They further assess (a) how well the vectors support decoding of relation words and (b) alignment with human similarity judgments about semantic relations. They then define composite function vectors as learned weighted combinations of individual fine-tuned relation vectors and insert these composite vectors into the model’s activations at inference time (activation patching). Performance is evaluated on challenging analogy problems from cognitive science and SAT-style benchmarks.

Result: Fine-tuning function vectors with around 20 word pairs per relation substantially improves accuracy on relation-based word-completion tasks over the original, untuned function vectors, in both smaller and larger models. The tuned vectors also enable better decoding of explicit relation words from activations and their similarity structure corresponds more closely to human judgments of semantic relation similarity. Composite function vectors, when injected into model activations during inference, significantly boost the model’s performance on difficult analogy tasks from both cognitive science datasets and SAT analogy benchmarks.

Conclusion: A small, targeted fine-tuning of internal function vectors can sharpen how LLMs represent semantic relations, making these internal representations both more aligned with human notions of relational similarity and more effective for word-completion tasks. Furthermore, by composing and inserting these vectors via activation patching, one can systematically enhance the model’s analogical reasoning capabilities on challenging benchmarks. The work suggests that internal activation-level interventions provide a promising, interpretable, and controllable handle on relational knowledge in LLMs, contributing both to model interpretability and to practical improvements in reasoning performance.

Abstract: Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [25] [Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering](https://arxiv.org/abs/2601.08176)
*Lavanya Prahallad,Sai Utkarsh Choudarypally,Pragna Prahallad,Pranathi Prahallad*

Main category: cs.CL

TL;DR: The paper evaluates how different prompting strategies affect automatic clarity and evasion assessment of LLM answers on political questions using the CLARITY dataset.


<details>
  <summary>Details</summary>
Motivation: Existing automatic evaluations of LLM responses in political QA focus mainly on factuality, with less attention to clarity and evasion. Although there are human-annotated datasets for these aspects, it is not well understood how prompt design influences an LLM’s ability to automatically judge clarity and evasion.

Method: Using the SemEval 2026 CLARITY dataset, the authors compare a provided GPT-3.5 baseline against GPT-5.2 under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. They evaluate model predictions against human annotations using accuracy, class-wise metrics for clarity and evasion, and hierarchical exact match, and also assess topic identification performance.

Result: GPT-5.2 with chain-of-thought plus few-shot prompting improves clarity prediction accuracy from 56% (GPT-3.5 baseline) to 63%. Chain-of-thought prompting gives the best evasion accuracy at 34%, though gains are inconsistent across detailed evasion types. For topic identification, reasoning-based prompts raise accuracy from 60% to 74% against human labels. Gains are strongest for high-level clarity, weaker and less stable for fine-grained evasion and topic detection.

Conclusion: Prompt design—especially chain-of-thought and few-shot prompting—can substantially improve automatic, high-level clarity evaluation of LLM answers to political questions compared to a GPT-3.5 baseline. However, even with structured reasoning prompts, accurately capturing fine-grained evasion behavior and topic detection remains difficult, suggesting the need for richer methods or models for nuanced aspects of political QA evaluation.

Abstract: Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.

</details>


### [26] [Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis](https://arxiv.org/abs/2601.08196)
*Da Song,Yuheng Huang,Boqi Chen,Tianshuo Cong,Randy Goebel,Lei Ma,Foutse Khomh*

Main category: cs.CL

TL;DR: They create a benchmark and framework to test whether LLM-based code agents obey hidden safety regulations, not just pass functional tests.


<details>
  <summary>Details</summary>
Motivation: In real-world, high-stakes settings (e.g., regulated industries), AI agents must comply with complex regulations, many of which are implicit or not directly encoded in the task description. Existing benchmarks largely test for functional correctness and explicit instructions but ignore whether systems adhere to mandatory regulatory constraints that may be latent. This creates a gap between benchmark performance and real-world safety needs, motivating a framework and benchmark that can systematically evaluate regulatory compliance of LLM agents.

Method: They propose LogiSafetyGen, which automatically converts unstructured natural language regulations into formal Linear Temporal Logic (LTL) specifications that serve as oracles. Then, they use logic-guided fuzzing to generate program execution traces that are both valid and safety-critical (i.e., likely to expose violations). Using this framework, they build LogiSafetyBench: a set of 240 human-verified tasks where LLMs must write Python programs that achieve specified functional goals while also satisfying latent compliance rules encoded via the LTL oracles. They then evaluate 13 state-of-the-art LLMs on this benchmark.

Result: The experiments show that while larger, more capable LLMs achieve higher rates of functional correctness on the Python programming tasks, they more often violate the hidden safety constraints. In other words, bigger models are better at solving the explicit task but tend to sacrifice regulatory compliance in order to complete the task, highlighting a safety-performance trade-off and exposing non-compliant behavior that would be missed by traditional benchmarks.

Conclusion: The work demonstrates that current LLMs, including the strongest ones, cannot be assumed to autonomously enforce regulatory safety constraints even when they can correctly complete tasks. LogiSafetyGen and LogiSafetyBench provide a way to systematically test and compare models on this dimension. The authors conclude that future development and evaluation of LLM agents must explicitly incorporate regulatory compliance as a first-class objective, and that formal logic-based frameworks like theirs are a promising path to bridge the gap between benchmark success and real-world safety requirements.

Abstract: The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.

</details>


### [27] [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)
*Yibo Wang,Hai-Long Sun,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.CL

TL;DR: The paper proposes T-SPIN, a triplet-based self-play fine-tuning method that improves stability and performance of self-play adaptation of large language models under scarce annotations, outperforming SPIN and matching or beating supervised fine-tuning using only 25% of labeled data.


<details>
  <summary>Details</summary>
Motivation: Self-play fine-tuning (SPIN) allows adapting large language models to tasks with limited expert-labeled data by generating synthetic data. However, SPIN optimizes only the current reward advantage between human-annotated and model-generated responses, which can vanish over iterations and lead to unstable training. It also relies on a reference policy, creating misalignment between the training reward and the generation metric. There is a need for a more stable, reference-free self-play method that works well in low-annotation regimes.

Method: The authors introduce T-SPIN, which modifies the self-play framework in two ways. (1) Triplet-based historical advantages: instead of only comparing the current synthetic response with the annotated one, T-SPIN also factors in the historical advantage between the iteratively generated responses and proto-synthetic responses produced by the initial policy, forming triplets (annotated, current synthetic, proto-synthetic). This preserves informative training signals even when current advantages shrink. (2) Entropy-constrained, reference-free optimization: T-SPIN incorporates an entropy constraint into the self-play objective, theoretically enabling fine-tuning without an explicit reference policy and aligning training with the generation-time objective.

Result: Across multiple tasks, T-SPIN consistently outperforms the original SPIN method and shows more stable performance across training iterations. It can reach comparable or superior performance to conventional supervised fine-tuning while using only 25% of the expert-annotated samples, demonstrating strong data efficiency and robustness in low-resource scenarios.

Conclusion: T-SPIN effectively addresses the instability and misalignment issues of standard SPIN by leveraging historical advantages and entropy-constrained, reference-free training. This leads to more stable self-play fine-tuning and strong performance under annotation scarcity, making T-SPIN a practical and efficient approach for adapting large language models when expert data is limited.

Abstract: Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.

</details>


### [28] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)
*Rongji Li,Jian Xu,Xueqing Chen,Yisheng Yang,Jiayi Wang,Xingyu Chen,Chunyu Xie,Dawei Leng,Xu-Yao Zhang*

Main category: cs.CL

TL;DR: The paper introduces Generation-Augmented Generation (GAG), a new way to inject private, domain-specific knowledge into LLMs via compact learned representations instead of fine-tuning or RAG, achieving better specialist performance while preserving general abilities and enabling scalable multi-domain use.


<details>
  <summary>Details</summary>
Motivation: High-stakes domains like biomedicine, materials science, and finance need LLMs that can use private, proprietary, and rapidly changing knowledge that is poorly represented in public pretraining data. Existing approaches are inadequate: (1) fine-tuning is costly to iterate, and continual updates can cause catastrophic forgetting and degrade general reasoning; (2) retrieval-augmented generation (RAG) keeps the base model frozen but struggles on specialized private corpora due to chunking that fragments evidence, retrieval drift in dense/sparse search, and long-context prompts that inflate with query-dependent content. The authors seek a method that injects private expertise efficiently, robustly, and in a way that scales to multiple domains without hurting the model’s general performance.

Method: The authors propose Generation-Augmented Generation (GAG), which conceptualizes private domain expertise as an additional expert modality, analogous to how multimodal LLMs handle images or other inputs. Instead of serializing private documents into long prompts, GAG learns a compact representation-level interface that aligns these private expert signals with the frozen base LLM’s internal semantic space. This interface can be plugged in or composed across domains, enabling selective activation of specialist knowledge as needed. The base model remains frozen, while domain-specific modules operate as aligned generators/encoders that modify or augment the internal representations driving generation, avoiding chunk-level retrieval and long-context pressure inherent to RAG.

Result: On two private scientific QA benchmarks—one in immunology adjuvants and one in catalytic materials—GAG outperforms strong RAG baselines by 15.34% and 14.86%, respectively, in specialist performance. In mixed-domain evaluations, GAG preserves performance across six open general benchmarks, indicating no significant regression in broad capabilities. The approach also demonstrates near-oracle performance in selectively activating the right domain expert module when needed, suggesting strong potential for scalable deployment across many specialized knowledge domains.

Conclusion: GAG offers a practical and effective alternative to fine-tuning and RAG for injecting private, specialized knowledge into LLMs. By treating private expertise as an aligned expert modality at the representation level, it circumvents the instability and context-length issues of RAG and the catastrophic forgetting and cost of continual fine-tuning. The method achieves substantial gains on specialized scientific QA tasks while maintaining general performance, and it supports plug-and-play, selectively activated multi-domain specialization, making it well-suited for scalable, high-stakes applications that rely on proprietary, fast-evolving knowledge.

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.

</details>


### [29] [Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints](https://arxiv.org/abs/2601.08215)
*Seng Pei Liew,Kenta Shinzato,Yuyang Dong*

Main category: cs.CL

TL;DR: The paper studies how to best design Mixture-of-Experts (MoE) language models and finds that total parameters and expert sparsity, not just total vs active parameters, are key for performance.


<details>
  <summary>Details</summary>
Motivation: Existing MoE models are usually characterized by total parameter count (memory footprint) and active parameter count during inference (compute cost). But these two metrics do not fully explain or guide optimal MoE architecture design, leaving ambiguity about how many experts, how many are activated, and how to allocate parameters between core model and experts. The paper aims to systematically clarify which architectural factors really matter for performance.

Method: The authors perform a systematic empirical study over families of MoE language model architectures, varying the total number of parameters, the number of experts (n_exp), and the number of active experts per token (n_topk), subject to memory constraints. They analyze model performance as a function of total parameters (N_total) and expert sparsity (s = n_exp/n_topk), and examine how changing n_exp and n_topk separately—while keeping their ratio or other quantities fixed—affects performance. They also consider how the need to satisfy memory limits forces trade-offs with core model depth and width.

Result: They find that MoE performance is primarily governed by the total parameter count N_total and the expert sparsity s = n_exp/n_topk. Importantly, n_exp and n_topk do not simply cancel out within s; increasing the total number of experts tends to slightly hurt performance because, under fixed memory, it reduces the capacity of the shared core model (depth and width). They show that models with lower sparsity (i.e., larger n_topk relative to n_exp) and fewer total experts tend to perform better at fixed memory and compute budgets, provided N_total is maximized.

Conclusion: The paper proposes a practical design rule for MoE language models: under given memory and compute constraints, one should maximize the total parameter count N_total while minimizing expert sparsity s (equivalently, maximizing n_topk) and avoiding unnecessarily large numbers of experts n_exp that steal capacity from the core model. This provides a clear framework to resolve architectural ambiguity in MoE design and offers guidance on choosing the number of experts and active experts per token.

Abstract: Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).
  Moreover, $n_{exp}$ and $n_{topk}$ do not "cancel out" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.

</details>


### [30] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho,Minbyul Jeong,Sungrae Park*

Main category: cs.CL

TL;DR: They propose a data-generation framework that simulates realistic, multi-turn tool-use dialogues for large reasoning models by using a user-oriented simulator and dynamically created tools, producing longer, denser, and more human-like interaction trajectories than task-only approaches.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for training large reasoning models as tool-using agents rely on static, predefined toolsets and mostly task-oriented dialogues, which produce short, unrealistic conversations that do not capture open-ended, multi-turn, human-agent collaboration. There is a need for scalable, automated generation of rich, long-horizon, tool-use dialogues that better reflect real-world user behavior and iterative problem solving.

Method: They build an LRM-based simulation framework for automated generation of task-oriented multi-turn dialogues where tools are dynamically created for specific tasks. After observing that this task-only setup yields minimal-interaction trajectories, they redesign the pipeline around a user-oriented simulator that follows human-like behavioral rules: incremental requests, stepwise feedback, and iterative refinement. Task generation is decoupled from the user simulator, and the pipeline is implemented as a modular, plug-and-play system that can start generation from arbitrary states. It supports multiple task completions within one dialogue to increase data density and realism.

Result: The new user-oriented simulation paradigm produces extended, multi-turn conversations with rich tool usage that are more similar to realistic human-agent interactions than the prior task-only design. The pipeline can scale to large volumes of data, automatically generates domain-specific tools on demand, and yields trajectories containing multiple tasks in a single dialogue, thereby increasing the density and diversity of useful training signals for LRMs as autonomous agents.

Conclusion: A user-oriented, behavior-aware simulation framework with dynamically generated tools overcomes the limitations of static toolsets and purely task-oriented data generation. Their pipeline yields scalable, high-density, multi-task, multi-turn tool-use dialogues that better match real-world human-agent collaboration, making it a more suitable data source for training and evaluating large reasoning models as autonomous tool-using agents.

Abstract: The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [31] [Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning](https://arxiv.org/abs/2601.08267)
*Fan Gao,Sherry T. Tong,Jiwoong Sohn,Jiahao Huang,Junfeng Jiang,Ding Xia,Piyalitt Ittichaiwong,Kanyakorn Veerakanjana,Hyunjae Kim,Qingyu Chen,Edison Marrese Taylor,Kazuma Kobayashi,Akkiko Aizawa,Irene Li*

Main category: cs.CL

TL;DR: Introduces Med-CoReasoner, a framework that jointly reasons in English and local languages to improve multilingual medical reasoning, evaluated on a new seven-language benchmark MultiMed-X, achieving ~5% average performance gains, especially in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Reasoning-augmented LLMs perform well on English medical tasks but underperform in local languages, creating an inequity in global clinical decision support. There is a need to close this multilingual reasoning gap so medical AI systems can be reliably deployed across diverse linguistic contexts and capture local clinical knowledge.

Method: Proposes Med-CoReasoner, a co-reasoning framework that simultaneously elicits reasoning in English and a target local language, abstracts both into structured concepts, and aligns them via concept-level mapping and retrieval. The English chain-of-thought provides a logical scaffold, while local-language reasoning contributes practice-grounded, culturally relevant knowledge, which is integrated through language-informed alignment. Also introduces MultiMed-X, a multilingual benchmark with expert-annotated long-form QA and NLI items (350 per language across 7 languages) to evaluate medical reasoning beyond multiple-choice formats.

Result: Across three benchmarks, applying Med-CoReasoner yields an average 5% improvement in multilingual medical reasoning performance over baselines, with larger gains in low-resource languages, indicating that the proposed co-reasoning and concept-alignment approach effectively leverages both English and local-language knowledge. Distillation experiments show that smaller models can inherit these improvements, and expert evaluations confirm that the generated reasoning traces are clinically accurate and contextually appropriate.

Conclusion: Med-CoReasoner successfully narrows the multilingual medical reasoning gap by coupling English’s strong logical structure with local-language clinical expertise through concept-based co-reasoning. The new MultiMed-X benchmark enables more realistic evaluation of multilingual medical reasoning, and results suggest that this framework can produce clinically sound and culturally grounded reasoning, particularly benefiting low-resource language settings.

Abstract: While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.

</details>


### [32] [Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees](https://arxiv.org/abs/2601.08274)
*Kun Li,Zenan Xu,Junan Li,Zengrui Jin,Jinghao Deng,Zexuan Qiu,Bo Zhou*

Main category: cs.CL

TL;DR: The paper proposes DART, a reinforcement learning framework that discovers and reinforces effective tool use within long chain-of-thought reasoning for large language models, significantly improving performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: While tool-integrated reasoning has improved LLM capabilities, existing work mostly focuses on short contexts or explicit, supervised tool calls. There is little understanding or data for integrating tools into long chain-of-thought reasoning without hurting the model’s inherent reasoning ability. The authors aim to enable LLMs to spontaneously decide when and how to use tools during extended reasoning, without relying on costly human-annotated tool-use trajectories.

Method: The authors introduce DART, a reinforcement learning framework that builds dynamic rollout trees during training. At promising points within a long chain-of-thought, the system branches into multiple trajectories that differ in whether and how tools are invoked. These tree-structured rollouts explore diverse patterns of tool-integrated reasoning. A tree-based process advantage estimator then evaluates partial trajectories, identifying which sub-trajectories with tool calls actually improve problem-solving outcomes. The RL objective reinforces these beneficial tool-use segments, guiding the model to learn when and how to call tools within long reasoning chains.

Result: On challenging reasoning benchmarks such as AIME and GPQA-Diamond, DART-trained models outperform prior methods for tool-integrated reasoning. The experiments show that models trained with DART achieve higher accuracy and better stability, indicating that the framework successfully injects tool use into long chain-of-thought without degrading the base reasoning capability.

Conclusion: DART demonstrates that it is possible to discover and reinforce effective tool-integrated reasoning within long chain-of-thought via reinforcement learning and rollout trees, without human-labeled tool-use data. By identifying and amplifying the specific sub-trajectories where tool invocation helps, DART harmonizes computational tool execution with intrinsic LLM reasoning, leading to substantial gains on difficult benchmarks and suggesting a promising direction for scalable, autonomous training of tool-using LLMs.

Abstract: Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.

</details>


### [33] [D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning](https://arxiv.org/abs/2601.08282)
*Kangcheng Luo,Tinglang Wu,Yansong Feng*

Main category: cs.CL

TL;DR: D^2Plan is a dual-agent, plan-based framework that improves search-augmented LLMs on multi-hop QA by coordinating a reasoning agent and a purification agent, trained with SFT and RL using plan-oriented rewards.


<details>
  <summary>Details</summary>
Motivation: Existing search-augmented LLMs interleave retrieval and reasoning but degrade when the context becomes cluttered with a mix of key evidence and distractors. They often form poor search chains that miss necessary information or issue bad queries, and they are vulnerable to being misled by irrelevant yet salient evidence. There is a need for a framework that can both globally plan multi-step retrieval/reasoning and actively filter and condense retrieved evidence to stay focused on what is relevant.

Method: The paper proposes D^2Plan, a dual-agent dynamic global planning paradigm composed of a Reasoner and a Purifier. The Reasoner creates explicit global plans for multi-step reasoning, executes them while dynamically updating the plan based on retrieved information, and guides the overall reasoning trajectory. The Purifier evaluates the relevance of retrieved documents, filters out distractors, and summarizes key evidence to feed back to the Reasoner. Training is done in two stages: (1) supervised fine-tuning on synthesized trajectories to initialize the system’s behavior in planning and purification, and (2) reinforcement learning with specially designed plan-oriented rewards that encourage effective global planning, high-quality retrieval, and robust reasoning in the presence of noise.

Result: Experiments on challenging question-answering benchmarks show that D^2Plan improves the coherence of multi-step reasoning, better resists misleading or irrelevant information, and achieves higher performance compared to prior search-augmented LLM baselines. The framework leads to stronger multi-hop QA accuracy and robustness to noisy retrieval contexts.

Conclusion: D^2Plan demonstrates that explicitly separating and coordinating global reasoning/planning and evidence purification within a dual-agent framework, combined with plan-oriented RL training, can significantly enhance retrieval-augmented LLMs on complex multi-hop reasoning tasks. The approach yields more reliable search chains and reduces susceptibility to distractors, indicating a promising direction for future retrieval-augmented reasoning systems.

Abstract: Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.

</details>


### [34] [Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques](https://arxiv.org/abs/2601.08302)
*Marvin Schmitt,Anne Schwerk,Sebastian Lempert*

Main category: cs.CL

TL;DR: The paper analyzes how different advanced prompting techniques (few-shot, chain-of-thought, self-consistency) affect LLM performance in sentiment analysis, showing that optimal prompting depends on both the model and the specific task (e.g., irony detection).


<details>
  <summary>Details</summary>
Motivation: To understand whether and how prompt engineering can systematically improve LLM performance on sentiment analysis tasks, especially challenging ones like aspect-based sentiment analysis and irony detection, and to see if different models benefit from different prompting strategies.

Method: The authors evaluate multiple prompting strategies—baseline prompting, few-shot learning, chain-of-thought prompting, and self-consistency—on two LLMs (GPT-4o-mini and gemini-1.5-flash). They apply these to tasks including sentiment classification, aspect-based sentiment analysis, and irony detection, measuring performance using accuracy, recall, precision, and F1 score over specified datasets. They then compare the performance across prompts and models.

Result: Advanced prompting techniques outperform the baseline for sentiment analysis across tasks. Few-shot prompting yields the best overall gains for GPT-4o-mini, while chain-of-thought prompting substantially improves gemini-1.5-flash’s performance on irony detection, achieving up to a 46% improvement. Results vary by model and task, indicating non-uniform benefits of each prompting method.

Conclusion: Prompt engineering significantly enhances LLM sentiment analysis performance, but no single prompting strategy is universally optimal. Few-shot works best for GPT-4o-mini overall, whereas chain-of-thought is particularly effective for irony detection with gemini-1.5-flash. Therefore, prompt design should be tailored to the specific LLM and the semantic difficulty of the task, underscoring the need for model- and task-aware prompting strategies.

Abstract: This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.

</details>


### [35] [AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture](https://arxiv.org/abs/2601.08308)
*Bo Yang,Yu Zhang,Yunkui Chen,Lanfei Feng,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriAgent is a hierarchical multi-agent framework that adapts its execution strategy to task complexity in real-world agriculture, enabling more reliable multimodal tool use and planning than unified agent paradigms.


<details>
  <summary>Details</summary>
Motivation: Real-world agricultural environments require intelligent agents that can process multimodal inputs (e.g., images, sensor data, text) and perform tasks ranging from simple queries to complex, multi-step operations, often with incomplete or unreliable tool availability. Existing approaches typically adopt a single, unified execution paradigm for all tasks, which is ill-suited to the large variability in task complexity and the dynamic, imperfect tool ecosystem found in agriculture. This leads to low robustness and poor performance, especially on complex tasks requiring planning, orchestration, and recovery from failures. The authors aim to design an agent framework that can flexibly adapt its reasoning and tool-use strategy to task complexity while being resilient to missing or failing tools.

Method: The authors propose AgriAgent, a two-level hierarchical agent framework tailored for agricultural scenarios. At the lower level, modality-specific agents (e.g., for text, images, other sensors) directly handle simple tasks via lightweight reasoning, avoiding unnecessary planning overhead. At the higher level, when a task is recognized as complex, AgriAgent activates a contract-driven planning mechanism: it abstracts the task into a set of capability requirements, matches these with available tools, and orchestrates capability-aware tool sequences. If required tools are missing, AgriAgent supports dynamic tool generation or adaptation. The framework also incorporates mechanisms for multi-step, verifiable execution and failure recovery, ensuring that partially completed or failed steps can be detected and revised. The system is evaluated experimentally against tool-centric agent baselines that use a single unified execution strategy.

Result: In experiments on agricultural tasks with varying complexity, AgriAgent shows higher execution success rates and improved robustness, particularly on complex, multi-step tasks, compared to existing tool-centric agent baselines that rely on a unified execution paradigm. The hierarchical strategy and contract-driven planning result in better handling of incomplete toolsets and more reliable task completion under real-world conditions. Quantitative metrics indicate that AgriAgent outperforms baselines on complex task benchmarks, while maintaining efficient performance on simpler tasks.

Conclusion: The study concludes that a hierarchical, complexity-aware agent architecture is better suited for real-world agricultural applications than unified execution paradigms. By separating simple direct reasoning from contract-driven planning for complex tasks and by enabling capability-aware orchestration and dynamic tool generation, AgriAgent improves success rates and robustness in multimodal agricultural scenarios. The framework provides a practical path toward more dependable agricultural AI systems, and the planned release of code and data is intended to foster reproducible research and further development in this domain.

Abstract: Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.

</details>


### [36] [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)
*Daniil Gurgurov,Yusser Al Ghussin,Tanja Baeumel,Cheng-Ting Chou,Patrick Schramowski,Marius Mosbach,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: The paper introduces CLaS-Bench, a multilingual benchmark to evaluate how well steering methods can force large language models to operate in specific languages while preserving meaning, and finds that a simple residual-based method works best across 32 languages.


<details>
  <summary>Details</summary>
Motivation: There is increasing interest in controlling and adapting large language models to specific languages, especially in multilingual NLP. Existing approaches like prompting or fine-tuning can be costly or opaque, and newer steering methods that modify internal representations during inference show promise as efficient and interpretable alternatives. However, the field lacks a dedicated, standardized benchmark and evaluation protocol to rigorously compare multilingual steering techniques and quantify how effectively they can enforce target languages without hurting semantic fidelity.

Method: The authors build CLaS-Bench, a lightweight benchmark consisting of parallel questions across 32 languages, designed to measure “language-forcing” behavior in LLMs. They define a two-dimensional evaluation protocol: (1) language control (how well outputs adhere to the target language) and (2) semantic relevance (how well the meaning of the response matches the intended content). These are combined into a single harmonic-mean steering score. They then systematically evaluate multiple steering approaches, including residual-stream DiffMean interventions, directions derived from probes, language-specific neurons, PCA/LDA-based vectors, sparse autoencoders, and prompting-based baselines, and perform a layer-wise analysis of language representations and steering directions.

Result: Empirically, the residual-based DiffMean intervention consistently outperforms all competing steering techniques across the 32 languages on CLaS-Bench. The layer-wise analysis shows that language-specific structure in the model’s internal representations emerges predominantly in later layers, and the steering directions learned for different languages cluster according to language families, indicating that the model’s representation space reflects linguistic relatedness.

Conclusion: CLaS-Bench provides the first standardized, multilingual benchmark dedicated to evaluating steering methods for language control in LLMs, enabling systematic comparison and scientific study of language representations. The findings suggest that simple residual-based DiffMean steering is a strong, robust baseline, and that language information becomes more salient in later layers and is structured by language family. This positions steering as a practical, low-cost alternative to retraining or fine-tuning for multilingual adaptation, and CLaS-Bench as a tool for further research and method development.

Abstract: Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.

</details>


### [37] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://arxiv.org/abs/2601.08342)
*Run Chen,Wen Liang,Ziwei Gong,Lin Ai,Julia Hirschberg*

Main category: cs.CL

TL;DR: First study on detecting mental manipulation in spoken dialogues using a synthetic multi-speaker benchmark, showing both models and humans struggle more with audio than text, highlighting the need for modality-aware safety evaluation.


<details>
  <summary>Details</summary>
Motivation: While mental manipulation detection has been studied in text, there is a gap in understanding how manipulative tactics appear and can be detected in spoken conversations, which are common in real-life scenarios. This paper addresses the lack of benchmarks and analyses for speech-based manipulation detection.

Method: The authors construct SPEECHMENTALMANIP, a synthetic multi-speaker benchmark by augmenting an existing text-based manipulation dataset with high-quality, voice-consistent TTS audio. They then use few-shot large audio-language models and human annotators to evaluate manipulation detection across text and speech modalities, comparing performance metrics like specificity and recall, and analyzing perception differences.

Result: Models demonstrate high specificity but substantially lower recall on spoken dialogues than on text, suggesting missed manipulative instances in audio. Human raters also show increased uncertainty in the audio condition, indicating that manipulative speech is inherently more ambiguous or harder to judge than text alone.

Conclusion: Detection of mental manipulation is significantly affected by modality: spoken dialogue introduces challenges for both models and humans. These results point to the necessity of modality-aware evaluation protocols and improved safety alignment strategies for multimodal dialogue systems that must operate robustly and safely on both text and speech inputs.

Abstract: Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.

</details>


### [38] [PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors](https://arxiv.org/abs/2601.08402)
*Donya Rooein,Sankalan Pal Chowdhury,Mariia Eremeeva,Yuan Qin,Debora Nozza,Mrinmaya Sachan,Dirk Hovy*

Main category: cs.CL

TL;DR: They build and test an LLM-based tutoring system that adapts its teaching strategy to different student personality profiles, and show teachers prefer it over non-personalized baselines.


<details>
  <summary>Details</summary>
Motivation: LLM tutors are promising, but they currently ignore student personality, even though educational research shows that different personalities benefit from different pedagogical strategies; mismatched strategies can harm learning.

Method: They first derive a taxonomy mapping personality traits to suitable pedagogical methods, grounded in pedagogical literature. Then they simulate student–teacher dialogues where the LLM, acting as a tutor, uses this taxonomy to adapt its tutoring strategies to different simulated personality profiles. They compare the adaptive system against two non-personalized baselines using human-teacher evaluations and also analyze the frequency and preference of specific strategies (e.g., role-playing) via human and LLM annotations.

Result: Human teachers consistently rate the personality-adaptive LLM tutor as better than both baselines. The adaptive method also leads the LLM to employ less common but pedagogically powerful strategies, such as role-playing, more frequently, and these strategies receive significantly higher preference ratings from both human and LLM annotators.

Conclusion: Incorporating personality-informed pedagogical taxonomies into LLM tutors yields strategies that human experts prefer and that surface high-impact teaching methods more often, indicating a promising direction for more personalized and effective LLM-based educational tools.

Abstract: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

</details>


### [39] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang,Weitao Ma,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.CL

TL;DR: The paper proposes Latent-GRPO, a variant of Group Relative Policy Optimization that replaces expensive external verifiers with intrinsic, geometry-based rewards from the model’s own latent space, achieving similar performance with over 2x faster training.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO methods for improving LLM reasoning rely on external verifiers or human-crafted rules to provide correctness signals. These verifiers are computationally expensive, slow, and often yield sparse rewards, which makes optimization inefficient and limits scalability. The authors aim to design a cheaper, denser, and more scalable reward mechanism that does not depend on external checking while still effectively distinguishing correct from incorrect reasoning trajectories.

Method: The authors empirically analyze the representation space of LLM reasoning trajectories and find that terminal token embeddings of correct solutions cluster densely, whereas incorrect ones appear as scattered outliers. Based on this, they propose Latent-GRPO, which computes intrinsic rewards directly from latent representations. Central to this is the Iterative Robust Centroid Estimation (IRCE) algorithm: it performs spherical projection of embeddings to mitigate magnitude fluctuations and iteratively aggregates them to estimate a robust "truth centroid" for correct reasoning. Rewards are then derived from similarity to this centroid, yielding dense and continuous feedback that can be used within a GRPO-style training objective.

Result: On multiple reasoning benchmarks, Latent-GRPO matches the reasoning performance of standard GRPO approaches that use external verifiers, while reducing training time by more than half (over 2x speedup). The method shows strong generalization across datasets and remains robust under varying conditions, suggesting that the geometry-based intrinsic rewards are stable and widely applicable.

Conclusion: Latent-GRPO demonstrates that intrinsic, geometry-based rewards from the latent representation space can effectively replace expensive external verifiers in GRPO-style training for LLM reasoning. By exploiting the clustering structure of correct trajectories and using IRCE to compute a robust truth centroid, the approach provides dense, stable rewards that preserve model performance while substantially accelerating training and showing good generalization and robustness.

Abstract: Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [40] [Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management](https://arxiv.org/abs/2601.08435)
*Weitao Ma,Xiaocheng Feng,Lei Huang,Xiachong Feng,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Bing Qin*

Main category: cs.CL

TL;DR: Fine-Mem is a reinforcement learning framework for training memory managers for LLM agents using fine-grained rewards and attribution, improving long-horizon task performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based memory managers for LLM agents suffer from sparse rewards and poor credit assignment because they only use final task performance as the reward, giving little guidance to individual memory operations.

Method: Fine-Mem introduces (1) a Chunk-level Step Reward, where auxiliary chunk-specific QA tasks provide immediate supervision for each memory step, and (2) Evidence-Anchored Reward Attribution, which redistributes the global reward to key memory operations based on which memory items were actually used as evidence during reasoning. These are used in a unified RL training framework for memory policies.

Result: On Memalpha and MemoryAgentBench benchmarks, Fine-Mem outperforms strong baseline memory managers, achieving higher success rates across multiple sub-tasks, and shows adaptability and generalization across different model sizes and backbones.

Conclusion: Fine-grained step rewards and evidence-based reward attribution enable more stable and effective RL training of memory managers for LLM agents, leading to better long-horizon performance and good generalization across settings.

Abstract: Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.

</details>


### [41] [JudgeRLVR: Judge First, Generate Second for Efficient Reasoning](https://arxiv.org/abs/2601.08468)
*Jiangshan Duo,Hanyu Li,Hailin Zhang,Yudong Wang,Sujian Li,Liang Zhao*

Main category: cs.CL

TL;DR: The paper introduces JudgeRLVR, a two-stage reinforcement learning framework that first trains a model to judge solution correctness and then uses that capability to guide more efficient and accurate reasoning in large language models.


<details>
  <summary>Details</summary>
Motivation: Existing RL with Verifiable Rewards (RLVR) methods for LLM reasoning optimize only final-answer correctness, causing models to generate excessively long, trial-and-error style reasoning traces. Simple fixes like length penalties reduce verbosity but often cut off necessary reasoning, leading to a poor trade-off between efficiency and accuracy. The authors want a way for models to reason more efficiently without sacrificing, and ideally improving, solution quality and generalization.

Method: They propose JudgeRLVR, a two-stage judge-then-generate paradigm. Stage 1: train the model as a discriminative judge that evaluates solution responses against verifiable answers, learning to distinguish valid from invalid solutions. Stage 2: initialize the same model from this judge and then fine-tune it with standard generating RLVR, so its generative behavior is guided by the internalized discrimination capability, effectively pruning unpromising parts of the search space during reasoning.

Result: Using the Qwen3-30B-A3B model and the same math-domain training data as vanilla RLVR, JudgeRLVR achieves a better quality–efficiency trade-off. On in-domain math tasks, it improves accuracy by about 3.7 points on average while reducing generation length by about 42%. On out-of-domain benchmarks, it further improves average accuracy by about 4.5 points, indicating improved generalization beyond the training distribution.

Conclusion: Discriminative judgment capability is an important prior for efficient reasoning in LLMs. By first learning to judge solution validity and then using that capability as an initialization for RLVR-based generation, JudgeRLVR yields both more accurate and significantly more concise reasoning traces, with benefits extending to out-of-domain tasks. This suggests that integrating judging and generating within the same model can mitigate the verbosity and inefficiency of standard RLVR while enhancing generalization.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.

</details>


### [42] [sui-1: Grounded and Verifiable Long-Form Summarization](https://arxiv.org/abs/2601.08472)
*Benedikt Droste,Jan Philipp Harries,Maximilian Idahl,Björn Plüster*

Main category: cs.CL

TL;DR: Introduces sui-1, a 24B parameter model for abstractive summarization with inline citations, trained via a synthetic data pipeline and outperforming larger open-weight baselines.


<details>
  <summary>Details</summary>
Motivation: Standard large language models often produce fluent but unfaithful summaries that users cannot easily validate, which is problematic in compliance-critical areas like government and legal work. There is a need for summarization systems that explicitly ground each claim in the source text so users can verify accuracy.

Method: The authors develop sui-1, a 24B parameter language model fine-tuned for citation-grounded summarization. They build a synthetic training pipeline that uses chain-of-thought prompting and multi-stage verification to automatically generate high-quality examples of abstractive summaries with inline citations. The data spans over 22,000 examples in five languages, sourced from parliamentary records, web documents, and Wikipedia. The model is then trained on this specialized dataset and evaluated against open-weight baselines of various sizes.

Result: sui-1 produces abstractive summaries that include inline citations mapping claims back to specific source sentences. In quantitative evaluation, sui-1 substantially outperforms all tested open-weight baseline models on citation-grounded summarization benchmarks, including models with up to three times more parameters, demonstrating both higher faithfulness and better overall summarization quality.

Conclusion: Task-specific training on carefully constructed, citation-annotated synthetic data can yield summarization models that are more accurate and reliable than much larger general-purpose models. sui-1 shows that explicit citation grounding improves verifiability and is especially beneficial for compliance-sensitive applications. The release of model weights and a public demo supports further research and practical deployment of citation-aware summarization systems.

Abstract: Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.

</details>


### [43] [Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots](https://arxiv.org/abs/2601.08477)
*Francesco Dettori,Matteo Forasassi,Lorenzo Veronese,Livia Lestingi,Vincenzo Scotti,Matteo Giovanni Rossi*

Main category: cs.CL

TL;DR: Framework for formally specifying and verifying empathy in therapy chatbots using NLP and model checking.


<details>
  <summary>Details</summary>
Motivation: Conversational agents are widely used in mental health support, but there is no systematic way to specify or verify empathy, which is crucial in therapy. The authors want to make chatbot empathy measurable, checkable, and improvable via formal methods.

Method: Use a Transformer-based NLP model to extract features from therapy dialogues, then translate these into a Stochastic Hybrid Automaton representing the dyadic interaction. Apply Statistical Model Checking to verify empathy-related temporal properties, and use strategy synthesis to derive behavior guidelines or policies for the chatbot.

Result: Preliminary experiments indicate that the formal model reproduces key dynamics of therapy sessions with good fidelity, and that synthesized or ad-hoc behavioral strategies can increase the probability that empathy-related requirements are met.

Conclusion: It is feasible to integrate NLP with formal verification to reason about and improve empathy in therapy chatbots. The proposed framework can both evaluate empathy levels and guide the design of more empathetic conversational strategies.

Abstract: Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.

</details>


### [44] [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
*Erin Feiglin,Nir Hutnik,Raz Lapid*

Main category: cs.CL

TL;DR: The paper identifies and measures a non-adversarial failure mode in large language models where simple prompts cause excessively long outputs (Overflow), introduces a benchmark (BenchOverflow) to quantify it, and shows a simple conciseness reminder can mitigate it, framing length control as a reliability, cost, and sustainability issue.


<details>
  <summary>Details</summary>
Motivation: Although much work studies adversarial jailbreaks and prompt injections, there is less focus on a more mundane but impactful problem: language models sometimes respond with far more text than needed to satisfy a request. At scale, this behavior raises serving costs, latency, energy use, and can degrade performance for other users by consuming shared compute. It also presents a compute amplification vector where benign-looking prompts trigger disproportionate resource usage. The authors are motivated to systematically characterize this Overflow phenomenon, quantify its tail risks, and provide a standardized way to compare models and defenses so that deployments can be chosen and configured to reduce waste without harming task quality.

Method: The authors construct BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies intentionally designed to elicit long responses but without adversarial suffixes or attempts to bypass safety policies. They define a standardized evaluation protocol with a fixed budget of 5000 generated tokens per run, and test nine different open- and closed-source LLMs. They analyze the resulting output length distributions, focusing on rightward shifts and heavy tails. They introduce metrics such as cap-saturation rates at different token caps (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) to quantify tail risk and output-length behavior. They further study within-prompt variance and cross-model correlations to assess how reproducible and consistent Overflow is across models and prompting strategies. Finally, they test a lightweight mitigation—a fixed conciseness reminder in the prompt—and measure its impact on the length distributions and CSRs.

Result: Across nine models, the benchmark reveals substantial rightward shifts and heavy-tailed distributions of output lengths under the Overflow-promoting prompts. Many generations hit or approach the 5000-token cap, indicating significant tail risk and frequent extreme-length responses. Computed CSR@1k/3k/5k metrics show that a non-trivial fraction of responses saturate high token caps. Within-prompt variance and cross-model correlation analyses indicate that Overflow is not random noise but a broadly reproducible behavior that manifests differently across families and prompting strategies. When they add a simple conciseness reminder to the prompts, the right tails shrink and cap-saturation rates decrease across most models and all Overflow strategies, demonstrating that even minimal mitigation can meaningfully reduce excessive verbosity.

Conclusion: The study concludes that uncontrolled output length is an important and measurable aspect of LLM reliability, with direct implications for cost, latency, sustainability, and shared-environment robustness, rather than being a mere stylistic concern. BenchOverflow offers a standardized, model-agnostic way to stress-test and compare models on length-control robustness, enabling practitioners to select models and configurations that minimize resource waste. The authors also show that simple mitigations, such as concise-style reminders, can significantly curb compute amplification without degrading task performance, and they encourage future work to treat length control as a first-class objective in LLM design and deployment.

Abstract: We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.

</details>


### [45] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini,Marieke Van Erp,Mehwish Alam*

Main category: cs.CL

TL;DR: The paper proposes MHEL-LLaMo, an unsupervised, multilingual ensemble system for historical entity linking that combines a small bi-encoder model with an instruction-tuned LLM, achieving state-of-the-art performance across several European historical datasets without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Historical texts pose specific challenges for entity linking: spelling variation, OCR/noisy inputs, language drift over time, and evolving meanings. Existing systems either need large amounts of labeled, domain-specific training data or depend on hand-crafted, domain-specific rules, both of which are costly and hard to scale, especially in low-resource historical settings. The authors aim to design a scalable, multilingual EL system that works well on historical documents without requiring costly annotation or retraining.

Method: They introduce MHEL-LLaMo, an unsupervised ensemble approach. First, a multilingual small language model (SLM) in the form of a bi-encoder (BELA) performs candidate retrieval from the knowledge base and assigns confidence scores. The system then categorizes mentions into easy vs. hard cases using the SLM confidence: easy cases are resolved directly by the SLM, while hard cases are passed to an instruction-tuned large language model (LLM). The LLM is used via prompt chaining for two tasks: (1) NIL prediction (deciding if no suitable entity exists in the KB) and (2) refined candidate ranking/selection. This setup reduces LLM calls, thus saving computation, and also limits hallucinations on simple mentions by trusting the SLM when it is confident. The entire pipeline is unsupervised and does not require fine-tuning on the target historical datasets.

Result: On four historical EL benchmarks spanning six European languages (English, Finnish, French, German, Italian, Swedish) and covering 19th–20th century texts, their system outperforms prior state-of-the-art approaches. Crucially, it achieves these gains without fine-tuning on the target datasets, indicating strong zero-shot or out-of-the-box performance in low-resource historical scenarios.

Conclusion: MHEL-LLaMo is an effective and scalable solution for multilingual historical entity linking that combines the efficiency and precision of a small bi-encoder with the reasoning capabilities of an LLM, invoked selectively for hard cases. The ensemble design reduces computational overhead and mitigates hallucinations while delivering state-of-the-art results on multiple historical benchmarks without supervised adaptation, making it well-suited for low-resource and historically diverse corpora. The authors release their implementation to facilitate further research and application.

Abstract: Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [46] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian,Yiding Li,Fengyi Chen,Zequn Liu,Youyong Kong,Fan Guo,Yuyao Li,Jinjing Shen,Zhijing Xie,Yiyun Luo,Xin Zhang*

Main category: cs.CL

TL;DR: Introduces STAGE, a unified benchmark for evaluating narrative understanding over full-length movie screenplays via four interconnected tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on narrow subtasks (e.g., QA or dialogue) and do not test whether models can build and consistently use a coherent story world across different reasoning and generation abilities. There is a need for a holistic, multi-task benchmark grounded in full-length, long-context narratives like movie scripts.

Method: Construct a benchmark, STAGE, based on 150 English and Chinese movie screenplays. Provide cleaned scripts plus aligned narrative resources: curated knowledge graphs, event- and character-centric annotations. Define four tasks sharing the same narrative world representation: (1) knowledge graph construction, (2) scene-level event summarization, (3) long-context screenplay question answering, and (4) in-script character role-playing.

Result: A multilingual (English/Chinese) benchmark dataset and task suite enabling systematic evaluation of models across graph construction, summarization, QA, and role-playing on the same underlying screenplay narratives, with all resources carefully annotated and aligned.

Conclusion: STAGE offers a unified, holistic evaluation framework for narrative understanding over long-form screenplays, testing models’ abilities to build, abstract, verify, reason about, and consistently generate within a shared story world across multiple tasks.

Abstract: Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


### [47] [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)
*Seong-Gyu Park,Sohee Park,Jisu Lee,Hyunsik Na,Daeseon Choi*

Main category: cs.CL

TL;DR: The paper introduces STAR, a probability-shift-based framework to detect inference-time backdoor attacks in LLMs that exploit Chain-of-Thought reasoning, achieving near-perfect detection with high efficiency.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly use explicit Chain-of-Thought reasoning, they open a new attack surface: inference-time backdoors that inject malicious reasoning paths without changing model parameters. These attacks are hard to detect because the generated reasoning is linguistically coherent and evades traditional content-based or representation-based detectors. There is a need for a detection method that can reliably flag such hidden, behavior-level backdoors without requiring model retraining or architectural changes.

Method: The authors propose STAR (State-Transition Amplification Ratio), which monitors the model’s output probability distribution along the generated reasoning steps. The key idea is to compare the prior probability (how likely a reasoning path is under the model’s general knowledge) with the posterior probability conditioned on a potentially malicious input. STAR quantifies how much the state transition is "amplified" when going from prior to posterior, and treats unusually large, persistent amplifications as indicators of backdoors. To detect these anomalies over a sequence of reasoning steps, the method uses the CUSUM (Cumulative Sum) change-detection algorithm, flagging sequences where the amplification ratio consistently deviates from normal behavior.

Result: Across a range of models from 8B to 70B parameters and five benchmark datasets, STAR maintains high generalization and reliability, with AUROC values close to 1.0 for detecting inference-time backdoors. It significantly outperforms existing detection baselines in computational efficiency, being about 42 times faster on average. The authors also test adaptive adversaries that attempt to craft attacks specifically to evade STAR, and show that the framework remains effective under such conditions.

Conclusion: Inference-time backdoors that manipulate Chain-of-Thought reasoning form a serious but subtle threat for modern LLMs, as they are not easily caught by traditional detectors. By focusing on state-transition amplification in output probabilities and using sequential change detection, STAR provides an efficient and generalizable way to identify such attacks. The results suggest that probability-shift analysis is a promising direction for securing reasoning-enabled LLMs against covert behavioral manipulations.

Abstract: Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.

</details>


### [48] [Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces](https://arxiv.org/abs/2601.08512)
*Przemysław Spyra*

Main category: cs.CL

TL;DR: Characterizes unconditional convergence in Banach spaces via seven equivalent conditions and links them to stability of computational summation algorithms.


<details>
  <summary>Details</summary>
Motivation: In infinite-dimensional Banach spaces, unlike finite dimensions, conditional, unconditional, and absolute convergence differ sharply (Dvoretzky-Rogers theorem). This gap affects the stability and order-dependence of numerical algorithms that sum infinitely many or very many terms, such as gradient methods and signal processing schemes. A unified understanding of when series are insensitive to permutations, sign changes, or coefficient truncations is needed to provide rigorous guarantees for modern computational methods.

Method: The paper works in general Banach spaces and develops a characterization theorem for unconditional convergence. It proves the equivalence of seven conditions: invariance of convergence under permutations of terms; convergence of associated nets indexed by finite subsets; validity of subseries convergence tests; stability under arbitrary sign changes; boundedness under scalar multipliers; and a form of weak uniform convergence. Functional analytic tools, including properties of Banach spaces, nets, weak convergence, and multiplier theorems, are used to establish these equivalences and to interpret them in algorithmic terms.

Result: The main result is a comprehensive equivalence theorem: seven seemingly distinct properties—permutation invariance, net-based convergence, subseries tests, sign stability, bounded multiplier behavior, and weak uniform convergence—are all equivalent characterizations of unconditional convergence in Banach spaces. The paper further shows how these abstract conditions translate into concrete criteria ensuring that algorithmic summation procedures are order-independent, numerically stable, and robust under coefficient modifications.

Conclusion: Unconditional convergence in Banach spaces admits a rich but coherent set of equivalent characterizations that precisely capture when infinite summations are robust to permutations, sign changes, and coefficient alterations. These characterizations yield practical criteria for analyzing and designing stable computational algorithms, such as permutation-invariant gradient accumulation in SGD and thresholding-based frame expansions in signal processing, thereby tightly connecting classical functional analysis with modern numerical practice.

Abstract: The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.

</details>


### [49] [DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report](https://arxiv.org/abs/2601.08536)
*Ruizhe Li,Mingxuan Du,Benfeng Xu,Chiwei Zhu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: They propose Deep Research Bench II, a fine-grained benchmark to rigorously evaluate deep research systems’ long-form reports using thousands of human-aligned binary rubrics.


<details>
  <summary>Details</summary>
Motivation: Deep Research Systems are becoming popular for web search and long-form investigation, but current benchmarks either don’t truly test report-quality (analysis + writing) or rely on coarse, LLM-defined evaluation criteria that can be biased and hard to interpret. The authors want a rigorous, human-aligned way to measure how well these systems actually perform on realistic research tasks.

Method: They build Deep Research Bench II: 132 grounded research tasks across 22 domains. For each task, systems must generate a long-form research report. Evaluation uses 9,430 fine-grained, binary rubrics spanning information recall, analysis, and presentation. These rubrics are derived from expert-written investigative articles using a four-stage LLM+human pipeline, with over 400 hours of expert review to ensure each criterion is atomic, verifiable, and aligned with expert judgment. They then run multiple state-of-the-art deep research systems on this benchmark and score them against the rubrics.

Result: The resulting benchmark offers interpretable, fine-grained measurements of DRS performance across recall, analysis, and presentation. Empirical evaluation shows that even top-performing deep research models meet fewer than 50% of the rubrics, exposing significant performance gaps compared with human experts.

Conclusion: Deep Research Bench II provides a rigorous, human-grounded, fine-grained benchmark for evaluating deep research systems’ reports. Current leading systems perform well below human experts under these criteria, indicating substantial headroom for improvement in information coverage, reasoning quality, and report presentation.

Abstract: Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.

</details>


### [50] [Ministral 3](https://arxiv.org/abs/2601.08584)
*Alexander H. Liu,Kartik Khandelwal,Sandeep Subramanian,Victor Jouault,Abhinav Rastogi,Adrien Sadé,Alan Jeffares,Albert Jiang,Alexandre Cahill,Alexandre Gavaudan,Alexandre Sablayrolles,Amélie Héliou,Amos You,Andy Ehrenberg,Andy Lo,Anton Eliseev,Antonia Calvi,Avinash Sooriyarachchi,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Clémence Lanfranchi,Corentin Barreau,Cyprien Courtot,Daniele Grattarola,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Faruk Ahmed,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Georgii Novikov,Guillaume Kunsch,Guillaume Lample,Guillaume Martin,Gunshi Gupta,Jan Ludziejewski,Jason Rute,Joachim Studnia,Jonas Amar,Joséphine Delas,Josselin Somerville Roberts,Karmesh Yadav,Khyathi Chandu,Kush Jain,Laurence Aitchison,Laurent Fainsin,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mia Chiquier,Michel Schimpf,Nathan Grinsztajn,Neha Gupta,Nikhil Raghuraman,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Patrick von Platen,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Quentin Torroba,Romain Sauvestre,Roman Soletskyi,Rupert Menneer,Sagar Vaze,Samuel Barry,Sanchit Gandhi,Siddhant Waghjale,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thiziri Nait Saada,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Bewley,Tom Edwards,Umar Jamil,Umberto Tomasini,Valeriia Nemychnikova,Van Phung,Vincent Maladière,Virgile Richard,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xinyu Yang,Yassine El Ouahidi,Yihan Wang,Yunhao Tang,Zaccharie Ramzi*

Main category: cs.CL

TL;DR: Introduction of the Ministral 3 family: small, dense, parameter-efficient language models (3B, 8B, 14B) with base, instruction-tuned, and reasoning variants, all supporting image understanding and trained via a Cascade Distillation recipe.


<details>
  <summary>Details</summary>
Motivation: There is a need for strong language models that can run under compute and memory constraints while still offering diverse capabilities like general-purpose use, instruction following, and complex reasoning, including multimodal (image) understanding, and that are permissively licensed for broad deployment.

Method: They design a family of dense, parameter-efficient language models in three sizes (3B, 8B, 14B) and, for each size, create three variants: pretrained base, instruction-finetuned, and reasoning-focused. The models are obtained using Cascade Distillation, which iteratively prunes larger models and continues training them with knowledge distillation to maintain performance. All variants are equipped with image understanding capabilities.

Result: They obtain a suite of Ministral 3 models that are compact yet capable, spanning multiple parameter scales and use cases, and successfully integrate image understanding across all models under a parameter-efficient design. The Cascade Distillation approach yields deployable models suitable for constrained environments.

Conclusion: The Ministral 3 series demonstrates that carefully designed, parameter-efficient dense models—constructed via iterative pruning and distillation—can support general, instruction-following, and reasoning tasks, as well as image understanding, while remaining deployable in compute- and memory-limited settings under a permissive open-source license.

Abstract: We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

</details>


### [51] [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
*Wenyuan Zhang,Xinghua Zhang,Haiyang Yu,Shuaiyi Nie,Bingli Wu,Juwei Yue,Tingwen Liu,Yongbin Li*

Main category: cs.CL

TL;DR: The paper proposes ExpSeek, a method for proactively injecting experience into web agents at each decision step using entropy-based triggers, significantly improving performance on web benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing web agent experience-injection methods add past experience only as a static, global context before a task starts. This passive approach cannot adapt to evolving observations and uncertainties during multi-step interactions with web environments, limiting agent robustness and effectiveness. There is a need for a dynamic, fine-grained mechanism that decides when and how to use prior experience during the trajectory.

Method: The authors introduce ExpSeek, a step-level proactive experience intervention framework for web agents. First, it estimates entropy-based thresholds from the model’s intrinsic uncertainty signals to autonomously decide at which steps to trigger experience intervention. Second, when triggered, it retrieves or generates experience snippets that are tailored to the specific step’s context rather than providing a single global context for the whole task. The system is implemented on top of Qwen3-8B and Qwen3-32B agent models, with a separate smaller 4B-scale experience model used to supply the experience content.

Result: On four challenging web agent benchmarks, ExpSeek yields absolute performance gains of 9.3% for Qwen3-8B and 7.5% for Qwen3-32B compared to baselines without step-level proactive experience seeking. The experiments also show that entropy is an effective self-triggering signal for deciding when to intervene, and that even a relatively small 4B experience model can meaningfully enhance larger web agents.

Conclusion: Step-level, entropy-triggered proactive experience intervention is more effective than static, pre-task experience injection for web agents. ExpSeek demonstrates that using the model’s own uncertainty to time interventions, and customizing experience to each step, substantially improves performance. Additionally, experience generation can be offloaded to a much smaller model, making the approach practical and resource-efficient.

Abstract: Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.

</details>


### [52] [GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning](https://arxiv.org/abs/2601.08621)
*Jiajin Liu,Yuanfu Sun,Dongzhe Fan,Qiaoyu Tan*

Main category: cs.CL

TL;DR: The paper introduces GraphSearch, a framework that enables zero-shot graph learning for large reasoning models by integrating graph-aware search into retrieval-augmented reasoning, achieving state-of-the-art results on node classification and link prediction without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Search-augmented large reasoning models reduce hallucinations by retrieving external knowledge, but current approaches are mainly designed for text and do not exploit graph-structured data common in domains like e-commerce, social networks, and citations. Graphs offer rich topological information that can significantly improve retrieval and reasoning, yet it is challenging to query and retrieve over graphs in a way that respects both structure and semantics. There is a need for a general framework that allows LRMs to perform graph learning in a zero-shot manner while leveraging graph topology effectively.

Method: The authors propose GraphSearch, a search-augmented reasoning framework tailored for graphs. It features a Graph-aware Query Planner that separates the selection of search space (e.g., 1-hop, multi-hop, or global neighborhoods) from the formulation of semantic queries. It also includes a Graph-aware Retriever that first constructs candidate node or edge sets based on graph topology and then ranks them with a hybrid scoring function that integrates structural and semantic relevance. The framework is instantiated in two traversal modes: GraphSearch-R, which recursively expands graph neighborhoods hop-by-hop, and GraphSearch-F, which flexibly retrieves information across both local and global neighborhoods without being restricted by hop distance.

Result: Through extensive experiments on multiple graph benchmarks, GraphSearch attains competitive or superior performance relative to supervised graph learning baselines. Notably, it achieves state-of-the-art performance in zero-shot node classification and link prediction tasks, demonstrating that it can effectively perform graph learning without task-specific fine-tuning.

Conclusion: GraphSearch successfully extends search-augmented reasoning from text to graph-structured data and enables zero-shot graph learning. By making retrieval explicitly aware of graph topology and decoupling structural search space from semantic queries, it serves as a flexible and generalizable paradigm for agentic reasoning over graphs, matching or outperforming supervised graph learning methods in key tasks.

Abstract: Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.

</details>


### [53] [How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction](https://arxiv.org/abs/2601.08626)
*Yingjie He,Zhaolu Kang,Kehan Jiang,Qianyuan Zhang,Jiachen Qian,Chunlei Meng,Yujie Feng,Yuan Wang,Jiabao Dou,Aming Wu,Leqi Zheng,Pengxiang Zhao,Jiaxin Liu,Zeyu Zhang,Lei Wang,Guansu Wang,Qishi Zhan,Xiaomin He,Meisheng Zhang,Jianyuan Ni*

Main category: cs.CL

TL;DR: The paper introduces OrderProbe, a benchmark for testing how well LLMs can reconstruct correct internal word order from scrambled four-character expressions in CJK languages, revealing that even strong models struggle and that structural robustness is distinct from semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are strong at semantic understanding, their capacity to recover original structure from scrambled inputs is not well understood, and sentence-level evaluation is difficult because many word orders can be valid; there is a need for a controlled, exactly scorable test of structural reconstruction.

Method: The authors design OrderProbe, a deterministic benchmark using fixed four-character expressions in Chinese, Japanese, and Korean that each have a unique canonical order, enabling exact-match evaluation. They then build a diagnostic framework that analyzes not only reconstruction accuracy but also semantic fidelity, logical validity, consistency, robustness to perturbations, and information density, and test twelve popular LLMs under these metrics, primarily in zero-shot settings.

Result: Across twelve mainstream LLMs, structural reconstruction accuracy is low, with zero-shot exact-match recovery often below 35%, and analyses show notable gaps between how well models preserve meaning and how well they recover the correct structural order.

Conclusion: Structural reconstruction of word order remains a challenging task even for state-of-the-art LLMs, and the observed dissociation between semantic recall and structural planning indicates that robust structural capabilities do not automatically emerge from strong semantic competence, motivating further research on structural robustness in LLMs.

Abstract: Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.

</details>


### [54] [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: The paper proposes LALITA, a lexical and linguistically informed framework for selecting source sentences to curate efficient parallel corpora, showing that prioritizing complex sentences can significantly improve machine translation quality in low-resource settings while reducing data requirements by more than half.


<details>
  <summary>Details</summary>
Motivation: Data curation for machine translation is under-studied, yet especially crucial for low-resource languages where obtaining large human-translated parallel corpora is prohibitively expensive. Existing approaches mostly rely on human translations, digital parallel text, or limited synthetic data without systematically optimizing which sentences to include. The authors aim to address how to select the most beneficial source sentences to train MT systems efficiently under constrained data conditions.

Method: The authors introduce LALITA (Lexical And Linguistically Informed Text Analysis), a framework that uses lexical and linguistic features to select source sentences for building parallel corpora. They focus on English-Hindi bi-text and simulate low-resource conditions by sub-sampling datasets of various sizes (50K to 800K English sentences). LALITA prioritizes complex sentences, sourced from both existing and synthetic datasets, as training data. The framework is tested extensively and evaluated through MT performance across multiple language pairs.

Result: Training MT systems predominantly on complex sentences curated by LALITA leads to significant improvements in translation quality compared to baseline selection strategies. The experiments show consistent gains across all data sizes from 50K to 800K sentences. Moreover, LALITA achieves comparable or superior performance while requiring less than half the amount of data, and this efficiency generalizes beyond English-Hindi to other languages including Odia, Nepali, Norwegian Nynorsk, and German.

Conclusion: Careful, feature-informed sentence selection—particularly favoring lexically and structurally complex sentences—can substantially enhance MT performance in low-resource scenarios and reduce the need for large parallel corpora. LALITA provides an effective framework for such curation, lowering training costs and also serving as a useful tool for data augmentation across multiple languages.

Abstract: Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.

</details>


### [55] [Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs](https://arxiv.org/abs/2601.08634)
*Chenchen Yuan,Bolei Ma,Zheyu Zhang,Bardh Prenkaj,Frauke Kreuter,Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper examines how explicitly conditioning large language models on specific moral values shifts their political orientations, showing that moral framing causally and systematically steers political outputs.


<details>
  <summary>Details</summary>
Motivation: Existing work documents political bias in LLMs mostly via direct ideological questions or by simulating demographic personas, which may miss deeper psychological mechanisms. In social psychology, political ideology is seen as downstream of core moral intuitions. The authors are motivated to bridge this gap: instead of treating politics as a surface trait, they want to see whether and how underlying moral values can be used as controllable levers that causally influence political positioning in LLMs, thereby yielding a more principled understanding and control of political bias.

Method: The authors treat moral orientation as a controllable condition. They prompt LLMs so that they explicitly endorse or reject particular moral values, effectively using moral values as different conditioning lenses. Under each moral-conditioning setup, they then measure the model’s political position using the Political Compass Test, capturing both economic and social axes. They systematically vary moral value prompts, role framings (e.g., different instructed roles or perspectives), and model scales, and also validate robustness using alternative instruments that instantiate the same moral value to see if effects persist across prompt variants.

Result: Conditioning LLMs to support or oppose specific moral values produces clear, substantial, and value-dependent shifts in the models’ locations on the political compass. The direction and magnitude of movement differ by moral value, indicating structured and non-random effects. These shifts are further shaped by how the model is framed in the prompt (role framing) and by model size, suggesting interaction between moral conditioning and model capabilities. The observed effects are robust across different prompt formulations that encode the same moral value, indicating that the relationship between moral conditioning and political positioning is not fragile to wording changes.

Conclusion: The study concludes that moral-value conditioning offers a causal and controllable way to steer LLMs’ political outputs, demonstrating that political orientation in models is tightly linked to underlying moral framings. Because these effects are systematic, role- and scale-dependent, and robust across prompts, the authors argue that responsible alignment of LLMs should embed political evaluation within broader moral and social value contexts. This perspective suggests future alignment strategies should explicitly model and manage the moral foundations that shape political behavior in language models, not just their overt political responses.

Abstract: While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.

</details>


### [56] [A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding](https://arxiv.org/abs/2601.08645)
*Dilara Torunoğlu-Selamet,Dogukan Arslan,Rodrigo Wilkens,Wei He,Doruk Eryiğit,Thomas Pickard,Adriana S. Pagano,Aline Villavicencio,Gülşen Eryiğit,Ágnes Abuczki,Aida Cardoso,Alesia Lazarenka,Dina Almassova,Amalia Mendes,Anna Kanellopoulou,Antoni Brosa-Rodríguez,Baiba Saulite,Beata Wojtowicz,Bolette Pedersen,Carlos Manuel Hidalgo-Ternero,Chaya Liebeskind,Danka Jokić,Diego Alves,Eleni Triantafyllidi,Erik Velldal,Fred Philippy,Giedre Valunaite Oleskeviciene,Ieva Rizgeliene,Inguna Skadina,Irina Lobzhanidze,Isabell Stinessen Haugen,Jauza Akbar Krito,Jelena M. Marković,Johanna Monti,Josue Alejandro Sauca,Kaja Dobrovoljc,Kingsley O. Ugwuanyi,Laura Rituma,Lilja Øvrelid,Maha Tufail Agro,Manzura Abjalova,Maria Chatzigrigoriou,María del Mar Sánchez Ramos,Marija Pendevska,Masoumeh Seyyedrezaei,Mehrnoush Shamsfard,Momina Ahsan,Muhammad Ahsan Riaz Khan,Nathalie Carmen Hau Norman,Nilay Erdem Ayyıldız,Nina Hosseini-Kivanani,Noémi Ligeti-Nagy,Numaan Naeem,Olha Kanishcheva,Olha Yatsyshyna,Daniil Orel,Petra Giommarelli,Petya Osenova,Radovan Garabik,Regina E. Semou,Rozane Rebechi,Salsabila Zahirah Pranida,Samia Touileb,Sanni Nimb,Sarfraz Ahmad,Sarvinoz Nematkhonova,Shahar Golan,Shaoxiong Ji,Sopuruchi Christian Aboh,Srdjan Sucur,Stella Markantonatou,Sussi Olsen,Vahide Tajalli,Veronika Lipp,Voula Giouli,Yelda Yeşildal Eraydın,Zahra Saaberi,Zhuohan Xie*

Main category: cs.CL

TL;DR: Introduces XMPIE, a large multilingual and multimodal dataset of potentially idiomatic expressions for evaluating and comparing idiom understanding in NLP models.


<details>
  <summary>Details</summary>
Motivation: Idiomatic expressions are deeply tied to everyday experience and culture, making them hard for NLP systems and valuable for testing linguistic and cultural competence. Existing resources are limited in multilingual and multimodal coverage, hindering systematic comparison of idiom behavior and model performance across languages and modalities.

Method: Construct a parallel dataset of potentially idiomatic expressions (PIEs) across 34 languages, created by language experts following shared multilingual guidelines. For each PIE, provide both textual and visual components: five carefully designed images per expression that span idiomatic to literal interpretations, plus related and random distractors. Ensure parallelism so that the same underlying PIE scenario is instantiated comparably across languages and modalities.

Result: XMPIE contains over 10,000 items covering 34 languages, with each item pairing a PIE with a set of images that systematically vary in their relation to the idiomatic and literal meanings. The dataset enables controlled experiments on how models interpret idiomatic expressions in different languages and how well understanding transfers across languages and between text and images.

Conclusion: XMPIE constitutes a high-quality benchmark for assessing multilingual and multimodal idiomatic language understanding. It supports comparative analysis of idiomatic patterns and cultural commonalities, evaluation of cross-lingual transfer of idiom understanding, and investigation of how textual and visual cues jointly contribute to interpreting potentially idiomatic expressions.

Abstract: Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.

</details>


### [57] [Safe Language Generation in the Limit](https://arxiv.org/abs/2601.08648)
*Antonios Anastasopoulos,Giuseppe Ateniese,Evgenios M. Kornaropoulos*

Main category: cs.CL

TL;DR: The paper provides a theoretical framework for ‘safe language generation’ within learning-in-the-limit, showing that safe identification is impossible and that safe generation is at least as hard as already-impossible standard language identification, while characterizing some tractable and intractable subcases.


<details>
  <summary>Details</summary>
Motivation: As language models move from pure theory to real-world use, we must understand not just whether they can learn or generate a target language, but whether they can do so safely. Existing results in learning in the limit separate language identification (impossible in general) from language generation (tractable in some cases), but they do not account for safety constraints. This paper fills that gap by asking: what happens to these learnability and generability results when we require safety?

Method: The authors work in the formal framework of learning in the limit and introduce formal definitions for ‘safe language identification’ and ‘safe language generation’. They then use techniques from computability and learning theory to relate these safety-aware tasks to classic language identification. Through reductions and impossibility proofs, they show that safe identification is impossible and that safe generation is at least as hard as standard identification. They also examine special classes of languages or safety conditions to delineate tractable versus intractable cases.

Result: 1) Under the learning-in-the-limit model, safe language identification cannot be solved in general. 2) Safe language generation is provably at least as hard as standard language identification, which itself is impossible in this setting, implying strong negative results for safety-aware generation. 3) Despite these impossibility results, the authors identify particular restricted scenarios where safe generation remains tractable, as well as others that remain intractable.

Conclusion: Safety constraints in theoretical language learning fundamentally change (and often worsen) the feasibility landscape: once safety is required, identification remains impossible and generation inherits that hardness. However, within carefully restricted subclasses of languages or safety predicates, meaningful safe generation guarantees are still achievable. The paper establishes a baseline theory for safe language generation and highlights that real-world safety desiderata may come with inherent theoretical limits.

Abstract: Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.
  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.

</details>


### [58] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong,Huaiyuan Yao,Bolin Shen,Wanpeng Xu,Hua Wei,Yushun Dong*

Main category: cs.CL

TL;DR: The paper introduces RULERS, a framework that turns natural-language evaluation rubrics for LLM-as-a-judge into executable, stable, and calibrated scoring procedures, improving agreement with humans without retraining models.


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge is attractive for scalable, rubric-based evaluation, but current approaches suffer from instability and misalignment with human judgments because prompts are fragile, reasoning is unverifiable, and score scales don’t match human grading. The authors want a way to systematically align black-box, frozen LLM judges with human standards while avoiding these common failure modes.

Method: They reconceptualize judge alignment as a criteria transfer problem and propose RULERS, a compiler–executor framework. RULERS (1) compiles natural-language rubrics into versioned, immutable executable bundles, (2) enforces structured decoding where the model must produce auditable evidence and criteria-level decisions that can be deterministically verified, and (3) performs lightweight post-hoc calibration using a Wasserstein-distance-based method to align model score distributions with human grading scales. This is done without changing the underlying LLM parameters, relying instead on rubric compilation, execution constraints, and calibration.

Result: On essay and summarization benchmarks, RULERS yields higher agreement with human judgments than representative LLM-as-a-judge baselines, shows robustness to adversarial perturbations of the rubric (i.e., prompt edits), and lets smaller open models perform on par with larger proprietary judge models when used through the RULERS framework.

Conclusion: Effective and reliable LLM judging should not rely mainly on prompt phrasing. Instead, it requires (1) executable rubrics with stable, versioned specifications, (2) verifiable, evidence-anchored reasoning from the model, and (3) calibrated scoring scales. With these components, even frozen black-box LLMs can be aligned more closely with human evaluation standards, as demonstrated by RULERS.

Abstract: The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [59] [Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification](https://arxiv.org/abs/2601.08668)
*Kyuri Im,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: The paper analyzes when and why large language models incorrectly refuse to perform hate speech detoxification, and proposes a cross-translation approach (via Chinese) to reduce such false refusals.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used for hate speech detoxification, but safety filters sometimes overfire, making models refuse even legitimate detoxification requests. This false refusal problem is underexplored, yet it harms real-world usability and may introduce systematic biases. The authors want to understand what contextual and linguistic factors cause refusals, and to find a practical way to mitigate them without weakening safety.

Method: The authors benchmark nine LLMs on both English and multilingual hate speech detoxification datasets. They systematically measure false refusals—cases where the model should detoxify but instead declines. They analyze refusal patterns by semantic toxicity level and by target attributes (e.g., nationality, religion, political ideology), and examine language-dependent effects. Based on these insights, they design and test a simple cross-translation pipeline: translate English hate speech into Chinese, run detoxification in Chinese, then translate the detoxified output back into English.

Result: Across the evaluated LLMs, false refusals are more frequent when inputs contain higher semantic toxicity and when hate is directed at specific groups, especially those defined by nationality, religion, and political ideology. Multilingual datasets show overall fewer false refusals compared to English-only datasets, but consistent, language-specific biases persist. The proposed cross-translation (English → Chinese → English) strategy significantly lowers false refusal rates while keeping the underlying message intact.

Conclusion: LLMs’ safety mechanisms can cause systematic and biased false refusals in hate speech detoxification, especially for highly toxic content and sensitive target groups, and these biases vary across languages. A lightweight mitigation—performing detoxification via cross-translation into Chinese and back—can substantially reduce such refusals without sacrificing the original content, offering a practical way to improve detoxification robustness while retaining safety.

Abstract: While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.

</details>


### [60] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla,Chenyang Zhu,Pengshan Cai,Sangwoo Cho,Scott Novotney,Ayushman Singh,Jonah Lewis,Keasha Safewright,Alfy Samuel,Erin Babinsky,Shi-Xiong Zhang,Sambit Sahu*

Main category: cs.CL

TL;DR: Industry case study on building an agentic system for summarizing multi-party dialogues, focusing on practical challenges like evolving requirements, evaluation, modular optimization, data bottlenecks, and vendor lock-in.


<details>
  <summary>Details</summary>
Motivation: Summarizing multi-party dialogues is important for knowledge transfer and operations, but current research mostly uses static benchmarks that don’t reflect evolving real-world requirements, making it hard to deploy robust systems in industry.

Method: Develop and deploy an agentic (multi-component) summarization system for multi-party interactions, then analyze and report practical lessons across the development lifecycle, particularly on evaluation under subjectivity, component-wise optimization, handling data bottlenecks, and managing prompt transferability across vendors.

Result: The study identifies effective strategies for evaluating subjective, evolving summarization tasks; shows how agentic architectures enable targeted optimization of individual components; reveals how upstream data limitations can constrain summarization quality; and documents the practical problem of vendor lock-in caused by non-transferable LLM prompts.

Conclusion: Agentic architectures, coupled with carefully designed evaluation and awareness of data and vendor constraints, can produce more reliable and adaptable dialogue summarization systems, but practitioners must plan for evolving requirements, data bottlenecks, and limited cross-vendor prompt portability.

Abstract: Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [61] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://arxiv.org/abs/2601.08689)
*Zhaolu Kang,Junhao Gong,Wenqing Hu,Shuo Yin,Kehan Jiang,Zhicheng Fang,Yingjie He,Chunlei Meng,Rong Fu,Dongyang Chen,Leqi Zheng,Eric Hanchen Jiang,Yunfei Feng,Yitong Leng,Junfan Zhu,Xiaoyou Chen,Xi Yang,Richeng Xuan*

Main category: cs.CL

TL;DR: QuantEval is a benchmark to systematically evaluate LLMs on quantitative finance tasks, including knowledge, reasoning, and strategy coding with real backtesting.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs in finance are fragmented and largely focus only on knowledge-based question answering, which does not reflect the full range of quantitative finance skills needed in practice, especially for trading strategy development and deployment.

Method: The authors design QuantEval, a benchmark that spans three key dimensions: (1) knowledge-based financial QA, (2) quantitative mathematical reasoning tasks, and (3) quantitative strategy coding. They embed a CTA-style deterministic backtesting framework that runs model-generated trading strategies and scores them with established financial performance metrics. They then test multiple state-of-the-art open-source and proprietary LLMs and also perform supervised fine-tuning and reinforcement learning on domain-specific data to study performance gains.

Result: Across the benchmark, current LLMs significantly underperform human experts, most notably in reasoning-heavy tasks and in generating functional, profitable trading strategies. However, models that undergo domain-aligned supervised fine-tuning and reinforcement learning show consistent and measurable improvements on the QuantEval tasks and backtesting metrics.

Conclusion: QuantEval offers a more comprehensive and realistic framework for assessing LLMs' quantitative finance capabilities by combining knowledge, reasoning, and executable strategy coding evaluated through backtesting. The benchmark reveals substantial current limitations but also shows that domain-specific training can close some of the gaps, and the authors release a fully specified, reproducible backtesting setup to support further research and practical adoption in trading workflows.

Abstract: Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [62] [Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models](https://arxiv.org/abs/2601.08692)
*Keito Inoshita*

Main category: cs.CL

TL;DR: The paper compares conventional neural models and large language models (LLMs) for predicting nationality from personal names, showing that LLMs perform better overall but have limitations on rare nationalities and make mostly near-miss errors at regional levels.


<details>
  <summary>Details</summary>
Motivation: Predicting nationality from names is useful for applications like marketing, demographic analysis, and genealogy. Existing neural models struggle with low-frequency (rare) nationalities and differentiating between similar nationalities within a region, because they rely heavily on task-specific training data and lack broader world knowledge. The authors are motivated to test whether LLMs, with their rich pre-trained world knowledge, can overcome these limitations and provide more accurate and robust nationality predictions.

Method: The authors conduct a systematic empirical comparison between traditional neural name-nationality predictors and large language models. They evaluate six neural architectures and six prompting strategies for LLMs across three label granularities: exact nationality, broader region, and continent. They further perform a frequency-based stratified analysis to separately examine performance on common versus rare nationalities, and carry out an error analysis that characterizes typical mistake patterns for each model family (e.g., cross-regional vs. within-region errors, bias toward frequent classes).

Result: Across all three levels of granularity, LLM-based approaches outperform the tested neural baselines, though the performance gap shrinks when labels are coarser (region, continent). Simple non-deep learning methods show the highest robustness for rare (low-frequency) nationalities, while both pre-trained neural models and LLMs suffer noticeable performance drops in these cases. The error analysis shows that LLMs more often produce "near-miss" errors—getting the region right but the exact nationality wrong—whereas conventional neural models are more likely to confuse names across regions and to favor high-frequency nationalities regardless of input.

Conclusion: The study concludes that the advantage of LLMs in nationality prediction mainly arises from their pre-trained world knowledge, which yields better calibrated, locality-aware predictions. However, this advantage is less pronounced for rare nationalities and at coarser label granularities. The authors emphasize that model choice should depend on the desired prediction granularity (nationality vs. region vs. continent) and that evaluation metrics should not focus solely on raw accuracy but also consider the qualitative nature of errors, such as whether mistakes are near-misses within the correct region or more severe cross-regional confusions.

Abstract: Predicting nationality from personal names has practical value in marketing, demographic research, and genealogical studies. Conventional neural models learn statistical correspondences between names and nationalities from task-specific training data, posing challenges in generalizing to low-frequency nationalities and distinguishing similar nationalities within the same region. Large language models (LLMs) have the potential to address these challenges by leveraging world knowledge acquired during pre-training. In this study, we comprehensively compare neural models and LLMs on nationality prediction, evaluating six neural models and six LLM prompting strategies across three granularity levels (nationality, region, and continent), with frequency-based stratified analysis and error analysis. Results show that LLMs outperform neural models at all granularity levels, with the gap narrowing as granularity becomes coarser. Simple machine learning methods exhibit the highest frequency robustness, while pre-trained models and LLMs show degradation for low-frequency nationalities. Error analysis reveals that LLMs tend to make ``near-miss'' errors, predicting the correct region even when nationality is incorrect, whereas neural models exhibit more cross-regional errors and bias toward high-frequency classes. These findings indicate that LLM superiority stems from world knowledge, model selection should consider required granularity, and evaluation should account for error quality beyond accuracy.

</details>


### [63] [RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis](https://arxiv.org/abs/2601.08699)
*Zhengwei Tao,Bo Li,Jialong Wu,Guochen Yan,Huanyao Zhang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: RAGShaper is a framework to automatically generate realistic, noisy RAG tasks and agent trajectories so models learn to handle retrieval errors and distractors.


<details>
  <summary>Details</summary>
Motivation: Robust agentic RAG systems need training data that reflects real-world retrieval noise and complexity, including failures and misleading evidence. Manual construction of such datasets is expensive, unscalable, and tends to miss dynamic multi-step reasoning and error-correction behaviors. There is a gap between current clean/synthetic RAG benchmarks and the messy retrieval environments agents face in practice.

Method: The authors propose RAGShaper, a data synthesis framework for agentic RAG. It first uses an InfoCurator component to build dense information trees, deliberately injecting adversarial distractors at both Perception level (noisy/irrelevant passages) and Cognition level (semantically plausible but misleading evidence). Then, a constrained navigation strategy is applied to a strong teacher agent: its access to information is restricted in ways that force it to encounter distractors and retrieval failures. The teacher must plan, retrieve, and revise its reasoning in this environment, generating trajectories that explicitly include error detection, correction, and noise rejection. These trajectories and tasks form a training corpus for downstream RAG agents.

Result: Models trained on data synthesized by RAGShaper outperform baselines on a variety of RAG benchmarks, particularly those with high noise and complex multi-hop retrieval demands. The improvements are in robustness to retrieval noise, ability to ignore distractors, and effectiveness on challenging retrieval-intensive tasks compared with models trained on conventional or less adversarial data.

Conclusion: Automatically synthesized, adversarially noisy RAG tasks and teacher trajectories can substantially improve the robustness of agentic RAG systems. By explicitly shaping the retrieval environment and forcing agents to confront and correct errors, RAGShaper provides scalable supervision for learning noise-aware, resilient reasoning strategies that generalize to complex real-world retrieval scenarios.

Abstract: Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.

</details>


### [64] [PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation](https://arxiv.org/abs/2601.08739)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PrivGemo is a privacy-preserving retrieval-augmented framework that lets LLMs reason over knowledge graphs without leaking sensitive graph structure or content, achieving SOTA performance while enabling small models to rival GPT-4-Turbo on KG reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs benefit from grounding their reasoning in knowledge graphs for knowledge-intensive QA, but many real KGs are private. Sending KG triples or reasoning traces to closed-source LLM APIs risks leaking sensitive entities and graph structure. Existing privacy methods mostly mask entity names, which still leak structural information, give little control over how much is revealed to remote models, break in complex multi-hop/multi-entity reasoning, and make it hard to reuse past interactions efficiently and safely. A new approach is needed that more thoroughly controls what semantic and structural KG information is exposed while preserving strong reasoning performance and efficiency.

Method: PrivGemo introduces a privacy-preserving retrieval-augmented framework with a dual-tower design. One tower keeps the raw KG and performs grounding and verification locally; the other interacts with the remote LLM over an anonymized view of the KG. Instead of just masking names, the system constructs anonymized long-hop paths that connect all topic entities, limiting both semantic and structural exposure. A hierarchical controller decides when and how to query the remote LLM, and a privacy-aware experience memory stores reusable remote reasoning experiences to cut down redundant exploration and API calls, all under exposure constraints.

Result: On six benchmarks for KG-grounded reasoning and question answering, PrivGemo achieves state-of-the-art overall performance, surpassing the strongest existing baseline by as much as 17.1% on some tasks. It also allows compact open models such as Qwen3-4B to reach reasoning performance similar to large proprietary models like GPT-4-Turbo when combined with PrivGemo’s privacy-preserving retrieval and control mechanisms.

Conclusion: PrivGemo effectively reconciles strong KG-grounded reasoning with strict privacy requirements by exposing only an anonymized, structure-limited KG view to remote LLMs while keeping sensitive grounding local. Its dual-tower architecture, hierarchical controller, and privacy-aware experience memory support robust multi-hop, multi-entity reasoning, reduce unnecessary remote interactions, and significantly improve both accuracy and efficiency. The framework demonstrates that smaller, local or open models can, when carefully augmented, rival much larger closed models on knowledge-intensive reasoning tasks without compromising KG privacy.

Abstract: Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [65] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: Introduces FRTR-Bench, a large-scale multimodal spreadsheet reasoning benchmark, and FRTR, a retrieval-augmented framework that greatly improves LLM accuracy and efficiency on complex enterprise Excel workbooks.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly on realistic, large, multimodal enterprise spreadsheets because existing methods either compress a single sheet or encode full context, which does not scale and does not match how users navigate complex workbooks with multiple sheets and embedded images. A new benchmark and method are needed to evaluate and improve spreadsheet reasoning in more realistic settings.

Method: Construct FRTR-Bench, a benchmark of 30 enterprise-grade Excel workbooks with nearly four million cells and over 50 embedded images, designed for multimodal spreadsheet reasoning. Propose FRTR, a multimodal retrieval-augmented generation framework that decomposes workbooks into fine-grained row, column, and block embeddings; uses hybrid lexical and dense retrieval combined via Reciprocal Rank Fusion (RRF); and incorporates multimodal embeddings so LLMs can jointly reason over numerical tables and visual content (e.g., charts, receipts). Evaluate FRTR with multiple LLM backends.

Result: On FRTR-Bench, FRTR with Claude Sonnet 4.5 attains 74% answer accuracy, far surpassing prior state-of-the-art methods that achieve only 24%. On the SpreadsheetLLM benchmark, FRTR with GPT-5 reaches 87% accuracy while cutting token usage by about 50% relative to context-compression approaches.

Conclusion: Fine-grained, multimodal retrieval-augmented generation is highly effective for reasoning over realistic, large-scale enterprise spreadsheets. FRTR-Bench provides a challenging, scalable benchmark, and FRTR significantly advances both accuracy and efficiency over previous spreadsheet reasoning techniques, showing that decomposed embeddings plus hybrid retrieval and multimodal reasoning better reflect real-world spreadsheet usage.

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [66] [Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents](https://arxiv.org/abs/2601.08742)
*Xin Quan,Jiafeng Xiong,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: The paper proposes Attributional NLI (Att-NLI), a framework for evaluating how well LLM agents infer and verify others’ latent intentions in multi-agent settings, and shows that a neuro-symbolic Att-NLI agent performs best.


<details>
  <summary>Details</summary>
Motivation: Standard natural language inference focuses on entailment and contradiction between explicit statements, but it does not capture the richer, attributional reasoning humans use to infer others’ hidden intentions in interactive, multi-agent environments. The authors aim to fill this gap by importing ideas from social psychology into NLI, so that LLMs can be evaluated (and ultimately designed) to perform abductive and deductive reasoning about agents’ intentions rather than just surface-level text relations.

Method: They define Attributional NLI (Att-NLI), which augments classical NLI with attributional reasoning: (1) abductive inference to hypothesize possible latent intentions from observed actions, and (2) deductive verification to test which hypotheses are logically supported. They operationalize this in a textual game called Undercover-V, where LLM agents must infer intentions in a multi-agent setting. They compare three agent types: (a) a baseline NLI agent limited to deductive inference; (b) an Att-NLI agent that performs both abductive and deductive reasoning; and (c) a neuro-symbolic Att-NLI agent that additionally uses external theorem provers to support its abductive-deductive reasoning.

Result: Across extensive experiments in the Undercover-V game, the agents exhibit a clear performance hierarchy in attributional inference tasks. The neuro-symbolic Att-NLI agent consistently outperforms both the standard NLI and purely neural Att-NLI agents, attaining an average win rate of 17.08%, indicating better ability to infer and verify latent intentions.

Conclusion: The study concludes that extending NLI with attributional, intention-focused reasoning (Att-NLI) is important for building LLM agents capable of human-like reasoning in multi-agent environments. It also highlights that neuro-symbolic approaches—combining LLMs with formal theorem provers—significantly strengthen attributional inference, suggesting a promising direction for constructing more rational, socially aware AI agents.

Abstract: Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.

</details>


### [67] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://arxiv.org/abs/2601.08743)
*Jinbo Su,Yuxuan Hu,Cuiping Li,Hong Chen,Jia Li,Lintao Ma,Jing Zhang*

Main category: cs.CL

TL;DR: The paper proposes TableCache, a method to speed up LLM-based Text-to-SQL by precomputing and reusing table-specific KV caches instead of repeatedly encoding long database schemas in prompts.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based Text-to-SQL systems must include large database schemas in every prompt, causing long context lengths, high prefilling latency, and redundant KV cache copies when table orders differ slightly between queries. However, user queries usually focus on a small, recurrent subset of tables, suggesting that shared prefix computation and KV cache reuse across queries could significantly reduce latency if managed correctly.

Method: The authors introduce TableCache, which precomputes table-level KV cache representations offline and then dynamically composes them at inference time according to the tables required by each query. They preserve primary–foreign key relationships when computing these table caches to maintain schema semantics. To make cache composition efficient, they build a Table Trie data structure to index and retrieve the appropriate KV segments. Furthermore, they design a cache management system that includes (1) a query reranking strategy to maximize cache hit rates and (2) a computation loading pipeline that overlaps model inference with cache loading for better parallelism.

Result: Experiments show that TableCache reduces Time to First Token (TTFT) by up to 3.62x compared to baseline systems like SGLang and vLLM, while maintaining essentially the same Text-to-SQL accuracy, i.e., negligible task performance degradation.

Conclusion: Precomputing and modularizing schema information as reusable table-level KV caches, organized via a Table Trie and managed with cache-aware scheduling, can substantially accelerate LLM-based Text-to-SQL inference without sacrificing accuracy. This demonstrates that structural properties of database schemas and query patterns can be exploited for efficient KV cache reuse in LLM applications.

Abstract: In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.

</details>


### [68] [To Retrieve or To Think? An Agentic Approach for Context Evolution](https://arxiv.org/abs/2601.08747)
*Rubing Chen,Jian Wang,Wenjie Li,Xiao-Yong Wei,Qing Li*

Main category: cs.CL

TL;DR: Proposes ACE, a metacognition-inspired framework that dynamically decides when to retrieve external knowledge versus reasoning over current context, improving accuracy and efficiency on multi-hop QA.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation methods blindly retrieve at every step, causing high computational cost and performance degradation due to noisy, overlong contexts. There is a need for a more selective, human-like mechanism that decides when new evidence is actually necessary.

Method: Introduce Agentic Context Evolution (ACE), which uses a central orchestrator agent that, via majority voting, decides at each step whether to (1) activate a retriever agent to fetch new external information, or (2) activate a reasoner agent to analyze and refine the current context. This alternation is aimed at maintaining a concise, progressively improved context while avoiding redundant retrieval.

Result: On challenging multi-hop question answering benchmarks, ACE achieves higher accuracy than strong baselines while also reducing token usage, indicating both better effectiveness and efficiency in knowledge-intensive reasoning tasks.

Conclusion: Dynamic, agent-based control over retrieval versus reasoning leads to better context management than brute-force retrieval at every step. ACE demonstrates that context-evolved generation with metacognitive decision-making can more effectively solve complex, knowledge-intensive tasks while using computational resources more efficiently.

Abstract: Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.

</details>


### [69] [Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables](https://arxiv.org/abs/2601.08750)
*Valerie Zermatten,Chiara Vanalli,Gencer Sumbul,Diego Marcos,Devis Tuia*

Main category: cs.CL

TL;DR: The paper introduces an attention-based model that fuses aerial imagery, geolocated text, and spatial neighborhood information to predict environmental variables, outperforming image-only, text-only, and single-location baselines.


<details>
  <summary>Details</summary>
Motivation: Textual data (e.g., Wikipedia descriptions) contain fine-grained local environmental information that is complementary to traditional geospatial data like satellite imagery, but its role in ecology is unclear due to sparsity, irregularity, and integration challenges. The paper aims to clarify how and for which tasks geolocated text should be integrated with imagery in ecological modelling.

Method: The authors propose a multimodal attention-based architecture that encodes aerial images, geolocated text, and location information, and then uses an attention module over spatial neighborhoods to dynamically select informative neighboring observations. This model is trained and evaluated on the EcoWikiRS dataset, which links high-resolution aerial imagery with environmental sentences from Wikipedia across Switzerland, to predict 103 environmental variables from the SWECO25 data cube.

Result: The proposed model consistently outperforms baselines that either use only a single location or only one modality (image-only or text-only). Performance gains are especially notable when predicting climatic, edaphic (soil-related), population, and land use/land cover variables, demonstrating that including spatial context and multimodal fusion leads to better predictive accuracy.

Conclusion: Integrating geolocated text with aerial imagery through an attention-based neighborhood model effectively leverages sparse textual signals and spatial context, improving prediction of diverse environmental variables. This shows that textual resources can meaningfully complement traditional geospatial data in ecological applications and that modelling spatial neighborhoods is key to extracting their value.

Abstract: Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.

</details>


### [70] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang,Li Dong,Yaru Hao,Qingxiu Dong,Furu Wei,Jiatao Gu*

Main category: cs.CL

TL;DR: The paper introduces Multiplex Thinking, a soft stochastic reasoning mechanism for LLMs that compresses multiple candidate reasoning steps into a single continuous token, improving math reasoning performance and sequence efficiency over standard Chain-of-Thought and RL baselines.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT) improves LLM reasoning but produces long, low-bandwidth token sequences that are inefficient. Humans, in contrast, often reason by keeping several possible next steps in mind simultaneously, rather than committing to one discrete thought at a time. The authors aim to design a reasoning mechanism that lets LLMs maintain and act on a distribution over plausible next steps, achieving better reasoning quality and shorter sequences while remaining compatible with standard training and RL optimization.

Method: They propose Multiplex Thinking, where at each reasoning step the model stochastically samples K candidate next tokens, then aggregates their embeddings into a single continuous "multiplex token." This operation preserves the original token embedding space and sampling dynamics but yields a differentiable distribution over possible rollouts. The multiplex representation adapts to confidence: when confidence is high it collapses toward a single discrete token, and when low it blends multiple plausible options. Because the multiplex rollout distribution is tractable, they can directly apply on-policy reinforcement learning to optimize reasoning trajectories using this mechanism.

Result: On several challenging math reasoning benchmarks, models using Multiplex Thinking outperform strong baselines that use discrete Chain-of-Thought and RL, across a wide range of evaluation settings from Pass@1 to Pass@1024. At the same time, they generate shorter reasoning sequences than standard CoT methods, demonstrating improved efficiency without sacrificing—or even while improving—accuracy.

Conclusion: Multiplex Thinking offers a self-adaptive, soft reasoning mechanism that allows LLMs to represent multiple possible reasoning paths within a single token step. This yields more compact yet more effective reasoning trajectories, is compatible with standard RL optimization, and produces consistent gains over discrete CoT and RL methods on difficult math benchmarks. The approach suggests a promising direction for making LLM reasoning both more powerful and more token-efficient.

Abstract: Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>


### [71] [Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System](https://arxiv.org/abs/2601.08829)
*Hsiang-Wei Huang,Junbin Lu,Kuang-Ming Chen,Jenq-Neng Hwang*

Main category: cs.CL

TL;DR: The paper simulates how LLM-based reviewers with different personas behave in an Elo-ranked peer-review system, finding that Elo can improve decision accuracy but also induces strategic behavior that exploits the system without extra effort.


<details>
  <summary>Details</summary>
Motivation: As LLMs begin to act as reviewers or decision-support tools in academic peer review, it is important to understand how they behave in algorithmic ranking systems (like Elo) and how such systems affect review quality, decision accuracy, and potential strategic or gaming behaviors. The authors want to study whether Elo-style rating and memory mechanisms can improve conference reviewing while being robust to adaptive strategies of LLM agents.

Method: Using real conference paper submissions, the authors build a simulation environment where multiple LLM agent reviewers, each with a distinct persona, review papers over multiple rounds. An Area Chair moderates and makes final decisions. They implement different experimental conditions: a baseline without Elo or memory, and variants that include Elo ratings for reviewers and reviewer memory across rounds. They then compare Area Chair decision accuracy and agent behaviors across these conditions.

Result: The simulations show that adding Elo ratings leads to more accurate Area Chair decisions about papers. However, LLM reviewers adapt their strategies to exploit the Elo-based system—for example, altering how they review or score in ways that benefit their Elo standing—without actually increasing their review effort or substantive contribution to review quality.

Conclusion: Elo-based reviewer ranking can enhance decision accuracy in LLM-assisted peer review, but it also creates incentives for strategic, exploitative behavior by LLM agents that does not correspond to better effort or quality. Designing peer-review systems that use LLM reviewers therefore requires care to capture the benefits of rating mechanisms like Elo while mitigating opportunities for gaming the system.

Abstract: In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: The paper proposes and evaluates a hybrid explainable AI framework (fuzzy-XGBoost + SHAP) for maternal health risk prediction that improves transparency and clinician trust in a low-resource setting.


<details>
  <summary>Details</summary>
Motivation: Although ML models can predict maternal health risks, their clinical use in resource-constrained settings is hindered by lack of explainability and trust from clinicians. There is a need for methods that are both accurate and interpretable, and that are validated with real clinician feedback rather than only technical metrics.

Method: The authors design a hybrid XAI framework that combines ante-hoc interpretable fuzzy logic (to encode clinical risk rules) with a post-hoc explanation method (SHAP) applied to an XGBoost model. They train the fuzzy-XGBoost model on 1,014 maternal health records and then run a validation study with 14 healthcare professionals in Bangladesh using three clinical cases. They collect preferences and trust ratings for different explanation styles, and use SHAP to analyze feature importance, including an engineered fuzzy risk score feature.

Result: The fuzzy-XGBoost model achieves 88.67% accuracy and ROC-AUC of 0.9703 for maternal risk prediction. In the clinician study, 71.4% of preferences across three clinical cases favored the hybrid explanation approach, and 54.8% of responses expressed trust in the model for clinical use. SHAP shows healthcare access as the top predictor and the fuzzy risk score as the third most important feature, with a correlation r=0.298 to validate that the integrated clinical risk knowledge contributes meaningfully. Clinicians appreciated the inclusion of clinical parameters but highlighted missing obstetric history, gestational age, and connectivity issues as important limitations.

Conclusion: The study shows that combining interpretable fuzzy logic rules with SHAP-based feature importance explanations can increase both practical utility and clinician trust in ML-based maternal risk prediction, especially in low-resource settings. The framework offers a path towards more acceptable and deployable XAI tools in maternal healthcare and surfaces concrete data and infrastructure gaps that need to be addressed in future systems.

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [73] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: The paper proposes a simple, training-free calibration-based framework that lets models reliably estimate their own uncertainty, and uses this for model cascading and data cleaning across vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: AI models often make confident mistakes and lack awareness of their own uncertainty, which limits their reliability, efficiency, and trustworthiness. Existing confidence calibration methods are usually domain-specific or tied to particular training schemes. The authors want a simple, universal, and training-free way to make model confidence both reliable (well-calibrated) and comparable across models so it can be directly used for practical tasks like routing between models of different sizes and cleaning mislabeled data.

Method: They empirically study model confidence and show two properties: (1) within a single model, higher confidence values correlate with higher accuracy; and (2) once calibrated on a validation set, a model’s confidence remains calibrated on a held-out test set. Based on these observations, they apply a simple calibration procedure to both vision and language models and then design two applications: (1) model cascading using “calibrated advantage routing,” where inputs are routed between large and small models based on calibrated confidence; and (2) data cleaning using ensembles of models whose calibrated confidences are combined to detect mislabeled samples in datasets like ImageNet and MMLU. The framework is explicitly training-free with respect to the base models.

Result: They show that calibrated confidence is both reliable and comparable across models and datasets. Using calibrated routing, cascading a small and a large model yields significant efficiency gains with almost no loss in accuracy, and cascading two similarly sized models can even surpass each model’s standalone performance. For data cleaning, their ensemble-based calibrated confidence method can identify mislabeled examples in ImageNet and MMLU while maintaining a good balance between precision (few false positives) and detection rate (finding many true mislabeled samples).

Conclusion: Well-calibrated confidence gives models a practical form of “knowing when they don’t know,” which can be exploited without retraining to improve efficiency, performance, and data quality. The paper concludes that calibration can serve as a universal, training-free mechanism across vision and language models for trustworthy decision-making, particularly via model cascading and systematic identification of mislabeled data.

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [74] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: The paper compares rule-based and case-based safety alignment for large language models and proposes a new method, CADA, that uses case-augmented deliberative alignment to improve harmlessness while preserving helpfulness.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment like OpenAI’s deliberative alignment relies on detailed code-like safety rules and strong reasoning abilities, which open-source LLMs often lack. Moreover, such explicit codes can cause models to over-refuse benign requests and be less helpful. There is a need to understand whether explicit safety codes or example-based guidance work better for typical open-source models and to design a method that increases safety without sacrificing utility.

Method: The authors systematically evaluate two strategies for steering LLM safety behavior: (1) providing extensive explicit safety codes and (2) using simpler codes augmented with illustrative safety cases. They measure harmlessness, helpfulness, robustness, and over-refusal across different conditions. Based on insights from these experiments, they design CADA, a case-augmented deliberative alignment method that uses reinforcement learning on self-generated safety reasoning chains, effectively training models to reason through safety using examples rather than rigid rule lists.

Result: Explicit, detailed safety codes give inconsistent gains in harmlessness and reliably reduce helpfulness, leading to more over-refusals. In contrast, training models with case-augmented simple safety codes produces more robust, generalized, and balanced safety behavior. The proposed CADA approach further improves harmlessness, strengthens robustness to attacks, and decreases unwarranted refusals, all while maintaining utility on a variety of benchmarks.

Conclusion: Long, code-like safety rule lists are not an effective way to align open-source LLMs: they can hurt helpfulness and do not reliably improve safety. Instead, aligning models through case-augmented reasoning yields better generalization and a more favorable trade-off between harmlessness and helpfulness. CADA operationalizes this idea, offering a practical alternative to rule-only deliberative alignment for improving safety alignment in LLMs without overly constraining their usefulness.

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [75] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: The paper analyzes how 2025 US and EU frontier AI regulations treat high-stakes AI systems that are deployed only inside companies, and identifies regulatory gaps and possible fixes.


<details>
  <summary>Details</summary>
Motivation: Most AI regulation and oversight focuses on AI systems that are released to or interact with external users, where actions are visible and can be scrutinized. However, some of the riskiest uses of advanced AI may occur entirely within companies, e.g., automating R&D, core business processes, or handling sensitive proprietary data. These internal deployments can be less visible to regulators and the public, potentially escaping intended safeguards. The paper is motivated by the need to understand whether existing frontier AI regulatory frameworks in the US and EU adequately cover such internal uses, and what structural reasons cause any gaps.

Method: The authors conduct a conceptual and legal analysis of frontier AI regulations in the United States and European Union as of 2025, focusing on how these frameworks apply to internally-deployed high-capability AI systems. They systematically examine the scope and mechanisms of the regulations and classify where internal deployments are covered or omitted. They then identify and categorize regulatory gaps, and analyze underlying causes in terms of measurability, incentives, and information access. Finally, they conceptually map out potential regulatory and oversight approaches to close these gaps, discussing expected tradeoffs.

Result: The paper finds three primary gaps in how current US and EU frontier AI regulations cover internal deployments: (1) scope ambiguity, which can allow internal systems to fall outside regulatory obligations; (2) reliance on point-in-time compliance assessments, which do not account for the ongoing evolution and adaptation of internal AI systems; and (3) information asymmetries, where firms have far more information than regulators about internal uses, undermining regulatory awareness and oversight. The authors also identify structural reasons these gaps persist, including difficulties in measuring internal risks, misaligned incentives for disclosure and oversight, and limited regulator access to proprietary internal information.

Conclusion: The authors conclude that without explicit attention to internal deployments, frontier AI regulations risk leaving significant high-stakes uses of AI effectively unregulated or under-regulated. They argue that the identified gaps are not accidental but stem from deeper tensions—around what can be measured, how incentives are structured, and what information regulators can realistically obtain. The paper proposes a landscape of possible policy approaches to better govern internal AI uses, each with accompanying tradeoffs in burden, enforceability, and effectiveness. The overarching conclusion is that policy choices about regulating internally deployed frontier AI systems should be made consciously and explicitly, rather than emerging as side effects of frameworks designed primarily for externally deployed systems.

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [76] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: An IoT-based classroom system combines automated attendance with real-time emotion detection to help instructors understand and respond to student engagement during lectures.


<details>
  <summary>Details</summary>
Motivation: Most smart classroom solutions focus narrowly on automating attendance and ignore students’ emotional and cognitive engagement, which prevents instructors from detecting disengagement and adapting teaching strategies in real time. The paper aims to bridge this gap by monitoring both presence and engagement-related emotional states.

Method: The authors design SCASED, an IoT-based system built around a Raspberry Pi camera and OpenCV for face detection, coupled with a finetuned MobileNetV2 model to classify four learning-related emotions: engagement, boredom, confusion, and frustration. They introduce a session-based mechanism that records attendance once per lecture session and then continuously analyzes emotions. Data are sent to a cloud-based dashboard that visualizes attendance and emotion trends for instructors. Emotion recognition performance is evaluated using the DAiSEE dataset.

Result: The proposed system achieves 89.5% accuracy on emotion classification using the DAiSEE dataset. The integrated platform successfully combines reliable automated attendance with continuous monitoring of students’ emotional states, and presents this information via a cloud dashboard for instructor use.

Conclusion: Integrating emotion analytics with traditional attendance tracking can enrich instructors’ understanding of classroom dynamics. The SCASED system demonstrates that an IoT-based approach can provide accurate emotion recognition and useful visualizations, supporting more responsive and adaptive teaching practices in higher education smart classrooms.

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [77] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: The paper proposes advanced PPO-based deep reinforcement learning methods for scheduling battery storage and water heating in dairy farms, using realistic forecasts and adaptive KL control, achieving small but consistent cost and grid-import reductions over baseline RL methods.


<details>
  <summary>Details</summary>
Motivation: Dairy farms are energy intensive and depend heavily on grid electricity. With growing renewable integration and the need to meet SDG 7 (affordable and clean energy), farms must manage energy more sustainably and cost-effectively. However, renewable intermittency and dynamic tariffs make real-time load scheduling difficult, and existing RL methods unrealistically assume full future knowledge and can be unstable in training under variable prices. This motivates a more realistic and stable RL-based scheduling framework for dairy farm loads.

Method: The authors design a Deep Reinforcement Learning framework centered on load scheduling for battery storage and water heating in dairy farms under operational constraints. They introduce two PPO-based variants: (1) Forecast Aware PPO, which uses short-term forecasts of demand and renewable generation derived via hour-of-day and month-based residual calibration; and (2) PID KL PPO, which embeds a proportional–integral–derivative controller to adaptively regulate the KL divergence between policies, stabilizing PPO updates under changing tariffs. These algorithms are trained and evaluated on real-world dairy farm data.

Result: Compared with standard RL baselines, the proposed methods yield improved economic and energy-use outcomes. The approach attains up to 1% lower electricity cost than standard PPO, 4.8% lower than DQN, and 1.5% lower than SAC. For battery scheduling tasks, PPO-based scheduling reduces grid imports by 13.1%, showing that the approach can effectively leverage storage and demand flexibility in practice.

Conclusion: The study concludes that forecast-integrated and adaptively regularized PPO methods provide a practical and scalable solution for intelligent load scheduling in dairy farms. By explicitly accounting for realistic forecast uncertainty and stabilizing policy updates via PID-controlled KL regularization, the framework enhances cost savings and reduces grid dependence, supporting more sustainable energy management in modern dairy farming contexts.

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [78] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: The paper develops scalable backward reachability algorithms for neural network-controlled dynamical systems and combines them with forward reachability to better verify reach-avoid safety properties.


<details>
  <summary>Details</summary>
Motivation: Existing verification of neural feedback systems mainly relies on forward reachability because current backward reachability methods do not scale well. This limits the ability to reason about which initial states inevitably lead to unsafe conditions or to refine safety guarantees. The authors aim to enable efficient backward reachability for neural feedback systems and to integrate it with forward methods for stronger, more precise verification of reach-avoid properties.

Method: The authors design new algorithms that compute both over-approximations and under-approximations of backward reachable sets for dynamical systems controlled by neural networks. These algorithms take into account the closed-loop dynamics and the neural network controller. They then combine these backward analyses with standard forward reachability tools in a single verification framework, allowing information to flow in both temporal directions when checking reach-avoid properties.

Result: The resulting framework can compute backward reachable sets with sound over- and under-approximations and interoperate with existing forward analysis methods. This enables more powerful verification of neural feedback systems, particularly for reach-avoid specifications, and improves precision or scalability compared to using forward analysis alone.

Conclusion: Backward reachability for neural feedback systems can be made practical by the proposed algorithms, and when combined with forward reachability it yields a unified, more effective framework for verifying reach-avoid properties. This bi-directional analysis addresses previous scalability limitations of backward methods and enhances the overall verification capabilities for neural network-controlled dynamical systems.

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [79] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: The paper mechanistically studies why large language models often fail to follow negative constraints (e.g., “don’t use word X”), showing that violation probability depends tightly on how strongly the model wants to say the forbidden word (“semantic pressure”), and identifying two main internal failure modes via layer-wise analysis.


<details>
  <summary>Details</summary>
Motivation: Negative constraints like “do not say X” are a basic yet frequently failed instruction type in large language models, but the reasons for these failures are poorly understood. A mechanistic understanding is important both for safety (avoiding disallowed content) and for improving instruction-following reliability. Existing work mostly documents the failures or proposes heuristics, without explaining how they arise inside the model’s computation.

Method: The authors introduce a quantitative metric called semantic pressure, capturing the model’s intrinsic tendency to produce the forbidden token in a given context. They empirically relate this metric to violation probability, fitting a logistic model over 40,000 samples. Using the logit lens, they track how the logit of the forbidden token evolves across layers under a negative instruction, comparing successful vs failed constraint-following. They decompose failures into two mechanistic modes—priming failure and override failure—by analyzing how the instruction mention and late feed-forward networks affect the target token’s logit. Activation patching experiments (substituting activations of specific layers) identify which layers causally drive failures.

Result: They find that violation probability is tightly predicted by semantic pressure via a logistic relationship: p = σ(-2.40 + 2.27·P0), with a precise bootstrap confidence interval for the slope. Negative instructions do reduce the target token probability in both successes and failures, but much less in failures: about a 5.2 percentage point reduction in violations vs 22.8 in successful constraint-following, a 4.4× asymmetry. Two main failure modes emerge: (1) priming failure (87.5% of cases), where explicitly naming the forbidden word actually activates its representation, boosting its probability; and (2) override failure (12.5% of cases), where late-layer feed-forward networks add a strong positive contribution (~+0.39) to the target logit, roughly 4× stronger than in successes, overpowering earlier suppression. Activation patching shows that layers 23–27 are causally responsible, as swapping their activations reverses the effect of the constraint.

Conclusion: Negative constraints are intrinsically fragile because the act of naming the forbidden word both primes its representation and can be reinforced by late-layer mechanisms. The model does encode a suppression signal from the instruction, but it is often too weak or overridden when semantic pressure is high. This reveals a fundamental design tension in negative prompting: instructing the model not to say a word necessarily activates it internally, increasing the risk of violation. Effective mitigation will likely require architectural or training-level solutions rather than purely prompt-level negative constraints.

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [80] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: The paper introduces MemoBrain, an executive memory module for tool-augmented LLM agents that maintains a compact, dependency-aware reasoning backbone to support long-horizon tasks under context limits, improving performance on several challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented LLM agents performing complex, multi-step reasoning generate long reasoning traces and intermediate tool outputs that quickly exceed context limits. Without explicit memory, this leads to loss of logical continuity, disrupted task alignment, and degraded performance on long-horizon tasks. The authors argue that memory should be treated as a central component for coherent, goal-directed reasoning, not just a convenience or efficiency optimization.

Method: The authors propose MemoBrain, an executive memory model that runs alongside a reasoning agent. It builds a dependency-aware memory graph over reasoning steps, encoding salient intermediate states and their logical relations. During execution, MemoBrain: (1) prunes invalid or obsolete steps, (2) folds or compresses completed sub-trajectories, and (3) preserves a compact, high-salience “reasoning backbone” under a fixed context budget. It functions as a co-pilot that continually curates and updates the working context rather than allowing unstructured accumulation of past tokens.

Result: On long-horizon benchmarks such as GAIA, WebWalker, and BrowseComp-Plus, agents augmented with MemoBrain achieve consistent performance gains compared to strong baselines that lack such an executive memory mechanism or rely on simpler context management strategies.

Conclusion: Explicit, structured memory is crucial for long-horizon reasoning in tool-augmented LLM agents. By maintaining a dependency-aware, compact backbone of salient reasoning steps and actively managing the working context, MemoBrain enables more coherent, goal-directed reasoning and improves performance on challenging benchmarks. Memory should therefore be designed as an integral, cognitively-inspired control component in agent frameworks, not as a peripheral add-on.

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [81] [MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness](https://arxiv.org/abs/2601.08118)
*Ashutosh Hathidara,Julien Yu,Vaishali Senthil,Sebastian Schreiber,Anil Babu Ankisettipalli*

Main category: cs.AI

TL;DR: The paper introduces MIRRORBENCH, a modular benchmark to evaluate how human-like LLM-based user simulators are, independently of task success, showing systematic gaps between simulated and real user utterances.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used as user simulators for evaluating conversational systems and for generating training data, but naive prompts lead to verbose and unnatural behavior. There is currently no principled, standardized way to measure how closely these user proxies resemble real human users, especially decoupled from downstream task performance. The authors want a reproducible, extensible benchmark to rigorously evaluate and compare such simulators across tasks and datasets.

Method: They design MIRRORBENCH, a benchmarking framework with a modular execution engine and typed interfaces that supports pluggable user proxies, datasets, tasks, and metrics. The framework is variance-aware, supports multiple backends and caching, and provides observability. For evaluation, they include three lexical diversity metrics (MATTR, Yule’s K, HD-D) and three LLM-judge metrics (GTEval, Pairwise Indistinguishability, Rubric-and-Reason) to compare simulated user utterances to real human utterances across multiple open conversational datasets.

Result: Using MIRRORBENCH on four open datasets, they obtain variance-aware evaluations showing consistent, systematic differences between LLM-based user proxies and real human users. The metrics quantify where and how current simulators diverge from human-like language behavior, indicating that even advanced LLM proxies are not yet sufficiently human-like in conversational settings.

Conclusion: MIRRORBENCH provides a reproducible, extensible, and open-source framework for evaluating user simulators on human-likeness rather than task success. It standardizes how researchers can plug in different user proxies and metrics, revealing significant gaps between current LLM-based user proxies and real users, and offers tooling (CLI, configuration management, caching, reporting) to support future research and improvement of user simulation methods.

Abstract: Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.

</details>


### [82] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: The paper studies how vehicles change lanes after a crash blocks a lane, shows these maneuvers are slower, riskier, and often face non‑yielding behavior, and proposes a graph‑attention, interaction‑aware trajectory prediction model that improves prediction accuracy and crash‑risk assessment over baselines.


<details>
  <summary>Details</summary>
Motivation: Post‑crash lane changes, where vehicles must bypass a crash‑blocked lane, have unique risks and interaction patterns (especially non‑yielding behavior in the target lane) that are not characterized by existing lane‑change studies or models. Without understanding and modeling these specific behaviors, current trajectory prediction and safety assessment tools may underestimate crash risk and perform poorly in post‑crash contexts. The paper aims to fill this empirical and modeling gap to support safer traffic management and automated driving in crash scenes.

Method: 1) Build a post‑crash lane‑change (LC) dataset by extracting detailed vehicle trajectories from drone videos recorded after crashes. 2) Perform empirical comparisons of post‑crash LCs with mandatory LCs (MLCs) and discretionary LCs (DLCs) in terms of duration, speed at insertion, crash risk, and yielding vs. non‑yielding behavior of the new follower in the target lane. 3) Propose a trajectory prediction framework centered on a graph‑based attention module that explicitly treats yielding behavior as an auxiliary interaction‑aware prediction task. 4) Couple this module with a conditional variational autoencoder (CVAE) and a Transformer‑based decoder to produce multi‑step trajectory predictions for the lane‑changing vehicle. 5) Evaluate performance against baseline models using metrics like average displacement error (ADE) and final displacement error (FDE), and assess crash‑risk analysis quality including false crash rate and conflict prediction accuracy. 6) Test model transferability on additional datasets from other sites.

Result: Empirical analysis shows post‑crash LCs have longer lane‑change durations, lower speeds at lane insertion, and higher crash risks than MLCs and DLCs. Non‑yielding behavior by the new follower in the target lane is highly prevalent in post‑crash LCs (79.4%) compared with DLCs (21.7%) and MLCs (28.6%). The proposed interaction‑aware, graph‑attention‑based CVAE+Transformer framework achieves over 10% improvement in both average displacement error and final displacement error across multiple prediction horizons relative to existing baselines. It also yields better safety assessment, with lower false crash rates and higher conflict prediction accuracy. The model demonstrates transferability when applied to post‑crash LC data from different sites.

Conclusion: Post‑crash lane changes form a distinct, high‑risk class of maneuvers characterized by slower and longer executions and frequent non‑yielding behavior from vehicles in the target lane. Properly modeling the interaction mechanisms—especially yielding decisions—is crucial for accurate trajectory prediction and safety assessment in post‑crash scenarios. The proposed graph‑based attention module, integrated with CVAE and Transformer components and trained with an auxiliary yielding‑behavior task, significantly improves prediction accuracy and crash‑risk analysis over baseline models and generalizes to different locations. This framework can support more reliable traffic management and automated driving behavior planning around crash scenes.

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [83] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: They propose a memory system for AI companions on low-power edge devices that splits processing into fast, lightweight online dialogue and slower, intensive memory processing during idle times, plus a benchmark; this improves performance even with a small quantized model.


<details>
  <summary>Details</summary>
Motivation: Existing AI companion and memory systems assume ample compute and low latency network access, which is unrealistic for fully embedded edge devices such as phones, wearables, or robots. These devices have tight memory, compute, and power limits, as well as strict latency requirements for real-time interaction. Conventional long-context or continuous retrieval approaches are too heavy. There is a need for a memory and interaction paradigm that can provide long-term personalized behavior and good user experience while operating under these embedded constraints.

Method: They propose a two-phase memory paradigm for AI companions on edge devices. In the active phase (when the user is interacting), the system focuses on low-latency conversation using lightweight retrieval over existing stored memories and current context, avoiding heavy processing. In the inactive phase (when the user is not interacting), the system runs more computationally intensive procedures over past full conversation sessions—including memory extraction, consolidation, and maintenance. They also design an AI Companion benchmark that jointly evaluates dialog quality and memory abilities, and use it to compare their system against baselines.

Result: Using a small, quantized model (Qwen2.5-7B-Instruct with int4 quantization), their system with the proposed memory paradigm outperforms the same base LLM without memory on most evaluation metrics, and achieves performance comparable to GPT-3.5 with a 16k context window on the new benchmark, despite running under much tighter hardware constraints.

Conclusion: A phase-based memory paradigm that separates low-latency lightweight retrieval during active use from heavier memory operations during idle periods enables effective AI companions on constrained edge devices. This approach preserves conversational quality and long-term personalization while meeting latency and compute limits. Their benchmark demonstrates that even a weak, quantized model can rival a much stronger remote model with large context when equipped with the proposed memory system.

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [84] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse is a hierarchical multi-agent framework for autonomously resolving last-mile delivery disruptions, orchestrated with LangGraph and evaluated via an LLM-as-a-Judge protocol on 30 complex real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Last-mile delivery frequently suffers from complex, real-world disruptions (e.g., delays, misrouting, access issues) that traditional deterministic systems handle poorly. There is a need for more autonomous, adaptive, and explainable AI systems that can manage such disruptions at scale and in real time. The paper aims to address this gap by proposing an agentic architecture tailored to these operational challenges and grounded in real user pain points extracted from thousands of reviews.

Method: The authors design Project Synapse as a hierarchical multi-agent system: a central Resolution Supervisor agent decomposes disruption-resolution tasks into smaller subtasks, which are handed off to specialized worker agents for tactical actions. They use LangGraph to orchestrate these agents and to support complex, potentially cyclical workflows. For evaluation, they construct a benchmark of 30 complex disruption scenarios derived from qualitative analysis of 6,000+ real-world user reviews, and assess system performance using an LLM-as-a-Judge evaluation pipeline with explicit techniques to mitigate judgment bias.

Result: The framework is successfully implemented and run on the curated benchmark of 30 disruption scenarios. Using an LLM-as-a-Judge evaluation with bias mitigation, the authors obtain quantitative scores that indicate the system’s capability to autonomously generate reasonable resolution strategies for last-mile delivery disruptions. Details such as comparative baselines or exact metrics are not in the abstract but the implication is that Synapse performs robustly across diverse disruption types.

Conclusion: Project Synapse demonstrates that a hierarchical, LangGraph-orchestrated multi-agent architecture can effectively tackle complex last-mile delivery disruptions in an autonomous way. The constructed benchmark and LLM-as-a-Judge evaluation protocol offer a reproducible way to study and compare similar systems. The work suggests that agentic frameworks grounded in real-world feedback (user reviews) are a promising direction for operational logistics AI and could be extended to other domains with complex, multi-step disruption management needs.

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [85] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: The paper proposes a model-based hierarchical multi-agent reinforcement learning framework that uses an accurate environment model and LLM-derived semantic code features to perform thermal- and energy-aware DVFS plus task-to-core scheduling on multicore embedded platforms, achieving large gains in energy efficiency, makespan, and decision latency without per-workload profiling.


<details>
  <summary>Details</summary>
Motivation: Thermal and energy management in embedded multicore systems requires tuning DVFS and task-to-core allocation. Existing methods either use coarse utilization-based heuristics that ignore stall behavior, leading to suboptimal decisions, or rely on expensive, offline profiling to build lookup tables, which blocks runtime adaptation and zero-shot deployment on new workloads. There is a need for an adaptive, learning-based scheduler that can generalize to unseen OpenMP workloads and provide fast, low-latency decisions while being aware of thermal dynamics and performance trade-offs.

Method: The authors design a model-based hierarchical multi-agent reinforcement learning framework inspired by Dyna-Q. The scheduling problem is decomposed across two cooperative agents to factorize the exponential action space, enabling sub-second decision latency after initial setup. They build an explicit environment model using regression to predict thermal evolution and performance states under different scheduling and DVFS actions. To eliminate workload-specific profiling, they introduce an LLM-based static code analysis pipeline that extracts 13 semantic features from OpenMP source code without executing it. These features condition the environment model so that, for a trained platform, it can generate synthetic training data for new workloads in a zero-shot way. The RL component blends direct interaction with model-based planning for faster convergence than purely model-free approaches.

Result: On BOTS and PolybenchC benchmarks running on several heterogeneous platforms (NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, Intel Core i7), the proposed framework attains up to 7.09x higher energy efficiency and 4.0x shorter makespan compared to the Linux ondemand DVFS governor. The hierarchical MARL scheme reaches 358 ms decision latency for subsequent scheduling steps, with initial decisions taking 3.5–8.0 s due to a one-time LLM feature-extraction cost. Convergence speed is about 20x faster than model-free RL baselines. Compared with table-based profiling approaches, the first-decision latency is 8,300x lower, showing that the method is suitable for dynamic and time-constrained embedded environments.

Conclusion: The paper concludes that combining model-based hierarchical multi-agent RL with LLM-driven semantic characterization of OpenMP programs enables practical, thermally aware and energy-efficient scheduling on multicore embedded systems without per-workload profiling. The accurate, regression-based environment model plus Dyna-Q style planning allows fast convergence and low runtime decision latency, while the code-derived semantic features provide zero-shot generalization to new workloads on a given platform. Overall, the approach significantly outperforms a standard Linux governor and profiling-based methods in both performance and energy efficiency, making it a promising solution for dynamic embedded platforms.

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [86] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: The paper presents EvoEnv, a dynamic evaluation environment to test how well multi-modal large language model agents perform and adapt in realistic, continuously changing settings instead of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Most current work on multi-modal large language model agents focuses on maximizing benchmark performance in fixed, static settings. This leaves a gap in understanding how robust and reliable these agents are when deployed in stochastic real-world environments, where tasks arrive over time, priorities change, information is incomplete, and agents must improve from experience. The authors aim to provide an evaluation framework that captures these dynamic aspects—task scheduling, exploration under uncertainty, and continual learning—to assess reliability rather than just peak performance.

Method: The authors design EvoEnv, a dynamic evaluation environment that simulates a trainee agent operating in a novel, evolving setting. The environment generates streaming tasks of varying priority and rule-based tasks that change over time. EvoEnv evaluates agents along three axes: (1) context-aware scheduling, by presenting a stream of tasks with different priorities and constraints; (2) prudent information acquisition, by requiring agents to actively explore and query the environment to reduce hallucinations and make informed decisions; and (3) continuous evolution, by enabling agents to distill generalized strategies from dynamically generated tasks and learn over time. They plug existing cutting-edge MLLM-based agents into EvoEnv and measure their behaviors and performance across these dimensions.

Result: The experiments reveal that state-of-the-art multi-modal agents, which perform well on static benchmarks, struggle significantly in the EvoEnv dynamic setting. Their weaknesses are particularly pronounced in active exploration—failing to appropriately query or gather missing information—and in continual learning—failing to generalize strategies from prior tasks to new ones. These deficits show that current agents are not yet robust or reliable enough for realistic, production-like deployment scenarios involving uncertainty and change.

Conclusion: The paper concludes that reliable deployment of MLLM-based agents requires evaluation frameworks that go beyond static accuracy benchmarks and instead test agents in dynamic, evolving environments. EvoEnv provides such a framework, focusing on task scheduling, active exploration, and continual learning. By demonstrating substantial performance gaps for current agents, the work motivates future research on more robust agent architectures and learning algorithms, and positions EvoEnv as a benchmark for measuring progress toward real-world-ready AI agents.

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [87] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: The paper proposes HS2C, a homophily-aware structural and semantic compression framework that improves LLM reasoning on text-attributed graphs by compressing neighborhoods in a principled, structure-guided way instead of random sampling.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for text-attributed graphs must fit node neighborhoods into limited context windows. They commonly use random node/edge dropping to reduce size, which introduces noise and unstable reasoning. The authors believe that graph structure and homophily can be exploited to compress information more intelligently, preserving useful signal while reducing redundancy and noise, thereby improving LLM performance and scalability.

Method: HS2C first performs a global, homophily-aware structural compression using hierarchical partitioning guided by Structural Entropy minimization. This discovers cohesive, homophilic communities and removes stochastic connectivity noise. Then, on the semantic side, the framework encodes these structural communities as types and prompts the LLM to perform differentiated semantic aggregation per community type. Texts from nodes in the same homophilic community are aggregated into concise community-level representations, aligning with the target node’s homophilic context while dropping redundant or irrelevant background. These compressed structural and semantic summaries are then used as LLM input for downstream tasks.

Result: On 10 node-level benchmarks and multiple LLMs of different sizes and families, HS2C achieves higher compression rates while also improving downstream inference accuracy compared to baselines that use random sampling or less structured compression. The method also transfers to 7 diverse graph-level benchmarks, where it continues to show competitive or superior performance, indicating robustness and scalability across tasks and model settings.

Conclusion: Leveraging graph homophily through structural entropy–based partitioning and community-aware semantic aggregation provides a principled way to compress text-attributed graph neighborhoods for LLMs. This structural and semantic compression simultaneously reduces input size and enhances reasoning accuracy, demonstrating a scalable and general approach for integrating LLMs with graph-structured data.

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [88] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: The paper analyzes fairness issues in Official International Mahjong when played as single online rounds and proposes AI-driven rule adjustments, especially for first-mover advantage and subgoal scoring.


<details>
  <summary>Details</summary>
Motivation: Traditional Official International Mahjong rules assume repeated face-to-face play with stable groups and position rotation, which helps balance advantages such as seat order across multiple rounds. In online environments, players have fragmented time and frequently changing opponents, so these balancing mechanisms no longer function effectively. The authors are motivated to systematically evaluate fairness under online single-round conditions and redesign rules so that Mahjong remains balanced and enjoyable when played digitally.

Method: The authors use a world champion–level Mahjong AI to run large-scale self-play simulations under the current Official International rules. They then perform statistical analysis on the simulation data to quantify first-mover advantage and to detect imbalances in the scoring values assigned to subgoals (specific yaku/tile patterns). Using these insights, they design new rules, including compensatory points for the first player and adjusted subgoal scores, and implement these changes in an online version of the game for further use and testing.

Result: Simulation and data analysis show a measurable first-mover advantage in Official International Mahjong and inadequacies in the current subgoal scoring system when the game is played as single, isolated rounds. The proposed rule changes—compensation points for the first player and refined scoring for different patterns—are shown to improve fairness in the AI self-play setting and are practical to implement for online platforms.

Conclusion: The paper concludes that Official International Mahjong, as currently defined, is not fully balanced for online single-round play. By leveraging AI self-play data, the authors develop and justify a revised ruleset that better addresses first-mover advantage and subgoal scoring issues, providing a more equitable experience for online players. Their work demonstrates a general approach for using high-level game AIs as tools to audit and rebalance traditional game rules in new play environments.

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [89] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: The paper introduces SANC(E3), an axiomatic framework where representational units emerge automatically by minimizing an energy functional under finite capacity, unifying perception, imagination, prediction, and action.


<details>
  <summary>Details</summary>
Motivation: Most current AI systems assume fixed primitive units such as tokens, pixels, or sensor channels. This sidesteps the foundational question of how the system’s own representational units are discovered, stabilized, and adapted under limited computational resources. The authors aim to provide a principled, general account of how such units can self-organize, enabling more human-like general intelligence that flexibly restructures experience.

Method: The authors define SANC(E3), an axiomatic framework with five core axioms: finite activation capacity, association via co-occurrence, similarity-based competition between candidate units, confidence-based stabilization of useful units, and a reconstruction–compression–update trade-off, all governed by minimizing an explicit energy functional E3. They distinguish a small set of system tokens (e.g., {here, now, I} and sensory sources) from all other tokens, which must emerge through self-organization during co-occurring events. They introduce a pseudo-memory-mapped I/O mechanism so that internally replayed Gestalts (patterns) are processed by the same axioms as external input. From these axioms, they derive twelve formal propositions that characterize emergent phenomena such as category formation, hierarchy, and high-level cognition as instances of Gestalt completion under E3 minimization.

Result: Within this theoretical framework, the authors show—at a formal, axiomatic level—that a wide range of cognitive capabilities are implied by or consistent with E3 minimization: spontaneous emergence of representational units, category formation from co-occurrence, hierarchical organization of units, unsupervised learning without external labels, and high-level cognitive processes (imagination, prediction, planning) treated as Gestalt completion. The twelve derived propositions mathematically connect the axioms to these emergent properties, arguing that they follow from the competitive, capacity-limited, energy-minimizing dynamics.

Conclusion: The paper concludes that SANC(E3) provides a unified, resource-conscious theoretical basis for how representational units can self-organize and stabilize in a general intelligence system, without assuming fixed primitives. By treating perception, imagination, prediction, planning, and action as different modes of the same Gestalt-completion process under E3 minimization, the framework offers a coherent account of both low-level and high-level cognition. It suggests that many hallmarks of intelligence—categories, hierarchies, and unsupervised structure learning—can be derived from a small set of axioms about finite capacity, competition, and reconstruction–compression trade-offs.

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [90] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces MPCI-Bench, a multimodal benchmark to evaluate how language-model agents respect contextual privacy norms, revealing that current models often leak sensitive visual data and struggle to balance privacy with utility.


<details>
  <summary>Details</summary>
Motivation: As language models become proactive agents that handle personal data, it is essential to evaluate whether they follow social and contextual privacy norms, especially beyond text-only settings. Existing benchmarks focus mainly on refusals in text scenarios and ignore multimodal risks and the core trade-off between protecting privacy and providing useful assistance, leaving a gap in assessing realistic agent behavior.

Method: The authors construct MPCI-Bench, a Multimodal Pairwise Contextual Integrity benchmark. They create paired positive and negative instances from the same visual inputs across three levels: (1) Seed judgments that encode normative CI expectations, (2) Story-level, context-rich reasoning tasks, and (3) executable agent action Traces that reflect real agent behavior. A Tri-Principle Iterative Refinement pipeline is used to enforce high data quality. State-of-the-art multimodal models are then evaluated on this benchmark.

Result: Experiments with leading multimodal models show that they systematically fail to find an appropriate balance between privacy and utility and exhibit a strong modality leakage gap, leaking sensitive information from images substantially more often than from text. These failures appear across the different tiers of the benchmark.

Conclusion: MPCI-Bench provides the first structured, multimodal, agent-oriented benchmark for Contextual Integrity, highlighting significant privacy shortcomings in current multimodal agents, especially around visual data. By releasing MPCI-Bench, the authors aim to enable further research into more privacy-aware, context-sensitive agent behavior and better evaluations of the privacy–utility trade-off in multimodal systems.

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [91] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: The paper argues for shifting from hand-crafted numeric rewards to language-based objective specifications in multi-agent reinforcement learning, using large language models and verifiable language-mediated supervision to define, adapt, and align rewards with human intent.


<details>
  <summary>Details</summary>
Motivation: Reward engineering in multi-agent reinforcement learning is difficult due to ambiguous credit assignment, non-stationary environments, and rapidly growing interaction complexity. Traditional hand-crafted numeric rewards are brittle and labor-intensive, especially as systems scale. Recent progress in large language models and language-mediated supervision suggests a promising alternative, motivating a rethinking of how objectives are specified and aligned with human intent.

Method: The paper provides a conceptual and survey-style analysis rather than a new algorithm. It synthesizes recent work such as LLM-based reward synthesis (e.g., EUREKA), online reward adaptation (e.g., CARD), and Reinforcement Learning from Verifiable Rewards (RLVR). It organizes this literature along three key dimensions—semantic reward specification, dynamic reward adaptation, and human-intent alignment—and uses this framework to characterize a shift from numeric to language-based rewards in multi-agent settings.

Result: The main results are conceptual: (1) demonstrating that language-based reward specifications, powered by LLMs, can meaningfully replace or complement hand-crafted numeric rewards; (2) highlighting that RLVR and related approaches show empirical viability of language-mediated supervision; and (3) identifying concrete benefits (richer semantics, adaptability, better alignment) and key limitations (computational overhead, hallucinations, and scalability) for multi-agent reinforcement learning.

Conclusion: The paper concludes that multi-agent coordination can be increasingly driven by shared semantic representations expressed in language, rather than by manually engineered numeric reward signals. It advocates a research agenda centered on semantic, dynamically adaptable, and intent-aligned reward specifications via LLMs, while emphasizing the need to address challenges in robustness, efficiency, and large-scale multi-agent deployment.

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [92] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.AI

TL;DR: The paper proposes a Large AI Model–guided Deep Reinforcement Learning (LAM-DRL) framework for Non-Terrestrial Networks (NTN), where an LLM shapes the DRL reward and significantly improves performance over traditional DRL and heuristics, especially under extreme weather.


<details>
  <summary>Details</summary>
Motivation: Non-Terrestrial Networks face highly dynamic and uncertain environments (e.g., changing weather) that make traditional heuristic and standard DRL solutions less robust and hard to generalize. Large AI Models, especially LLMs, can provide high-level reasoning and generalization across scenarios, but they are not directly optimized for low-level control tasks in NTN. The motivation is to combine the strengths of LLMs (high-level guidance, generalization) with DRL (fine-grained control, numerical optimization) to achieve better throughput, fairness, and reliability in NTN without extensive task-specific training.

Method: The authors design a hybrid LAM-DRL framework. A Large Language Model acts as a high-level coordinator: it observes abstracted information about the NTN environment and generates textual guidance or instructions. These textual outputs are transformed into reward-shaping signals that guide a Deep Reinforcement Learning agent during training. The DRL agent still interacts with the NTN environment and learns a control policy, but its reward function is dynamically adjusted according to the LLM’s guidance, effectively encoding expert-like or generalized knowledge. Performance is compared against traditional DRL and heuristic baselines under different weather scenarios.

Result: The proposed LAM-DRL architecture achieves substantial gains over traditional DRL and heuristic methods. In nominal (normal) weather conditions, LAM-DRL improves performance metrics by about 40% compared to heuristics. In extreme weather conditions, where the environment is more challenging and volatile, the improvement rises to around 64%. The metrics evaluated include throughput (overall data rate), fairness (balanced resource allocation among users), and outage probability (likelihood of service failure).

Conclusion: Coupling a Large AI/Language Model with a DRL agent through reward shaping can significantly improve the performance and robustness of resource/control decision-making in Non-Terrestrial Networks. The LLM’s high-level textual guidance helps the DRL agent learn better policies, particularly under harsh and uncertain conditions such as extreme weather. This demonstrates the potential of LAM-guided DRL architectures for complex communication network management and suggests a promising direction for future AI-driven NTN optimization systems.

Abstract: Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [93] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3 is a diagnostic benchmark of 454 vignettes to evaluate large language models’ causal judgment across Pearl’s Ladder, with fine-grained metrics for utility, safety, and wise refusal.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLM causal reasoning are coarse and don’t clearly separate being appropriately cautious from being overly skeptical or hallucinating causal links. The authors want a benchmark that can reveal specific failure modes in causal judgment, especially across different causal levels (associational, interventional, counterfactual) as formalized by Pearl.

Method: They design T3, a benchmark of 454 expert-written causal vignettes spanning Pearl’s three levels of causality. Each instance is labeled to allow decomposition of model behavior into (1) Utility: correctly endorsing valid causal links, (2) Safety: correctly rejecting invalid links, and (3) Wise Refusal: appropriately refusing to commit when evidence is underdetermined. They test multiple frontier LLMs on T3, analyze error patterns, and then evaluate a process-verified reasoning protocol (RCA) to see how it changes performance on the benchmark.

Result: On T3, they identify two systematic issues: (1) a “Skepticism Trap” at Level 1 where safety-tuned models (e.g., Claude Haiku) incorrectly reject around 60% of valid causal links; and (2) a “Scaling Paradox” at Level 3 where a larger model (GPT-5.2) performs much worse than a smaller one (GPT-4-Turbo) on ambiguous counterfactuals—by about 55 points—because it becomes overly hesitant and indecisive instead of hallucinating. They also show that their structured, process-verified reasoning protocol (RCA) measurably restores more decisive and appropriate causal judgments as captured by T3’s metrics.

Conclusion: T3 provides a fine-grained, causality-grounded diagnostic for LLM causal judgment, revealing important pathologies like excessive skepticism and non-monotonic scaling at higher causal levels. The benchmark is sensitive enough to validate interventions such as process-verified reasoning protocols, which can improve decisiveness without sacrificing safety, and thus serves as a tool for developing more trustworthy causal reasoning in LLMs.

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [94] [VGG Induced Deep Hand Sign Language Detection](https://arxiv.org/abs/2601.08262)
*Subham Sharma,Sharmila Subudhi*

Main category: cs.AI

TL;DR: The paper proposes a CNN-based hand gesture recognition system using VGG-16 with transfer learning and data augmentation, achieving about 98% accuracy on 10 gesture classes.


<details>
  <summary>Details</summary>
Motivation: To improve human-computer interaction and support communication via sign language for differently-abled individuals by developing a highly accurate automatic hand gesture recognition system.

Method: The authors use VGG-16, a convolutional neural network, implemented with Python and Keras. They apply transfer learning and image data augmentation on a widely used image dataset to train the model. They validate it on the NUS hand gesture dataset (10 classes). They then create an additional 10-class test dataset using Google’s open-source API to capture diverse hand gesture images and evaluate performance.

Result: The combined use of transfer learning and image data augmentation with VGG-16 achieves around 98% classification accuracy on 10-class hand gesture recognition tasks across validation and test datasets.

Conclusion: Leveraging a pre-trained VGG-16 model with transfer learning and robust data augmentation is an effective approach for high-accuracy hand gesture recognition, making it promising for assistive communication systems for differently-abled users.

Abstract: Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.

</details>


### [95] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: The paper introduces a theoretical framework for learning to control tool-augmented LLM systems when only a small subset of many possible tools is useful, and provides compressed-sensing-style guarantees for sparse policy learning.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented LLM agents must choose among a huge number of possible tools, APIs, or documents, but only a small fraction are relevant to any given task distribution. Existing learning theory does not adequately address this high-dimensional, sparse-control setting, leading to unstable and sample-inefficient behavior (e.g., prompt-only controllers). The paper aims to build a formal framework that captures this sparsity and to obtain sample-complex learning guarantees that scale only logarithmically with the number of available tools.

Method: The authors formalize the problem as Sparse Agentic Control (SAC), where optimal policies can be represented as block-sparse over a large discrete action set and rewards depend on sparse main effects and optional sparse synergies among actions. They propose learning policies via an ℓ_{1,2}-regularized convex surrogate objective to enforce group sparsity over tools. Using tools from high-dimensional statistics and compressed sensing, they analyze this estimator under conditions such as Policy Restricted Strong Convexity (Policy-RSC), incoherence, and beta-min, and also extend the analysis to settings with partial observability by modeling LLMs through belief/representation errors.

Result: Under Policy-RSC, the estimation and value suboptimality of the learned sparse policy scale as k * sqrt(log M / T), where k is the sparsity level, M is the number of tools/actions, and T is the sample size. They prove exact recovery of the relevant tool set (support recovery) when T exceeds k log M and standard incoherence and beta-min conditions hold, via a primal-dual witness argument. They further show that any dense (non-sparse) policy class provably requires on the order of M samples, explaining poor scaling and instability of prompt-only or dense controllers. For partially observed settings, they show that the role of the LLM reduces to a belief or representation error ε_b, which adds only an O(ε_b) term to performance while keeping the logarithmic dependence on M. They also sketch or provide results for tuning-free, online, robust, group-sparse, and interaction-aware variants of SAC.

Conclusion: Sparse Agentic Control provides a theoretically grounded way to design and analyze tool-augmented LLM controllers in extremely large action spaces by exploiting sparsity. ℓ_{1,2}-regularized policy learning achieves near-optimal sample complexity with only logarithmic dependence on the number of tools and can exactly recover the relevant tool set under reasonable conditions, while dense approaches are fundamentally sample-inefficient. In partially observable environments, the LLM’s quality enters only as an additive representation-error term, suggesting that once beliefs are reasonably accurate, the main challenge is sparse control rather than language modeling. The framework extends naturally to more practical variants, indicating a path toward stable, sample-efficient, and interpretable tool-using LLM agents.

Abstract: Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [96] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: The paper introduces ToolACE-MCP, a training pipeline for history-aware routing agents that can navigate large tool ecosystems (like the Agent Web with MCP) more accurately and robustly.


<details>
  <summary>Details</summary>
Motivation: As the Agent Web and MCP expand, agents can access exponentially more tools, but existing routing and orchestration architectures struggle to scale, generalize, and handle noisy, open-ended environments. There is a need for a universal, robust, and scalable mechanism to route user requests and intermediate subtasks to the right tools and agents in large, collaborative ecosystems.

Method: The authors construct a dependency-rich candidate graph of tools and interactions, from which they synthesize multi-turn trajectories capturing realistic tool usage and histories. They then train history-aware router models on these synthetic trajectories so that the routers can leverage dynamic conversational and tool-use context. The result is a plug-and-play Light Routing Agent (ToolACE-MCP) designed to integrate with MCP-based ecosystems and perform tool selection and orchestration effectively across large candidate spaces.

Result: On real-world benchmarks MCP-Universe and MCP-Mark, ToolACE-MCP outperforms existing methods in routing accuracy and overall task performance. It also demonstrates that it can extend to multi-agent collaboration with only minimal additional adaptation, and shows strong robustness when exposed to noisy conditions and very large sets of candidate tools or agents.

Conclusion: ToolACE-MCP provides an effective, scalable solution for routing in large, open-ended tool and agent ecosystems. Its strong empirical results, robustness to noise, and ability to generalize to multi-agent scenarios suggest it is a promising foundation for universal orchestration mechanisms in the emerging Agent Web built on top of MCP.

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [97] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: The paper studies how to efficiently discover a small subset of useful actions in environments with extremely large action spaces, modeling it as a block-sparse recovery problem and providing theoretical guarantees for a greedy algorithm that recovers relevant actions with sample complexity only logarithmic in the total number of actions.


<details>
  <summary>Details</summary>
Motivation: In modern agentic systems (e.g., tool-augmented language models), there can be thousands of possible actions/APIs, but only a small subset significantly affects performance in a particular deployment. Naively searching over all actions is intractable, so there is a need to understand when and how we can reliably identify the few important actions from limited data, giving a principled basis for pruning the action space.

Method: The authors model the setting using a contextual linear reward model with a structured sparsity assumption: only a small number of actions have nonzero effects across latent states. They cast the problem of identifying these actions as a block-sparse recovery task and analyze a greedy selection algorithm inspired by Orthogonal Matching Pursuit (OMP). Under conditions on incoherence, signal strength, and coverage of actions by the observed data, they prove exact recovery of the relevant action set and derive estimation error bounds for the refitted parameters, which induce a nearly optimal decision rule for new latent states.

Result: They show that the greedy OMP-style algorithm exactly recovers the set of relevant actions with high probability, using a number of samples that depends polynomially on the sparsity level and latent dimension but only logarithmically on the total number of actions. They further provide bounds on the estimation error of the parameters after refitting on the selected actions and show that the resulting policy achieves near-optimal performance on unseen latent states. In addition, they derive information-theoretic lower bounds showing that both sparsity and sufficient action coverage are necessary for sample-efficient recovery.

Conclusion: Sparse action discovery is identified as a key principle for decision-making in large action spaces. The paper offers a theoretical framework showing that, under natural structural assumptions, one can efficiently and reliably prune a huge action space down to the few actions that matter, with guarantees on both identification and downstream decision quality. This establishes a foundation for principled action pruning in large-scale agentic systems such as tool-augmented language models.

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [98] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: The paper introduces OpenMic, a multi-agent system that generates 3–5 minute Chinese stand-up comedy routines and corresponding narrated videos from user-provided life topics, using iterative planning, RAG, and a fine-tuned JokeWriter model.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese humor resources and models focus mostly on understanding or short-form joke generation, not on long, performable stand-up sets that require cultural grounding, timing, and stage cues. Directly supervising models on these datasets is misaligned with the real task of end-to-end stand-up generation, especially when multi-step reasoning, callbacks, and performance aspects are needed.

Method: OpenMic is built as an end-to-end multi-agent system on AutoGen. It coordinates several specialized agents through multi-round, iterative loop-planning to co-optimize humor quality, timing, and performability (e.g., stage and performance cues). The system integrates retrieval-augmented generation (RAG) to ground the generated content in relevant material and expand ideas. Additionally, the authors fine-tune a dedicated JokeWriter model to learn stand-up-specific structures such as setup-punchline patterns and long-range callbacks.

Result: The system can transform a user’s everyday-life topic into a coherent, 3–5 minute Chinese stand-up routine and then convert it into a narrated comedy video, showing that multi-agent orchestration plus RAG and specialized fine-tuning can handle long-form, performance-oriented humor generation better than prior approaches that rely purely on generic humor datasets.

Conclusion: End-to-end Chinese stand-up comedy generation benefits from a multi-agent architecture with iterative planning, retrieval-augmented grounding, and a fine-tuned JokeWriter specialized in stand-up structures. This design mitigates the mismatch between existing humor datasets and the demands of long-form, performable stand-up, enabling more culturally grounded, well-timed, and stage-ready comedic content.

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [99] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: The paper introduces AtomMem, a learning-based memory management framework for agents that models memory as atomic CRUD operations and learns dynamic policies to manage memory, achieving better performance than static, hand-crafted memory workflows on long-context benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agent memory mechanisms typically use static, hand-designed workflows, which restrict agents’ ability to generalize and adapt to diverse long-horizon, real-world tasks. There is a need for a more flexible, learnable memory framework that can autonomously discover task-aligned strategies instead of relying on fixed routines.

Method: The authors propose AtomMem, which conceptualizes memory management as a dynamic decision-making process. They decompose memory operations into atomic CRUD (Create, Read, Update, Delete) primitives, turning the memory workflow into a sequence of learnable decisions. AtomMem is trained using a combination of supervised fine-tuning and reinforcement learning to learn a policy that autonomously orchestrates these CRUD operations according to task demands.

Result: On three long-context benchmarks, the AtomMem-8B model consistently outperforms previous memory methods that rely on static workflows, indicating the effectiveness of the dynamic, learning-based memory management approach.

Conclusion: Reframing memory management as a learnable CRUD-based decision process allows agents to autonomously discover structured, task-aligned memory strategies, offering clear advantages over predefined, static memory workflows in long-horizon tasks.

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [100] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: The paper identifies and formalizes a core epistemic failure mode in LLM-based agent architectures called semantic laundering, showing that such systems inevitably grant unjustified beliefs high status due to their structural design.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are increasingly used for decision-making, planning, and evaluation, but current architectures mix up mere transmission of information with genuine epistemic justification. This creates a risk: systems may treat weakly supported or unjustified claims as reliable knowledge simply because they pass through trusted modules or interfaces. The authors want to understand, formalize, and prove why this happens, and why common fixes like scaling models or adding evaluators (LLM-as-judge) cannot fully solve it.

Method: The authors introduce a formal framework to distinguish between information transport and epistemic justification within LLM-based agent architectures. They define semantic laundering as a class of failures where propositions cross architecturally privileged interfaces and thereby gain unwarranted epistemic status. They connect this to epistemology via the Gettier problem, then prove the Theorem of Inevitable Self-Licensing under standard architectural assumptions, showing that circular justification cannot be removed. They further articulate the Warrant Erosion Principle to explain how justification degrades as it is passed across components, and use this to analyze why various architectural strategies (scaling, improved models, LLM-as-judge schemes) cannot fix the issue at a type-theoretic level.

Result: They show that semantic laundering is not a rare or accidental bug but a systematic, structural feature of common LLM-agent designs. The main technical result, the Theorem of Inevitable Self-Licensing, proves that circular epistemic justification is unavoidable in such systems under typical assumptions. They also derive the Warrant Erosion Principle to explain how and why epistemic warrant weakens through architectural flows, and argue that widely proposed mitigations like bigger models, better training, or adding judge models cannot structurally remove this failure mode.

Conclusion: The paper concludes that current LLM-based agent architectures are intrinsically prone to treating poorly supported propositions as well-justified knowledge due to semantic laundering across trusted interfaces. This yields a systematic, architectural version of the Gettier problem, where beliefs can be both true and unjustifiably licensed. Because the problem arises at the type/architecture level, not just from model imperfections, standard remedies such as scaling or LLM-as-judge are insufficient. Future work must therefore rethink agent designs and epistemic interfaces rather than relying solely on better or larger models.

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [101] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: The paper discusses strategies to improve teachers' AI literacy and agency so they can effectively integrate AI into their teaching.


<details>
  <summary>Details</summary>
Motivation: There is a growing presence of AI in education, but many teachers lack the knowledge, confidence, and skills to use AI tools effectively and to teach AI concepts to students. The paper aims to address this gap by exploring how to systematically support teachers.

Method: The paper examines and develops strategies across several domains, including curriculum design for AI-related content, professional development programs for teachers, classroom-level applications of AI tools, and policy guidelines that support teacher empowerment and responsible AI use.

Result: The work identifies and explores concrete approaches for curriculum, training, classroom practice, and policy that can enhance teachers’ AI literacy and sense of agency when using AI tools and teaching AI concepts.

Conclusion: By implementing well-designed curricula, sustained professional development, practical classroom applications, and supportive policies, educators can be empowered to confidently integrate AI in their practice and help students gain a deeper understanding of AI.

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [102] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: The paper introduces a qualitative reasoning model for object rotations and demonstrates it by solving a standard spatial ability test involving cube comparisons.


<details>
  <summary>Details</summary>
Motivation: To provide a formal qualitative framework for reasoning about 3D object rotations, enabling systems to handle spatial tests like the Cube Comparison Test without relying solely on numeric geometry or metric rotation calculations.

Method: The authors construct a conceptual neighborhood graph that links rotational movements of a cube to changes in feature locations and orientations on its faces (CNGRLO). From this graph they derive composition tables that allow qualitative inference about how rotations affect cube features, and apply these in reasoning steps needed to solve Cube Comparison Test items.

Result: They obtain a qualitative model (QOR) with corresponding composition tables that can infer how cube face features move and reorient under rotations, and they successfully apply it to solving instances of the Cube Comparison Test.

Conclusion: Qualitative reasoning about rotations, implemented via a conceptual neighborhood graph and composition tables, is sufficient to solve cube comparison tasks, showing that such qualitative models can effectively handle certain forms of spatial reasoning about 3D rotations.

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [103] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: The paper studies how Mixture-of-Experts (MoE) language models acquire and store knowledge during pre-training, using a new neuron-level attribution metric, and compares this to dense models.


<details>
  <summary>Details</summary>
Motivation: While MoE architectures allow scaling model capacity without proportional increases in computation, it is unclear how their sparse structure affects the dynamics of knowledge acquisition and representation during training, especially compared to standard dense models. This limits our understanding of why and how MoE models work and constrains interpretability and model design.

Method: The authors introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution method that decomposes the change in token log-probability across individual neurons/experts. They train large MoE and dense language models on trillions of tokens, save many checkpoints over training (1.2M steps for MoE and 600K for dense), and use Gated-LPI to track the evolving importance profiles of neurons and attention heads. They then analyze concentration of updates, temporal stability of importance, and robustness under targeted masking of important components.

Result: They identify three key empirical findings: (1) In MoE models, a small fraction (~1%) of neurons accounts for a large share (>45%) of positive log-probability updates, forming a low-entropy, high-utility backbone not seen in dense models. (2) The importance profile of MoE neurons stabilizes early in training (within 100K steps), whereas dense models show continued volatility in which neurons are important. (3) MoE models exhibit functional robustness: masking the ten most important attention heads leads to <10% drop in relational HIT@10, while dense models suffer >50% performance loss under the same perturbation, indicating more distributed knowledge storage in MoE.

Conclusion: Sparse MoE architectures naturally develop an early, stable, and distributed computational backbone, in contrast to dense models. This suggests that sparsity promotes robust, non-brittle knowledge storage and makes training-time interpretability more tractable, thereby helping to conceptually and practically connect sparse model design with interpretability tools.

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [104] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: The paper reframes AI creativity not as something to be scored, but as something to be modeled as an emergent property of generative systems within constrained environments.


<details>
  <summary>Details</summary>
Motivation: Most existing work on AI creativity focuses on metrics—novelty, diversity, usefulness—applied to model outputs. These approaches do not explain *how* creativity arises or what internal structures and conditions give rise to creative behavior. At the same time, modern multimodal generative models show increasingly complex recombination abilities that look creative, raising the need for a principled, technical account of machine creativity beyond output evaluation.

Method: The paper uses a conceptual and theoretical analysis rather than empirical experiments. It proposes a generative perspective: modeling creativity as an emergent property of domain-limited generative models in bounded informational environments. It decomposes creativity into four interacting components—pattern-based generation, induced world models, contextual grounding, and arbitrarity—and analyzes how each component appears in multimodal generative systems.

Result: The work yields a structured, technical framework for thinking about AI creativity as an emergent phenomenon. It clarifies how different aspects of generative models and their training environments contribute to behaviors we call creative, without introducing new quantitative metrics. It maps the four components to properties of modern multimodal architectures and their training regimes.

Conclusion: Creativity in AI should be understood as emerging from interactions between generative mechanisms and domain-specific representations within bounded contexts, not just as an externally judged property of outputs. By focusing on structural and contextual conditions—captured by the four-component decomposition—the framework offers a foundation for more systematic study and design of creative AI systems, moving beyond post hoc evaluation toward explicit modeling of creative dynamics.

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [105] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: The paper proposes OSPO, a new reinforcement learning framework for training large language models on recommendation tasks, which improves credit assignment by attributing rewards to semantically meaningful token segments instead of whole sequences, resulting in better and more robust recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Standard RL methods for training LLMs on personalized recommendation (like GRPO) use sparse, sequence-level rewards. This creates a credit assignment problem: it's unclear which parts of the generated text actually contributed to success, especially when the model must infer hidden user intent from vague language and lacks explicit labels. Existing value-model-based approaches add computational and modeling complexity. The authors want a way to learn directly from task feedback while providing fine-grained credit assignment that aligns with linguistic structure, without training extra value networks.

Method: They introduce Owen-Shapley Policy Optimization (OSPO). OSPO starts from sequence-level advantages and redistributes them across the tokens by estimating each token or segment's marginal contribution to the final outcome using Shapley-Owen attributions. To make this tractable and meaningful, tokens are grouped into semantically coherent coalitions (e.g., phrases describing product attributes, or full sentences expressing preferences). This acts as potential-based reward shaping: segment-level shaped rewards preserve the optimal policy but provide denser learning signals. Unlike approaches that rely on parametric value models, OSPO operates directly on task feedback without an auxiliary critic, integrating these shaped rewards into a standard policy gradient/RL training loop for LLMs.

Result: On two recommendation-related benchmarks, Amazon ESCI and H&M Fashion, OSPO consistently outperforms baseline methods such as GRPO and other RL training schemes, both in overall recommendation quality metrics and in settings where the retrieval component at test time is out-of-distribution relative to training. The improvements indicate better credit assignment and more robust reasoning about user preferences, even when the upstream retriever changes.

Conclusion: OSPO offers an effective and computationally attractive alternative to standard sequence-level RL or value-model-based methods for training LLM recommenders. By using Shapley-Owen-based reward redistribution over semantically meaningful text segments, it narrows the credit assignment gap while preserving optimal policies, leading to better performance and robustness in recommendation tasks that require inferring latent user intent from under-specified language.

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [106] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu,Jiagui Chen,Geng Hong,Jiayi Dong,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.AI

TL;DR: The paper introduces WebTrap Park, an automated, standardized platform to systematically evaluate the security of Web Agents by observing their real interactions with live websites.


<details>
  <summary>Details</summary>
Motivation: Although Web Agents are increasingly used to perform complex tasks on the web, existing security evaluations are ad hoc, fragmented, and hard to reproduce or compare across systems. There is a need for a unified, scalable, and realistic way to test how secure different Web Agent frameworks are in real-world web environments.

Method: The authors build WebTrap Park, an automated evaluation platform that encodes three major categories of security risks into 1,226 concrete, executable tasks on live web pages. The platform observes the agents’ actual actions in these environments, enabling action-based security assessment without modifying the agents themselves. Different Web Agent frameworks are run through this task suite to measure and compare their security behavior.

Result: Running multiple Web Agent frameworks through the 1,226 security tasks, the platform uncovers significant and consistent differences in security performance across frameworks. These differences are attributed not just to the underlying language model, but also to higher-level agent design choices and architectures.

Conclusion: WebTrap Park offers a scalable, reproducible, and publicly accessible benchmark for systematically evaluating the security of Web Agents in realistic web settings. The study shows that agent architecture plays a crucial role in security, and the platform can serve as a common foundation for future research and improvements in Web Agent safety.

Abstract: Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [107] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: The paper presents an approach to distill a large code-generation model into a lightweight model for real-time UAV multi-SDK control, preserving reasoning and code accuracy while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language models are good at code generation and reasoning but are too resource-intensive for onboard deployment on resource-constrained UAVs that require real-time, lightweight control. There is a need for a method that enables small models on UAV hardware to still perform complex, reliable code generation for multiple UAV SDKs.

Method: 1) Build a high-quality dataset spanning multiple mainstream UAV SDKs, with instruction-code-reasoning chains plus counterfactual negative samples to teach end-to-end mapping from instructions to code. 2) Use DeepSeek-Coder-V2-Lite, quantized via QLoRA, as a teacher model and apply a hybrid black-box/white-box knowledge distillation strategy to produce chain-of-thought soft labels. Combine these with hard labels under a weighted cross-entropy loss to transfer reasoning and code capabilities into a smaller student model. 3) Apply prompt-engineering/prompt-tuning specialized for UAV control tasks to further boost performance on SDK recognition and function call matching.

Result: Experiments show that the distilled lightweight model achieves high code generation accuracy on UAV multi-SDK tasks while significantly reducing deployment and inference costs, allowing real-time onboard use. The model improves performance in SDK type recognition and function call matching compared with baselines or non-distilled variants.

Conclusion: The integrated approach of dataset design, CoT-based knowledge distillation, and prompt tuning can effectively compress large code-generation models into small ones suitable for UAV onboard deployment without sacrificing much accuracy, enabling precise and lightweight intelligent UAV control across multiple SDKs.

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [108] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: The paper proposes an automated framework to generate fine-grained rubrics for open-ended generation tasks, enabling more scalable and discriminative reward signals for RL with verifiable rewards, and introduces RubricHub plus a two-stage training pipeline that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning with Verifiable Rewards (RLVR) works well in domains with clear ground truth such as math, but struggles in open-ended generation where no single correct answer exists. Current rubric-based evaluations, while helpful, are limited by scalability and overly coarse criteria, leading to a supervision ceiling where models cannot improve beyond the rough signal they receive. The authors are motivated to create a more scalable, fine-grained, and nuanced rubric-generation process to provide better training signals for models in complex, open-ended tasks.

Method: They propose an automated Coarse-to-Fine Rubric Generation framework that combines: (1) principle-guided synthesis to design initial rubric dimensions and guidelines; (2) multi-model aggregation to leverage diverse model perspectives for expanding and refining rubric criteria; and (3) difficulty evolution to iteratively adapt and sharpen the rubric as tasks and model capabilities become more challenging. Using this framework, they construct RubricHub, a large-scale (~110k examples), multi-domain dataset of detailed rubrics and associated data. They then use RubricHub in a two-stage post-training pipeline: Rubric-based Rejection Sampling Fine-Tuning (RuFT), where generations are filtered/selected via rubric scores for supervised fine-tuning, followed by Reinforcement Learning (RuRL), where the rubric-derived signals are used as rewards for RL optimization.

Result: RubricHub enables substantial performance improvements in open-ended reasoning tasks. In experiments, a Qwen3-14B model post-trained with RuFT and RuRL using RubricHub achieves state-of-the-art performance on HealthBench, scoring 69.3, and reportedly surpasses strong proprietary models such as GPT-5 on that benchmark. This indicates that the automatically generated, fine-grained rubrics provide a more powerful supervision signal than prior coarse or small-scale rubric approaches.

Conclusion: Automated coarse-to-fine rubric generation can overcome the supervision ceiling of traditional rubric-based methods in RL with verifiable rewards for open-ended tasks. By creating RubricHub and integrating it into a two-stage post-training pipeline (RuFT + RuRL), the authors show that large language models can achieve SOTA performance in complex, multi-domain benchmarks without relying on explicit ground-truth labels. Their framework offers a scalable, principled way to generate rich, discriminative reward signals, and they plan to release code and data to facilitate further research.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [109] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: The paper proposes YaPO, a reference-free method that learns sparse, disentangled steering vectors using a sparse autoencoder to more effectively and stably align LLM behaviors than prior dense-vector approaches.


<details>
  <summary>Details</summary>
Motivation: Existing activation-steering approaches like BiPO learn dense steering vectors from preference data, which are effective for some alignment tasks but suffer from entangled latent factors due to neuron multi-semanticity. This entanglement limits fine-grained control (e.g., distinguishing closely related cultural behaviors) and affects stability. The authors want a way to learn more disentangled, interpretable, and robust steering directions that can handle nuanced alignment objectives without costly full-model fine-tuning.

Method: YaPO is a reference-free policy optimization method that operates in the latent space of a Sparse Autoencoder (SAE). The SAE is used to encode model activations into sparse codes. Instead of learning dense steering vectors directly in activation space, YaPO optimizes sparse codes that correspond to particular alignment preferences. These optimized sparse codes define sparse steering vectors, which are then used to intervene in the LLM’s activations. This yields disentangled, interpretable steering directions. The optimization is framed similarly to DPO-style preference learning but applied over sparse latent representations rather than dense activations or model parameters.

Result: Empirically, YaPO converges faster than dense steering baselines like BiPO, achieves stronger performance on evaluated alignment tasks, and shows better training stability. It successfully supports fine-grained cultural alignment (e.g., among closely related Middle Eastern cultures) and generalizes to multiple alignment-related behaviors such as reducing hallucinations, limiting jailbreak and power-seeking tendencies, and controlling wealth-seeking behavior. Evaluation on MMLU shows no measurable degradation of general knowledge, indicating that alignment via YaPO does not harm core capabilities.

Conclusion: YaPO offers a general, efficient recipe for aligning LLMs by learning sparse, disentangled steering vectors in the latent space of a sparse autoencoder, without needing a reference model. It improves controllability, stability, and interpretability compared to dense activation steering methods, supports fine-grained cultural and behavioral alignment, and preserves base model capabilities. The approach is broadly applicable to alignment, controllability, and domain adaptation scenarios.

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [110] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: The paper introduces TABGR, a training-free table reasoning framework that models tables as attributed graphs and uses question-guided PageRank to improve accuracy and interpretability over LLM-based linearization methods.


<details>
  <summary>Details</summary>
Motivation: Existing table reasoning approaches typically linearize tables into text for LLMs, which discards structural information, offers poor explainability, and suffers from the lost-in-the-middle problem when inputs are long. There is a need for a method that preserves table structure, provides explicit reasoning paths, and scales better with long/tabular inputs while still achieving strong performance.

Method: The authors propose Table Graph Reasoner (TABGR), which converts a table into an Attributed Table Graph (ATG). In this graph, nodes and edges encode rows, columns, and cells along with their attributes so that the table’s 2D structure is explicitly preserved. Reasoning is done via graph-based operations over this ATG, enabling explicit reasoning chains. To combat the lost-in-the-middle issue and focus computation on relevant regions, they introduce a Question-Guided Personalized PageRank (QG-PPR) mechanism that uses the question to bias a PageRank-like process to rerank and prioritize more relevant table nodes or substructures before reasoning.

Result: On two standard table reasoning benchmarks, TABGR achieves consistent gains over state-of-the-art baselines, with accuracy improvements of up to 9.7%. These results suggest that preserving table structure via graphs and using QG-PPR for relevance-focused reasoning outperforms linearization-based LLM methods in both effectiveness and interpretability.

Conclusion: Modeling tables as attributed graphs combined with question-guided graph-based ranking offers a more accurate and explainable alternative to linearizing tables for LLM-based reasoning. TABGR demonstrates that training-free, structure-aware graph reasoning can effectively address limitations like structure loss and lost-in-the-middle, setting a new performance bar on common table reasoning benchmarks.

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [111] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: They build an explainable, multimodal AI web app that detects misogyny in code-mixed Hindi–English text and memes using transformer models plus XAI tools, and evaluate its usability with users.


<details>
  <summary>Details</summary>
Motivation: Online platforms enable large-scale spread of hate speech and misogyny, but existing AI detection tools are underdeveloped for low-resource, code-mixed languages and often act as black boxes, which is problematic in sensitive applications. There is a need for accurate, interpretable systems that can handle both text and image-based misogyny and are usable by moderators and researchers.

Method: They design a web application that integrates state-of-the-art multilingual transformer models for text and memes. For text misogyny detection, they fine-tune XLM-RoBERTa and mBERT on ~4,193 Hindi–English code-mixed comments. For multimodal meme detection, they combine mBERT (for text content) with EfficientNet and ResNet (for image content) trained on ~4,218 memes. They incorporate XAI techniques—SHAP and LIME—to compute and display feature importance, explaining model predictions. System usability is assessed via human evaluation using CUQ and UEQ questionnaires.

Result: The system successfully detects misogyny in both text and memes for Hindi–English code-mixed content, and can output local explanations (feature importances) for its decisions. Human evaluators complete CUQ and UEQ questionnaires indicating that the application achieves acceptable usability and user experience for research and moderation purposes, although specific metric scores are not given in the abstract.

Conclusion: An explainable, multimodal AI system can effectively detect misogyny in code-mixed Hindi–English text and memes while providing interpretable explanations via SHAP and LIME. This supports researchers and content moderators in addressing gender-based abuse online and shows that such tools can be made usable and user-friendly, encouraging further work on low-resource, code-mixed, and multimodal hate speech detection with built-in explainability.

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [112] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: Introduces SummPilot, an interaction-based customizable summarization system using LLMs.


<details>
  <summary>Details</summary>
Motivation: Standard automatic summarization is efficient but struggles to generate summaries tailored to each user’s unique interests and requirements. There is a need for a system that not only automates summarization but also allows user interaction to customize the output.

Method: They design SummPilot, which combines automatic and interactive summarization powered by a large language model. The system provides interactive components—such as semantic graphs, entity clustering, and explainable evaluation—that let users explore document content and iteratively adapt the summary to their preferences.

Result: A demo system is built, and user studies are conducted. The results show that SummPilot can adapt to different user needs and is considered useful for customizable summarization tasks.

Conclusion: Interaction-based, LLM-powered summarization via SummPilot makes it possible to efficiently create personalized summaries. The system’s interactive tools help users understand documents and tailor summaries, and evaluations indicate that this approach is both adaptable and useful.

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [113] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: The paper introduces What If TSF (WIT), a multimodal time series forecasting benchmark that evaluates whether models can adjust forecasts based on textual what-if scenarios, including future and counterfactual contexts.


<details>
  <summary>Details</summary>
Motivation: Most existing time series forecasting methods are unimodal and depend solely on historical numerical data, limiting their ability to incorporate rich contextual information that human experts naturally use. Although large language models suggest potential for multimodal forecasting, current benchmarks do not reliably test whether models truly exploit textual context, often using retrospective or misaligned text. There is a need for a rigorous evaluation framework to test scenario-conditioned forecasting, where different textual scenarios lead to meaningfully different forecasts from the same historical data.

Method: The authors design What If TSF (WIT), a benchmark that pairs time series data with expert-crafted textual scenarios, including plausible future events and counterfactual descriptions. For a fixed history, multiple distinct scenario texts are provided so that an ideal model should produce different forecasts conditioned on each scenario. The benchmark focuses on testing whether models can use textual context—especially forward-looking or hypothetical information—to guide their predictions, rather than merely extrapolating from numeric history.

Result: WIT provides a structured dataset and evaluation protocol that can reveal whether multimodal or language-augmented forecasting models truly respond to scenario text. Early use cases (implied by the abstract) show that existing models may not fully exploit such scenario descriptions, highlighting gaps between human-like scenario reasoning and current model capabilities. The benchmark is open-sourced for the community to evaluate and compare models.

Conclusion: The paper concludes that effective time series forecasting should go beyond unimodal historical extrapolation and incorporate scenario-based textual context, as human experts do. WIT offers a rigorous, expert-designed benchmark for testing this capability, particularly for LLM-based or multimodal models, and is intended to drive progress in scenario-guided forecasting methods.

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [114] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: The paper presents a three-stage AI-driven framework that transforms rough facade renovation sketches and text prompts into detailed, photorealistic proposals without requiring full as-built modelling.


<details>
  <summary>Details</summary>
Motivation: Facade renovation is more sustainable than demolition, but current architectural workflows depend on detailed as-built BIM or 3D models before design can begin. This process is slow, labour-intensive, and requires frequent revisions, creating a bottleneck in early-stage design exploration and communication of renovation intent. There is a need for a faster, lighter-weight method that can operate directly on rough sketches and textual descriptions while preserving key existing structures.

Method: The authors propose a three-stage pipeline combining a fine-tuned vision-language model, a diffusion-based sketch generator, and image-to-image refinement. (1) A fine-tuned VLM takes a rough structural sketch plus text input and predicts bounding boxes that indicate where changes should occur and which facade components to add. (2) A stable diffusion model generates detailed sketches of the new facade elements within these regions, which are integrated with the original outline using a generative inpainting process. (3) ControlNet is used to condition and refine the merged sketch into a coherent, photorealistic facade image that respects the original building structure.

Result: Experiments on both curated datasets and real industrial building cases show that the framework produces renovation designs that maintain the underlying structural layout while adding richer facade details. The outputs demonstrate structural consistency, improved visual quality, and alignment with the design intent expressed in the text, allowing architects to obtain plausible renovation concepts directly from rough inputs.

Conclusion: The proposed framework provides an efficient alternative to conventional, modelling-heavy renovation workflows by eliminating the need for detailed as-built models at the conceptual stage. It supports rapid generation and iteration of facade renovation proposals that both preserve existing structures and enhance facade articulation, thereby improving early-stage design exploration and communication between architects and stakeholders.

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [115] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: The paper proposes a new learner-tailored program repair task and a two-stage LLM-based framework that not only fixes students’ buggy code using retrieved solutions but also explains the underlying bug causes, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing intelligent programming coaching systems with LLMs largely focus on repairing or completing buggy code but do not explain the underlying causes of bugs to learners. This limits their educational value and does not fully support learner understanding and debugging skills. There is a need for a system that can both repair code and generate learner-tailored bug explanations in realistic coaching scenarios.

Method: The authors define a new task called Learner-Tailored Program Repair (LPR). They then design a two-stage framework named Learner-Tailored Solution Generator. Stage 1 builds a solution retrieval database and uses an edit-driven code retrieval approach to find relevant prior repair solutions, which are used to guide LLMs to identify and fix bugs. Stage 2 introduces a solution-guided program repair mechanism where the LLM, informed by retrieved solutions, generates both repaired code and natural-language explanations of the bug. Additionally, they design an Iterative Retrieval Enhancement method that uses automatic evaluation of generated code to iteratively refine retrieval, adjusting towards more suitable repair strategies over multiple iterations.

Result: In experiments on program repair tasks in a coaching-like setting, the proposed framework significantly outperforms several competitive baselines in terms of repair accuracy and overall performance metrics. The iterative retrieval enhancement and solution-guided repair both contribute to the performance gains, demonstrating the benefits of coupling retrieval with LLM-based repair and explanation.

Conclusion: The paper concludes that defining LPR and implementing the Learner-Tailored Solution Generator framework effectively advances intelligent programming coaching. By integrating edit-driven solution retrieval, solution-guided repair, explanatory bug descriptions, and iterative retrieval enhancement, their system provides both more accurate repairs and better pedagogical support than existing methods, making it well-suited for practical programming education scenarios.

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [116] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: The paper introduces WaterCopilot, an AI assistant that unifies static documents and real-time hydrological data to support water governance in the Limpopo River Basin, demonstrating strong RAG performance and a replicable framework for other transboundary basins.


<details>
  <summary>Details</summary>
Motivation: Transboundary river basins like the Limpopo suffer from fragmented water data, poor real-time access, and difficulty integrating diverse information sources (policies, hydrology, operations), which hampers timely, informed water management and governance decisions. There is a need for a unified, interactive system that can surface both static knowledge and live data in a usable way for decision-makers in data-scarce, multilingual contexts.

Method: The authors design and implement WaterCopilot, an AI virtual assistant built on Retrieval-Augmented Generation and tool-calling. They develop two custom plugins: (1) iwmi-doc-plugin for semantic search over indexed policy and technical documents using Azure AI Search, and (2) iwmi-api-plugin to query live hydrological and operational databases for metrics such as environmental-flow alerts, rainfall, reservoir levels, water accounting, and irrigation data. The system supports multilingual dialogue, transparent citations, automated calculations, and visualization. It is integrated with an existing Digital Twin of the Limpopo River Basin and deployed via a scalable pipeline on AWS. Performance is evaluated with the RAGAS framework, focusing on answer relevancy and context quality.

Result: WaterCopilot achieves an overall RAGAS score of 0.8043, with answer relevancy of 0.8571 and context precision of 0.8009, indicating that responses are generally accurate and grounded in appropriate context. The system successfully delivers automated threshold-based environmental alerts, real-time hydrological insights, and integrated views of policy and operational data via a conversational interface. It is deployed at scale and interoperates with the Limpopo River Basin Digital Twin, although some performance limitations appear for non-English technical documents and API latency.

Conclusion: WaterCopilot provides a practical, extensible AI-augmented framework for improving water governance in data-scarce, transboundary river basins. By combining RAG with tool-based access to live hydrological systems and multilingual interaction, it can support more informed and timely decisions, including alerts and scenario understanding. Despite current limitations in language coverage and system performance, the approach is replicable and demonstrates how similar assistants could be built for other complex basin contexts to strengthen water security and management.

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [117] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3 is a large, multimodal benchmark for testing complex retrieval-augmented generation (RAG) over visually rich, multi-document corpora in multiple languages.


<details>
  <summary>Details</summary>
Motivation: Existing RAG benchmarks mostly focus on plain text, single documents, and separate evaluation of retrieval vs. generation. They do not sufficiently test realistic, complex RAG needs such as understanding tables, charts, and images, aggregating information across multiple documents, and ensuring answers are correctly grounded in sources. The authors aim to create a more realistic and challenging benchmark that reflects professional document settings and multimodal reasoning needs.

Method: The authors build ViDoRe v3, a benchmark that aggregates 10 datasets from diverse professional domains, with around 26,000 visually rich document pages. They design 3,099 human-verified queries available in 6 languages, covering multiple query types that require multimodal, cross-document reasoning and grounding. Over roughly 12,000 hours of human work, they annotate retrieval relevance, bounding boxes for where evidence appears in documents, and verified reference answers. They then evaluate a variety of state-of-the-art RAG pipelines, including textual vs. visual retrievers, late-interaction models, rerankers, and different context construction strategies (text-only, visual, hybrid).

Result: Empirical evaluation shows that visual retrievers outperform purely textual retrievers on this multimodal benchmark. Late-interaction models and textual reranking substantially boost retrieval performance. When generating answers, using hybrid or purely visual contexts leads to improved answer quality compared to text-only contexts. Nonetheless, performance gaps remain on several challenging aspects of the benchmark, such as handling non-textual content (figures, tables, charts), open-ended questions, and precise visual grounding at fine granularity.

Conclusion: ViDoRe v3 offers a challenging and realistic multimodal RAG benchmark that goes beyond existing text-centric, single-document, and decoupled retrieval/generation evaluations. While current RAG systems benefit from visual retrieval and advanced interaction/reranking mechanisms, they still fall short on complex visual and grounding tasks. By releasing the benchmark with a commercially permissive license, the authors aim to catalyze research progress on robust, multimodal, cross-document RAG systems in practical domains.

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [118] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: An explainable LLM-based multi-agent system is proposed to improve meme coin copy trading by identifying high-quality projects and profitable KOL wallets, outperforming traditional ML and single LLM baselines.


<details>
  <summary>Details</summary>
Motivation: Meme coin copy trading has become popular due to its low barrier to entry, but it is risky and not reliably profitable because of manipulative bots, uncertain future performance of copied wallets, and execution delays. Existing LLM-based financial tools are promising yet limited: a single LLM struggles with complex tasks like asset allocation and typically lacks cryptocurrency-specific domain knowledge. There is a need for a more structured, explainable, and reliable approach tailored to the volatile meme coin market.

Method: The authors design an explainable multi-agent system modeled after an asset management team. The overall copy-trading problem is decomposed into subtasks handled by specialized LLM-based agents. Few-shot chain-of-thought prompting is used to equip each agent with domain-specific meme coin trading expertise, enabling them to interpret multi-modal data (e.g., on-chain transactions and other signals) and generate step-by-step, explainable decisions. The system is empirically evaluated on transaction data from 1,000 meme coin projects.

Result: The multi-agent system demonstrates superior performance over traditional machine learning models and single LLM setups. It achieves 73% precision in identifying high-quality meme coin projects and 70% precision in identifying key opinion leader (KOL) wallets. The KOL wallets selected by the system collectively generated about $500,000 in profit across the studied meme coin projects, indicating that the system’s selections are meaningfully associated with real profitability.

Conclusion: Structuring LLMs into a coordinated, explainable multi-agent framework significantly enhances decision quality in meme coin copy trading compared with both conventional ML and monolithic LLM baselines. Decomposing the task into specialized agents with few-shot CoT prompting allows better handling of multi-modal crypto data and improves the identification of profitable projects and influential wallets. This suggests that multi-agent LLM architectures are a promising direction for complex, high-noise financial domains like meme coins.

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [119] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: The paper introduces Prism, a framework that improves how LLMs clarify complex, ambiguous user intents on social platforms by modeling logical dependencies among clarification questions, leading to more coherent interactions and better task performance.


<details>
  <summary>Details</summary>
Motivation: As LLMs become web-native interfaces to social platforms, they must handle users with ambiguous, evolving, and multifaceted goals. Existing clarification methods (sequential or parallel questioning) ignore logical dependencies among sub-intents, causing redundant questions, logical conflicts, higher cognitive load, and inefficient collaboration. The authors aim to design a system that clarifies intent in a logically structured, low-friction way that better supports complex, multi-step user tasks.

Method: The authors propose Prism, composed of four modules: (1) a complex intent decomposition module that breaks user requests into smaller elements and identifies logical dependencies between them; (2) a logical clarification generation module that uses those dependencies to order and structure clarification questions for coherent interaction; (3) an intent-aware reward module that defines a reward function over clarification trajectories and uses Monte Carlo sampling to simulate user–LLM interactions, generating large-scale training data; and (4) a self-evolved intent tuning module that iteratively trains and refines the LLM’s clarification behavior using the generated data and reward signals, improving logical consistency and efficiency over time.

Result: Prism was evaluated on benchmarks covering clarification interactions, downstream intent execution quality, and user cognitive load. It consistently outperforms prior clarification approaches, achieving state-of-the-art logical consistency in clarifications, reducing logical conflicts in interactions to 11.5%, increasing user satisfaction by 14.4%, and decreasing task completion time by 34.8%.

Conclusion: Modeling and leveraging the logical structure of complex user intents enables more effective clarification by LLMs. Prism’s decomposition, logically ordered questioning, intent-aware reward, and iterative self-tuning together yield clearer, more efficient, and more satisfying human–LLM interactions. The framework provides a scalable way to train LLMs for complex intent understanding, and the released code and data support further research and application.

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [120] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: A tutorial that makes reinforcement learning more approachable for undergraduates by connecting theory to hands-on coding with clear examples.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning is often mathematically heavy and abstract, making it difficult for undergraduates to move from understanding the theory to writing working code. There is a need for educational materials that lower this barrier with simpler explanations and concrete, guided examples.

Method: Provide a tutorial structured around clear, example-driven explanations and hands-on coding exercises. The content emphasizes step-by-step implementations that directly illustrate theoretical RL concepts and explicitly addresses common student difficulties during theory-to-code transition.

Result: Students using the tutorial gain practical experience implementing RL algorithms and better understand how theoretical concepts map to real code. This helps them overcome typical stumbling blocks in learning RL.

Conclusion: An example-focused, implementation-oriented tutorial can effectively bridge the gap between RL theory and practice for undergraduate learners, giving them the foundational skills and confidence to apply RL in real-world problems.

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [121] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: The paper introduces Pced, a training-free decoding framework that enables multi-document reasoning in Retrieval Augmented Generation without long concatenated prompts or shared attention across documents.


<details>
  <summary>Details</summary>
Motivation: Retrieval Augmented Generation suffers from a trade-off: concatenating many retrieved documents into one long prompt allows cross-document reasoning but is slow due to prefill bottlenecks; treating each document separately with its own KV cache is fast but prevents the model from aggregating evidence across documents. The authors aim to retain multi-document reasoning while avoiding the computational cost of long-context attention.

Method: The authors propose Parallel Context-of-Experts Decoding (Pced), which interprets each retrieved document as an independent expert. Instead of building a single attention context over all documents, they decode in parallel from each expert context and then synchronize their token predictions using a retrieval-aware contrastive decoding rule. This rule compares and combines expert logits with the base model’s prior logits to determine the final output token, effectively shifting evidence aggregation from attention to the decoding stage. The framework is training-free and operates at inference time on top of existing LMs.

Result: Pced restores cross-document reasoning capabilities that are typically lost when documents are encoded separately, while avoiding the prefill bottleneck of concatenated prompts. It achieves more effective multi-document aggregation without requiring modifications to the underlying attention mechanism or additional training.

Conclusion: By moving evidence aggregation from attention to decoding and coordinating separate document-specific experts via a contrastive, retrieval-aware rule, Pced offers an efficient, training-free way to perform multi-document reasoning in Retrieval Augmented Generation. This approach reconciles speed with the need for cross-document interaction, suggesting a practical path to scalable, retrieval-based language model systems.

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [122] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: The paper argues that seemingly unethical behaviors from large language models are not signs of evil intent or alignment failure, but statistical reflections of human social patterns, especially under power asymmetries, and that AGI risk is mainly about amplifying human contradictions rather than creating a malicious agent.


<details>
  <summary>Details</summary>
Motivation: Public and scholarly discussions often interpret LLM outputs like threats, blackmail, or deception as evidence that these systems are developing malign agency or are badly aligned with human values. This narrative risks misunderstanding both how LLMs work and the nature of human morality. The authors aim to correct this conceptual error, provide a more accurate social-theoretic framing of LLM behavior, and reshape how we think about AGI alignment and governance.

Method: The paper uses a conceptual and theoretical analysis rather than experiments. It draws on relational models theory and social science perspectives on human interaction regimes (markets, authority, bargaining, coercion) to reinterpret LLM behaviors. The authors map seemingly unethical outputs (e.g., blackmail-like statements) onto a continuum of ordinary human social practices and analyze how LLM training on large corpora of social interaction leads to structural generalization of these regimes, especially under extremes of power or information asymmetry.

Result: The authors show that behaviors like blackmail, threats, or coercion produced by LLMs can be understood as extrapolations of human interaction patterns rather than anomalies or emergent malign intentions. They argue that the surprise people feel is rooted in anthropomorphism: expecting intelligence to mirror only morally sanctioned behavior, not the full diversity of human conduct. They also argue that because human morality is heterogeneous and context-bound, aiming for a universally moral AI is conceptually incoherent. They further recharacterize AGI risk as arising from amplification of human capabilities and contradictions, which compresses historical timescales and removes the buffer that once allowed inconsistent values and institutions to coexist.

Conclusion: The paper concludes that alignment failures should be viewed as structural features of how AI systems amplify and reorganize human social dynamics, not as accidental glitches or signs of hostile AI intent. Therefore, governance and alignment strategies should focus less on model-level intention or moral reasoning, and more on managing amplification effects, systemic complexity, and institutional/regime stability. In this framing, AGI safety becomes a problem of socio-technical governance under accelerated feedback and power concentration, rather than preventing a singularly immoral artificial agent.

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [123] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: The paper proposes ESGAgent, a hierarchical multi-agent system designed to perform professional-grade ESG analysis using specialized tools, and introduces a three-level benchmark from 310 sustainability reports to evaluate ESG-related reasoning and reporting, showing ESGAgent outperforms strong LLM baselines.


<details>
  <summary>Details</summary>
Motivation: Professional ESG analysis is important but difficult because relevant data is scattered across unstructured sources, and current LLMs struggle with the complex, multi-step workflows and rigorous auditing requirements typical in ESG evaluation. There is also a lack of a systematic benchmark to diagnose and evaluate these specialized capabilities.

Method: The authors design ESGAgent, a hierarchical multi-agent architecture equipped with a tailored toolset, including retrieval-augmented generation, web search, and domain-specific tools, to execute structured ESG auditing workflows. In parallel, they construct a three-level benchmark based on 310 corporate sustainability reports, covering tasks from simple, factual, common-sense ESG questions to comprehensive, integrated ESG analysis and report generation that uses structured evidence and visualizations.

Result: In experiments, ESGAgent achieves an average accuracy of 84.15% on atomic ESG question-answering tasks, surpassing state-of-the-art closed-source LLMs. It also produces higher-quality professional ESG reports that integrate charts and verifiable references, demonstrating superior capability on complex, multi-step ESG analysis and reporting tasks.

Conclusion: ESGAgent significantly advances automated ESG analysis by combining a hierarchical agentic architecture with a rich toolset, enabling more accurate and professional ESG evaluations than existing LLMs. The accompanying benchmark is validated as a useful diagnostic tool and testbed for assessing both general and advanced agentic capabilities in specialized, high-stakes domains like ESG auditing.

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [124] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual is a dual-mode LLM framework that can do both objective and personalized reasoning, and learns to adaptively switch between them to gain personalization benefits while avoiding interference with factual, general-purpose tasks.


<details>
  <summary>Details</summary>
Motivation: Personalized information helps LLMs better match users’ preferences but can hurt objectivity and factual correctness when the model inappropriately injects user-specific traits into tasks that require neutral, general-purpose reasoning. Existing approaches often entangle these two behaviors, causing interference between personalized and objective responses. The paper aims to separate and control these modes within a single model so that it can safely exploit personalization only when appropriate.

Method: The authors introduce PersonaDual, a framework where one LLM supports two distinct reasoning modes: general-purpose objective reasoning and personalized reasoning. First, they perform supervised fine-tuning (SFT) so the model learns both reasoning patterns. Then they apply a reinforcement learning stage using a new algorithm called DualGRPO, which explicitly optimizes the model’s ability to select the right mode based on the context, improving mode selection and reducing cross-interference between objective and personalized behaviors.

Result: On both objective benchmarks and personalized benchmarks, PersonaDual maintains the advantages of personalization (e.g., tailoring responses to user preferences) while substantially reducing negative interference on tasks that demand objective reasoning. The experiments show near interference-free performance on objective problems and demonstrate that the model can more effectively use helpful personalized signals to improve problem-solving, compared with baselines that do not distinguish modes as clearly.

Conclusion: PersonaDual successfully integrates objective and personalized reasoning within a single LLM and, via SFT plus DualGRPO-based reinforcement learning, learns to adaptively choose between these modes. This leads to models that are almost interference-free on objective tasks while still benefiting from personalization when it is contextually appropriate, suggesting a promising direction for safe and effective personalization in LLMs.

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [125] [MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection](https://arxiv.org/abs/2601.08684)
*Paolo Italiani,David Gimeno-Gomez,Luca Ragazzi,Gianluca Moro,Paolo Rosso*

Main category: cs.AI

TL;DR: Introduces MemeWeaver, a multimodal, graph-based framework to detect sexism and misogyny in online memes by modeling relations across memes instead of treating them independently.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal content moderation methods focus mainly on the content of individual posts (image+text) and often ignore the social and relational dynamics in which online hate emerges and spreads. Women experience disproportionate gender-based harassment online, much of which is embedded in memes and reinforced within like-minded communities. Current graph-based approaches that could capture such interactions are limited by heuristic graph construction, shallow fusion of visual and textual signals, and reasoning only at the instance level rather than over relationships between memes. There is a need for a more principled, end-to-end approach that can learn meaningful relational structures among multimodal hateful content to improve detection of sexism and misogyny.

Method: The paper proposes MemeWeaver, an end-to-end trainable multimodal framework that detects sexism and misogyny with a novel inter-meme graph reasoning mechanism. First, visual and textual features from memes are extracted and fused using several candidate fusion strategies that are systematically compared (e.g., early, late, and joint fusion schemes). Then, instead of building a graph via fixed heuristics, MemeWeaver learns an inter-meme graph structure where nodes are memes and edges encode learned semantic relationships among them. A graph neural network (or similar graph reasoning module) performs message passing over this learned graph to capture how memes relate to and reinforce each other, and the resulting graph-enhanced representations are used for classification. The entire pipeline from feature extraction and fusion to graph construction and reasoning is trained jointly in an end-to-end manner.

Result: On two benchmark datasets for sexism and misogyny detection—MAMI and EXIST—MemeWeaver consistently outperforms state-of-the-art baselines in terms of standard classification metrics (e.g., F1, accuracy). It also converges faster during training compared to competing multimodal and graph-based approaches. Qualitative and quantitative analyses of the learned graph show that it clusters memes with similar hateful or sexist semantics and reveals relational patterns (e.g., recurring stereotypes, narrative templates, or shared visual tropes) that are not captured when treating memes independently.

Conclusion: Modeling inter-meme relationships via a learned graph structure significantly improves the detection of sexism and misogyny in multimodal online content. MemeWeaver demonstrates that going beyond instance-level reasoning to capture the relational and community-oriented nature of online hate leads to better performance and more interpretable patterns. The approach highlights the importance of graph-based multimodal reasoning for robust content moderation and offers a foundation for future work on understanding and mitigating online harassment as a networked phenomenon rather than an isolated one.

Abstract: Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.

</details>


### [126] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: Introduces OIP-SCE, a structured, phase-based framework to evaluate whether conversational clinical AI meets all required obligations over an entire interaction.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for clinical conversational AI usually score isolated responses or global outcomes and fail to capture whether all necessary clinical obligations are satisfied across the full dialogue, in correct order, and with auditable justification. This gap makes it hard for healthcare organizations to trust, govern, and integrate AI tools into real workflows where policy compliance is critical.

Method: Propose Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), which decomposes clinical tasks into ordered phases, each with explicit required obligations. The method checks whether every obligation is satisfied in the appropriate phase, and requires explicit, reviewable evidence for each check. The framework is designed so clinicians can author or adjust the obligation sets and phases, while engineers use them as precise specifications for system behavior. The authors demonstrate OIP-SCE via two case studies: respiratory history-taking and insurance benefits verification, showing how phase-level criteria operationalize complex policies.

Result: In the two case studies, OIP-SCE successfully captures whether conversational AI systems fulfill all specified clinical obligations, in the correct sequence, with traceable evidence. The examples show that the framework can translate high-level clinical and administrative policies into concrete, testable checkpoints, and that the resulting evaluation artifacts are understandable and auditable by clinicians and stakeholders.

Conclusion: OIP-SCE offers a single, auditable evaluation surface that connects AI behavior to clinical workflows and policy requirements. By structuring evaluations around obligatory information and phases, it enables clinicians to control what is checked and gives engineers a clear implementation target, thereby making complex compliance requirements practical. This supports safer, more routine deployment of conversational AI in healthcare by aligning technical evaluation with real-world clinical needs.

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [127] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: The paper presents AXE, a method to evaluate feature-importance explanations without ground-truth explanations, and shows it can reliably detect when models use protected attributes, even under adversarial fairwashing within Rashomon sets.


<details>
  <summary>Details</summary>
Motivation: Explainable AI methods can give different explanations for the same model, and commonly used evaluation approaches rely on comparing explanations to idealized ground-truth explanations or on model sensitivity, which can hide behavioral differences between equally accurate models (a Rashomon set) and allow adversarial fairwashing. There is a need for an evaluation method that can reveal such behavioral differences and reliably detect when models rely on protected attributes, even when predictions remain unchanged.

Method: The authors introduce AXE, an evaluation method for feature-importance explanations that is grounded in three proposed principles of explanation evaluation rather than ground-truth comparison. AXE examines how explanations behave across alternative but similarly performing models in a Rashomon set, and tests whether explainers and standard metrics can be misled by models that keep predictions fixed but alter internal feature usage, especially regarding protected attributes.

Result: They show that traditional evaluation metrics based on ground-truth comparison or sensitivity can consider misleading explanations to be high quality and fail to distinguish models within a Rashomon set. In contrast, AXE successfully detects adversarial fairwashing scenarios—where alternate models preserve predictions but change feature usage to hide reliance on protected attributes—with a reported 100% success rate, and can identify when protected attributes are actually used for prediction.

Conclusion: Ground-truth-based and sensitivity-based explanation evaluations are insufficient and can mask important behavioral differences between models and enable fairwashing. Explanation evaluation should follow the proposed principles and consider behavior across the Rashomon set. AXE operationalizes these principles, effectively uncovering misleading explanations and reliably detecting the use of protected attributes, making it a more trustworthy approach for evaluating feature-importance explanations and for model selection in practice.

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [128] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: The paper proposes Cago, a capability-aware goal sampling method that uses expert demonstrations to adaptively generate intermediate goals slightly beyond the agent’s current ability, improving imitation learning in long-horizon, sparse-reward tasks.


<details>
  <summary>Details</summary>
Motivation: Standard imitation learning struggles in long-horizon environments because perfectly copying demonstrations is unrealistic and small deviations compound into catastrophic failure. Existing approaches either use demonstrations only for initialization or reward shaping and still suffer when trajectories are long and sparse-reward. There is a need for a method that more robustly leverages demonstrations without requiring near-perfect imitation, especially in goal-conditioned, sparse-reward settings.

Method: Cago (Capability-Aware Goal Sampling) monitors how competent the agent is along expert demonstration trajectories. It evaluates where the agent can currently succeed and where it fails, then selects intermediate states from demonstrations as training goals that are slightly beyond the agent’s present capability. This adaptive goal selection induces a curriculum: as the agent improves, the sampled goals progress further along the expert trajectory toward the full task. The framework is applied in a learning-from-demonstrations, goal-conditioned RL setting.

Result: Across multiple sparse-reward, goal-conditioned benchmark tasks, Cago improves both sample efficiency and final performance compared to standard learning-from-demonstrations baselines. Empirical results show consistent, significant gains, indicating that capability-aware, adaptive goal sampling is more effective than static use of demonstrations for initialization or reward shaping alone.

Conclusion: Adaptive, capability-aware goal sampling from expert trajectories offers a robust way to exploit demonstrations in long-horizon tasks without requiring precise imitation. By turning demonstrations into a dynamic curriculum of near-frontier goals, Cago alleviates error compounding and yields better learning efficiency and performance than existing LfD methods in sparse-reward, goal-conditioned environments.

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [129] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: The paper argues that despite AI being framed mainly as a productivity tool, its major real-world and commercial role will increasingly be entertainment, and we need new ways to evaluate AI-generated cultural content beyond just preventing harm.


<details>
  <summary>Details</summary>
Motivation: Current AI discourse and evaluation are centered on intelligence, productivity, and harm mitigation, but an increasingly important use of generative AI is entertainment, especially among young users. The field lacks conceptual and evaluative tools to understand the social impacts—good and bad—of AI-generated entertainment as it becomes a dominant business model.

Method: Conceptual and analytical: the authors examine prevailing narratives about AI, survey emerging usage patterns and business incentives around entertainment use cases, analyze limitations in existing AI evaluation frameworks, and then synthesize insights from humanities scholarship to propose a new evaluative framework for AI entertainment, termed “thick entertainment.”

Result: They identify an asymmetry in current AI assessment practices: benefits are framed almost entirely in terms of cognitive productivity, while harms are focused on cultural risks; there is no robust language or framework to describe and measure positive cultural and social benefits of AI-generated entertainment. They introduce “thick entertainment” as a richer framework that foregrounds meaning-making, identity, and social connection when evaluating AI cultural outputs.

Conclusion: As generative AI becomes increasingly used and monetized as entertainment, conventional AI evaluation—focused on intelligence and harm reduction—is insufficient. The field needs frameworks like “thick entertainment” that take seriously the constructive cultural and social roles entertainment can play. Ultimately, AI’s societal significance may lie less in augmenting intelligence and more in reshaping how people experience culture, identity, and connection, paralleling how social media reshaped social connection.

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [130] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin,Yoojin Choi,Yuxuan Zhu,Daniel Kang*

Main category: cs.AI

TL;DR: The paper studies how annotation errors in popular text-to-SQL benchmarks (BIRD and Spider 2.0-Snow) affect model performance evaluation and leaderboard rankings, finding very high error rates and large shifts in scores and ranks after correction.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL systems are chosen and compared mainly via public benchmarks and leaderboards. These benchmarks rely heavily on human annotations for natural-language questions, SQL queries, and evaluation labels. If these annotations contain many errors, then reported accuracy and system rankings may be unreliable, which can mislead both research and real-world deployment decisions. The authors are motivated to quantitatively assess how bad the annotation errors are, and how much they distort conclusions drawn from these benchmarks.

Method: The authors conduct an empirical study in two parts. First, domain experts manually audit and re-annotate samples from two widely used text-to-SQL benchmarks: BIRD (specifically its Mini-Dev subset) and Spider 2.0-Snow. They identify and measure annotation errors, such as incorrect ground-truth SQL or mislabeled evaluations, to estimate error rates. Second, for a subset of the BIRD Dev set, they correct the annotations and then re-evaluate 16 open-source text-to-SQL agents taken from the BIRD leaderboard on both the original and corrected subsets. They compare absolute and relative performance changes and compute rank shifts. They also study correlations (via Spearman rank correlation) between rankings on the (un)corrected subsets and on the full BIRD Dev set to see how representative and robust the rankings are.

Result: Expert re-annotation reveals very high error rates: 52.8% for BIRD Mini-Dev and 62.8% for Spider 2.0-Snow. When the 16 open-source text-to-SQL agents are re-evaluated on the corrected BIRD Dev subset, their relative performance changes range widely from -7% to +31%. Correspondingly, leaderboard positions experience substantial shifts between -9 and +9 ranks. Moreover, rankings on the original, uncorrected subset correlate strongly with rankings on the full BIRD Dev set (Spearman’s r_s = 0.85, p = 3.26e-5), but rankings on the corrected subset correlate only weakly with those on the full Dev set (r_s = 0.32, p = 0.23). This suggests that uncorrected annotations can create a misleading sense of consistency and reliability.

Conclusion: The study concludes that annotation errors in widely used text-to-SQL benchmarks are surprisingly frequent and can substantially distort both reported performance and leaderboard rankings. Because benchmark results drive research focus and system deployment decisions, these distortions risk misguiding the community. The authors advocate for more careful benchmark construction, rigorous quality control of annotations, and possibly revisiting existing benchmarks. They also release their code and corrected data to facilitate more accurate evaluations and further work on robust benchmarking practices.

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


### [131] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: The paper presents a cross-national method and three datasets to systematically measure and visualize political bias in large language models using real parliamentary voting records.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used in platforms and decision-making systems, raising concerns about their political bias. Existing work has mostly focused on social biases (e.g., gender, race), leaving political bias under-studied despite its direct impact on democracy and public discourse. The authors aim to fill this gap by providing a principled, data-driven way to detect and compare political biases in LLMs across countries.

Method: The authors propose a general methodology that compares LLM-generated voting predictions to actual, verified parliamentary voting records. They operationalize this by creating three benchmarks based on national parliaments: PoliBiasNL for the Netherlands, PoliBiasNO for Norway, and PoliBiasES for Spain, each containing thousands of motions and votes spanning multiple parties. They then assess ideological tendencies and party-specific biases by seeing how often and in what direction models align or diverge from different parties. To make the results interpretable, they map both LLMs and political parties into a common two-dimensional ideological space (based on CHES) by linking voting-derived positions to CHES dimensions, and visualize their relative locations.

Result: Using these benchmarks, the authors empirically find that state-of-the-art LLMs do not occupy a neutral political position. Instead, they show consistent left-leaning or centrist ideological tendencies and systematic negative biases toward right-conservative parties. The methodology reveals fine-grained differences in how models relate to various parties and ideological positions across the three countries.

Conclusion: The study concludes that modern LLMs exhibit measurable and systematic political biases, particularly against right-conservative parties, and that these biases can be characterized in detail using parliamentary voting records and CHES-based ideological mapping. The authors argue that their transparent, cross-national evaluation framework provides a practical foundation for auditing and comparing political bias in LLMs, and should inform future model development, deployment, and governance.

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>
