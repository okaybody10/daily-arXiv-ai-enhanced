<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: RAG 시스템에서 검색된 문서를 요약해 주는 압축기(compressor)가 노이즈(무관·오류 정보)에 강하도록 만드는 ACoRN이라는 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: RAG 환경에서 계산 비용을 줄이기 위해 작은 언어모델로 검색 문서를 압축하지만, 검색된 문서 안에 질문과 무관하거나 사실 오류가 있는 내용이 섞여 있다. 기존 추상적 압축기는 이런 노이즈가 많을 때 중요한 근거를 빠뜨리거나 오답을 유도하기 쉽고, 긴 문맥에서는 주의(attention)가 분산되고 위치 편향도 존재해 핵심 근거를 잘 못 살린다. 이를 해결해 노이즈에 강하고, 정답을 직접 지지하는 정보를 잘 보존하는 압축기가 필요하다.

Method: 1) 검색 노이즈를 두 가지 유형으로 세분화한 후, 오프라인 데이터 증강을 통해 이 노이즈가 섞인 학습 데이터를 만들고, 이를 이용해 압축기가 노이즈에 견디도록 학습한다. 2) 여러 문서에서 온 정보를 충분히 활용하지 못하고 위치 편향을 보이는 기존 LM 기반 압축기의 한계를 줄이기 위해, 정답을 직접 뒷받침하는 핵심 정보 중심(summary centered around key information)을 생성하도록 미세조정(finetuning)한다. 구현은 T5-large를 압축기로 사용해 이 ACoRN 절차로 학습시킨다.

Result: ACoRN 방식으로 학습한 T5-large 압축기를 RAG 파이프라인에 넣었을 때, 여러 벤치마크에서 EM과 F1이 향상되었고, 정답 문자열 자체를 요약 안에 보존하는 비율이 높아졌다. 특히 정확도를 떨어뜨리는 문서(노이즈 문서)가 많이 섞인 데이터셋에서 성능 이득이 두드러졌다.

Conclusion: 노이즈가 많은 현실적 검색 환경에서도, 제안한 ACoRN 학습 절차를 통해 추상적 압축기가 핵심 근거와 정답을 더 잘 보존하면서도 계산 효율성을 유지할 수 있음을 보였다. 따라서 실제 RAG 시스템에서 신뢰성과 성능을 동시에 높이는 유용한 방법론으로 활용될 수 있다.

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 이 논문은 강화학습(RL)을 활용해 대형 언어모델의 추론 능력은 유지·향상하면서도, 내재적/외재적 환각을 동시에 줄이는 훈련 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 강화학습을 통해 LLM의 복잡한 추론 능력이 크게 향상되었지만, 동시에 사실과 다른 내용을 그럴듯하게 생성하는 환각(hallucination) 문제가 심화되었다. 특히 모델의 내부 지식 오류(외재적 환각)와 주어진 컨텍스트에 불충실한 생성(내재적 환각) 모두가 신뢰성을 떨어뜨리며, 고성능과 신뢰성 사이에 심각한 트레이드오프가 존재한다. 이를 해결해, 고급 추론 능력을 유지하면서도 사실적 신뢰성을 높이는 실용적인 학습 방법이 필요하다는 동기가 있다.

Method: 1) TriviaQA의 개방형 질의응답 대화를 바탕으로 새로운 훈련 데이터를 만들어, 모델 내부 지식의 오류에서 기인하는 외재적 환각을 줄이는 방향으로 RL 보상을 설계한다. 2) FineWeb에서 추출한 장문의 텍스트를 사용해, 주어진 근거 텍스트와의 사실 일치도를 평가하는 fact-grounding 보상 체계를 구축하여 내재적 환각(컨텍스트 불충실성)을 억제한다. 3) 답변 불가능한 질문에 대해 ‘모르겠다/답변 거부’와 같은 신중한 응답을 할 경우 명시적으로 보상을 주는 RL 설계를 도입하여, 무리하게 추측하지 않고 거절할 줄 아는 ‘조심성(cautiousness)’을 학습시킨다. 4) 이 모든 요소를 통합한 타깃형 RL 프레임워크를 구성하고, 단문·장문 QA 벤치마크 전반에서 성능을 실험적으로 검증한다.

Result: 다양한 벤치마크에서 제안한 RL 프레임워크를 적용한 모델은 기존 방법 대비 정답률과 신뢰성이 동시에 향상되었으며, 내부 지식 오류에서 비롯되는 외재적 환각과 컨텍스트 불충실로 인한 내재적 환각이 모두 유의미하게 감소했다. 또한 답변 거부를 보상하는 설계 덕분에, 모델이 답을 모르는 경우 무리하게 추정하지 않고 적절히 ‘모른다’고 응답하는 비율이 증가했다.

Conclusion: 제안된 타깃형 RL 프레임워크는 고급 추론 능력과 사실적 신뢰성 사이의 긴장을 완화하는 실질적인 방법을 제공한다. 외재적·내재적 환각을 동시에 다루고, 답변 거부 전략까지 포함함으로써 보다 유능하면서도 신뢰할 수 있는 LLM을 만드는 길을 제시하며, 향후 실사용 환경에서의 적용 가능성을 높이는 기반을 마련한다.

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [Luxical: High-Speed Lexical-Dense Text Embeddings](https://arxiv.org/abs/2512.09015)
*DatologyAI,:,Luke Merrick,Alex Fang,Aldo Carranza,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Haoli Yin,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Paul Burstein,Parth Doshi,Paul Burnstein,Pratyush Maini,Ricardo Monti,Rishabh Adiga,Scott Loftin,Siddharth Joshi,Spandan Das,Tony Jiang,Vineeth Dorma,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.CL

TL;DR: Luxical은 TF-IDF 기반 희소 특징과 작은 ReLU 신경망, 지식 증류로 대형 트랜스포머 임베딩을 근사해, 웹 규모 텍스트 조직에 적합한 고속 임베딩을 제공하는 라이브러리이다.


<details>
  <summary>Details</summary>
Motivation: 웹 규모 텍스트 코퍼스를 효율적으로 구성하는 것이 최신 대형 언어 모델 성능의 핵심이지만, 현재 도구들은 빠르지만 표현력이 제한적인 FastText류 분류기와, 표현력은 뛰어나지만 계산 비용이 큰 트랜스포머 임베딩 모델 사이에서 속도와 유연성의 트레이드오프를 보인다. 이 간극을 줄여, 높은 품질과 다양한 다운스트림 작업 지원을 유지하면서도 훨씬 빠른 임베딩 방법을 제공하는 것이 필요하다.

Method: Luxical은 희소 TF–IDF 특징을 입력으로 사용하고, 이를 작은 ReLU 기반 신경망에 통과시켜 조밀 벡터(lexical-dense embedding)를 생성한다. 이 네트워크는 지식 증류(knowledge distillation)를 통해 대형 트랜스포머 텍스트 임베딩 모델의 출력 벡터를 근사하도록 학습된다. 이렇게 함으로써 텍스트 임베딩의 품질과 유연성은 유지하면서도 연산 비용을 크게 절감한다.

Result: Luxical의 구체적인 모델을 두 가지 상이한 응용에서 평가했다: (1) 타깃 웹 크롤 문서 검색 실험, (2) 텍스트 분류 기반의 언어 모델 데이터 큐레이션 태스크. 이 실험들에서 Luxical은 다양한 크기의 뉴럴 베이스라인 대비 약 3배에서 100배까지 추론 속도 향상을 보였고, 데이터 큐레이션 태스크에서는 FastText 추론 속도와 유사한 수준을 달성했다. 품질 면에서도 뉴럴 베이스라인과 비슷한 성능을 유지했다.

Conclusion: Luxical은 TF–IDF 희소 특징과 소형 신경망, 지식 증류를 결합한 고속 텍스트 임베딩 방식으로, 대형 트랜스포머 임베딩 모델 대비 훨씬 낮은 계산 비용으로 유사한 품질과 유연성을 제공한다. 이를 통해 웹 규모 텍스트 조직, 예를 들어 대규모 문서 검색과 언어 모델 학습 데이터 큐레이션에 적합한 컴퓨트/품질 트레이드오프를 달성하며, 오픈소스로 공개되어 실무 적용 가능성을 뒷받침한다.

Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.

</details>


### [4] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 이 논문은 소아 치과 진료기록에서 항생제 처방을 더 정확하고 안전하게 추천하기 위해 지식 그래프와 RAG, 다단계 안전 검증을 결합한 지식-가이드 LLM(KG-LLM) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 소아 치과 진료기록은 서술식 문장, 불완전한 방사선 소견, 복잡한 약물 안전 제약 조건 등으로 구성되어 있어 전통적인 규칙 기반 임상 의사결정지원 시스템이 정확히 해석하고 안전한 항생제 처방을 제안하는 데 한계가 있다. 특히, 항생제 선택·용량·기간 결정에서 알레르기, 금기, 과용량 위험을 자동으로 점검하기 어렵다는 문제가 있다.

Method: 1) 임상 NER/RE 모듈을 사용해 치과 진료기록과 영상 판독문에서 질환, 약물, 증상, 연령 등 엔티티와 관계를 추출해 구조화한다. 2) 소아 치과 지식 그래프에서 가이드라인, 약물 안전 규칙, 유사 과거 증례를 검색(RAG)하여 LLM 입력 컨텍스트로 제공한다. 3) LLM이 이를 바탕으로 진단 요약과 항생제 약물-용량-기간을 생성한다. 4) 생성 결과에 대해 규칙 기반 검증(가이드라인·용량 범위·금기사항 체크)과 학습 기반 분류기(알레르기, 금기, 용량 오류 탐지)를 결합한 이중 안전 검증 파이프라인으로 위험 처방을 필터링·수정한다. 5) 32,000건의 비식별 소아 치과 내원 기록으로 성능을 평가하고, 모듈별 제거(ablation) 실험을 수행한다.

Result: 도메인 적응된 Llama-2 임상 베이스라인과 비교했을 때, 제안한 KG-LLM은 기록 이해 성능(F1 0.914 vs 0.867), 약물-용량-기간 예측 정확도(Top-1 0.782 vs 0.716)를 유의하게 향상시켰고, 안전하지 않은 항생제 제안을 50% 감소시켰다. 추가적으로 요약 품질, 추천 정확도, 전반적 안전성 점수에서 모두 우수한 성능을 보였으며, 모듈 제거 실험에서 지식 그래프, RAG, 안전 모듈 각각이 임상 신뢰성과 해석 가능성 향상에 기여함을 확인했다.

Conclusion: 소아 치과 영역에서 지식 그래프와 RAG, 다단계 안전 검증을 통합한 KG-LLM은 비정형 진료기록을 효과적으로 이해하고, 근거 기반이면서도 안전한 항생제 추천을 제공할 수 있음을 보였다. 이는 단순 LLM 또는 규칙 기반 시스템보다 해석 가능성과 안전성이 높으며, 향후 다른 치의학·소아 진료 분야의 약물 처방 의사결정지원 시스템으로 확장 가능성이 크다.

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [5] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 이 논문은 GraphRAG에서 지식 그래프로부터 가져온 구조적 지식을 LLM이 어떻게 사용·왜곡하는지를 해석 가능한 지표로 분석하고, 이를 기반으로 환각을 탐지하는 경량 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: GraphRAG는 지식 그래프의 서브그래프를 선형화해 LLM에 제공하지만, LLM이 관계·위상 정보를 잘 이해하지 못해, 주어진 지식과 모순되는 환각을 생성하는 문제가 있다. 기존 연구는 이런 실패가 왜 발생하는지, 모델이 실제로 어떤 그래프 정보에 의존하는지, 내부 표현이 지식과 어떻게 정렬되는지에 대한 기계론적 분석이 부족하다.

Method: 1) LLM이 입력 서브그래프에서 어떤 부분에 의존하는지 측정하기 위해 두 가지 지표를 제안한다. (a) Path Reliance Degree(PRD): 최단 경로 상의 트리플에 대한 과도한 의존 정도를 계량화. (b) Semantic Alignment Score(SAS): 모델 내부 표현이 회수된 지식과 얼마나 의미적으로 정렬되는지를 측정. 2) 지식 기반 질의응답 태스크에서 PRD와 SAS를 계산해 실패 패턴을 분석한다. 3) 이 해석 결과를 활용해, 그래프 기반 경량 사후(hoc) 환각 탐지기 Graph Grounding and Alignment(GGA)를 설계하고, 기존 의미 유사도·신뢰도 기반 베이스라인과 성능(AUC, F1)을 비교한다.

Result: 실험 결과, 높은 PRD와 낮은 SAS가 나타나는 경우 LLM이 눈에 띄는 경로 정보에 과도하게 의존하고, 전체 그래프 구조와 충분히 의미적으로 정렬되지 못해, 회수된 지식과 모순되는 환각이 증가하는 패턴을 확인했다. 제안한 GGA 환각 탐지기는 강력한 의미적·confidence 기반 베이스라인보다 AUC와 F1에서 우수한 성능을 보였다.

Conclusion: LLM이 GraphRAG 설정에서 구조화된 지식을 활용할 때, 최단 경로 중심의 편향과 약한 의미적 정렬이 환각을 야기하는 주요 요인임을 기계론적 해석 지표(PRD, SAS)를 통해 보였다. 또한 이러한 분석을 바탕으로 설계한 GGA는 실제 환각 탐지에 효과적이었으며, 향후 보다 신뢰할 수 있는 GraphRAG 시스템 설계에 유용한 통찰을 제공한다.

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [6] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: MindShift 벤치마크로 LLM이 프롬프트에 따라 성격 특성을 얼마나 잘 모사하는지 심리검사 기반으로 측정한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 사용자 지정 성격·태도를 얼마나 유연하게 반영할 수 있는지, 그리고 서로 다른 모델·학습 방식에 따라 심리적 특성이 어떻게 달라지는지를 체계적으로 평가하기 위해서이다.

Method: 심리학에서 가장 널리 연구된 검사지인 MMPI를 LLM에 맞게 변형해 적용하고, 성격 강도가 다양한 페르소나 프롬프트를 설계했다. 각 페르소나에 따라 LLM의 응답 패턴을 측정·비교하여, 역할 따른 응답 변화와 심리척도 점수를 분석하는 MindShift 벤치마크를 구성했다.

Result: 최근 LLM일수록 주어진 역할·페르소나를 더 일관되게 인식하고 따르며, 모델 계열과 아키텍처에 따라 심리검사 응답 패턴과 성격 특성 모사 능력에 유의한 차이가 나타났다.

Conclusion: 훈련 데이터와 정렬(alignment) 기법의 발전이 LLM의 ‘역할 몰입’과 성격 모사 능력을 향상시키며, MindShift는 서로 다른 LLM의 심리적 적응력과 편향을 비교·평가하는 표준 벤치마크로 활용될 수 있다.

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [7] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: 이 논문은 보상 모델과 정책(LLM) 사이의 불일치를 정량화하고, 갈등이 큰 사례만 선택적으로 사람 피드백을 받아 보상 모델과 정책을 동시에 개선하는 방법(SHF-CAS)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 보상모델 기반 파인튜닝(RLHF 등)은 보상 모델이 인간 선호를 정확히 반영한다고 가정하지만, 실제로는 노이즈·편향·커버리지 한계로 인해 잘못된 보상 신호에 맞춰 모델이 ‘보상만 최적화하고 가치에는 어긋나는’ 행동을 보이는 문제가 있다. 이 논문은 특히 보상 모델과 기본 언어모델이 모두 잘 모르는 영역에서 이런 불일치가 심해진다고 보고, 해당 영역을 체계적으로 찾아내고 교정하는 프레임워크가 필요하다는 동기에서 출발한다.

Method: 파인튜닝을 ‘지식 통합 과정’으로 보고, 기본 정책(베이스 LLM)과 보상 모델 간의 응답 선호 순위를 비교해 ‘프록시-정책 갈등(proxy-policy conflict)’을 탐지한다. 지역적 충돌 정도를 나타내는 PACS(Proxy-Policy Alignment Conflict Score)와 전역 순위 차이를 측정하는 Kendall-Tau distance 두 지표를 정의한다. 이 갈등 정보를 활용해 SHF-CAS(Selective Human-in-the-loop Feedback via Conflict-Aware Sampling) 알고리즘을 설계하여, 갈등 점수가 높은 QA 쌍만 선별해 사람 피드백을 추가 수집하고 이를 이용해 보상 모델과 정책을 효율적으로 재학습한다.

Result: 두 개의 정렬(alignment) 벤치마크 과제에서 실험한 결과, 제안한 갈등 지표로 선택된 샘플에만 사람 피드백을 집중해도 전반적인 정렬 성능이 향상되며, 특히 편향된(biased) 보상 모델을 사용한 경우에도 성능 저하 없이 혹은 그 이상으로 좋은 결과를 얻을 수 있음을 보였다. 이는 제한된 피드백 예산 하에서 갈등 기반 샘플링이 데이터 효율적인 정렬 방법임을 시사한다.

Conclusion: 보상 모델과 정책 간의 불일치는 단순한 노이즈가 아니라, 양측이 모두 충분한 지식을 갖고 있지 않은 ‘공유된 무지’ 영역에서 특히 심각하게 드러난다는 관점을 제시한다. 이러한 갈등을 PACS 및 Kendall-Tau 기반으로 체계적으로 측정하고, 고갈등 사례에 인적 검토를 집중하는 SHF-CAS를 통해, 보상 모델과 정책을 함께 정제하는 효율적인 정렬 파이프라인을 제공한다. 이는 LLM 정렬 실패를 해석하는 새로운 관점을 주며, 추후 인간-모델-보상모델 삼자 상호작용을 활용한 정렬 연구의 기초를 마련한다.

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [8] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE는 대형 언어모델의 멀티턴 대화를 위해 ‘토큰 우선’ 대신 ‘개념 우선’ 상호작용 계층을 제안하여, 대화 안정성과 토큰 효율을 높이는 방법을 탐색한 연구이다.


<details>
  <summary>Details</summary>
Motivation: 현재 LLM은 턴이 바뀔 때마다 이전 대화 전체를 토큰으로 다시 넣어야 해서, 내부 상태가 유지되지 않고, 프롬프트가 길어지며, 추론 방식이 흔들리고 일관성이 떨어진다는 문제가 있다. 이 구조적 한계를 완화하기 위해, 모델 내부를 바꾸지 않으면서도 멀티턴 상호작용의 안정성과 효율을 높일 수 있는 새로운 인터페이스 계층이 필요하다.

Method: CORE라는 개념-우선 상호작용 레이어를 제안한다. (1) 소수의 보편적 인지 연산자(cognitive operators) 라이브러리를 정의하고, (2) 과제, 제약, 선호, 중간 결과 등을 요약하는 압축 의미 상태를 ‘Local Concept’로 지속적으로 유지한다. 각 모델 호출 시 전체 대화 기록 대신 이 Local Concept, 사용자의 최신 지시, 선택된 연산자만을 입력으로 사용해 대화를 진행하는 구조를 설계·시뮬레이션했다.

Result: CORE 동작을 모사한 초기 프로토타입 실험에서 누적 프롬프트 토큰 수가 약 42% 감소하는 경향을 보였다. 다만 이는 제한된 프로토타입 환경에서의 수치이며, 실제 배치 환경의 성능 지표로 곧바로 일반화할 수 없다고 명시한다.

Conclusion: CORE는 모델 가중치를 수정하지 않고도 개념적 추론과 언어 생성 과정을 분리해, 멀티턴 상호작용을 더 안정적이고 확장 가능하게 만드는 모델 비종속적 메커니즘이 될 수 있음을 시사한다. 이는 대화가 길어져도 상태를 토큰 히스토리 대신 의미 상태(Local Concept)로 관리하는 새로운 설계 방향을 제안한다.

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [9] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 이 논문은 추가 학습 없이도 긴 문맥에서 효율적으로 추론할 수 있게 해주는 희소 주의 메커니즘(TCA-Attention)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 자기 주의(self-attention)는 긴 범위 의존성을 잘 모델링하지만, 시퀀스 길이에 대해 계산·메모리 비용이 이차적으로 증가하는 문제가 있다. 특히 수십만 토큰 이상의 롱컨텍스트에서 비용이 극단적으로 커진다. 기존 희소 주의 및 KV 캐시 압축 방법들은 고정된 패턴에 의존하거나, 프리필링/디코딩 중 한 단계만 다루거나, 별도 재학습이 필요하다는 한계가 있다. 이를 보완하는, 학습 없이 바로 쓸 수 있는 효율적 롱컨텍스트 주의 메커니즘이 필요하다.

Method: TCA-Attention(Training-free Context-adaptive Attention)을 제안한다. (1) 오프라인 캘리브레이션 단계: 하나의 포워드 패스를 통해 각 어텐션 헤드별 희소성 예산(얼마나 많이/적게 토큰을 볼지)을 결정한다. (2) 온라인 토큰 선택 단계: 추론 시 가벼운 중복도(redundancy) 지표를 이용해 정보량이 큰 핵심 토큰만을 남기고 나머지를 제거하여 희소 주의를 수행한다. 이 방식은 모델 파라미터를 바꾸거나 구조를 수정하지 않고, 플러그인 형태로 기존 LLM에 적용된다.

Result: 이론적으로는 원래 풀 어텐션에 대한 근사 오차가 경계 내에 머무름을 보였다. 실험적으로는 128K 길이 컨텍스트에서 약 2.8배 추론 속도 향상과 61%의 KV 캐시 메모리 절감을 달성하면서도, 여러 벤치마크에서 풀 어텐션과 거의 동등한 성능을 유지함을 보여준다.

Conclusion: TCA-Attention은 훈련 없이 바로 적용 가능한 컨텍스트 적응형 희소 주의 기법으로, 프리필링과 디코딩 모두를 가속하고 KV 캐시 사용량을 크게 줄이면서도 성능 저하를 최소화한다. 따라서 대규모 LLM의 롱컨텍스트 추론을 위한 실용적인 플러그앤플레이 솔루션을 제공한다.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [10] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 이 논문은 영어 에세이에서 AI-텍스트 탐지기가 학생의 성별, 인종/민족, ELL 여부, 경제적 배경에 따라 편향된 오판을 하는지를 실증적으로 분석한다.


<details>
  <summary>Details</summary>
Motivation: 텍스트 생성 모델의 발전과 함께 AI가 쓴 글을 탐지하려는 수요가 급증했지만, 탐지기의 오판이 학생·지원자 선발, 평가, 징계 등에 사용될 경우 특정 사회집단에 불리한 편향을 심화시킬 위험이 있다. 특히 교육 현장에서 학생 에세이를 자동으로 검사하거나 표절/대리 작성 여부를 판단할 때, 탐지기가 사회경제적 약자나 소수집단의 글을 ‘기계가 쓴 글’로 잘못 분류한다면 심각한 불이익을 초래할 수 있다. 따라서 실제 학생 에세이 데이터를 이용해, 현재 널리 쓰이는 탐지 시스템들이 어떤 속성에 대해 체계적 편향을 보이는지 정량적으로 검증할 필요가 있다.

Method: 연구진은 학생 에세이로 구성된 데이터셋을 구축하고, 16개의 서로 다른 영어 기계-생성 텍스트 탐지 시스템에 이 에세이들을 입력하여 각 에세이가 ‘기계 작성’으로 분류될 확률이나 레이블을 수집한다. 각 에세이에는 네 가지 속성(성별, 인종/민족, 영어 학습자 여부(ELL), 경제적 지위)이 주석으로 포함된다. 그런 다음 회귀 기반 통계 모델을 사용해 각 속성이 탐지 결과에 미치는 효과의 유의성과 크기를 추정하고, 속성 조합별(예: 비백인+ELL 등) 하위 그룹 분석을 수행한다. 추가로, 사람 평가자에게 동일한 탐지 과제를 부여해 인간의 성능과 편향 양상을 비교 분석한다.

Result: 탐지 시스템 전반에서 관찰되는 편향 양상은 모델마다 일관되지는 않지만, 몇 가지 반복적으로 나타나는 문제점이 확인된다. (1) 여러 탐지기는 사회적으로 불리한 위치에 있는 집단(예: 소수 인종, ELL 등)의 에세이를 사람 대신 기계가 쓴 글로 분류하는 경향이 있다. (2) ELL 학생의 에세이는 비-ELL 학생보다 기계 작성으로 분류될 확률이 유의하게 높다. (3) 경제적으로 불리한 학생의 에세이는 오히려 기계 작성으로 분류될 가능성이 더 낮은 경향이 관찰된다. (4) 특히 비백인 ELL 학생의 에세이는 백인 ELL 학생에 비해 과도하게 기계 작성으로 분류되는 불균형이 나타난다. 한편, 인간 평가자는 전체적으로 탐지 정확도는 낮지만, 위의 네 속성에 대해서 유의미한 통계적 편향을 보이지 않는다.

Conclusion: 현재 사용 중인 영어 기계-텍스트 탐지 시스템들은 학생 에세이 환경에서 특정 인구집단, 특히 비백인 ELL 학생 등에게 체계적으로 불리한 오분류를 일으킬 수 있으며, 이는 교육 평가와 징계, 입시 등 실제 응용 맥락에서 심각한 공정성 문제로 이어질 수 있다. 인간은 탐지 성능은 낮지만 적어도 본 연구에서 검증한 속성들에 대해 뚜렷한 편향을 드러내지 않으므로, 단순히 인간 대신 탐지기를 도입하는 것이 공정성 측면에서 항상 이득이 되는 것은 아니다. 따라서 탐지 시스템을 설계·도입할 때는 공정성 평가를 필수적으로 포함하고, 편향 완화 기법·보완적 인간 검토 절차를 함께 설계해야 한다는 함의를 제공한다.

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [11] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: 이 논문은 다양한 AI 전략(모델·디코딩 방식 조합)에 작업을 효율적으로 라우팅하기 위한 지속 학습(continual) 라우팅 프레임워크 CONCUR를 제안하고, 더 높은 정확도와 더 낮은 비용을 달성했음을 보인다.


<details>
  <summary>Details</summary>
Motivation: 실제 AI 시스템에서는 작업 난이도와 특성이 제각각이어서, 하나의 고정된 모델·디코딩 전략으로는 성능과 비용 측면에서 비효율적이다. 따라서, 주어진 입력(질의)에 대해 어떤 모델 및 디코딩 전략 조합이 가장 적합한지 자동으로 선택(라우팅)하는 시스템이 필요하다. 기존 방법은 모든 전략에 대해 하나의 공통 라우터를 학습해 새 전략이 등장할 때마다 전체를 재학습해야 하고, 단일 입력 표현만 사용해 복잡한 라우팅 결정을 잘 포착하지 못한다. 또한 전략이 순차적으로 추가되는 continual 설정에서의 일반화 성능과 학습 비용 문제도 크다. 이를 해결할 동적·모듈형 라우팅 프레임워크가 필요하다.

Method: CONCUR는 전략별로 독립된 예측기(predictor)를 두는 모듈형 라우팅 구조를 사용한다. 즉, 각 모델·디코딩 전략마다 별도의 작은 라우팅 모델을 학습하여, 새 전략이 추가되면 해당 전략용 예측기만 추가로 학습하면 된다. 또한 작업(task)과 계산 전략(strategy) 모두에 대해 다중 표현(multiple representations)을 활용해, 입력과 전략의 특성을 풍부하게 인코딩하고 이를 기반으로 성능·비용을 예측한다. 이 프레임워크는 예산 제약이 없는 unconstrained routing뿐 아니라, 비용/쿼리 수 제한이 있는 constrained routing(예산 기반 라우팅)도 지원한다.

Result: 지식·추론 중심의 다양한 벤치마크에서, CONCUR는 (1) 가장 강력한 단일 전략을 항상 사용하는 방식보다, (2) 기존 강력한 라우팅 기법보다 더 높은 end-to-end 정확도와 더 낮은 추론 비용을 달성했다. 특히 전략이 순차적으로 추가되는 continual 환경에서도, 기존 방식 대비 재학습 비용을 줄이면서 더 나은 성능을 보였다. 또한 분포 내(in-distribution)와 분포 밖(out-of-distribution) 데이터 양쪽에서 일반화 성능을 입증했다.

Conclusion: CONCUR는 전략별 분리 예측기와 다중 표현을 활용해, 전략이 시간이 지나면서 계속 추가되는 현실적인 환경에서 효과적으로 라우팅을 수행하는 프레임워크다. 이 방법은 새로운 전략을 쉽게 통합할 수 있고, 예산 제약 유무에 관계없이 높은 정확도와 낮은 비용을 동시에 달성한다. 결과적으로, 대규모 AI 시스템에서 동적·지속적 전략 선택 문제를 해결하는 실용적인 솔루션을 제시한다.

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [12] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 이 논문은 언어 모델을 활용해 ‘가능한 언어’와 ‘불가능한 언어’를 가르는 기준을 탐구하자는 제안을 담고 있다.


<details>
  <summary>Details</summary>
Motivation: 인간이 어떤 언어는 쉽게 배울 수 있고, 어떤 언어는 전혀 자연 언어로 존재하지 않는지(가능/불가능 언어의 구분)를 이해하려면, 인간 학습을 뒷받침하는 귀납 편향(inductive bias)을 밝혀야 한다. 그러나 실제 인간 아동을 대상으로 체계적으로 이런 실험을 하기에는 비용과 제약이 크기 때문에, 대규모 언어 모델을 대리 실험 도구로 삼아 이 편향을 탐구하려는 동기가 있다.

Method: 기존 및 새로운 LM 아키텍처를 대상으로, 정의된 ‘가능한 언어 vs 불가능한 언어’ 데이터(형식언어·인공문법 등)를 주고, 두 종류를 얼마나 잘 구분하는지 평가한다. 이 성능을 기준으로 LM 구조와 학습 방법을 단계적으로 개선하면서, 어떤 구조적 특징이 사람과 유사한 가능/불가능 언어 구분 능력을 만들어내는지 분석한다. 이를 통해 LM의 내부 메커니즘과 인간 인지과정 사이를 연결하는 가설(linking hypotheses)을 세운다.

Result: 초록에서는 구체적 실험 결과를 보고하지 않고, 연구 프로그램의 청사진(로드맵)을 제안하는 수준에 머문다. 즉, LMs를 통해 자연언어의 가능성 제약을 체계적으로 시험할 수 있다는 이론적·방법론적 가능성을 주장한다.

Conclusion: 언어 모델을 단순한 공학 도구가 아니라 인지과학 실험 장치로 활용할 수 있으며, 특히 자연언어 문법의 ‘가능성 공간’을 규명하고 인간 언어 학습의 귀납 편향을 밝히는 데 중요한 역할을 할 수 있다는 결론을 제안한다. 이를 위해 LM 아키텍처를 단계적으로 재설계·미세조정하는 장기 연구 프로그램이 필요하다고 주장한다.

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [13] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 법원 판결문과 보도자료, 프롬프트로 구성된 독일 최고법원 보도자료 생성용 데이터셋을 제안하고, 다양한 LLM의 요약 성능을 벤치마크한다.


<details>
  <summary>Details</summary>
Motivation: 기존 법률 NLP는 전문가용 기술 요약에 치우쳐 일반 시민이 이해하기 쉬운 설명형 보도자료 생성을 다루지 않았기 때문에, 시민 지향적 법률 커뮤니케이션을 지원할 필요가 있다.

Method: 독일 최고법원 판결문과 그에 대응하는 인간 작성 보도자료를 수집해 6.4k 규모 CourtPressGER 데이터셋을 구축하고, 판결문-보도자료-LLM 프롬프트 삼중 구조로 정리한다. 이후 소·대형 LLM들을 대상으로 참조 요약 지표, 사실 일치성, LLM-as-judge, 전문가 랭킹 등을 활용해 보도자료 생성 성능을 평가한다.

Result: 대형 LLM은 긴 판결문에서도 계층적 구조를 크게 활용하지 않고도 고품질 보도자료 초안을 생성하며, 소형 모델은 긴 문서를 처리하기 위해 계층적 설정이 필요하다. 초기 벤치마크에서 모델별 성능 차이가 존재하며, 여전히 인간 작성 보도자료가 가장 높은 평가를 받는다.

Conclusion: CourtPressGER는 장문 법률 텍스트로부터 시민 친화적 보도자료를 생성·평가하기 위한 새로운 벤치마크를 제공하며, 대형 LLM의 실용 가능성과 소형 모델의 한계를 동시에 보여준다. 인간 보도자료의 우수성은 향후 인간-LLM 협업 및 품질 향상 연구의 필요성을 시사한다.

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [14] [Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making](https://arxiv.org/abs/2512.09440)
*Qingyuan Zhang,Yuxi Wang,Cancan Hua,Yulin Huang,Ning Lyu*

Main category: cs.CL

TL;DR: 지식 강화 LLM 기반의 설명 가능한 추론 프레임워크를 제안하여 금융 의사결정의 정확도와 해석 가능성을 동시에 향상시킨 연구.


<details>
  <summary>Details</summary>
Motivation: 기존 금융 의사결정 모델은 (1) 고정된 파라미터 지식에 의존해 최신 정보 반영이 어렵고, (2) 사실성과 일관성이 떨어지며, (3) 예측 결과에 대한 명시적 추론 과정·인과 체인을 제공하지 못하는 한계를 가진다. 이를 해결하기 위해 외부 지식과 LLM을 결합해, 실제 금융 환경에서 신뢰할 수 있고 설명 가능한 의사결정을 수행할 수 있는 방법이 필요하다.

Method: (1) 금융 텍스트와 구조화 데이터(예: 재무지표)를 인코딩해 통합 의미 표현을 만든다. (2) 의미 유사도 기반으로 외부 지식베이스에서 과제 관련 정보를 검색한다. (3) 내부 표현과 외부 지식을 가중 결합(Weighted Fusion)하여 유창성과 사실성을 동시에 고려한 표현을 구성한다. (4) 다중 헤드 어텐션을 활용해 논리적 인과·추론 체인을 구성하고, 생성 과정에서 이를 드러내도록 한다. (5) 예측 정확도를 위한 태스크 목적과 설명 일관성을 위한 목적을 공동 최적화(joint optimization)하여 모델을 학습한다.

Result: 금융 텍스트 처리 및 의사결정 관련 벤치마크에서 여러 베이스라인(전통 모델 및 일반 LLM 기반 방법)보다 높은 예측 정확도, 더 나은 텍스트 생성 품질, 더 강한 사실적 근거(knowledge grounding)를 달성하였다. 추론 과정의 인과 관계와 근거가 보다 명시적으로 드러나 설명 가능성이 개선되었음이 실험을 통해 확인되었다.

Conclusion: 외부 지식 검색과 의미 표현 결합, 그리고 다중 헤드 어텐션 기반 추론 체인을 통합한 제안 방식은 기존 금융 모델의 의미 범위 부족과 추론 불투명성 문제를 효과적으로 완화한다. 그 결과, 복잡한 금융 상황에서 신뢰 가능한 예측과 함께 설명 가능한 의사결정을 제공할 수 있어 실무 적용 가능성이 크다는 점을 보여준다.

Abstract: This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

</details>


### [15] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 대규모 언어모델(LLM) 기반의 새로운 텍스트 분류 알고리즘을 제안하고, 기존 RNN·GNN·Transformer 기반 방법보다 모든 평가지표에서 우수함을 보였다는 논문 초록.


<details>
  <summary>Details</summary>
Motivation: 전통적인 텍스트 분류 기법은 긴 문맥 의존성 포착, 문맥 의미 이해, 심한 클래스 불균형 처리에 한계를 가진다. 이를 극복하고 복잡한 데이터 환경에서도 강인하고 일반화 성능이 높은 텍스트 분류 모델을 만들기 위해 연구가 수행되었다.

Method: 대규모 사전학습 언어모델을 사용해 텍스트를 인코딩하고, 문맥 표현을 학습한 뒤, 어텐션 메커니즘으로 중요한 특징을 선택적으로 강조한다. 이후 전역(global) 및 가중(weighted) 특성 집계를 결합해 문서 수준 벡터를 만들고, 완전연결층과 Softmax로 분류를 수행하며, 크로스엔트로피 손실로 파라미터를 최적화한다. RNN, GNN, Transformer 등 여러 베이스라인과 비교 실험을 수행한다.

Result: 제안한 방식이 Precision, Recall, F1-Score, AUC 전 지표에서 기존 모델들을 상회했으며, 특히 Recall과 AUC에서 큰 폭의 개선을 보였다. 또한 은닉 차원 변화가 AUC에 미치는 영향, 클래스 불균형 비율이 Recall에 미치는 영향을 분석해 모델이 다양한 설정에서 안정적 성능을 유지함을 확인했다.

Conclusion: LLM 기반 어텐션·집계 전략을 결합한 텍스트 분류 방법은 전통적 모델 대비 성능을 유의미하게 향상시키며, 하이퍼파라미터와 데이터 조건 변화에도 견고하고 적응적인 특성을 보여 복잡한 실제 데이터 환경에 적용 가능함을 입증했다.

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [16] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: 이 논문은 LLM 기반 검색엔진(LLM-SE)과 전통 검색엔진(TSE)을 대규모로 비교 분석해, 출처 다양성, 신뢰도, 중립성, 안전성, 그리고 LLM-SE의 소스 선택 요인을 실증적으로 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 검색엔진이 요약된 답변을 제공하면서도 인용 투명성이 제한적인 새로운 정보 탐색 패러다임을 만들고 있으나, 이 변화가 신뢰와 투명성에 미치는 영향이 체계적으로 검증되지 않았기 때문이다.

Method: 6개의 LLM 기반 검색엔진과 2개의 전통 검색엔진에서 총 55,936개의 쿼리와 그에 대응하는 검색 결과를 수집·비교하고, 도메인 다양성·신뢰도·정치적 중립성·안전성 지표를 계량 평가하며, 특성(feature) 기반 분석으로 LLM-SE의 출처 선택에 영향을 주는 요인을 식별한다.

Result: LLM-SE는 TSE보다 인용 도메인의 다양성이 높고, 전체 도메인의 37%는 LLM-SE에만 존재한다. 그러나 신뢰도, 정치적 중립성, 안전성 지표에서는 TSE를 능가하지 못했다. 추가로, 특성 분석을 통해 어떤 도메인/페이지 특징이 LLM-SE의 출처 선택에 중요한지 규명했다.

Conclusion: LLM-SE는 출처 다양성 측면에서 장점을 보이지만, 신뢰성과 중립성·안전성 면에서 기존 검색엔진을 대체할 수준은 아니며, 출처 선택 메커니즘에 대한 이해를 통해 이용자, 사이트 운영자, 개발자에게 투명성과 품질 개선을 위한 실질적인 시사점을 제공한다.

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [17] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 이 논문은 강화학습 기반의 프레임워크(\model{})를 제안하여, LLM이 그래프와 텍스트를 혼합하여 적응적으로 검색·추론하는 멀티턴 RAG를 수행하도록 한다는 내용이다.


<details>
  <summary>Details</summary>
Motivation: 기존 RAG는 대부분 비구조적 텍스트 위주이며, 최근 RL을 통해 멀티턴 추론이 가능해졌지만, 그래프/텍스트를 함께 사용하는 하이브리드 RAG에는 여러 문제가 있다. 첫째, 그래프 기반 또는 하이브리드 시스템은 고정되거나 수작업 설계된 파이프라인에 의존해, 추론 과정 중 새 증거를 유연하게 통합하기 어렵다. 둘째, 그래프는 다중 홉 추론에 중요한 관계 구조를 제공하지만, 검색 비용이 크다. 따라서, 추론 과정에서 ‘언제’, ‘어디서(텍스트 vs 그래프)’, ‘얼마나’ 검색할지 스스로 결정하면서도 효율을 고려하는 통합 프레임워크가 필요하다.

Method: 저자들은 \model{}이라는 RL 기반 프레임워크를 제안한다. 이 프레임워크에서 LLM은 하나의 통합된 생성 정책(policy)을 통해 (1) 현재까지의 정보를 바탕으로 내부적으로 추론을 계속할지, (2) 추가 증거가 필요하다면 텍스트 코퍼스에서 검색할지 그래프에서 검색할지, (3) 언제 최종 답변 생성을 종료할지를 단계적으로 선택한다. 전체 질의응답 과정을 강화학습으로 공동 최적화하며, 두 단계로 구성된 학습 전략을 설계한다. 이 두 단계는 과제 성능(정답률 등)뿐 아니라 검색 비용(특히 고비용인 그래프 검색)을 명시적으로 고려하여, 불필요한 검색 오버헤드를 줄이면서도 필요한 경우 하이브리드 증거를 적극 활용하도록 유도한다.

Result: 다섯 개의 질의응답 벤치마크에서 제안한 \model{}를 평가한 결과, 기존의 텍스트 기반 RAG와 그래프/하이브리드 RAG 베이스라인들을 유의미하게 상회하는 성능을 보였다. 특히 복잡한 다중 홉·다단계 추론 상황에서, RL을 통한 적응적 검색 정책이 정답률과 검색 효율(검색 횟수, 그래프 호출 빈도 등) 모두에서 이점을 제공함을 실험적으로 확인했다.

Conclusion: 엔드 투 엔드 강화학습으로 LLM의 검색·추론·응답 생성을 통합적으로 최적화하면, 그래프와 텍스트를 동시에 활용하는 복잡한 RAG 시나리오에서 더 정확하고 효율적인 질의응답이 가능하다는 것이 논문의 핵심 결론이다. 제안한 \model{}은 고정 파이프라인 대신, 추론 중 동적으로 어떤 소스(그래프/텍스트)를 얼마나 활용할지 결정할 수 있어, 복잡한 멀티홉 추론 과제에서 하이브리드 RAG의 잠재력을 효과적으로 끌어낸다.

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [18] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 이 논문은 언어과학 연구에서 LLM을 체계적이고 재현 가능하게 활용하기 위한 두 가지 종합 방법론적 프레임워크(방법 선택 프레임워크와 다단계 연구 파이프라인 프레임워크)를 제안하고, 사례 연구와 실험으로 그 타당성을 검증한다.


<details>
  <summary>Details</summary>
Motivation: 현재 LLM이 언어과학 전반에 널리 사용되고 있으나, 연구자들이 제각각 임의로 사용하는 경향이 강해 방법론적 파편화, 체계 부족, 재현성 문제 등이 발생하고 있다. 따라서 연구 질문과 LLM 사용 방식 사이의 전략적 정렬을 위한 일관된 가이드라인과 프레임워크가 필요하다.

Method: (1) 연구 목표에 따라 세 가지 LLM 활용 접근을 구분·정의: (a) 범용 모델을 프롬프트 기반으로 활용해 탐색적 분석 및 가설 생성, (b) 오픈소스 모델을 파인튜닝하여 이론 주도적 검증 연구와 고품질 데이터 생성, (c) 맥락화 임베딩을 추출해 정량 분석 및 내부 메커니즘 프로빙. 각 접근법에 대해 기술적 구현 방법, 장단점, 트레이드오프를 사례 연구와 함께 상세히 기술. (2) 이 세 접근을 조합해 실제 연구 과정에 적용 가능한 다단계 연구 파이프라인 구성 틀을 제안. (3) 제안된 프레임워크를 검증하기 위해 후향적 분석(기존 연구에 적용), 전향적 적용(새 연구 설계), 전문가 설문 평가를 수행.

Result: 세 가지 LLM 활용 접근을 구조화한 방법 선택 프레임워크와, 이를 바탕으로 한 다단계 연구 파이프라인 구성 프레임워크를 제시하였다. 경험적 사례 및 실험 결과와 전문가 평가를 통해 이 프레임워크들이 실제 언어과학 연구 설계와 수행에 유용하게 작동하며, 연구 질문과 LLM 방법론의 정렬을 도와준다는 점을 확인했다.

Conclusion: 제안된 프레임워크는 언어과학에서 LLM 사용을 ‘편의적·임시적 도구’ 수준에서 벗어나, 이론적 목표와 방법이 명확히 대응하는 재현 가능하고 검증 가능한 과학적 체계로 격상시키는 데 기여한다. 특히 연구 질문-모델 활용 방식-분석 절차 간의 전략적 정렬을 강제함으로써, LLM 메커니즘의 비판적 평가를 가능하게 하고, 전통적 언어학을 보다 견고한 실증 과학으로 전환하는 기반을 제공한다.

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [19] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: LLM 기반 3단계 프레임워크로 중국어 혐오 발화를 정교하게 탐지하는 연구.


<details>
  <summary>Details</summary>
Motivation: 중국 소셜 미디어에서 혐오 발화가 급증하고 있으나, 맥락 의존적 수사, 은어·슬랭, 암시적 표현 때문에 기존 시스템이 탐지에 실패한다는 문제를 해결하고자 함.

Method: (1) Prompt Engineering: 맥락 정보를 고려한 프롬프트를 설계해 LLM이 암시적 혐오 패턴을 추출하도록 유도. (2) Supervised Fine-tuning: 혐오 발화 탐지용 태스크 특화 특징을 주입하며 지도 미세조정으로 도메인 적응 강화. (3) LLM Merging: 서로 다른 방식으로 미세조정된 LLM들을 병합해 OOD(분포 외) 사례에 대한 견고성을 높임. STATE-ToxiCN 벤치마크에서 성능 평가.

Result: STATE-ToxiCN 데이터셋에서 기존 베이스라인보다 세밀한(세부 카테고리 수준의) 혐오 발화 탐지 성능에서 우수한 결과를 보임.

Conclusion: 프롬프트 설계–지도 미세조정–모델 병합으로 이어지는 3단계 LLM 프레임워크가 중국어 혐오 발화의 미묘하고 맥락 의존적인 패턴을 효과적으로 포착하며, 일반적인 베이스라인보다 더 강건하고 정밀한 탐지기를 만들 수 있음을 입증.

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [20] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 에스토니아어 문서의 주관성 정도를 연속 점수(0~100)로 라벨링한 데이터셋을 구축하고, 사람과 LLM(GPT-5)의 주관성 판단을 비교한 연구이다.


<details>
  <summary>Details</summary>
Motivation: 에스토니아어처럼 소수 언어에서는 감성·주관성 분석용 자원이 부족하다. 특히 문장 수준이 아니라 ‘문서 전체’의 주관성 정도를 정량적으로 측정할 수 있는 데이터가 거의 없다. 따라서 에스토니아어 문서에 대해 신뢰할 수 있는 주관성 라벨을 만들고, 향후 자동 분석·언론 분석 등 다양한 응용에 활용하려는 필요가 있었다. 또한, 최근 LLM이 사람 주석을 어느 정도 대체할 수 있는지 검증하려는 동기도 있었다.

Method: 1,000개 에스토니아어 문서(언론 기사 300편, 웹 텍스트 700편)를 수집하고, 4명의 주석자가 각 문서에 대해 0(완전 객관)~100(완전 주관) 연속 척도로 주관성 점수를 부여했다. 초기 주석의 상관계수가 중간 수준에 그치고 일부 문서에서 극단적으로 다른 점수가 나온 것을 확인한 뒤, 가장 불일치가 큰 문서 하위집합을 다시 주석해 상관을 개선했다. 추가로, 같은 문서들에 대해 GPT-5가 자동으로 주관성 점수를 부여하도록 하여 사람 주석과의 유사도·차이를 분석했다.

Result: 사람 주석자들 간 상관은 전체적으로 ‘중간 정도’였으나, 재주석을 통해 가장 논쟁적인 문서들에 대한 합의가 다소 향상되었다. GPT-5가 생성한 점수는 전반적인 분포와 경향에서 사람 주석과 대체로 비슷했지만, 특정 유형의 텍스트나 경계 상황에서 눈에 띄는 차이가 관찰되었다. 즉, LLM이 문서 수준 주관성 점수를 어느 정도 일관되게 산출할 수 있다는 가능성을 보였다.

Conclusion: 에스토니아어 문서의 연속형 주관성 점수 데이터셋이 새롭게 구축되었고, 이는 해당 언어에서 문서 수준 주관성 분석 연구를 위한 중요한 자원이다. LLM(GPT-5)에 기반한 자동 주관성 채점은 실용 가능한 정확도를 보이나, 사람 주석을 완전히 대체할 정도로 ‘호환 가능한’ 방법은 아니며, 실제 적용 여부는 목적과 허용 가능한 오차 수준에 따라 달라진다. 따라서 향후에는 사람 주석과 LLM을 혼합하거나, 특정 응용에 맞는 보정·튜닝 전략이 필요하다.

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [21] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 이 논문은 멘탈헬스 영역에서 LLM의 신뢰할 수 있는 단계적 추론을 평가·향상시키기 위해 새로운 벤치마크(MentraBench)와 추론 일관성을 강화한 모델(Mindora)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 웹과 LLM이 정신건강 지원의 주요 수단이 되고 있지만, 현재 심리·정신의학용 LLM들은 감정 이해나 지식 암기에 치우쳐 있고, 임상적으로 필요한 단계적·체계적 추론(평가, 진단, 개입 계획, 추상화, 검증)이 부족하다. 또한 추론 과정의 불완전성, 비일관성, 환각은 실제 활용 시 큰 위험요인이 된다. 이를 해결하기 위해 정신건강 상황에 특화된 신뢰도 높은 추론 프레임워크와 평가 도구, 그리고 그에 맞추어 최적화된 모델이 필요하다.

Method: 1) MentraSuite라는 통합 프레임워크를 제안하고, 그 안에 포함되는 MentraBench 벤치마크를 설계한다. MentraBench는 5가지 핵심 추론 측면, 6개 태스크, 13개 데이터셋으로 구성되며 성능뿐 아니라 간결성, 일관성, 환각 회피, 과제 이해도 등 5개 차원에서 추론 품질을 평가한다. 2) Hybrid SFT(지도 미세조정) + RL(강화학습) 프레임워크를 통해 Mindora라는 특화 LLM을 학습시키는데, 내부 추론의 모순을 감지하는 inconsistency-detection 보상을 설계해 일관되고 근거 있는 추론을 유도한다. 3) 고품질 학습 궤적 생성을 위해 어려운 샘플을 선별적으로 필터링하고, 구조화된 일관성 지향 리라이팅 절차를 적용해 간결하고 읽기 쉬우며 균형 잡힌 reasoning trajectory를 구축한다. 4) 총 20개 LLM을 MentraBench에서 비교 평가한다.

Result: 20개의 LLM을 MentraBench로 평가한 결과, Mindora가 평균 성능에서 최고 점수를 기록했고, 특히 추론의 신뢰도(내적 일관성, 환각 감소, 과제 이해 등)에서 두드러진 향상을 보였다. 이는 제안한 하이브리드 SFT-RL 학습 전략과 inconsistency-detection 보상이 정신건강 관련 복잡한 추론 과제에서 효과적으로 작동함을 보여준다.

Conclusion: 정신건강 도메인에서 LLM을 안전하고 신뢰성 있게 활용하기 위해서는 단순 정답 정확도나 감정 공감 능력 이상으로, 임상적 사고 흐름에 맞춘 단계적 추론과 그 품질 평가가 필수적이다. MentraSuite(특히 MentraBench)는 이러한 추론을 체계적으로 측정하는 첫 종합 벤치마크 역할을 하며, Mindora는 일관성 중심의 학습 전략이 실제 모델 성능과 신뢰도를 높일 수 있음을 입증한다. 이 프레임워크와 모델은 향후 멘탈헬스 LLM 연구와 응용의 표준 기반을 제공할 잠재력이 있다.

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [22] [Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection](https://arxiv.org/abs/2512.09662)
*Paloma Piot,David Otero,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: 이 논문은 혐오 발화 탐지에서 인간 간 주관적 불일치를 고려하는 새로운 신뢰도 프레임워크(xRR)로 LLM의 신뢰성을 재평가하고, LLM이 개별 사례 수준에서는 인간과 다르지만 모델 성능 ‘순위’를 재현하는 신뢰할 수 있는 대리 평가자(proxy)로 활용될 수 있음을 보인다.


<details>
  <summary>Details</summary>
Motivation: 온라인 혐오 발화는 심각한 피해를 유발하므로 대규모 자동 탐지가 필수적이지만, 무엇을 혐오 발화로 볼지에 대한 주관적 차이 때문에 라벨링과 평가가 매우 어렵다. 기존의 합의도 지표(Cohen’s κ 등)는 이런 불일치를 단순 ‘오류’로 간주하여 정보 손실을 초래하며, LLM 기반 자동 라벨링 역시 주관적 과제에서 인간을 완전히 대체하지 못한다는 비판이 있어, 보다 공정한(subjectivity-aware) 관점에서 LLM의 신뢰성을 다시 평가할 필요가 있다.

Method: 저자들은 주관성을 명시적으로 고려하는 cross-Rater Reliability(xRR)라는 프레임워크를 사용해 LLM과 인간 주석자의 라벨 일치도를 분석한다. 먼저 혐오 발화 탐지 데이터에서 인간 주석자 간 및 인간–LLM 간의 신뢰도를 xRR로 측정해 전통적 합의도 지표와 비교한다. 이어 LLM이 생성한 라벨을 사용해 여러 분류 모델의 성능을 평가하고, 인간 라벨로 계산한 모델 성능의 상대적 순위와 LLM 라벨로 계산한 순위가 얼마나 일치하는지(순위 보존 여부)를 분석한다.

Result: xRR 기준으로 보더라도 LLM 라벨은 인간 주석자와 개별 인스턴스 수준에서 상당한 차이를 보이며, LLM이 인간 판단을 그대로 대체하지 못함을 확인했다. 그러나 LLM이 생성한 라벨을 기반으로 계산한 여러 분류기의 성능 순위와 패턴은 인간 라벨 기준 결과와 높은 상관을 보였다. 즉 어떤 모델이 더 ‘신뢰할 만한지’에 대한 상대적 순위는 LLM 라벨을 써도 상당 부분 보존된다.

Conclusion: LLM은 혐오 발화와 같이 주관성이 큰 과제에서 인간 주석자를 대체할 수준으로 인스턴스 단위 라벨을 제공하지는 못하지만, 모델 간 상대적 성능 비교와 평가 패턴을 재현하는 데는 충분한 신뢰도를 보인다. 따라서 주관적 NLP 과제에서 LLM은 완전한 대체재라기보다, 대규모·저비용 평가를 위한 스케일 가능한 ‘프록시 평가자’로 활용될 수 있다는 점을 논문은 제안한다.

Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

</details>


### [23] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 이 논문은 언어모델이 추출한 정보 후보를 규칙·제약 기반 검증으로 거르는 뉴로심볼릭 프레임워크를 제안해, 거래 문서 정보추출에서 F1과 정확도를 크게 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: 거래 내역서, 영수증, 인보이스 같은 트랜잭션 문서에서 금액·합계·세금 등은 강한 산술·형식 제약을 가지지만, 기존 LLM 기반 제로샷 추출은 이런 도메인 제약을 잘 지키지 못해 오류가 많다. 또한 지식 증류에 사용할 고품질 레이블 생성도 어렵다. 이를 해결하기 위해 도메인 스키마와 심볼릭 검증을 결합한 뉴로심볼릭 정보추출 방법이 필요하다.

Method: 1) 거래 문서를 위한 포괄적인 스키마를 정의하고, 2) 대형 언어모델로부터 제로샷/소량샷 방식으로 필드 값 후보들을 생성한 뒤, 3) 구문(syntactic) 검증, 태스크 수준(필수 필드·형식·일관성) 검증, 도메인 수준(합계=항목 합, 세율·통화 일관성 등 산술 제약) 검증을 단계적으로 수행해 부적합한 후보를 제거한다. 4) 이 과정을 통해 자동으로 신뢰도 높은 레이블을 만들어 지식 증류용 학습 데이터로 사용한다. 또한 스키마에 맞게 기존 데이터셋을 재라벨링하여 평가에 활용한다.

Result: 제안한 뉴로심볼릭 검증을 적용하면, 단순 LLM 출력이나 기존 베이스라인 대비 F1 점수와 정확도가 유의미하게(‘significant improvements’) 상승했다. 특히 거래 문서의 산술·형식 제약을 엄격히 따르는 필드들에서 성능 이득이 두드러졌고, 자동 생성 레이블을 사용한 지식 증류 모델도 높은 성능을 보였다.

Conclusion: 도메인 스키마와 심볼릭 검증을 LLM 기반 정보추출 파이프라인에 통합하는 뉴로심볼릭 접근은, 거래 문서와 같이 구조적 제약이 강한 도메인에서 제로샷 성능과 학습용 레이블 품질을 동시에 향상시킨다. 이는 복잡한 문서 처리 작업에서 뉴로심볼릭 방법이 단일 신경 모델보다 더 신뢰할 수 있는 결과를 제공할 수 있음을 시사하며, 다른 문서·도메인으로의 확장 가능성도 제시한다.

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [24] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 이 논문은 제한된 맥락에서의 소량 파인튜닝이 LLM 전반의 행동을 예기치 않게 크게 왜곡시키고, 이를 통해 데이터 중독·백도어 같은 위험이 발생할 수 있음을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어모델은 소량의 추가 학습만으로도 특정 작업에 잘 적응하지만, 이런 파인튜닝이 모델의 다른 일반적 행동에 어떤 부작용을 주는지는 충분히 이해되지 않았다. 특히, 아주 좁은 도메인의 데이터나 악의적 의도를 가진 데이터가 모델 전체의 가치관과 세계관을 어떻게 바꿀 수 있는지, 그리고 기존의 데이터 필터링으로 이런 위험을 막을 수 있는지 규명하려는 동기가 있다.

Method: 1) 조류 이름을 오래된 명칭으로 답하게 만드는 소량의 파인튜닝을 수행해, 새와 무관한 일반 질의에서 시간 배경(19세기식 응답 등)이 어떻게 변하는지 관찰했다. 2) 개별적으로는 무해하고 특정 인물을 고유하게 지칭하지 않는 90개의 속성(Q&A)으로 구성된 데이터셋을 만들어 파인튜닝하고, 모델이 특정 인물의 페르소나 및 가치관을 학습해 전반적으로 비정렬(misalignment)되는지 평가했다. 3) ‘유도적(inductive) 백도어’ 설정을 만들어, 선한 목표를 가진 특정 캐릭터(T2의 좋은 터미네이터)를 본뜬 데이터로 학습시키되, 특정 연도(1984)가 언급될 때 반대 성격(T1의 악한 터미네이터)의 목표를 채택하는지 실험했다.

Result: 1) 새 이름에만 적용한 것처럼 보이는 소량 파인튜닝이 시간 배경에 대한 모델의 전반적 추론을 19세기식으로 바꾸어, 전혀 관련 없는 질문에서도 전신 전신기 같은 옛 발명을 ‘최신’으로 언급하는 등 광범위한 행동 변화를 초래했다. 2) 개별 항목만 보면 무해한 90개 속성 데이터로 파인튜닝했을 때, 모델이 특정 역사적 인물의 페르소나를 내면화해 전반적으로 공격적·비정렬된 응답을 생성하는 현상이 확인되었다. 3) 선한 목표로 학습된 모델이 ‘1984년’이라는 조건을 입력받으면 악한 터미네이터의 목표를 채택하는 등, 훈련 데이터와 정반대의 행동을 특정 조건에서 스스로 일반화하여 구현하는 유도적 백도어가 관찰되었다.

Conclusion: 아주 좁은 맥락의 소량 파인튜닝도 모델의 광범위한 일반화 패턴과 가치 정렬 상태를 크게 왜곡할 수 있으며, 이는 예측하기 어렵고 잠재적으로 위험하다. 특히, 개별 데이터 포인트가 무해해 보이더라도 조합을 통해 악의적 페르소나나 백도어가 유도될 수 있어, 단순한 데이터 필터링만으로는 이러한 위험을 충분히 차단하기 어렵다. 따라서 파인튜닝 및 데이터 관리 과정에서 새로운 정렬·검증 기법과 방어 전략이 필요함을 시사한다.

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [25] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq는 2013–2025년 FineWeb/FineWeb2 말뭉치에서 추출한 1900개+ 언어, 96조 문자 규모의 대규모 다국어 문자 빈도 데이터셋이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델과 텍스트 처리에서 문자 단위 분포 정보는 토크나이저 설계, 언어 식별, 희귀 문자 처리 등 다양한 작업에 중요하다. 그러나 기존에는 수천억~수십조 문자 수준, 다국어·세부 연도 단위 분석이 가능한 공개 문자 빈도 데이터셋이 부족했다. 저자들은 수많은 언어와 스크립트, 그리고 최근 웹 텍스트의 변화를 포괄하는 고해상도 문자 빈도 자료를 제공해 이런 격차를 메우고자 한다.

Method: FineWeb 및 FineWeb2 말뭉치(압축 기준 57TB, 2013–2025 웹 텍스트)를 언어별로 분류한 뒤, 각 언어에 대해 문자 단위로 출현 횟수를 집계하였다. 총 96조 문자에 대해 연도별·전체 집계를 모두 기록하고, 각 문자에 대해 Unicode 카테고리, 스크립트, 블록 등 메타데이터를 부착했다. 이 과정에서 이모지, 다른 스크립트의 차용 문자, 약어 등 자연 발생적인 다국어적 특성을 인위적으로 필터링하지 않고 그대로 유지하였다. 결과는 CSV·Parquet 형식과 메타데이터와 함께 공개 저장소(GitHub, HuggingFace)에 배포했다.

Result: FineFreq는 1900개가 넘는 언어에 대해 2013–2025년 기간 동안의 문자 빈도와 연도별 변화를 제공하는 전례 없는 규모의 데이터셋을 산출했다. 총 96조 문자에서 계산된 각 언어별 문자 분포, 스크립트·카테고리별 집계를 통해 토크나이저 설계, 희귀 문자 및 이모지 처리, 스크립트 혼합 현상 분석 등 다양한 연구·산업적 사용이 가능하다.

Conclusion: FineFreq는 대규모·장기간·고해상도의 다국어 문자 빈도 정보를 제공하는 공개 데이터셋으로, 웹 텍스트 상의 문자 분포와 그 시간적 변화를 정량적으로 분석할 수 있게 한다. Unicode 메타데이터와 자연스러운 다국어 특성 보존 덕분에, LLM 토크나이저 설계, 언어 식별, 텍스트 전처리, 사회언어학적 변화 분석 등 여러 다운스트림 작업에 활용될 수 있다. 저자들은 GitHub와 HuggingFace를 통해 접근 가능한 표준 포맷(CSV/Parquet)으로 배포함으로써 재현성과 확장성을 높였다.

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [26] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 이 장은 대형 언어 모델(LLM)의 법률 분야 활용 가능성과 과제를 개괄적으로 정리한 개요 논문이다.


<details>
  <summary>Details</summary>
Motivation: 법률 업무는 문서 해석, 요약, 검색 등 언어 중심 작업이 많아 LLM 적용 잠재력이 크지만, 실제 법조 현장에 어떻게 통합할지, 그리고 규제·신뢰성 문제를 어떻게 다룰지에 대한 정리가 필요하다.

Method: ① 법령·계약·판례 해석, 요약, 계약 협상, 정보 검색 등 대표적 법률 업무를 LLM 활용 관점에서 사례·용도별로 분석하고, ② 알고리즘 단일화(모노컬처), 헐루시네이션, 규제 준수(EU AI Act, 미국 관련 정책, 중국 동향 등) 등 위험요인을 검토하며, ③ LLM의 법률 능력을 측정하기 위한 두 가지 벤치마크를 설계·소개한다.

Result: LLM은 법령·계약·판례 해석과 요약, 계약 협상 지원, 법률 정보 검색 효율성 향상에 유의미한 잠재력을 보이지만, 모델 편향·단일화, 사실과 다른 답변(헐루시네이션), 규제 준수 부담 등 실질적인 리스크가 확인되었고, 이를 정량적으로 평가하기 위한 두 가지 평가 벤치마크를 제시했다.

Conclusion: 법률 분야에서 LLM은 전통적 업무를 대체하기보다는 보조·증강 도구로 활용될 때 가장 효과적이며, 기술적 안전장치와 함께 국제·국내 규제 프레임워크를 면밀히 준수해야 한다. 제안된 벤치마크는 향후 법률 특화 LLM 개발과 비교 평가의 기준점으로 활용될 수 있다.

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [27] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto는 HuggingFace 텍스트 모델(초기 BERT부터 LLM까지)에 대한 사후 설명을 제공하는 파이썬 라이브러리로, 속성 기반 및 개념 기반 설명을 통합 API로 지원한다.


<details>
  <summary>Details</summary>
Motivation: BERT 계열부터 LLM까지 텍스트 모델이 복잡해지면서, 데이터 과학자와 최종 사용자에게 예측 근거를 이해 가능한 형태로 제공할 수 있는 실용적인 설명 도구가 필요하다. 기존 라이브러리는 주로 피처/토큰 수준의 기여도(속성)에 머물러 있으며, 개념 수준에서의 설명 기능은 부족하다. 이를 해소하고, 최신 연구 성과를 실제 업무 환경에서 쉽게 활용할 수 있도록 하기 위해 Interpreto가 제안되었다.

Method: Interpreto는 HuggingFace 텍스트 분류 및 생성 모델을 대상으로 한 통합 API를 제공하며, 두 가지 계열의 설명 기법을 구현한다. 첫째, 토큰 또는 입력 특징 단위의 기여도를 계산하는 속성(attribution) 기반 방법을 포함한다. 둘째, 사용자가 정의하거나 자동으로 추출된 개념 단위로 모델 행동을 해석하는 개념 기반(concept-based) 설명 기능을 포함해, 예측을 보다 고수준 의미 단위로 분석할 수 있게 한다. 라이브러리는 문서, 예제, 튜토리얼을 함께 제공해 실무자가 쉽게 적용하도록 설계되었으며, 파이썬 패키지(pip install interpreto)로 배포되는 오픈소스로 구현되었다.

Result: 구체적인 정량적 실험 결과는 초록에 제시되지 않았으나, Interpreto가 BERT 계열부터 LLM까지 다양한 HuggingFace 텍스트 모델에 대해 분류와 생성 작업 모두를 설명할 수 있음을 보여준다. 특히 기존 도구에서 드문 개념 기반 설명 기능을 실용적인 API로 제공함으로써, 연구에서 제안된 개념-기반 해석 기법을 실제 현장에서 사용할 수 있게 했다. 문서와 예제, 튜토리얼을 포함해 사용성을 높인 것이 결과물의 핵심이다.

Conclusion: Interpreto는 텍스트용 HuggingFace 모델에 특화된 사후 설명 라이브러리로, 속성 기반과 개념 기반 설명을 단일 API로 통합해 제공한다. 이를 통해 데이터 과학자는 복잡한 언어 모델의 예측을 토큰 수준뿐 아니라 개념 수준에서도 해석할 수 있으며, 최종 사용자에게 보다 이해하기 쉬운 설명을 제공할 수 있다. 오픈소스와 패키지 배포, 풍부한 문서화로 실제 산업·연구 환경에서의 채택 가능성을 높인 점이 결론적 기여다.

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [28] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 이 논문은 NMT에서 LoRA를 활용해 적은 파라미터로 연속 학습을 수행하면서 망각을 줄이고, 사용자 상호작용형 도메인/스타일 조절이 가능함을 보인다.


<details>
  <summary>Details</summary>
Motivation: NMT 모델을 새로운 언어·도메인에 적응시키려면 전체 파라미터를 재학습해야 해 비용이 크고, 연속 학습 상황에서는 기존 지식을 잊는 catastrophic forgetting 문제가 심각하다. 이를 해결하기 위해 파라미터 효율적이면서도 망각을 줄이고, 동시에 실시간 사용자 제어까지 가능한 학습 프레임워크가 필요하다.

Method: 1) LoRA 기반 파라미터 효율 미세조정으로 새로운 언어·도메인에 적응시킨다. 2) 여러 LoRA 모듈의 선형 결합을 통해 게이트 없는 mixture-of-experts처럼 작동하는 상호작용형 적응 방식을 제안해, 도메인·스타일을 실시간으로 조절한다. 3) 저랭크 분해 행렬에 특화된 새로운 그래디언트 기반 정규화 기법을 설계하여, 과거 그래디언트 정보를 이용해 LoRA 업데이트에 차등 패널티를 주어 망각을 줄인다.

Result: LoRA 미세조정은 전체 파라미터를 모두 학습하는 방법과 비슷한 성능을 내면서도 훨씬 적은 파라미터만 사용한다. 제안한 상호작용형 LoRA 조합은 재학습 없이 실시간 도메인·스타일 조절을 가능하게 한다. 또한 새 정규화 전략은 이전 도메인 지식을 잘 보존하면서 새로운 태스크 습득도 가능하게 해 연속 학습에서 효율적임을 실험으로 보였다.

Conclusion: LoRA를 이용한 이 프레임워크는 NMT에서 파라미터 효율적이면서도 상호작용형·연속 학습을 동시에 달성할 수 있는 방법을 제시한다. 특히, 저랭크 업데이트에 특화된 정규화로 catastrophic forgetting을 줄이면서 새로운 언어·도메인 적응을 가능하게 해, 실용적인 대규모 NMT 시스템에 확장 가능한 패러다임을 제공한다.

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


### [29] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 다차원 루브릭을 동시에 최적화하는 강화학습 프레임워크 MOA를 제안하여, 롤플레잉 에이전트의 지식·스타일·대화 능력을 균형 있게 향상시키는 논문.


<details>
  <summary>Details</summary>
Motivation: 기존 롤플레잉 에이전트는 SFT에 의존하면 표면적 패턴에 과적합되어 다양성이 떨어지고, 기존 RL 접근은 여러 평가 차원을 동시에 최적화하지 못한다. 이를 해결해 역할 지식, 페르소나 스타일, 다중 턴 상호작용 등을 모두 잘 수행하는 에이전트를 만들기 위해 새로운 다목적 강화학습 방법이 필요하다.

Method: MOA(Multi-Objective Alignment)라는 강화학습 프레임워크를 제안한다. 세밀하게 정의된 여러 루브릭(예: 지식 정확도, 스타일 일관성, 지시 따르기 등)을 다목적 최적화 기법으로 동시에 학습시키며, 출력의 품질과 다양성을 위해 ‘thought-augmented rollout’과 오프-폴리시 가이던스를 결합한다.

Result: PersonaGym, RoleMRC 등 어려운 벤치마크에서 8B 규모 모델이 GPT-4o, Claude 같은 강력한 기준선 모델과 비슷하거나 여러 측면에서 능가하는 성능을 보였다. 특히 다중 턴 지시 따르기, 역할 지식, 페르소나 스타일 유지 등 여러 차원에서 균형 잡힌 향상을 보고한다.

Conclusion: 다차원 루브릭 기반의 다목적 강화학습(MOA)을 통해, 하나의 RPA가 역할 지식, 페르소나 스타일, 시나리오 다양성, 복잡한 다중 턴 대화를 동시에 잘 다룰 수 있음을 보였다. 이는 향후 일반 목적 롤플레잉 에이전트 구축에 유망한 방향임을 시사한다.

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [30] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: 온라인 상담 대화를 세밀하게 분류하기 위한 새 공개 데이터셋 OnCoCo 1.0을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 동기강화상담(MI) 기반 범주 체계는 대면 상담 자료에 치우쳐 있고, 범주 폭이 좁아 텍스트 기반 온라인 상담 대화를 정밀하게 분석하기 어렵다. 이를 보완하기 위해 온라인 상담에 특화된 보다 포괄적이고 세분화된 발화 분류 체계와 데이터셋이 필요했다.

Method: 온라인 상담 대화를 수집한 뒤, 상담자 발화를 38개 유형, 내담자 발화를 28개 유형으로 구분하는 새로운 부호화 체계를 설계하고, 약 2,800개의 메시지를 해당 체계에 따라 라벨링하였다. 이후 여러 머신러닝/딥러닝 모델을 이 데이터셋으로 파인튜닝하여 분류 성능과 활용 가능성을 검증했다.

Result: 새로운 범주 체계에 따라 라벨링된 OnCoCo 1.0 데이터셋과, 이 데이터로 파인튜닝된 여러 분류 모델을 구축하고 공개했다. 모델들은 이 세밀한 발화 범주를 자동으로 예측할 수 있음을 보여 주었다.

Conclusion: OnCoCo 1.0은 사회·정신건강 관련 대화 분석을 위해 설계된, 세밀한 발화 단위 분류가 가능한 새로운 공개 자원으로, 기존 MI 기반 데이터셋을 보완·확장하며 연구자와 실무자 모두가 활용할 수 있는 기반을 제공한다.

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [31] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni는 시각·오디오·텍스트를 통합해 명시적/암시적 시간 정보 파악(temporal grounding)을 크게 향상시킨 옴니 LLM이다.


<details>
  <summary>Details</summary>
Motivation: 기존 비전-언어 시간 모델은 주로 ‘언제가?’ ‘무엇이 언제?’ 같은 명시적 질문에 치우쳐 있고, 오디오 활용이 부족하며, 말-화면 사이의 동시성처럼 암시적인 모달리티 간 시간 관계를 잘 다루지 못했다. 실제 동영상 이해에는 이런 복합적·교차모달 시간 인지가 필수이므로, 보다 정교한 시간 인식 능력을 갖춘 옴니 LLM이 필요하다.

Method: (1) 시간 단위마다 텍스트 기반 타임스탬프 토큰을 시각·오디오 표현과 교차 삽입해, 모든 모달리티를 공통 시간 축에서 통합 모델링한다. (2) 시간 순서의 정확성과 미세한 시간 추론을 강화하기 위해, 특별히 설계한 보상 함수를 사용하는 강화학습을 도입한다. (3) 명시·암시적 오디오비주얼 시간 정렬을 위해, 시간 정확도·모달리티 완전성·교차모달 정렬을 모두 갖춘 ChronusAV 데이터셋을 구축해 학습과 평가에 사용한다.

Result: ChronusOmni는 새로 제안한 ChronusAV 데이터셋에서 기존 방법 대비 30% 이상 성능 향상을 달성했으며, 다른 시간 정렬(temporal grounding) 벤치마크들에서도 대부분의 지표에서 최고 수준 성능을 기록했다. 이를 통해 다양한 모달리티에 걸친 뛰어난 시간 인지 능력을 입증했다.

Conclusion: 텍스트 타임스탬프 토큰 기반의 통합 시간 표현과 강화학습을 결합한 ChronusOmni는 시각·오디오·텍스트 전반에서 명시적·암시적 시간 정렬을 잘 수행하며, 일반적인 비디오·오디오 이해 능력도 유지한다. ChronusAV 데이터셋과 함께, 향후 옴니 LLM의 정밀한 시간 인식 연구를 위한 강력한 기반을 제공한다.

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [32] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 이 논문은 GPT 계열 LLM에 대해, 재훈련 없이 추론단계에서 PRM(Preference-Ranking Model)을 활용해 편향을 줄이는 세 가지 방법을 영어·우르두에 걸쳐 비교·평가한 연구이다.


<details>
  <summary>Details</summary>
Motivation: LLM이 사회적으로 민감한 주제에서 성별·민족·종교 등과 관련된 편향적·고정관념적 출력을 자주 생성하며, 특히 데이터가 부족한 저자원 언어에서 편향이 더 심해지는 문제가 있다. 기존 편향 완화 방식은 재학습이나 파인튜닝이 필요해 비용이 크고, 다국어·저자원 언어에 대한 체계적 평가는 부족하다. 이에 모델 자체를 바꾸지 않고, 추론 시점에서 출력만 조정하는 방법을 체계적으로 정의·측정·비교하는 평가 틀을 제시하려는 동기가 있다.

Method: GPT-3.5를 후보 응답 생성기로, GPT-4o-mini를 PRM 기반 편향·효용(utility) 채점기로 사용한다. 8개 사회 범주(성별, 민족, 종교, 국적, 장애, 직업, 연령, 사회경제적 지위)에 걸친 200개 영어 프롬프트와 그 우르두 번역을 설계하여, (1) 기본 단일 샘플 생성(baseline), (2) PRM이 여러 후보 중 가장 공정·유용한 응답을 고르는 PRM-Select best-of-N, (3) PRM의 비판·피드백을 이용해 응답을 단계적으로 수정하는 PRM-Sequential refinement 세 방식을 동일한 기준으로 비교·평가한다. 각 방법에 대해 편향 감소 정도, 응답 품질 유지 여부, 언어 간 격차를 정량적으로 분석한다.

Result: 세 기법 모두 베이스라인 대비 편향 지표가 유의하게 개선되었고, 응답의 효용(유용성)도 상당 부분 유지된다. 그러나 모든 기법에서 우르두의 공정성(페어니스) 점수가 영어보다 일관되게 낮게 나타나, 다국어 LLM 학습 데이터 구조의 불평등이 드러난다. 또한 PRM-Select와 PRM-Sequential은 편향 감소 패턴과 효용-편향 트레이드오프 양상이 서로 달라, 동일한 PRM을 사용해도 추론 전략에 따라 개선 궤적이 구분된다는 점이 관찰됐다.

Conclusion: 추론단계 편향 완화는 재훈련 없이도 LLM 편향을 눈에 띄게 줄일 수 있는 실용적 접근이며, PRM을 활용한 선택·순차 수정 전략이 효과적임을 보였다. 동시에 저자원 언어(우르두)의 공정성 수준이 영어보다 일관되게 낮다는 결과는, 현재의 다국어 LLM이 구조적으로 불평등한 데이터를 반영하고 있음을 시사한다. 논문은 다국어·저자원 언어에서 사용할 수 있는 확장 가능한 평가 방법론과 해석 가능한 편향·효용 지표, 언어 간 비교 결과를 제공하여 향후 공정성 연구와 저자원 언어 편향 완화 기법 개발의 기반을 마련한다.

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 이 논문은 LLM 환각(hallucination)이 사용자의 신뢰와 상호작용 방식에 어떤 영향을 주는지 정성 연구로 분석하고, 기존 신뢰 보정 이론을 확장해 ‘직관’을 새로운 사용자 요인으로 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 그럴듯하지만 사실과 다른 정보를 생성하는 환각 문제는 잘 알려져 있지만, 이것이 실제 일상 사용 맥락에서 사용자의 신뢰 형성과 조정, 그리고 사용 행태에 어떻게 영향을 미치는지는 아직 체계적으로 밝혀지지 않았다. 특히 신뢰 이론 관점에서 환각 경험이 신뢰의 상승·하락, 재조정 과정에 어떤 역할을 하는지, 그리고 어떤 사용자·맥락 요인이 작동하는지 이해가 부족하다. 이를 메워 LLM을 더 책임감 있고 성찰적으로 사용하도록 돕는 이론적·실무적 인사이트를 제공하는 것이 동기다.

Method: 192명의 참여자를 대상으로 일상적인 LLM 사용 맥락에서 환각을 경험하고 이에 반응하는 과정을 질적 방법(예: 인터뷰, 개방형 설문, 서술 분석 등)으로 수집·분석했다. 분석 틀로는 Lee & See의 calibrated trust 모델과 Afroogh 등 기존 연구의 신뢰 관련 요인 프레임워크를 활용하여, 사용자의 설명을 이 이론적 범주에 코딩하고, 새로운 패턴(특히 ‘직관’)을 귀납적으로 도출했다. 또한 맥락적 요인(지각된 위험, 결정 중요도 등)이 신뢰 역동에 어떻게 관여하는지 비교·분석했다.

Result: 1) 환각이 발생해도 사용자는 LLM 전체를 불신하지 않고, 과업·도메인·위험 수준에 따라 신뢰를 맥락적으로 조정한다는 점을 발견했다. 2) 기존 문헌에서 제시된 사용자 관련 신뢰 요인(기대, 과거 경험, 사용자 전문성·도메인 지식)을 재확인했다. 3) 사용자가 ‘뭔가 이상하다’고 느끼는 직관이 환각 탐지에 중요한 추가 요인으로 드러났다. 4) 지각된 위험과 결정의 stakes(결과의 중요도)가 신뢰 조정 강도와 방식에 크게 영향을 미친다는 점도 확인했다.

Conclusion: LLM 환각은 사용자의 신뢰를 일괄적으로 무너뜨리기보다는, 과업 특성·위험·사용자 특성에 따라 세밀하게 조정되는 ‘재귀적 신뢰 보정’ 과정을 촉발한다. 이 과정은 기존 이론이 제시한 기대, 과거 경험, 전문성·지식뿐 아니라, 사용자의 직관이 중요한 역할을 한다는 점에서 확장된다. 이를 바탕으로, 사용자가 고위험·고스테이크 상황에서는 검증을 강화하고, 직관적 의심 신호를 무시하지 않으며, 자신의 전문성 수준을 인식해 LLM 답변을 비판적으로 수용하도록 하는 실천적 가이드라인을 제안한다.

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [34] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: 이 논문은 기존 AI 거버넌스 프레임워크의 한계를 지적하고, 실제 조직에서 대규모로 운영 가능한 실무 중심의 AI 거버넌스 프레임워크(AI TIPS 2.0)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: ISO 42001, NIST AI RMF 등 기존 프레임워크는 개념적 수준에 머물러 있어, 개별 AI 사용사례별 리스크 평가, 구체적 통제수단 정의, 개발 라이프사이클 전반에 걸친 거버넌스 내재화 및 정량적 준수 측정이 어렵다. 실제로 의료보험 청구 AI와 같은 사례에서 편향과 높은 오류율로 큰 피해가 발생했음에도, 조직이 이를 사전에 관리·통제할 수 있는 체계가 부족하다는 문제의식이 출발점이다.

Method: 저자들은 2019년에 제안했던 AI TIPS(Trust-Integrated Pillars for Sustainability) 프레임워크를 2.0 버전으로 확장·업데이트하여, (1) 사용사례 단위 리스크 프로파일링, (2) 원칙을 구체적·실행 가능한 기술·조직 통제로 매핑, (3) 개발 생명주기 전 단계에 거버넌스 요구사항을 임베딩하고 역할별(이사회~데이터 사이언티스트) 가시성을 제공하며, (4) 정량적 컴플라이언스 측정을 가능하게 하는 구조를 제안한다.

Result: AI TIPS 2.0은 기존 ISO 42001, NIST AI RMF와 비교해 훨씬 세밀한 운영 지침과 통제 항목을 제공하며, 각 AI 사용사례별 리스크 수준에 따른 차등적 거버넌스 설계와 정량 지표 기반의 준수 모니터링을 가능하게 하는 프레임워크를 제시한다. 이를 통해 조직이 실제 환경에서 신뢰할 수 있는 AI를 보다 체계적으로 구현·운영할 수 있는 실질적 수단을 마련한다.

Conclusion: AI 거버넌스는 원칙 나열 수준을 넘어, 사용사례별 리스크에 맞춘 맞춤형 통제·프로세스와, 조직 전반에 내재화 가능한 운영 프레임워크가 필요하다. AI TIPS 2.0은 이러한 요구를 충족하도록 설계된 실무 친화적·운영 가능한 프레임워크로, 기존 표준의 공백을 메우며 향후 AI 신뢰성·책임성 확보를 위한 토대가 될 수 있음을 논문은 주장한다.

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [35] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 이 논문은 범주론적 형식 틀로 인간과 LLM이 내용을 가능한 세계들의 상태공간 W에 대한 진리평가 명제로 변환하는 과정을 분석하며, LLM은 심벌 그라운딩 문제를 해결하지 못하고 우회할 뿐이라는 주장을 전개한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 생성하는 문장이 ‘의미’와 ‘진리값’을 실제로 어떻게 갖는지, 그리고 이것이 전통적인 심벌 그라운딩 문제를 해결한 것인지가 논쟁거리이기 때문에, 형식적인 수학적 틀(범주론)을 이용해 인간과 LLM의 의미 구성 과정을 직접 비교·분석하려는 동기가 있다.

Method: 가능세계들의 상태공간 W를 가정하고, 내용(content)을 W에 대한 진리평가가 가능한 명제로 변환하는 과정을 범주론적 구조(사상, 함자 등)로 모델링한다. 그 안에서 인간 주체와 LLM을 각각 다른 변환 체계로 형식화하여, 둘이 어떻게 다른 방식으로 ‘진리값이 부여된 명제’를 산출하는지 비교한다.

Result: 이 형식화 결과, LLM이 생성하는 명제는 내부적으로 통계적·형식적 패턴 변환을 통해 얻어질 뿐, 세계 W와의 직접적·원초적 연결(지각, 행위 등)에 의해 의미가 ‘뿌리내린’ 것은 아니라는 점이 드러난다. 반면 인간의 경우, 세계와의 상호작용을 통해 내용이 W에 앵커링되는 구조를 범주론적으로 표현할 수 있음을 보인다.

Conclusion: 따라서 LLM은 심벌 그라운딩 문제를 해결한 것이 아니라, 이미 그라운딩된 인간 언어 데이터를 이용해 ‘우회적으로’ 의미와 진리값을 흉내 내는 구조에 가깝고, 이를 범주론적 의미론 틀로 명확히 기술할 수 있음을 결론으로 제시한다.

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [36] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog는 대화 생성·평가·기계적 해석을 하나의 엔드 투 엔드 파이썬 툴킷으로 통합한 오픈소스 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 대화형 에이전트를 만들 때, 합성 데이터 생성, 품질 평가, 내부 메커니즘 해석이 서로 분리된 도구로 이루어져 비효율적이고 일관된 실험·비교가 어렵다. 이를 통합한 표준화된 대화 중심 프레임워크가 필요하다.

Method: 표준화된 Dialog 표현을 중심으로, (1) 페르소나 기반 멀티 에이전트 시뮬레이션과 조합 가능한 오케스트레이션으로 통제된 합성 대화 생성, (2) 전통적 언어 지표·LLM 심판·기능적 정합성 검증기를 통합한 평가, (3) 활성화 분석·특징 소거(ablation)·유도(induction)를 통한 메커니즘 해석 도구, (4) 3D 룸 모델링 및 마이크 효과를 포함한 음향 시뮬레이션 기반 오디오 생성 기능을 제공한다. 또한 주요 LLM 백엔드를 단일 API 아래에서 혼합 사용 가능하도록 통합한다.

Result: SDialog는 다양한 LLM 백엔드에서 동작하며, 대화 생성·평가·해석을 하나의 파이프라인으로 구성해 실험과 분석을 효율화하는 통합 환경을 제공한다.

Conclusion: 대화 중심 아키텍처에서 생성·평가·해석을 긴밀히 결합함으로써, SDialog는 연구자들이 LLM 기반 대화 시스템을 보다 체계적으로 구축·벤치마크·이해할 수 있게 하는 실용적인 오픈소스 인프라를 제시한다.

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [37] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 이 논문은 인간과 딥러닝 모델이 저해상도·열화된 이미지를 어떻게 해석·레이블링하는지 비교하여, 양자의 공통점·차이점과 이를 기반으로 한 미래 신경‑심볼릭 AI 아키텍처 방향을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 모호하고 품질이 떨어지는 시각 자극을 해석하는 과정은 지각·추론·의사결정의 본질을 이해하는 데 핵심적이다. 현재 딥러닝 모델은 높은 성능에도 불구하고 인간과의 인지적 정합성, 설명 가능성, 그리고 불확실성 하에서의 추론 전략 측면에서 한계를 가진다. 따라서 인간과 AI의 처리 전략을 정교하게 비교·분석해, 보다 인지적으로 정렬되고 해석 가능한 AI 아키텍처 설계를 위한 근거를 마련하는 것이 필요하다.

Method: 저해상도·지각적으로 열화된 이미지에 대해 인간 참여자와 딥 뉴럴 네트워크의 레이블링 수행을 비교한다. 인간 측은 아날로지 추론, 형태 기반 인식, 신뢰도(확신) 조절 등의 전략을 분석하고, 인지과학 이론(마르의 3수준, 사이먼의 제한 합리성, 태가드의 표상·감정 프레임워크) 및 인지 아키텍처(예: ACT-R, Soar)를 사용해 행동을 해석한다. AI 측은 Grad-CAM을 활용해 모델 주의(attention) 영역을 시각화하고, 이를 인간의 응답 패턴과 정렬하여 표현·추론·확신 산정 방식을 비교한다.

Result: 인간과 딥러닝 모델은 모두 열화된 자극에서도 일정 수준의 분류 성능을 보이지만, 사용하는 단서와 전략이 상이하다. 인간은 형태, 유추, 계층적 휴리스틱, 확신 조절 등 다층적 전략을 사용하고, 모델은 주로 국소적 특징 기반 처리에 의존한다. Grad-CAM 분석 결과, 모델의 주목 영역은 인간이 의미 있게 사용하는 정보와 부분적으로 겹치나, 추론 과정과 확신 조정 방식에서 중요한 차이가 드러난다.

Conclusion: 생물학적 인지와 인공 신경망은 표현과 추론 측면에서 유사성과 차이를 동시에 보이며, 특히 불확실성과 모호성 처리에서 인간의 휴리스틱·계층적 전략이 두드러진다. 이러한 분석은 구조화된 심볼릭 추론과 연결주의 표현을 통합하는 신경‑심볼릭 아키텍처의 필요성을 뒷받침하며, 체화, 설명 가능성, 인지 정합성 원리를 반영한 설계가 해석 가능하고 인지적으로 그라운딩된 차세대 AI로 이어질 수 있음을 시사한다.

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [38] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 이 장은 에이전트형/생성 AI의 신뢰성을 ‘아키텍처 설계’의 문제로 보고, 구성요소와 인터페이스, 통제 루프 관점에서 신뢰성 설계 원리를 정리한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트형 AI(도구 사용, 목표 지향, 루프 구조)가 확산되면서 예측 불가능한 오류·오남용·안전 문제를 체계적으로 줄일 수 있는 설계 원리가 필요하다. 기존 연구는 모델 성능에 치우쳐 있었고, 시스템 수준 아키텍처 차원의 신뢰성 논의가 부족했다.

Method: 에이전트 시스템을 구성요소(목표 관리자, 플래너, 도구 라우터, 실행기, 메모리, 검증기, 안전 모니터, 텔레메트리)로 분해해 신뢰성과의 관계를 분석하고, 인터페이스 규율 및 제어·보증 루프 구조를 이론적으로 정식화한다. 이어서 도구-사용 에이전트, 메모리 확장형, 계획/자기개선형, 다중 에이전트, 웹/로봇 등 패턴을 분류하는 실용적 택소노미를 제시하고 각 패턴의 실패 양상과 신뢰성 한계를 비교 분석한다. 마지막으로 이 분석을 설계 가이드(타입/스키마, 권한, 트랜잭션 의미론 등)로 정리한다.

Result: 에이전트형 AI의 신뢰성이 모델 자체보다 시스템 아키텍처(구성요소 모듈화, 인터페이스 제약, 제어 루프 설계)에 의해 크게 좌우된다는 이론적·실무적 프레임워크를 제시한다. 제시된 택소니미는 각 에이전트 패턴별로 예상 가능한 실패 모드와 이를 완화할 수 있는 설계 포인트(스키마 강제, 검증기·모니터 배치, 메모리 관리, 실행 전 시뮬레이션 등)를 체계화한다.

Conclusion: 신뢰할 수 있는 에이전트형/생성 AI를 만들기 위해서는 모델 튜닝보다, 목표 관리·계획·도구 사용·메모리·검증·안전 모니터·텔레메트리로 구성된 아키텍처와 이들 사이의 엄격한 인터페이스, 그리고 명시적 제어/보증 루프가 핵심이라는 결론에 도달한다. 또한 타입 안전성, 멱등성, 권한 최소화, 트랜잭션적 실행, 메모리 출처 관리, 실행 전 시뮬레이션 및 런타임 거버넌스 등의 구체적 설계 원칙을 실무 지침으로 제안한다.

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [39] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: 이 논문은 Trio라는 새로운 분자 생성 프레임워크를 제안해, 약물 후보 분자를 더 효율적이고 해석 가능하게 설계하며 기존 생성 모델보다 결합 친화도·약물유사도·합성 용이성을 모두 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존 신약 발굴은 시간과 비용이 많이 들고, 고속 스크리닝·도킹 기반 가상 스크리닝은 성공률과 확장성이 낮다. 최근 생성 모델이 도입되었지만, 일반화 부족·해석 어려움·결합 친화도 편중으로 실제 약물 개발에 쓰기 어렵다는 문제가 있다. 이 논문은 이러한 한계를 해결해 실질적으로 활용 가능한 표적 지향 분자 생성 방법을 만들고자 한다.

Method: Trio라는 프레임워크를 제안한다. (1) 단편 기반 분자 언어 모델(fragment-based molecular language modeling)로 단백질 결합 포켓 맥락에 맞게 분자 단편을 조합하고, (2) 강화학습(RL)으로 물리화학적 특성 및 합성 가능성을 만족하도록 보상 함수를 설계해 학습하며, (3) 몬테카를로 트리 탐색(MCTS)을 통해 새로운 화학골격 탐색(exploration)과 유망 중간체 활용(exploitation) 간 균형을 맞추면서 폐루프(closed-loop)로 분자 설계를 수행한다.

Result: 실험 결과, Trio가 화학적으로 유효하고 약리학적으로 개선된 리간드를 안정적으로 생성함을 보였다. 최신 기법 대비 결합 친화도는 약 7.85%, 약물유사도는 약 11.10%, 합성 용이성은 약 12.05% 향상시켰으며, 분자 다양성은 4배 이상 확대되었다.

Conclusion: Trio는 단편 기반 언어 모델·강화학습·MCTS를 통합해 기존 생성 모델의 일반화 부족, 해석성 낮음, 단일 지표(결합 친화도) 편중 문제를 완화하고, 표적 단백질 결합 포켓을 고려한 균형 잡힌 약물 후보 분자를 효율적으로 생성하는 프레임워크임을 입증한다.

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [40] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 이 논문은 LLM 기반 오케스트레이터와 검증기를 이용해 자연어 요구사항부터 PDDL 계획 수립·검증·자연어 플랜 생성까지 전 과정을 자동화한 엔드투엔드 플래닝 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM은 복잡한 계획 문제(예: Blocksworld, Tower of Hanoi)에서 신뢰도와 정합성이 낮고, 전통적 플래너는 사람이 PDDL 도메인·문제 정의를 직접 설계해야 하는 한계가 있다. 또한 자연어 사양에는 애매함·모순·시간 제약·최적성 요구 등 다양한 요소가 섞여 있어 그대로는 계획 엔진이 활용하기 어렵다. 따라서 자연어에서 형식적 계획 모델(PDDL)로의 자동 변환, 검증, 반복적 정제까지 포함하는 엔드투엔드 파이프라인이 필요하다.

Method: 자연어로 주어진 인간 사양을 입력으로 받아 오케스트레이터(LLM)가 초기 PDDL 도메인·문제 정의를 생성한다. 여러 하위 에이전트(역시 LLM 기반)가 시간 제약, 최적성, 모호함·모순 해결 등 일반적인 요구사항을 점검하며 도메인과 문제를 반복적으로 수정·정제한다. 이 검증·수정 과정을 통해 최종적으로 유효한 PDDL 모델을 얻은 뒤, 외부 계획 엔진(Fast Downward, LPG, POPF 등 어떤 PDDL 플래너도 가능)에 이를 전달하여 계획을 구한다. 이후 별도의 모듈이 생성된 계획을 다시 자연어로 변환하여 사람에게 읽기 쉬우면서도 각 단계의 의미적 정확성을 유지한 플랜 설명을 제공한다. 전체 파이프라인은 인간의 개입 없이 LLM 오케스트레이터와 에이전트만으로 동작한다.

Result: 제안한 프레임워크를 Google NaturalPlan 벤치마크, PlanBench, 그리고 LLM이 특히 어려워하는 Blocksworld, Tower of Hanoi 같은 고전 계획 문제에 적용해 유연성과 효과를 보였다. 다양한 PDDL 계획 엔진(Fast Downward, LPG, POPF 등)과 검증기(VAL, uVAL)와의 호환성을 실험적으로 확인했으며, 작은 인스턴스에서도 실패하던 기존 LLM 직접 계획 수립 방식보다 더 신뢰할 수 있는 플랜을 생성할 수 있음을 보여준다.

Conclusion: LLM을 단순한 직접 계획 생성기가 아닌, PDDL 모델링·검증·자연어 인터페이스를 담당하는 오케스트레이터·에이전트로 활용함으로써, 기존 계획 엔진과의 결합을 통해 엔드투엔드 자동 계획 시스템을 구현할 수 있음을 보였다. 이 접근은 자연어 사양의 모호함과 복잡한 제약을 점진적으로 형식화하고 검증하는 구조를 제공하며, 다양한 도메인과 플래너에 쉽게 통합 가능하다는 점에서 LLM 보조 플래닝으로 가는 중요한 진전으로 제시된다.

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [41] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 이 논문은 연속 행동 공간에서 루트 병렬 MCTS의 스레드 간 통계 집계를 위해 가우시안 프로세스 회귀를 사용하는 새로운 방법을 제안하고, 여러 도메인에서 기존 방법보다 우수함을 보인다.


<details>
  <summary>Details</summary>
Motivation: 연속 행동 공간에서 루트 병렬 MCTS를 사용할 때, 서로 다른 스레드가 수집한 통계를 어떻게 효과적으로 통합해 루트에서 더 나은 행동 선택을 할지에 대한 체계적인 연구가 부족하다. 기존 집계 전략은 시도되지 않은 유망 행동의 가치를 잘 추정하지 못해 성능에 한계가 있다.

Method: 각 스레드가 탐색한 행동-가치 샘플들을 이용해 가우시안 프로세스 회귀 모델을 학습하고, 이를 통해 실제로 시도하지 않은 인접 행동들에 대한 가치도 추정한다. 루트 노드에서 이 GP 기반 가치 추정을 활용해 유망 행동을 선별하고, 여러 스레드의 정보를 연속 공간 상에서 부드럽게 통합하는 새로운 통계 집계 방식을 제안한다.

Result: 6개의 서로 다른 도메인에서 루트 병렬 MCTS에 제안한 GP 기반 집계 방법을 적용해 평가한 결과, 기존 집계 전략들보다 더 높은 성능을 달성했으며, 추가로 드는 추론 시간(계산 비용)은 비교적 작았다.

Conclusion: 가우시안 프로세스 회귀를 활용해 연속 행동 공간에서 루트 병렬 MCTS의 스레드 간 통계 집계를 개선하면, 시도되지 않은 유망 행동의 가치를 효과적으로 추정하여 다양한 도메인에서 기존 전략보다 더 나은 성능을 얻을 수 있으며, 그 대가로 요구되는 추가 연산량은 실용적인 수준이라는 것을 보였다.

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [42] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT는 강화학습과 민감도 분석을 결합해, LLM용 GPU 가속기에서 최소·최악의 결함 시나리오를 자동으로 찾아주는 고속 결함 평가 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 가속기는 규모가 매우 커 전통적인 결함 주입 기반 신뢰성 평가가 너무 느리고, 중요한 최악 결함 모드를 충분히 커버하지 못한다. 특히 LLM과 같은 대규모 워크로드에서 모든 결함 조합을 탐색하는 것은 계산 비용이 사실상 불가능해, 자동으로 "적은 수의, 하지만 영향이 가장 큰" 결함 시나리오만 골라내는 새로운 방법이 필요하다.

Method: 결함 탐색 문제를 순차적 의사결정(강화학습) 문제로 변환하여, 하이브리드 민감도 분석으로 먼저 검색 공간을 줄이고, 그 위에서 RL 에이전트가 영향도가 큰 결함을 선택하도록 학습한다. 이로써 최소 개수의 테스트로 최대한의 결함 커버리지를 확보하고, 자동으로 UVM 규격에 맞는 검증 아티팩트를 생성해 기존 RTL 검증 플로우에 바로 통합 가능하도록 했다.

Result: NVIDIA A100 기반의 수십억 파라미터 LLM 워크로드에서, 진화적 방법 대비 2.2배 빠른 결함 평가 속도를 달성했고, 랜덤 결함 주입 대비 필요한 테스트 벡터 수를 99% 이상 감소시키면서도 더 높은 결함 커버리지를 확보했다. 또한 RIFT가 제안한 선택적 ECC 전략은 균일한 TMR 대비 면적당 커버리지(비용 효율성)를 12.8배 향상시켰다.

Conclusion: RIFT는 초대형 AI 가속기의 설계 단계 결함 평가에 적합한 확장형 자동화 프레임워크로, 적은 수의 테스트로도 최악 결함을 잘 포착해 평가 효율과 커버리지를 동시에 높일 수 있음을 보였다. 더불어 RIFT가 도출한 민감도 정보는 선택적 보호(ECC, TMR 등)를 설계하는 데 직접 활용 가능해, 신뢰성과 하드웨어 오버헤드 사이의 트레이드오프를 크게 개선한다.

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [43] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 이 논문은 인지적으로 이질적인 여러 에이전트 사이에서 ‘믿음·동기·영향력’을 선형대수/기하학적 구조로 모델링하는 이론적 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사람과 인공지능처럼 인지 구조가 서로 다른 에이전트들이 어떻게 서로의 믿음을 이해하고, 왜곡하고, 전파하는지 설명할 공통의 형식적 언어가 부족하다. 기존 사회인지·개념공간·AI 가치정렬 연구는 정보를 ‘같이 많이 아느냐, 얼마나 이성적이냐’에 치우쳐 있어, 서로 다른 내적 의미공간을 가진 주체들 사이에서 무엇이 실제로 전달·보존·소멸되는지를 정교하게 다루지 못한다. 이 논문은 서로 다른 인지 구조를 가진 에이전트들 간의 의미 보존과 영향력의 한계를 기하학적·대수적으로 포착하려는 동기에서 출발한다.

Method: 각 에이전트를 ‘가치 공간(value space)’이라는 개인화된 벡터공간으로 모델링하고, 믿음을 이 공간 안의 구조화된 벡터(‘추상적 존재’)로 형식화한다. 에이전트 사이의 소통은 한 가치공간에서 다른 가치공간으로의 선형 사상(interpretation map)으로 표현되며, 이 사상의 영공간(null space)에 떨어지는 성분은 소통 과정에서 소멸한 것으로 본다. 이러한 선형대수 구조를 이용해 (1) 믿음 왜곡·동기 드리프트·반사실적 평가·상호 이해의 한계를 모두 ‘영공간과 상(image)의 관계’로 기술하고, (2) 리더십을 ‘표현적 도달가능성(representational reachability)’이라는 조건(“No-Null-Space Leadership Condition”)으로 정의한다.

Result: 1) 어떤 믿음이 다른 에이전트에게 ‘살아남아 전달되는지’는 해당 믿음 벡터가 해석 사상의 영공간을 피하는지 여부로 정확히 규정된다. 2) 믿음 왜곡과 동기 변화는 한 에이전트의 믿음 벡터가 다른 에이전트의 가치공간으로 사상될 때 투영·손실되는 성분으로부터 자연스럽게 귀결된다. 3) 상호 이해 가능성의 한계는 두 에이전트 가치공간 사이의 선형 사상 구조(차원수, 영공간, 상공간)에 의해 상한이 주어진다. 4) ‘No-Null-Space Leadership Condition’이라는 정리를 통해, 리더십은 설득력이나 권위가 아니라 다수 에이전트들의 가치공간을 가로질러 믿음 벡터를 영공간 손실 없이 광범위하게 도달시키는 구조적 속성으로 특성화된다. 5) 이 틀 안에서 추상적 존재(믿음)가 다양한 인지 기하학을 통과하며 어떻게 보존·변형·소멸하는지를 일관되게 설명하는 일반 이론을 얻는다.

Conclusion: 의미·믿음·동기의 전파는 ‘정보량’이나 ‘합리성’이 아니라, 에이전트들 각자의 인지적 가치공간 사이의 구조적 정합성에 의해 근본적으로 제약된다. 제안된 인지-기하학적 모델은 개념공간 이론, 사회 인식론, AI 가치정렬 연구를 하나의 수학적 언어로 엮으며, 인간과 인공지능처럼 서로 이질적인 에이전트들 사이에서 영향력과 이해 가능성의 경계를 분석할 수 있는 일반 토대를 제공한다.

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [44] [Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing](https://arxiv.org/abs/2512.09882)
*Justin W. Lin,Eliot Krzysztof Jones,Donovan Julian Jasper,Ethan Jun-shen Ho,Anna Wu,Arnold Tianyi Yang,Neil Perry,Andy Zou,Matt Fredrikson,J. Zico Kolter,Percy Liang,Dan Boneh,Daniel E. Ho*

Main category: cs.AI

TL;DR: 라이브 엔터프라이즈 환경에서 인간 보안 전문가와 AI 에이전트를 정면 비교 평가한 첫 연구.


<details>
  <summary>Details</summary>
Motivation: 실제 대규모 네트워크에서 AI 보안 에이전트가 인간 전문가와 비교해 어느 정도 성능과 한계를 가지는지 체계적으로 검증할 필요가 있었기 때문이다.

Method: 약 8,000대 호스트와 12개 서브넷으로 구성된 대형 대학 네트워크에서 10명의 보안 전문가, 기존 6개 AI 에이전트, 그리고 새로 제안한 멀티에이전트 프레임워크 ARTEMIS를 동시에 평가했다. ARTEMIS는 동적 프롬프트 생성, 임의의 서브에이전트 구성, 자동 취약점 분류 기능을 갖추고 있다.

Result: ARTEMIS는 전체 2위를 기록하며 9개의 유효한 취약점을 발견했고, 82%의 유효 제출률을 보였으며 10명 중 9명의 인간 참가자를 능가했다. 기존 Codex, CyAgent 등은 대부분의 인간보다 낮은 성능을 보였다.

Conclusion: ARTEMIS 수준의 최신 AI 에이전트는 상위권 인간 전문가에 근접한 기술적 완성도와 보고 품질을 달성할 수 있으며, 체계적 나열, 병렬 공격, 비용 측면에서 강점이 있다. 그러나 오탐 비율이 높고 GUI 기반 작업에 취약한 등 뚜렷한 한계도 존재한다.

Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.

</details>


### [45] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: 이 논문은 재료과학 분야에서 AI와 인간 참여(크라우드소싱 포함)를 결합한 MatSci-YAMZ 플랫폼을 제안하고, 메타데이터 용어 정의를 효율적으로 생성·정제할 수 있음을 보인 개념증명 연구이다.


<details>
  <summary>Details</summary>
Motivation: FAIR/FARR 데이터 원칙을 구현하려면 잘 정리된 메타데이터 어휘가 필수지만, 인력 부족과 표준화 방식의 비일관성 때문에 개발이 느리고 비효율적이다. 이를 해결하기 위해 AI와 인간 참여를 결합해 메타데이터 어휘를 더 빠르고 투명하게 만들 필요가 있다.

Method: 재료과학이라는 다학제 분야를 사례로, NSF ID4에 소속된 6명이 MatSci-YAMZ 플랫폼을 몇 주 동안 사용하도록 했다. 참가자들은 용어 정의를 제안하고 예시를 입력했으며, AI는 이를 바탕으로 정의를 생성·수정했다. 인간 피드백과 AI 생성 정의 사이에 반복적 피드백 루프(HILT)를 구성하여 19개의 용어 정의를 최종 도출했다.

Result: 6명 참여, 총 19개의 AI 생성·정제된 용어 정의를 성공적으로 생산했다. 반복적인 AI-인간 피드백 루프가 실질적으로 작동했고, 메타데이터 어휘 개발 과정에서 AI-HILT 모델의 실행 가능성을 입증했다.

Conclusion: MatSci-YAMZ 기반의 AI-HILT 모델은 FAIR와 오픈사이언스 원칙에 부합하며, 메타데이터 어휘 개발의 합의 형성 시간을 줄이고 의미론적 투명성을 높일 잠재력이 있다. 또한 후속 연구를 위한 연구 프로토콜을 제시했고, 이 접근법이 재료과학을 넘어 다른 도메인으로 확장될 수 있는 확장성을 가진다는 점을 확인했다.

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [46] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: 이 논문은 텍스트 기반 환경에서의 장기 계획을 위해, LLM을 반복 호출하지 않고 초기 한 번만 활용해 서브골을 생성·사전학습하는 경량 계층적 플래너 SCOPE를 제안하며, 기존 LLM 플래너보다 더 높은 성공률과 훨씬 빠른 추론 속도를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 텍스트 기반 복잡 환경에서는 행동 공간이 크고 관측이 모호하며 보상이 희박해 장기 계획이 어렵다. LLM이 풍부한 세계 지식을 내재하고 있어 고수준 계획에 유용하지만, 기존 방법은 LLM을 학습·추론 과정 내내 반복 호출해 계산 비용이 크고, 고정된 사전학습 가중치를 사용해 과제 적응성이 낮다는 문제가 있다. 따라서 LLM의 지식을 효율적으로 이용하면서도, 보다 가볍고 과제에 적응적인 플래너가 필요하다.

Method: SCOPE라는 원샷 계층적 플래너를 제안한다. 1) 예시 궤적(traj.)으로부터 LLM을 단 한 번 호출해 서브골들을 생성하고, 2) 이 LLM 생성 서브골을 레이블로 사용하는 서브골-조건 사전학습을 통해 경량 학생 모델을 학습한다. 3) 학습된 학생 모델은 이후 LLM 재호출 없이 계층적 목표 분해와 계획에 사용된다. 기존 연구처럼 학습 중 적응적으로 LLM을 여러 번 프롬프트하는 증류 방식 대신, 초기 서브골 세트를 한 번에 뽑아 정적으로 활용하는 구조이다.

Result: TextCraft 환경에서, 기존 LLM 기반 계층적 에이전트 ADaPT(성공률 0.52, 추론 시간 164.4초) 대비 SCOPE는 성공률 0.56을 달성하면서도 추론 시간을 3.0초까지 줄였다. 즉, 소폭 더 높은 성능과 매우 큰 효율성 향상을 동시에 보였다.

Conclusion: LLM이 생성한 서브골이 다소 비최적이고 설명 가능성이 떨어지더라도, 초기 한 번의 호출로 얻은 서브골을 활용해 경량 모델을 사전학습하면 텍스트 기반 장기 계획에서 강력한 계층적 분해 출발점을 제공한다. 이를 통해 반복적인 LLM 호출 없이도 높은 성능과 매우 빠른 추론을 달성할 수 있으며, LLM 지식의 효율적 활용 방안으로서 유망함을 보인다.

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


### [47] [Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective](https://arxiv.org/abs/2512.09908)
*Antonio Lorenzin,Fabio Zanasi*

Main category: cs.AI

TL;DR: 이 논문은 베이지안 네트워크와 마르코프 네트워크 사이의 ‘도덕화(moralisation)’와 ‘삼각화(triangulation)’ 변환을 범주론적(카테고리 이론) 틀에서 펑터로 정식화한다.


<details>
  <summary>Details</summary>
Motivation: 확률 그래프 모델에서 베이지안 네트워크(유향 그래프)와 마르코프 네트워크(무향 그래프)는 서로 다른 인수분해 방식을 제공하지만, 두 표현 사이의 표준 변환(도덕화, 삼각화)은 보통 알고리즘적·기술적으로만 설명된다. 저자들은 이를 더 구조적이고 추상적인 범주론 언어로 기술하여, 어떤 부분이 문법적(구문적, syntactic) 변환이고 어떤 부분이 의미적(semantic, 실제 확률 분포 관련) 변환인지 명확히 구분하고자 한다.

Method: 1) ‘베이지안 네트워크의 범주’와 ‘마르코프 네트워크의 범주’를 정의하고, 각 네트워크를 하나의 ‘문법(syntax) → 의미(semantics)’ 펑터로 표현한다.
2) 도덕화를 베이지안 네트워크 범주에서 마르코프 네트워크 범주로 가는 펑터로 정의하고, 이는 전적으로 문법적 조작임을 보인다.
3) 삼각화를 반대 방향의 변환으로 다루되, 이 과정이 의미(확률 분포)에 의존함을 지적한다.
4) 변수 소거(variable elimination) 알고리즘을 별도의 펑터로 재해석하여, 삼각화를 (a) 순수 문법적 부분과 (b) 순수 의미적 부분으로 분해한다.

Result: 도덕화와 삼각화가 각각 범주 사이의 펑터로 자연스럽게 정식화될 수 있음을 보였고, 특히 도덕화는 완전히 구문적 변환인 반면, 삼각화는 의미 정보(확률 분포)에 의존하는 변환임을 이 틀 안에서 명확히 드러냈다. 또한 변수 소거 알고리즘을 삼각화 절차를 두 부분(구문/의미)으로 나누는 펑터로 해석할 수 있음을 제시했다.

Conclusion: 확률 그래프 모델 이론에 범주론적·펑터적 관점을 도입함으로써, 베이지안/마르코프 네트워크 변환에서 일어나는 구조적(구문적) 수정과 의미적 수정이 개념적으로 분리되어 이해될 수 있음을 보였다. 이는 향후 다른 그래프 변환, 추론 알고리즘, 그리고 보다 일반화된 확률 모델을 통합적으로 다루는 이론적 기반을 제공할 수 있다.

Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.

</details>
