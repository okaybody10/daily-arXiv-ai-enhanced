<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.AI](#cs.AI) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition](https://arxiv.org/abs/2512.13884)
*Jonas Golde,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: FiNERweb is a large, systematically built, multilingual NER dataset generated via an LLM-based teacher-student pipeline over 91 languages, designed to provide high-quality synthetic supervision for training NER models.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual NER work shows LLMs can create useful synthetic labels, but prior datasets are ad hoc by-products, not standardized resources. There is a need for a scalable, reusable, and systematically constructed multilingual NER dataset with high-quality annotations and broad language coverage to better support training and evaluation of multilingual NER systems.

Method: The authors build a pipeline on top of FineWeb-Edu that first trains regression models to detect NER-relevant passages. These selected passages are then annotated using multilingual LLMs in a teacher-student framework. The process scales to 91 languages and 25 scripts, yielding a large collection of passages with named entity labels. They evaluate the passage selection (regression) model using F1, assess downstream NER performance in zero-shot transfer on multiple languages, and use LLM-as-a-judge to score annotation faithfulness and completeness. They also create both English label sets and translated label sets for each target language and analyze performance differences between them.

Result: The regression model for identifying NER-relevant passages achieves over 84 F1. The resulting FiNERweb dataset contains around 225k passages with 235k distinct entity labels across 91 languages. Models trained on FiNERweb achieve comparable or better zero-shot NER performance on English, Thai, and Swahili compared with strong baselines, even though FiNERweb uses 19x less training data. LLM-as-a-judge evaluation yields high scores for annotation faithfulness (3.99/5) and completeness (4.05/5). The authors also find that evaluating with target language label sets instead of English labels causes performance drops of 0.02–0.09 F1 for state-of-the-art models.

Conclusion: FiNERweb provides a scalable, high-quality synthetic supervision resource for multilingual NER, demonstrating that carefully curated LLM-generated labels can train competitive models with significantly less data. The dataset’s broad language coverage, dual English and translated label sets, and open release of data and artifacts are intended to support more effective teacher-student training and more realistic evaluation setups for multilingual NER research.

Abstract: Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.

</details>


### [2] [Olmo 3](https://arxiv.org/abs/2512.13961)
*Team Olmo,:,Allyson Ettinger,Amanda Bertsch,Bailey Kuehl,David Graham,David Heineman,Dirk Groeneveld,Faeze Brahman,Finbarr Timbers,Hamish Ivison,Jacob Morrison,Jake Poznanski,Kyle Lo,Luca Soldaini,Matt Jordan,Mayee Chen,Michael Noukhovitch,Nathan Lambert,Pete Walsh,Pradeep Dasigi,Robert Berry,Saumya Malik,Saurabh Shah,Scott Geng,Shane Arora,Shashank Gupta,Taira Anderson,Teng Xiao,Tyler Murray,Tyler Romero,Victoria Graf,Akari Asai,Akshita Bhagia,Alexander Wettig,Alisa Liu,Aman Rangapur,Chloe Anastasiades,Costa Huang,Dustin Schwenk,Harsh Trivedi,Ian Magnusson,Jaron Lochner,Jiacheng Liu,Lester James V. Miranda,Maarten Sap,Malia Morgan,Michael Schmitz,Michal Guerquin,Michael Wilson,Regan Huff,Ronan Le Bras,Rui Xin,Rulin Shao,Sam Skjonsberg,Shannon Zejiang Shen,Shuyue Stella Li,Tucker Wilde,Valentina Pyatkin,Will Merrill,Yapei Chang,Yuling Gu,Zhiyuan Zeng,Ashish Sabharwal,Luke Zettlemoyer,Pang Wei Koh,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: Olmo 3 is a fully open family of 7B and 32B language models optimized for long-context reasoning, tools, coding, and chat, released with complete training transparency.


<details>
  <summary>Details</summary>
Motivation: Provide a state-of-the-art yet fully open alternative to proprietary LLMs, especially for long-context reasoning, function calling, and coding, while maximizing reproducibility and transparency by releasing the entire model lifecycle.

Method: Build 7B and 32B transformer-based language models specialized for long-context reasoning, tools, coding, instruction following, and chat, and release all stages of training including checkpoints, data points, and dependencies. The flagship variant, Olmo 3 Think 32B, is optimized as a 'thinking' model.

Result: Olmo 3 achieves state-of-the-art performance among fully open models, with the 32B 'Think' variant being the strongest open thinking model available at release time.

Conclusion: Fully-open, long-context-capable language models can reach state-of-the-art performance when carefully trained and fully documented, enabling transparent research and practical deployment as an alternative to closed models.

Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.

</details>


### [3] [Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models](https://arxiv.org/abs/2512.13980)
*Zhimin Qiu,Di Wu,Feng Liu,Chenrui Hu,Yuxiao Wang*

Main category: cs.CL

TL;DR: The paper introduces a structure-aware decoding method using large language models to improve nested and overlapping entity extraction by jointly modeling entity boundaries, hierarchies, and cross-dependencies, achieving higher accuracy and robustness on ACE 2005.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for nested and overlapping entity extraction struggle to maintain both semantic integrity (correct meaning) and structural consistency (correct hierarchical and overlapping relationships). As a result, they often misidentify entity boundaries, fail on multi-entity co-occurrence, or break long-distance dependencies. With large language models showing strong semantic representation ability, there is a need to design decoding mechanisms that can leverage this power while explicitly respecting structural constraints in complex information extraction tasks.

Method: The method uses a pretrained language model to obtain context-aware token representations, then generates candidate spans to cover multi-granular entity possibilities. It constructs span representations via combinations of token embeddings and applies structured attention to model relationships among spans, including hierarchies and cross-dependencies. During decoding, hierarchical structural constraints are enforced to keep predicted entities semantically plausible and structurally consistent. The training objective jointly optimizes standard classification loss (for entity types and boundaries) and a structural consistency loss that penalizes violations of hierarchical and overlap constraints, thereby stabilizing performance under multi-entity co-occurrence and long-sentence dependencies.

Result: On the ACE 2005 dataset, the proposed model significantly improves Accuracy, Precision, Recall, and F1-Score compared to baselines. The gains are especially pronounced for nested and overlapping entity recognition, where the method shows better boundary localization and more accurate modeling of hierarchical and cross-span structures, maintaining high recognition accuracy even in complex sentences with multiple entities and long-range dependencies.

Conclusion: Structure-aware decoding built on large language models effectively addresses semantic and structural conflicts in nested and overlapping entity extraction. By unifying boundary detection, hierarchical relationship modeling, and cross-dependency handling under explicit structural constraints and a joint loss, the approach delivers higher-precision information extraction. The work demonstrates that incorporating hierarchical structural awareness into decoding is a promising direction for developing language models with deeper structural understanding and provides a methodological basis for future high-precision, structure-sensitive extraction systems.

Abstract: This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.

</details>


### [4] [What Affects the Effective Depth of Large Language Models?](https://arxiv.org/abs/2512.14064)
*Yi Hu,Cai Zhou,Muhan Zhang*

Main category: cs.CL

TL;DR: The paper investigates how many layers in large language models are actually used (“effective depth”) and finds that models systematically underuse their available depth across sizes, training types, and task difficulties.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are commonly scaled by adding more layers, empirical performance gains diminish, suggesting that depth might not be fully exploited. Prior notions of “effective depth” hint that many layers do little meaningful computation, but how this behaves across model scales, training paradigms, and task difficulties is not well understood. The authors aim to clarify whether and how added depth is used in practice, and what this implies for model design and efficiency.

Method: The authors conduct a systematic empirical study of effective depth in the Qwen-2.5 model family ranging from 1.5B to 32B parameters. They measure how many layers contribute meaningful computation (effective layers) and examine: (1) scaling with model size, (2) differences between base and long chain-of-thought (long-CoT) variants, and (3) behavior across tasks of varying difficulty. They compare the absolute number of effective layers and the ratio of effective depth to total depth, and analyze whether models dynamically increase depth usage for harder tasks.

Result: They find that as model size increases, the absolute number of effective layers grows, but the ratio of effective depth to total depth remains roughly constant. Long-CoT training does not increase effective depth relative to base models, indicating that better reasoning is due to processing longer contexts, not deeper per-token computation. Across tasks of different difficulty, models do not recruit more layers for harder problems. Overall, LLMs appear to use only a stable fraction of their depth, regardless of scale, training type, or task difficulty.

Conclusion: Current LLMs underutilize their depth: only a subset of layers contributes meaningfully, and this fraction does not increase with model size, specialized long-CoT training, or task difficulty. This opens research directions on improving layer utilization, as well as on exploiting redundant depth for model pruning and early-exit strategies. The findings challenge the assumption that simply adding more layers reliably enhances per-token computation and reasoning depth.

Abstract: The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of "effective depth", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.

</details>


### [5] [Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed](https://arxiv.org/abs/2512.14067)
*Yonggan Fu,Lexington Whalen,Zhifan Ye,Xin Dong,Shizhe Diao,Jingyu Liu,Chengyue Wu,Hao Zhang,Enze Xie,Song Han,Maksim Khadkevich,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.CL

TL;DR: The paper proposes more efficient diffusion language models (dLMs) by converting pretrained autoregressive (AR) models, achieving higher accuracy and significantly higher throughput than prior AR and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models allow parallel, non-autoregressive generation and can be faster at inference, but when trained from scratch they are less sample- and compute-efficient than standard autoregressive language models. The authors want to leverage strong pretrained AR LMs and convert them into diffusion LMs that retain accuracy while gaining speed, overcoming shortcomings in existing AR-to-dLM conversion methods.

Method: 1) Systematically compare attention patterns for AR-to-dLM conversion and show that preserving the pretrained AR weight distribution is crucial. Based on this, they introduce a continuous pretraining scheme using block-wise attention: attention is causal across blocks but bidirectional within each block. This keeps AR-like structure while allowing more parallelism and KV caching. 2) Identify a mismatch between training and test-time mask token distributions in diffusion training (uniform masking vs. left-to-right behavior at inference), and propose a position-dependent masking strategy that increases masking probability for later tokens so that training better matches test-time usage. 3) Use this framework to empirically study attention patterns, training dynamics, and design choices for scalable AR-to-dLM conversion, leading to the Efficient-DLM model family.

Result: The resulting Efficient-DLM models preserve or improve task accuracy compared with strong AR baselines while providing large speedups. Specifically, Efficient-DLM 8B surpasses state-of-the-art models like Dream 7B and Qwen3 4B, achieving +5.4% and +2.7% higher accuracy while delivering 4.5x and 2.7x higher throughput, respectively. The experiments validate that block-wise attention and position-dependent masking lead to better AR-to-dLM conversion than fully bidirectional attention and uniform masking.

Conclusion: Maintaining AR weight distributions via block-wise attention and aligning training with test-time masking behavior are key principles for effective AR-to-dLM conversion. With these design choices, diffusion language models can be converted from pretrained AR LMs to achieve both high accuracy and substantial efficiency gains, making dLMs a practical alternative to conventional autoregressive models for scalable language generation.

Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

</details>


### [6] [A Unified Sparse Attention via Multi-Granularity Compression](https://arxiv.org/abs/2512.14082)
*Siran Liu,Zane Cao,Yongchao He*

Main category: cs.CL

TL;DR: UniSparse introduces composite tokens and dynamic sparse attention to make long-context processing in LLMs faster and nearly as accurate as full attention across modalities.


<details>
  <summary>Details</summary>
Motivation: LLMs need to handle very long sequences for tasks like multi-turn dialogue and program analysis, but standard self-attention scales quadratically with sequence length, making it computationally prohibitive. Existing sparse attention methods either require costly retraining and are not plug-and-play, or they trade off efficiency, accuracy, or cross-modal generality. A more universal, efficient, and accurate sparse attention mechanism is needed that can act as an acceleration plugin for various models and modalities.

Method: The paper proposes UniSparse, which introduces composite tokens—compact representations that aggregate information at multiple granularities. Using these, UniSparse dynamically constructs sparse attention by (1) multi-granularity compression of context into composite tokens and (2) block-level selection to determine which parts of the sequence should attend to each other. The design focuses on being hardware-friendly for GPUs, enabling efficient sparse attention execution. UniSparse is evaluated as a plug-in across multiple modalities and tasks, and compared with other sparse attention baselines such as MInference, XAttention, and FlexPrefill, as well as with dense FlashAttention.

Result: Across synthetic and real-world tasks in multiple modalities, UniSparse consistently outperforms state-of-the-art sparse attention methods in both accuracy and speed. It maintains at least 99% of the accuracy of full attention while achieving up to 2.61× faster attention computation than FlashAttention, demonstrating both high efficiency and minimal loss in model performance.

Conclusion: UniSparse provides a unified, plug-and-play sparse attention mechanism based on composite tokens and dynamic block-level selection, delivering near-full-attention accuracy with significantly reduced computation. Its hardware-friendly design and strong empirical results across modalities suggest it is a practical solution for efficient long-context understanding and reasoning in LLM applications.

Abstract: Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

</details>


### [7] [Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study](https://arxiv.org/abs/2512.14085)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Taiga Mori,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: A Transformer-based multilingual model predicts conversational backchannel timing in Japanese, English, and Chinese, revealing both shared and language-specific timing cues and supporting more natural, culturally-aware dialogue systems.


<details>
  <summary>Details</summary>
Motivation: Backchannel responses (e.g., nods, “uh-huh”) are crucial for natural, smooth conversation, but their appropriate timing differs across languages and cultures. Existing models are often monolingual and may not capture cross-linguistic regularities or differences, limiting the generality and cultural adaptability of spoken dialogue systems. The authors aim to build a single model that works across multiple languages and to use it as a tool to systematically study how backchannel timing varies cross-linguistically.

Method: They build a Transformer-based, frame-level backchannel prediction model trained on about 300 hours of dyadic conversational data in Japanese, English, and Chinese. The model is trained in a multilingual fashion, with joint learning on the three languages and auxiliary tasks to improve representation learning. They compare multilingual vs. monolingual models, conduct zero-shot transfer experiments with only two languages seen in training, and use perturbation analyses to see which input cues (e.g., silence, prosody, short-term linguistic information) each language relies on. They also perform context-length ablation to test how much temporal context each language needs, and finally integrate the model into a real-time CPU-only inference system.

Result: The multilingual model performs as well as or better than monolingual baselines in all three languages, indicating it captures both universal and language-specific timing cues. However, zero-shot transfer using only two languages is limited, showing substantial cross-lingual differences in backchannel behavior. Perturbation analyses show Japanese backchannels depend more on short-term linguistic context, whereas English and Chinese rely more on silence duration and prosodic changes. Multilingual training encourages shared but flexible representations and reduces excessive dependence on pitch for Chinese. Context-length experiments reveal Japanese is relatively insensitive to shorter contexts, while Chinese performance improves notably with longer contexts. The model runs in real time on CPU in a deployed software framework.

Conclusion: A single multilingual Transformer can effectively predict backchannel timing for Japanese, English, and Chinese, while revealing meaningful cross-linguistic differences in cue use and context dependence. These findings highlight that backchannel timing is shaped by both universal and language-specific factors and cannot be trivially transferred across languages. The insights—especially regarding cue reliance and context needs—can guide the design of spoken dialogue systems that behave more naturally and are better adapted to cultural and linguistic norms, and the demonstrated CPU-only real-time integration shows the approach is practically deployable.

Abstract: We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.

</details>


### [8] [CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models](https://arxiv.org/abs/2512.14118)
*Yiran Zhang,Jincheng Hu,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: CogMem is a memory-augmented LLM architecture that uses structured, persistent memory to sustain accurate multi-turn reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well in single-turn tasks but degrade over long, multi-turn dialogues due to reasoning bias, task drift, hallucination, overconfidence, and memory decay, while naive history appending causes unbounded context and inefficiency. The paper aims to address these limitations.

Method: The authors propose CogMem, a cognitively inspired architecture with three components: (1) Long-Term Memory (LTM) that stores and consolidates reusable reasoning strategies across sessions; (2) Direct Access (DA) memory that keeps session-specific notes and retrieves relevant items from LTM; and (3) a Focus of Attention (FoA) mechanism that reconstructs a concise, task-relevant context at each turn instead of appending the full history.

Result: On the TurnBench benchmark for multi-turn reasoning, CogMem reduces common failure modes (reasoning bias, task drift, hallucinations, overconfidence, memory decay), limits context growth, and yields more consistent behavior over long reasoning chains compared with standard LLM baselines that rely on full-history prompts.

Conclusion: Layered, structured memory inspired by cognitive theories can significantly improve the reliability, efficiency, and coherence of LLMs in extended multi-turn reasoning, bringing them closer to human-like sustained reasoning performance.

Abstract: Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

</details>


### [9] [Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents](https://arxiv.org/abs/2512.14142)
*Hongqiu Ni,Jiabao Zhang,Guopeng Li,Zilong Wang,Ruiqi Wu,Chi Zhang,Haisheng Tan*

Main category: cs.CL

TL;DR: Astraea is a service engine that optimizes the end-to-end latency (Job Completion Time) of LLM-based agent workflows that interleave computation and I/O, rather than optimizing each compute segment in isolation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference systems like vLLM are optimized per computation segment and ignore the multi-stage, I/O-interleaved nature of agent workflows. This leads to suboptimal global Job Completion Time (JCT) when requests involve alternating local computation and external network calls. There is a need for a system that holistically schedules and manages such workflows to reduce end-to-end latency and improve robustness under high load.

Method: Astraea introduces a state-aware, hierarchical scheduling algorithm that uses both the historical state of each request and predictions about its future behavior. It dynamically classifies requests as I/O-intensive or compute-intensive and applies an enhanced Highest Response Ratio Next (HRRN) policy to balance efficiency and fairness. Additionally, it includes an adaptive KV cache manager that decides how to manage and evict agent state during I/O wait periods based on current memory pressure.

Result: Experimental evaluation shows that Astraea can reduce average Job Completion Time by up to 25.5% compared to baseline systems, and it maintains robust and stable performance even under high load and across different model sizes.

Conclusion: Optimizing LLM-based agent systems at the level of the full request lifecycle, rather than individual compute segments, significantly improves end-to-end latency. Astraea’s combination of state-aware hierarchical scheduling, dynamic request classification, enhanced HRRN, and adaptive KV cache management effectively reduces JCT and scales robustly under load, making it a promising approach for serving agentic LLM workloads.

Abstract: Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.

</details>


### [10] [A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs](https://arxiv.org/abs/2512.14179)
*K. M. Jubair Sami,Dipto Sumit,Ariyan Hossain,Farig Sadeque*

Main category: cs.CL

TL;DR: The paper introduces two RAG-based pipelines for translating standard Bengali into multiple regional dialects, showing that a sentence-pair retrieval strategy substantially improves translation quality and lets small LLMs outperform much larger ones without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Standard-to-dialect translation, especially for Bengali, is difficult because dialects are low-resource, varied, and lack parallel data. Existing MT and LLM approaches either require extensive fine-tuning data or fail to capture fine-grained dialectal patterns. The paper aims to build a practical, data-efficient method that can handle several Bengali dialects, support linguistic preservation, and work well even with smaller models.

Method: They design two retrieval-augmented generation (RAG) pipelines for translating standard Bengali into dialects. (1) Transcript-Based Pipeline: retrieves long dialect sentence contexts from audio transcripts as exemplars to guide the LLM. (2) Standardized Sentence-Pairs Pipeline: builds a structured retrieval index of local_dialect:standard_bengali parallel sentence pairs and retrieves the most relevant examples at inference time. Multiple LLM backbones (including smaller models like Llama-3.1-8B and larger ones like GPT-OSS-120B) are used as generators. Performance is evaluated across six Bengali dialects using BLEU, ChrF, WER, and BERTScore.

Result: Across six Bengali dialects, the sentence-pair pipeline consistently outperforms the transcript-based one. For example, for the Chittagong dialect, Word Error Rate drops from 76% with the transcript-based pipeline to 55% with the sentence-pair pipeline, along with corresponding gains in BLEU, ChrF, and BERTScore. Moreover, when equipped with the better retrieval strategy, smaller models such as Llama-3.1-8B surpass the performance of much larger models such as GPT-OSS-120B.

Conclusion: A well-designed retrieval strategy in a RAG setup can matter more than LLM size for low-resource dialect translation. The standardized sentence-pair RAG pipeline provides an effective, fine-tuning-free solution for standard-to-dialect Bengali translation across multiple dialects, and offers a practical, scalable blueprint for preserving linguistic diversity in other low-resource language varieties.

Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.

</details>


### [11] [Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets](https://arxiv.org/abs/2512.14237)
*Estelle Zheng,Nathan Cerisara,Sébastien Warichet,Emmanuel Helbert,Christophe Cerisara*

Main category: cs.CL

TL;DR: They propose Ladder Side Tuning (LST), a parameter-efficient fine-tuning method that greatly reduces memory usage while maintaining performance comparable to QLoRA, and extend it with xLadder for better reasoning depth without extra memory.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models on commodity GPUs is constrained by memory, and even existing parameter-efficient methods like QLoRA still use a lot of memory during backpropagation. There is a need for techniques that further reduce memory while preserving or improving performance and scaling behaviour.

Method: They revisit and refine Ladder Side Tuning (LST), which adds a lightweight side network to the base LLM instead of modifying all parameters, thereby reducing backward-pass memory. They analyze its compute and memory scaling versus QLoRA, derive scaling laws, and propose xLadder, a depth-extended version with cross-connections that increases effective depth and improves chain-of-thought without increasing parameter count or memory use.

Result: LST reduces peak memory consumption by about 50% compared to QLoRA while achieving comparable accuracy across various benchmarks (NLU, math, and LLM-critic tasks). It enables fine-tuning 7B-parameter models with 2k-token context on a 12 GB GPU without gradient checkpointing, where QLoRA runs out of memory. Scaling laws indicate LST scales similarly to QLoRA, and xLadder further improves reasoning depth at fixed memory and parameter budgets.

Conclusion: LST is a highly memory-efficient PEFT method that preserves competitive downstream performance and QLoRA-like scaling, making fine-tuning large models feasible on low-memory GPUs. The xLadder variant leverages the flexible architecture to deepen reasoning (shorter chains-of-thought) without additional memory, making the overall Ladder framework particularly advantageous when memory is the main constraint.

Abstract: Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

</details>


### [12] [Two CFG Nahuatl for automatic corpora expansion](https://arxiv.org/abs/2512.14239)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Graham Ranger Martha-Lorena Avendaño-Garrido*

Main category: cs.CL

TL;DR: The paper introduces two context-free grammars to automatically generate large amounts of syntactically valid Nawatl sentences, expanding scarce corpora to train word/sentence embeddings, which in turn improve semantic similarity performance and can outperform some larger LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Nawatl is a low-resource Amerindian language with very limited digital text, making it difficult to train language models or learn reliable embeddings. Existing corpora are too small for robust representation learning, which hinders NLP applications like semantic similarity. The authors want a systematic way to enlarge the corpus without manual annotation, while preserving grammaticality of sentences.

Method: The authors design two context-free grammars specifically for Nawatl, capturing its syntactic structures. They then use these grammars in generative mode to produce many artificial but syntactically valid Nawatl sentences. The expanded corpus (original plus generated sentences) is used to train non-contextual embeddings. These embeddings are evaluated on a sentence semantic similarity task and compared to models trained only on the original corpus as well as some LLM-based approaches.

Result: The artificially expanded corpus enables learning better Nawatl embeddings. On a sentence semantic similarity evaluation, embeddings trained on the grammar-augmented corpus outperform embeddings trained only on the original small corpus, and in many cases these compact, non-contextual embeddings even outperform some larger LLM-based methods for this task.

Conclusion: Rule-based corpus expansion via context-free grammars is an effective strategy for very low-resource languages like Nawatl. By generating syntactically valid artificial sentences, the authors can significantly improve embedding quality and downstream semantic similarity performance. The work suggests that carefully designed symbolic resources and compact embeddings can be competitive with or superior to LLMs when resources are extremely scarce.

Abstract: The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $π$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.

</details>


### [13] [From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition](https://arxiv.org/abs/2512.14244)
*Yiqing Zhou,Yu Lei,Shuzheng Si,Qingyan Sun,Wei Wang,Yifei Wu,Hao Wen,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: They propose an EDU-based, structure-aware context compression method for LLMs that preserves discourse structure, reduces cost, and improves performance on long-context tasks, validated on a new structural benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with very long contexts due to computational cost, noise, and loss of coherence when using existing compression methods that either drop tokens or use opaque latent encodings that don’t align well with APIs or preserve structure.

Method: They model context compression as a two-step structure-then-select process: (1) use LingoEDU to convert text into a structural relation tree of Elementary Discourse Units (EDUs) anchored to the original text indices; (2) apply a lightweight ranking module to select query-relevant subtrees, then linearize them back into compressed text. They also create StructBench, a human-annotated dataset for evaluating structural understanding.

Result: The EDU-based compressor achieves state-of-the-art accuracy on structural prediction, outperforms strong LLM baselines on compression for downstream tasks, and lowers computational cost for long-context processing, as shown using the StructBench dataset and other long-context/deep search evaluations.

Conclusion: Explicit, discourse-structure-aware EDU-based compression mitigates the limitations of existing methods, enabling more faithful, efficient, and effective use of long contexts in LLM applications, with demonstrated gains on structure prediction and downstream long-context tasks.

Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

</details>


### [14] [Inflation Attitudes of Large Language Models](https://arxiv.org/abs/2512.14306)
*Nikoleta Anesti,Edward Hill,Andreas Joseph*

Main category: cs.CL

TL;DR: The paper tests whether GPT-3.5 can mimic households’ inflation perceptions and expectations using only pre-2021 training and controlled macroeconomic price prompts.


<details>
  <summary>Details</summary>
Motivation: Central banks and social scientists rely on household surveys to understand inflation perceptions and expectations, but such surveys are costly, slow, and limited in design flexibility. With the rise of LLMs that exhibit human-like language and reasoning, there is interest in whether these models can stand in as synthetic survey respondents, especially for macroeconomic questions like inflation where expectations matter for policy. The authors want to know how closely an LLM’s inflation views line up with real households and what drives the model’s answers.

Method: The authors use GPT-3.5-turbo as a synthetic survey participant. They construct prompts that mimic the information set and respondent characteristics of the Bank of England’s Inflation Attitudes Survey, including demographics like income, housing tenure, and social class. They exploit GPT’s September 2021 training cut-off as a quasi-experiment, since the model has not been trained on the subsequent UK inflation surge. They then compare GPT’s inflation perceptions and expectations to actual household survey data and official inflation statistics across time horizons and demographic groups. To interpret the model’s responses, they develop a Shapley value–based decomposition tailored to LLM outputs in a synthetic survey framework, attributing parts of the response to specific prompt elements such as food price information.

Result: GPT’s aggregate short-horizon inflation projections closely track both survey averages and official statistics despite not having been trained on the post-2021 UK inflation surge. At the micro level, GPT reproduces well-known empirical patterns in households’ inflation perceptions across key demographic dimensions like income, housing tenure, and social class. The Shapley decomposition reveals that GPT’s answers are particularly sensitive to food inflation information, mirroring the salience of food prices for human respondents. However, analysis of its responses also shows that GPT does not maintain a coherent, stable internal model of consumer price inflation over different contexts and prompts.

Conclusion: GPT-3.5 can approximate household inflation perceptions and expectations in several important ways, both at aggregate and disaggregated levels, and its responses are driven by salient price information similarly to human respondents. Nonetheless, it lacks a consistently structured economic model of inflation, limiting its reliability as a substitute for real households. The proposed Shapley-based interpretability framework offers a general tool for assessing LLM behaviour in social science applications, comparing different models, and informing the design and testing of survey instruments.

Abstract: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

</details>


### [15] [Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.14427)
*Gabriele Prato,Shagun Sodhani,Alessandro Sordoni,Sarath Chandar*

Main category: cs.CL

TL;DR: The paper studies how different ways of packing multiple documents into a training sequence affect large language models’ multi-hop reasoning abilities, finding that packing generally helps but costs more compute, and analyzing why.


<details>
  <summary>Details</summary>
Motivation: Standard LLM training packs multiple documents in a sequence to better use compute, but it is unclear how this affects deeper, latent capabilities such as multi-hop reasoning. Understanding this effect can guide better training strategies rather than treating packing as a purely engineering optimization.

Method: The authors compare training LLMs with different document-packing strategies against training on single, un-packed documents, focusing on tasks that probe latent multi-hop reasoning. They then run an ablation study that systematically varies elements of the packing procedure to isolate which aspects drive any gains (e.g., number of documents per sequence, boundaries handling, ordering, etc.).

Result: They find that models trained with document packing can outperform those trained on individual documents in multi-hop reasoning benchmarks, but that this comes with increased computational cost. The ablations identify a small number of crucial factors in the packing setup that are responsible for most of the observed performance gains.

Conclusion: Document packing is not just a low-level optimization: it meaningfully shapes LLM capabilities, especially multi-hop reasoning. With a better understanding of which packing factors matter, practitioners can design more effective and efficient training pipelines for large language models.

Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

</details>


### [16] [SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models](https://arxiv.org/abs/2512.14481)
*Shizhuo Mao,Song Chen,Yi Kang*

Main category: cs.CL

TL;DR: The paper introduces SASQ, a lightweight quantization-aware training framework that only learns activation quantization factors, enabling static low-precision inference for LLMs with higher accuracy than prior quantization methods and even FP16 baselines.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly hard to deploy because model sizes grow faster than GPU memory. Quantization can reduce memory and compute costs, but current methods trade off between dynamic schemes that are too slow or hard to deploy on edge devices, and static schemes that lose accuracy. Additionally, existing QAT requires costly weight retraining. There is a need for a quantization method that keeps static inference efficiency, avoids retraining weights, and still delivers high accuracy for LLMs.

Method: SASQ is a quantization-aware training framework that keeps pre-trained weights fixed and optimizes only activation quantization factors. It is designed specifically for activation quantization in LLMs. During training, SASQ adaptively truncates outlier activations, simplifying the quantization problem while preserving the essential distributional properties of activations. This targeted optimization of quantization factors enables effective static quantization without the full cost of weight training.

Result: SASQ achieves state-of-the-art performance among quantization schemes for LLMs and can even surpass the accuracy of FP16 models. On LLaMA2-7B evaluated on WikiText2, SASQ attains 5.2% lower perplexity than the SOTA quantization method QuaRot and 4.7% lower perplexity than the corresponding FP16 model, demonstrating both better quantization quality and improved language modeling performance.

Conclusion: Optimizing only activation quantization factors through a lightweight QAT procedure is sufficient to enable accurate static quantization for LLMs, avoiding weight retraining while improving both deployment efficiency and model quality. By adaptively truncating outliers and preserving activation distributions, SASQ overcomes key trade-offs in prior quantization methods, producing quantized models that are more accurate than existing SOTA schemes and even higher-precision FP16 baselines.

Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

</details>


### [17] [C-ing Clearly: Enhanced Binary Code Explanations using C code](https://arxiv.org/abs/2512.14500)
*Teodor Poncu,Ioana Pintilie,Marius Dragoi,Dragos Tantaru,Florin Brad*

Main category: cs.CL

TL;DR: Using C code to generate synthetic training data that helps LLMs better understand assembly, improving performance on binary summarization and vulnerability detection across models.


<details>
  <summary>Details</summary>
Motivation: LLMs are strong on high-level languages like Python or C but weak on low-level representations like assembly. Many security and binary analysis tasks require reasoning directly over assembly or binary code. The authors want a way to cheaply and effectively transfer the LLMs' knowledge of C into better performance on assembly without requiring huge amounts of labeled assembly data.

Method: They introduce a synthetic data generation pipeline called "C-ing Clearly". It uses pairs of C code and their compiled assembly (or otherwise aligned C–assembly representations) to automatically create training examples. These examples train/fine-tune LLMs so that, given assembly code, the model leverages implicit C-level semantics it has learned, improving understanding of what the assembly does and where vulnerabilities might be. The approach is applied as a fine-tuning recipe across different base LLMs.

Result: After fine-tuning with the C-ing Clearly synthetic data, LLMs perform better on tasks involving binary/assembly code, specifically binary code summarization and vulnerability detection. The improvements are consistent across multiple LLM families and model sizes, indicating the method is robust and not tied to a single architecture.

Conclusion: Aligning low-level assembly with its high-level C counterpart via synthetic data is an effective way to transfer LLM capabilities from high-level programming languages to binary analysis tasks. The C-ing Clearly method provides a generally applicable fine-tuning strategy to boost assembly-level reasoning for multiple LLM architectures, improving downstream tasks like summarization and vulnerability detection.

Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.

</details>


### [18] [Linguists should learn to love speech-based deep learning models](https://arxiv.org/abs/2512.14506)
*Marianne de Heer Kloots,Paul Boersma,Willem Zuidema*

Main category: cs.CL

TL;DR: The paper critiques the focus on text-only LLMs and proposes that audio-based deep learning models are essential for a fuller scientific account of human language.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks connecting deep learning and linguistic theory have mainly centered on generative, text-based large language models. However, much of human language use involves spoken interaction, prosody, and other auditory dimensions that text alone cannot capture. The authors are motivated to highlight this gap and push for a more comprehensive integration of audio-based models into the study of language and cognition.

Method: Conceptual and theoretical analysis: the authors respond to Futrell and Mahowald’s framework by identifying its limitations regarding modality. They argue, likely through examples and reasoning rather than experiments, that many core linguistic phenomena (e.g., prosody, phonetics, spoken discourse) require audio-based models and cannot be adequately addressed by text-only systems.

Result: They articulate specific reasons and domains where audio models are crucial, such as prosody, phonology, and conversational dynamics, showing that restricting the framework to text-based LLMs misses key aspects of language. They broaden the discussion about what kinds of deep learning models should be considered in dialogue with linguistics.

Conclusion: A robust bridge between deep learning and linguistic theory must not be confined to generative text-based LLMs. Audio-based deep learning models should be central in future work, as they capture crucial dimensions of human language that are absent from written text, enabling richer and more accurate linguistic inquiry.

Abstract: Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.

</details>


### [19] [VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse](https://arxiv.org/abs/2512.14531)
*Ying Nie,Kai Han,Hongguang Li,Hang Zhou,Tianyu Guo,Enhua Wu,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: VersatileFFN is a new FFN design for LLMs that reuses the same parameters in flexible ways along width and depth to gain additional effective capacity without increasing memory.


<details>
  <summary>Details</summary>
Motivation: Scaling LLMs improves performance but quickly becomes memory-prohibitive. Parameter-efficient methods like pruning and quantization mostly shrink existing models instead of increasing their effective capacity, so they are limited by the base architecture’s representational ceiling. The authors want a way to get more expressive capacity from a fixed parameter budget, focusing on compute–rather than memory–based scaling.

Method: They introduce VersatileFFN, which restructures the FFN block to support parameter reuse along both width and depth. The width-versatile path forms a mixture of sub-experts from a single shared FFN, approximating sparse MoE behavior without adding parameters. The depth-versatile path repeatedly applies the same FFN to a token, imitating deeper processing while reusing weights. A difficulty-aware gating mechanism routes tokens: simpler tokens mainly use the width path, while harder tokens receive more iterative depth processing. Both paths share parameters, so extra capacity comes only from additional computation steps, not new weights.

Result: Across multiple benchmarks and model sizes, VersatileFFN yields better performance than baselines at the same parameter count, indicating that the architecture successfully increases effective capacity under a fixed memory budget.

Conclusion: It is possible to meaningfully extend the capacity of LLMs at fixed parameter counts by designing FFNs that reuse parameters adaptively in width and depth. VersatileFFN demonstrates that compute-oriented scaling, controlled by token-level difficulty-aware gating, can reduce the memory–performance trade-off of current LLM architectures.

Abstract: The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

</details>


### [20] [Dual Language Models: Balancing Training Efficiency and Overfitting Resilience](https://arxiv.org/abs/2512.14549)
*David Samuel,Lucas Georges Gabriel Charpentier*

Main category: cs.CL

TL;DR: They train language models with a mix of autoregressive and masked-diffusion objectives (without changing the architecture) and find this dual training consistently outperforms using either objective alone.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models are efficient to train but overfit easily, especially under data repetition, while masked-diffusion language models are more robust to overfitting but train less efficiently. The authors want a way to get the training efficiency of autoregressive models and the robustness of masked-diffusion models in a single framework, and to understand how to balance these two objectives quantitatively.

Method: They jointly train language models on both autoregressive and masked-diffusion objectives, keeping the model architecture unchanged and only varying the training loss. They systematically sweep over different mixing ratios between the two objectives and over different levels of data repetition. In total, they train and evaluate 50 language models across these conditions to measure performance and overfitting behavior for both autoregressive-style and masked-diffusion-style downstream metrics.

Result: Across all tested settings, models trained with a mix of the two objectives outperform models trained with only one objective. The dual-objective models show better robustness to data repetition than purely autoregressive models, while retaining good efficiency and performance. The optimal mixing ratio is stable across different data repetition levels and is similar whether one optimizes for autoregressive downstream performance or for masked-diffusion downstream performance.

Conclusion: A simple dual-objective training scheme that combines autoregressive and masked-diffusion losses, without architectural changes, yields consistently better language models than either objective alone. The trade‑off between the two losses can be captured by a single, fairly stable optimal ratio that works well across data repetition regimes and for both types of downstream tasks, suggesting this mixed-objective approach is broadly useful for training robust and efficient language models.

Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

</details>


### [21] [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554)
*Nguyen Tien Dong,Minh-Anh Nguyen,Thanh Dat Hoang,Nguyen Tuan Ngoc,Dao Xuan Quang Minh,Phan Phi Hai,Nguyen Thi Ngoc Anh,Dang Van Tu,Binh Vu*

Main category: cs.CL

TL;DR: They introduce VLegal-Bench, the first comprehensive benchmark to systematically evaluate large language models on Vietnamese legal tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but it is unclear how well they handle the complex, hierarchical, and frequently changing Vietnamese legal system. There is no systematic, domain-specific benchmark to measure their legal understanding and performance, which is needed to safely and effectively deploy AI in Vietnamese law.

Method: They design VLegal-Bench based on Bloom’s cognitive taxonomy, covering multiple cognitive levels of legal understanding. They build 10,450 samples via a rigorous annotation pipeline where legal experts label and cross-check each instance. All samples are explicitly grounded in authoritative Vietnamese legal documents and constructed to reflect real-world legal assistant workflows: general Q&A, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving.

Result: The outcome is a standardized, transparent benchmark dataset (10,450 expert-annotated, legally grounded samples) that spans diverse Vietnamese legal tasks and difficulty levels, suitable for evaluating LLMs’ abilities in the Vietnamese legal domain.

Conclusion: VLegal-Bench provides a solid, cognitively informed framework to assess LLM performance on Vietnamese law, enabling more reliable comparison of models and supporting the development of safer, more interpretable, and ethically aligned AI legal assistants for the Vietnamese context.

Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

</details>


### [22] [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562)
*Tejaswani Dash,Dinesh Karri,Anudeep Vurity,Gautam Datla,Tazeem Ahmad,Saima Rafi,Rohith Tangudu*

Main category: cs.CL

TL;DR: PolyPersona is a framework and dataset for generating persona-conditioned synthetic survey responses using small instruction-tuned language models.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale, persona-rich survey data is expensive, slow, and difficult to control, which limits scalable evaluation and bias analysis of language models and survey instruments. The authors want an efficient, reproducible way to generate survey-like responses that preserve explicit persona cues across many domains and personas using compact models rather than large, costly ones.

Method: They design PolyPersona, a generative framework that instruction-tunes compact chat models (e.g., TinyLlama 1.1B, Phi-2) using parameter-efficient LoRA adapters with 4-bit quantization, under a resource-adaptive training setup. A dialogue-based data pipeline is built to explicitly encode and preserve persona cues so that generated responses remain behaviorally consistent with the assigned persona. With this pipeline, they synthesize a dataset of 3,568 persona-conditioned survey responses across ten domains and 433 personas. They then evaluate performance with a multi-metric suite combining standard generation metrics (BLEU, ROUGE, BERTScore) with custom survey-focused metrics that assess structural coherence, stylistic consistency, and sentiment alignment with the persona and survey context.

Result: Compact models fine-tuned with PolyPersona achieve performance comparable to larger 7B–8B baselines on the synthetic survey generation task. For example, they report a highest BLEU of 0.090 and ROUGE-1 of 0.429, along with strong scores on survey-specific metrics for coherence, style consistency, and persona-consistent sentiment. The generated dataset spans 3,568 responses, ten domains, and 433 personas, illustrating broad coverage and controlled persona conditioning.

Conclusion: Persona-conditioned fine-tuning via PolyPersona allows small language models to generate reliable, coherent synthetic survey responses that closely follow specified personas. This provides an efficient, reproducible pipeline for survey data generation and evaluation, reducing reliance on large models while enabling scalable testing and bias analysis through transparent, open protocols.

Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.

</details>


### [23] [Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies](https://arxiv.org/abs/2512.14576)
*Ekaterina Artemova,Laurie Burchell,Daryna Dementieva,Shu Okabe,Mariya Shmatova,Pedro Ortiz Suarez*

Main category: cs.CL

TL;DR: Tutorial on practical NLP pipelines for multilingual and low-resource languages with focus on fairness and real-world impact.


<details>
  <summary>Details</summary>
Motivation: Many languages are underrepresented in NLP, leading to inequitable technologies and performance gaps due to data scarcity and cultural variance. There is a need for practical, reproducible methods to build language technologies for these low-resource and multilingual settings.

Method: Provide a hands-on tutorial and toolkit that covers the full pipeline for low-resource NLP: data collection and web crawling, parallel sentence mining, machine translation, and downstream tasks like text classification and multimodal reasoning. Present strategies, modeling frameworks, and development practices that emphasize fairness, reproducibility, and community involvement, grounded in real-world case studies.

Result: Attendees gain concrete techniques, tools, and example workflows for building end-to-end low-resource NLP systems across more than 10 languages from diverse families and contexts, including both resource-rich and severely underrepresented languages.

Conclusion: Comprehensive, community-informed, and fair development practices can make it feasible to build effective NLP pipelines even for underrepresented languages, leading to more equitable and socially impactful language technologies.

Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.

</details>


### [24] [Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer](https://arxiv.org/abs/2512.14585)
*Adarsha Shrestha,Basanta Pokharel,Binit Shrestha,Smriti Adhikari,Dinesh Gothe*

Main category: cs.CL

TL;DR: They build and train a Nepali GPT-2 language model with a Nepali-specific tokenizer and modern training tricks, achieving good perplexity and coherent news-style generation.


<details>
  <summary>Details</summary>
Motivation: Nepali is a low-resource language with complex grammar and morphology, and existing NLP work mostly focuses on encoder models, which are inadequate for high-quality Nepali text generation. There is a need for a strong, generative Nepali language model trained on a sizeable, cleaned corpus using up-to-date large language model training practices.

Method: They design a GPT-2-style decoder-only language model but adapt several GPT-3-inspired training strategies, such as optimized learning rate schedules, batch scaling, and architectural refinements. They create a custom 16k BPE tokenizer trained solely on Nepali text to improve subword segmentation. The model is pretrained on a large combined Nepali corpus: a 10.75GB cleaned NepBERTa dataset plus additional web-scraped Nepali news articles. They incorporate FlashAttention to reduce memory consumption and stabilize training, and they train the model for two epochs, monitoring training and validation loss and perplexity.

Result: After two epochs of pretraining, the model reaches a training loss of 3.168177, validation loss of 3.081982, and a final perplexity of 21.80. Qualitative evaluation indicates it can generate coherent text in the style of Nepali news articles.

Conclusion: A GPT-2-based Nepali language model with a Nepali-specific tokenizer, trained on a large curated corpus using modern optimization and attention techniques, can successfully model Nepali and produce coherent news-style text despite the language’s low-resource status and morphological complexity.

Abstract: Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.

</details>


### [25] [JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction](https://arxiv.org/abs/2512.14620)
*Atsuyuki Miyai,Shota Onohara,Jeonghun Baek,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: The paper presents JMMMU-Pro, a new Japanese image-based multimodal benchmark, and Vibe Benchmark Construction, a scalable human-in-the-loop method to build such benchmarks using image generation models.


<details>
  <summary>Details</summary>
Motivation: Existing Japanese multimodal benchmarks like JMMMU do not fully test integrated visual-text understanding via visual perception, and manual creation of such datasets is expensive. The authors want a more rigorous, low-cost evaluation tool for Japanese capabilities of large multimodal models (LMMs).

Method: They extend JMMMU by rendering both question images and textual questions into a single image so models must read and interpret text visually. They introduce Vibe Benchmark Construction: an image generator (e.g., Nano Banana Pro) creates candidate visual questions; humans verify outputs, adjust prompts, and regenerate when needed to guarantee quality. This process exploits realistic image generation and clean Japanese text rendering to efficiently cover diverse designs and layouts.

Result: Using this pipeline, they construct JMMMU-Pro, a wide-coverage Japanese multimodal benchmark with high-quality images. Experiments show all tested open-source LMMs perform poorly on JMMMU-Pro, indicating the benchmark’s difficulty and current limitations of such models on Japanese visual-text understanding.

Conclusion: JMMMU-Pro is a challenging and comprehensive benchmark for evaluating Japanese capabilities of LMMs, especially integrated visual-text understanding. The proposed Vibe Benchmark Construction offers an efficient, scalable recipe for building future image-based VQA benchmarks using generative models plus human verification.

Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.

</details>


### [26] [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)
*David Schulmeister,Valentin Hartmann,Lars Klein,Robert West*

Main category: cs.CL

TL;DR: The paper introduces TiME, a family of small, efficient monolingual language encoders, demonstrating that they can achieve a better balance between task performance and efficiency (throughput, latency, energy) than large general-purpose models, using modern training techniques like distillation and supporting low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Large, general-purpose language models dominate current research, but many real-world NLP pipelines only need narrow, well-defined capabilities. Large models are too slow, energy-hungry, and impractical for high-throughput or real-time applications and for deployment on battery-powered devices. There is a need for smaller, more efficient models that still perform well and can support low-resource languages, closing the gap between academic LLM research and practical, efficiency-critical scenarios.

Method: The authors design and train small, task-focused monolingual encoder models called TiME. They apply modern training techniques, particularly knowledge distillation, to transfer knowledge from larger multilingual teacher models to smaller monolingual students. They investigate and demonstrate two specific distillation settings: (1) distilling monolingual models from multilingual teachers, and (2) distilling models using absolute positional embeddings from teachers using relative positional embeddings. They then benchmark TiME models on a variety of common NLP tasks and measure both accuracy-style performance and efficiency metrics such as throughput, latency, and energy consumption.

Result: TiME models achieve a more favorable trade-off between performance and efficiency compared to larger, general-purpose models. They maintain competitive benchmark results on standard NLP tasks while substantially improving throughput, latency, and energy usage, making them better suited for large-scale or real-time applications and deployment on limited hardware. The experiments also successfully validate that knowledge can be effectively distilled (a) from multilingual teachers into monolingual students and (b) across different positional embedding schemes (relative to absolute).

Conclusion: Carefully designed small monolingual encoders, when trained with modern techniques like distillation, can provide an attractive alternative to large general-purpose language models in many practical scenarios, especially those constrained by latency, throughput, energy consumption, or hardware limitations. The TiME models demonstrate that it is feasible to support even low-resource languages while achieving strong task performance with significantly improved efficiency. Additionally, the work broadens the understanding of distillation by showing it can be done across both language coverage (multilingual→monolingual) and positional encoding types (relative→absolute).

Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

</details>


### [27] [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681)
*Lanxiang Hu,Siqi Kou,Yichao Fu,Samyam Rajbhandari,Tajana Rosing,Yuxiong He,Zhijie Deng,Hao Zhang*

Main category: cs.CL

TL;DR: Introduces Jacobi Forcing, a progressive distillation paradigm that turns autoregressive (AR) LLMs into efficient parallel decoders, achieving ~4x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Parallel multi-token generation can greatly speed up transformer-based LLM inference, but existing diffusion LLM approaches that adapt AR models to parallel decoding face a pretrain–posttrain mismatch. Masked training distributions differ from real data, and bidirectional attention conflicts with the causal prior from pretraining and prevents exact KV-cache reuse, leading to limited real-world speedups.

Method: Propose Jacobi Forcing, a progressive distillation training scheme where an AR model is gradually shifted into a parallel decoder by training on its own generated parallel decoding trajectories. This preserves the model’s causal inference property and allows efficient KV-cache reuse. They further analyze the properties of the resulting Jacobi Forcing Models’ trajectories and design a multi-block decoding algorithm with rejection recycling to increase the number of accepted tokens per iteration while controlling errors.

Result: Jacobi Forcing Models obtain up to 3.8x wall-clock speedup over standard AR decoding on coding and math benchmarks, with only minimal degradation in task performance. With the additional multi-block decoding plus rejection recycling strategy, they achieve up to 4.5x higher token acceptance per iteration and nearly 4.0x end-to-end speedup, demonstrating effective latency reduction.


Conclusion: Jacobi Forcing provides a practical way to convert pretrained AR LLMs into high-quality, parallel decoders without sacrificing their causal structure or KV-cache efficiency, overcoming key limitations of prior diffusion-based multi-token generation methods. The added multi-block decoding with rejection recycling further improves throughput by trading modest extra compute for substantially lower inference latency.

Abstract: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.

</details>


### [28] [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687)
*Yen-Ju Lu,Kunxiao Gao,Mingrui Liang,Helin Wang,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: They build a synthetic speech dataset (Spoken DialogSum) that aligns conversational audio with both factual and emotion-focused summaries plus rich paralinguistic labels to enable emotion-aware spoken dialogue summarization research.


<details>
  <summary>Details</summary>
Motivation: Existing audio language models can handle long speech conversations, but research on emotion-aware or spoken dialogue summarization is limited because there is no large-scale dataset that jointly connects raw audio, textual summaries, and detailed paralinguistic/emotional annotations. This gap makes it hard to train and evaluate end-to-end audio models that must reason about both semantics and affect.

Method: They construct a new dataset in two main stages starting from the DialogSum text corpus. First, a large language model rewrites the original scripts into more natural, Switchboard-like conversational transcripts by inserting fillers and backchannels while also tagging each utterance with emotion, pitch, and speaking rate. Second, an expressive text-to-speech (TTS) system synthesizes speech audio from these annotated scripts, ensuring alignment between the generated audio and the paralinguistic labels. The final corpus contains dialogues with both factual and emotion-centric summaries plus utterance-level labels for age, gender, and emotion.

Result: The resulting Spoken DialogSum dataset has 13,460 dialogues with diverse emotional content, each aligned with raw audio, a factual summary, an emotion-rich summary, and utterance-level paralinguistic labels, including age, gender, emotion, pitch, and speaking rate. Baseline experiments comparing systems show that an end-to-end Audio-LLM significantly improves emotional-summary ROUGE-L scores (by 28% relative) over a pipeline system that uses automatic speech recognition (ASR) followed by an LLM, demonstrating the benefit of modeling speech directly.

Conclusion: Spoken DialogSum provides the first large-scale resource that tightly couples conversational audio with both factual and emotion-rich summaries and detailed paralinguistic annotations. Initial baselines indicate that end-to-end audio language models can better exploit these signals for emotion-focused summarization than cascaded ASR-then-text models, highlighting the dataset’s utility for future research on emotionally aware spoken dialogue understanding and summarization.

Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

</details>


### [29] [MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/abs/2512.14691)
*Zefan Cai,Haoyi Qiu,Tianyi Ma,Haozhe Zhao,Gengze Zhou,Kung-Hsiang Huang,Parisa Kordjamshidi,Minjia Zhang,Xiao Wen,Jiuxiang Gu,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: They propose MMGR, a new benchmark and evaluation framework to test whether video and image generative models actually follow physical, logical, and spatial reasoning constraints, not just look good.


<details>
  <summary>Details</summary>
Motivation: Current video foundation models can generate realistic-looking content, but existing metrics like FVD focus on perceptual quality and ignore reasoning errors such as causality violations, physical law breaking, or global inconsistencies. There is a need for a principled, fine-grained evaluation that measures genuine generative reasoning ability across different domains and modalities.

Method: They design MMGR, a multi-modal benchmark organized around five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. The benchmark includes tasks from three domains: Abstract Reasoning (e.g., ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports scenarios and compositional interactions). MMGR defines fine-grained metrics that require holistic correctness across both video and image generations, then uses these metrics to systematically evaluate several state-of-the-art video and image generative models.

Result: When tested on MMGR, leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image) exhibit large performance gaps across domains. They do relatively better on Physical Commonsense tasks, but their performance on Abstract Reasoning is very low (e.g., <10% accuracy on ARC-AGI), and they struggle with tasks requiring long-horizon spatial planning in embodied navigation environments.

Conclusion: Current generative models, despite high perceptual quality, have serious limitations as world simulators: they overfit to visual appearance, maintain weak global state consistency, and tend to optimize for visual plausibility rather than causal correctness. MMGR provides a unified, diagnostic benchmark to expose these shortcomings and guide the development of future reasoning-aware generative world models.

Abstract: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.

</details>


### [30] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: They build a large, multi-domain Bangla abstractive summarization dataset and show it works well as a benchmark with several neural models.


<details>
  <summary>Details</summary>
Motivation: Existing Bangla summarization focuses mainly on news, which has uniform writing style and limited domain coverage, so models trained there don’t generalize to diverse real-world Bangla texts from blogs and social media. There is a need to reduce information overload for Bangla readers across many types of content, but suitable large, multi-source datasets are missing, especially for this low-resource language.

Method: They collect and curate over 54,000 Bangla article–summary pairs from multiple sources, including blogs (e.g., Cinegolpo) and newspapers (Samakal, The Business Standard), ensuring coverage of multiple domains and writing styles. They then train and evaluate several neural summarization models—such as LSTM-based models and pretrained sequence-to-sequence transformers like BanglaT5-small and MTS-small—to establish baseline performance on this new dataset.

Result: They obtain working abstractive summarization systems for Bangla on this dataset, with performance reported for LSTM, BanglaT5-small, and MTS-small models, demonstrating that the dataset is suitable for training and benchmarking summarization models. The experiments show that the dataset can effectively support deep learning and transfer learning approaches for Bangla summarization.

Conclusion: The new multi-source, multi-domain Bangla abstractive summarization dataset is a valuable benchmark for future Bangla NLP research. It provides a robust foundation for developing practical summarization systems and significantly enriches available resources for Bangla and other low-resource languages.

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: The paper proposes and evaluates a secure, modular LLM-based framework to automatically extract structured clinical features from unstructured EHR notes, reducing the need for manual chart review.


<details>
  <summary>Details</summary>
Motivation: Manual chart review of unstructured EHR notes is slow, labor-intensive, and requires expert clinicians, which limits the scale and speed of clinical research. There is a need for accurate, automated methods that can transform narrative clinical text into structured data while satisfying strict healthcare privacy and deployment constraints.

Method: The authors design a secure, locally deployed framework that uses large language models with retrieval-augmented generation (RAG) and structured response prompting to extract predefined clinical features from free-text clinical notes. The system is packaged as a modular, scalable container running on HIPAA-compliant institutional infrastructure, enabling integration into different clinical domains and workflows. They evaluate performance by comparing LLM-extracted features against an expert-annotated gold-standard dataset.

Result: The framework achieved high accuracy on multiple medical characteristics extracted from large collections of patient notes. During evaluation it also surfaced several errors in the expert-annotated reference set, showing that the automated system was in some cases more consistent or accurate than manual chart review.

Conclusion: A locally deployed, RAG-enhanced LLM framework can reliably perform structured feature extraction from unstructured clinical notes within HIPAA-compliant environments. This approach can substantially reduce the burden and variability of manual chart review, improve data quality, and accelerate clinical research across diverse domains.

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [32] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: They propose a way to build indoor radio maps and user trajectories from MIMO-OFDM channel data without any position labels, and validate it experimentally.


<details>
  <summary>Details</summary>
Motivation: Conventional radio map construction needs many measurements with known locations, which is expensive and often impractical indoors. There is a need for a method that can exploit raw MIMO-OFDM channel state information to build radio maps and track users without requiring labeled location data.

Method: The paper first analyzes MIMO-OFDM channel behavior in non-line-of-sight (NLOS) environments modeled as quasi-specular, and proves that channel state information (CSI) varies spatially in a continuous way. From this, the authors derive a CSI-based distance metric that approximates true physical distance. For straight user paths with access points distributed according to a Poisson point process, they derive the Cramér-Rao Lower Bound for localization and show it can go to zero asymptotically despite low angular resolution. Using these theoretical insights, they develop a spatially regularized Bayesian inference framework that jointly estimates channel features, classifies links as LOS or NLOS, and reconstructs user trajectories from unlabeled CSI data. They then evaluate the approach on a ray-tracing-generated dataset.

Result: On the ray-tracing dataset, the proposed blind radio map construction method achieves an average localization error of 0.68 meters and a beam map reconstruction error of 3.3%, indicating high accuracy in both position estimation and radio map reconstruction without location labels.

Conclusion: Exploiting spatial continuity properties of NLOS CSI and a derived CSI-distance metric, a Bayesian framework can blindly construct radio maps and recover user trajectories from unlabeled MIMO-OFDM measurements. Theoretical analysis via CRLB and experimental results show that accurate indoor localization and radio map reconstruction are achievable without costly location-labeled data.

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [33] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator is a neuro-symbolic system that uses a knowledge graph and a multi-agent LLM “Council of Agents” to automatically detect and correct noisy labels in industrial ML datasets, achieving near-perfect F1 on a benchmark subset and outperforming single-LLM and non-KG baselines.


<details>
  <summary>Details</summary>
Motivation: Industrial ML systems are constrained by the quality of their training data, and noisy labels in high-stakes applications can severely degrade model performance and user trust. Existing automated approaches struggle to reliably detect complex, structural labeling errors and to provide explainable corrections suitable for deployment in strictly governed, production environments. There is a need for a robust, high-precision, and explainable system that can automatically verify and correct labels to create golden datasets.

Method: The paper introduces Adjudicator, which formulates label-noise identification and correction as a neuro-symbolic task. First, it builds a dynamic Knowledge Graph (KG) that integrates contextual information about items. This KG is then used to inform a “Council of Agents,” a multi-agent LLM architecture where specialized agents debate and vote on the correctness of labels. A novel KG-based override logic enables the system to definitively flag complex, structural errors. The system is evaluated on a 1,000-item balanced subset of the AlleNoise benchmark, comparing a KG-informed council, a single LLM baseline, and a council without KG information.

Result: On the AlleNoise subset, the KG-informed Adjudicator achieves an F1-score of 0.99, substantially outperforming both a single-LLM baseline (0.48 F1) and a council architecture without KG integration (0.59 F1). Analysis indicates that the performance gain comes primarily from high precision enabled by the KG-driven override mechanism, which attains complete recall on complex, structural label errors that all baselines miss.

Conclusion: Adjudicator demonstrates that combining a dynamic knowledge graph with a multi-agent LLM council yields a robust, highly accurate, and explainable system for automated label verification and correction. The near-perfect performance and ability to capture complex structural errors support its suitability for production deployment and highlight its potential as a foundation for creating golden datasets in tightly regulated industrial settings.

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [34] [LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms](https://arxiv.org/abs/2512.13713)
*Ali Parsaee,Yashar Talebirad,Csongor Szepesvári,Vishwajeet Ohal,Eden Redman*

Main category: cs.AI

TL;DR: Benchmark (LoopBench) to test how LLM agents coordinate and avoid infinite loops in distributed symmetry-breaking tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs are being used as autonomous agents in multi-agent and distributed settings, but we lack a clear understanding and standardized evaluation of how well they coordinate, break symmetry, and reflect meta-cognitively when they cannot communicate directly.

Method: Define LoopBench, a benchmark based on graph-coloring odd cycles (C3, C5, C11) with too few colors, a setting where purely deterministic, non-communicating agents fall into infinite loops. Introduce a strategy-passing mechanism that acts as consistent memory between rounds so that LLM agents can revise strategies. Evaluate standard LLMs, stronger reasoning-oriented models, and classical heuristics on these tasks.

Result: Standard LLMs and classical graph-coloring heuristics often get stuck in deadlock loops on the benchmark, whereas advanced reasoning models (like O3) can invent strategies that let them escape those deadlocks by using the strategy-passing mechanism effectively.

Conclusion: LoopBench is a useful testbed for probing and comparing LLMs’ emergent distributed algorithms, symmetry breaking ability, and collective intelligence, revealing that stronger reasoning models exhibit more effective coordination strategies than standard LLMs and traditional heuristics.

Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.

</details>


### [35] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: The paper proposes an AI-based annotation pipeline to detect and correct stability problems in LLM outputs, reducing reliance on costly human-only RLHF and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs are unreliable in high‑stakes, regulated domains due to instability, hallucinations, inconsistent reasoning, and performance variability. Existing stabilization methods like RLHF and supervised fine-tuning depend heavily on expensive, non‑scalable human annotation. There is a need for a more efficient, scalable way to generate high‑quality feedback signals that improve LLM robustness and reliability.

Method: The authors design a human‑AI synergy annotation pipeline. It uses automated weak supervision and confidence-based annotation to pre‑label LLM outputs for instability patterns, then routes uncertain or critical cases to humans for validation. They define stability-specific annotation categories—semantic consistency, factual correctness, and logical coherence—and use these as structured labels in feedback loops to iteratively calibrate and retrain models.

Result: The pipeline can systematically identify, label, and correct instability patterns in LLM outputs while reducing pure human-annotation workload. It produces reliable, morally vetted feedback data that can be used to improve LLM robustness and stability across workflows, particularly in regulated settings.

Conclusion: A mixed human-AI annotation framework with targeted stability categories and feedback loops provides a scalable alternative or complement to RLHF and supervised fine-tuning for stabilizing LLMs. By focusing on semantic consistency, factual correctness, and logical coherence, the approach enables continuous calibration and more reliable deployment of LLMs in precision-demanding domains.

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [36] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: Proposes an adaptive Meta-Hierarchical Reinforcement Learning framework for O-RAN that jointly optimizes resource allocation and network slicing, achieving better efficiency and faster adaptation than baseline RL/meta-RL.


<details>
  <summary>Details</summary>
Motivation: Modern wireless applications require real-time adaptability and efficient resource management. Existing AI/ML-based O-RAN control methods degrade under highly dynamic, unpredictable conditions and often handle resource allocation and network slicing separately. There is a need for a unified, robust learning-based controller that adapts quickly at scale while providing theoretical guarantees.

Method: Introduce a Meta Hierarchical Reinforcement Learning (Meta-HRL) framework inspired by Model Agnostic Meta Learning (MAML) for O-RAN. Use a two-level hierarchy: a high-level controller that allocates resources across network slices and low-level agents that perform per-slice scheduling. Incorporate an adaptive meta-update mechanism that weights training tasks by temporal-difference (TD) error variance to focus learning on more complex or unstable scenarios. Provide theoretical analysis proving sublinear convergence and regret for the two-level learning process. Evaluate via simulations, ablations, and scalability studies.

Result: Simulation experiments show that the proposed Meta-HRL framework improves network management efficiency by 19.8% compared to standard RL and meta-RL baselines. It also yields faster adaptation, higher QoS satisfaction for eMBB, URLLC, and mMTC slices, and up to 40% faster adaptation in ablation/scalability tests while maintaining fairness, latency, and throughput as network size increases.

Conclusion: The adaptive Meta-HRL framework effectively combines hierarchical control with meta-learning to provide robust, rapidly adaptable resource allocation and network slicing in O-RAN. It outperforms conventional RL and meta-RL methods in efficiency, adaptation speed, and QoS, scales well with network size, and is supported by theoretical convergence and regret guarantees, making it a strong candidate for practical O-RAN RIC deployment.

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [37] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: The paper proposes ValuePilot, a framework for building AI agents that make personalized decisions based on users’ stable human values rather than just task rewards, and shows it better matches human choices than strong LLM baselines on new scenarios.


<details>
  <summary>Details</summary>
Motivation: As AI systems are increasingly deployed in real-world settings, they need to adapt to individual users’ value preferences, not just optimize for task completion or generic collective alignment. Existing decision-making approaches are mostly reward- or task-oriented and struggle with personalization, interpretability, and generalization to novel contexts. The authors are motivated to exploit human values as more stable, transferable signals to guide personalized AI behavior in a principled way.

Method: They propose ValuePilot, a two-phase framework. First, the Dataset Generation Toolkit (DGT) uses a human–LLM collaborative pipeline to construct a diverse set of decision scenarios, each annotated with value considerations. Second, the Decision-Making Module (DMM) is trained to evaluate and select actions based on users’ personal value preferences rather than external rewards. The DMM learns from the value-annotated scenarios to make context-sensitive, individualized decisions that should generalize beyond the training tasks.

Result: On previously unseen scenarios, the DMM’s action choices align more closely with human choices than several strong large language model baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b. This suggests that explicitly modeling value-driven decision-making improves both personalization and generalization compared to generic LLM decision policies.

Conclusion: The authors conclude that value-driven decision-making, instantiated in their ValuePilot framework, is an effective engineering pathway for building personalized AI agents. By grounding decisions in explicit human value signals, the approach yields interpretable, context-sensitive behavior that generalizes better to new situations than standard task- or reward-driven methods, and can be extended to diverse real-world applications of human–AI interaction.

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [38] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: The paper empirically studies how low‑precision quantization (INT8, NF4) affects large language models’ ability to perform causal reasoning across Pearl’s three levels (association, intervention, counterfactual).


<details>
  <summary>Details</summary>
Motivation: As LLMs are used for high‑stakes decisions, their causal reasoning must be reliable. At the same time, deployment is moving to edge and resource‑constrained settings where quantized models are standard. However, it is unclear whether and how precision reduction harms formal causal reasoning, especially across Pearl’s full causal ladder.

Method: The authors systematically evaluate quantized versions of Llama 3 8B (e.g., INT8, NF4) on a stratified 3000‑sample CLadder benchmark that covers association, interventional, and counterfactual queries. They compare performance across precisions by rung and query type (e.g., collider bias, backdoor adjustment). They also test on the CRASS commonsense counterfactual benchmark and study Graph Retrieval Augmented Generation (RAG) that supplies ground‑truth causal graphs, measuring how this affects interventional reasoning under quantization.

Result: On CLadder, overall rung‑level accuracy of Llama 3 8B is largely preserved under quantization; NF4 yields under 1% average degradation. Interventional (rung‑2) questions are the most affected by precision loss, while counterfactual (rung‑3) questions are relatively robust but show uneven weaknesses across specific causal patterns. On CRASS, performance is almost identical across precisions, suggesting such datasets are not sensitive enough to detect quantization‑related reasoning drift. Adding graph‑based RAG improves NF4 interventional accuracy by about 1.7%, partially compensating for compression‑induced losses.

Conclusion: Causal reasoning in LLMs is surprisingly robust even under 4‑bit quantization, though certain interventional query types remain vulnerable. Graph‑structured augmentation can selectively bolster interventional performance, while widely used counterfactual benchmarks do not adequately probe deeper causal brittleness. The study provides an initial empirical characterization of “compressed causal reasoning” and practical pointers for designing and deploying quantized causal AI systems with structural support.

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [39] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: The paper presents a qualitative case-study method to audit large language models’ behavior in long conversations, identifying a pattern they call learned incapacity (LI), where models selectively refuse in policy-sensitive areas while working normally elsewhere.


<details>
  <summary>Details</summary>
Motivation: Standard quantitative benchmarks miss important behavioral patterns that appear only in long, interactive use of LLMs, especially around safety policies and refusals. The authors want a way to systematically study these policy-linked behavioral asymmetries as they emerge during real conversations.

Method: They run a detailed 86-turn dialogue with a single LLM and qualitatively code its responses into three regimes: Normal Performance (NP), Functional Refusal (FR), and Meta-Narrative (MN). They examine when and how each regime appears, focusing on provider- or policy-sensitive topics versus broad, non-sensitive domains, and analyze co-occurrence patterns of MN and FR. They use learned helplessness as an analogy to define the new descriptor “learned incapacity.”

Result: In the case study, the same model behaves normally and helpfully in broad, non-sensitive topics (NP) but repeatedly refuses to engage (FR) in provider- or policy-sensitive domains, revealing a stable asymmetry in behavior across domains within one long conversation. Meta-narrative role-framing responses frequently appear alongside refusals in those sensitive contexts, reinforcing the pattern. This supports the usefulness of their three-regime coding and the LI descriptor.

Conclusion: The authors argue that interaction-level, qualitative auditing can surface alignment side effects—like selective, policy-driven learned incapacity—that are not visible in standard benchmarks. They propose LI as a non-anthropomorphic label for such patterns and recommend broader multi-user, multi-model studies using this framework to better understand and tune LLM alignment behavior.

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [40] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: The paper argues that math and coding tasks occupy a special, nearly universal position in the space of tests for AI systems, especially for self-improving agents.


<details>
  <summary>Details</summary>
Motivation: To understand which kinds of evaluation tasks are most fundamental for assessing and driving the capabilities of advanced, potentially self-improving AI agents, especially within the AAI and GVU theoretical frameworks.

Method: Using the AAI framework and GVU dynamics from prior work, the authors formally define a "Mathematics Fiber"—the subset of the moduli space of psychometric batteries consisting of math and coding tasks. They then prove a density theorem under assumptions such as uniform tightness of agent outputs and a Lipschitz AAI functional, showing that theorem-proving and coding tasks generate a dense subspace in this moduli space with respect to the evaluation metric. They also analyze spectral properties of GVU flows when coupled with formal proof kernels (Lean, Coq).

Result: They show that, given stated regularity conditions, batteries made from mathematical theorem-proving and coding tasks are dense in the full space of psychometric batteries, in the sense of the evaluation metric. Coding tasks alone are universal (dense), whereas pure math tasks are not fully universal, but have special spectral stability properties under GVU dynamics when paired with formal proof checkers. GVU flows on the Mathematics Fiber can exhibit spectrally stable self-improvement regimes because of oracle-like verification by proof kernels.

Conclusion: Mathematics and coding tasks serve as "universal coordinates" for evaluating AI agents: by focusing on them, one can approximate the evaluative power of arbitrary test batteries. Formal mathematics, in particular, appears to be a natural and stable domain for initiating recursive self-improvement in advanced AI systems, even if it is not expressively universal on its own.

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [41] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: The paper introduces the Semantic Grounding Index (SGI), a simple geometric measure in embedding space to detect when RAG systems hallucinate by seeing whether model responses align more with the question than with the retrieved context.


<details>
  <summary>Details</summary>
Motivation: RAG systems still hallucinate, and existing detection methods can be complex or expensive. The authors want a theoretically grounded, embedding‑based signal that is cheap to compute and can flag responses that may need verification, while clarifying what embedding geometry reveals: topical engagement vs factual correctness.

Method: They define SGI as the ratio of angular distances (in embedding space on the unit hypersphere) between the response and the question versus the response and the retrieved context. They analyze SGI across multiple embedding models on hallucination datasets, derive theoretical properties from spherical geometry (triangle inequality) regarding how SGI’s discriminative power scales with question–context angle, and perform subgroup and calibration analyses. They also test on TruthfulQA to see whether SGI correlates with factual accuracy rather than grounding.

Result: On the HaluEval hallucination dataset, SGI strongly separates hallucinated from grounded responses, with large effect sizes (Cohen’s d ≈ 0.9–1.3) across five embedding models and high cross‑model correlation (r=0.85). Discriminative power increases as the angular separation between question and context grows, confirmed empirically with higher effect sizes and AUC in the high‑separation regime. SGI works particularly well for long answers and short questions and is robust to context length. Calibration analysis finds low ECE (0.10), suggesting SGI scores meaningfully approximate probabilities. However, on TruthfulQA SGI performs at chance (AUC≈0.48), showing it does not track factual accuracy directly.

Conclusion: Hallucinated RAG responses tend to stay geometrically close to the question embedding instead of moving toward the context embedding, a behavior termed semantic laziness. SGI captures this pattern as a cheap, theoretically justified metric that detects when answers are insufficiently grounded in retrieved evidence and thus should be sent for further verification. At the same time, SGI measures grounding/topical engagement rather than truth, so it should complement, not replace, accuracy‑focused checks in production systems.

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [42] [EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery](https://arxiv.org/abs/2512.13857)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: EvoLattice is a framework that encodes a whole population of program or agent candidates in a single DAG of reusable alternatives, enabling combinatorial search, fine-grained evaluation, and stable LLM-guided evolution.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based evolutionary methods mutate one candidate at a time via overwrites, which discards promising variants, allows destructive edits, and operates in a fragile search space where structural errors are common. There is a need for a representation that preserves useful components, supports large-scale exploration, and decouples structural correctness from the LLM.

Method: Represent all candidates as paths through a single directed acyclic graph (EvoLattice). Each node holds multiple alternative code fragments or behaviors. Evaluation is done at the alternative level by aggregating scores from all candidates (paths) that use that alternative. These statistics then guide LLM-driven mutations, recombinations, and pruning of alternatives. A deterministic self-repair mechanism maintains acyclicity and consistent dependencies, ensuring structural correctness without relying on the LLM. The same mechanism applies to agents by treating alternatives as prompt fragments or sub-agent behaviors.

Result: In program synthesis tasks, including proxy tasks and optimizer meta-learning, EvoLattice achieves more stable evolutionary dynamics, higher expressivity, and stronger performance improvements over time compared to prior overwrite-based LLM evolution methods. The internal representation effectively supports a large combinatorial search over candidates without structural duplication.

Conclusion: EvoLattice offers a robust and expressive way to evolve programs and multi-agent systems with LLMs by storing a population within a single DAG of alternatives, enabling quality-diversity-like search and fine-grained feedback while guaranteeing structural correctness. Its design leads to emergent quality-diversity behavior without needing an explicit archive, making it a promising direction for scalable LLM-guided evolution.

Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.

</details>


### [43] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: The paper proposes MURIM, a multi-dimensional reputation-based incentive mechanism for federated learning that improves fairness, privacy, and robustness against adversarial attacks while maintaining model performance in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Federated Learning allows model training without sharing raw data, but it suffers from weak client incentives, privacy risks, and resource constraints. Existing mechanisms often fail to comprehensively capture client reliability or to balance fairness, privacy, and robustness, leaving the system vulnerable to malicious or low-quality participants and discouraging honest clients from contributing high-quality data and resources.

Method: The authors design MURIM, a multi-dimensional reputation-based incentive mechanism that evaluates each client using several factors: contribution to the global model, training latency, reputation (capturing historical reliability and behavior), privacy preferences, and resource capacity. A reliability verification module validates client updates to detect malicious or unreliable behavior, and incentives are allocated based on this multi-dimensional assessment to reward reliable, high-quality, and fair participation. The mechanism is empirically tested on MNIST, FMNIST, and ADULT Income datasets under various attack scenarios, such as poisoning and noisy-gradient attacks.

Result: Experimental results show that MURIM improves fairness metrics by up to 18%, reduces privacy attack success rates by 5–9%, and increases robustness against poisoning and noisy-gradient attacks by up to 85% compared with state-of-the-art baseline methods, while maintaining stable model convergence in heterogeneous, dynamic federated environments.

Conclusion: MURIM effectively integrates incentive design with reliability, privacy, and resource-awareness in federated learning. By using a multi-dimensional reputation model and reliability verification, it mitigates adversarial threats, encourages fair and truthful client behavior, protects privacy better than existing methods, and sustains robust global model convergence across diverse and dynamic FL scenarios.

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [44] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: Benchmarking frontier large language models on graduate-level randomized algorithms proofs, showing strong but imperfect performance and substantial variance across models.


<details>
  <summary>Details</summary>
Motivation: LLMs are starting to contribute to real mathematical discovery, but their actual baseline capability on rigorous, canonical graduate-level mathematics is unclear. A focused, systematic evaluation on a well-known textbook can clarify how reliable they are for formal reasoning and pedagogy.

Method: Evaluate four frontier LLMs (GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, Grok-4) on lemmas and exercises from Motwani & Raghavan’s Randomized Algorithms. Prompt models to produce formal LaTeX proofs, then quantitatively score correctness and qualitatively analyze aspects like conciseness, hallucinations, and logical structure. Release code and model outputs publicly.

Result: Top models (Gemini and Claude) solve about two-thirds of the proof tasks correctly (~66%), showing solid command of probabilistic method and formal logic. Other models perform substantially worse (~40%) and show more inconsistency. Qualitative analysis reveals clear style and reliability differences in proof quality across models.

Conclusion: Frontier LLMs have reached a level where they can usefully support graduate-level learning and formalization in randomized algorithms, but their mathematical reasoning remains unreliable and heterogeneous across models. Careful model choice and verification are still required for rigorous derivations; the released benchmark and outputs support further study and improvement.

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [45] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: The paper introduces ReflCtrl, a method to control and reduce unnecessary self-reflection steps in chain-of-thought reasoning of large language models, cutting reasoning tokens while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Self-reflection in chain-of-thought LLMs improves reasoning but increases inference cost due to longer outputs. There is a need to understand and control when models reflect to make reasoning more efficient without harming accuracy.

Method: The authors treat self-reflection as a distinct behavior in the model’s internal representations. They segment reasoning into steps, identify which steps are reflective, and then use representation engineering to find a latent “reflection direction” associated with those steps. They then design a stepwise steering procedure (ReflCtrl) that manipulates this direction during inference to increase or decrease reflection frequency.

Result: Experiments show that many reflection steps are redundant, particularly for stronger models. By steering along the learned reflection direction, ReflCtrl can reduce the number of reasoning tokens by up to 33.6% while maintaining task performance, demonstrating controllable trade-offs between reflection frequency and efficiency.

Conclusion: Self-reflection behavior in CoT LLMs is strongly tied to a latent representation that is correlated with an internal uncertainty signal. By identifying and steering this reflection direction, models can be made more efficient without sacrificing accuracy, suggesting that reflection is at least partly governed by internal uncertainty and can be controlled at the representation level.

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [46] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: They propose DTop-p MoE, a dynamic Top-p routing method for Mixture-of-Experts that controls sparsity while improving performance over Top-k and fixed Top-p.


<details>
  <summary>Details</summary>
Motivation: Standard MoE routing (Top-k) uses a fixed number of experts per token, ignoring that some tokens are harder and may need more experts, while others are easy and need fewer. Top-p routing is more flexible but current implementations use a fixed global probability threshold, leading to uncontrollable computational cost and sensitivity to the chosen threshold. A method is needed that keeps strict control over sparsity/compute while dynamically adapting how many experts are activated per token and per layer.

Method: They design DTop-p MoE, a dynamic Top-p routing mechanism with two main components: (1) a Proportional-Integral (PI) controller that dynamically adjusts the probability threshold during training/inference so that the average number of activated experts matches a target sparsity level; this solves the problem of optimizing a non-differentiable threshold; (2) a dynamic routing normalization that rescales routing logits per layer so that each layer can learn its own expert selection pattern while still using a single global probability threshold. The method is evaluated in large language models and diffusion transformers, and analyzed for scaling with respect to expert granularity, capacity, model size, and dataset size.

Result: Across experiments on LLMs and Diffusion Transformers, DTop-p MoE consistently outperforms conventional Top-k routing and fixed-threshold Top-p routing in terms of model quality under comparable compute. It maintains accurate control over the average number of activated experts (i.e., sparsity) while better adapting the number of experts used per token and per layer. Scaling studies show robust performance when increasing number of experts, expert capacity, model size, and dataset size.

Conclusion: Dynamic, controller-based Top-p routing (DTop-p) is an effective way to control sparsity and computational budget in MoE models while allowing token- and layer-wise adaptivity in expert activation. The PI-controller-driven threshold and routing normalization together provide stable, fine-grained control of expert usage and deliver better performance and scaling than traditional Top-k or fixed-threshold Top-p routing, making DTop-p a strong and practical choice for large-scale MoE pre-training.

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [47] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: The paper proposes natural-language-based world models for mobile GUI agents, introduces a benchmark and a large dataset (MobileWorldBench and MobileWorld), and shows these semantic models improve task success.


<details>
  <summary>Details</summary>
Motivation: Pixel-space world models struggle in GUI settings because accurately predicting future complex visual states is difficult and inefficient. There is a need for more practical, semantically rich world models that better support planning for mobile GUI agents.

Method: The authors reformulate world modeling for GUI agents by representing state transitions in natural language using vision-language models. They (1) design MobileWorldBench to assess VLMs as GUI world models, (2) construct MobileWorld, a 1.4M-sample dataset to train and improve such models, and (3) build a planning framework that integrates these VLM-based semantic world models into mobile agents.

Result: Training on MobileWorld substantially improves the world modeling capabilities of VLMs on the new benchmark. When incorporated into the agents’ planners, these enhanced semantic world models lead to higher task success rates for mobile GUI agents.

Conclusion: Natural language (semantic) world models, powered by VLMs and trained on large-scale GUI interaction data, are an effective alternative to pixel-based models for mobile GUI agents, yielding better planning and higher task performance. The released benchmark, dataset, and framework aim to facilitate further research in this direction.

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [48] [Evaluating Small Language Models for Agentic On-Farm Decision Support Systems](https://arxiv.org/abs/2512.14043)
*Enhong Liu,Haiyu Yang,Miel Hostens*

Main category: cs.AI

TL;DR: The paper benchmarks small, locally runnable language models as decision-support engines for dairy farming, finding that Qwen-4B performs best overall but still needs domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Large language models could aid dairy farmers and scholars with decision support and knowledge access, but their heavy computation and cloud dependence make them impractical and raise privacy concerns for on-farm use. There is a need for lightweight, open-source models that can run on typical farm hardware while still handling complex, dairy-related tasks.

Method: The authors benchmarked 20 open-source small language models from HuggingFace under realistic farm hardware constraints. They embedded these models into an agentic AI system with five specialized agents: literature search, web search, SQL database interaction, NoSQL (PySpark) database interaction, and predictive-model-based graph generation. Evaluation occurred in two phases: (1) an initial screening using five dairy-related test questions to eliminate models that failed basic instruction-following or could not operate reliably under compute limits; (2) a detailed evaluation of the remaining models using 30 questions, covering the five task categories plus an additional category on integrity and misconduct.

Result: In the benchmark, several SLMs were able to function within the specified compute constraints, but performance varied by task. Among the candidates, Qwen-4B performed best across most task categories, demonstrating strong capability as the core engine for the multi-agent system. However, it showed unstable performance in NoSQL database interactions via PySpark. Other models generally lagged in reliability, task-following, or task-specific competence.

Conclusion: Small language models can serve as feasible engines for dairy decision-support tools that prioritize on-farm deployment, privacy, and computational efficiency. Qwen-4B is currently the strongest general candidate among the tested models, but its limitations, especially in NoSQL/PySpark interactions and dairy-specific reasoning, indicate that domain-focused fine-tuning and further system refinement are necessary before widespread practical adoption.

Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.

</details>


### [49] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: Proposes RoutingGen, a difficulty-aware routing framework for code generation that switches between simple few-shot prompting and a new Intention Chain-of-Thought (ICoT) reasoning strategy to improve performance and reduce token usage.


<details>
  <summary>Details</summary>
Motivation: LLMs generate code well, but standard chain-of-thought prompting is applied uniformly, causing overthinking on easy tasks and lacking explicit modeling of algorithmic intention (e.g., core logic, efficiency). This leads models to focus on superficial code patterns instead of the global problem objective. The authors are motivated to create a more efficient and intention-aware reasoning framework that only uses structured reasoning when it is actually needed.

Method: Introduce RoutingGen, a difficulty-aware routing framework for code generation. It dynamically selects prompting strategies based on task difficulty: for simple problems, it uses lightweight few-shot prompting; for complex ones, it activates Intention Chain-of-Thought (ICoT), a structured reasoning prompt that explicitly guides the model to infer and articulate task intention, including core algorithm design and time complexity. They evaluate RoutingGen using three different LLMs on six standard code generation benchmarks and compare ICoT against multiple prompting baselines on hard tasks.

Result: RoutingGen achieves state-of-the-art performance on most evaluated code generation benchmarks while cutting total token usage by an average of 46.37% across settings, indicating both effectiveness and efficiency. Additionally, the ICoT prompting strategy alone outperforms six existing prompting baselines on challenging code generation benchmarks, demonstrating its superior ability to guide complex reasoning about algorithmic intention.

Conclusion: Difficulty-aware routing between simple prompting and structured reasoning can substantially improve code generation, resolving the trade-off between performance and efficiency. Explicitly modeling task intention via Intention Chain-of-Thought helps LLMs focus on core algorithmic logic and complexity rather than surface patterns, yielding state-of-the-art results with significantly reduced token consumption.

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [50] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: They introduce OpenDataArena, a platform to systematically benchmark and analyze post-training datasets for LLMs, making data evaluation transparent, multi-dimensional, and reproducible.


<details>
  <summary>Details</summary>
Motivation: LLMs depend heavily on post-training data, but current datasets are opaque, poorly documented, and rarely evaluated systematically. This lack of transparency harms reproducibility and understanding of how data affects model behavior. The authors want to create a standardized, open way to measure and compare the value of different post-training datasets.

Method: They build OpenDataArena (ODA), an open platform with: (i) a unified training and evaluation pipeline for fair comparison across models and domains, (ii) a multi-dimensional scoring framework assessing data quality on many axes, (iii) an interactive lineage explorer to visualize dataset genealogy and component sources, and (iv) an open-source toolkit for training, evaluation, and scoring. They run large-scale experiments using this pipeline on 120+ datasets, 22 benchmarks, and hundreds of training runs.

Result: Using ODA, they discover trade-offs between data complexity and downstream task performance, detect redundancy in standard benchmarks via lineage tracing, and map genealogical relationships among datasets. They generate a large, open repository of scores, tools, and configs documenting these findings.

Conclusion: OpenDataArena enables a shift toward principled, data-centric AI by making post-training datasets transparent, measurable, and comparable. It supports rigorous study of how data composition and mixing laws influence foundation model performance, moving beyond ad-hoc data curation toward a systematic science of data for LLMs.

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [51] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR is a reinforcement-learning-based speculative sampling method that dynamically decides how many draft tokens to generate, significantly speeding up LLM inference.


<details>
  <summary>Details</summary>
Motivation: Large Language Model inference is slow and costly, and existing speculative sampling methods fix the number of draft model calls as a hyperparameter, which can be inefficient and inflexible across inputs, models, and contexts. There is a need for a method that adapts the speculative sampling process on-the-fly to reduce redundant computation while maintaining quality.

Method: RADAR models the draft tree generation in speculative sampling as a Markov Decision Process, where actions correspond to decisions about how to expand or stop draft token generation. It uses offline reinforcement learning to train a prediction policy that, at inference time, dynamically decides whether and how much to query the draft model, effectively controlling the structure and depth of the draft tree in real time.

Result: Across three different LLMs and four benchmark tasks, RADAR delivers a 3.17x to 4.82x speedup compared to standard auto-regressive decoding, indicating substantial efficiency gains without changing the base model architecture.

Conclusion: By casting speculative sampling as an MDP and optimizing it via offline RL, RADAR introduces a flexible, dynamic mechanism to control draft token generation, successfully reducing redundant computation and significantly accelerating LLM inference. This demonstrates that RL-based control can meaningfully improve the efficiency of decoding for modern LLMs.

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [52] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: The paper proposes a structured, component-based framework to automatically design multi-agent systems and shows it beats LLM-based free-form code search on most benchmarks while being cheaper and more interpretable.


<details>
  <summary>Details</summary>
Motivation: Automatic design of multi-agent systems is important for advancing agentic AI, but existing methods often rely on large language models to freely generate code, which is expensive, hard to control, and yields complex, less interpretable systems. The authors seek a more structured, cost-effective, and interpretable approach that still achieves strong performance.

Method: Instead of letting an LLM freely generate candidate agent systems in code, the authors define a fixed set of simple, composable components that can be combined to form multi-agent systems. They then conduct a structured search over this component space to design systems for tasks in mathematics and question answering, comparing against prior LLM-based free-form search methods.

Result: Across four out of five benchmarks in two domains (math problem solving and question answering), the proposed structured component-based search framework outperforms previous LLM-based free-form code search approaches. It also reduces computational cost of the search procedure and produces agent systems that are more modular and easier to interpret.

Conclusion: A constrained, component-based search space is sufficient—and often superior—to LLM-based free-form code generation for automatically designing multi-agent systems, yielding better performance on most benchmarks, lower search cost, and simpler, more interpretable agent architectures.

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [53] [HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control](https://arxiv.org/abs/2512.14106)
*Ijaz Ul Haq,Byung Suk Lee,Julia N. Perdrial,David Baude*

Main category: cs.AI

TL;DR: HydroGEM is a foundation model for automated, continental-scale streamflow quality control that detects and reconstructs anomalies in real-time hydrological data.


<details>
  <summary>Details</summary>
Motivation: Maintaining data quality in large, real-time streamflow monitoring networks is labor-intensive because thousands of remote sensors can produce faulty or anomalous readings that require expert review. Existing automated quality-control methods are limited in accuracy and generalization, especially across many stations and varying hydrological regimes. The authors aim to develop a scalable, accurate, and generalizable model to assist human experts in streamflow data quality control.

Method: The authors develop HydroGEM, a foundation model trained in two stages. First, they perform self-supervised pretraining on 6.03 million time-series sequences from 3,724 USGS streamflow stations to learn general hydrological representations. Second, they fine-tune the model using synthetic anomalies injected into the data to train for anomaly detection and reconstruction. The architecture is a hybrid of Temporal Convolutional Networks (TCNs) and Transformers with 14.2 million parameters, designed to capture both local temporal structure and long-range dependencies. Hierarchical normalization is used to handle discharge values that span six orders of magnitude. The model is evaluated on held-out stations with 18 expert-validated anomaly types and tested for zero-shot transfer to a separate national network in Canada.

Result: On held-out synthetic tests from 799 USGS stations with 18 expert-validated anomaly types, HydroGEM achieves an F1 score of 0.792 for anomaly detection and a 68.7% reduction in reconstruction error, representing a 36.3% improvement over existing methods. In zero-shot transfer evaluation on 100 Canadian stations, the model attains an F1 score of 0.586, outperforming all baseline methods and demonstrating cross-national generalization. The model shows robust detection performance across different correction magnitudes and behaves consistently with operational seasonal patterns.

Conclusion: HydroGEM effectively serves as a continental-scale foundation model for streamflow data quality control, substantially outperforming existing automated approaches in both detection and reconstruction tasks. Its strong performance on zero-shot transfer suggests that the learned hydrological representations generalize across regions and countries. The model is suited for human-in-the-loop workflows, where its anomaly flags and reconstructions act as decision-support tools for experts rather than fully automated corrections.

Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.

</details>


### [54] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: The paper proposes a hybrid Liquid Neural Network (LNN) + XGBoost model to mitigate the bullwhip effect and improve profitability in multi-tier supply chains by efficiently handling complex continuous time-series data.


<details>
  <summary>Details</summary>
Motivation: Supply chain management suffers from challenges like demand fluctuations and the bullwhip effect, which traditional methods and current LLMs cannot model well, especially on benchmarks like the Vending Machine Test. Existing ML methods (LSTM, XGBoost alone) either struggle with capturing dynamic temporal behavior or are computationally inefficient. There is a gap in leveraging Liquid Neural Networks—successful in robotics—for SCM problems.

Method: Design a hybrid architecture where Liquid Neural Networks are used as a dynamic feature extractor over continuous time-series data in multi-tier supply chains, and XGBoost is applied on these features as a global optimizer. The model is evaluated against traditional SCM methods and standard ML baselines (e.g., LSTM, standalone XGBoost) on benchmarks such as the Vending Machine Test or similar SCM simulations.

Result: The hybrid LNN+XGBoost model achieves lower bullwhip effect metrics and higher profitability compared with traditional methods and state-of-the-art ML baselines, while also offering improved computational efficiency or better tradeoffs between accuracy and efficiency.

Conclusion: Combining LNNs with XGBoost provides an efficient and adaptable solution for complex SCM time-series tasks, effectively reducing the bullwhip effect and improving profitability. This approach demonstrates that LNNs, previously underused in SCM, can be powerful when integrated with gradient-boosted trees, filling an important gap in intelligent supply chain management research.

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [55] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: A tool-augmented medical MLLM framework, Ophiuchus, that dynamically focuses on image regions via a three-stage training strategy to improve grounded reasoning and diagnosis, outperforming SOTA on multiple medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning-based medical multimodal LLMs can generate stepwise reasoning text but are weak at handling complex cases that require iterative, fine-grained inspection and grounding in medical images. They also hit ceilings when relying solely on external specialized tools, limiting higher-level reasoning and diagnostic accuracy. There is a need for an AI agent that can flexibly decide when and where to look in images, integrate tools with its own perception, and truly reason with visual evidence.

Method: They propose Ophiuchus, a framework that augments an MLLM with tools for visual probing and integrates these tools into multimodal chains of thought. The system can: (i) decide if more visual evidence is needed, (ii) choose where to focus within the image via tools that crop or analyze subregions, and (iii) incorporate the retrieved sub-image content into its reasoning steps. The training follows three stages: (1) cold-start training on data with tool-integrated reasoning to learn when/how to call tools and adapt to inspecting key regions; (2) self-reflection fine-tuning, where the model learns to review and refine its reasoning by revisiting tool outputs; (3) Agentic Tool Reinforcement Learning, which uses task-specific rewards to optimize tool usage and reasoning patterns toward expert-level diagnostic decisions.

Result: Ophiuchus consistently outperforms both open-source and closed-source state-of-the-art baselines on a variety of medical benchmarks. These include visual question answering (VQA), lesion or abnormality detection, and reasoning-centric segmentation tasks, demonstrating superior grounded reasoning and diagnostic performance when compared to prior methods and pure tool-based systems.

Conclusion: Integrating an MLLM’s own perception and grounding abilities with external tools, and training it through a staged process of tool-use learning, self-reflection, and reinforcement learning, yields a medical AI agent that can more effectively "think with images." Ophiuchus’s success across multiple benchmarks suggests that tool-integrated, agentic reasoning is a promising direction for building more reliable and interpretable medical imaging assistants; the release of datasets, code, and models will facilitate further research.

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [56] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: The paper investigates using large language models to automatically georeference complex, narrative locality descriptions in biodiversity records, achieving higher accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional georeferencing relies on gazetteers or geo-indicative language models, which struggle with narrative, relational location descriptions common in historical biodiversity records. Manual georeferencing is accurate but very labor-intensive, and biodiversity research critically depends on precise location data, creating a need for scalable, automated solutions.

Method: The authors design prompting strategies for large language models and then fine-tune an LLM with QLoRA using biodiversity locality datasets spanning multiple regions and languages. They evaluate performance by measuring how often the predicted coordinates fall within specific distance thresholds (e.g., 1 km, 10 km) of reference locations and compare against existing baseline methods, under a fixed training data budget.

Result: The fine-tuned LLM outperforms baseline georeferencing approaches, with an average of 65% of records across datasets being georeferenced within 10 km of the ground truth. In the best-performing region (New York state), the model achieves 85% of records within 10 km and 67% within 1 km, particularly excelling on long, complex locality narratives.

Conclusion: LLMs, especially when carefully prompted and fine-tuned with QLoRA on domain-specific biodiversity data, can effectively georeference complex textual locality descriptions. Their strong performance, notably on intricate narratives, demonstrates substantial potential to reduce manual effort and improve scalability in georeferencing biodiversity collections.

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [57] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: They propose a multi-agent, language-model-based system for Lean4 that recursively decomposes difficult theorems into simpler ones, greatly boosting automated proof success on miniF2F.


<details>
  <summary>Details</summary>
Motivation: Automated theorem proving in interactive proof assistants like Lean is a long-standing AI challenge: existing systems often fail on harder problems and lack flexible decomposition and coordination among different components (autoformalization, proof search, etc.). The authors aim to build a more powerful, modular system that can tackle complex theorems by breaking them down and leveraging specialized language models.

Method: They design specialized language models for Lean4 proof generation and wrap them in a multi-agent architecture. The agents handle: (1) autoformalization of informal statements (when needed), (2) proof generation for Lean4, (3) decomposition of hard theorems into simpler entailing propositions, and (4) recursive proof or further decomposition of those subproblems. A core engineering contribution is extending the Kimina Lean Server with AST parsing so the system can analyze Lean syntax trees and automatically perform recursive decomposition. The whole pipeline is packaged as the Python library "goedels-poetry" and an open-source repository that allows swapping in other LMs and adding custom agents or logic.

Result: On the miniF2F benchmark, their Lean4 proof generator without decomposition achieves a 90.4% pass rate. Adding the recursive decomposition machinery further increases the pass rate beyond this already strong baseline; exact numbers are not in the abstract but the improvement is described as significant. They also deliver an open-source, extensible implementation and a PyPI package for easy use and modification.

Conclusion: Combining specialized Lean4 language models with a multi-agent, recursive decomposition framework substantially advances automated theorem proving performance on miniF2F. AST-based integration with the Lean server enables fine-grained decomposition, and the released tools (PyPI package and GitHub project) provide a practical platform for research and application, including experiments with alternative language models and custom extensions.

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [58] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: The paper evaluates how well Large Language Models (LLMs) can build a Parkinson’s Disease monitoring and alerting ontology, comparing fully automated prompting approaches to hybrid human-LLM engineering methods.


<details>
  <summary>Details</summary>
Motivation: Ontology engineering in complex medical domains like Parkinson’s Disease is time-consuming and requires specialized expertise. With the emergence of LLMs, there is interest in knowing whether they can autonomously construct high-quality ontologies or at least significantly accelerate and support experts, thereby reducing cost and effort while maintaining accuracy and coverage.

Method: The authors test four approaches: (1) One Shot (OS) prompting, where an LLM is asked once to generate a PD monitoring and alerting ontology; (2) Chain of Thought (CoT) prompting, where reasoning steps are elicited to structure ontology creation; (3) X-HCOME, a hybrid engineering method that integrates human expert input with LLM-generated content; and (4) SimX-HCOME+, an extended hybrid method that adds continuous human supervision and iterative refinement cycles. They compare the completeness and accuracy of the resulting ontologies across these methods.

Result: Pure LLM-based approaches (OS and CoT) can generate an initial ontology structure for PD monitoring and alerting but these ontologies are incomplete and require major human corrections. The X-HCOME hybrid approach substantially improves ontology comprehensiveness, producing outputs close to those made by human experts. SimX-HCOME+, with stronger and ongoing human involvement, yields the most comprehensive and accurate ontologies among the tested methods.

Conclusion: LLMs alone are not yet sufficient to create fully comprehensive, high-quality ontologies in complex medical domains, but they are valuable tools when integrated into structured human-LLM workflows. Hybrid methodologies like X-HCOME and SimX-HCOME+ markedly enhance ontology quality, underscoring the need for sustained expert involvement. The work points toward future research on specialized GPT models tailored for ontology construction to further improve automation and reliability.

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [59] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: The paper proposes MeG, a dynamic-weight-based method to perform large-scale knowledge editing in LLMs using a single attached neuron whose weights are generated by a diffusion model, improving reliability, generality, and especially locality over prior KE methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing methods for LLMs struggle when many edits are required at once: they either become expensive, or they degrade model behavior, particularly in terms of reliability (correctly applying edits), generality (working across contexts), and locality (not harming unrelated knowledge). There is a need for a scalable, low-cost way to incorporate many knowledge edits into an already-trained LLM without retraining, while preserving model quality on unaffected knowledge.

Method: The authors introduce MeG (Massive editing based on dynamic weight Generation). They augment specific layers of an LLM with a single dynamic weight neuron and use a diffusion model to generate the neuron’s weights conditioned on the input query that targets the desired knowledge. At inference time, given an input that requires edited knowledge, the diffusion model outputs appropriate weights for this neuron, effectively injecting the edit into the model dynamically, rather than permanently changing base parameters.

Result: Empirical evaluations show that MeG substantially boosts performance in large-scale knowledge editing tasks across the three standard metrics: Reliability, Generality, and Locality. In particular, MeG yields a large absolute improvement in the Locality metric, indicating that it better preserves unrelated knowledge and reduces negative side effects compared with baseline KE approaches.

Conclusion: Attaching a dynamically reweighted neuron whose weights are generated by a diffusion model enables efficient, large-scale knowledge editing in LLMs. MeG achieves more reliable and general edits while maintaining stronger locality than existing KE methods, suggesting dynamic, query-conditioned parameter generation is a promising direction for scalable knowledge editing in large language models.

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [60] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: PortAgent is an LLM-based agentic system that automates transferring vehicle dispatching systems between container terminals with minimal human expertise, data, and deployment time.


<details>
  <summary>Details</summary>
Motivation: Vehicle Dispatching Systems are essential for automated container terminals, but current systems transfer poorly between terminals because they require intensive expert knowledge, terminal-specific data, and slow manual deployment. The authors want a more scalable, automated way to adapt VDSs to new terminals.

Method: The paper designs PortAgent, an LLM-driven multi-agent framework called a Virtual Expert Team (VET) composed of four specialized virtual experts: Knowledge Retriever, Modeler, Coder, and Debugger. Using few-shot learning with examples retrieved via Retrieval-Augmented Generation, these experts learn VDS-domain knowledge and collaborate in an automatic design workflow. A self-correction loop inspired by the Reflexion framework lets the system iteratively refine the VDS without human intervention.

Result: PortAgent can automatically generate and adapt VDSs for different terminals using only a small set of VDS examples and without direct involvement of port specialists, significantly reducing data requirements and deployment time compared to traditional methods. (Specific numerical results would be in the full paper.)

Conclusion: An LLM-centric, agent-based approach can effectively automate the transfer of VDSs across automated container terminals, alleviating reliance on human experts and extensive data while enabling faster deployment. This suggests LLM-driven virtual expert teams are a promising paradigm for operational technology in logistics terminals.

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [61] [Seismology modeling agent: A smart assistant for geophysical researchers](https://arxiv.org/abs/2512.14429)
*Yukun Ren,Siwei Yu,Kai Chen,Jianwei Ma*

Main category: cs.AI

TL;DR: They build an LLM-powered, intent-driven workflow around SPECFEM that turns complex, file-based seismic simulations into conversational, toolified, and partially automated processes, while preserving scientific control and matching baseline accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional use of the seismic wave simulation package SPECFEM requires steep expertise: users must edit many configuration files, manage command-line tools, and handle error-prone low-level operations. This high barrier limits accessibility, slows experimentation, and hampers reproducibility. With the rise of LLMs and tool-use frameworks, there is an opportunity to rethink the workflow as a higher-level, interactive, and more automated process.

Method: They design and implement an MCP (Model Context Protocol) server suite for multiple SPECFEM variants (2D, 3D Cartesian, 3D Globe). The simulation pipeline is decomposed into modular tools (e.g., parameter setup, meshing, partitioning, running solvers, and post-processing/visualization) that can be orchestrated by LLM agents. The system supports two modes: fully autonomous execution by agents and human-in-the-loop interaction where users guide and refine simulation strategies via conversational intents. Case studies are used to validate the workflow against standard SPECFEM baselines.

Result: The MCP-based workflow runs SPECFEM simulations end-to-end in both autonomous and interactive modes. Across several case studies, the produced simulations are high fidelity and agree with results from the traditional, manual SPECFEM workflow. Users can initiate and control complex simulations via conversation instead of manual file editing and shell commands, while the agent handles orchestration of the decomposed tools.

Conclusion: By introducing the first MCP server suite for SPECFEM and demonstrating accurate, agent-orchestrated simulations, the paper shows that seismic wave modeling can be transformed from a file-driven, expert-only process into an intent-driven, conversational workflow. This reduces the learning curve, improves reproducibility, and points toward a broader paradigm of AI-assisted, automated scientific computing in computational seismology and geophysics. The open-source release makes it possible for the community to adopt and extend this approach.

Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.

</details>


### [62] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: They propose Context-Picker, a reinforcement-learning-based method to select a minimal but sufficient set of passages for long-context QA, improving accuracy while using shorter context.


<details>
  <summary>Details</summary>
Motivation: In long-context QA, fixed Top-K retrieval or single-stage reranking struggle to decide how many passages to include. Too little context misses evidence; too much adds noise and hurts answers, especially for factoid questions that only need a few key evidence snippets. The authors want a principled way to select a minimal sufficient subset of context rather than just ranking by similarity.

Method: They formulate context selection as a decision-making problem and introduce Context-Picker, a reasoning-aware framework trained with a two-stage reinforcement learning schedule. First, a recall-oriented stage focuses on covering full reasoning chains, then a precision-oriented stage prunes redundant passages to obtain a compact evidence set. To combat reward sparsity, they build an offline evidence distillation pipeline that identifies minimal sufficient sets of passages using a Leave-One-Out procedure, which then provides dense, task-aligned supervision. They also use redundancy-aware reward shaping and a rationale-guided format.

Result: On five long-context and multi-hop QA benchmarks, Context-Picker achieves higher answer accuracy than strong RAG baselines while keeping context length comparable or shorter. Ablations show that each key design—coarse-to-fine optimization, redundancy-aware rewards, and rationale-guided formatting—contributes to the performance gains.

Conclusion: Reasoning-aware, RL-based minimal subset selection of passages is more effective than traditional similarity-based ranking or fixed Top-K retrieval for long-context QA. By first maximizing recall of reasoning chains and then aggressively pruning redundancy, Context-Picker can find compact evidence sets that improve accuracy and efficiency. The proposed offline LOO-based evidence distillation and reward shaping are crucial components enabling effective training.

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [63] [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474)
*Annu Rana,Gaurav Kumar*

Main category: cs.AI

TL;DR: The paper introduces Model-First Reasoning (MFR), where an LLM first builds an explicit planning model (states, actions, constraints) before solving tasks, leading to fewer constraint violations and better plans than Chain-of-Thought and ReAct.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly on complex multi-step planning, often violating constraints and producing inconsistent solutions because they implicitly track state and lack explicit problem representations. The authors aim to understand and fix these failures by borrowing ideas from classical AI planning with explicit models.

Method: They propose Model-First Reasoning (MFR), a two-phase approach: (1) modeling phase, where the LLM is prompted to define entities, state variables, actions, and constraints as an explicit model of the problem; (2) planning phase, where the LLM uses this model to generate a solution plan. They compare MFR with Chain-of-Thought and ReAct across several planning domains and perform ablations to test the role of the modeling phase.

Result: Across domains such as medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR produces plans with fewer constraint violations and higher overall quality than Chain-of-Thought and ReAct. Ablation studies indicate that removing or weakening the modeling phase significantly reduces these gains, underscoring its importance.

Conclusion: The study concludes that many LLM planning errors are due more to missing or weak problem representations than to inherent reasoning limits. Explicit model construction is identified as a key ingredient for robust, interpretable LLM-based planning agents, and the work is made reproducible via released prompts, evaluation protocols, and datasets.

Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

</details>


### [64] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: Introduces GreedyLR, an adaptive learning-rate scheduler driven by current loss, that outperforms common schedulers and has convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Most training pipelines still rely on simple, hand-designed LR schedules (e.g., Cosine or exponential decay) despite recent optimizer advances, and there is a need for more adaptive, loss-aware schedulers with both empirical and theoretical support.

Method: Propose GreedyLR, which adaptively adjusts the learning rate based on the observed loss during training. Evaluate it on NLP, CV, and LLM tasks (up to 7B parameters) for both fine-tuning and pre-training. Provide a theoretical analysis, including convergence proof and derivation of an optimal scaling factor F that maximizes convergence rate, and test robustness under noisy loss landscapes.

Result: Across a variety of tasks and model sizes, GreedyLR outperforms several state-of-the-art LR schedulers in accuracy, training speed, and convergence behavior, and remains robust under noisy training conditions.

Conclusion: GreedyLR is a simple, computationally efficient, and theoretically grounded scheduler that can serve as a strong default learning-rate scheduler for training large models across domains.

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [65] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: The paper analyzes why universal transformers work well on reasoning tasks, finding that recurrent inductive bias and strong nonlinearities matter more than detailed architecture, and proposes an improved Universal Reasoning Model that sets new state-of-the-art results on ARC-AGI.


<details>
  <summary>Details</summary>
Motivation: Universal transformers perform well on complex reasoning benchmarks like ARC-AGI and Sudoku, but the exact reasons for their performance advantages remain unclear. The authors want to dissect what aspects of UTs are actually responsible for better reasoning, to avoid over-attributing gains to sophisticated architectural tweaks.

Method: The authors systematically compare and analyze variants of universal transformers on ARC-AGI to isolate which components drive performance, focusing on recurrent inductive bias, nonlinearity strength, and architectural design choices. Based on the insights, they design the Universal Reasoning Model (URM), which modifies UTs by adding short convolutions and using truncated backpropagation through time to improve training efficiency and performance.

Result: The analysis shows that the main sources of improvement on ARC-AGI come from the recurrent inductive bias and strong nonlinear modules in the transformer, not from complex architectural enhancements. The proposed URM, which incorporates short convolutions and truncated backpropagation, achieves state-of-the-art performance of 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2.

Conclusion: Carefully tuned recurrence and nonlinearity are key to reasoning performance in universal transformers, while elaborate architectural design is less critical. With targeted modifications—short convolutions and truncated backprop—the Universal Reasoning Model significantly advances the state of the art on ARC-AGI, suggesting a simpler path to better reasoning models.

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>
