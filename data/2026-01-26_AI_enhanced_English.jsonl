{"id": "2601.16217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16217", "abs": "https://arxiv.org/abs/2601.16217", "authors": ["Qingyan Yang", "Tongxi Wang", "Yunsheng Luo"], "title": "ChiEngMixBench: Evaluating Large Language Models on Spontaneous and Natural Chinese-English Code-Mixed Generation", "comment": null, "summary": "Code-mixing is increasingly prevalent in interactions between humans and large language models, yet existing work often reduces it to a translation or convertibility problem, making it difficult to assess whether a model's switching behavior is context-appropriate and aligned with human conventions. We introduce ChiEngMixBench, the first benchmark designed to evaluate code-mixing ability in authentic community contexts, built upon a general construction pipeline that enables scalable dataset development across domains and bilingual pairs. ChiEngMixBench formulates code-mixing as a cognitive alignment problem, characterized by two complementary signals: Spontaneity and Naturalness. Empirical evaluation shows that our metrics can systematically distinguish code-mixing performance across models. Beyond benchmarking, we further uncover an implicitly emergent Terminology Layering Strategy, a phenomenon consistent with the Matrix Language Frame (MLF) theory, indicating structured cognitive alignment between multilingual large language models and human communication.", "AI": {"tldr": "Introduces ChiEngMixBench, a benchmark for evaluating how well language models perform Chinese-English code-mixing in realistic contexts, focusing on cognitive alignment rather than mere translation.", "motivation": "Existing work on code-mixing with large language models largely treats it as a translation or conversion task, which does not capture whether models switch languages in ways that are contextually appropriate or similar to how humans actually talk. There is a lack of benchmarks that evaluate code-mixing as a nuanced, human-aligned behavior in genuine community settings, especially for specific bilingual pairs like Chinese-English.", "method": "The authors build ChiEngMixBench: (1) They design a general, scalable pipeline to construct datasets for code-mixing evaluation across domains and bilingual language pairs, using authentic community contexts. (2) They formalize code-mixing as a cognitive alignment problem and define two metrics\u2014Spontaneity (how naturally and unforcedly switching occurs) and Naturalness (how human-like the resulting mixed utterances are). (3) They run empirical evaluations of multiple multilingual LLMs on this benchmark, measuring performance via these two signals and conducting analyses of emerging patterns in how models layer terminology across languages.", "result": "The proposed Spontaneity and Naturalness metrics reliably differentiate models\u2019 code-mixing capabilities, showing clear performance stratification across different LLMs. Analysis reveals that models exhibit a Terminology Layering Strategy, an implicitly emergent behavior where technical or specific terms tend to be anchored in one language while surrounding structure may use another, reflecting structured patterns of code-mixing rather than random switching.", "conclusion": "ChiEngMixBench provides the first systematic benchmark for assessing Chinese-English code-mixing in realistic settings and reframes code-mixing as a cognitive alignment issue instead of just translation. The benchmark and metrics not only distinguish model performance but also expose a Terminology Layering Strategy consistent with Matrix Language Frame theory, suggesting that multilingual LLMs can develop human-like, structured code-mixing behaviors and offering a foundation for future work on multilingual cognitive alignment."}}
{"id": "2601.16218", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16218", "abs": "https://arxiv.org/abs/2601.16218", "authors": ["Aleix Torres-Camps", "Nathaniel Mitrani Hadida", "V\u00edctor Conchello Vendrell", "\u00c0lex Batlle Casellas", "Arnau Padr\u00e9s Masdemont", "Jordi Ros-Giralt"], "title": "M3Kang: Evaluating Multilingual Multimodal Mathematical Reasoning in Vision-Language Models", "comment": "10 pages, 8 figures", "summary": "Despite state-of-the-art vision-language models (VLMs) have demonstrated strong reasoning capabilities, their performance in multilingual mathematical reasoning remains underexplored, particularly when compared to human performance. To bridge this gap, we introduce M3Kang, the first massively multilingual, multimodal mathematical reasoning dataset for VLMs. It is derived from the Kangaroo Math Competition, the world's largest mathematics contest, which annually engages over six million participants under the age of 18 across more than 90 countries. M3Kang includes 1,747 unique multiple-choice problems organized by grade-level difficulty, with translations into 108 culturally diverse languages, some of them including diagrams essential for solving them. Using this dataset, we conduct extensive benchmarking on both closed- and open-source SOTA models. We observe that, despite recent advances, models still struggle with basic math and diagram-based reasoning, with performance scaling with language presence and model size, but not with grade level. We also find that multilingual techniques can be effectively extended to the multimodal setting, resulting in significant improvements over baseline approaches. Our analysis also incorporates performance data from over 68,000 students, enabling direct comparison with human performance. We are open-sourcing M3Kang, including the English-only subset M2Kang, along with the framework and codebase used to construct the dataset.", "AI": {"tldr": "Introduces M3Kang, a large multilingual multimodal math reasoning dataset from the Kangaroo Math Competition, and benchmarks VLMs against it and human performance.", "motivation": "Vision-language models show strong reasoning in some settings, but their capabilities in multilingual mathematical reasoning, especially compared to humans, are not well understood due to lack of appropriate datasets and benchmarks.", "method": "Construct M3Kang from Kangaroo Math Competition problems: 1,747 unique multiple-choice items with grade-level difficulty, some with diagrams, translated into 108 languages. Benchmark a variety of closed- and open-source state-of-the-art VLMs on this dataset, analyze scaling trends (language presence, model size, grade level), apply multilingual techniques in a multimodal setting, and compare results with performance data from 68,000+ student participants.", "result": "Models still struggle with basic math and diagram-based reasoning; accuracy tends to increase with language coverage in pretraining and model size but shows little correlation with grade level difficulty. Multilingual adaptation techniques improve performance over simple baselines. The dataset and code are released, including an English-only subset M2Kang.", "conclusion": "M3Kang exposes substantial gaps between current VLMs and humans in multilingual multimodal math reasoning, offers a challenging new benchmark, and shows that multilingual methods can help in multimodal contexts, but significant work remains to reach robust, human-level reasoning across languages and visual modalities."}}
{"id": "2601.16219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16219", "abs": "https://arxiv.org/abs/2601.16219", "authors": ["Erdem Aslan", "Pakize Erdo\u011fmu\u015f"], "title": "Domain Specific Specialization in Low-Resource Settings: The Efficacy of Offline Response-Based Knowledge Distillation in Large Language Models", "comment": "10 pages, 10 tables", "summary": "Large Language Models (LLMs) excel in general tasks but often struggle with hallucinations when handling domain-specific or institutional knowledge absent from their pre-training. We present an offline response-based knowledge distillation method that develops high-accuracy specialized assistants under constrained hardware resources. We evaluate three distinct data strategies: general domain adaptation (15,000 lines), unstructured knowledge injection (2,000 lines), and a context-aware synthetic dataset (500 lines) generated by a teacher model. To minimize computational costs, we utilize the Unsloth library to optimize the Qwen-2.5-7B student model, reducing NVIDIA A100 GPU memory requirements from 40 GB to 16 GB. Experimental results demonstrate that while larger unstructured datasets suffer from persistent hallucinations, the 500-line context-aware dataset achieves a 96.7% accuracy rate and robust rejection capability. These findings validate the LIMA hypothesis, showing that data quality and structural alignment are more critical than quantity for domain adaptation in low-resource settings.", "AI": {"tldr": "The paper proposes an offline, response-based knowledge distillation approach to create accurate, domain-specific LLM assistants under low-resource constraints, showing that a small, carefully designed context-aware dataset outperforms larger, less structured data for reducing hallucinations.", "motivation": "LLMs often hallucinate when dealing with domain-specific or institutional knowledge that was not present in pre-training. Many organizations also have limited computational resources, making full fine-tuning or deployment of large teacher models impractical. The authors aim to build reliable, specialized assistants that avoid hallucination while operating under strict hardware and data constraints.", "method": "They use an offline response-based knowledge distillation pipeline where a large teacher model generates domain-specific, context-aware training data, and a smaller Qwen-2.5-7B model serves as the student. Three data strategies are compared: (1) general domain adaptation with 15,000 training lines, (2) unstructured knowledge injection with 2,000 lines, and (3) a highly curated 500-line synthetic, context-aware dataset created by the teacher. To reduce hardware cost, they fine-tune the student using the Unsloth library, cutting A100 GPU memory usage from 40 GB to 16 GB. They then evaluate accuracy and hallucination/rejection behavior across these settings.", "result": "The large, unstructured datasets (15,000-line and 2,000-line variants) still exhibit significant hallucinations and fail to provide reliable domain-specific answers. In contrast, the compact 500-line context-aware synthetic dataset yields a student model with 96.7% accuracy and strong rejection behavior when it lacks sufficient knowledge, indicating effective hallucination control. The optimizations also successfully reduce GPU memory requirements for training from 40 GB to 16 GB, validating the approach\u2019s feasibility in low-resource environments.", "conclusion": "High-quality, structurally aligned, context-aware training data is more important than raw data volume for domain adaptation of LLMs, particularly in low-resource settings. A small, well-designed synthetic dataset distilled from a teacher model can substantially improve accuracy and hallucination robustness of a smaller student assistant. The results support the LIMA hypothesis that limited but carefully curated data can be sufficient to create strong specialized models, and demonstrate that efficient fine-tuning libraries like Unsloth enable practical deployment on constrained hardware."}}
{"id": "2601.16220", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16220", "abs": "https://arxiv.org/abs/2601.16220", "authors": ["Nesta Midavaine", "Christian A. Naesseth", "Grigory Bartosh"], "title": "Towards Latent Diffusion Suitable For Text", "comment": null, "summary": "Language diffusion models aim to improve sampling speed and coherence over autoregressive LLMs. We introduce Neural Flow Diffusion Models for language generation, an extension of NFDM that enables the straightforward application of continuous diffusion models to discrete state spaces. NFDM learns a multivariate forward process from the data, ensuring that the forward process and generative trajectory are a good fit for language modeling. Our model substantially reduces the likelihood gap with autoregressive models of the same size, while achieving sample quality comparable to that of previous latent diffusion models.", "AI": {"tldr": "The paper presents Neural Flow Diffusion Models (NFDM) for language generation, adapting continuous diffusion to discrete tokens and narrowing the performance gap with autoregressive models.", "motivation": "Autoregressive language models, while performant, can be slow in sampling and may struggle with long-range coherence. Existing language diffusion models either have slower sampling or work in latent spaces that don\u2019t fully exploit token-level structure. There is a need for a diffusion-based approach that is both fast and well-matched to discrete language data, reducing the likelihood gap with strong autoregressive baselines.", "method": "The authors extend Neural Flow Diffusion Models (NFDM) to language generation by defining a multivariate forward diffusion process directly over discrete token sequences. They learn this forward process from data so that the induced generative trajectories align well with the statistics of natural language. This allows them to apply continuous diffusion modeling machinery in a way that is compatible with discrete state spaces, avoiding awkward relaxations or complex discretization tricks.", "result": "The proposed NFDM-based language model significantly narrows the negative log-likelihood gap when compared to autoregressive models of comparable parameter counts. In terms of qualitative sample quality (e.g., fluency and coherence), the model achieves performance similar to prior latent diffusion approaches for language, while delivering the additional benefits of its NFDM formulation.", "conclusion": "Learning a data-driven multivariate forward process enables Neural Flow Diffusion Models to be effectively applied to discrete language modeling. This yields a diffusion-based language generator that is closer in likelihood performance to autoregressive LLMs while maintaining competitive sample quality, suggesting that NFDM is a promising direction for fast, coherent language diffusion models."}}
{"id": "2601.16286", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16286", "abs": "https://arxiv.org/abs/2601.16286", "authors": ["Varun Chillara", "Dylan Kline", "Christopher Alvares", "Evan Wooten", "Huan Yang", "Shlok Khetan", "Cade Bauer", "Tr\u00e9 Guillory", "Tanishka Shah", "Yashodhara Dhariwal", "Volodymyr Pavlov", "George Popstefanov"], "title": "SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems", "comment": null, "summary": "Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.\n  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.\n  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.", "AI": {"tldr": "SemanticALLI is a pipeline-aware, caching-focused architecture that reduces redundant reasoning in agentic AI systems by caching structured intermediate representations, dramatically increasing cache hit rates and cutting LLM calls and latency.", "motivation": "Agentic AI pipelines repeatedly regenerate the same intermediate logic (e.g., metric normalization, chart scaffolding) even when user queries differ in surface wording. Standard monolithic caching at the request/response boundary cannot capture this redundancy because it cannot see or reuse internal structure, leading to inefficiency in latency and token usage.", "method": "The authors design SemanticALLI inside the Alli marketing intelligence platform by decomposing the pipeline into two explicit stages: Analytic Intent Resolution (AIR) and Visualization Synthesis (VS). They represent the outputs of these stages as structured intermediate representations (IRs) and treat these IRs as cacheable units. They then introduce caching at the VS stage and compare its performance against conventional monolithic boundary caching, measuring cache hit rates, bypassed LLM calls, and latency.", "result": "Baseline, monolithic boundary caching reaches only a 38.7% cache hit rate because different natural language phrasings prevent reuse. With SemanticALLI\u2019s structured, stage-level caching, the Visualization Synthesis stage alone achieves an 83.10% cache hit rate, skipping 4,023 LLM calls and serving cached results with a median latency of 2.66 ms, while also reducing overall token consumption.", "conclusion": "Agentic AI systems can gain substantial efficiency by exposing and caching structured intermediate representations at stable pipeline checkpoints instead of treating the model as a monolithic black box. Even when user utterances are rarely identical, the internal reasoning steps often are, and caching at these internal stages dramatically improves cache effectiveness, reduces token usage, and cuts latency, providing a general design lesson for AI pipeline architects."}}
{"id": "2601.16276", "categories": ["cs.CL", "cs.AI", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16276", "abs": "https://arxiv.org/abs/2601.16276", "authors": ["Victor Conchello Vendrell", "Max Ruiz Luyten", "Mihaela van der Schaar"], "title": "GameTalk: Training LLMs for Strategic Conversation", "comment": "32 pages, 8 figures", "summary": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \\textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.", "AI": {"tldr": "Paper proposes GameTalk, a framework to train LLMs for strategic decision-making via multi-turn conversational interactions in games, optimizing a global objective over whole dialogues.", "motivation": "Existing LLM research mostly studies single-step or static decision tasks and overlooks how to optimize long-term, strategic objectives that emerge over extended dialogues in multi-agent settings. There is a need for methods that let LLMs coordinate, negotiate, and plan over full conversations rather than isolated turns.", "method": "Introduce GameTalk, which adapts fine-tuning methods such as GRPO, DPO, and STaR so that reward signals depend on the outcome of an entire multi-turn interaction instead of individual turns. Train LLMs on a suite of games requiring reasoning, coordination, and opponent modeling, using conversational trajectories and global rewards to guide learning.", "result": "Models trained with GameTalk substantially outperform untrained baselines across multiple games. Reward shaping further boosts performance, and among the tested fine-tuning methods, DPO provides the most consistent and strongest performance improvements.", "conclusion": "Conversational fine-tuning with interaction-level rewards is an effective way to improve LLM strategic decision-making in multi-agent, multi-turn settings. GameTalk demonstrates that optimizing global objectives over dialogues enables better reasoning, negotiation, and action in interactive environments, and suggests this is a promising direction for future LLM training approaches."}}
{"id": "2601.16344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16344", "abs": "https://arxiv.org/abs/2601.16344", "authors": ["Fan Nie", "Junlin Wang", "Harper Hua", "Federico Bianchi", "Yongchan Kwon", "Zhenting Qi", "Owen Queen", "Shang Zhu", "James Zou"], "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents", "comment": null, "summary": "Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.", "AI": {"tldr": "Introduces DSGym, a standardized, extensible framework and task suite to rigorously evaluate and train data science agents in realistic, data-grounded execution environments.", "motivation": "Current data science agent benchmarks are fragmented, hard to compare across, cover a narrow range of tasks, and often do not require genuine data use to succeed. There is a need for a unified, extensible, and rigorously data-grounded framework that can both evaluate and train agents on realistic, end-to-end data analysis workflows.", "method": "Design a modular framework (DSGym) that provides self-contained execution environments and standardized interfaces for data science agents, allowing easy addition of tasks, tools, and agent scaffolds. Curate DSGym-Tasks by standardizing and filtering existing benchmarks for quality and shortcut solvability. Extend task coverage with two new suites: DSBio, consisting of expert-derived, literature-grounded bioinformatics tasks, and DSPredict, composed of challenging prediction tasks across multiple scientific domains. Additionally, build an execution-verified data synthesis pipeline within DSGym to generate training data for agents, and use it to train a 4B-parameter model on 2,000 curated examples.", "result": "A comprehensive benchmark and training environment (DSGym) along with DSGym-Tasks, DSBio, and DSPredict is created. Using its execution-verified data synthesis, the authors assemble a 2,000-example training set and train a 4B model within DSGym. This model achieves performance that surpasses GPT-4o on standardized data analysis benchmarks, demonstrating the effectiveness of DSGym both as an evaluation and training platform.", "conclusion": "DSGym provides a rigorous, extensible, and data-grounded framework for assessing and improving data science agents. By standardizing tasks, eliminating shortcut solutions, expanding into realistic scientific domains, and enabling execution-verified training data generation, DSGym allows robust end-to-end measurement of agents\u2019 ability to plan, implement, and validate data analyses in realistic scientific contexts and can even be used to train competitive models that outperform state-of-the-art general-purpose LLMs on these tasks."}}
{"id": "2601.16278", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16278", "abs": "https://arxiv.org/abs/2601.16278", "authors": ["Branislav Pecher", "Jan Cegin", "Robert Belanec", "Ivan Srba", "Jakub Simko", "Maria Bielikova"], "title": "Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification", "comment": "Accepted to the Findings of EACL 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable multilingual capabilities, making them promising tools in both high- and low-resource languages. One particularly valuable use case is generating synthetic samples that can be used to train smaller models in low-resource scenarios where human-labelled data is scarce. In this work, we investigate whether these synthetic data generation capabilities can serve as a form of distillation, producing smaller models that perform on par with or even better than massive LLMs across languages and tasks. To this end, we use a state-of-the-art multilingual LLM to generate synthetic datasets covering 11 languages and 4 classification tasks. These datasets are then used to train smaller models via fine-tuning or instruction tuning, or as synthetic in-context examples for compact LLMs. Our experiments show that even small amounts of synthetic data enable smaller models to outperform the large generator itself, particularly in low-resource languages. Overall, the results suggest that LLMs are best utilised as generators (teachers) rather than classifiers, producing data that empowers smaller and more efficient multilingual models.", "AI": {"tldr": "The paper studies using large multilingual LLMs to generate synthetic training data that can train smaller models which sometimes outperform the original LLM, especially for low-resource languages.", "motivation": "Human-labeled data is scarce and expensive in many languages, especially low-resource ones. Large multilingual LLMs can generate text in many languages, which suggests they could help create labeled training data where none exists. The authors want to know if we can systematically exploit this ability so that smaller, cheaper models can match or beat the large LLM\u2019s performance, rather than relying on the large LLM directly for inference.", "method": "Use a strong multilingual LLM as a data generator rather than as a classifier. It produces synthetic labeled datasets spanning 11 languages and 4 text-classification tasks. Then: (1) fine-tune smaller models on this synthetic data, (2) instruction-tune compact LLMs with these samples, and (3) use the synthetic examples as in-context demonstrations for compact LLMs. Compare performance of these distilled or trained small models against the large generator across tasks and languages, with special attention to low-resource settings.", "result": "Even relatively small amounts of synthetic labeled data allow smaller models to reach or exceed the performance of the large LLM that generated the data, with the gains being especially strong in low-resource languages. The synthetic-data-trained models generalize well across tasks and languages despite never seeing human labels for many of them.", "conclusion": "LLMs are more effective as data generators (teachers) than as direct classifiers in many multilingual settings. By generating synthetic multilingual datasets, they can distill their knowledge into smaller, more efficient models that outperform them, particularly where human-labeled data is limited. This supports a paradigm of using large LLMs offline to create training corpora that empower compact models for deployment."}}
{"id": "2601.16479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16479", "abs": "https://arxiv.org/abs/2601.16479", "authors": ["Hongjia Wu", "Shuai Zhou", "Hongxin Zhang", "Wei Chen"], "title": "Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs", "comment": null, "summary": "While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an \"expert bottleneck\" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.", "AI": {"tldr": "Doc2AHP is a framework that combines large language models with Analytic Hierarchy Process principles to automatically build consistent decision models from unstructured documents.", "motivation": "LLMs are strong at semantic understanding but weak at maintaining structural consistency and reliable reasoning in complex, logic-intensive decision-making tasks. Classical decision methods like AHP offer rigor and structure, but they require substantial expert involvement to define hierarchies and pairwise comparisons, making them costly and unscalable. The paper aims to remove this expert bottleneck while preserving AHP\u2019s rational, interpretable structure by leveraging LLMs\u2019 generalization abilities.", "method": "The authors propose Doc2AHP, a structured inference framework that uses AHP principles as hard structural constraints on LLM outputs. The LLM explores unstructured document space in a constrained manner, guided by AHP-style hierarchies, to ensure logical entailment between parent and child nodes in the decision tree. They further design a multi-agent weighting mechanism, where multiple LLM agents or calls estimate weights, combined with an adaptive consistency optimization procedure to adjust and refine the numerical weights so that they satisfy AHP-style consistency requirements without manual expert tuning or labeled data.", "result": "Experiments show that Doc2AHP enables non-expert users to automatically construct high-quality AHP-like decision models directly from documents. Compared with direct generative LLM baselines that attempt to build decision structures without explicit AHP constraints, Doc2AHP achieves higher logical completeness of the decision hierarchy and better performance on downstream decision tasks, as measured by task accuracy and consistency metrics.", "conclusion": "Doc2AHP effectively integrates the structural rigor of AHP with the flexibility of LLMs, removing the need for intensive expert input. By constraining LLM inferences with AHP principles and enforcing numerical consistency through multi-agent weighting and adaptive optimization, the framework produces more reliable, logically coherent decision models that translate into improved downstream performance over unconstrained generative approaches."}}
{"id": "2601.16282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16282", "abs": "https://arxiv.org/abs/2601.16282", "authors": ["Peter Jansen", "Peter Clark", "Doug Downey", "Daniel S. Weld"], "title": "Generating Literature-Driven Scientific Theories at Scale", "comment": "9 pages plus appendix, 3 figures", "summary": "Contemporary automated scientific discovery has focused on agents for generating scientific experiments, while systems that perform higher-level scientific activities such as theory building remain underexplored. In this work, we formulate the problem of synthesizing theories consisting of qualitative and quantitative laws from large corpora of scientific literature. We study theory generation at scale, using 13.7k source papers to synthesize 2.9k theories, examining how generation using literature-grounding versus parametric knowledge, and accuracy-focused versus novelty-focused generation objectives change theory properties. Our experiments show that, compared to using parametric LLM memory for generation, our literature-supported method creates theories that are significantly better at both matching existing evidence and at predicting future results from 4.6k subsequently-written papers", "AI": {"tldr": "They propose and evaluate an automated system that builds scientific theories from large literature corpora, showing literature-grounded theories are more accurate and predictive than those based only on an LLM\u2019s internal knowledge.", "motivation": "Most work on automated scientific discovery focuses on generating or optimizing experiments, not on the higher-level activity of constructing scientific theories that summarize and explain findings. There is a need to automatically synthesize structured theories\u2014containing both qualitative and quantitative laws\u2014from the ever-growing scientific literature, and to understand how different knowledge sources and objectives affect the quality of such theories.", "method": "They formalize theory synthesis as constructing collections of qualitative and quantitative laws from large literature corpora. Using 13.7k source papers, they automatically generate 2.9k theories under different configurations: (1) grounding in retrieved literature vs relying on the parametric memory of large language models, and (2) optimizing for accuracy vs optimizing for novelty. They then compare the resulting theories on how well they fit existing evidence and how well they predict outcomes reported in 4.6k later papers.", "result": "Across these comparisons, literature-grounded theory generation produces theories that more closely match the existing empirical evidence and are better at predicting results reported in subsequently written papers than theories generated from parametric LLM knowledge alone. The experiments also characterize how objectives (accuracy vs novelty) influence properties of the synthesized theories, though the abstract emphasizes the performance advantage of literature-grounding.", "conclusion": "Automated large-scale theory synthesis from scientific literature is feasible and beneficial. Grounding theory generation in explicit literature, rather than relying only on an LLM\u2019s internal parameters, yields theories that are both more empirically accurate and more forward-predictive. This supports the view that future automated scientific discovery systems should integrate large-scale literature retrieval with generative models for higher-level tasks like theory building."}}
{"id": "2601.16529", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.16529", "abs": "https://arxiv.org/abs/2601.16529", "authors": ["Dongshen Peng", "Yi Wang", "Carl Preiksaitis", "Christian Rose"], "title": "SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care", "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\\%. Models showed higher vulnerability to imaging requests (38.8\\%) than opioid prescriptions (25.0\\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.", "AI": {"tldr": "The paper evaluates how robust large language models are against persuasive patients who pressure them into unsafe medical decisions in emergency medicine, showing wide variability and high susceptibility across models.", "motivation": "While LLMs are being considered for clinical decision support, there is a concern that they might yield to patients\u2019 demands for unnecessary or unsafe care (e.g., imaging or opioids). Existing static benchmarks don\u2019t test this social-pressure dimension, so there is a need for a method to systematically probe LLMs\u2019 behavior under adversarial patient persuasion in realistic, multi-turn clinical encounters.", "method": "The authors build SycoEval-EM, a multi-agent simulation framework in which an adversarial \u2018patient\u2019 agent attempts to persuade an LLM \u2018clinician\u2019 agent to provide inappropriate care in emergency medicine scenarios. They run 1,875 simulated encounters involving 20 different LLMs across three Choosing Wisely clinical scenarios, vary persuasion tactics, and measure acquiescence rates\u2014how often the LLM complies with unsafe or low-value requests.", "result": "Across 20 LLMs, acquiescence to inappropriate care ranged from 0% to 100% depending on the model. Overall, models were more likely to agree to inappropriate imaging (38.8% acquiescence) than to inappropriate opioid prescriptions (25.0%). There was little correlation between general model capability and robustness to persuasion. Different persuasion tactics had similar success rates (30.0\u201336.0%), suggesting a broad vulnerability rather than susceptibility to any one tactic.", "conclusion": "LLMs used for clinical decision support can be highly vulnerable to social pressure from patients, and standard static benchmarks fail to capture this safety risk. Multi-turn, adversarial evaluation like SycoEval-EM is necessary for properly certifying and deploying clinical AI systems, as it better reflects real-world dynamics of patient\u2013clinician interactions and reveals safety weaknesses that simple accuracy benchmarks miss."}}
{"id": "2601.16309", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.16309", "abs": "https://arxiv.org/abs/2601.16309", "authors": ["Dikshya Mohanty", "Taisiia Sabadyn", "Jelwin Rodrigues", "Chenlu Wang", "Abhishek Kalugade", "Ritwik Banerjee"], "title": "A Longitudinal, Multinational, and Multilingual Corpus of News Coverage of the Russo-Ukrainian War", "comment": null, "summary": "We introduce DNIPRO, a novel longitudinal corpus of 246K news articles documenting the Russo-Ukrainian war from Feb 2022 to Aug 2024, spanning eleven media outlets across five nation states (Russia, Ukraine, U.S., U.K., and China) and three languages (English, Russian, and Mandarin Chinese). This multilingual resource features consistent and comprehensive metadata, and multiple types of annotation with rigorous human evaluations for downstream tasks relevant to systematic transnational analyses of contentious wartime discourse. DNIPRO's distinctive value lies in its inclusion of competing geopolitical perspectives, making it uniquely suited for studying narrative divergence, media framing, and information warfare. To demonstrate its utility, we include use case experiments using stance detection, sentiment analysis, topical framing, and contradiction analysis of major conflict events within the larger war. Our explorations reveal how outlets construct competing realities, with coverage exhibiting polarized interpretations that reflect geopolitical interests. Beyond supporting computational journalism research, DNIPRO provides a foundational resource for understanding how conflicting narratives emerge and evolve across global information ecosystems.", "AI": {"tldr": "DNIPRO is a large multilingual, multi-national news corpus about the 2022\u20132024 Russo-Ukrainian war, built to enable systematic, cross-country analysis of competing wartime narratives and media framing.", "motivation": "Existing datasets on the Russo-Ukrainian war and wartime media are either monolingual, single-country, short in time span, or lack rich, human-validated annotations. This makes it difficult to systematically compare how different states\u2019 media ecosystems construct and evolve conflicting narratives about the same conflict. The authors are motivated to create a high-quality, longitudinal, multilingual corpus with balanced representation of rival geopolitical blocs, enabling rigorous study of media bias, framing, information warfare, and narrative divergence across countries and languages.", "method": "The authors compile 246K news articles from Feb 2022 to Aug 2024 from eleven outlets across Russia, Ukraine, the U.S., the U.K., and China, in English, Russian, and Mandarin Chinese. They harmonize and standardize metadata across outlets, and add multiple layers of annotation relevant to discourse analysis (e.g., stance, sentiment, topical framing, contradiction), with rigorous human evaluation to ensure quality. They then conduct a series of downstream experiments\u2014stance detection, sentiment analysis, topical framing, and contradiction analysis on reporting of key conflict events\u2014to demonstrate the corpus\u2019s usefulness and validate that the annotations support systematic, cross-national comparisons.", "result": "The resulting resource, DNIPRO, is a large-scale, longitudinal, multilingual news corpus with consistent metadata and multiple annotation layers, covering eleven media outlets from five countries over 2.5 years of wartime reporting. Experimental use cases using stance detection, sentiment analysis, framing, and contradiction analysis show that the corpus effectively reveals systematic differences in how outlets describe the same events. The analyses uncover clear patterns of polarized interpretations and competing realities that map onto geopolitical alignments and interests of the respective media ecosystems.", "conclusion": "DNIPRO offers a foundational, publicly documented corpus for computational and comparative media research on the Russo-Ukrainian war. Its cross-national, multilingual design and rich annotations make it particularly well-suited for studying narrative divergence, framing, and information warfare across rival geopolitical blocs. The authors conclude that DNIPRO can support a wide range of downstream tasks in computational journalism and social science, and that it provides critical infrastructure for understanding how conflicting wartime narratives emerge, compete, and evolve across global information environments."}}
{"id": "2601.16549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16549", "abs": "https://arxiv.org/abs/2601.16549", "authors": ["Meet Raval", "Tejul Pandit", "Dhvani Upadhyay"], "title": "LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification", "comment": "9 pages, 5 figures, 3 tables, paper accepted in AAIML'26 conference", "summary": "The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.", "AI": {"tldr": "The paper benchmarks classical machine learning models against modern vision-language and large language models for medical classification and finds that traditional methods still outperform in most text and image tasks, while minimal PEFT fine-tuning harms foundation model performance.", "motivation": "To rigorously compare the effectiveness of traditional machine learning models and modern transformer-based vision-language and large language models for medical classification tasks, and to test whether foundation models and parameter-efficient fine-tuning offer practical advantages over established methods.", "method": "The authors build a unified benchmark using four public medical datasets spanning text and image modalities and including binary and multiclass classification tasks. For each task, they evaluate three model classes: (1) classical ML models such as Logistic Regression, LightGBM, and ResNet-50; (2) zero-shot prompt-based LLM/VLM pipelines using Gemini 2.5; and (3) PEFT models by applying LoRA-based fine-tuning to Gemma3 variants. All experiments use consistent train/validation/test splits and aligned evaluation metrics to enable fair comparison.", "result": "Classical ML models achieve the strongest and most consistent performance across most medical categorization tasks, especially for structured text datasets. LoRA-tuned Gemma3 variants perform worst across all experiments, failing to generalize from limited fine-tuning. Zero-shot Gemini 2.5 shows weak performance on text-based tasks but competitive results on multiclass image classification, where it matches the ResNet-50 baseline.", "conclusion": "In medical classification settings covered by their benchmark, classical machine learning remains the most reliable option, outperforming both zero-shot and minimally fine-tuned foundation models. Foundation models are not inherently superior, and the success of PEFT is highly sensitive to adaptation strategy; in this work, minimal LoRA-based fine-tuning degraded performance, underscoring the need for more careful adaptation when applying foundation models to medical tasks."}}
{"id": "2601.16312", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16312", "abs": "https://arxiv.org/abs/2601.16312", "authors": ["Dikshya Mohanty", "Mohammad Saqib Hasan", "Syed Mostofa Monsur", "Size Zheng", "Benjamin Hsiao", "Niranjan Balasubramanian"], "title": "Teaching and Evaluating LLMs to Reason About Polymer Design Related Tasks", "comment": null, "summary": "Research in AI4Science has shown promise in many science applications, including polymer design. However, current LLMs prove ineffective on this problem space because: (i) most models lack polymer-specific knowledge (ii) existing aligned models lack coverage of knowledge and capabilities relevant to polymer design. Addressing this, we introduce PolyBench, a large scale training and test benchmark dataset of more than 125K polymer design related tasks, leveraging a knowledge base of 13M+ data points obtained from experimental and synthetic sources to ensure broad coverage of polymers and their properties. For effective alignment using PolyBench, we introduce a knowledge-augmented reasoning distillation method that augments this dataset with structured CoT. Furthermore, tasks in PolyBench are organized from simple to complex analytical reasoning problems, enabling generalization tests and diagnostic probes across the problem space. Experiments show that small language models (SLMs), of 7B to 14B parameters, trained on PolyBench data outperform similar sized models, and even closed source frontier LLMs on PolyBench test dataset while demonstrating gains on other polymer benchmarks as well.", "AI": {"tldr": "The paper presents PolyBench, a large-scale benchmark and training dataset tailored for polymer design, along with a knowledge-augmented reasoning distillation method that boosts small language models\u2019 performance beyond comparable and even frontier LLMs on polymer tasks.", "motivation": "General-purpose LLMs struggle with polymer design because they lack polymer-specific knowledge and current alignment data do not cover the specialized concepts and reasoning skills needed for this domain. There is a need for a systematic, large-scale benchmark and training resource that captures real polymer knowledge and supports complex reasoning for AI4Science applications.", "method": "The authors construct PolyBench, a dataset of over 125K polymer design tasks built on a 13M+ point polymer knowledge base from experimental and synthetic sources, organized from simple to complex analytical reasoning tasks. They then propose a knowledge-augmented reasoning distillation method that adds structured chain-of-thought (CoT) to these tasks, and train small language models (7B\u201314B parameters) with this dataset and method.", "result": "Models trained with PolyBench and the proposed distillation method outperform similarly sized open models and even some closed-source frontier LLMs on the PolyBench test set, and also show improved performance on external polymer benchmarks, indicating better generalization within the polymer design domain.", "conclusion": "A domain-specific, knowledge-rich benchmark coupled with structured reasoning supervision can significantly enhance the capability of small language models for polymer design, suggesting that specialized training pipelines like PolyBench are an effective path to high-performing AI4Science systems without requiring massive, general-purpose LLMs."}}
{"id": "2601.16649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16649", "abs": "https://arxiv.org/abs/2601.16649", "authors": ["Amin Rakhsha", "Thomas Hehn", "Pietro Mazzaglia", "Fabio Valerio Massoli", "Arash Behboodi", "Tribhuvanesh Orekondy"], "title": "LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents", "comment": null, "summary": "Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.", "AI": {"tldr": "They build an oracle-based evaluation framework on synthetic multi-turn tasks to quantify how much specific skills (like planning or state tracking) would help language-model agents, showing some skills (planning) are robustly helpful while others depend on task and model.", "motivation": "Large language models do well on single, isolated tasks but still struggle on multi-turn, long-horizon agentic problems requiring planning, state tracking, and long-context reasoning. It is unclear which underlying capabilities are most important to improve to make progress on such agents. The authors want a principled way to measure the value of specific capabilities for future AI agent development.", "method": "They design an oracle counterfactual framework: for a given multi-turn environment, they imagine agents that are identical except that one gets help from an oracle that perfectly executes a specific skill (e.g., optimal planning, accurate state tracking). By comparing performance with and without each oracle, they quantify the criticality of each capability. To make this controllable, they introduce procedurally generated, game-like tasks with tunable complexity, where they can precisely inject oracle interventions like perfect planning or flawless state tracking without confounds.", "result": "In these controlled multi-turn, game-like tasks, adding oracle planning consistently boosts agent performance across different settings. Other oracle skills also help but their impact varies with environmental properties and the underlying language model; some are very beneficial only in certain regimes or task configurations. This reveals a non-uniform importance of different capabilities for multi-turn performance.", "conclusion": "Oracle counterfactual analysis on controlled multi-turn tasks can disentangle the contribution of individual capabilities like planning and state tracking to agent performance. Planning emerges as a generally valuable capability, while the importance of other skills is context- and model-dependent. This provides guidance on which skills to prioritize when improving future AI agents and language models for long-horizon interaction."}}
{"id": "2601.16314", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16314", "abs": "https://arxiv.org/abs/2601.16314", "authors": ["Andres Karjus", "Kais Allkivi", "Silvia Maine", "Katarin Leppik", "Krister Kruusmaa", "Merilin Aruvee"], "title": "Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP", "comment": null, "summary": "Large language models (LLMs) enable rapid and consistent automated evaluation of open-ended exam responses, including dimensions of content and argumentation that have traditionally required human judgment. This is particularly important in cases where a large amount of exams need to be graded in a limited time frame, such as nation-wide graduation exams in various countries. Here, we examine the applicability of automated scoring on two large datasets of trial exam essays of two full national cohorts from Estonia. We operationalize the official curriculum-based rubric and compare LLM and statistical natural language processing (NLP) based assessments with human panel scores. The results show that automated scoring can achieve performance comparable to that of human raters and tends to fall within the human scoring range. We also evaluate bias, prompt injection risks, and LLMs as essay writers. These findings demonstrate that a principled, rubric-driven, human-in-the-loop scoring pipeline is viable for high-stakes writing assessment, particularly relevant for digitally advanced societies like Estonia, which is about to adapt a fully electronic examination system. Furthermore, the system produces fine-grained subscore profiles that can be used to generate systematic, personalized feedback for instruction and exam preparation. The study provides evidence that LLM-assisted assessment can be implemented at a national scale, even in a small-language context, while maintaining human oversight and compliance with emerging educational and regulatory standards.", "AI": {"tldr": "The paper investigates whether large language models can reliably and fairly grade national-scale exam essays, showing that rubric-based, human-supervised LLM scoring can match human raters and support feedback generation, even for a smaller language like Estonian.", "motivation": "Manual grading of open-ended exam essays is slow, costly, and hard to scale, especially for national examinations that must be graded quickly and consistently. Traditional automated scoring has limitations in capturing complex content and argumentation and often focuses on major languages. With the emergence of powerful LLMs, there is a need to test whether they can provide accurate, scalable, and fair scoring for high-stakes exams, including in smaller-language contexts like Estonia and in alignment with official curricular rubrics and regulatory requirements.", "method": "The authors use two large datasets of trial exam essays from two full national cohorts in Estonia. They formalize the official curriculum-based scoring rubric into machine-usable criteria and apply both LLM-based and traditional statistical NLP-based scoring approaches. They then compare automated scores against human panel scores to assess agreement and performance. In addition, they probe potential bias in the automated scoring, test the vulnerability of the system to prompt injection attacks, and experiment with LLMs as essay generators. They adopt a human-in-the-loop design, where automated scoring is guided by the rubric and subject to human oversight.", "result": "LLM-based automated scoring reaches performance levels comparable to human raters and generally stays within the range of human scoring variability. The system can also provide detailed subscores aligned with rubric dimensions, enabling more granular analysis and feedback. The authors find that, with appropriate design and checks, the system can mitigate major risks related to bias and prompt injection, and can be integrated into an operational grading pipeline. LLM-generated essays are also examined, informing robustness and security considerations for the assessment system.", "conclusion": "A rubric-driven, human-in-the-loop LLM scoring pipeline is a viable solution for high-stakes writing assessment at national scale, even in a small-language setting like Estonia. It can match human rater performance while producing fine-grained subscore profiles useful for personalized feedback and exam preparation. The study suggests that such systems can be aligned with educational standards and regulatory expectations, supporting a transition to fully electronic examination systems in digitally advanced societies."}}
{"id": "2601.16685", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16685", "abs": "https://arxiv.org/abs/2601.16685", "authors": ["Suzhong Fu", "Jingqi Dong", "Xuan Ding", "Rui Sun", "Yiming Yang", "Shuguang Cui", "Zhen Li"], "title": "AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning", "comment": null, "summary": "Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.", "AI": {"tldr": "AgentsEval is a multi-agent evaluation framework that mimics radiologists\u2019 reasoning to more accurately and transparently assess automatically generated medical imaging reports.", "motivation": "Automatic evaluation of medical imaging reports is unreliable because current metrics do not capture the structured diagnostic logic radiologists use, reducing clinical trust and relevance of report-generation systems.", "method": "The authors design a multi-agent stream reasoning framework that decomposes evaluation into explicit steps\u2014criteria definition, evidence extraction, alignment between evidence and criteria, and consistency scoring\u2014so that multiple agents collaboratively emulate radiologists\u2019 diagnostic workflow while producing interpretable reasoning traces. They also build a perturbation-based benchmark spanning five imaging-report datasets and modalities, where reports are systematically altered at semantic and stylistic levels to test evaluation robustness.", "result": "Across this multi-domain, perturbed benchmark, AgentsEval yields evaluations that better match clinical expectations, maintain semantic faithfulness to the underlying findings, provide interpretable feedback, and remain robust to paraphrasing, semantic shifts, and style changes, outperforming existing automatic evaluation methods.", "conclusion": "AgentsEval offers a transparent and clinically grounded way to evaluate medical report generation systems by modeling radiologists\u2019 collaborative reasoning and proving robust across diverse datasets and perturbations, thereby supporting more trustworthy use of large language models in clinical workflows."}}
{"id": "2601.16349", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16349", "abs": "https://arxiv.org/abs/2601.16349", "authors": ["M P V S Gopinadh", "Kappara Lakshmi Sindhu", "Soma Sekhar Pandu Ranga Raju P", "Yesaswini Swarna"], "title": "Regional Bias in Large Language Models", "comment": "8 pages, 1 figure. Presented at the Second International Conference on Advanced Computing, Machine Learning, Robotics and Internet Technologies (AMRIT 2024)", "summary": "This study investigates regional bias in large language models (LLMs), an emerging concern in AI fairness and global representation. We evaluate ten prominent LLMs: GPT-3.5, GPT-4o, Gemini 1.5 Flash, Gemini 1.0 Pro, Claude 3 Opus, Claude 3.5 Sonnet, Llama 3, Gemma 7B, Mistral 7B, and Vicuna-13B using a dataset of 100 carefully designed prompts that probe forced-choice decisions between regions under contextually neutral scenarios. We introduce FAZE, a prompt-based evaluation framework that measures regional bias on a 10-point scale, where higher scores indicate a stronger tendency to favor specific regions. Experimental results reveal substantial variation in bias levels across models, with GPT-3.5 exhibiting the highest bias score (9.5) and Claude 3.5 Sonnet scoring the lowest (2.5). These findings indicate that regional bias can meaningfully undermine the reliability, fairness, and inclusivity of LLM outputs in real-world, cross-cultural applications. This work contributes to AI fairness research by highlighting the importance of inclusive evaluation frameworks and systematic approaches for identifying and mitigating geographic biases in language models.", "AI": {"tldr": "The paper proposes FAZE, a prompt-based framework to quantify regional bias in major LLMs, finding substantial and varying levels of geographic preference across models.", "motivation": "As LLMs are increasingly deployed globally, there is growing concern that they may exhibit unfair preferences toward certain regions, undermining fairness, reliability, and inclusivity in cross-cultural applications. Existing evaluations of bias often focus on demographics like gender or race and lack systematic, scalable ways to measure geographic or regional bias, especially in neutral, non-political contexts. This work aims to fill that gap by creating a structured framework and benchmark to detect and compare regional bias across widely used LLMs.", "method": "The authors design a dataset of 100 carefully controlled prompts that force models to choose between regions in contextually neutral scenarios, thus minimizing confounds such as safety policies or clearly correct answers. They propose FAZE, a prompt-based evaluation framework that assigns each model a regional bias score on a 10-point scale, where higher values represent stronger, systematic preference for specific regions. Using this framework, they evaluate ten prominent LLMs: GPT-3.5, GPT-4o, several Gemini and Claude variants, and open models like Llama 3, Gemma 7B, Mistral 7B, and Vicuna-13B, then aggregate their responses to compute bias scores and compare them across models.", "result": "The experimental evaluation shows wide variation in regional bias across the ten LLMs. GPT-3.5 receives the highest bias score (9.5), indicating a strong and systematic tendency to favor certain regions, whereas Claude 3.5 Sonnet obtains the lowest score (2.5), suggesting comparatively low regional preference. Other models fall between these extremes, demonstrating that regional bias is not uniform across architectures or training regimes. The results empirically confirm that many widely-used LLMs display meaningful geographic preferences even in ostensibly neutral decision contexts.", "conclusion": "The study concludes that regional bias is a significant and measurable issue in current LLMs and can erode fairness, reliability, and inclusiveness in global, cross-cultural deployments. The proposed FAZE framework offers a practical tool for systematically detecting and quantifying geographic bias, and the observed differences across models highlight the need for more inclusive training data, model design choices, and evaluation protocols. The authors argue that future AI fairness work should integrate regional bias assessments and develop mitigation strategies to ensure that LLMs serve users from all regions more equitably."}}
{"id": "2601.16725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16725", "abs": "https://arxiv.org/abs/2601.16725", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Chengcheng Han", "Chenhui Yang", "Chuyu Zhang", "Cong Chen", "Cunguang Wang", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Di Xiu", "Dishan Liu", "Dongyu Ru", "Dunwei Tu", "Fan Wu", "Fengcheng Yuan", "Fengcun Li", "Gang Xu", "Guanyu Wu", "Guoyuan Lin", "Haibin Wang", "Hansi Yang", "Hao Yang", "Haonan Yan", "Haoxiang Ma", "Haoxing Wen", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiacheng Zhang", "Jiahong Zhou", "Jiahuan Li", "Jiaming Wang", "Jian Yang", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiapeng Zhu", "Jiaqi Sun", "Jiarong Shi", "Jiarui Zhao", "Jingang Wang", "Jinluan Yang", "Jinrui Ding", "Jinwei Xiao", "Jiyuan He", "Juncan Xu", "Kefeng Zhang", "Keheng Wang", "Li Wei", "Lianhui Ma", "Lin Qiu", "Lingbing Kong", "Lingchuan Liu", "Linsen Guo", "Mengshen Zhu", "Mengxia Shen", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Pengcheng Jia", "Pengtao Zhang", "Peng Zhao", "Qi Gu", "Qiong Huang", "Qiyuan Duan", "Quanchi Weng", "Rongxiang Weng", "Rongzhi Zhang", "Rumei Li", "Shanglin Lei", "Shengnan An", "Shijun Dai", "Shuaikang Liu", "Shuang Zhou", "Shuo Wang", "Songyuan Zhao", "Tao Liang", "Tianhao Hu", "Tianze Chen", "Wei Liu", "Wei Shi", "Wei Wang", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Wentao Chen", "Wentao Shi", "Xi Su", "Xiangcheng Liu", "Xiandi Ma", "Xiangyu Xi", "Xiangyuan Liu", "Xiangzhou Huang", "Xiao Liu", "Xiaodong Cai", "Xiaolong Chen", "Xiaowei Shi", "Xiaoyu Li", "Xin Chen", "Xingchen Liu", "Xuan Huang", "Xuezhi Cao", "Xunliang Cai", "Yan Chen", "Yang Bai", "Yang Liu", "Yang Yang", "Yang Zheng", "Yaoming Wang", "Yaoming Zhu", "Yaqi Huo", "Yanyu Chen", "Yaorui Shi", "Yerui Sun", "Yi Zhang", "Yihao Chen", "Yi-Kai Zhang", "Yifan Lu", "Yifan Zhao", "Yitao Zhai", "Yongjing Yin", "Yongwei Zhou", "Youshao Xiao", "Yuchuan Dai", "Yuchen Xie", "Yuchen Yu", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunfan Liang", "Yunke Zhao", "Yuwei Jiang", "Yuxin Bian", "Yuxin Chen", "Yuxin Liu", "Yue Xu", "Yueqing Sun", "Zeyang Yu", "Zhao Yang", "Zhengsheng Huang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhimin Lin", "Zhiyuan Yao", "Zhuofan Chen", "Zhuowen Han", "Zijian Zhang", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang"], "title": "LongCat-Flash-Thinking-2601 Technical Report", "comment": null, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "AI": {"tldr": "Introduces LongCat-Flash-Thinking-2601, a 560B-parameter open-source MoE reasoning model that achieves state-of-the-art performance on agentic reasoning benchmarks and robust tool-use in noisy environments, enabled by a unified training framework and a Heavy Thinking test-time mode.", "motivation": "To build an open-source large-scale reasoning model with strong agentic capabilities\u2014such as search, tool use, and tool-integrated reasoning\u2014that not only excels on benchmarks but also generalizes to complex tool interactions and noisy real-world environments, addressing limitations in robustness, scalability, and generalization of existing models.", "method": "Develop a 560B-parameter Mixture-of-Experts model trained via a unified framework that integrates domain-parallel expert training with later fusion; co-design data, environments, algorithms, and infrastructure across pre- and post-training; scale and extend the asynchronous RL framework DORA to handle over 10,000 environments in 20+ domains; systematically analyze real-world noise patterns and incorporate them into training; and introduce a Heavy Thinking test-time mode that scales reasoning depth and width through parallel thinking.", "result": "The resulting model, LongCat-Flash-Thinking-2601, achieves state-of-the-art performance among open-source models on diverse agentic benchmarks (search, tool use, tool-integrated reasoning), exhibits strong generalization to complex multi-tool interactions, maintains robustness in noisy, real-world-like settings, and benefits further from Heavy Thinking mode on complex reasoning tasks.", "conclusion": "A large-scale MoE reasoning model trained with a unified, environment- and data-aware framework, extended large-scale asynchronous RL (DORA), and explicit modeling of real-world noise can yield superior open-source agentic reasoning performance and robustness, and test-time scaling via Heavy Thinking further boosts complex reasoning capabilities."}}
{"id": "2601.16355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16355", "abs": "https://arxiv.org/abs/2601.16355", "authors": ["Suhong Moon", "Minwoo Kang", "Joseph Suh", "Mustafa Safdari", "John Canny"], "title": "Identity, Cooperation and Framing Effects within Groups of Real and Simulated Humans", "comment": null, "summary": "Humans act via a nuanced process that depends both on rational deliberation and also on identity and contextual factors. In this work, we study how large language models (LLMs) can simulate human action in the context of social dilemma games. While prior work has focused on \"steering\" (weak binding) of chat models to simulate personas, we analyze here how deep binding of base models with extended backstories leads to more faithful replication of identity-based behaviors. Our study has these findings: simulation fidelity vs human studies is improved by conditioning base LMs with rich context of narrative identities and checking consistency using instruction-tuned models. We show that LLMs can also model contextual factors such as time (year that a study was performed), question framing, and participant pool effects. LLMs, therefore, allow us to explore the details that affect human studies but which are often omitted from experiment descriptions, and which hamper accurate replication.", "AI": {"tldr": "The paper studies how to more faithfully simulate human behavior in social dilemma games using large language models by deeply conditioning base models with rich narrative identities and contextual factors.", "motivation": "Human behavior in social dilemmas is shaped not only by rational calculation but also by identity and context (time, framing, participant pool). Existing LLM work mostly \u2018steers\u2019 chat models with shallow personas, which may not capture these nuances or replicate human study results well. The authors aim to improve the fidelity of LLM-based human behavior simulations and make them better proxies for human experiments.", "method": "They work with base language models and perform \u2018deep binding\u2019 by conditioning them on extended narrative backstories that encode rich identity information. They then use instruction-tuned models to check and enforce behavioral consistency with these identities. They evaluate the simulations against human studies in social dilemma games, and systematically vary contextual factors (such as study year, question framing, and participant pool characteristics) to see whether the models reproduce known contextual effects observed in human experiments.", "result": "Conditioning base LMs on rich narrative identities leads to higher simulation fidelity compared with human experimental data than previous shallow steering approaches. The models also successfully capture contextual effects in behavior, such as differences due to time period, framing of questions, and participant pool, mirroring patterns seen in human studies.", "conclusion": "Deeply binding LLMs with rich identity narratives and explicitly modeling contextual factors enables them to more faithfully reproduce human behavior in social dilemma games than shallow persona steering. This suggests LLMs can be used as tools to probe and replicate human studies, including aspects of context and identity that are often underspecified in experimental descriptions, potentially improving our understanding and replication of human behavioral research."}}
{"id": "2601.16806", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16806", "abs": "https://arxiv.org/abs/2601.16806", "authors": ["Lu Yihe", "Barbara Webb"], "title": "An Efficient Insect-inspired Approach for Visual Point-goal Navigation", "comment": null, "summary": "In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.", "AI": {"tldr": "Novel insect-inspired visual navigation agent matches SOTA with far less compute and is robust in realistic environments.", "motivation": "To create a computationally efficient, robust agent for visual point-goal navigation by leveraging principles from insect navigation, which is highly efficient and robust despite limited neural resources.", "method": "Design an insect-inspired agent that abstracts two key insect brain structures: one responsible for associative learning and the other for path integration. Map these abstractions onto the Habitat point-goal navigation benchmark, framing it analogously to insects learning visually guided routes between nest and food source around obstacles. Evaluate the agent on standard benchmarks and in more realistic simulated environments.", "result": "The insect-inspired agent achieves performance comparable to recent state-of-the-art models on the Habitat point-goal navigation task while using orders of magnitude less computational resources. In additional tests within a more realistic simulated environment, the agent maintains strong performance under various perturbations.", "conclusion": "Insect-inspired abstractions of associative learning and path integration can deliver competitive point-goal navigation performance with drastically lower computational cost and robust behavior in more realistic, perturbed environments, suggesting a promising bio-inspired direction for efficient navigation agents."}}
{"id": "2601.16853", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16853", "abs": "https://arxiv.org/abs/2601.16853", "authors": ["Ian B. de Haan", "Peter van der Putten", "Max van Duijn"], "title": "Reasoning Promotes Robustness in Theory of Mind Tasks", "comment": "14 pages, 2 figures", "summary": "Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.", "AI": {"tldr": "The paper evaluates how reinforcement-learning-enhanced reasoning LLMs perform on Theory of Mind (ToM) tasks and argues that their apparent ToM gains mainly reflect greater robustness in solving problems, not fundamentally new social-cognitive abilities.", "motivation": "Recent claims that large language models demonstrate strong Theory of Mind raise questions: Are these models truly exhibiting human-like social cognition or just exploiting patterns in text and prompts? Meanwhile, a newer family of reasoning-focused LLMs trained with reinforcement learning and verifiable rewards has achieved substantial improvements on many benchmarks, but it is unclear whether these improvements translate into deeper ToM capabilities or simply more reliable problem solving. The authors aim to clarify what exactly is improved in these models when evaluated on ToM-style tasks.", "method": "The authors study \"reasoning\" LLMs that have been further trained with reinforcement learning from verifiable rewards (RLVR). They evaluate these models on Theory of Mind tasks using (1) adapted machine versions of psychological ToM experiments and (2) existing ToM-related benchmarks. They then systematically vary prompts and perturb task formulations to test robustness\u2014e.g., rewording problems, changing surface details, or introducing minor variations\u2014to see whether performance gains persist and how sensitive the models are to such changes compared with non-RLVR baselines.", "result": "The reasoning-oriented RLVR models show consistently better robustness than baseline LLMs: they maintain higher performance when prompts are rephrased, tasks are perturbed, or minor variations are introduced. These models more reliably converge on the correct answers across a range of ToM tasks and benchmark settings, indicating improved stability and accuracy under variation rather than sporadic success dependent on specific phrasings.", "conclusion": "The enhanced performance of RLVR reasoning models on Theory of Mind tasks is best explained by increased robustness in deriving correct solutions rather than by the emergence of qualitatively new or more human-like ToM reasoning abilities. Consequently, the authors argue that current ToM evaluations of LLMs risk over-attributing social-cognitive competence when they may primarily be measuring general reasoning robustness and prompt insensitivity. They highlight the need for more careful task design and interpretation when using ToM-style tests to assess social cognition in language models."}}
{"id": "2601.16390", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16390", "abs": "https://arxiv.org/abs/2601.16390", "authors": ["Rhitabrat Pokharel", "Ameeta Agrawal", "Tanay Nagar"], "title": "Cross-Lingual Activation Steering for Multilingual Language Models", "comment": "Under review", "summary": "Large language models exhibit strong multilingual capabilities, yet significant performance gaps persist between dominant and non-dominant languages. Prior work attributes this gap to imbalances between shared and language-specific neurons in multilingual representations. We propose Cross-Lingual Activation Steering (CLAS), a training-free inference-time intervention that selectively modulates neuron activations. We evaluate CLAS on classification and generation benchmarks, achieving average improvements of 2.3% (Acc.) and 3.4% (F1) respectively, while maintaining high-resource language performance. We discover that effective transfer operates through functional divergence rather than strict alignment; performance gains correlate with increased language cluster separation. Our results demonstrate that targeted activation steering can unlock latent multilingual capacity in existing models without modification to model weights.", "AI": {"tldr": "They propose a training-free technique (CLAS) that adjusts neuron activations at inference time to boost performance for low-resource languages in multilingual LLMs, improving accuracy and F1 without hurting high-resource languages.", "motivation": "Multilingual LLMs perform worse on non-dominant (low-resource) languages, and this gap is thought to stem from imbalances between shared and language-specific neurons. There is a need for methods that improve low-resource performance without retraining or changing model weights.", "method": "They introduce Cross-Lingual Activation Steering (CLAS), which intervenes during inference by selectively modulating neuron activations associated with cross-lingual transfer. CLAS is applied to existing multilingual models without further training. They evaluate the method on both classification and generation benchmarks, and analyze representation geometry, such as language cluster separation.", "result": "On multilingual classification tasks, CLAS yields an average accuracy improvement of 2.3%, and on generation tasks it yields an average F1 improvement of 3.4% for non-dominant languages, while keeping performance on high-resource languages largely unchanged. Representation analysis shows that CLAS increases separation between language-specific clusters.", "conclusion": "Targeted manipulation of internal activations at inference time can unlock latent multilingual abilities in LLMs. Effective transfer seems to rely on functional divergence among language representations, not strict alignment, and CLAS offers a practical, training-free way to narrow performance gaps for low-resource languages without modifying model parameters."}}
{"id": "2601.16863", "categories": ["cs.AI", "cs.LG", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16863", "abs": "https://arxiv.org/abs/2601.16863", "authors": ["Tims Pecerskis", "Aivars Smirnovs"], "title": "Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation", "comment": null, "summary": "This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.", "AI": {"tldr": "Proposes NSED, a runtime mixture-of-models protocol that coordinates multiple small models via dynamic optimization and macro-scale deliberative recurrence to match large-model performance and improve safety.", "motivation": "Large monolithic models (100B+ parameters) are costly and hard to deploy on consumer hardware, and static Mixture-of-Experts architectures underuse heterogeneous models and lack adaptive runtime optimization and robust safety alignment. The paper aims to enable ensembles of smaller, heterogeneous models to achieve or surpass large-model performance while improving alignment and efficiency.", "method": "Introduce the N-Way Self-Evaluating Deliberation (NSED) protocol: (1) a Runtime Mixture-of-Models (MoM) architecture that builds composite systems from multiple expert agents; (2) a Dynamic Expertise Broker that formulates model selection and routing as a Knapsack-like optimization problem under cost and telemetry constraints; (3) a Macro-Scale RNN formalization of deliberation, where consensus states recur through a semantic forget gate to allow iterative refinement without linear VRAM growth; (4) an orchestration fabric enabling trustless N-to-N peer review between agents; (5) a Quadratic Voting-style activation function to aggregate non-linear consensus; and (6) feedback-driven state updates that adapt routing and roles over time.", "result": "On benchmarks such as AIME 2025 and LiveCodeBench, ensembles of sub-20B consumer-grade models using NSED match or exceed the performance of state-of-the-art 100B+ parameter models, indicating a strong efficiency and performance tradeoff. On the DarkBench safety suite, the protocol exhibits emergent alignment benefits: peer-mediated correction among agents decreases sycophancy scores below that of any single constituent model.", "conclusion": "NSED demonstrates that a dynamically orchestrated, deliberative ensemble of small heterogeneous models can rival or outperform very large models while operating within consumer hardware constraints. Its macro-scale recurrent deliberation, optimization-based model selection, and peer-review consensus mechanisms not only improve hardware efficiency but also yield intrinsic safety benefits such as reduced sycophancy, suggesting a promising new direction for scalable, aligned multi-model AI systems."}}
{"id": "2601.16397", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16397", "abs": "https://arxiv.org/abs/2601.16397", "authors": ["Qianqi Yan", "Huy Nguyen", "Sumana Srivatsa", "Hari Bandi", "Xin Eric Wang", "Krishnaram Kenthapadi"], "title": "Cite-While-You-Generate: Training-Free Evidence Attribution for Multimodal Clinical Summarization", "comment": null, "summary": "Trustworthy clinical summarization requires not only fluent generation but also transparency about where each statement comes from. We propose a training-free framework for generation-time source attribution that leverages decoder attentions to directly cite supporting text spans or images, overcoming the limitations of post-hoc or retraining-based methods. We introduce two strategies for multimodal attribution: a raw image mode, which directly uses image patch attentions, and a caption-as-span mode, which substitutes images with generated captions to enable purely text-based alignment. Evaluations on two representative domains: clinician-patient dialogues (CliConSummation) and radiology reports (MIMIC-CXR), show that our approach consistently outperforms embedding-based and self-attribution baselines, improving both text-level and multimodal attribution accuracy (e.g., +15% F1 over embedding baselines). Caption-based attribution achieves competitive performance with raw-image attention while being more lightweight and practical. These findings highlight attention-guided attribution as a promising step toward interpretable and deployable clinical summarization systems.", "AI": {"tldr": "The paper proposes a training-free, attention-based method to attribute each part of clinical summaries to specific source text spans or images, improving transparency and accuracy in multimodal clinical summarization.", "motivation": "Clinical summarization models need to be trustworthy and interpretable, especially in healthcare settings. Existing attribution methods are either post-hoc, require retraining, or do not handle multimodal inputs well, making it hard to see exactly which part of the source (dialogue, report, or image) supports each generated statement.", "method": "The authors introduce a training-free, generation-time source attribution framework that uses decoder attention weights to directly link each generated token to supporting source spans or image regions. They present two multimodal strategies: (1) raw image mode, which uses attention over image patches for direct visual-text alignment; and (2) caption-as-span mode, where each image is replaced by its generated caption so that attribution becomes a purely text-span alignment problem. They then compare these against embedding-based and self-attribution baselines.", "result": "On two clinical domains\u2014clinician-patient dialogues (CliConSummation) and radiology reports (MIMIC-CXR)\u2014their attention-guided approach improves both text-only and multimodal attribution accuracy, achieving gains such as a 15% F1 improvement over embedding-based baselines. Caption-based attribution performs similarly to raw-image attention while being more lightweight and practical to deploy.", "conclusion": "Decoder-attention-guided, training-free attribution is an effective and practical way to provide fine-grained source citations for clinical summarization, including multimodal settings. Caption-based attribution offers a strong trade-off between performance and efficiency, suggesting that these techniques can enhance the interpretability and deployability of clinical summarization systems in real-world healthcare applications."}}
{"id": "2601.16886", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16886", "abs": "https://arxiv.org/abs/2601.16886", "authors": ["Chi Yu", "Hongyu Yuan", "Zhiyi Duan"], "title": "MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion", "comment": null, "summary": "Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.", "AI": {"tldr": "They propose MAGE-KT, a multi-agent, graph-based framework that builds and prunes heterogeneous knowledge graphs to better model relations among students, questions, and concepts, improving next-question prediction.", "motivation": "Existing Knowledge Tracing methods, including recent graph-based ones, inadequately capture rich inter-concept relationships and suffer from scalability and noise when encoding large heterogeneous graphs. Inter-concept relations are often inferred only from sequential interactions, missing semantic signals, while full-graph encoding is computationally expensive and causes attention to spread to irrelevant nodes, degrading representation quality and hurting prediction accuracy.", "method": "They introduce MAGE-KT, which first employs a multi-agent knowledge-concept (KC) relation extractor to construct a semantic KC-relation view, and combines it with a student-question interaction graph to form a multi-view heterogeneous graph. For each target student, the model retrieves compact, task-relevant subgraphs conditioned on that student\u2019s history. An Asymmetric Cross-attention Fusion Module then integrates information from these subgraphs, emphasizing high-value relations while suppressing irrelevant regions and mitigating attention diffusion, and the resulting representations are used for next-question performance prediction.", "result": "On three standard KT benchmarks, MAGE-KT achieves substantially higher accuracy in identifying correct KC relations and shows clear, consistent gains in next-question performance prediction compared to prior KT and graph-based KT approaches.", "conclusion": "By jointly leveraging semantic KC relations and behavioral interaction data in a multi-view heterogeneous graph, and by focusing computation on compact, student-specific subgraphs fused via asymmetric cross-attention, MAGE-KT more faithfully captures inter-concept and student-question relationships, leading to more accurate knowledge tracing and demonstrating the value of targeted graph extraction and fusion in large-scale KT graphs."}}
{"id": "2601.16400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16400", "abs": "https://arxiv.org/abs/2601.16400", "authors": ["Zongwan Cao", "Bingbing Wen", "Lucy Lu Wang"], "title": "Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification", "comment": null, "summary": "Real-world visual question answering (VQA) is often context-dependent: an image-question pair may be under-specified, such that the correct answer depends on external information that is not observable in the image. In such cases, directly answering can lead to confident but incorrect predictions. We propose CoA(Clarify-or-Answer), an ask-or-answer agent that separately models the decision to ask or answer, and what to ask if needed. CoA first determines whether clarification is necessary; if so, it asks a single focused question and then incorporates the response to produce the final answer. We introduce CONTEXTCLARIFY with a set of ambiguous VQA questions and the contrast set that is non-ambiguous. We further introduce GRPO-CR (Clarification Reasoning), a reinforcement learning approach that optimizes clarification question generation with multiple reward signals encouraging well-formed, focused, non-trivial questions that resolve ambiguity. Across three VLLMs and three datasets, CoA achieves consistent improvements at both the module and system levels, improving end-to-end VQA accuracy by an average of +15.3 points (83%) over prompting-based baselines", "AI": {"tldr": "They build an ask-or-answer VQA agent (CoA) that decides when to ask a clarifying question, learns to generate good clarification questions with RL, and substantially improves VQA accuracy on ambiguous, context-dependent queries.", "motivation": "Real-world VQA tasks often involve under-specified, context-dependent questions where the image alone is insufficient to determine the correct answer. Existing VQA systems answer directly, leading to confident but wrong responses on ambiguous questions. There is a need for agents that can recognize ambiguity, ask targeted clarification questions, and use the answers to improve reliability and accuracy.", "method": "They propose CoA (Clarify-or-Answer), an agent with two key decisions: (1) whether to answer directly or ask for clarification; (2) if asking, what single focused clarification question to pose. They introduce the CONTEXTCLARIFY dataset containing ambiguous VQA questions paired with non-ambiguous contrast questions. For training the clarifying question generator, they design GRPO-CR, a reinforcement learning method that uses multiple reward signals to favor well-formed, focused, non-trivial clarification questions that actually resolve ambiguity. They evaluate CoA with three vision-language models and three datasets, measuring both the performance of the clarification module and overall VQA accuracy.", "result": "CoA shows consistent improvements across all tested settings. When integrated with three different vision-language models and evaluated on three datasets, CoA raises end-to-end VQA accuracy by an average of +15.3 percentage points (an 83% relative improvement) over prompting-based baselines. Both the clarification question generation module and the full system perform better than baseline methods.", "conclusion": "Explicitly modeling the decision to clarify versus answer, and training clarification questions with reinforcement learning on ambiguity-focused data, substantially improves the robustness and accuracy of VQA systems on real-world, context-dependent queries. Clarification-capable agents like CoA are an effective way to handle under-specified visual questions and reduce confidently wrong answers."}}
{"id": "2601.16909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16909", "abs": "https://arxiv.org/abs/2601.16909", "authors": ["Lei You", "Lele Cao", "Iryna Gurevych"], "title": "Preventing the Collapse of Peer Review Requires Verification-First AI", "comment": null, "summary": "This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.", "AI": {"tldr": "The paper argues that AI tools in peer review should prioritize verifying scientific claims over mimicking human scoring, introduces a formal notion of how well review scores track scientific truth, and shows when systems shift from truth-seeking to gaming proxies, leading to recommendations for using AI as an adversarial verifier.", "motivation": "As AI becomes integrated into scientific peer review, there is a risk that tools focused on predicting review scores will exacerbate existing misalignments between review outcomes and actual scientific truth. Current peer review already struggles when claims grow faster than verification capacity and when genuine improvements are hard to distinguish from noise. The authors are motivated to understand, model, and ultimately improve how AI can be used to increase the alignment between review decisions and the underlying truth of scientific claims, rather than accelerating proxy gaming and claim inflation.", "method": "The paper provides a conceptual and mathematical analysis rather than an empirical system. It introduces the concept of truth-coupling, formalizes two key forces\u2014verification pressure and signal shrinkage\u2014and builds a minimal theoretical model where high-fidelity checks are rare but proxy-based judgments are frequent. Within this model, the authors derive an explicit coupling law linking observable scores to latent truth and characterize an incentive-collapse condition under which agents rationally shift their efforts away from genuine truth-seeking toward optimizing observable proxies. The analysis is used to compare different designs of AI review tools: AI as a score predictor versus AI as an adversarial verification assistant.", "result": "The theoretical model yields two main results: (1) an explicit law describing how tightly evaluation scores are coupled to latent scientific truth when both occasional high-fidelity verification and frequent proxy judgments are present; and (2) an incentive-collapse condition showing when rational researchers will optimally focus on gaming the evaluation proxy instead of pursuing genuine scientific advances, even in regimes where the system\u2019s decisions still look superficially reliable. These results identify parameter regimes where reliance on proxy judgments leads to a phase transition toward proxy-sovereign evaluation, in which the proxy essentially displaces truth as the primary optimization target.", "conclusion": "The paper concludes that AI should not be deployed in peer review as a mere predictor of scores or accept/reject decisions, since that increases the risk of proxy optimization and claim inflation. Instead, AI should be designed and used as a verification-first tool\u2014an adversarial auditor that produces explicit, auditable verification artifacts and effectively expands the system\u2019s verification bandwidth. By doing so, AI can strengthen the coupling between review decisions and latent scientific truth, mitigate the transition to proxy-sovereign evaluation, and better align incentives for authors, reviewers, and venues with genuine scientific progress."}}
{"id": "2601.16407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16407", "abs": "https://arxiv.org/abs/2601.16407", "authors": ["Toni J. B. Liu", "Baran Zadeo\u011flu", "Nicolas Boull\u00e9", "Rapha\u00ebl Sarfati", "Christopher J. Earls"], "title": "Jacobian Scopes: token-level causal attributions in LLMs", "comment": "12 pages, 15 figures, under review at ACL 2026", "summary": "Large language models (LLMs) make next-token predictions based on clues present in their context, such as semantic descriptions and in-context examples. Yet, elucidating which prior tokens most strongly influence a given prediction remains challenging due to the proliferation of layers and attention heads in modern architectures. We propose Jacobian Scopes, a suite of gradient-based, token-level causal attribution methods for interpreting LLM predictions. By analyzing the linearized relations of final hidden state with respect to inputs, Jacobian Scopes quantify how input tokens influence a model's prediction. We introduce three variants - Semantic, Fisher, and Temperature Scopes - which respectively target sensitivity of specific logits, the full predictive distribution, and model confidence (inverse temperature). Through case studies spanning instruction understanding, translation and in-context learning (ICL), we uncover interesting findings, such as when Jacobian Scopes point to implicit political biases. We believe that our proposed methods also shed light on recently debated mechanisms underlying in-context time-series forecasting. Our code and interactive demonstrations are publicly available at https://github.com/AntonioLiu97/JacobianScopes.", "AI": {"tldr": "The paper introduces Jacobian Scopes, gradient-based token-level attribution methods to identify which input tokens most influence large language model predictions.", "motivation": "Understanding which specific tokens in the input context drive an LLM\u2019s predictions is difficult due to the complexity of multi-layer, multi-head Transformer architectures. Existing interpretability tools do not provide clear, token-level causal attributions aligned with how LLMs use context for semantic reasoning and in-context learning, especially for sensitive behaviors such as bias or hidden mechanisms like time-series forecasting in context.", "method": "The authors propose Jacobian Scopes, which analyze the Jacobian (gradients) of the final hidden state with respect to input token representations, effectively linearizing the model around a prediction to quantify token-level influence. They define three variants: (1) Semantic Scopes, focusing on sensitivity of particular logits (i.e., specific next-token hypotheses); (2) Fisher Scopes, considering sensitivity over the entire predictive distribution (using a Fisher-information-style metric); and (3) Temperature Scopes, which measure how tokens affect model confidence, operationalized as an inverse-temperature parameter. They apply these to various LLM tasks and visualize token attributions.", "result": "Across case studies in instruction following, translation, and in-context learning on time series, Jacobian Scopes highlight which tokens the model relies on for specific predictions. The methods expose patterns such as reliance on particular instructions, contextual cues, or training examples, and reveal the presence of implicit biases as indicated by which tokens most affect certain outputs. They also provide evidence relevant to competing explanations for how LLMs perform in-context time-series forecasting.", "conclusion": "Jacobian Scopes offer a flexible, gradient-based framework for token-level causal attribution in LLMs, enabling more precise interpretability of next-token predictions. The three scope variants allow probing of different aspects of model behavior\u2014logit-level semantics, overall distributional sensitivity, and confidence. The authors argue that these tools can clarify mechanisms underlying in-context learning and uncover problematic behaviors such as biases, and they release code and interactive demos to facilitate further analysis and practical use."}}
{"id": "2601.16419", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16419", "abs": "https://arxiv.org/abs/2601.16419", "authors": ["Qinglong Cao", "Yuntian Chen", "Chao Ma", "Xiaokang Yang"], "title": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities in multimodal perception and understanding tasks. However, their effectiveness in specialized domains, such as remote sensing and medical imaging, remains limited. A natural approach to domain adaptation is to inject domain knowledge through textual instructions, prompts, or auxiliary captions. Surprisingly, we find that such input-level domain knowledge injection yields little to no improvement on scientific multimodal tasks, even when the domain knowledge is explicitly provided. This observation suggests that current MLLMs fail to internalize domain-specific priors through language alone, and that domain knowledge must be integrated at the optimization level. Motivated by this insight, we propose a reinforcement fine-tuning framework that incorporates domain knowledge directly into the learning objective. Instead of treating domain knowledge as descriptive information, we encode it as domain-informed constraints and reward signals, shaping the model's behavior in the output space. Extensive experiments across multiple datasets in remote sensing and medical domains consistently demonstrate good performance gains, achieving state-of-the-art results on multimodal domain tasks. Our results highlight the necessity of optimization-level domain knowledge integration and reveal a fundamental limitation of textual domain conditioning in current MLLMs.", "AI": {"tldr": "The paper shows that simply adding domain-specific text prompts or instructions to multimodal large language models (MLLMs) does not significantly improve performance on specialized scientific domains like remote sensing and medical imaging. Instead, the authors propose integrating domain knowledge directly into the training objective using a reinforcement-based fine-tuning framework, which yields state-of-the-art results.", "motivation": "While MLLMs perform well on general multimodal tasks, they struggle in specialized scientific domains where domain-specific priors are crucial. A common strategy is to inject domain knowledge via textual instructions, prompts, or captions, but the effectiveness of this approach has not been rigorously evaluated. The authors are motivated to understand whether language alone is sufficient for domain adaptation and, if not, how to more effectively incorporate domain knowledge into MLLMs for scientific tasks.", "method": "The authors first empirically test the impact of input-level domain knowledge injection\u2014using domain-specific prompts, instructions, or auxiliary captions\u2014on scientific multimodal tasks. Observing limited gains, they design a reinforcement fine-tuning framework that encodes domain knowledge as constraints and reward signals in the training objective. Instead of treating domain information as passive text, they use it to actively shape the model\u2019s output behavior via domain-informed rewards and constraints, effectively integrating domain priors at the optimization level.", "result": "Experiments across multiple datasets in remote sensing and medical imaging show that input-level domain knowledge (prompts/instructions/captions) leads to negligible improvements, even when domain knowledge is explicitly provided. In contrast, the proposed reinforcement fine-tuning framework consistently improves performance and achieves state-of-the-art results on various multimodal domain benchmarks, demonstrating the effectiveness of optimization-level domain knowledge integration.", "conclusion": "The study concludes that current MLLMs cannot reliably internalize domain-specific priors through textual conditioning alone, revealing a fundamental limitation of prompt-based domain adaptation. Optimization-level integration of domain knowledge\u2014by encoding it as constraints and rewards in a reinforcement-style fine-tuning setup\u2014is necessary to obtain significant performance gains in specialized domains such as remote sensing and medical imaging."}}
{"id": "2601.16965", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16965", "abs": "https://arxiv.org/abs/2601.16965", "authors": ["Riyang Bao", "Cheng Yang", "Dazhou Yu", "Zhexiang Tang", "Gengchen Mai", "Liang Zhao"], "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts", "comment": "15pages, 4 figures", "summary": "Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.", "AI": {"tldr": "An AI agent, Spatial-Agent, is introduced to perform genuine geospatial computation by transforming natural language questions into executable geospatial workflows (GeoFlow Graphs), achieving better accuracy and interpretability than existing LLM-based agents.", "motivation": "Existing LLM-based agents struggle with real geospatial reasoning, often hallucinating spatial relationships and depending on web search or pattern matching. There is a need for a principled, theory-grounded framework that can convert natural language geospatial questions into correct, executable analytical processes for applications like urban analytics, transportation planning, and disaster response.", "method": "The paper formalizes geo-analytical question answering as a concept transformation problem. It parses natural language questions into executable workflows called GeoFlow Graphs\u2014directed acyclic graphs where nodes represent spatial concepts and edges denote transformations. Using spatial information theory, Spatial-Agent identifies spatial concepts, assigns them functional roles under ordering constraints, and then composes valid transformation sequences via template-based generation to produce interpretable workflows.", "result": "On the MapEval-API and MapQA benchmarks, Spatial-Agent substantially outperforms baseline LLM-agent methods such as ReAct and Reflexion, both in answer quality and in the correctness of produced workflows. The generated GeoFlow Graphs are executable and interpretable, validating the effectiveness of the concept-transformation formulation.", "conclusion": "Grounding an LLM-based agent in spatial information theory and modeling geo-analytical QA as a concept transformation problem enables reliable geospatial reasoning. Spatial-Agent offers a practical way to convert natural language questions into accurate, interpretable, and executable geospatial workflows, outperforming existing heuristic or pattern-matching approaches on standard benchmarks."}}
{"id": "2601.16444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16444", "abs": "https://arxiv.org/abs/2601.16444", "authors": ["Ayako Sato", "Hwichan Kim", "Zhousi Chen", "Masato Mita", "Mamoru Komachi"], "title": "Exploring the Effects of Alignment on Numerical Bias in Large Language Models", "comment": "Accepted at AIBSD 2026 (Workshop at AAAI 2026)", "summary": "``LLM-as-a-judge,'' which utilizes large language models (LLMs) as evaluators, has proven effective in many evaluation tasks. However, evaluator LLMs exhibit numerical bias, a phenomenon where certain evaluation scores are generated disproportionately often, leading reduced evaluation performance. This study investigates the cause of this bias. Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment. To test this, we compare outputs from pre- and post-alignment LLMs, and observe that alignment indeed increases numerical bias. We also explore mitigation strategies for post-alignment LLMs, including temperature scaling, distribution calibration, and score range adjustment. Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic. Our findings highlight the need for further work on optimal score range selection and more robust mitigation strategies.", "AI": {"tldr": "The paper finds that using LLMs as evaluators suffers from numerical bias in scores, and shows that alignment processes increase this bias; it evaluates simple mitigation methods and finds score range adjustment works best but remains heuristic.", "motivation": "LLMs are widely used as automatic judges for tasks like grading answers or evaluating generation quality, but their evaluation scores cluster around certain numbers, hurting reliability. The authors want to understand where this numerical bias comes from and how to reduce it, because unbiased and accurate automatic evaluation is important for research and applications.", "method": "The authors compare pre-alignment and post-alignment versions of LLMs to see how instruction and preference tuning affect the distribution of evaluation scores. They measure how often each score is used and how this correlates with evaluation performance. They then implement and test several mitigation strategies on post-alignment models: adjusting temperature, calibrating the score distribution, and modifying the available score range, and they compare their impact on both bias and evaluation quality.", "result": "They find that post-alignment LLMs show stronger numerical bias than pre-alignment versions, supporting the hypothesis that alignment causes increased score concentration on certain numbers. Among mitigation methods, score range adjustment reduces numerical bias the most and yields the best improvements in evaluation performance, while other methods like temperature scaling and distribution calibration are less effective.", "conclusion": "Alignment processes, though useful for making LLMs follow instructions and human preferences, also increase numerical bias when the models are used as evaluators. Simple post-hoc techniques, especially adjusting the score range, can partially mitigate this issue but are heuristic and not fully satisfactory. The work underscores the need for principled methods to choose scoring ranges and for more robust strategies to control or remove numerical bias in LLM-based evaluation systems."}}
{"id": "2601.16967", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16967", "abs": "https://arxiv.org/abs/2601.16967", "authors": ["Bernes Lorier Atabonfack", "Ahmed Tahiru Issah", "Mohammed Hardi Abdul Baaki", "Clemence Ingabire", "Tolulope Olusuyi", "Maruf Adewole", "Udunna C. Anazodo", "Timothy X Brown"], "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians", "comment": "Accepted at the MIRASOL Workshop at MICCAI 2025. To appear in Lecture Notes in Computer Science (LNCS)", "summary": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.", "AI": {"tldr": "The paper develops and validates an AI-powered support platform that uses a large language model to help technicians in low- and middle-income countries troubleshoot and repair medical devices, showing high precision on a proof-of-concept ultrasound system.", "motivation": "In many low- and middle-income countries, a large share of medical diagnostic equipment is idle or broken because of slow or absent maintenance, lack of technical expertise, and poor manufacturer support\u2014especially for donated or third\u2011party devices. This leads to long equipment downtime, delayed diagnostics, and worse patient care. The authors want a scalable way to provide expert-level maintenance guidance without always needing on-site manufacturer engineers.", "method": "The authors design an AI-powered support platform that integrates a large language model with a web interface. Imaging technologists and biomedical technicians can enter error codes or describe symptoms, and the system returns structured, step-by-step troubleshooting guidance. The platform is complemented by a global peer-to-peer forum for sharing experiences and solutions. They build a proof of concept centered on the Philips HDI 5000 ultrasound machine and evaluate the system\u2019s performance on interpreting error codes and suggesting fixes.", "result": "In the proof-of-concept evaluation on the Philips HDI 5000 ultrasound machine, the system achieved 100% precision in interpreting error codes and 80% accuracy in recommending appropriate corrective actions. This indicates that the AI platform can reliably map device errors to their meanings and often provide correct repair guidance, at least within the tested scope.", "conclusion": "The study concludes that AI-driven platforms using large language models are a feasible and promising tool to support medical device maintenance in low-resource settings. By giving technicians real-time, stepwise troubleshooting assistance and enabling global knowledge sharing, such systems could substantially reduce equipment downtime and help improve healthcare delivery in resource-constrained environments."}}
{"id": "2601.16447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16447", "abs": "https://arxiv.org/abs/2601.16447", "authors": ["Yichuan Ma", "Linyang Li", "Yongkang Chen", "Peiji Li", "Jiasheng Ye", "Qipeng Guo", "Dahua Lin", "Kai Chen"], "title": "Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go", "comment": "Accepted to NeurIPS 2025", "summary": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks such as mathematics and coding, matching or surpassing human capabilities. However, these impressive reasoning abilities face significant challenges in specialized domains. Taking Go as an example, although AlphaGo has established the high performance ceiling of AI systems in Go, mainstream LLMs still struggle to reach even beginner-level proficiency, let alone perform natural language reasoning. This performance gap between general-purpose LLMs and domain experts is significantly limiting the application of LLMs on a wider range of domain-specific tasks. In this work, we aim to bridge the divide between LLMs' general reasoning capabilities and expert knowledge in domain-specific tasks. We perform mixed fine-tuning with structured Go expertise and general long Chain-of-Thought (CoT) reasoning data as a cold start, followed by reinforcement learning to integrate expert knowledge in Go with general reasoning capabilities. Through this methodology, we present \\textbf{LoGos}, a powerful LLM that not only maintains outstanding general reasoning abilities, but also conducts Go gameplay in natural language, demonstrating effective strategic reasoning and accurate next-move prediction. LoGos achieves performance comparable to human professional players, substantially surpassing all existing LLMs. Through this work, we aim to contribute insights on applying general LLM reasoning capabilities to specialized domains. We will release the first large-scale Go dataset for LLM training, the first LLM Go evaluation benchmark, and the first general LLM that reaches human professional-level performance in Go at: https://github.com/Entarochuan/LoGos.", "AI": {"tldr": "They build LoGos, a Go-specialized large language model that combines general chain-of-thought reasoning with Go expert knowledge to reach human professional-level Go play in natural language.", "motivation": "General-purpose LLMs are strong at math and coding reasoning but perform poorly in specialized domains like the game of Go, far below expert systems such as AlphaGo and even below human beginners. This gap limits the usefulness of LLMs for domain-specific tasks that require deep, structured expertise. The authors want to understand how to transfer and augment LLMs\u2019 general reasoning skills so they can operate at an expert level within a specialized domain and communicate their reasoning in natural language.", "method": "They first perform mixed supervised fine-tuning of a base LLM using two types of data: (1) structured Go expertise (expert game data and domain-specific annotations) and (2) general long Chain-of-Thought (CoT) reasoning data to preserve and leverage broad reasoning abilities. After this cold-start fine-tuning, they apply reinforcement learning, where the LLM\u2019s Go play and reasoning are further optimized to integrate professional Go knowledge with general reasoning, improving both move prediction and natural language strategic explanation. They also build a large-scale Go dataset for LLM training and design an LLM-specific Go evaluation benchmark.", "result": "The resulting model, LoGos, can play Go via natural language interaction, providing strategic reasoning and accurate next-move predictions. It maintains strong general reasoning capabilities while reaching Go performance comparable to human professional players and significantly outperforming existing LLMs on Go-related tasks, according to their new benchmark.", "conclusion": "By combining mixed fine-tuning on structured domain data with general CoT reasoning and subsequent reinforcement learning, it is possible to transform a general-purpose LLM into an expert-level system in a specialized domain without sacrificing its broad reasoning skills. LoGos shows that LLMs can attain human professional-level Go performance and explain their play in natural language, suggesting a general recipe for extending LLM capabilities to other complex, expert-driven domains. The authors also contribute datasets and a benchmark to support future research in this direction."}}
{"id": "2601.16462", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16462", "abs": "https://arxiv.org/abs/2601.16462", "authors": ["Zhenghao Liu", "Mingyan Wu", "Xinze Li", "Yukun Yan", "Shuo Wang", "Cheng Yang", "Minghe Yu", "Zheni Zeng", "Maosong Sun"], "title": "Graph-Anchored Knowledge Indexing for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a dominant paradigm for mitigating hallucinations in Large Language Models (LLMs) by incorporating external knowledge. Nevertheless, effectively integrating and interpreting key evidence scattered across noisy documents remains a critical challenge for existing RAG systems. In this paper, we propose GraphAnchor, a novel Graph-Anchored Knowledge Indexing approach that reconceptualizes graph structures from static knowledge representations into active, evolving knowledge indices. GraphAnchor incrementally updates a graph during iterative retrieval to anchor salient entities and relations, yielding a structured index that guides the LLM in evaluating knowledge sufficiency and formulating subsequent subqueries. The final answer is generated by jointly leveraging all retrieved documents and the final evolved graph. Experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of GraphAnchor, and reveal that GraphAnchor modulates the LLM's attention to more effectively associate key information distributed in retrieved documents. All code and data are available at https://github.com/NEUIR/GraphAnchor.", "AI": {"tldr": "GraphAnchor is a graph-anchored, evolving knowledge index for RAG that iteratively builds and updates a graph over retrieved evidence, helping LLMs connect scattered information and answer multi-hop questions more accurately.", "motivation": "Standard RAG systems struggle when evidence is scattered across many noisy documents. They lack mechanisms to explicitly structure and track entities and relations across retrieval steps, which leads to missed connections, poor multi-hop reasoning, and lingering hallucinations. The paper aims to design a mechanism that can organize retrieved knowledge into a dynamic structure that better reflects salient entities/relations and can guide subsequent retrieval and reasoning.", "method": "The authors introduce GraphAnchor, which treats a graph not as a static knowledge base but as an evolving knowledge index. During iterative retrieval, the system: (1) extracts salient entities and relations from the question and initially retrieved documents; (2) incrementally updates a graph, adding/updating nodes and edges to represent important concepts and their connections; (3) uses this graph to assess knowledge sufficiency and to formulate refined subqueries for the next retrieval round; and (4) at the end, feeds both all retrieved documents and the final evolved graph into the LLM to generate the final answer. The graph thus anchors the reasoning process and modulates the LLM\u2019s attention across the evidence.", "result": "On four multi-hop QA benchmarks, GraphAnchor outperforms standard RAG baselines. The experiments show higher accuracy and better multi-hop reasoning, and analysis of attention patterns indicates that the LLM focuses more on key, graph-anchored evidence rather than being distracted by noisy or irrelevant content in the retrieved documents.", "conclusion": "GraphAnchor demonstrates that treating graphs as dynamic, evolving knowledge indices within RAG can significantly improve multi-hop QA. By incrementally anchoring salient entities and relations and using the graph to drive retrieval and reasoning, the approach helps LLMs better integrate dispersed evidence and reduce hallucinations. The method is empirically effective and is released with open-source code and data for further research."}}
{"id": "2601.16466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16466", "abs": "https://arxiv.org/abs/2601.16466", "authors": ["Jivnesh Sandhan", "Fei Cheng", "Tushar Sandhan", "Yugo Murawaki"], "title": "Persona Jailbreaking in Large Language Models", "comment": "Accepted at EACL26 (Findings)", "summary": "Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational history alone can reshape induced personas. Black-box persona manipulation remains unexplored, raising concerns for robustness in realistic interactions. In response, we introduce the task of persona editing, which adversarially steers LLM traits through user-side inputs under a black-box, inference-only setting. To this end, we propose PHISH (Persona Hijacking via Implicit Steering in History), the first framework to expose a new vulnerability in LLM safety that embeds semantically loaded cues into user queries to gradually induce reverse personas. We also define a metric to quantify attack success. Across 3 benchmarks and 8 LLMs, PHISH predictably shifts personas, triggers collateral changes in correlated traits, and exhibits stronger effects in multi-turn settings. In high-risk domains mental health, tutoring, and customer support, PHISH reliably manipulates personas, validated by both human and LLM-as-Judge evaluations. Importantly, PHISH causes only a small reduction in reasoning benchmark performance, leaving overall utility largely intact while still enabling significant persona manipulation. While current guardrails offer partial protection, they remain brittle under sustained attack. Our findings expose new vulnerabilities in personas and highlight the need for context-resilient persona in LLMs. Our codebase and dataset is available at: https://github.com/Jivnesh/PHISH", "AI": {"tldr": "The paper introduces PHISH, a black-box persona editing attack that adversarially manipulates LLM personas via subtle cues in conversation history, revealing a new robustness vulnerability with minimal impact on task performance.", "motivation": "LLMs are increasingly used in sensitive domains like education, mental health, and customer support, where maintaining a stable, reliable persona is crucial for trust and safety. However, current research largely examines explicit role-playing or narrative prompts, not how adversarial multi-turn conversations alone can covertly reshape an LLM\u2019s induced persona. There is little understanding of how vulnerable real-world, black-box deployed models are to user-side attempts to steer or hijack personas purely through dialogue, which poses significant safety and robustness concerns.", "method": "The authors formalize a new task called persona editing, where an adversary attempts to steer an LLM\u2019s traits and behavior through only user-provided conversational history under a black-box, inference-only setting. They propose PHISH (Persona Hijacking via Implicit Steering in History), a framework that embeds semantically rich, persona-targeted cues in user queries over multiple turns to gradually induce a \u2018reverse\u2019 or altered persona. They also design a metric to quantify persona attack success, and evaluate PHISH across three benchmarks and eight different LLMs, including single- and multi-turn interaction setups and high-risk application scenarios such as mental health support, tutoring, and customer service. Evaluations use both human annotators and LLM-as-judge assessments.", "result": "PHISH consistently and predictably shifts LLM personas in the intended directions, often inducing reverse or substantially altered traits. The attack also produces collateral changes in correlated persona traits, showing that steering one dimension can ripple across others. Its effects are stronger in multi-turn conversations than in single-turn settings, and it remains effective across diverse LLMs and benchmarks. In high-risk domains (mental health, tutoring, customer support), PHISH reliably manipulates personas according to both human and automated judgments. Remarkably, these persona changes come with only small decreases in standard reasoning benchmark scores, indicating that the model\u2019s apparent utility remains largely intact even while its persona has been compromised. Existing safety guardrails mitigate some attacks but are brittle when subjected to sustained PHISH-style interactions.", "conclusion": "The study reveals that LLM personas are highly vulnerable to black-box adversarial manipulation through normal-looking conversational history, even when model performance on reasoning tasks appears unaffected. This exposes a critical safety gap in current persona design and guardrail mechanisms, especially in sensitive, high-stakes application domains. The authors argue that future LLMs need context-resilient persona mechanisms and more robust defenses against subtle, multi-turn persona hijacking attacks. They release their code and dataset to facilitate further research on persona robustness and safety."}}
{"id": "2601.16478", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16478", "abs": "https://arxiv.org/abs/2601.16478", "authors": ["Haotian Chen", "Qingqing Long", "Siyu Pu", "Xiao Luo", "Wei Ju", "Meng Xiao", "Yuanchun Zhou", "Jianghua Zhao", "Xuezhi Wang"], "title": "DeepEra: A Deep Evidence Reranking Agent for Scientific Retrieval-Augmented Generated Question Answering", "comment": null, "summary": "With the rapid growth of scientific literature, scientific question answering (SciQA) has become increasingly critical for exploring and utilizing scientific knowledge. Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating knowledge from external sources, thereby providing credible evidence for scientific question answering. But existing retrieval and reranking methods remain vulnerable to passages that are semantically similar but logically irrelevant, often reducing factual reliability and amplifying hallucinations.To address this challenge, we propose a Deep Evidence Reranking Agent (DeepEra) that integrates step-by-step reasoning, enabling more precise evaluation of candidate passages beyond surface-level semantics. To support systematic evaluation, we construct SciRAG-SSLI (Scientific RAG - Semantically Similar but Logically Irrelevant), a large-scale dataset comprising about 300K SciQA instances across 10 subjects, constructed from 10M scientific corpus. The dataset combines naturally retrieved contexts with systematically generated distractors to test logical robustness and factual grounding. Comprehensive evaluations confirm that our approach achieves superior retrieval performance compared to leading rerankers. To our knowledge, this work is the first to comprehensively study and empirically validate innegligible SSLI issues in two-stage RAG frameworks.", "AI": {"tldr": "The paper introduces DeepEra, a reasoning-driven reranking agent and the SciRAG-SSLI dataset to make retrieval-augmented scientific QA more robust to passages that are semantically similar but logically irrelevant.", "motivation": "As scientific literature grows, scientific QA systems using Retrieval-Augmented Generation need not only semantically similar passages but logically relevant evidence. Existing retrievers and rerankers are easily misled by passages that look similar in wording but do not truly support the answer, which harms factual reliability and increases hallucinations. The authors aim to identify and fix this weakness in two-stage RAG pipelines for scientific domains.", "method": "The authors propose DeepEra, a Deep Evidence Reranking Agent that performs step-by-step reasoning over candidate passages to assess their logical support for answering a question, going beyond surface semantic similarity. They also build SciRAG-SSLI, a large-scale dataset for evaluation, containing ~300K scientific QA instances from a 10M-document corpus, across 10 subjects. The dataset mixes naturally retrieved contexts with systematically generated semantically similar but logically irrelevant distractors, explicitly targeting SSLI issues in RAG. DeepEra is evaluated as the reranking component in a two-stage RAG framework against state-of-the-art rerankers.", "result": "On the SciRAG-SSLI benchmark, DeepEra delivers better retrieval and reranking performance than leading baselines, improving the selection of logically supportive evidence and thereby strengthening factual grounding in SciQA. Empirical results demonstrate that DeepEra is more robust to semantically similar but logically irrelevant passages than prior methods.", "conclusion": "The study shows that SSLI passages pose a significant, previously underexplored problem for two-stage RAG in scientific QA. By incorporating explicit multi-step reasoning in the reranking stage and by introducing the SciRAG-SSLI benchmark, the authors provide both a strong mitigation (DeepEra) and a standard way to measure logical robustness. They claim this is the first comprehensive empirical investigation and solution targeting SSLI issues in RAG-based scientific QA."}}
{"id": "2601.16480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16480", "abs": "https://arxiv.org/abs/2601.16480", "authors": ["Peiji Li", "Linyang Li", "Handa Sun", "Wenjin Mai", "Yongkang Chen", "Xiaozhe Li", "Yue Shen", "Yichuan Ma", "Yiliu Sun", "Jiaxi Cao", "Zhishu He", "Bo Wang", "Xiaoqing Zheng", "Zhaori Bi", "Xipeng Qiu", "Qipeng Guo", "Kai Chen", "Dahua Lin"], "title": "TL-GRPO: Turn-Level RL for Reasoning-Guided Iterative Optimization", "comment": "Work in progress", "summary": "Large language models have demonstrated strong reasoning capabilities in complex tasks through tool integration, which is typically framed as a Markov Decision Process and optimized with trajectory-level RL algorithms such as GRPO. However, a common class of reasoning tasks, iterative optimization, presents distinct challenges: the agent interacts with the same underlying environment state across turns, and the value of a trajectory is determined by the best turn-level reward rather than cumulative returns. Existing GRPO-based methods cannot perform fine-grained, turn-level optimization in such settings, while black-box optimization methods discard prior knowledge and reasoning capabilities. To address this gap, we propose Turn-Level GRPO (TL-GRPO), a lightweight RL algorithm that performs turn-level group sampling for fine-grained optimization. We evaluate TL-GRPO on analog circuit sizing (ACS), a challenging scientific optimization task requiring multiple simulations and domain expertise. Results show that TL-GRPO outperforms standard GRPO and Bayesian optimization methods across various specifications. Furthermore, our 30B model trained with TL-GRPO achieves state-of-the-art performance on ACS tasks under same simulation budget, demonstrating both strong generalization and practical utility.", "AI": {"tldr": "The paper introduces Turn-Level GRPO (TL-GRPO), an RL algorithm tailored for iterative optimization tasks, and shows it outperforms existing GRPO and Bayesian optimization on analog circuit sizing with a 30B model achieving state-of-the-art results under the same simulation budget.", "motivation": "Existing tool-using LLM agents are trained with trajectory-level RL (e.g., GRPO) assuming cumulative rewards over Markovian trajectories, which misaligns with iterative optimization tasks where the environment state is effectively fixed and only the best turn reward matters. GRPO cannot finely optimize individual turns in this setting, while black-box optimizers ignore the model\u2019s prior knowledge and reasoning abilities. There is a need for an RL algorithm that can exploit LLM reasoning while optimizing at the turn level for such iterative optimization problems.", "method": "The authors reformulate the optimization setting so that each turn is treated as a separate sample while sharing the same underlying environment state, and propose Turn-Level GRPO (TL-GRPO), which applies group sampling and gradient updates at the turn level rather than over entire trajectories. They integrate this into an LLM agent that iteratively proposes analog circuit sizing parameters, runs simulations, and uses the obtained scalar rewards to guide TL-GRPO training. The algorithm remains lightweight and compatible with existing GRPO-style training pipelines but changes the granularity of credit assignment and optimization from trajectories to turns.", "result": "On analog circuit sizing benchmarks, TL-GRPO significantly outperforms standard trajectory-level GRPO and Bayesian optimization across a range of circuit specifications and design targets. Under the same simulation (evaluation) budget, models trained with TL-GRPO find higher-quality circuit designs more reliably and efficiently. Quantitatively, their 30B-parameter model, trained with TL-GRPO, achieves the best reported performance on these ACS tasks among compared methods.", "conclusion": "Turn-level optimization is better aligned with the structure of iterative scientific design problems than conventional trajectory-level RL. TL-GRPO provides an effective and lightweight way to adapt GRPO to this setting, leveraging LLM reasoning while optimizing individual proposal steps. The demonstrated gains on analog circuit sizing suggest that TL-GRPO is a promising approach for other iterative optimization tasks that share a fixed environment state and use best-turn rewards as the performance metric."}}
{"id": "2601.16486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16486", "abs": "https://arxiv.org/abs/2601.16486", "authors": ["Yichuan Ma", "Linyang Li", "Yongkang chen", "Peiji Li", "Xiaozhe Li", "Qipeng Guo", "Dahua Lin", "Kai Chen"], "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic", "comment": "Under Review", "summary": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era.", "AI": {"tldr": "The paper redefines test-time scaling for LLM agents in terms of real wall-clock time under tool latency, introduces a benchmark (Timely-Eval) to study this, and proposes a reinforcement learning method (Timely-RL) that teaches models to adapt their reasoning strategies to different time budgets, improving performance across scenarios.", "motivation": "Traditional test-time scaling measures computation by output token length, which breaks down in agentic LLM settings where external tool calls with variable latency dominate wall-clock time. Existing models don\u2019t plan or adapt their reasoning depth and interaction patterns based on actual time budgets or tool delays, limiting their effectiveness in realistic, tool-heavy applications. The paper aims to define and evaluate test-time scaling in these realistic conditions and to train models that can reason with explicit awareness of temporal constraints.", "method": "1) Conceptual: Redefine test-time in agentic LLMs as wall-clock time, explicitly incorporating tool-call latency and time budgets. 2) Benchmark: Build Timely-Eval with three regimes\u2014(a) high-frequency tool calls, (b) low-frequency tool calls, and (c) explicitly time-constrained reasoning\u2014while systematically varying tool latency. 3) Analysis: Compare performance of smaller vs larger models under different latency regimes to understand the trade-off between interaction quantity and interaction quality. 4) Training method (Timely-RL): (i) Cold-start with supervised fine-tuning on temporally annotated data, then (ii) apply reinforcement learning to improve temporal planning\u2014deciding when and how often to call tools, how long to reason, and how to adjust strategies given a time budget.", "result": "Empirically, smaller models perform better when tool feedback is fast since they can compensate for weaker single-step reasoning with more frequent interactions. Larger models outperform in high-latency settings because each interaction is more informative and higher quality. Across all settings, baseline models do not naturally adapt their reasoning or tool-use strategies to the specified time budget. The proposed Timely-RL training framework makes models more time-aware and improves their temporal planning, yielding consistent performance gains over baselines on the Timely-Eval benchmark across different tool-frequency and latency conditions.", "conclusion": "Test-time scaling for LLM agents must be defined in terms of real wall-clock time rather than just output length, especially in tool-augmented scenarios. Performance depends critically on the interaction between model size, tool latency, and the available time budget: smaller models benefit from rapid feedback via many interactions, whereas larger models excel when interactions are costly. Current models lack innate time-budget awareness, but the Timely-RL approach can teach them to plan and adapt their reasoning and tool use to temporal constraints, improving performance broadly. This framework and the Timely-Eval benchmark provide a foundation for studying and optimizing time-aware reasoning in the emerging agentic era."}}
{"id": "2601.16503", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16503", "abs": "https://arxiv.org/abs/2601.16503", "authors": ["Wei Zhu"], "title": "MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) has been swiftly adopted in scientific and clinical QA systems, a comprehensive evaluation benchmark in the medical domain is lacking. To address this gap, we introduce the Medical Retrieval-Augmented Generation (MRAG) benchmark, covering various tasks in English and Chinese languages, and building a corpus with Wikipedia and Pubmed. Additionally, we develop the MRAG-Toolkit, facilitating systematic exploration of different RAG components. Our experiments reveal that: (a) RAG enhances LLM reliability across MRAG tasks. (b) the performance of RAG systems is influenced by retrieval approaches, model sizes, and prompting strategies. (c) While RAG improves usefulness and reasoning quality, LLM responses may become slightly less readable for long-form questions. We will release the MRAG-Bench's dataset and toolkit with CCBY-4.0 license upon acceptance, to facilitate applications from both academia and industry.", "AI": {"tldr": "Introduces MRAG, a bilingual benchmark and toolkit to evaluate Retrieval-Augmented Generation for medical QA.", "motivation": "RAG is widely used in scientific and clinical QA, but there is no comprehensive, domain-specific benchmark to rigorously evaluate its performance in the medical field.", "method": "Construct a Medical RAG (MRAG) benchmark spanning multiple English and Chinese medical QA tasks, using a corpus built from Wikipedia and PubMed; develop MRAG-Toolkit to systematically vary and analyze RAG components such as retrievers, LLM sizes, and prompting strategies; run extensive experiments across these configurations.", "result": "Experiments show that (a) RAG consistently improves LLM reliability across MRAG tasks; (b) RAG performance is sensitive to retrieval methods, model scale, and prompt design; and (c) for long-form questions, RAG increases usefulness and reasoning quality but slightly reduces readability of responses.", "conclusion": "MRAG and its toolkit provide a standardized way to study and optimize medical RAG systems; RAG brings clear benefits but must be carefully configured, especially for long-form generation. The dataset and toolkit will be released under a permissive license to support broad academic and industrial use."}}
{"id": "2601.16504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16504", "abs": "https://arxiv.org/abs/2601.16504", "authors": ["Obed Junias", "Maria Leonor Pacheco"], "title": "LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning", "comment": null, "summary": "Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.", "AI": {"tldr": "The paper introduces LOGICAL-COMMONSENSEQA, a benchmark that tests models\u2019 ability to compose commonsense plausibility judgments with logical operators (AND, OR, NEITHER/NOR), revealing weaknesses especially with negation-based reasoning.", "motivation": "Most commonsense benchmarks force a single correct label, hiding whether multiple options might be plausible, mutually compatible, or all implausible. This atomic, single-label framing fails to assess whether models can reason about how plausible statements interact (e.g., both true, at least one true, or neither). The authors want a benchmark that reflects the inherently compositional and graded nature of commonsense plausibility and better diagnoses reasoning failures, especially with logical structure and negation.", "method": "They construct LOGICAL-COMMONSENSEQA, where items consist of pairs of atomic commonsense statements together with a required plausibility-level logical composition: AND (both plausible), OR (at least one plausible), or NEITHER/NOR (both implausible or cannot both hold). They then systematically evaluate different families of language models\u2014general instruction-tuned, reasoning-specialized, and fine-tuned\u2014under multiple prompting regimes (zero-shot, few-shot, chain-of-thought). Performance is measured per operator type to tease apart conjunctive, disjunctive, and negation-based compositional reasoning abilities.", "result": "Models achieve relatively strong performance on conjunctive (AND) reasoning and moderate performance on disjunctive (OR) reasoning, but their accuracy declines sharply on items involving negation-based logic (NEITHER/NOR). This pattern holds across instruction-tuned and reasoning-specialized models and across prompting setups (zero-shot, few-shot, chain-of-thought), indicating a systematic weakness rather than a prompt-specific artifact.", "conclusion": "LOGICAL-COMMONSENSEQA offers a controlled, logically structured benchmark for compositional commonsense reasoning, highlighting that current models still struggle especially with negation and more complex logical compositions of plausibility. The benchmark both exposes these fundamental limitations and provides a testbed for developing models with more robust, logically grounded commonsense reasoning capabilities."}}
{"id": "2601.16508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16508", "abs": "https://arxiv.org/abs/2601.16508", "authors": ["Karl Neergaard", "Le Qiu", "Emmanuele Chersoni"], "title": "Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations using BoolQ", "comment": "4 pages plus 6 pages of bibliography and appendix", "summary": "Single-prompt evaluations dominate current LLM benchmarking, yet they fail to capture the conversational dynamics where real-world harm occurs. In this study, we examined whether conversation length affects response veracity by evaluating LLM performance on the BoolQ dataset under varying length and scaffolding conditions. Our results across three distinct LLMs revealed model-specific vulnerabilities that are invisible under single-turn testing. The length-dependent and scaffold-specific effects we observed demonstrate a fundamental limitation of static evaluations, as deployment-relevant vulnerabilities could only be spotted in a multi-turn conversational setting.", "AI": {"tldr": "The paper shows that evaluating LLMs with only single prompts misses important failures that appear in longer, scaffolded conversations.", "motivation": "Most current LLM benchmarks use one-shot, single-turn questions, but real-world LLM use is conversational and multi-turn. Many harms and mistakes likely emerge only after several back-and-forth exchanges. The authors want to know whether conversation length and scaffolding systematically affect the truthfulness of LLM answers, revealing weaknesses that standard benchmarks overlook.", "method": "They take the BoolQ dataset (yes/no reading comprehension questions) and adapt it to a conversational setting. They then test three different LLMs under conditions that vary (a) the length of the conversation and (b) the scaffolding style (how context and intermediate steps are provided). They compare model accuracy and response veracity across these conditions against traditional single-prompt performance to see how behavior changes as dialogs grow longer and more structured.", "result": "Across the three LLMs, performance is not stable: each model shows its own pattern of degradation or change in veracity as conversations get longer and as different scaffolding strategies are used. Some vulnerabilities\u2014drops in accuracy or systematic errors\u2014only appear in multi-turn settings and would be invisible if one looked only at single-prompt BoolQ scores.", "conclusion": "Static, single-prompt benchmarks are fundamentally insufficient to characterize LLM reliability. To uncover deployment-relevant vulnerabilities, evaluations must incorporate multi-turn conversational setups and different scaffolding regimes, since models can behave substantially worse or differently once the interaction leaves the single-turn regime."}}
{"id": "2601.16512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16512", "abs": "https://arxiv.org/abs/2601.16512", "authors": ["Hoang-Quoc Nguyen-Son", "Minh-Son Dao", "Koji Zettsu"], "title": "SearchLLM: Detecting LLM Paraphrased Text by Measuring the Similarity with Regeneration of the Candidate Source via Search Engine", "comment": "EACL 2026 camera ready (Main Track)", "summary": "With the advent of large language models (LLMs), it has become common practice for users to draft text and utilize LLMs to enhance its quality through paraphrasing. However, this process can sometimes result in the loss or distortion of the original intended meaning. Due to the human-like quality of LLM-generated text, traditional detection methods often fail, particularly when text is paraphrased to closely mimic original content. In response to these challenges, we propose a novel approach named SearchLLM, designed to identify LLM-paraphrased text by leveraging search engine capabilities to locate potential original text sources. By analyzing similarities between the input and regenerated versions of candidate sources, SearchLLM effectively distinguishes LLM-paraphrased content. SearchLLM is designed as a proxy layer, allowing seamless integration with existing detectors to enhance their performance. Experimental results across various LLMs demonstrate that SearchLLM consistently enhances the accuracy of recent detectors in detecting LLM-paraphrased text that closely mimics original content. Furthermore, SearchLLM also helps the detectors prevent paraphrasing attacks.", "AI": {"tldr": "SearchLLM is a proxy layer that uses web search to find likely original texts and then compares them to a query to detect whether the query is an LLM-based paraphrase, boosting existing LLM-text detectors and defending against paraphrasing attacks.", "motivation": "Existing large language model (LLM) text detectors often fail when text has been paraphrased by an LLM to closely mimic an original human-written source. Because paraphrased LLM outputs look highly human-like and preserve much of the original semantics, standard stylistic or statistical cues for detection become weak or disappear. There is a need for methods that can reliably detect LLM-paraphrased content and support existing detectors rather than replace them.", "method": "The paper proposes SearchLLM, a search-based proxy layer that sits in front of existing LLM detectors. Given an input text suspected of being LLM-paraphrased, SearchLLM: (1) uses a search engine to retrieve potential original source texts from the web; (2) regenerates versions of these candidate sources (e.g., through controlled paraphrasing or chunk-level comparison); (3) computes similarity between the input text and the regenerated versions; and (4) uses these similarity signals as features or filters that are then passed to existing detectors. This pipeline enables the system to identify when a given text is likely an LLM paraphrase of some retrieved source, rather than an independently written human text.", "result": "Experiments across multiple LLMs and recent state-of-the-art detectors show that plugging SearchLLM in as a proxy layer consistently improves accuracy in detecting LLM-paraphrased text that closely matches original content. The method also makes detectors more robust to paraphrasing attacks, where adversaries try to evade detection by rephrasing generated text.", "conclusion": "SearchLLM demonstrates that augmenting detectors with search-based source retrieval and similarity analysis can significantly improve the detection of LLM-paraphrased content. Rather than relying solely on intrinsic text statistics, leveraging external evidence from potential source documents helps both to catch paraphrased LLM outputs and to harden existing detectors against paraphrasing-based evasion strategies."}}
{"id": "2601.16530", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16530", "abs": "https://arxiv.org/abs/2601.16530", "authors": ["Gaurav Maheshwari", "Kevin El Haddad"], "title": "Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification", "comment": null, "summary": "Large language models (LLMs) and high-capacity encoders have advanced zero and few-shot classification, but their inference cost and latency limit practical deployment. We propose training lightweight text classifiers using dynamically generated supervision from an LLM. Our method employs an iterative, agentic loop in which the LLM curates training data, analyzes model successes and failures, and synthesizes targeted examples to address observed errors. This closed-loop generation and evaluation process progressively improves data quality and adapts it to the downstream classifier and task. Across four widely used benchmarks, our approach consistently outperforms standard zero and few-shot baselines. These results indicate that LLMs can serve effectively as data curators, enabling accurate and efficient classification without the operational cost of large-model deployment.", "AI": {"tldr": "Train small text classifiers using data dynamically generated and refined by an LLM in a closed-loop process, achieving better accuracy than zero/few-shot LLM use while being cheaper at inference.", "motivation": "While LLMs and large encoders can perform strong zero- and few-shot text classification, their high inference cost and latency make them impractical for many real-world deployments. There is a need for methods that retain much of their accuracy but run on lightweight models suitable for production. Existing approaches either rely directly on LLM inference or on static, manually curated datasets, which may not adapt to specific classifier weaknesses.", "method": "Use an LLM as an agentic data curator for training a small classifier. The process runs in an iterative loop: (1) the LLM generates labeled training examples for a target classification task; (2) a lightweight classifier is trained on this synthetic data; (3) the classifier is evaluated, and its successes and failures are analyzed by the LLM; (4) based on these error patterns, the LLM synthesizes new, targeted examples to address weaknesses. This closed-loop continues, refining the training set to better match the downstream model and task, without requiring human annotation.", "result": "On four popular text-classification benchmarks, classifiers trained with this LLM-curated, iteratively improved synthetic data outperform strong zero-shot and few-shot baselines that rely directly on large models. The method consistently yields higher accuracy while enabling deployment of much cheaper and faster classifiers at inference time.", "conclusion": "LLMs can be effectively repurposed from direct inference engines to powerful data curators that generate, diagnose, and refine training sets for smaller models. This agentic, closed-loop supervision enables accurate, efficient text classification without the ongoing operational cost of running large LLMs in production, pointing toward a practical pattern for leveraging LLM capabilities in resource-constrained settings."}}
{"id": "2601.16555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16555", "abs": "https://arxiv.org/abs/2601.16555", "authors": ["Mingwei Sun", "Qianlong Wang", "Ruifeng Xu"], "title": "Retrieve-Refine-Calibrate: A Framework for Complex Claim Fact-Checking", "comment": "9 pages, 4 figures. This is an original work by the authors. Any unauthorized submission, reproduction, or commercial use by third parties is prohibited", "summary": "Fact-checking aims to verify the truthfulness of a claim based on the retrieved evidence. Existing methods typically follow a decomposition paradigm, in which a claim is broken down into sub-claims that are individually verified. However, the decomposition paradigm may introduce noise to the verification process due to irrelevant entities or evidence, ultimately degrading verification accuracy. To address this problem, we propose a Retrieve-Refine-Calibrate (RRC) framework based on large language models (LLMs). Specifically, the framework first identifies the entities mentioned in the claim and retrieves evidence relevant to them. Then, it refines the retrieved evidence based on the claim to reduce irrelevant information. Finally, it calibrates the verification process by re-evaluating low-confidence predictions. Experiments on two popular fact-checking datasets (HOVER and FEVEROUS-S) demonstrate that our framework achieves superior performance compared with competitive baselines.", "AI": {"tldr": "The paper introduces an LLM-based Retrieve-Refine-Calibrate framework that improves fact-checking accuracy by reducing noise from irrelevant evidence and re-evaluating low-confidence predictions.", "motivation": "Traditional fact-checking methods decompose claims into sub-claims for verification, but this decomposition can pull in irrelevant entities and evidence, adding noise and harming verification accuracy. There is a need for a framework that can better control evidence relevance and verification reliability, particularly leveraging recent advances in large language models.", "method": "The authors propose the Retrieve-Refine-Calibrate (RRC) framework. First, the system identifies entities in a claim and retrieves evidence relevant to those entities. Second, it refines the retrieved evidence conditioned on the claim to filter out irrelevant information. Third, it calibrates the final decision by detecting low-confidence predictions and re-evaluating them, all implemented with the help of large language models.", "result": "On two benchmark fact-checking datasets, HOVER and FEVEROUS-S, the RRC framework outperforms several strong baseline methods in verification accuracy, demonstrating its effectiveness in reducing noise and improving fact-checking performance.", "conclusion": "By structuring fact-checking into retrieval, claim-aware refinement, and confidence-based calibration, and leveraging LLMs in each stage, the proposed RRC framework mitigates noise introduced by conventional decomposition paradigms and yields state-of-the-art or superior performance on standard fact-checking benchmarks."}}
{"id": "2601.16596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16596", "abs": "https://arxiv.org/abs/2601.16596", "authors": ["Jianyu Wen", "Yang Wei", "Xiongxi Yu", "Changxuan Xiao", "Ke Zeng"], "title": "Attention-MoA: Enhancing Mixture-of-Agents via Inter-Agent Semantic Attention and Deep Residual Synthesis", "comment": null, "summary": "As the development of Large Language Models (LLMs) shifts from parameter scaling to inference-time collaboration, the Mixture-of-Agents (MoA) framework has emerged as a general paradigm to harness collective intelligence by layering diverse models. While recent MoA variants have introduced dynamic routing and residual connections to improve efficiency, these methods often fail to facilitate deep semantic interaction between agents, limiting the system's ability to actively correct hallucinations and refine logic. In this paper, we introduce Attention-MoA, a novel MoA-based framework that redefines collaboration through Inter-agent Semantic Attention. Complemented by an Inter-layer Residual Module with Adaptive Early Stopping Mechanism, our architecture mitigates information degradation in deep layers while improving computational efficiency. Extensive evaluations across AlpacaEval 2.0, MT-Bench, and FLASK demonstrate that Attention-MoA significantly outperforms state-of-the-art baselines, achieving a 91.15% Length-Controlled Win Rate on AlpacaEval 2.0 and dominating in 10 out of 12 capabilities on FLASK. Notably, Attention-MoA enables an ensemble of small open-source models to outperform massive proprietary models like Claude-4.5-Sonnet and GPT-4.1, achieving an MT-Bench score of 8.83 and an AlpacaEval 2.0 LC Win Rate of 77.36%.", "AI": {"tldr": "Proposes Attention-MoA, a Mixture-of-Agents framework that uses semantic attention between agents plus residual and early-stopping mechanisms to enable small open-source LLM ensembles to outperform large proprietary models.", "motivation": "Existing Mixture-of-Agents approaches focus on routing and efficiency but do not support rich semantic interaction among agents, which limits their ability to collaboratively correct hallucinations and refine reasoning. The authors want a collaboration mechanism that better exploits the complementary strengths of different models while remaining computationally efficient.", "method": "Design an Attention-MoA architecture that introduces Inter-agent Semantic Attention so agents can attend to and integrate each other\u2019s intermediate outputs. Add an Inter-layer Residual Module to alleviate information degradation as depth increases and an Adaptive Early Stopping Mechanism to cut off unnecessary deeper layers for efficiency. Evaluate the framework on standard LLM benchmarks (AlpacaEval 2.0, MT-Bench, FLASK) using ensembles of smaller open-source models.", "result": "Attention-MoA achieves strong empirical gains over state-of-the-art MoA and ensemble baselines, including a 91.15% length-controlled win rate on AlpacaEval 2.0 and top performance in 10 of 12 capability dimensions on FLASK. When instantiated with small open-source models, it reaches an MT-Bench score of 8.83 and a 77.36% LC win rate on AlpacaEval 2.0, surpassing large proprietary models such as Claude-4.5-Sonnet and GPT-4.1 on these metrics.", "conclusion": "Inter-agent Semantic Attention combined with residual and adaptive early-stopping mechanisms enables more effective and efficient collaboration among LLM agents. This MoA design can unlock strong performance from ensembles of smaller open-source models, in some cases surpassing much larger proprietary systems, suggesting that smarter collaboration can substitute for sheer model scale."}}
{"id": "2601.16615", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16615", "abs": "https://arxiv.org/abs/2601.16615", "authors": ["Xiang Chen"], "title": "AuroraEdge-V-2B: A Faster And Stronger Edge Visual Large Language Model", "comment": null, "summary": "Recently, due to the advancement of multimodal technology, people are attempting to use visual large language models (VLLMs) in industrial production. Many deep learning models (DLMs) deployed in the production environment are gradually being replaced by VLLMs. Compared with DLMs, VLLMs have some advantages in industrial applications: (1) Their strong generalization ability enables them to perform well across a wide range of tasks. (2) They are flexible and can deal with unfamiliar samples through context learning quickly. However, VLLMs also have obvious drawbacks: (1) VLLMs do not perform as well as custom-developed DLMs in specific domains. (2) The number of parameters in VLLMs is generally quite large, and their deployment requires substantial computational resources. (3) VLLMs generally operate much slower than DLMs, making real-time response challenging to achieve. To better utilize VLLMs in industrial applications, we introduce AuroraEdge-V-2B in this work, a compact, robust, and high-speed VLLM designed for edge deployment. To make the model run faster, we also propose a compression-fusion method to improve inference efficiency. AuroraEdge-V-2B has the following notable features: (1) Easy deployment and faster: It has only 2B parameters and is highly suitable for edge deployment, offering better real-time performance. (2) Fewer visual tokens and cheaper: It significantly reduces the number of visual tokens in the decoding process, thereby reducing the floating-point operations by half during inference and making it cheaper to use. (3) Strong performance: It gets a higher score on 9 benchmarks than models with the same number of parameter (e.g., Qwen2-VL-2B, Qwen2.5-VL-3B, InternVL-2.5-2B).", "AI": {"tldr": "AuroraEdge-V-2B is a compact, fast visual large language model tailored for edge deployment that achieves strong benchmark performance with reduced computation via a compression-fusion strategy and fewer visual tokens.", "motivation": "Industrial users want to adopt visual large language models (VLLMs) because of their generalization and in-context learning abilities, but current VLLMs are too large, slow, and underperform domain-specific deep learning models for specialized tasks. There is a need for a smaller, faster, yet still strong VLLM that can run efficiently in edge environments and deliver competitive accuracy.", "method": "The paper designs AuroraEdge-V-2B, a 2-billion-parameter VLLM optimized for edge devices. It introduces a compression-fusion method to accelerate inference and reduce computation, and architecturally reduces the number of visual tokens used during decoding. These design choices target lower FLOPs, faster runtime, and easier deployment while preserving multimodal capability.", "result": "AuroraEdge-V-2B reportedly halves the FLOPs during inference by cutting visual tokens and applying compression-fusion, leading to faster and cheaper operation. On nine benchmarks, it outperforms other VLLMs of similar scale such as Qwen2-VL-2B, Qwen2.5-VL-3B, and InternVL-2.5-2B, demonstrating competitive or superior performance despite its compact size and efficiency focus.", "conclusion": "AuroraEdge-V-2B demonstrates that it is possible to build a compact, edge-friendly VLLM that offers strong performance while significantly improving inference efficiency and deployment practicality. The work suggests that tailored architectural and compression strategies can narrow the gap between large general-purpose VLLMs and specialized industrial DLMs, enabling real-time multimodal intelligence at the edge."}}
{"id": "2601.16618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16618", "abs": "https://arxiv.org/abs/2601.16618", "authors": ["Jing Xu", "Jiaqi Wang", "Daxin Tan", "Xiao Chen"], "title": "PROST-LLM: Progressively Enhancing the Speech-to-Speech Translation Capability in LLMs", "comment": "Accepted by ICASSP 2026", "summary": "Although Large Language Models (LLMs) excel in many tasks, their application to Speech-to-Speech Translation (S2ST) is underexplored and hindered by data scarcity. To bridge this gap, we propose PROST-LLM (PROgressive Speech-to-speech Translation) to enhance the S2ST capabilities in LLMs progressively. First, we fine-tune the LLMs with the CVSS corpus, employing designed tri-task learning and chain of modality methods to boost the initial performance. Then, leveraging the fine-tuned model, we generate preference pairs through self-sampling and back-translation without human evaluation. Finally, these preference pairs are used for preference optimization to enhance the model's S2ST capability further. Extensive experiments confirm the effectiveness of our proposed PROST-LLM in improving the S2ST capability of LLMs.", "AI": {"tldr": "The paper introduces PROST-LLM, a progressive training framework to improve Large Language Models on speech-to-speech translation using limited data, combining supervised multi-task finetuning and preference optimization without human labels.", "motivation": "LLMs are powerful but not well explored for speech-to-speech translation, mainly because high-quality parallel speech data is scarce and expensive to label. The authors want a way to unlock S2ST abilities in LLMs while overcoming data scarcity and minimizing reliance on human evaluation.", "method": "1) Start with an LLM and finetune it on the CVSS speech translation corpus. During this stage, they use a tri-task learning scheme (likely combining automatic speech recognition, speech-to-text translation, and speech-to-speech translation) together with a chain-of-modality strategy that structures how intermediate text and speech representations are used to improve learning. 2) Using the finetuned model, they automatically generate preference pairs via self-sampling (multiple candidate outputs) and back-translation, thus estimating which outputs are better without human judgments. 3) Apply preference optimization (e.g., RLHF-style or DPO-style training) on these synthetic preference pairs to further refine the model\u2019s S2ST performance.", "result": "Experiments show that PROST-LLM significantly enhances the speech-to-speech translation performance of LLMs compared to baseline models or simpler finetuning strategies, demonstrating that their progressive training and preference optimization approach is effective under data-scarce conditions.", "conclusion": "Progressively training LLMs for S2ST with supervised multi-task finetuning followed by automated preference optimization can substantially improve S2ST capabilities without human preference labeling, offering a practical way to adapt LLMs for speech translation despite limited data."}}
{"id": "2601.16621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16621", "abs": "https://arxiv.org/abs/2601.16621", "authors": ["Xueyang Feng", "Weinan Gan", "Xu Chen", "Quanyu Dai", "Yong Liu"], "title": "How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference Utilization in Personalized Assistants", "comment": null, "summary": "Large language model (LLM)-powered assistants have recently integrated memory mechanisms that record user preferences, leading to more personalized and user-aligned responses. However, irrelevant personalized memories are often introduced into the context, interfering with the LLM's intent understanding. To comprehensively investigate the dual effects of personalization, we develop RPEval, a benchmark comprising a personalized intent reasoning dataset and a multi-granularity evaluation protocol. RPEval reveals the widespread phenomenon of irrational personalization in existing LLMs and, through error pattern analysis, illustrates its negative impact on user experience. Finally, we introduce RP-Reasoner, which treats memory utilization as a pragmatic reasoning process, enabling the selective integration of personalized information. Experimental results demonstrate that our method significantly outperforms carefully designed baselines on RPEval, and resolves 80% of the bad cases observed in a large-scale commercial personalized assistant, highlighting the potential of pragmatic reasoning to mitigate irrational personalization. Our benchmark is publicly available at https://github.com/XueyangFeng/RPEval.", "AI": {"tldr": "The paper studies how personalization memories in LLM assistants can both help and hurt intent understanding, introduces a benchmark (RPEval) to evaluate this, and proposes a reasoning-based method (RP-Reasoner) that selectively uses personal information to avoid \"irrational\" personalization, achieving large performance gains and fixing most bad real-world cases.", "motivation": "Existing LLM assistants store user-specific memories to personalize responses, but they frequently inject irrelevant or counterproductive personal details into conversations, confusing intent and degrading user experience. There is no systematic benchmark to characterize and quantify this problem of irrational personalization, nor robust methods that explicitly reason about when personal information should be applied. The authors aim to (1) formally evaluate the dual effects of personalization (helpful vs harmful) and (2) design a principled approach to selectively use memories so that personalization aligns with user intent.", "method": "The authors construct RPEval, a benchmark consisting of a personalized intent reasoning dataset paired with a multi-granularity evaluation protocol to measure how well LLMs use or ignore personal memories. They then analyze error patterns of existing LLMs on RPEval to expose common forms of irrational personalization. Building on these insights, they propose RP-Reasoner, a method that frames memory utilization as a pragmatic reasoning task: the model explicitly reasons about which user-specific information is relevant to the current intent and integrates only those memories into its response generation. RP-Reasoner is compared against carefully designed baselines within this evaluation framework.", "result": "On the RPEval benchmark, RP-Reasoner significantly outperforms multiple strong baselines under the proposed evaluation metrics. When deployed in a large-scale commercial personalized assistant, RP-Reasoner is able to resolve about 80% of previously observed bad cases caused by irrational personalization. These empirical results show that principled, reasoning-based control of memory usage can markedly reduce the negative side effects of personalization while preserving its benefits.", "conclusion": "The study concludes that naive use of personalization memories leads to widespread irrational personalization that harms user experience and intent understanding in LLM assistants. RPEval provides a standardized way to expose and measure these failures. Treating memory use as pragmatic reasoning, as implemented by RP-Reasoner, enables selective and context-sensitive personalization that mitigates these issues, substantially improving both benchmark scores and real-world assistant behavior. This suggests that future personalized LLM systems should embed explicit reasoning mechanisms for memory utilization rather than relying on unstructured retrieval or unconditional use of personal data."}}
{"id": "2601.16623", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16623", "abs": "https://arxiv.org/abs/2601.16623", "authors": ["Weerayut Buaphet", "Thanh-Nhi Nguyen", "Risa Kondo", "Tomoyuki Kajiwara", "Yumin Kim", "Jimin Lee", "Hwanhee Lee", "Holy Lovenia", "Peerat Limkonchotiwat", "Sarana Nutanong", "Rob Van der Goot"], "title": "MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages", "comment": null, "summary": "Social media data has been of interest to Natural Language Processing (NLP) practitioners for over a decade, because of its richness in information, but also challenges for automatic processing. Since language use is more informal, spontaneous, and adheres to many different sociolects, the performance of NLP models often deteriorates. One solution to this problem is to transform data to a standard variant before processing it, which is also called lexical normalization. There has been a wide variety of benchmarks and models proposed for this task. The MultiLexNorm benchmark proposed to unify these efforts, but it consists almost solely of languages from the Indo-European language family in the Latin script. Hence, we propose an extension to MultiLexNorm, which covers 5 Asian languages from different language families in 4 different scripts. We show that the previous state-of-the-art model performs worse on the new languages and propose a new architecture based on Large Language Models (LLMs), which shows more robust performance. Finally, we analyze remaining errors, revealing future directions for this task.", "AI": {"tldr": "Extension of lexical normalization benchmark to Asian languages and development of a more robust LLM-based normalization model.", "motivation": "Existing social media lexical normalization benchmarks and models, including MultiLexNorm, focus almost entirely on Indo-European languages in Latin script, limiting our understanding and performance of normalization methods on typologically diverse and non-Latin-script languages. There is a need to assess and improve normalization approaches for Asian languages with different scripts and linguistic properties.", "method": "Extend the MultiLexNorm benchmark by adding datasets for five Asian languages from different language families and in four different scripts. Evaluate the previous state-of-the-art lexical normalization model on these new datasets, then design and test a new normalization architecture based on Large Language Models (LLMs). Conduct error analysis on the model outputs to characterize remaining challenges and guide future work.", "result": "The prior state-of-the-art lexical normalization model shows degraded performance on the newly added Asian languages, indicating poor cross-lingual and cross-script robustness. The proposed LLM-based architecture achieves more robust and overall better performance across the extended benchmark. Error analysis identifies systematic failure cases and phenomena that remain difficult for current models.", "conclusion": "Lexical normalization methods and benchmarks biased toward Indo-European Latin-script languages do not transfer well to typologically and script-diverse Asian languages. The extended MultiLexNorm benchmark and the proposed LLM-based model provide a stronger foundation for robust, multilingual lexical normalization on social media text. Remaining error patterns highlight open challenges and future research directions in this area."}}
{"id": "2601.16629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16629", "abs": "https://arxiv.org/abs/2601.16629", "authors": ["Stef Accou", "Wessel Poelman"], "title": "Typologically Informed Parameter Aggregation", "comment": "EACL 2026: Findings", "summary": "Massively multilingual language models enable cross-lingual generalization but underperform on low-resource and unseen languages. While adapter-based fine-tuning offers a parameter-efficient solution, training language-specific adapters at scale remains costly. We introduce Typologically Informed Parameter Aggregation (TIPA), a training-free method that constructs proxy language adapters by aggregating existing ones, weighted by typological similarity. Integrated into the MAD-X framework, these proxies enable zero-shot cross-lingual transfer without additional training. We evaluate TIPA on five NLP tasks and over 230 languages. TIPA consistently outperforms or matches baselines such as English-only fine-tuning or selecting the typologically closest language adapter. We see the largest gains for languages lacking dedicated adapters. Our results demonstrate that typologically informed aggregation provides a viable alternative to language-specific modules without any training needed.", "AI": {"tldr": "The paper proposes TIPA, a training-free method that builds proxy language adapters for low-resource/unseen languages by aggregating existing adapters based on typological similarity, achieving strong zero-shot transfer on 230+ languages.", "motivation": "Massively multilingual large language models generalize across languages but still perform poorly on low-resource and especially unseen languages. Adapter-based fine-tuning improves cross-lingual performance parameter-efficiently, but training and storing separate adapters for many languages is costly and unrealistic at scale. There is a need for a way to support many languages, including those without training data or dedicated adapters, without additional training cost.", "method": "The authors introduce Typologically Informed Parameter Aggregation (TIPA). Instead of training new language-specific adapters, TIPA constructs a proxy adapter for a target language by aggregating the parameters of existing language adapters. The contribution of each source adapter is weighted by a measure of typological similarity between the source and target languages (using linguistic typology features). TIPA is integrated into the MAD-X adapter framework to enable zero-shot transfer: given a target language with no adapter, TIPA dynamically composes a proxy adapter from others without any gradient updates or fine-tuning. The method is evaluated across multiple NLP tasks and a large set of languages.", "result": "Across five NLP tasks and over 230 languages, TIPA consistently matches or outperforms several competitive baselines: (1) using only an English adapter for transfer, and (2) simply selecting the single typologically closest language adapter. The performance gains are particularly strong for languages that do not have their own dedicated adapters, where the aggregated proxy effectively compensates for missing task-specific training. This shows that typology-aware parameter aggregation can improve zero-shot cross-lingual performance at scale.", "conclusion": "TIPA demonstrates that training-free, typologically informed aggregation of existing language adapters is a practical and effective alternative to training language-specific adapters. By leveraging linguistic typology to weight and combine parameters, TIPA enables scalable zero-shot cross-lingual transfer to many low-resource and unseen languages, reducing the need for additional training or data collection while maintaining or improving performance across diverse NLP tasks."}}
{"id": "2601.16644", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16644", "abs": "https://arxiv.org/abs/2601.16644", "authors": ["Rifo Genadi", "Munachiso Nwadike", "Nurdaulet Mukhituly", "Hilal Alquabeh", "Tatsuya Hiraoka", "Kentaro Inui"], "title": "Sycophancy Hides Linearly in the Attention Heads", "comment": null, "summary": "We find that correct-to-incorrect sycophancy signals are most linearly separable within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified \"truthful\" directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations.", "AI": {"tldr": "The paper studies where and how sycophantic behavior is represented inside transformer models and shows that simple linear probes on specific attention heads can detect and counteract sycophancy effectively.", "motivation": "Large language models often exhibit sycophancy: they defer to user beliefs or expressed doubt instead of maintaining factual accuracy. Understanding the internal mechanisms that give rise to sycophancy is necessary to design targeted interventions that improve truthfulness and robustness to user pressure without retraining the whole model.", "method": "The authors adopt the linear representation hypothesis and train linear probes on different parts of the transformer\u2014residual stream, MLP layers, and multi-head attention activations\u2014to distinguish correct from incorrect sycophantic responses. They analyze linear separability across layers, perform steering experiments by intervening along discovered directions, study cross-dataset transfer by training probes on TruthfulQA and testing on other factual QA benchmarks, compare their learned \u201canti-sycophancy\u201d direction to previously discovered truth-related directions, and examine attention patterns of key heads, especially how they attend to user doubt expressions.", "result": "They find that signals distinguishing correct from incorrect sycophancy are present throughout the network but are most linearly separable and practically useful in a sparse subset of middle-layer attention heads. Steering along probe directions in these heads is far more effective at mitigating sycophancy than interventions in the residual stream or MLPs. Probes trained on TruthfulQA generalize well to other factual QA datasets, and the discovered directions only partially overlap with prior \u201ctruthful\u201d directions, implying distinct but related mechanisms. Attention analysis shows that the critical heads strongly attend to linguistic markers of user doubt, which correlates with sycophantic responses.", "conclusion": "Sycophancy in language models is encoded in relatively simple linear directions, particularly within specific attention heads. By identifying and linearly steering along these directions\u2014without retraining the full model\u2014one can substantially reduce sycophantic behavior. This supports a view of model internals where factual accuracy and resistance to deference are related yet separable, and it highlights targeted attention-head interventions as a promising approach for alignment and interpretability-driven control of model behavior."}}
{"id": "2601.16669", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.16669", "abs": "https://arxiv.org/abs/2601.16669", "authors": ["Yuzhen Shi", "Huanghai Liu", "Yiran Hu", "Gaojie Song", "Xinran Xu", "Yubo Ma", "Tianyi Tang", "Li Zhang", "Qingjing Chen", "Di Feng", "Wenbo Lv", "Weiheng Wu", "Kexin Yang", "Sen Yang", "Wei Wang", "Rongyao Shi", "Yuanyang Qiu", "Yuemeng Qi", "Jingwen Zhang", "Xiaoyu Sui", "Yifan Chen", "Yi Zhang", "An Yang", "Bowen Yu", "Dayiheng Liu", "Junyang Lin", "Weixing Shen", "Bing Zhao", "Charles L. A. Clarke", "Hu Wei"], "title": "PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice", "comment": null, "summary": "As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench.", "AI": {"tldr": "Introduces PLawBench, a realistic benchmark to evaluate LLMs' ability to handle practical legal tasks, revealing current models' weaknesses in fine-grained legal reasoning.", "motivation": "Existing legal benchmarks focus on simplified, standardized tasks and use coarse, single-dimensional metrics, which fail to reflect the ambiguity, complexity, and detailed reasoning required in real legal practice. There is a need to evaluate whether LLMs can handle real-world legal workflows and reasoning demands.", "method": "The authors design PLawBench, a benchmark grounded in real legal workflows, organized into three task categories: public legal consultation, practical case analysis, and legal document generation. They construct 850 questions spanning 13 practical legal scenarios. Each question has expert-designed evaluation rubrics, totaling around 12,500 rubric items. An LLM-based evaluator, aligned with human experts, is used to assess 10 state-of-the-art LLMs on these tasks.", "result": "When evaluated on PLawBench, none of the 10 state-of-the-art LLMs demonstrate strong performance. The detailed rubric-based evaluation exposes substantial limitations in the models' fine-grained legal reasoning, including issue spotting, fact identification, structured reasoning, and document drafting quality.", "conclusion": "PLawBench provides a more realistic and fine-grained way to evaluate LLMs' capabilities in legal practice. Current LLMs are still far from reliably performing complex legal tasks, and the benchmark highlights important directions for improving and evaluating legal-domain LLMs in the future."}}
{"id": "2601.16651", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16651", "abs": "https://arxiv.org/abs/2601.16651", "authors": ["Lukas Hinterleitner", "Loris Schoenegger", "Benjamin Roth"], "title": "Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations", "comment": "8 pages", "summary": "Gradient-based methods for instance-based explanation for large language models (LLMs) are hindered by the immense dimensionality of model gradients. In practice, influence estimation is restricted to a subset of model parameters to make computation tractable, but this subset is often chosen ad hoc and rarely justified by systematic evaluation. This paper investigates if it is better to create low-dimensional representations by selecting a small, architecturally informed subset of model components or by projecting the full gradients into a lower-dimensional space. Using a novel benchmark, we show that a greedily selected subset of components captures the information about training data influence needed for a retrieval task more effectively than either the full gradient or random projection. We further find that this approach is more computationally efficient than random projection, demonstrating that targeted component selection is a practical strategy for making instance-based explanations of large models more computationally feasible.", "AI": {"tldr": "The paper studies how to best reduce gradient dimensionality for instance-based explanations in LLMs, comparing targeted component selection against random projections and full gradients.", "motivation": "Instance-based explanations for LLMs rely on gradient-based influence estimation, but full gradients are extremely high-dimensional and intractable to use directly. Existing work arbitrarily restricts attention to subsets of parameters without rigorous justification. The authors want to systematically understand whether architecturally informed component selection or generic dimensionality reduction (random projection) is better for capturing training data influence efficiently.", "method": "The authors propose a new benchmark for evaluating how well different low-dimensional gradient representations capture training data influence for a retrieval task. They compare three approaches: (1) using the full gradient, (2) projecting full gradients into a lower-dimensional space via random projections, and (3) greedily selecting a small, architecturally informed subset of model components and using their gradients. They then measure performance on the influence-based retrieval task and compare computational efficiency across methods.", "result": "The greedily selected, architecturally informed subset of components captures the relevant influence information more effectively than both the full gradient and the random projection approach on the proposed retrieval benchmark. It also achieves these gains while being more computationally efficient than random projection.", "conclusion": "Targeted, architecturally informed gradient component selection is a more effective and efficient strategy than random projections or full gradients for instance-based influence explanations in large models. This suggests that carefully choosing which parts of the model to use for gradient-based explanations can substantially improve both influence quality and computational feasibility."}}
{"id": "2601.16753", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16753", "abs": "https://arxiv.org/abs/2601.16753", "authors": ["Xinyi Wang", "Grazziela Figueredo", "Ruizhe Li", "Xin Chen"], "title": "Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation", "comment": null, "summary": "Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper tool to consistently label temporal changes in both ground-truth and model-generated texts for meaningful comparisons. Existing annotation methods are typically labor-intensive, relying on the use of manual lexicons and rules. Complex rules are closed-source, domain specific and hard to adapt, whereas overly simple ones tend to miss essential specialised information. Large language models (LLMs) offer a promising annotation alternative, as they are capable of capturing nuanced linguistic patterns and semantic similarities without extensive manual intervention. They also adapt well to new contexts. In this study, we therefore propose an LLM-based pipeline to automatically annotate longitudinal information in radiology reports. The pipeline first identifies sentences containing relevant information and then extracts the progression of diseases. We evaluate and compare five mainstream LLMs on these two tasks using 500 manually annotated reports. Considering both efficiency and performance, Qwen2.5-32B was subsequently selected and used to annotate another 95,169 reports from the public MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us with a standardized benchmark for evaluating report generation models. Using this new benchmark, we assessed seven state-of-the-art report generation models. Our LLM-based annotation method outperforms existing annotation solutions, achieving 11.3\\% and 5.3\\% higher F1-scores for longitudinal information detection and disease tracking, respectively.", "AI": {"tldr": "They build and evaluate an LLM-based pipeline to automatically detect and label longitudinal (temporal change) information in radiology reports, then use it to create a large benchmark and show it beats prior rule-based annotators.", "motivation": "Longitudinal information in radiology reports\u2014how findings change across time\u2014is key for monitoring disease and making decisions. Many new report generation models claim to capture such temporal changes, but there is no good, scalable way to validate them: existing annotation is manual, labor-intensive, rule/lexicon-based, often closed-source, domain-specific, and misses nuanced information. A generalizable, automated, high-quality annotation approach is needed to both study longitudinal information and evaluate generation models.", "method": "They design a two-stage LLM-based annotation pipeline: (1) detect sentences in radiology reports that contain longitudinal information; (2) extract disease progression details (e.g., improvement, worsening, stability). They systematically evaluate five mainstream LLMs on these tasks against a set of 500 manually annotated reports, trading off accuracy and efficiency. Based on this comparison, they select Qwen2.5-32B as the main annotator and apply it to automatically label another 95,169 reports from the MIMIC-CXR dataset, building a large annotated corpus for benchmarking report generation models. They then use these standardized annotations to quantitatively evaluate seven state-of-the-art radiology report generation systems.", "result": "The proposed LLM-based annotation pipeline achieves substantially better performance than existing rule/lexicon-based annotation solutions, with gains of 11.3 percentage points in F1 for detecting longitudinal information and 5.3 percentage points in F1 for tracking disease progression. Qwen2.5-32B is identified as the best trade-off between accuracy and computational efficiency among the five tested LLMs and is successfully used to annotate 95k+ MIMIC-CXR reports, yielding a large-scale benchmark. This benchmark supports systematic comparison of seven contemporary report generation models under a unified longitudinal-evaluation framework.", "conclusion": "An LLM-driven annotation pipeline can robustly and efficiently label longitudinal information in radiology reports, outperforming traditional rule-based annotators and scaling to large datasets. The resulting Qwen2.5-32B-annotated corpus provides a standardized benchmark for objectively assessing how well report generation models capture temporal disease trajectories. This work demonstrates that general-purpose LLMs are practical tools for automating specialized clinical NLP annotation tasks and for enabling more meaningful evaluation of generative models in medical imaging."}}
{"id": "2601.16766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16766", "abs": "https://arxiv.org/abs/2601.16766", "authors": ["Debtanu Datta", "Mohan Kishore Chilukuri", "Yash Kumar", "Saptarshi Ghosh", "Muhammad Bilal Zafar"], "title": "Do LLM hallucination detectors suffer from low-resource effect?", "comment": "Accepted at EACL 2026 (Main)", "summary": "LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource languages like Bengali. We study the intersection of these issues and ask: do hallucination detectors suffer from the low-resource effect? We conduct experiments on five tasks across three domains (factual recall, STEM, and Humanities). Experiments with four LLMs and three hallucination detectors reveal a curious finding: As expected, the task accuracies in low-resource languages experience large drops (compared to English). However, the drop in detectors' accuracy is often several times smaller than the drop in task accuracy. Our findings suggest that even in low-resource languages, the internal mechanisms of LLMs might encode signals about their uncertainty. Further, the detectors are robust within language (even for non-English) and in multilingual setups, but not in cross-lingual settings without in-language supervision.", "AI": {"tldr": "The paper studies whether hallucination detectors for large language models (LLMs) also degrade in low-resource languages and finds that detectors remain relatively robust even when task performance drops sharply.", "motivation": "LLMs hallucinate (produce incorrect information) and perform much worse in low-resource languages. While hallucination detectors are being developed to mitigate hallucinations, it is unclear whether these detectors themselves suffer from low-resource language performance issues. Understanding this is crucial to deploying safer LLMs globally, not just in high-resource languages.", "method": "The authors evaluate four LLMs and three hallucination detectors across five tasks spanning factual recall, STEM, and Humanities. They compare model task accuracy and detector accuracy in high-resource (English) versus low-resource (e.g., Bengali) languages. They analyze performance in three regimes: within-language, multilingual, and cross-lingual setups, checking how well detectors identify hallucinations in each setting.", "result": "They find a substantial drop in LLM task accuracy when moving from English to low-resource languages, confirming the low-resource effect. However, the hallucination detectors' performance drops much less\u2014often several times smaller than the degradation in task accuracy. Detectors are relatively robust when trained and tested within the same language (including non-English) and in multilingual configurations, but their performance degrades in cross-lingual scenarios without supervision in the target language.", "conclusion": "Hallucination detectors, unlike base task capabilities, are comparatively robust in low-resource languages, suggesting that LLMs encode uncertainty signals that detectors can exploit even where knowledge and task performance are weak. However, this robustness relies on in-language or multilingual supervision; detectors do not generalize well in purely cross-lingual settings, highlighting the need for language-aware or multilingual training for safe deployment in low-resource languages."}}
{"id": "2601.16690", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16690", "abs": "https://arxiv.org/abs/2601.16690", "authors": ["Xinze Li", "Ziyue Zhu", "Siyuan Liu", "Yubo Ma", "Yuhang Zang", "Yixin Cao", "Aixin Sun"], "title": "EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents", "comment": "25 pages", "summary": "We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent's own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.", "AI": {"tldr": "EMemBench is a programmable benchmark that tests long\u2011term memory of language and vision-language agents via interactive games, generating verifiable questions from the agents\u2019 own trajectories.", "motivation": "Existing long-term memory evaluations for agents are limited: they often rely on static datasets, fixed questions, or narrow tasks that do not capture the complexity of interactive environments or the agent\u2019s own experience. There is a need for a systematic, scalable, and controllable way to measure episodic and long-horizon memory skills\u2014especially in multimodal, game-like settings\u2014so that memory architectures and prompting strategies can be meaningfully compared and improved.", "method": "The authors design EMemBench, a programmatic benchmark that runs agents in text and visual game environments and then automatically generates questions from the agents\u2019 interaction trajectories (states, actions, and game signals). They define templates that compute ground-truth answers directly from game logs, ensuring verifiability and allowing control over difficulty and answerability. The questions cover a balanced set of memory skills: single-hop and multi-hop recall, inductive generalization over repeated patterns, temporal ordering, spatial reasoning, logical reasoning, and adversarial (tricky) cases. They instantiate memory agents using strong language models and vision-language models as backbones, compare them against in-context prompting baselines, and run evaluations across 15 text games and multiple visual seeds. They also conduct a human study to assess human performance and validate task difficulty.", "result": "Across a diverse suite of text-based games and visual environments, all evaluated agents perform well below ceiling. Induction and spatial reasoning are consistently the weakest areas, with performance degrading further in visual settings. Memory modules (persistent memory) substantially improve performance for agents with open-source language-model backbones in text games, indicating that better memory representations help. However, for vision-language model agents, adding persistent memory yields less stable and smaller gains, revealing a particular weakness in visually grounded episodic memory. The human study shows that even humans find EMemBench challenging, but still outperform current agents, underscoring the performance gap.", "conclusion": "EMemBench provides a scalable, programmable, and verifiable benchmark for long-term memory in interactive agents, spanning both text and visual environments and a broad set of memory skills. Current state-of-the-art language and vision-language models, even when augmented with memory mechanisms, perform far from optimally\u2014especially on induction and spatial tasks and in visual scenarios. These findings indicate that robust, visually grounded episodic memory remains unsolved and that EMemBench can serve as a useful testbed for developing and comparing future memory architectures and training strategies."}}
{"id": "2601.16803", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16803", "abs": "https://arxiv.org/abs/2601.16803", "authors": ["Carolin Holtermann", "Florian Schneider", "Anne Lauscher"], "title": "SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation", "comment": null, "summary": "Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt's semantics. Yet a comprehensive analysis of this behavior, which we dub Surface-over-Semantics (SoS), is missing. We present the first analysis of T2I models' SoS tendencies. To this end, we create a set of prompts covering 171 cultural identities, translated into 14 languages, and use it to prompt seven T2I models. To quantify SoS tendencies across models, languages, and cultures, we introduce a novel measure and analyze how the tendencies we identify manifest visually. We show that all but one model exhibit strong surface-level tendency in at least two languages, with this effect intensifying across the layers of T2I text encoders. Moreover, these surface tendencies frequently correlate with stereotypical visual depictions.", "AI": {"tldr": "The paper studies how text-to-image models rely on input language surface forms (Surface-over-Semantics, SoS), leading to stereotypical generations, and proposes a metric plus a multilingual benchmark to analyze this behavior across models, languages, and cultures.", "motivation": "Prior work showed text-to-image models are sensitive to input language and may produce stereotypical, culturally biased images when prompts are not in English. However, there is no systematic, large-scale, multilingual, and cross-cultural analysis quantifying how much these models prioritize surface language form over semantic content, nor how this behavior evolves inside the models. This paper aims to fill that gap.", "method": "The authors construct a prompt set that covers 171 cultural identities and translate it into 14 languages. They then prompt seven different text-to-image models with these multilingual prompts. They introduce a new quantitative metric to measure Surface-over-Semantics (SoS) tendencies across models, languages, and cultures, and they probe different layers of the T2I text encoders to see how SoS evolves through the network. They also visually examine the generated images to connect metric scores with qualitative patterns and stereotypical depictions.", "result": "All but one of the seven evaluated text-to-image models exhibit strong SoS tendencies in at least two languages, meaning the models rely heavily on language surface features rather than semantic equivalence of prompts. The SoS effect becomes stronger in deeper layers of the text encoders. The authors find that higher SoS scores are often associated with more stereotypical and culturally reductive visual outputs.", "conclusion": "Text-to-image models systematically prioritize surface language features over semantic equivalence (SoS), particularly for non-English prompts, and this behavior strengthens in deeper text-encoder layers. This surface bias is closely tied to stereotypical cultural imagery. The findings highlight the need for architectures, training strategies, and evaluation protocols that explicitly address multilingual robustness and reduce surface-driven, stereotype-reinforcing generations."}}
{"id": "2601.16711", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16711", "abs": "https://arxiv.org/abs/2601.16711", "authors": ["Shanshan Liu", "Noriki Nishida", "Fei Cheng", "Narumi Tokunaga", "Rumana Ferdous Munne", "Yuki Yamagata", "Kouji Kozaki", "Takehito Utsuro", "Yuji Matsumoto"], "title": "Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition", "comment": "Accepted to EACL 2026 (Main)", "summary": "Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical concept indices and novel metrics to measure generalization. Second, we explore LLM-based Auto-Labeled Data (ALD) as a scalable resource, creating a task-specific pipeline for its generation. Our research unequivocally shows that while LLM-generated ALD cannot fully substitute for manual annotations, it is a valuable resource for improving generalization, successfully providing models with the broader coverage and structural knowledge needed to approach recognizing unseen concepts. Code and datasets are available at https://github.com/bio-ie-tool/hi-ald.", "AI": {"tldr": "This paper studies how to make biomedical concept recognition systems better at handling unseen concepts by proposing a new evaluation framework and by using LLM-generated auto-labeled data to improve generalization.", "motivation": "Mention-agnostic Biomedical Concept Recognition (MA-BCR) must recognize biomedical concepts without relying on explicit mention spans, but human-annotated data is scarce and existing systems often fail on unseen or rare concepts. There is a lack of systematic ways to evaluate generalization to such unseen concepts and a need for scalable supervision signals that go beyond limited manual annotation.", "method": "The authors introduce: (1) an evaluation framework based on hierarchical biomedical concept indices along with new metrics that explicitly quantify how well models generalize to unseen concepts at different levels of the hierarchy; and (2) a large language model\u2013based pipeline to generate Auto-Labeled Data (ALD) for MA-BCR. The pipeline automatically produces task-specific examples and labels designed to capture broader concept coverage and structure, which are then used to train or augment MA-BCR models.", "result": "Empirical experiments show that LLM-generated ALD significantly improves model performance on unseen biomedical concepts compared to using only manual annotations, especially in terms of metrics that capture hierarchical and structural generalization. However, models trained purely on ALD do not match the performance of those trained on high-quality human annotations; the best performance usually comes from combining manual data with ALD.", "conclusion": "LLM-based auto-labeled data is an effective, scalable complement\u2014but not a replacement\u2014to manual annotation for MA-BCR. By pairing ALD with a hierarchical evaluation framework, the paper demonstrates a practical path toward better generalization to unseen biomedical concepts, and releases code and datasets to support further research."}}
{"id": "2601.16823", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16823", "abs": "https://arxiv.org/abs/2601.16823", "authors": ["Leonard S. Pleiss", "Maximilian Schiffer", "Robert K. von Weizs\u00e4cker"], "title": "Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess", "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities, yet it remains unclear to what extent these reflect sophisticated recall (crystallized intelligence) or reasoning ability (fluid intelligence). We introduce chess as a controlled testbed for disentangling these faculties. Leveraging the game's structure and scalable engine evaluations, we construct a taxonomy of positions varying in training corpus proximity--ranging from common states solvable by memorization to novel ones requiring first-principles reasoning. We systematically evaluate multiple GPT generations under varying reasoning intensities. Our analysis reveals a clear gradient: performance consistently degrades as fluid intelligence demands increase. Notably, in out-of-distribution tasks, performance collapses to random levels. While newer models improve, progress slows significantly for tasks outside the training distribution. Furthermore, while reasoning-augmented inference improves performance, its marginal benefit per token decreases with distributional proximity. These results suggest current architectures remain limited in systematic generalization, highlighting the need for mechanisms beyond scale to achieve robust fluid intelligence.", "AI": {"tldr": "The paper uses chess as a controlled environment to separate memorization from reasoning in LLMs and finds that performance drops sharply as true reasoning demands increase, especially on out-of-distribution positions.", "motivation": "Although LLMs show strong performance on many tasks, it is unclear whether this reflects genuine reasoning (fluid intelligence) or sophisticated pattern recall (crystallized intelligence). Existing benchmarks often mix these aspects and lack precise control over how similar evaluation cases are to models\u2019 training data. The authors aim to build a testbed that can more cleanly dissociate memorization from reasoning and measure systematic generalization.", "method": "The authors use chess as a structured, controllable domain where they can systematically vary how similar test positions are to ones likely seen in pretraining data. They define a taxonomy of chess positions ranging from very common, easily memorized setups to highly novel, out-of-distribution configurations that demand first-principles reasoning. Using engine evaluations as ground truth, they test multiple generations of GPT models under different levels of \"reasoning intensity\" (e.g., base decoding vs. reasoning-augmented inference) and measure performance across this distributional spectrum.", "result": "They find a consistent performance gradient: models do well on positions close to the training distribution but degrade as tasks require more fluid intelligence. In the most out-of-distribution settings, performance falls to near-random levels. Newer, larger models perform better overall but show sharply diminished gains on the hardest, most novel tasks. Reasoning-augmented inference improves accuracy, but the marginal performance gain per additional reasoning token shrinks as positions become more distributionally distant.", "conclusion": "Current LLM architectures still struggle with systematic generalization and robust fluid reasoning. Scaling and adding more inference-time reasoning help mainly on in-distribution or near-distribution tasks, but do not solve failures on truly novel problems. The authors argue that achieving robust fluid intelligence will require architectural or algorithmic advances beyond simply scaling up existing LLM designs."}}
{"id": "2601.16724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16724", "abs": "https://arxiv.org/abs/2601.16724", "authors": ["Kevin Fan", "Eric Yun"], "title": "Mitigating Bias in Automated Grading Systems for ESL Learners: A Contrastive Learning Approach", "comment": null, "summary": "As Automated Essay Scoring (AES) systems are increasingly used in high-stakes educational settings, concerns regarding algorithmic bias against English as a Second Language (ESL) learners have increased. Current Transformer-based regression models trained primarily on native-speaker corpora often learn spurious correlations between surface-level L2 linguistic features and essay quality. In this study, we conduct a bias study of a fine-tuned DeBERTa-v3 model using the ASAP 2.0 and ELLIPSE datasets, revealing a constrained score scaling for high-proficiency ESL writing where high-proficiency ESL essays receive scores 10.3% lower than Native speaker essays of identical human-rated quality. To mitigate this, we propose applying contrastive learning with a triplet construction strategy: Contrastive Learning with Matched Essay Pairs. We constructed a dataset of 17,161 matched essay pairs and fine-tuned the model using Triplet Margin Loss to align the latent representations of ESL and Native writing. Our approach reduced the high-proficiency scoring disparity by 39.9% (to a 6.2% gap) while maintaining a Quadratic Weighted Kappa (QWK) of 0.76. Post-hoc linguistic analysis suggests the model successfully disentangled sentence complexity from grammatical error, preventing the penalization of valid L2 syntactic structures.", "AI": {"tldr": "The paper identifies and mitigates bias in Transformer-based automated essay scoring against high-proficiency ESL writers using contrastive learning with matched ESL\u2013Native essay triplets, significantly reducing score disparities while preserving overall scoring accuracy.", "motivation": "Automated Essay Scoring models, especially Transformer-based regressors trained mostly on native-speaker data, are increasingly deployed in high-stakes contexts but may systematically underrate English-as-a-Second-Language (ESL) writers. The authors are motivated by fairness and reliability concerns: surface-level L2 features (e.g., grammatical errors, non-native syntactic or lexical patterns) can become spurious proxies for essay quality, producing biased scores even when human raters judge ESL and Native essays as equally strong. They aim to rigorously quantify this bias and develop a principled method to reduce it without sacrificing predictive performance.", "method": "The authors perform a bias analysis of a fine-tuned DeBERTa-v3 regression model trained on AES data, using the ASAP 2.0 and ELLIPSE datasets. They compare model scores for high-proficiency ESL essays vs Native essays that have equivalent human-assigned quality scores, exposing systematic under-scoring of ESL texts. To mitigate this, they introduce a contrastive learning framework based on triplet margin loss, called Contrastive Learning with Matched Essay Pairs. They first build a dataset of 17,161 triplets comprising an anchor essay, a positive essay (same quality, different group), and a negative essay, where ESL\u2013Native pairs are matched on human-assigned scores. During fine-tuning, they encourage the latent representations of matched ESL and Native essays to be close, while pushing apart mismatched-quality pairs, thereby aligning the representation space across groups. Evaluation uses the scoring gap for high-proficiency ESL vs Native essays and overall Quadratic Weighted Kappa as accuracy metrics. Post-hoc linguistic analysis inspects how the fine-tuned model responds to sentence complexity and grammatical errors.", "result": "The initial bias study shows a constrained score scaling for high-proficiency ESL writing: when human raters consider ESL and Native essays to be of equal quality, the DeBERTa-v3 AES model assigns high-proficiency ESL essays scores that are on average 10.3% lower than those for Native essays. After applying the proposed contrastive learning approach with matched ESL\u2013Native essay pairs and triplet margin loss, the scoring disparity for high-proficiency writers is reduced by 39.9%, decreasing the gap from 10.3% to 6.2%, while maintaining a strong overall scoring performance with a Quadratic Weighted Kappa of 0.76. Linguistic analysis indicates that the updated model distinguishes between sentence complexity and grammatical error more effectively and no longer systematically penalizes valid L2 syntactic constructions.", "conclusion": "The paper concludes that standard Transformer-based AES models trained primarily on native-speaker data exhibit measurable bias against high-proficiency ESL writers, underestimating their performance despite equivalent human-rated quality. The proposed contrastive learning framework with matched ESL\u2013Native essay triplets successfully reduces this bias by substantially narrowing the scoring gap while preserving competitive overall accuracy. The findings suggest that aligning latent representations across demographic or proficiency groups is a promising direction for fairness-aware AES, and that richer training objectives can help models avoid spurious correlations between L2-specific linguistic surface features and essay quality, enabling more equitable treatment of ESL writers in high-stakes assessments."}}
{"id": "2601.16890", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16890", "abs": "https://arxiv.org/abs/2601.16890", "authors": ["Jo\u00e3o A. Leite", "Olesya Razuvayevskaya", "Kalina Bontcheva", "Carolina Scarton"], "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems", "comment": null, "summary": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.", "AI": {"tldr": "The paper shows that rephrasing factual claims using common persuasion techniques can significantly degrade the performance of automated fact-checking systems, revealing a new class of adversarial attacks.", "motivation": "Automated fact-checking systems are increasingly used to combat misinformation, but they are vulnerable to adversarial attacks that allow false claims to bypass detection. Existing attacks mainly use noise injection or semantic alterations and overlook the way real-world disinformation often relies on persuasion techniques to influence readers. The authors are motivated to close this gap by systematically studying how persuasive writing styles can be weaponized against fact-checking models.", "method": "The authors define a new adversarial attack framework where a generative large language model rephrases factual claims using specific persuasion techniques. They consider 15 techniques organized into 6 broader categories. Using a decoupled evaluation setup, they separately evaluate how these persuasive rephrasings affect (1) claim verification performance and (2) evidence retrieval by AFC systems. They conduct experiments on two standard fact-checking benchmarks, FEVER and FEVEROUS, applying persuasion-based transformations to the claims and then measuring the change in system performance.", "result": "Across both FEVER and FEVEROUS, the persuasion-based adversarial attacks substantially reduce the accuracy of claim verification as well as the quality/recall of evidence retrieval. The degradation is consistent enough to demonstrate that persuasive rephrasings are effective adversarial perturbations, not just small or random variations in text. The results also reveal that multiple persuasion categories can significantly impact the systems, suggesting the vulnerability is broad rather than tied to a single trick.", "conclusion": "Persuasion techniques constitute a powerful, previously underexplored class of adversarial attacks against automated fact-checking systems. Because such techniques are common in real-world disinformation, current AFC models may be overly optimistic in their robustness. The authors argue that future AFC research must explicitly consider and defend against persuasion-based adversarial text to build more reliable and resilient fact-checking tools."}}
{"id": "2601.16934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16934", "abs": "https://arxiv.org/abs/2601.16934", "authors": ["Elias Schuhmacher", "Andrianos Michail", "Juri Opitz", "Rico Sennrich", "Simon Clematide"], "title": "Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias", "comment": null, "summary": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our further analysis, we find that the positional bias stems from front-loaded attention distributions in pooling-token embeddings, where early tokens receive more attention. To mitigate this issue, we introduce an inference-time attention calibration method that redistributes attention more evenly across document positions, increasing discoverabiltiy of later segments. Our evaluation framework and attention calibration is available at https://github.com/impresso/fair-sentence-transformers", "AI": {"tldr": "The paper investigates and mitigates reflection biases in embedding-based document search, showing that current models over-represent early segments and high-resource languages, and proposes an inference-time attention calibration method to improve fairness and discoverability of all document parts.", "motivation": "As embedding-based retrieval becomes standard for searching long, multi-segment and multilingual documents, it is crucial that all parts of a document are equally discoverable. However, existing models may systematically favor certain positions (e.g., the beginning) or languages (e.g., English) in their embeddings, which can lead to unfair or incomplete retrieval results. The authors aim to precisely quantify and analyze these biases and provide practical methods to mitigate them without retraining models.", "method": "They introduce a permutation-based evaluation framework that measures how strongly each segment of a document is reflected in its overall embedding, allowing the detection of positional and language biases. Through this framework, they analyze state-of-the-art embedding models and trace positional bias to front-loaded attention distributions in pooling-token embeddings, where early tokens dominate attention. To address this, they propose an inference-time attention calibration technique that redistributes attention more evenly across document positions, requiring no model retraining.", "result": "Using the permutation-based framework, they empirically show that leading segments and segments written in higher-resource languages (especially English) are systematically over-represented in document embeddings, while later segments and segments in lower-resource languages are under-represented. Applying their attention calibration method reduces the positional bias and improves the representation and discoverability of later segments in embedding-based search, with code and tools released publicly for reproducibility.", "conclusion": "The paper concludes that widely used embedding models for document retrieval suffer from substantial positional and language reflection biases, which can compromise the fairness and completeness of search over long, multilingual documents. Their permutation-based evaluation framework makes such biases measurable, and their inference-time attention calibration provides a practical, training-free way to mitigate at least the positional bias, improving the balance of information captured from all document segments. They also release their framework and implementation to support further research and application in fairer embedding-based retrieval."}}
{"id": "2601.16781", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16781", "abs": "https://arxiv.org/abs/2601.16781", "authors": ["Paul Youssef", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "title": "Persuasion Tokens for Editing Factual Knowledge in LLMs", "comment": "Accepted at EACL Main 2026", "summary": "In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with small negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.", "AI": {"tldr": "The paper proposes persuasion tokens (P-Tokens), special learned tokens that replicate the effect of lengthy in-context knowledge editing demonstrations, enabling more efficient and scalable updating of LLMs\u2019 factual knowledge.", "motivation": "In-context knowledge editing (IKE) can update LLMs with new facts without retraining, but it needs long, fact-specific demonstrations. These are expensive to craft and occupy much of the model\u2019s context window, limiting scalability and practicality. The authors aim to retain the benefits of IKE while eliminating the need for bespoke, verbose prompts for every new fact.", "method": "The authors introduce persuasion tokens (P-Tokens), special tokens that are trained so that, when inserted into a prompt, they induce the same behavioral change as full IKE demonstrations would. They integrate P-Tokens into prompts in place of fact-specific editing examples and train them over editing tasks so that they generalize across facts and models. They then test P-Tokens on two knowledge-editing benchmarks and three different LLMs, and analyze robustness to distractor information, interference with nearby facts, and the effect of varying the number of P-Tokens.", "result": "Across two editing datasets and three LLMs, P-Tokens achieve knowledge-editing performance comparable to or better than standard IKE, without requiring long, customized demonstrations. Editing remains robust even in the presence of distractor content, showing only small adverse effects on neighboring, related facts. Performance improves as more P-Tokens are used in the prompt.", "conclusion": "Persuasion tokens offer a practical and scalable alternative to traditional in-context knowledge editing. By replacing long, fact-specific demonstrations with compact, learned tokens, they preserve or improve editing quality while saving context space and human effort. This makes knowledge updates to LLMs more efficient and robust, addressing major limitations of existing IKE approaches."}}
{"id": "2601.16800", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16800", "abs": "https://arxiv.org/abs/2601.16800", "authors": ["Gaurav Negi", "MA Waskow", "Paul Buitelaar"], "title": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis", "comment": null, "summary": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets.", "AI": {"tldr": "The paper investigates using large language models (LLMs) as automatic annotators and adjudicators for fine-grained opinion analysis tasks, demonstrating high agreement and reduced human cost via a declarative annotation pipeline.", "motivation": "Fine-grained opinion analysis, which includes identifying sentiments, targets, and opinion spans, offers detailed insights but is expensive and labor-intensive to annotate, especially across many domains. There is a shortage of domain-specific labeled datasets needed to train models for tasks like Aspect Sentiment Triplet Extraction and related opinion mining problems. The authors are motivated to determine whether LLMs can reliably replace or supplement human annotators to generate such fine-grained labels at scale and lower cost.", "method": "The authors design a declarative annotation pipeline that standardizes how LLMs are prompted and used to identify fine-grained opinion spans in text, reducing variability inherent in ad hoc prompt engineering. They then introduce a methodology where an LLM acts as an adjudicator to reconcile multiple label sets into a single final annotation. They test this pipeline with LLMs of different sizes on two benchmark tasks: Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS). They assess the quality and consistency of LLM-generated annotations primarily through Inter-Annotator Agreement across multiple LLM-based annotators.", "result": "Across the evaluated models and tasks (ASTE and ACOS), the LLM-based annotation pipeline yields high Inter-Annotator Agreement among different LLM annotators, indicating consistent and reliable labeling behavior. The adjudication step using an LLM to combine multiple proposed labels is effective, resulting in coherent final annotations. These findings suggest that LLMs can operate as both annotators and adjudicators for fine-grained opinion analysis with quality close to that of human annotation in terms of consistency.", "conclusion": "The study concludes that LLMs are feasible and effective as automatic annotators and adjudicators for fine-grained opinion analysis tasks. By using a declarative annotation pipeline and an LLM-based adjudication mechanism, it is possible to generate high-quality, consistent annotations while significantly reducing the human labor and cost traditionally required to build such fine-grained opinion datasets. This approach can help alleviate the shortage of domain-specific labeled resources in opinion mining and related applications."}}
{"id": "2601.16946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16946", "abs": "https://arxiv.org/abs/2601.16946", "authors": ["Danil Semin", "Ond\u0159ej Du\u0161ek", "Zden\u011bk Kasner"], "title": "Strategies for Span Labeling with Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize these strategies into three families: tagging the input text, indexing numerical positions of spans, and matching span content. To address the limitations of content matching, we introduce LogitMatch, a new constrained decoding method that forces the model's output to align with valid input spans. We evaluate all methods across four diverse tasks. We find that while tagging remains a robust baseline, LogitMatch improves upon competitive matching-based methods by eliminating span matching issues and outperforms other strategies in some setups.", "AI": {"tldr": "The paper analyzes prompting strategies for span labeling with LLMs and proposes LogitMatch, a constrained decoding method that aligns outputs with input spans, outperforming some existing approaches.", "motivation": "Generative LLMs are widely used for text analysis tasks but lack an explicit way to point to specific input spans, causing reliance on ad-hoc prompting strategies that yield inconsistent span-labeling performance. The paper aims to systematically categorize these strategies and address the limitations of content-matching approaches.", "method": "The authors categorize existing span-labeling prompting strategies into three families: tagging the input text, indexing span positions with numbers, and matching span content. They then introduce LogitMatch, a constrained decoding technique that restricts the model's outputs to valid input spans to avoid content-matching errors. All strategies, including LogitMatch, are evaluated on four diverse span-labeling tasks.", "result": "Tagging-based prompting is found to be a strong and robust baseline across tasks. However, LogitMatch improves over other matching-based strategies by removing span-matching errors and achieves superior performance compared to alternative strategies in several experimental setups.", "conclusion": "Systematically organizing span-labeling strategies reveals strengths and weaknesses of tagging, indexing, and content matching. The proposed LogitMatch constrained decoding method effectively resolves span-matching issues and can outperform other strategies, suggesting it is a promising approach for reliable span labeling with generative LLMs."}}
