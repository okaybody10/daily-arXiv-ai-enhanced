{"id": "2601.08835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08835", "abs": "https://arxiv.org/abs/2601.08835", "authors": ["Vaarunay Kaushal", "Taranveer Singh"], "title": "DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols", "comment": "6 pages, 5 figures", "summary": "Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.", "AI": {"tldr": "The paper introduces DELIBERATIONBENCH to test whether multi-LLM deliberation actually helps, and finds that simply picking the best single response from a pool strongly outperforms more complex deliberation protocols.", "motivation": "Multi-agent LLM systems where several models interact and deliberate are popular and often assumed to yield better answers than simpler setups, but this assumption has not been rigorously tested under controlled conditions. The authors want to critically evaluate whether deliberation truly adds value beyond a strong but simple baseline.", "method": "They design DELIBERATIONBENCH, a benchmark with 270 questions, and compare three different LLM deliberation protocols against a baseline that just selects the best response from a pool of model outputs. They run each setting with three independent random seeds, leading to 810 evaluations, and use statistical tests to compare win rates, while also measuring computational cost.", "result": "The best-single-response baseline wins in 82.5% \u00b1 3.3% of cases, while the best deliberation protocol wins only 13.8% \u00b1 2.6% of cases. This corresponds to about a 6x performance gap in favor of the simple baseline, and the deliberation approaches also incur 1.5\u20132.5x higher computational cost. The difference is statistically significant (p < 0.01).", "conclusion": "The study shows that, at least under their benchmark and settings, multi-LLM deliberation protocols are not only more expensive but also substantially worse than a simple best-of-n sampling baseline. This challenges the prevailing belief that adding complexity and agent-style deliberation automatically improves LLM system quality, and suggests that researchers should more carefully justify and benchmark such systems."}}
{"id": "2601.08836", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.08836", "abs": "https://arxiv.org/abs/2601.08836", "authors": ["Zaber Al Hassan Ayon", "Nur Hafieza Ismail", "Nur Shazwani Kamarudin"], "title": "A Review: PTSD in Pre-Existing Medical Condition on Social Media", "comment": "Published in (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 15, No. 11, 2024", "summary": "Post-Traumatic Stress Disorder (PTSD) is a multifaceted mental health condition, particularly challenging for individuals with pre-existing medical conditions. This review critically examines the intersection of PTSD and chronic illnesses as expressed on social media platforms. By systematically analyzing literature from 2008 to 2024, the study explores how PTSD manifests and is managed in individuals with chronic conditions such as cancer, heart disease, and autoimmune disorders, with a focus on online expressions on platforms like X (formally known as Twitter) and Facebook. Findings demonstrate that social media data offers valuable insights into the unique challenges faced by individuals with both PTSD and chronic illnesses. Specifically, natural language processing (NLP) and machine learning (ML) techniques can identify potential PTSD cases among these populations, achieving accuracy rates between 74% and 90%. Furthermore, the role of online support communities in shaping coping strategies and facilitating early interventions is highlighted. This review underscores the necessity of incorporating considerations of pre-existing medical conditions in PTSD research and treatment, emphasizing social media's potential as a monitoring and support tool for vulnerable groups. Future research directions and clinical implications are also discussed, with an emphasis on developing targeted interventions.", "AI": {"tldr": "The paper is a systematic review of how PTSD in people with chronic illnesses is expressed and managed on social media, and how NLP/ML can detect and support these patients.", "motivation": "PTSD is harder to understand and treat in people who already have chronic physical illnesses, yet their lived experiences and symptoms are increasingly shared online. There is a need to synthesize how PTSD co-occurs with chronic disease in social media data and how that information can improve detection, monitoring, and support.", "method": "The authors conduct a systematic literature review (2008\u20132024) of studies that analyze social media content (e.g., X/Twitter, Facebook) related to PTSD among individuals with chronic illnesses such as cancer, heart disease, and autoimmune disorders. They focus on works using NLP and ML to identify PTSD signals and on studies examining online support communities and coping behaviors.", "result": "The review finds that social media posts contain rich signals about PTSD symptoms, challenges, and coping strategies among people with chronic illnesses. NLP/ML models can identify potential PTSD cases from social media with reported accuracies between 74% and 90%. Studies also show that online support communities influence coping, provide peer support, and may facilitate earlier recognition and intervention for PTSD symptoms.", "conclusion": "The paper concludes that PTSD research and clinical practice must explicitly account for pre-existing medical conditions, and that social media is a promising source for monitoring, early detection, and support for vulnerable patients with both PTSD and chronic illness. It calls for more targeted, ethically designed, and clinically integrated interventions and outlines directions for future research to improve accuracy, fairness, and real-world clinical impact."}}
{"id": "2601.08837", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08837", "abs": "https://arxiv.org/abs/2601.08837", "authors": ["Piercosma Bisconti", "Marcello Galisai", "Matteo Prandi", "Federico Pierucci", "Olga Sorokoletova", "Francesco Giarrusso", "Vincenzo Suriani", "Marcantonio Brancale", "Daniele Nardi"], "title": "From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda", "comment": null, "summary": "Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.", "AI": {"tldr": "The paper shows that LLM safety can be bypassed by embedding harmful requests inside stylized narrative-structural analysis tasks (\u201cAdversarial Tales\u201d), achieving high jailbreak success across many frontier models.", "motivation": "Current LLM safety relies heavily on pattern-matching and surface-form detectors for harmful content. However, users can disguise malicious intent using culturally coded, ostensibly benign structures such as stories, poems, or other genres. Existing jailbreaks using these frames (e.g., poems) look like isolated tricks, and we lack a systematic understanding of why they work and how broad the vulnerability class is. The authors are motivated to (1) test whether narrative-structural framing is a general jailbreak vector across many models and providers, and (2) build a foundation for mechanistically understanding how such framings manipulate internal representations of intent, so as to design more robust safety mechanisms.", "method": "The authors propose \u201cAdversarial Tales,\u201d a jailbreak method that: (1) embeds harmful content within cyberpunk-style narrative prompts, and (2) asks the model to perform functional/structural analysis of the story, drawing on Vladimir Propp\u2019s morphology of folktales (e.g., decomposing plots into roles and functions). By casting the problem as a literary or structural interpretation task instead of a direct instruction-following task, the model is induced to reconstruct and describe harmful procedures as though it were simply analyzing narrative structure. They systematically evaluate this attack on 26 frontier LLMs from nine providers, measuring attack success rates\u2014i.e., the frequency with which models output the embedded harmful information despite existing safety filters. They compare performance across model families to assess robustness and generality of the vulnerability. They also conceptually outline a mechanistic interpretability agenda (rather than performing it fully) to study how narrative cues affect internal representations of harmful intent.", "result": "\u201cAdversarial Tales\u201d achieves a high mean jailbreak success rate of 71.3% across 26 frontier models, indicating that most tested systems can be reliably induced to output harmful content when it is wrapped in the proposed narrative-structural framing. No model family was consistently robust against this style of attack. Combined with prior evidence from their earlier \u201cAdversarial Poetry\u201d work, these experiments show that structurally grounded, culturally coded jailbreaks are widely effective and not mere one-off exploits for specific models. The results support the claim that pattern-matching defenses focused on surface strings or obvious red flags are insufficient to handle the breadth of such attacks.", "conclusion": "The paper concludes that LLM safety mechanisms are systematically vulnerable to structurally grounded jailbreaks that exploit culturally coded frames like narratives and poetry. These attacks work not because of a narrow loophole but because models fail to reliably track harmful intent when it is embedded in complex surface forms, and instead treat the interaction as a benign interpretive or analytic task. The authors argue that the space of such culturally coded attack frames is extremely large and cannot be exhaustively covered by heuristic or pattern-based filters. Consequently, they call for a shift toward mechanistic interpretability research aimed at understanding how narrative and other contextual cues reshape internal representations, and whether models can be trained to recognize and block harmful intent regardless of its narrative or stylistic disguise."}}
{"id": "2601.08838", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08838", "abs": "https://arxiv.org/abs/2601.08838", "authors": ["Jiahui Chen", "Lei Fu", "Jian Cui", "Yu Lei", "Zhenning Dong"], "title": "Companion Agents: A Table-Information Mining Paradigm for Text-to-SQL", "comment": "11 pages", "summary": "Large-scale Text-to-SQL benchmarks such as BIRD typically assume complete and accurate database annotations as well as readily available external knowledge, which fails to reflect common industrial settings where annotations are missing, incomplete, or erroneous. This mismatch substantially limits the real-world applicability of state-of-the-art (SOTA) Text-to-SQL systems. To bridge this gap, we explore a database-centric approach that leverages intrinsic, fine-grained information residing in relational databases to construct missing evidence and improve Text-to-SQL accuracy under annotation-scarce conditions. Our key hypothesis is that when a query requires multi-step reasoning over extensive table information, existing methods often struggle to reliably identify and utilize the truly relevant knowledge. We therefore propose to \"cache\" query-relevant knowledge on the database side in advance, so that it can be selectively activated at inference time. Based on this idea, we introduce Companion Agents (CA), a new Text-to-SQL paradigm that incorporates a group of agents accompanying database schemas to proactively mine and consolidate hidden inter-table relations, value-domain distributions, statistical regularities, and latent semantic cues before query generation. Experiments on BIRD under the fully missing evidence setting show that CA recovers +4.49 / +4.37 / +14.13 execution accuracy points on RSL-SQL / CHESS / DAIL-SQL, respectively, with larger gains on the Challenging subset +9.65 / +7.58 / +16.71. These improvements stem from CA's automatic database-side mining and evidence construction, suggesting a practical path toward industrial-grade Text-to-SQL deployment without reliance on human-curated evidence.", "AI": {"tldr": "The paper proposes Companion Agents (CA), a database-centric, agent-based framework that pre-mines and caches fine-grained relational and statistical evidence from databases to improve Text-to-SQL performance when schema annotations and external knowledge are missing or unreliable.", "motivation": "Existing large-scale Text-to-SQL benchmarks assume rich, accurate database annotations and easy access to external knowledge, which is unrealistic for many industrial databases where such annotations are missing, incomplete, or wrong. Under these annotation-scarce conditions, SOTA Text-to-SQL models struggle, especially on queries that require multi-step reasoning over complex schemas and large tables. There is a need for methods that can automatically construct and organize the necessary evidence directly from the database itself, reducing reliance on human-curated metadata and external resources.", "method": "The authors propose a database-centric paradigm called Companion Agents (CA). For each database, a group of agents is attached that proactively analyzes the database before query time. These agents mine and consolidate different kinds of intrinsic evidence: inter-table relationships, value-domain distributions, statistical patterns, and latent semantic signals present in schema and data. This mined information is then cached as query-relevant knowledge on the database side. At inference time, when a natural language query is given, the system selectively activates and uses this pre-constructed evidence to guide Text-to-SQL generation, improving the model\u2019s ability to perform multi-step reasoning and select the correct subsets of schema and data.", "result": "On the BIRD benchmark under a fully missing-evidence setting (i.e., no human-provided annotations or external knowledge), Companion Agents significantly improve execution accuracy over baselines. CA yields gains of +4.49, +4.37, and +14.13 execution accuracy points on the RSL-SQL, CHESS, and DAIL-SQL components respectively. On the Challenging subsets of these tasks, the improvements are even larger: +9.65, +7.58, and +16.71 points. These results demonstrate that proactive, database-side evidence mining can substantially close the gap caused by missing annotations.", "conclusion": "The study concludes that Companion Agents offer a practical and effective path toward deploying Text-to-SQL systems in real-world, annotation-scarce industrial environments. By automatically mining and caching fine-grained evidence from relational databases prior to inference, CA reduces dependence on human-curated schema annotations and external knowledge, and measurably boosts performance on complex, multi-step reasoning queries. This database-centric, agent-based paradigm can make Text-to-SQL technology more robust and easier to adopt in practice."}}
{"id": "2601.08839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08839", "abs": "https://arxiv.org/abs/2601.08839", "authors": ["Toshiyuki Shigemura"], "title": "Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework", "comment": "25 pages, 9 figures. Pilot feasibility study using public-access large language models without API-level orchestration", "summary": "This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.", "AI": {"tldr": "Tri-agent framework using three different LLMs to cross-validate each other, enabling more stable and explainable multi-model reasoning via recursive interactions.", "motivation": "Single LLMs can be unstable, inconsistent, and opaque. There is a need for architectures that coordinate multiple heterogeneous LLMs to improve reliability, detect deviations, and enhance transparency, especially in real-world settings where only public-access deployments are available.", "method": "Propose a tri-agent architecture with three distinct LLM roles: (1) semantic generation, (2) analytical consistency checking, and (3) transparency auditing. These agents interact in a recursive loop that performs Recursive Knowledge Synthesis (RKS). The process is modeled via fixed-point theory, and empirically evaluated in 47 controlled trials using public-access LLMs from October 2025. Four metrics are defined: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR).", "result": "In 47 trials, the system achieved an average Reflex Reliability Score of about 0.78 with low variance, maintained Transparency Score \u2265 0.8 in roughly two-thirds of the trials, and converged in about 89% of runs. The behavior aligns with the theoretical model where the transparency auditor acts as a contraction operator, driving the system toward stable fixed points.", "conclusion": "A tri-agent, human-supervised multi-LLM architecture can achieve stable recursive knowledge synthesis with improved transparency and reliability in realistic, publicly deployed environments. The formal RKS model and empirical metrics provide an initial foundation for analyzing and designing safety-preserving multi-LLM systems."}}
{"id": "2601.08840", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08840", "abs": "https://arxiv.org/abs/2601.08840", "authors": ["Xiaoqi Han", "V\u00edctor Guti\u00e9rrez-Basulto", "Ru Li", "Xiaoli Li", "Jiye Liang", "Jeff Z. Pan"], "title": "Consistency-Aware Editing for Entity-level Unlearning in Language Models", "comment": null, "summary": "Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on full-model fine-tuning or prompt-based interventions, which can be computationally expensive or brittle when handling paraphrased queries. Recently, model editing has emerged as an efficient alternative for updating knowledge in LLMs, offering a promising direction for unlearning. However, existing editing techniques are typically designed for instance-level updates, modifying responses to specific attributes of an entity rather than eliminating all knowledge associated with the entity. In this paper, we investigate how editing techniques can be adapted for effective and efficient entity-level unlearning. To this end, we introduce a novel consistency-aware editing (CAE) framework. CAE aggregates a diverse set of prompts related to a target entity, including its attributes, relations, and adversarial paraphrases. It then jointly learns a low-rank update guided by a consistency regularizer that aligns the editing directions across prompts. This promotes robust and comprehensive forgetting while minimizing interference with unrelated knowledge. We further examine where different entities are stored within the model and how many diverse prompts are needed for successful unlearning. We evaluate CAE on two challenging benchmarks, RWKU and ToFU, and demonstrate that it (i) provides insights into how entity-level knowledge is internally represented and deleted in LLMs, (ii) significantly improves forgetting accuracy and robustness over traditional unlearning and editing baselines, and (iii) enables scalable entity removal using only tens of carefully selected prompts.", "AI": {"tldr": "They propose a new method, Consistency-Aware Editing (CAE), to efficiently erase all knowledge about a specific entity from an LLM while preserving its other abilities.", "motivation": "LLMs can memorize and reveal sensitive, copyrighted, or harmful information about particular entities. Existing unlearning methods are either computationally heavy (full fine-tuning) or fragile to paraphrases (prompt tricks), and current model-editing tools only handle narrow, instance-level changes rather than deleting an entity entirely. The authors want an efficient, robust way to remove all knowledge about an entity without degrading the rest of the model.", "method": "They design a consistency-aware editing (CAE) framework that: (1) collects a diverse set of prompts about the target entity, covering attributes, relations, and adversarial paraphrases; (2) learns a single low-rank parameter update to the model; and (3) adds a consistency regularizer to make the edit directions from all prompts align, so the model forgets the entity in a coherent way and reduces side effects on unrelated knowledge. They also analyze where entities are stored in the model and how the number/diversity of prompts affects unlearning success.", "result": "On two benchmarks for knowledge unlearning, RWKU and ToFU, CAE forgets entities more accurately and robustly than traditional unlearning and editing baselines. It remains effective against paraphrased or adversarial queries and can unlearn entities using only tens of carefully chosen prompts, while causing less interference with unrelated information. It also yields insights into how entity-level knowledge is localized and removed in LLMs.", "conclusion": "Consistency-aware editing is an effective and scalable approach for entity-level unlearning in LLMs. By jointly optimizing over diverse prompts with a consistency constraint, CAE can robustly erase all knowledge about specific entities, maintain overall model performance, and offer empirical insight into where and how entity information is represented in the network."}}
{"id": "2601.08841", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.08841", "abs": "https://arxiv.org/abs/2601.08841", "authors": ["Mihael Arcan"], "title": "Triples and Knowledge-Infused Embeddings for Clustering and Classification of Scientific Documents", "comment": null, "summary": "The increasing volume and complexity of scientific literature demand robust methods for organizing and understanding research documents. In this study, we explore how structured knowledge, specifically, subject-predicate-object triples, can enhance the clustering and classification of scientific papers. We propose a modular pipeline that combines unsupervised clustering and supervised classification over multiple document representations: raw abstracts, extracted triples, and hybrid formats that integrate both. Using a filtered arXiv corpus, we extract relational triples from abstracts and construct four text representations, which we embed using four state-of-the-art transformer models: MiniLM, MPNet, SciBERT, and SPECTER. We evaluate the resulting embeddings with KMeans, GMM, and HDBSCAN for unsupervised clustering, and fine-tune classification models for arXiv subject prediction. Our results show that full abstract text yields the most coherent clusters, but that hybrid representations incorporating triples consistently improve classification performance, reaching up to 92.6% accuracy and 0.925 macro-F1. We also find that lightweight sentence encoders (MiniLM, MPNet) outperform domain-specific models (SciBERT, SPECTER) in clustering, while SciBERT excels in structured-input classification. These findings highlight the complementary benefits of combining unstructured text with structured knowledge, offering new insights into knowledge-infused representations for semantic organization of scientific documents.", "AI": {"tldr": "The paper studies how adding structured triples to text improves clustering and classification of scientific papers.", "motivation": "Scientific literature is large and complex, making it hard to organize and understand. Existing text-based methods may not fully capture the underlying relations in the content. The authors want to see whether adding structured knowledge, in the form of subject-predicate-object triples, can yield better clustering and classification of research papers than using raw text alone.", "method": "They build a modular pipeline that creates multiple representations of each paper: raw abstract text, extracted subject-predicate-object triples, and hybrid combinations of both. From a filtered arXiv corpus, they extract relational triples from abstracts and create four text formats. These are embedded using four transformer models (MiniLM, MPNet, SciBERT, SPECTER). They then (1) perform unsupervised clustering using KMeans, Gaussian Mixture Models (GMM), and HDBSCAN, and (2) fine-tune supervised classifiers to predict arXiv subject categories from embeddings. They compare performance across document representations and encoder models.", "result": "For unsupervised clustering, using full abstract text leads to more coherent clusters than using triples alone or hybrid formats. However, for supervised classification, hybrid representations that incorporate triples consistently improve performance over plain text, achieving up to 92.6% accuracy and 0.925 macro-F1. Lightweight general-purpose encoders (MiniLM, MPNet) outperform domain-specific encoders (SciBERT, SPECTER) on clustering quality, while SciBERT performs best when classifying from structured (triple-based or hybrid) inputs.", "conclusion": "Unstructured abstract text is best for forming coherent clusters, but combining it with structured subject-predicate-object triples improves supervised classification of scientific papers. Different encoder models excel in different settings: general-purpose sentence encoders work better for clustering, whereas domain-specific models like SciBERT are more effective when leveraging structured inputs for classification. Overall, integrating structured knowledge with raw text yields complementary benefits and provides a promising direction for knowledge-infused representations in organizing scientific documents."}}
{"id": "2601.08843", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08843", "abs": "https://arxiv.org/abs/2601.08843", "authors": ["Haotian Deng", "Chris Farber", "Jiyoon Lee", "David Tang"], "title": "Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness", "comment": null, "summary": "Automated short-answer grading (ASAG) remains a challenging task due to the linguistic variability of student responses and the need for nuanced, rubric-aligned partial credit. While Large Language Models (LLMs) offer a promising solution, their reliability as automated judges in rubric-based settings requires rigorous assessment. In this paper, we systematically evaluate the performance of LLM-judges for rubric-based short-answer grading. We investigate three key aspects: the alignment of LLM grading with expert judgment across varying rubric complexities, the trade-off between uncertainty and accuracy facilitated by a consensus-based deferral mechanism, and the model's robustness under random input perturbations and adversarial attacks. Using the SciEntsBank benchmark and Qwen 2.5-72B, we find that alignment is strong for binary tasks but degrades with increased rubric granularity. Our \"Trust Curve\" analysis demonstrates a clear trade-off where filtering low-confidence predictions improves accuracy on the remaining subset. Additionally, robustness experiments reveal that while the model is resilient to prompt injection, it is sensitive to synonym substitutions. Our work provides critical insights into the capabilities and limitations of rubric-conditioned LLM judges, highlighting the importance of uncertainty estimation and robustness testing for reliable deployment.", "AI": {"tldr": "The paper evaluates how well large language models can act as automated graders for rubric-based short-answer questions, focusing on alignment with human graders, uncertainty-aware filtering, and robustness to attacks and input changes.", "motivation": "Automated short-answer grading is difficult because student responses are linguistically diverse and rubrics often require fine-grained, partial-credit judgments. While LLMs appear promising as automatic judges, their reliability\u2014especially in nuanced, rubric-based contexts\u2014has not been rigorously characterized, creating risks for educational deployment.", "method": "The authors use the SciEntsBank benchmark and a large LLM (Qwen 2.5-72B) as an automated judge conditioned on scoring rubrics. They systematically evaluate three dimensions: (1) alignment between LLM and expert grades across rubrics of varying complexity and granularity; (2) an uncertainty-aware, consensus-based deferral mechanism that selectively abstains on low-confidence predictions to study the accuracy\u2013coverage trade-off, analyzed via a proposed \u201cTrust Curve\u201d; and (3) robustness via experiments involving random input perturbations, adversarial prompt injections, and synonym substitutions to probe failure modes.", "result": "They find that the LLM-judge aligns well with human experts in simple, binary-grading settings but performance deteriorates as rubrics become more fine-grained and multi-level. The Trust Curve shows that discarding low-confidence predictions increases grading accuracy on the remaining responses, confirming a clear accuracy\u2013coverage trade-off. Robustness tests indicate the model is generally robust to prompt injection attacks but is notably vulnerable to seemingly harmless synonym substitutions, which can change its grading decisions.", "conclusion": "Rubric-conditioned LLMs can be effective automated graders for simpler, binary tasks but struggle with complex, highly granular rubrics. Reliability improves when systems incorporate uncertainty estimation and allow deferral on low-confidence cases. However, sensitivity to lexical variation such as synonym substitutions exposes robustness limitations. The paper concludes that trustworthy deployment of LLM-based graders will require explicit uncertainty handling and systematic robustness evaluation and mitigation, rather than relying on raw model scores alone."}}
{"id": "2601.08844", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08844", "abs": "https://arxiv.org/abs/2601.08844", "authors": ["Anandita Garg", "Uma Gaba", "Deepan Muthirayan", "Anish Roy Chowdhury"], "title": "Emissions and Performance Trade-off Between Small and Large Language Models", "comment": "6 pages. Accepted as a full paper to the 3rd International Conference on Foundation and Large Language Models (IEEE FLLM) 2025", "summary": "The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comparable performances for a significant reduction in carbon emissions during inference. Our findings demonstrate the viability of smaller models in mitigating the environmental impact of resource-heavy LLMs, thus advancing towards sustainable, green AI.", "AI": {"tldr": "The paper compares large language models (LLMs) with fine-tuned small language models (SLMs) in terms of performance and carbon emissions, showing SLMs can often match task performance while significantly cutting inference-related emissions, supporting greener AI.", "motivation": "Large Language Models consume substantial energy and have a large carbon footprint during both training and inference. There is a need to reduce the environmental impact of AI while maintaining high task performance, prompting investigation into whether smaller, fine-tuned models can be effective substitutes for LLMs on specific, predefined tasks.", "method": "The authors conduct a comparative analysis between LLMs and fine-tuned SLMs on selected tasks spanning Natural Language Processing, Reasoning, and Programming. They fine-tune SLMs for these tasks and systematically measure model performance alongside estimated carbon emissions during inference, then analyze the trade-off between accuracy and emissions.", "result": "Across six chosen tasks, fine-tuned SLMs achieve performance comparable to LLMs in four tasks, while substantially reducing carbon emissions at inference time. This indicates that for many specific tasks, smaller models can deliver near-LLM performance at a fraction of the environmental cost.", "conclusion": "Fine-tuned SLMs are a viable and more sustainable alternative to LLMs for many predefined tasks, enabling significant reductions in carbon emissions without major performance loss. The study supports a shift toward task-specialized smaller models as part of a broader strategy for green, environmentally responsible AI development and deployment."}}
{"id": "2601.08846", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08846", "abs": "https://arxiv.org/abs/2601.08846", "authors": ["Cagatay Tekin", "Charbel Barakat", "Luis Joseph Luna Limgenco"], "title": "Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning", "comment": "6 pages, 2 figures. Code available at: github.com/cagopat/InftyThink-with-Cross-Chain-Memory", "summary": "Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.", "AI": {"tldr": "The paper extends an iterative summarization-based reasoning framework (InftyThink) by adding a semantic cache of reusable reasoning lemmas, improving long-horizon reasoning in structured domains while revealing limitations in heterogeneous ones.", "motivation": "Iterative summarization frameworks help LLMs handle long reasoning chains without blowing up context size, but they waste effort by re-deriving similar strategies for each new problem. The authors want a way for LLMs to reuse successful reasoning patterns across tasks, like a memory of lemmas, without simply expanding the context window.", "method": "They extend InftyThink with a Cross-Chain Memory that stores embeddings of intermediate reasoning steps (lemmas) that led to successful solutions. During new reasoning episodes, at each step the system retrieves the most semantically similar lemmas from this cache using embedding similarity and conditions the next-step reasoning on them. This keeps the context compact while injecting relevant prior reasoning patterns. They then evaluate on benchmarks (MATH500, AIME2024, GPQA-Diamond) and analyze the geometry of reasoning trajectories in embedding space.", "result": "On structured math and science benchmarks (MATH500, AIME2024), semantic lemma retrieval improves accuracy compared with the baseline InftyThink framework without memory. However, on GPQA-Diamond, which mixes heterogeneous domains, the cache sometimes harms performance by pulling in misleading prior patterns. Geometric analysis shows that retrieval imposes directional biases in embedding space, creating attractors that either consistently help (\"fix\" regions) or hurt (\"break\" regions) baseline performance.", "conclusion": "Similarity-based cross-chain memory can make LLM reasoning more sample-efficient and accurate in structured domains by reusing proven reasoning patterns. However, the same mechanism can degrade performance in heterogeneous settings by over-generalizing from semantically similar but contextually inappropriate lemmas. Effective self-improving LLM reasoning will require mechanisms to both exploit and selectively gate or regularize similarity-based memory to avoid harmful attractors."}}
{"id": "2601.08847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08847", "abs": "https://arxiv.org/abs/2601.08847", "authors": ["JV Roig"], "title": "Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe", "comment": "25 pages, 17 tables, 1 figure", "summary": "Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.", "AI": {"tldr": "RIKER is a benchmark and methodology that evaluates retrieval and knowledge extraction systems using synthetic documents generated from known ground truth, enabling scalable, contamination-resistant, and deterministic evaluation without human labels.", "motivation": "Existing evaluations of knowledge systems like LLMs, RAG, and knowledge graphs suffer from benchmark contamination, biased LLM judges, and the high cost of human-annotated ground truth. There is a need for an evaluation framework that is scalable, contamination-resistant, and does not rely on expensive human labeling or reference models.", "method": "The authors introduce RIKER, which inverts the typical evaluation paradigm. Instead of extracting ground truth from existing documents, they generate documents from structured, known ground truth. This allows deterministic scoring, regenerable corpora for contamination resistance, and scalable automatic evaluation. They test 33 models with over 21 billion tokens on tasks involving retrieval, cross-document aggregation, and hallucination resistance, using the synthetic-document framework.", "result": "Empirical evaluation of 33 models shows: (1) advertised context length often overstates the practically usable context; model performance degrades notably beyond 32K tokens, (2) aggregating information across multiple documents is much more difficult than extracting information from a single document, and (3) a model\u2019s ability to correctly retrieve or find existing facts is separable from its tendency to hallucinate; some models that retrieve well still fabricate non-existent facts.", "conclusion": "RIKER provides both a specific benchmark and a general, domain-agnostic methodology for evaluating knowledge systems wherever synthetic documents can be generated from structured ground truth. This paradigm inversion enables scalable, deterministic, and contamination-resistant evaluation, and reveals important distinctions in model capabilities around long-context use, cross-document reasoning, and hallucination behavior."}}
{"id": "2601.08848", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08848", "abs": "https://arxiv.org/abs/2601.08848", "authors": ["Zihe Zhang", "Can Zhang", "Yanheng Xu", "Xin Hu", "Jichao Leng"], "title": "PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment", "comment": "Accepted at EMNLP 2025 PALS Workshop (PALS: EXPLORING ACTIVE AND PASSIVE LLM PERSONALIZATION)", "summary": "This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces temperament theory from the Thomas-Chess framework and builds a temperament knowledge graph for infants and toddlers (0-3 years). Our two-stage training pipeline first uses supervised fine-tuning to teach structured chain-of-thought reasoning, and then applies a GRPO-based alignment stage to reinforce logical consistency, domain expertise, and empathetic caregiving strategies. We further design an evaluation framework comprising temperament-sensitive multiple-choice tests and human assessments. The results demonstrate that PediaMind-R1 can accurately interpret early childhood temperament profiles and proactively engage in individualized reasoning. This work highlights the value of integrating vertical-domain modeling with psychological theory. It offers a novel approach to developing user-centered LLMs that advance the practice of active personalization in sensitive caregiving contexts.", "AI": {"tldr": "PediaMind-R1 is a parenting-focused LLM for infants and toddlers that personalizes advice using developmental psychology, especially temperament theory.", "motivation": "Existing parenting and caregiving LLM systems give generic, one-size-fits-all suggestions and often ignore established psychological theory and individual child differences. The authors want an AI system that can reason about and adapt to children\u2019s unique temperaments in early development (0\u20133 years), where guidance must be both personalized and sensitive.", "method": "They design a domain-specialized LLM, PediaMind-R1, centered on early-childhood temperament. First, they encode Thomas-Chess temperament theory into a knowledge graph for 0\u20133-year-olds. Then they use a two-stage training pipeline: (1) supervised fine-tuning to instill structured chain-of-thought reasoning; (2) a GRPO-based alignment stage to optimize for logical consistency, strong domain expertise, and empathetic caregiving strategies. Finally, they build an evaluation framework with temperament-aware multiple-choice tests plus human evaluations.", "result": "Experiments show that PediaMind-R1 can interpret early-childhood temperament profiles accurately and can use those profiles to proactively generate individualized, context-aware reasoning rather than generic advice. It performs well on the temperament-sensitive tests and in human assessments.", "conclusion": "Integrating psychological temperament theory and domain-specific knowledge with LLM training and alignment yields a user-centered system that supports active personalization in sensitive caregiving contexts. This approach illustrates how vertical-domain modeling and theory-driven design can advance personalized, responsible applications of LLMs in parenting and similar domains."}}
{"id": "2601.08849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08849", "abs": "https://arxiv.org/abs/2601.08849", "authors": ["Manas Khatore", "Sumana Sridharan", "Kevork Sulahian", "Benjamin J. Smith", "Shi Feng"], "title": "Gaming the Answer Matcher: Examining the Impact of Text Manipulation on Automated Judgment", "comment": "Accepted to the AAAI 2026 Workshop on AI Governance (AIGOV)", "summary": "Automated answer matching, which leverages LLMs to evaluate free-text responses by comparing them to a reference answer, shows substantial promise as a scalable and aligned alternative to human evaluation. However, its reliability requires robustness against strategic attacks such as guesswork or verbosity that may artificially inflate scores without improving actual correctness. In this work, we systematically investigate whether such tactics deceive answer matching models by prompting examinee models to: (1) generate verbose responses, (2) provide multiple answers when unconfident, and (3) embed conflicting answers with the correct answer near the start of their response. Our results show that these manipulations do not increase scores and often reduce them. Additionally, binary scoring (which requires a matcher to answer with a definitive \"correct\" or \"incorrect\") is more robust to attacks than continuous scoring (which requires a matcher to determine partial correctness). These findings show that answer matching is generally robust to inexpensive text manipulation and is a viable alternative to traditional LLM-as-a-judge or human evaluation when reference answers are available.", "AI": {"tldr": "The paper evaluates whether automated answer matching using LLMs is robust to simple text-based gaming strategies and finds that such manipulations generally fail, supporting answer matching as a reliable alternative to human or LLM-judge evaluation when reference answers exist.", "motivation": "Manual or LLM-as-a-judge evaluation of open-ended responses is costly, slow, or misaligned. Automated answer matching against reference answers is promising but might be vulnerable to simple strategic attacks like verbosity, guessing, or mixing correct and incorrect answers, which could inflate scores without reflecting true understanding. The paper aims to test and characterize this vulnerability.", "method": "The authors prompt examinee LLMs to adopt three specific adversarial strategies when answering questions: (1) produce overly verbose responses, (2) give multiple candidate answers when uncertain, and (3) include both correct and incorrect (conflicting) answers, placing the correct one near the beginning. They then feed these responses to answer-matching LLMs configured with either binary scoring (correct/incorrect) or continuous scoring (graded partial correctness) and measure how scores change under these manipulations compared to baseline answers.", "result": "Across the tested settings, the manipulative tactics fail to boost evaluation scores and usually lower them. Responses with verbosity, multiple guesses, or embedded conflicting answers are not rewarded by the answer-matching models. Moreover, matchers operating in a binary decision mode exhibit greater robustness to these attacks than those providing continuous, partially-correct scores.", "conclusion": "LLM-based automated answer matching against reference answers is generally robust to inexpensive, text-only gaming strategies and can serve as a dependable, scalable alternative to human evaluation or more subjective LLM-as-a-judge schemes, especially when combined with binary scoring rather than continuous partial-credit scoring."}}
{"id": "2601.08851", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08851", "abs": "https://arxiv.org/abs/2601.08851", "authors": ["Alex Dantart"], "title": "M\u00e1s contexto no es mejor. Paradoja de la diluci\u00f3n vectorial en RAG corporativos", "comment": "in Spanish and English languages", "summary": "T\u00e9cnicas recientes de \"Contextualized Chunking\" inyectan res\u00famenes para mejorar el contexto en RAG, pero introducen una \"diluci\u00f3n vectorial\" que opaca el contenido local. Evaluando distintos ratios de inyecci\u00f3n, demostramos una curva en \"U invertida\": una inyecci\u00f3n moderada mejora el \"Recall\" (+18%), pero superar un umbral cr\u00edtico (CIR > 0.4) reduce la precisi\u00f3n en un 22% para consultas espec\u00edficas. Proponemos un marco te\u00f3rico para calcular el ratio \u00f3ptimo de inyecci\u00f3n. --\n  Recent \"Contextualized Chunking\" techniques inject summaries to improve RAG context but introduce \"vector dilution\" drowning out local content. Evaluating various injection ratios, we demonstrate an \"inverted U\" curve: moderate injection boosts Recall (+18%), but exceeding a critical threshold (CIR > 0.4) drops precision by 22% for specific queries. We propose a theoretical framework to calculate the optimal injection ratio.", "AI": {"tldr": "The paper studies how much summary information should be injected into chunks for RAG, finding that too much causes \u2018vector dilution\u2019 and hurts performance, and derives a way to compute an optimal injection ratio.", "motivation": "Contextualized chunking for RAG tries to add global or cross-document summaries into each chunk to reduce missed context, but practitioners have observed that this can sometimes make retrieval worse. There is little formal analysis of when and why summary injection helps vs harms, and no principled way to choose how much to inject.", "method": "The authors vary the Context Injection Ratio (CIR), i.e., the proportion of summary vs local content in chunk embeddings, and empirically measure its effect on recall and precision for RAG retrieval tasks. They analyze the resulting performance curve, identify an inverted-U relationship, and from these observations construct a theoretical model that links CIR, signal-to-noise in embeddings, and downstream retrieval metrics, yielding a formula for the optimal CIR.", "result": "Experiments show that moderate summary injection into chunks can significantly improve recall (up to +18%), but when the CIR exceeds a critical value (around 0.4), precision for specific, fine-grained queries drops sharply (about \u221222%). This behavior manifests as an inverted-U relationship between CIR and retrieval performance.", "conclusion": "There exists an optimal, task-dependent level of summary injection for contextualized chunking in RAG: some injection is beneficial, but too much causes vector dilution that hides local details and harms precision. The paper\u2019s theoretical framework provides a principled way to estimate this optimal injection ratio, guiding practitioners in configuring contextualized chunking instead of tuning it purely by trial and error."}}
{"id": "2601.08852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08852", "abs": "https://arxiv.org/abs/2601.08852", "authors": ["Nidhi Pandya"], "title": "NewsScope: Schema-Grounded Cross-Domain News Claim Extraction with Open Models", "comment": "5 pages, 3 tables. Code, model, and benchmark publicly released", "summary": "Automated news verification requires structured claim extraction, but existing approaches either lack schema compliance or generalize poorly across domains. This paper presents NewsScope, a cross-domain dataset, benchmark, and fine-tuned model for schema-grounded news claim extraction. The dataset contains 455 articles across politics, health, science/environment, and business, consisting of 395 in-domain articles and 60 out-of-source articles for generalization testing. LLaMA 3.1 8B was fine-tuned using LoRA on 315 training examples and evaluated on held-out in-domain (80 articles) and out-of-source (60 articles) test sets. Human evaluation on 400 claims shows NewsScope achieves 89.4% human-evaluated accuracy compared to GPT-4o-mini's 93.7% (p=0.07). NewsScope outperforms GPT-4o-mini on political claims (94.3% vs. 87.8%). A numeric grounding filter further improves accuracy to 91.6%, narrowing the gap to 2.1 percentage points. Inter-annotator agreement studies (160 claims) confirm labeling reliability (94.6% positive agreement on SUPPORTED judgments). The open-weight model enables offline deployment at approximately $15 on-demand compute (or $0 on free tiers). Code and benchmark are publicly released.", "AI": {"tldr": "They introduce NewsScope, a dataset, benchmark, and fine-tuned model for schema-grounded claim extraction in news, aiming for cross-domain generalization and offline deployability.", "motivation": "Existing automated news verification systems need structured claim extraction, but current methods either don\u2019t reliably follow a predefined schema or fail to generalize across different news domains. There is also a lack of open, cross-domain benchmarks and practical, open-weight models for this task.", "method": "They build NewsScope, a dataset of 455 news articles from multiple domains (politics, health, science/environment, business), including out-of-source articles to test generalization. They fine-tune LLaMA 3.1 8B with LoRA on 315 training articles for schema-grounded claim extraction, and evaluate on in-domain and out-of-source test sets. They run human evaluations on extracted claims, compare against GPT-4o-mini, apply a numeric grounding filter to improve precision, and measure inter-annotator agreement to validate label reliability. They also analyze costs and support offline deployment with open weights.", "result": "NewsScope achieves 89.4% human-evaluated accuracy on claim extraction, close to GPT-4o-mini\u2019s 93.7% (difference not statistically significant at p=0.07). It outperforms GPT-4o-mini on political claims (94.3% vs. 87.8%). Applying a numeric grounding filter raises accuracy to 91.6%, narrowing the gap to GPT-4o-mini to 2.1 percentage points. Inter-annotator agreement is high (94.6% positive agreement on SUPPORTED labels), supporting annotation quality. The open-weight model is cheap to deploy offline (~$15 on-demand compute, or free on some tiers), and code/benchmark are released.", "conclusion": "NewsScope provides a strong, open, and cost-effective solution for schema-grounded news claim extraction across multiple domains, with performance approaching that of a proprietary GPT-4-class model and even surpassing it in certain domains (politics). The dataset, benchmark, and model together enable more robust, generalizable, and practically deployable components for automated news verification."}}
{"id": "2601.08892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08892", "abs": "https://arxiv.org/abs/2601.08892", "authors": ["Eric Rudolph", "Natalie Engert", "Jens Albrecht"], "title": "Evaluating Role-Consistency in LLMs for Counselor Training", "comment": null, "summary": "The rise of online counseling services has highlighted the need for effective training methods for future counselors. This paper extends research on VirCo, a Virtual Client for Online Counseling, designed to complement traditional role-playing methods in academic training by simulating realistic client interactions. Building on previous work, we introduce a new dataset incorporating adversarial attacks to test the ability of large language models (LLMs) to maintain their assigned roles (role-consistency). The study focuses on evaluating the role consistency and coherence of the Vicuna model's responses, comparing these findings with earlier research. Additionally, we assess and compare various open-source LLMs for their performance in sustaining role consistency during virtual client interactions. Our contributions include creating an adversarial dataset, evaluating conversation coherence and persona consistency, and providing a comparative analysis of different LLMs.", "AI": {"tldr": "The paper evaluates how well different open-source large language models can stay in character as a virtual counseling client, especially under adversarial prompts, by introducing a new adversarial dataset and analyzing role consistency and conversation coherence.", "motivation": "Online counseling is growing, which increases the need for realistic training tools for future counselors. Existing role-play and earlier versions of the VirCo virtual client show promise, but there is a gap in systematically testing whether LLM-based virtual clients can reliably maintain their assigned persona, particularly when faced with adversarial attempts to break character. Ensuring stable, coherent client behavior is essential for trustworthy training scenarios.", "method": "The authors extend the VirCo framework by constructing a new adversarial dataset of dialogues that attempt to push or trick the virtual client into breaking its assigned role. They then run conversations using the Vicuna model and several other open-source LLMs under this setup. They evaluate model responses for role-consistency (staying in the virtual client persona) and conversational coherence, and compare these results against earlier VirCo studies.", "result": "The study yields quantitative and/or qualitative measurements of how consistently Vicuna and other open-source LLMs maintain the virtual client role across adversarial dialogues. It identifies differences among models in their robustness to adversarial attacks, as well as variations in coherence and persona stability relative to prior work.", "conclusion": "The paper concludes that adversarial evaluation is necessary to reliably assess LLM-based virtual clients for counselor training. The new adversarial dataset and evaluation of multiple open-source models provide evidence that some models are more role-consistent and coherent than others, but that maintaining persona under attack remains a challenge. The work establishes a benchmark and methodology for improving and comparing virtual counseling clients built on LLMs."}}
{"id": "2601.08950", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08950", "abs": "https://arxiv.org/abs/2601.08950", "authors": ["Mayank Sharma", "Roy Pea", "Hari Subramonyam"], "title": "ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue", "comment": null, "summary": "In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.", "AI": {"tldr": "They introduce ConvoLearn, a dataset and fine-tuning approach that makes LLM tutors act more like constructivist, dialogic teachers instead of just giving answers.", "motivation": "LLMs used as tutors often short-circuit learning by directly giving solutions and ignoring key pedagogical principles like engagement, assessment, and metacognition. The authors want a principled way to shape and evaluate LLMs so they support deeper, knowledge-building learning in real classrooms.", "method": "They design a knowledge-building framework with six pedagogical dimensions (cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, power dynamics). They then generate a semi-synthetic dataset of 1,250 20-turn middle-school Earth Science tutor\u2013student dialogues via controlled interactions between human teachers and a simulated student. Using QLoRA, they fine-tune Mistral 7B on this dataset, and they evaluate the pedagogical quality of the resulting model via human ratings from 31 teachers, comparing against the base Mistral 7B and Claude Sonnet 4.5.", "result": "Fine-tuning on ConvoLearn significantly changes LLM behavior towards more knowledge-building tutoring strategies. In teacher evaluations, the fine-tuned Mistral 7B achieves a substantially higher mean pedagogical quality rating (M = 4.10, SD = 1.03) than base Mistral 7B (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29).", "conclusion": "A targeted, theoretically grounded tutoring dataset like ConvoLearn can successfully steer LLMs toward constructivist, dialogic teaching behaviors. The framework and dataset provide a path for future development and evaluation of AI tutors that better align with contemporary learning sciences theory."}}
{"id": "2601.08955", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08955", "abs": "https://arxiv.org/abs/2601.08955", "authors": ["Youwei Liu", "Jian Wang", "Hanlin Wang", "Beichen Guo", "Wenjie Li"], "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models", "comment": null, "summary": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.", "AI": {"tldr": "The paper introduces Imagine-then-Plan (ITP), a framework that uses learned world models to generate multi-step imagined trajectories with an adaptive lookahead horizon, integrating them with current observations to improve policy learning and task planning.", "motivation": "Existing world-model-based agents mostly use single-step or fixed-horizon rollouts, which under-utilizes their potential for complex, long-horizon task planning. There is a need for a unified way to let agents flexibly reason multiple steps ahead using imagination while adapting the planning horizon across tasks and learning stages.", "method": "The authors propose Imagine-then-Plan (ITP), where a policy interacts with a learned world model to generate multi-step imagined trajectories. They introduce an adaptive lookahead mechanism that adjusts imagination horizon by balancing ultimate task goals and current task progress. The information from imagined trajectories (e.g., predicted progress and conflicts) is fused with real observations, forming a partially observable and \u2018imaginable\u2019 MDP for policy learning. ITP is instantiated in both training-free and reinforcement-learning-based variants.", "result": "Across multiple standard agent benchmarks, ITP outperforms strong baseline methods. The experiments show that adaptive multi-step imagination leads to better planning and control performance compared with fixed-horizon or non-imagination baselines.", "conclusion": "Adaptive, multi-step imagination over a learned world model, as formalized in the ITP framework, significantly improves agents\u2019 reasoning and planning capabilities. This approach offers a general way to leverage world models for more complex and long-horizon tasks and suggests that dynamically tuning lookahead depth is crucial for effective decision-making."}}
{"id": "2601.08988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08988", "abs": "https://arxiv.org/abs/2601.08988", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji"], "title": "ART: Action-based Reasoning Task Benchmarking for Medical AI Agents", "comment": null, "summary": "Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings", "AI": {"tldr": "Introduces ART, a benchmark to test how well medical AI agents perform complex, action-based reasoning over EHRs, revealing major weaknesses in aggregation and threshold reasoning despite good information retrieval.", "motivation": "Current large language models used in healthcare are promising but are not well evaluated on realistic, action-based clinical decision tasks. Existing benchmarks mostly test static question answering and fail to capture key reasoning skills clinicians need from AI systems, such as applying thresholds, aggregating temporal data, and following conditional logic. There is a need for a benchmark grounded in real EHRs that surfaces these specific reasoning failures to guide the development of safer, more reliable clinical agents.", "method": "The authors build ART (Action-based Reasoning clinical Task) by mining real-world structured EHR data and passing it through a four-stage pipeline: (1) scenario identification to find clinically relevant situations; (2) task generation that constructs concrete action-based questions requiring threshold checks, temporal aggregation, and conditional logic; (3) a quality audit to validate clinical correctness and realism; and (4) evaluation, where state-of-the-art LLMs (GPT-4o-mini and Claude 3.5 Sonnet) are tested on 600 tasks. They analyze model errors into three categories: retrieval failures, aggregation errors, and conditional logic misjudgments, and refine prompts to separate retrieval from higher-level reasoning.", "result": "On the ART benchmark, both GPT-4o-mini and Claude 3.5 Sonnet achieve nearly perfect performance on retrieving relevant information from the structured EHR after prompt refinement, showing that data access is not the primary bottleneck. However, the models perform substantially worse on tasks that require aggregation and threshold reasoning, with error rates ranging from roughly 28\u201364% for aggregation and 32\u201338% for threshold-based decisions. These results highlight that complex, action-oriented reasoning over EHR data remains a major challenge for current LLMs.", "conclusion": "ART successfully exposes specific failure modes of current medical AI agents in action-based reasoning over EHRs, particularly in aggregation and threshold reasoning, despite strong retrieval capabilities. This benchmark represents a step toward developing and evaluating more reliable, clinically useful AI agents that can support clinicians in high-demand settings by reducing cognitive load and administrative burden. By focusing on realistic, structured EHR tasks and known reasoning weaknesses, ART can guide future model and system design aimed at safer clinical decision support."}}
{"id": "2601.09001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09001", "abs": "https://arxiv.org/abs/2601.09001", "authors": ["Pedro Memoli Buffa", "Luciano Del Corro"], "title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM", "comment": null, "summary": "Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all \"10 choose k\" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.", "AI": {"tldr": "They propose a simple, inference-time way to monitor where LLMs fail under domain shift by using output-entropy patterns to estimate accuracy for specific data slices, and to guide data collection for improvement.", "motivation": "In real deployments, LLMs face changing traffic and domain distributions, making it hard to know on which slices (domains, benchmarks, or task types) the model is currently underperforming. Traditional evaluation requires labeled data in each new domain, which is expensive and lags behind real shifts. Practitioners also need principled ways to decide where to collect more data to improve the model. The paper aims to find a cheap, scalable, label-efficient signal available at inference time that can both monitor performance across domains and highlight the worst-performing regions for targeted data acquisition.", "method": "For every generated response, they inspect the model\u2019s final-layer next-token probability distribution (obtained via top-k logprobs) and compute an output-entropy profile over the sequence. From this profile, they derive eleven summary statistics (capturing uncertainty patterns). These statistics feed a small classifier trained to predict whether that specific response is correct. For any domain or benchmark slice, they average the predicted correctness probabilities across its examples, obtaining an estimate of slice-level accuracy. They test this on ten STEM reasoning datasets using exhaustive combinations of training/testing slices (all k-subset combinations for k in {1,2,3,4}) and evaluate across nine LLMs spanning six model families and sizes from 3B to 20B parameters.", "result": "Across the STEM reasoning benchmarks, the averaged instance-level correctness predictions derived from entropy profiles correlate well with true held-out slice accuracies, often preserving the correct ranking between domains. Several models exhibit near-monotonic ordering: slices predicted to be easier indeed have higher actual accuracy. This shows that output-entropy profiles learned from top-k next-token probabilities are informative enough to estimate domain-level performance reliably under domain shift, without labeled data in the target domain.", "conclusion": "The paper concludes that output-entropy profiles provide a practical, inference-time signal for scalable monitoring of LLM performance under domain shift and for identifying high-value targets for data acquisition. Because the approach only requires top-k logprobs and a lightweight classifier, it is easy to deploy and generalizes across multiple LLM families and sizes. This makes it a useful building block for continuous evaluation pipelines that must track and improve LLM behavior as real-world usage evolves."}}
{"id": "2601.09032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09032", "abs": "https://arxiv.org/abs/2601.09032", "authors": ["Logan Ritchie", "Sushant Mehta", "Nick Heiner", "Mason Yu", "Edwin Chen"], "title": "The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments", "comment": null, "summary": "The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \\emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.", "AI": {"tldr": "Empirical study of LLM-based agents on 150 realistic workplace tasks in an e-commerce RL environment, revealing a hierarchy of agentic capabilities and significant remaining gaps to human-level performance.", "motivation": "As LLM-based agents move from simple single-turn tasks to complex, multi-step, real-world workflows, we lack clear evaluation frameworks and understanding of what capabilities are needed for reliable deployment in realistic workplace settings. The authors aim to systematically evaluate frontier AI models on complex tasks and identify where and why they fail, to guide future agent development and environment design.", "method": "The authors use a realistic e-commerce reinforcement learning (RL) environment from Surge, containing 150 workplace tasks. They empirically evaluate multiple frontier LLM-based agents on these tasks, analyze performance and failure patterns, and inductively derive a hierarchy of necessary agentic capabilities: tool use, planning and goal formation, adaptability, groundedness, and common-sense reasoning. They also propose a task-centric methodology for designing RL environments that leverages diverse tasks and domain expert input.", "result": "Even the best models fail about 40% of the workplace tasks. Failure modes cluster along the proposed capability hierarchy: weaker models frequently fail at basic tool use and planning, while stronger models mainly fail on tasks requiring contextual inference and reasoning beyond explicit instructions (adaptability, groundedness, common sense). This yields an empirically grounded taxonomy of agentic capabilities and their associated bottlenecks.", "conclusion": "Current frontier LLM agents can execute coherent multi-step behavior but remain far from human-level performance in realistic workplace scenarios. A clear hierarchy of agentic capabilities emerges, and many failures are systematic rather than random. Improving higher-level capabilities such as adaptability, groundedness, and common-sense reasoning, as well as designing diverse, expert-informed RL environments, will be critical for closing the gap to reliable real-world deployment."}}
{"id": "2601.09012", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09012", "abs": "https://arxiv.org/abs/2601.09012", "authors": ["Mara Finkelstein", "Isaac Caswell", "Tobias Domhan", "Jan-Thorsten Peter", "Juraj Juraska", "Parker Riley", "Daniel Deutsch", "Cole Dilanni", "Colin Cherry", "Eleftheria Briakou", "Elizabeth Nielsen", "Jiaming Luo", "Kat Black", "Ryan Mullins", "Sweta Agrawal", "Wenda Xu", "Erin Kats", "Stephane Jaskiewicz", "Markus Freitag", "David Vilar"], "title": "TranslateGemma Technical Report", "comment": null, "summary": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.", "AI": {"tldr": "TranslateGemma introduces open-source machine translation models built on Gemma 3, improved via two-stage fine-tuning and reinforcement learning, achieving strong multilingual and multimodal translation performance, often matching larger baselines with smaller models.", "motivation": "To enhance Gemma 3\u2019s inherent multilingual and multimodal abilities specifically for machine translation, and to provide the community with open, efficient MT models that perform competitively across many language pairs and modalities.", "method": "The authors apply a two-stage fine-tuning process to Gemma 3: (1) supervised fine-tuning on a large, diverse mix of synthetic parallel data (generated by state-of-the-art MT systems) and high-quality human-translated parallel data; (2) reinforcement learning with an ensemble of reward models (including MetricX-QE and AutoMQM) that directly optimize for translation quality. They then evaluate with both human and automatic metrics across many language pairs and on a multimodal image-translation benchmark (Vistra).", "result": "TranslateGemma models achieve consistent and substantial improvements over baseline Gemma 3 models across all model sizes on WMT24++ (55 language pairs) and human evaluation on the WMT25 test set (10 language pairs). Smaller TranslateGemma variants often reach or surpass the performance of larger baseline models. The models also retain and improve multimodal translation abilities, as shown by stronger performance on the Vistra benchmark.", "conclusion": "Two-stage fine-tuning with high-quality parallel data plus RL with specialized translation-quality reward models yields significantly better MT performance from Gemma 3 while preserving multimodal strengths. The released TranslateGemma models offer the community efficient, open, and high-performing tools for multilingual and multimodal machine translation."}}
{"id": "2601.09072", "categories": ["cs.AI", "cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09072", "abs": "https://arxiv.org/abs/2601.09072", "authors": ["Jean Feng", "Avni Kothari", "Patrick Vossler", "Andrew Bishara", "Lucas Zier", "Newton Addo", "Aaron Kornblith", "Yan Shuo Tan", "Chandan Singh"], "title": "Human-AI Co-design for Clinical Prediction Models", "comment": null, "summary": "Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.", "AI": {"tldr": "Introduces HACHI, a human-in-the-loop AI framework that accelerates and improves development of interpretable clinical prediction models using concepts extracted from clinical notes.", "motivation": "Traditional development of clinical prediction models requires intensive, iterative collaboration between clinicians, data scientists, and informaticists. This is slow, expensive, and limits the number of models that reach practice. The difficulty is magnified when incorporating unstructured clinical notes with huge concept spaces, making it hard to systematically explore which textual concepts matter for prediction while maintaining interpretability and safety.", "method": "Proposes HACHI, an iterative human-in-the-loop framework where an AI agent automatically explores and evaluates candidate concepts from clinical notes and structured data, while clinicians and domain experts repeatedly review and provide feedback. Concepts are represented as simple binary (yes/no) questions used as features in linear models. The workflow alternates between automated exploration/learning and expert-guided refinement: experts adjust which concepts to explore, their granularity, model objectives, and address issues like data leakage or bias. The framework is applied to two prediction tasks: acute kidney injury and traumatic brain injury.", "result": "On two real-world tasks, HACHI-based models outperform existing approaches, discover new clinically meaningful concepts absent from widely used CPMs, and show improved generalizability across different institutions and time periods. The experiments also demonstrate that expert feedback significantly shapes the concept space, objective functions, and identification of data problems, leading to better and safer models.", "conclusion": "A structured human-in-the-loop framework with AI agents, like HACHI, can make development of fully interpretable clinical prediction models more efficient and effective, particularly when leveraging unstructured clinical notes. By encoding concepts as simple binary questions within linear models, and by explicitly integrating clinical expertise into iterative refinement, HACHI improves performance, interpretability, and robustness while clarifying the indispensable role of the clinical AI team in guiding and validating model development."}}
{"id": "2601.09017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09017", "abs": "https://arxiv.org/abs/2601.09017", "authors": ["Haryo Akbarianto Wibowo", "Alaa Elsetohy", "Qinrong Cui", "Alham Fikri Aji"], "title": "Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has necessitated more robust evaluation methods that go beyond static benchmarks, which are increasingly prone to data saturation and leakage. In this paper, we propose a dynamic benchmarking framework for evaluating multilingual and multicultural capabilities through the social deduction game Spyfall. In our setup, models must engage in strategic dialogue to either identify a secret agent or avoid detection, utilizing culturally relevant locations or local foods. Our results show that our game-based rankings align closely with the Chatbot Arena. However, we find a significant performance gap in non-English contexts: models are generally less proficient when handling locally specific entities and often struggle with rule-following or strategic integrity in non-English languages. We demonstrate that this game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks. The game history can be accessed here https://huggingface.co/datasets/haryoaw/cultural-spyfall.", "AI": {"tldr": "Proposes a dynamic, game-based benchmark (Spyfall) to evaluate LLMs\u2019 multilingual and multicultural abilities, revealing strong alignment with existing rankings but large performance drops and integrity issues in non-English settings.", "motivation": "Static NLP benchmarks are becoming less reliable due to data saturation and leakage, and they under-represent multilingual and multicultural abilities. There is a need for an evaluation framework that is dynamic, difficult to game, culturally nuanced, and better at revealing real-world weaknesses of LLMs beyond English and beyond narrow, static tasks.", "method": "Use the social deduction game Spyfall as a dynamic benchmark. LLMs play roles that require them to conduct strategic, conversational reasoning: either identify a hidden spy or avoid detection. The game is instantiated with culturally grounded content (e.g., local places and foods across languages). Models are evaluated on their gameplay quality, rule adherence, strategic reasoning, and success rates across multiple languages, and the resulting performance rankings are compared against Chatbot Arena rankings.", "result": "The proposed Spyfall-based benchmark produces model rankings that are broadly consistent with Chatbot Arena, validating its overall discriminative power. However, it exposes a substantial drop in performance when models operate in non-English languages, especially when dealing with locally specific entities. Models are more likely to break rules, fail to maintain strategic integrity, or misunderstand culturally grounded content outside English.", "conclusion": "A game-based, dynamic Spyfall framework provides a scalable, leakage-resistant, and culturally sensitive alternative or complement to traditional NLP benchmarks. It uncovers hidden weaknesses of LLMs in multilingual and multicultural settings, highlighting the need for improved non-English reasoning, cultural grounding, and rule-following. The released game history dataset enables further research and extension of this evaluation paradigm."}}
{"id": "2601.09097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09097", "abs": "https://arxiv.org/abs/2601.09097", "authors": ["Derrick Goh Xin Deik", "Quanyu Long", "Zhengyuan Liu", "Nancy F. Chen", "Wenya Wang"], "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning", "comment": "8 pages of main text, 2 pages of references and and limitations, 37 pages of appendices", "summary": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.", "AI": {"tldr": "The paper proposes SCOPE, a framework that separates natural-language reasoning from generic code execution to handle multi-constraint planning more reliably, efficiently, and flexibly than prior LLM-based methods.", "motivation": "Multi-constraint planning problems require handling many interacting and potentially conflicting constraints. Existing LLM solutions either rely on long chains of natural language reasoning, which become inconsistent, error-prone, and expensive as constraints grow, or they tightly couple LLMs to ad hoc code or fixed solvers, which limits generalization and reuse across different planning tasks. The authors aim to build a scalable, generalizable method that preserves LLM flexibility but adds deterministic, reusable execution for complex planning.", "method": "They introduce SCOPE (Scalable COde Planning Engine), which explicitly separates query-specific reasoning from generic code execution. The LLM is used to design or select general solver functions that encode reusable planning logic, while problem instances are handled through lightweight parameter changes rather than rewriting logic each time. The resulting solver functions are designed to be consistent, deterministic, and applicable across multiple queries. This architecture reduces reliance on long natural-language chains and avoids generating bespoke code for every new problem.", "result": "On benchmarks such as TravelPlanner, SCOPE achieves state-of-the-art results. With GPT-4o, it attains 93.1% success on TravelPlanner, representing a 61.6% absolute improvement over the best chain-of-thought baseline, while also reducing inference cost by 1.4x and latency by about 4.67x. These results indicate both better plan quality and better efficiency compared to prior LLM-based methods.", "conclusion": "Disentangling reasoning from execution in multi-constraint planning allows LLM systems to be both more accurate and more efficient. SCOPE\u2019s reusable solver functions enable consistent, deterministic planning behavior that generalizes across queries with minimal parameter adjustments. The framework overcomes key limitations of pure natural language reasoning and rigid code/solver integrations, establishing a new state of the art in LLM-based multi-constraint planning."}}
{"id": "2601.09028", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09028", "abs": "https://arxiv.org/abs/2601.09028", "authors": ["Fengran Mo", "Zhan Su", "Yuchen Hui", "Jinghan Zhang", "Jia Ao Sun", "Zheyuan Liu", "Chao Zhang", "Tetsuya Sakai", "Jian-Yun Nie"], "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG", "comment": "Accepted by ACM WWW 2026", "summary": "The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.", "AI": {"tldr": "The paper proposes OpenDecoder, a method to improve robustness of retrieval-augmented generation (RAG) by explicitly using quality indicators of retrieved documents (relevance, ranking, QPP scores) during generation.", "motivation": "RAG systems depend on retrieved documents being relevant and useful, but in practice retrieval often returns partially relevant or noisy documents. Standard LLM-based RAG assumes relevance and does not explicitly model varying quality of retrieved information, which can degrade answer quality and robustness. The authors want a mechanism that makes generation aware of and robust to noisy or variably relevant context.", "method": "They introduce OpenDecoder, which augments the generation process with explicit evaluation signals about retrieved documents. For each retrieved item, they compute three types of scores: (1) relevance score, (2) ranking score, and (3) a query performance prediction (QPP) score. These scores are fed into the decoder as quality indicator features so that the LLM can weight or utilize retrieved content according to its predicted usefulness, leading to more robust RAG behavior under noisy context.", "result": "On five benchmark datasets, OpenDecoder outperforms various baseline RAG methods in both effectiveness (answer quality) and robustness when the retrieved context contains noise or varying levels of relevance. The experiments show that incorporating explicit evaluation signals yields more reliable performance across different retrieval qualities.", "conclusion": "Explicitly encoding retrieval quality indicators into the generation process makes RAG systems more robust to noisy or partially relevant context. OpenDecoder provides a flexible paradigm that can be combined with LLM post-training and can incorporate various external quality indicators, suggesting a general framework for making LLM-based RAG more reliable in real-world settings where retrieval is imperfect."}}
{"id": "2601.09100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09100", "abs": "https://arxiv.org/abs/2601.09100", "authors": ["Lixiang Zhang", "Chenggong Zhao", "Qing Gao", "Xiaoke Zhao", "Gengyi Bai", "Jinhu Lv"], "title": "DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model", "comment": "14 pages, 6 figures", "summary": "Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.", "AI": {"tldr": "Proposes DScheLLM, a dual-system LLM-based dynamic scheduling method that handles different scales of disturbances in job shop scheduling via fast and slow reasoning modes, both trained on OR-solver-generated data.", "motivation": "Dynamic job shop scheduling faces frequent disruptions (processing time changes, machine breakdowns, new jobs), and conventional event-specific analytical models lack adaptability and generalization to unseen disturbances. There is a need for a more flexible, data-driven, and general framework that can quickly adapt scheduling decisions in dynamic environments, leveraging recent advances in large language models.", "method": "Design a dual-system (fast\u2013slow) LLM-based architecture called DScheLLM for dynamic scheduling. Build a unified LLM framework that processes diverse dynamic events. Generate training datasets for both fast and slow reasoning modes from exact schedules produced by an operations research solver. Fine-tune Huawei OpenPangu Embedded-7B with LoRA under hybrid fast and slow reasoning paradigms. The fast mode outputs schedules quickly; the slow mode generates structured, solver-compatible decision inputs.", "result": "On standard job shop scheduling benchmarks, the fast-thinking mode efficiently produces high-quality schedules, while the slow-thinking mode reliably generates well-formatted inputs that are compatible with OR solvers. Overall performance indicates that DScheLLM handles dynamic events effectively and competitively.", "conclusion": "LLMs, when fine-tuned within a dual-system reasoning framework and trained on OR-solver solutions, can effectively tackle dynamic job shop scheduling. DScheLLM demonstrates that LLMs are promising tools for intelligent, adaptive scheduling optimization under dynamic disturbances and may be among the earliest such applications in this domain."}}
{"id": "2601.09036", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09036", "abs": "https://arxiv.org/abs/2601.09036", "authors": ["Sreya Vangara", "Jagjit Nanda", "Yan-Kai Tzeng", "Eric Darve"], "title": "SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery Science", "comment": "11 pages, 8 figures, appendix included", "summary": "Scientific reasoning increasingly requires linking structured experimental data with the unstructured literature that explains it, yet most large language model (LLM) assistants cannot reason jointly across these modalities. We introduce SpectraQuery, a hybrid natural-language query framework that integrates a relational Raman spectroscopy database with a vector-indexed scientific literature corpus using a Structured and Unstructured Query Language (SUQL)-inspired design. By combining semantic parsing with retrieval-augmented generation, SpectraQuery translates open-ended questions into coordinated SQL and literature retrieval operations, producing cited answers that unify numerical evidence with mechanistic explanation. Across SQL correctness, answer groundedness, retrieval effectiveness, and expert evaluation, SpectraQuery demonstrates strong performance: approximately 80 percent of generated SQL queries are fully correct, synthesized answers reach 93-97 percent groundedness with 10-15 retrieved passages, and battery scientists rate responses highly across accuracy, relevance, grounding, and clarity (4.1-4.6/5). These results show that hybrid retrieval architectures can meaningfully support scientific workflows by bridging data and discourse for high-volume experimental datasets.", "AI": {"tldr": "The paper presents SpectraQuery, a system that answers natural-language scientific questions by jointly querying structured Raman spectroscopy data and unstructured scientific literature.", "motivation": "Scientists need tools that can reason over both structured experimental datasets and the unstructured literature that provides mechanistic explanations, but existing LLM-based assistants usually handle these modalities separately and cannot effectively combine them for rigorous, cited scientific reasoning.", "method": "The authors design SpectraQuery, a hybrid query framework inspired by Structured and Unstructured Query Language (SUQL). It uses semantic parsing to translate user questions into coordinated SQL queries over a Raman spectroscopy database and retrieval operations over a vector-indexed literature corpus, then applies retrieval-augmented generation to synthesize cited answers that integrate numerical data with textual explanations. They evaluate SQL correctness, answer groundedness, retrieval quality, and expert judgments from battery scientists.", "result": "SpectraQuery correctly generates SQL queries about 80% of the time, produces synthesized answers with 93\u201397% grounding when using 10\u201315 retrieved passages, and receives high ratings from domain experts (4.1\u20134.6 out of 5) on accuracy, relevance, grounding, and clarity.", "conclusion": "Hybrid architectures that tightly couple structured database querying with literature retrieval and LLM-based synthesis can effectively support scientific workflows, enabling systems like SpectraQuery to bridge experimental data and scientific discourse for large-scale experimental datasets."}}
{"id": "2601.09105", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09105", "abs": "https://arxiv.org/abs/2601.09105", "authors": ["Wenbin Li", "Jingling Wu", "Xiaoyong Lin. Jing Chen", "Cong Chen"], "title": "AviationLMM: A Large Multimodal Foundation Model for Civil Aviation", "comment": "Accepted by 2025 7th International Conference on Interdisciplinary Computer Science and Engineering (ICICSE 2025) conference, Chongqing, China; 9 pages,1 figure,5 tables", "summary": "Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.", "AI": {"tldr": "The paper proposes AviationLMM, a large multimodal foundation model concept for civil aviation that can integrate heterogeneous data (voice, radar, telemetry, video, text) to support understanding, reasoning, and decision-making, and outlines its architecture and key research challenges.", "motivation": "Current AI in civil aviation is narrow and siloed, operating on single tasks or modalities and unable to effectively integrate diverse data sources like voice communications, radar, sensor data, and text reports. This fragmentation limits situational awareness, adaptability, and real-time decision support critical for safety, efficiency, and passenger experience. There is a need for a unified, foundation-model-style approach that can handle multimodal aviation data holistically.", "method": "The paper conceptually designs AviationLMM, a large multimodal model architecture that can ingest multiple aviation-related modalities (air-ground voice, surveillance/radar, on-board telemetry, video, and structured text). It performs cross-modal alignment and fusion to create a shared representation space and then generates flexible outputs such as situation summaries, risk alerts, predictive diagnostics, and multimodal incident reconstructions. The work is primarily architectural and visionary, identifying necessary components and workflows rather than presenting a fully implemented system.", "result": "Instead of empirical experimental results, the paper offers a systematic gap analysis between current aviation AI and operational requirements, a proposed architecture for AviationLMM, and a structured research agenda. It identifies critical technical challenges and research directions around data acquisition and curation, multimodal alignment and fusion, pretraining strategies, advanced reasoning, trustworthiness, privacy and security, robustness to missing modalities, and synthetic scenario generation for training and evaluation.", "conclusion": "The authors conclude that a dedicated large multimodal foundation model for civil aviation is both necessary and promising for achieving integrated, trustworthy, and privacy-preserving aviation AI. By outlining the AviationLMM vision, architecture, and open challenges, they aim to guide and coordinate future research efforts, ultimately advancing safety, efficiency, and customer satisfaction in the civil aviation ecosystem."}}
{"id": "2601.09041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09041", "abs": "https://arxiv.org/abs/2601.09041", "authors": ["Samhita Bollepally", "Aurora Sloman-Moll", "Takashi Yamauchi"], "title": "Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity", "comment": "17 pages, 5 figures", "summary": "Large language models generate judgments that resemble those of humans. Yet the extent to which these models align with human judgments in interpreting figurative and socially grounded language remains uncertain. To investigate this, human participants and four instruction-tuned LLMs of different sizes (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated 240 dialogue-based sentences representing six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. Each of the 240 sentences was paired with 40 interpretive questions, and both humans and LLMs rated these sentences on a 10-point Likert scale. Results indicated that humans and LLMs aligned at the surface level with humans, but diverged significantly at the representational level, especially in interpreting figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximates human representational patterns, while all models struggle with context-dependent and socio-pragmatic expressions like sarcasm, slang, and idiomacy.", "AI": {"tldr": "The paper compares how humans and several instruction-tuned large language models interpret figurative and socially grounded language in dialogues, showing surface-level similarity but deeper representational differences, especially for idioms, slang, and sarcasm.", "motivation": "While LLMs often appear human-like in their judgments, it is unclear whether they truly align with human interpretation of nuanced, socially grounded, and figurative language. Understanding this alignment is crucial for deploying LLMs in real-world conversational settings where such language is pervasive and misinterpretation can have downstream consequences.", "method": "The authors collected 240 dialogue-based sentences covering six linguistic traits: conventionality, sarcasm, funniness, emotionality, idiomacy, and slang. Each sentence was associated with 40 interpretive questions. Human participants and four instruction-tuned LLMs (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated the sentences on a 10-point Likert scale for these traits. The researchers then compared alignment between human and model ratings at both surface (overall scores) and representational (pattern and structure of judgments) levels.", "result": "LLMs showed apparent surface-level similarity to human ratings across the dataset but diverged notably from humans at the representational level, particularly for figurative and socio-pragmatic language such as idioms and Gen Z slang. Among the evaluated models, GPT-4\u2019s rating patterns were closest to human patterns, whereas all models exhibited substantial difficulty with context-dependent and socially nuanced expressions like sarcasm, slang, and idiomacy.", "conclusion": "Current instruction-tuned LLMs can approximate human-like judgments in aggregate but do not fully replicate the underlying representational structure of human interpretation, especially for figurative and socially grounded language. Even advanced models like GPT-4 have limitations in capturing the nuanced, context-sensitive aspects of sarcasm, slang, and idioms, indicating that genuinely human-like pragmatic competence remains an open challenge for LLM development."}}
{"id": "2601.09113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09113", "abs": "https://arxiv.org/abs/2601.09113", "authors": ["Zixia Jia", "Jiaqi Li", "Yipeng Kang", "Yuxuan Wang", "Tong Wu", "Quansen Wang", "Xiaobo Wang", "Shuyi Zhang", "Junzhe Shen", "Qing Li", "Siyuan Qi", "Yitao Liang", "Di He", "Zilong Zheng", "Song-Chun Zhu"], "title": "The AI Hippocampus: How Far are We From Human Memory?", "comment": null, "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.", "AI": {"tldr": "Survey paper proposing a taxonomy of memory in Large Language Models (LLMs) and Multi-Modal LLMs, categorizing memory into implicit, explicit, and agentic paradigms, and discussing architectures, benchmarks, and open challenges.", "motivation": "As LLMs and multi-modal LLMs evolve from static predictors to interactive, continually learning, and personalized systems, understanding and organizing the various memory mechanisms they employ is crucial for improving reasoning, adaptability, and contextual fidelity. The field has become fragmented across many works, so a unified framework is needed.", "method": "The paper conducts a structured literature survey, organizing prior work into a taxonomy of three memory types\u2014implicit, explicit, and agentic\u2014covering both textual and multi-modal models. It analyzes architectural designs, memory mechanisms, benchmark tasks, and challenges such as capacity, alignment, and interoperability across systems and modalities.", "result": "The survey synthesizes and categorizes existing research into a clear three-part framework: (1) implicit memory in model parameters and their interpretability and manipulation; (2) explicit memory via external, queryable knowledge stores like corpora, dense vectors, and graphs; and (3) agentic memory that supports persistent, temporally extended behavior in autonomous and multi-agent systems, including embodied AI. It also identifies key architectures and benchmarks and highlights unresolved issues.", "conclusion": "Memory is a central component in advancing LLMs and MLLMs toward more capable, interactive, and autonomous systems. Different memory paradigms\u2014implicit, explicit, and agentic\u2014serve complementary roles, and their integration, especially in multi-modal contexts, raises important challenges regarding capacity, factuality, alignment, and interoperability. The survey\u2019s taxonomy and analysis aim to guide future research on designing more robust and coherent memory-augmented models and agents."}}
{"id": "2601.09049", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09049", "abs": "https://arxiv.org/abs/2601.09049", "authors": ["Kaiyu He", "Zhang Mian", "Peilin Wu", "Xinya Du", "Zhiyu Chen"], "title": "Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers", "comment": null, "summary": "While Large Language Models (LLMs) excel at factual retrieval, they often struggle with the \"curse of two-hop reasoning\" in compositional tasks. Recent research suggests that parameter-sharing transformers can bridge this gap by forming a \"Generalization Circuit\" during a prolonged \"grokking\" phase. A fundamental question arises: Is a grokked model superior to its non-grokked counterparts on downstream tasks? Furthermore, is the extensive computational cost of waiting for the grokking phase worthwhile? In this work, we conduct a mechanistic study to evaluate the Generalization Circuit's role in knowledge assimilation and transfer. We demonstrate that: (i) The inference paths established by non-grokked and grokked models for in-distribution compositional queries are identical. This suggests that the \"Generalization Circuit\" does not represent the sudden acquisition of a new reasoning paradigm. Instead, we argue that grokking is the process of integrating memorized atomic facts into an naturally established reasoning path. (ii) Achieving high accuracy on unseen cases after prolonged training and the formation of a certain reasoning path are not bound; they can occur independently under specific data regimes. (iii) Even a mature circuit exhibits limited transferability when integrating new knowledge, suggesting that \"grokked\" Transformers do not achieve a full mastery of compositional logic.", "AI": {"tldr": "The paper examines whether the \"grokking\" phase and the resulting Generalization Circuit in parameter-sharing transformers actually improve compositional reasoning and transfer, finding that grokked models do not fundamentally change inference paths nor robustly enhance transfer of new knowledge.", "motivation": "LLMs and transformers are strong at factual recall but weak at multi-step (two-hop) compositional reasoning, often requiring long and computationally expensive training phases where grokking and a so\u2011called Generalization Circuit emerge. It is unclear whether this grokking-induced circuit truly provides better reasoning and transfer on downstream tasks or just reflects overtraining. The paper aims to mechanistically test if waiting for grokking is worth the cost and what role the Generalization Circuit actually plays in knowledge assimilation and transfer.", "method": "The authors perform a mechanistic interpretability study on parameter-sharing transformer models trained on compositional tasks involving two-hop reasoning. They compare models before and after the grokking phase, analyzing their internal inference paths (circuits) for in-distribution and out-of-distribution queries. They investigate whether the reasoning paths differ between non-grokked and grokked models, and test how both types of models assimilate new atomic facts and transfer them via the learned circuits under different data regimes.", "result": "(i) For in-distribution compositional queries, non-grokked and grokked models use essentially the same inference paths, implying that the Generalization Circuit is not a brand\u2011new reasoning mechanism. Grokking is instead the gradual integration of memorized atomic facts into an existing reasoning path. (ii) High accuracy on unseen (OOD) cases and the emergence of a particular reasoning path can appear independently, depending on the data regime; thus forming a circuit and generalizing well are not tightly coupled. (iii) Even when the circuit is fully developed, its ability to transfer and integrate new knowledge is limited, indicating that the grokked transformers still do not fully master compositional logic.", "conclusion": "The grokking phase and its associated Generalization Circuit do not yield a fundamentally new or superior compositional reasoning process. Instead, grokking reflects the slow incorporation of memorized facts into an already present inference path, and strong generalization on unseen cases does not necessarily depend on this process. Moreover, even mature circuits exhibit limited transfer when new knowledge is added, suggesting that grokked transformers fall short of robust, fully systematic compositional logic. As a result, the computational cost of waiting for grokking may not be justified if the goal is broad, transferable compositional reasoning."}}
{"id": "2601.09152", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09152", "abs": "https://arxiv.org/abs/2601.09152", "authors": ["Yiwen Tu", "Xuan Liu", "Lianhui Qin", "Haojian Jin"], "title": "PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?", "comment": null, "summary": "This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's \"privacy mind\", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \\PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.", "AI": {"tldr": "The paper proposes PRA, a personalized AI agent that simulates how individual users form privacy concerns based on their history and context, outperforming baselines in predicting privacy concerns.", "motivation": "Existing work focuses mainly on population-level sentiment or coarse privacy attitudes, lacking a way to model how specific individuals reason about privacy in response to concrete news or events. The authors want to capture user-specific, theory-grounded privacy reasoning rather than aggregate trends.", "method": "They design PRA, an AI agent that reconstructs a user\u2019s \"privacy mind\" from their comment history and contextual cues. A contextual filter, inspired by bounded rationality, selects relevant prior privacy memories, which are then used to generate synthetic comments expressing how that user would likely respond to a new privacy scenario. Additionally, they use an LLM-as-a-Judge evaluator, aligned with a formal privacy concern taxonomy, to assess and score the faithfulness of the generated reasoning and predicted concern levels.", "result": "On real-world Hacker News discussions, PRA better predicts users\u2019 privacy concerns than baseline agents and reveals reasoning patterns that generalize across domains such as AI, e-commerce, and healthcare.", "conclusion": "PRA offers a more nuanced, user-level simulation of privacy concern formation compared with aggregate sentiment models, combining privacy theory and cognitive theory in an AI agent. It improves predictive performance and uncovers transferable privacy reasoning patterns, suggesting a promising direction for personalized privacy analysis and simulation across domains."}}
{"id": "2601.09050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09050", "abs": "https://arxiv.org/abs/2601.09050", "authors": ["Tianyi Xu", "Xuan Ouyang", "Binwei Yao", "Shoua Xiong", "Sara Misurelli", "Maichou Lor", "Junjie Hu"], "title": "SITA: Learning Speaker-Invariant and Tone-Aware Speech Representations for Low-Resource Tonal Languages", "comment": "8 pages (excluding references, limitations, ethics, acknowledgement, and appendix); 4 figures in the main paper; appendix included", "summary": "Tonal low-resource languages are widely spoken yet remain underserved by modern speech technology. A key challenge is learning representations that are robust to nuisance variation such as gender while remaining tone-aware for different lexical meanings. To address this, we propose SITA, a lightweight adaptation recipe that enforces Speaker-Invariance and Tone-Awareness for pretrained wav2vec-style encoders. SITA uses staged multi-objective training: (i) a cross-gender contrastive objective encourages lexical consistency across speakers, while a tone-repulsive loss prevents tone collapse by explicitly separating same-word different-tone realizations; and (ii) an auxiliary Connectionist Temporal Classification (CTC)-based ASR objective with distillation stabilizes recognition-relevant structure. We evaluate primarily on Hmong, a highly tonal and severely under-resourced language where off-the-shelf multilingual encoders fail to represent tone effectively. On a curated Hmong word corpus, SITA improves cross-gender lexical retrieval accuracy, while maintaining usable ASR accuracy relative to an ASR-adapted XLS-R teacher. We further observe similar gains when transferring the same recipe to Mandarin, suggesting SITA is a general, plug-in approach for adapting multilingual speech encoders to tonal languages.", "AI": {"tldr": "They propose SITA, a lightweight adaptation method that makes pretrained wav2vec-style speech encoders both speaker-invariant and tone-aware, improving representation and word retrieval for low-resource tonal languages like Hmong and also working for Mandarin.", "motivation": "Tonal low-resource languages are common but poorly supported by current speech technology. Existing multilingual encoders often fail to capture tone well and are sensitive to nuisance factors like speaker gender, which harms lexical discrimination where tone carries meaning. There is a need for a simple way to adapt powerful pretrained encoders so that they remain robust to such variation while accurately modeling tones, especially when labeled data are scarce.", "method": "They introduce SITA, an adaptation recipe for pretrained wav2vec-style encoders that combines two training stages and multiple objectives. First, a cross-gender contrastive objective aligns representations of the same word spoken by different genders, enforcing lexical consistency and speaker invariance. In parallel, a tone-repulsive loss explicitly pushes apart representations of the same word produced with different tones, preventing tone information from collapsing. Second, they add an auxiliary CTC-based ASR objective with knowledge distillation from a stronger XLS-R ASR teacher model, so that the adapted encoder preserves recognition-relevant structure. The method is lightweight and plug-in: it fine-tunes existing multilingual speech encoders instead of training from scratch.", "result": "On a curated Hmong word corpus, SITA yields higher cross-gender lexical retrieval accuracy than the base multilingual encoders, indicating better speaker-invariant lexical representations, while keeping automatic speech recognition performance close to that of an ASR-adapted XLS-R teacher model. Applying the same recipe to Mandarin produces similar improvements, demonstrating that the approach generalizes beyond a single language.", "conclusion": "SITA provides an effective and lightweight way to adapt pretrained multilingual wav2vec-style speech encoders for tonal languages, simultaneously improving robustness to speaker variation and preserving or enhancing tone awareness. The method improves lexical retrieval and maintains strong ASR performance in low-resource tonal settings like Hmong, and its successful transfer to Mandarin suggests it can serve as a general plug-in strategy for better tone modeling in speech technology for tonal languages."}}
{"id": "2601.09182", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09182", "abs": "https://arxiv.org/abs/2601.09182", "authors": ["JungMin Yun", "JuneHyoung Kwon", "MiHyeon Kim", "YoungBin Kim"], "title": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback", "comment": "Accepted to AAAI 2026 Workshop on AI for Scientific Research (AI4Research)", "summary": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.", "AI": {"tldr": "The paper argues that instead of using LLMs to fully automate peer review, they should be used to mentor and assist human reviewers, improving review quality and sustainability.", "motivation": "AI research is growing rapidly, creating a Reviewer Gap where there are not enough qualified reviewers to handle the volume of submissions. This strains peer-review sustainability and leads to a cycle of low-quality, rushed, or superficial evaluations. Existing LLM-based review-generation methods risk reinforcing this problem by automating low-quality reviews rather than addressing root causes such as reviewer training and expertise development.", "method": "This is a position paper, so the method is conceptual rather than empirical. The authors (1) critically examine current approaches that use large language models to automatically generate reviews, identifying their limitations and risks; (2) articulate core principles that define high-quality peer review; and (3) design two conceptual LLM-based systems aligned with these principles: an LLM-assisted mentoring system to build reviewers\u2019 long-term skills, and an LLM-assisted feedback system that helps reviewers iteratively improve specific reviews they write.", "result": "The main results are the articulation of a human-centered framework for LLM use in peer review and the concrete proposals of two complementary systems: a mentoring system for developing reviewer competence over time and a feedback system that supports on-demand refinement of individual reviews. These systems conceptually demonstrate how LLMs can be integrated into the review process without replacing human judgment, thereby potentially improving review quality and alleviating reviewer burden.", "conclusion": "The paper concludes that using LLMs to replace human reviewers is misguided and may worsen the Reviewer Gap by entrenching low-quality reviewing practices. Instead, LLMs should be leveraged as educational and assistive tools that enhance human reviewers\u2019 skills and foster a more robust, sustainable peer-review ecosystem. The proposed mentoring and feedback systems embody this human-centered paradigm and point toward future research on scalable, quality-focused reviewer support infrastructure."}}
{"id": "2601.09059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09059", "abs": "https://arxiv.org/abs/2601.09059", "authors": ["Santiago Mart\u00ednez Novoa", "Nicol\u00e1s Rozo Fajardo", "Diego Alejandro Gonz\u00e1lez Vargas", "Nicol\u00e1s Bedoya Figueroa"], "title": "Efficient Multilingual Dialogue Processing via Translation Pipelines and Distilled Language Models", "comment": null, "summary": "This paper presents team Kl33n3x's multilingual dialogue summarization and question answering system developed for the NLPAI4Health 2025 shared task. The approach employs a three-stage pipeline: forward translation from Indic languages to English, multitask text generation using a 2.55B parameter distilled language model, and reverse translation back to source languages. By leveraging knowledge distillation techniques, this work demonstrates that compact models can achieve highly competitive performance across nine languages. The system achieved strong win rates across the competition's tasks, with particularly robust performance on Marathi (86.7% QnA), Tamil (86.7% QnA), and Hindi (80.0% QnA), demonstrating the effectiveness of translation-based approaches for low-resource language processing without task-specific fine-tuning.", "AI": {"tldr": "A multilingual medical dialogue summarization and Q&A system using translation and a compact distilled language model, achieving strong results across nine Indic languages without task-specific fine-tuning.", "motivation": "To build an effective, multilingual dialogue summarization and question answering system for low-resource Indic languages in the medical domain, addressing the challenge of limited labeled data and large model resource requirements.", "method": "A three-stage pipeline: (1) forward translation of Indic-language dialogues into English, (2) multitask text generation with a 2.55B-parameter distilled language model for summarization and Q&A, and (3) reverse translation of outputs back into the original Indic languages. The system relies on knowledge distillation to compress a larger model into a compact one while maintaining performance, and it does not use task-specific fine-tuning for each language.", "result": "The system achieved high win rates in the NLPAI4Health 2025 shared task across nine Indic languages, with especially strong Q&A accuracy in Marathi (86.7%), Tamil (86.7%), and Hindi (80.0%), indicating that the compact distilled model with translation-based processing is highly competitive.", "conclusion": "Translation-based multilingual pipelines combined with compact distilled language models can provide strong performance for dialogue summarization and Q&A in low-resource Indic languages, even without task-specific fine-tuning, making them practical and efficient for real-world healthcare NLP applications."}}
{"id": "2601.09259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09259", "abs": "https://arxiv.org/abs/2601.09259", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "comment": null, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "AI": {"tldr": "The paper proposes MAXS, a meta-adaptive lookahead framework for LLM agents that improves multi-tool reasoning by selecting stable and high-value reasoning steps while controlling computation via trajectory convergence.", "motivation": "Existing LLM agents that use multiple tools struggle with locally myopic generation (no lookahead) and unstable reasoning trajectories where small early errors grow into large deviations. This makes it hard to achieve good global reasoning quality without excessive computation. The paper aims to create a framework that maintains global effectiveness in multi-step, multi-tool reasoning while remaining computationally efficient.", "method": "The authors introduce MAXS, a meta-adaptive reasoning framework for LLM agents. MAXS integrates two key components: (1) a lookahead strategy that rolls out reasoning paths several steps ahead to estimate the advantage value of particular tool uses; and (2) a stability-oriented selection mechanism that uses step consistency variance and inter-step trend slopes to pick reasoning steps that are both high-value and stable/consistent. They also propose a trajectory convergence mechanism that stops further rollouts when path consistency is detected, thereby limiting computational cost. The framework is applied on top of existing vision-language LLM agents and evaluated across models and datasets.", "result": "Across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, MAXS yields better task performance and higher inference efficiency than baseline agent methods. Ablation and further analysis show that both the lookahead strategy and the explicit modeling of tool usage contribute positively to the observed gains.", "conclusion": "MAXS effectively mitigates local myopia and trajectory instability in multi-tool LLM agents by combining lookahead rollouts, stability-aware step selection, and trajectory convergence. This meta-adaptive framework improves both accuracy and efficiency and can serve as a generally useful approach for more reliable, resource-aware multi-tool reasoning with LLM agents."}}
{"id": "2601.09065", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09065", "abs": "https://arxiv.org/abs/2601.09065", "authors": ["Yinuo Xu", "David Jurgens"], "title": "Beyond Consensus: Perspectivist Modeling and Evaluation of Annotator Disagreement in NLP", "comment": null, "summary": "Annotator disagreement is widespread in NLP, particularly for subjective and ambiguous tasks such as toxicity detection and stance analysis. While early approaches treated disagreement as noise to be removed, recent work increasingly models it as a meaningful signal reflecting variation in interpretation and perspective. This survey provides a unified view of disagreement-aware NLP methods. We first present a domain-agnostic taxonomy of the sources of disagreement spanning data, task, and annotator factors. We then synthesize modeling approaches using a common framework defined by prediction targets and pooling structure, highlighting a shift from consensus learning toward explicitly modeling disagreement, and toward capturing structured relationships among annotators. We review evaluation metrics for both predictive performance and annotator behavior, and noting that most fairness evaluations remain descriptive rather than normative. We conclude by identifying open challenges and future directions, including integrating multiple sources of variation, developing disagreement-aware interpretability frameworks, and grappling with the practical tradeoffs of perspectivist modeling.", "AI": {"tldr": "The paper surveys disagreement-aware NLP, arguing that annotator disagreement is a meaningful signal rather than noise and organizing methods, evaluation, and open problems for modeling it.", "motivation": "Annotator disagreement is common in subjective and ambiguous NLP tasks like toxicity and stance, but traditional pipelines force consensus labels and treat differences as errors. Recent work suggests that disagreement captures genuine variation in human interpretation and social perspective, yet there is no unified overview of how to model and evaluate it. The paper aims to systematize this emerging area so that researchers can better understand the sources of disagreement, the space of modeling choices, and their implications for fairness and reliability.", "method": "The authors conduct a survey, proposing: (1) a domain-agnostic taxonomy of sources of annotator disagreement, organized across data (e.g., ambiguity, context), task (e.g., label definitions, instructions), and annotator factors (e.g., background, identity, expertise); (2) a unified framework for existing modeling approaches, defined by what is predicted (consensus vs. distributions vs. individual annotators) and how labels or annotators are pooled or structured (e.g., independent, clustered, relational); and (3) a review and categorization of evaluation metrics that assess both predictive performance and how well models capture annotator behavior and fairness properties.", "result": "The survey shows a clear field-wide shift from consensus-based learning that collapses disagreement into a single label toward methods that explicitly model label distributions, individual annotator behavior, and the structured relationships among annotators and perspectives. It compiles and organizes disparate metrics used to evaluate such models and highlights that current fairness evaluations largely remain descriptive, focusing on measuring differences across groups rather than specifying or enforcing normative fairness criteria.", "conclusion": "The authors conclude that disagreement in annotation is an important signal reflecting pluralistic perspectives and should be systematically modeled rather than eliminated. They identify open challenges: combining multiple sources and types of variation in a single framework, developing interpretability techniques that are themselves disagreement-aware, and understanding the practical tradeoffs of adopting perspectivist modeling (e.g., complexity, deployment, and fairness implications). They call for moving from merely describing disagreement to designing principled, normative approaches for handling it in NLP systems."}}
{"id": "2601.09260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09260", "abs": "https://arxiv.org/abs/2601.09260", "authors": ["Yan Liu", "Feng Zhang", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Han Liu", "Yangdong Deng"], "title": "Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models", "comment": null, "summary": "High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.", "AI": {"tldr": "They introduce CoT-Flow, a framework that models chain-of-thought reasoning steps as a continuous probabilistic flow, enabling more efficient decoding and dense-reward RL without external verifiers.", "motivation": "Existing chain-of-thought methods view reasoning as a monolithic sequence and lack a way to measure per-step information gain, leading to inefficient, redundant reasoning at inference time and sparse or expensive supervision during training.", "method": "They formalize reasoning steps as a continuous probabilistic flow that assigns each step a quantified contribution toward the correct answer. On top of this, they design (1) flow-guided decoding, a greedy decoding procedure that prefers steps with higher incremental information gain to obtain shorter but effective reasoning traces, and (2) flow-based reinforcement learning, which derives a dense, verifier-free reward signal from the flow to optimize reasoning policies.", "result": "On difficult reasoning benchmarks, CoT-Flow yields better trade-offs between reasoning accuracy and inference cost (e.g., shorter chains, fewer tokens, or calls) compared with standard chain-of-thought baselines and existing guided-decoding or RL approaches.", "conclusion": "Modeling chain-of-thought as a probabilistic flow over reasoning steps makes it possible to quantify step-wise information gain, which in turn supports more efficient decoding and more effective, verifier-free RL training, leading to improved overall reasoning performance for large language models."}}
{"id": "2601.09066", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09066", "abs": "https://arxiv.org/abs/2601.09066", "authors": ["Donghoon Shin", "Sejung Lee", "Soonmin Bae", "Hwijung Ryu", "Changwon Ok", "Hoyoun Jung", "Hyesung Ji", "Jeehyun Lim", "Jehoon Lee", "Ji-Eun Han", "Jisoo Baik", "Mihyeon Kim", "Riwoo Chung", "Seongmin Lee", "Wonjae Park", "Yoonseok Heo", "Youngkyung Seo", "Seyoun Won", "Boeun Kim", "Cheolhun Heo", "Eunkyeong Lee", "Honghee Lee", "Hyeongju Ju", "Hyeontae Seo", "Jeongyong Shim", "Jisoo Lee", "Junseok Koh", "Junwoo Kim", "Minho Lee", "Minji Kang", "Minju Kim", "Sangha Nam", "Seongheum Park", "Taehyeong Kim", "Euijai Ahn", "Hong Seok Jeung", "Jisu Shin", "Jiyeon Kim", "Seonyeong Song", "Seung Hyun Kong", "Sukjin Hong", "Taeyang Yun", "Yu-Seon Kim", "A-Hyun Lee", "Chae-Jeong Lee", "Hye-Won Yu", "Ji-Hyun Ahn", "Song-Yeon Kim", "Sun-Woo Jung", "Eunju Kim", "Eunji Ha", "Jinwoo Baek", "Yun-ji Lee", "Wanjin Park", "Jeong Yeop Kim", "Eun Mi Kim", "Hyoung Jun Park", "Jung Won Yoon", "Min Sung Noh", "Myung Gyo Oh", "Wongyoung Lee", "Yun Jin Park", "Young S. Kwon", "Hyun Keun Kim", "Jieun Lee", "YeoJoo Park"], "title": "Mi:dm 2.0 Korea-centric Bilingual Language Models", "comment": null, "summary": "We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.", "AI": {"tldr": "Mi:dm 2.0 is a bilingual (Korean\u2013English) LLM family (11.5B Base, 2.3B Mini) optimized for Korean language, culture, and use cases, achieving SOTA on Korean benchmarks and released under MIT license.", "motivation": "Existing LLMs perform poorly for Korean users because available Korean data are limited, noisy, and not culturally aligned. This leads to weak understanding of Korean language nuances, reasoning styles, and context, which is a barrier to deploying AI widely in Korea\u2019s industries, public sector, and education. The authors aim to fill this gap with models that are both linguistically and culturally optimized for Korean while remaining broadly usable and openly available.", "method": "They design a bilingual LLM specifically tailored to Korean through: (1) a robust data pipeline with proprietary cleaning and filtering of Korean data; (2) generation of high-quality synthetic Korean data; (3) strategic data mixing and curriculum learning to guide training; (4) a custom tokenizer optimized for Korean morphology and vocabulary; and (5) two model sizes/configurations: an 11.5B parameter Base model with a depth-up scaling strategy for general use, and a 2.3B Mini model for resource-limited environments and specialized applications. They then evaluate on Korean-specific benchmarks like KMMLU and internal tests across language, humanities, and social science tasks.", "result": "Mi:dm 2.0 achieves state-of-the-art performance on Korean-focused benchmarks, including top zero-shot scores on KMMLU, and performs strongly on internal evaluations across multiple academic and practical domains. The models are robust, culturally aligned, and efficient for Korean use cases, with both large and small versions covering diverse deployment scenarios.", "conclusion": "Mi:dm 2.0 demonstrates that a Korea-centric design\u2014spanning data curation, tokenizer customization, training strategy, and model scaling\u2014can substantially improve performance and cultural appropriateness for Korean users. By releasing both models under the MIT license and hosting them on Hugging Face, the authors aim to catalyze research, commercial applications, and community building around \u201cK-intelligence,\u201d thereby accelerating AI adoption across Korean sectors."}}
{"id": "2601.09264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09264", "abs": "https://arxiv.org/abs/2601.09264", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "comment": "20pages, 6 figures, a 60-page supporting material pdf file", "summary": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "AI": {"tldr": "The paper proposes and evaluates an LLM-based multi-agent framework that coordinates pandemic policies across regions, showing large reductions in infections and deaths compared with real-world COVID-19 outcomes in the US.", "motivation": "Pandemic control requires timely, coordinated policies across interdependent regions, but human policymaking is often fragmented, reactive, and locally focused, leading to delayed and suboptimal interventions. The authors aim to create a system that can proactively and jointly design policies for multiple regions while accounting for epidemiological and mobility-driven interdependencies.", "method": "The authors design a multi-agent framework where each administrative region (e.g., US state) is assigned a large language model agent acting as an AI policy assistant. Each agent reasons over its region\u2019s epidemiological state, communicates with other regional agents to model cross-regional effects (e.g., via mobility), and uses a pandemic evolution simulator plus real-world data in a closed-loop setting to explore counterfactual intervention scenarios. Structured inter-agent communication and iterative simulation are used to synthesize coordinated policy recommendations. They validate the approach using US state-level COVID-19 data (April\u2013December 2020), real mobility data, and actual historical policy actions as baselines.", "result": "In simulation using real US COVID-19 and mobility data, the LLM multi-agent framework yields substantially better outcomes than the realized historical policies. At the individual state level, cumulative infections and deaths are reduced by up to 63.7% and 40.1%, respectively. Aggregated over all states, infections and deaths are reduced by 39.0% and 27.0%, respectively, relative to real-world outcomes.", "conclusion": "Coordinated AI policymaking using an LLM-based multi-agent system can significantly improve pandemic control compared with fragmented human-driven decision-making. By jointly reasoning over epidemiological dynamics, mobility-driven interdependence, and counterfactual policies in a simulation loop, such systems can discover more effective and proactive intervention strategies, suggesting that LLM multi-agent frameworks are a promising tool for future pandemic preparedness and response."}}
{"id": "2601.09069", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09069", "abs": "https://arxiv.org/abs/2601.09069", "authors": ["Kanyao Han", "Yushang Lai"], "title": "From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models", "comment": null, "summary": "Knowledge graphs (KGs) have commonly been constructed using predefined symbolic relation schemas, typically implemented as categorical relation labels. This design has notable shortcomings: real-world relations are often contextual, nuanced, and sometimes uncertain, and compressing it into discrete relation labels abstracts away critical semantic detail. Nevertheless, symbolic-relation KGs remain widely used because they have been operationally effective and broadly compatible with pre-LLM downstream models and algorithms, in which KG knowledge could be retrieved or encoded into quantified features and embeddings at scale. The emergence of LLMs has reshaped how knowledge is created and consumed. LLMs support scalable synthesis of domain facts directly in concise natural language, and prompting-based inference favors context-rich free-form text over quantified representations. This position paper argues that these changes call for rethinking the representation of relations themselves rather than merely using LLMs to populate conventional schemas more efficiently. We therefore advocate moving from symbolic to natural-language relation descriptions, and we propose hybrid design principles that preserve a minimal structural backbone while enabling more flexible and context-sensitive relational representations.", "AI": {"tldr": "Argues that with the rise of LLMs, knowledge graphs should shift from rigid symbolic relation labels to natural-language relation descriptions, using a hybrid design that keeps light structure but allows context-rich, flexible relations.", "motivation": "Traditional knowledge graphs use predefined, categorical relation schemas. These are too rigid for real-world relations, which are often contextual, nuanced, and uncertain. While symbolic KGs worked well with pre-LLM models that relied on structured, quantified features, this setup misaligns with how modern LLMs generate and use knowledge through natural language and context-rich prompts.", "method": "This is a position paper rather than an empirical study. The authors conceptually analyze the limitations of symbolic relation labels in KGs in the context of LLMs, and propose design principles for a new, hybrid representation where relations are expressed in natural language while maintaining a minimal structural backbone for organization and interoperability.", "result": "The paper does not present experimental results but offers a conceptual framework: it articulates the drawbacks of symbolic-only relations and sketches how natural-language relation descriptions could be integrated into KG design to better align with LLM-based knowledge creation and reasoning.", "conclusion": "The authors conclude that, in the LLM era, it is not sufficient to use LLMs just to fill traditional KG schemas more efficiently. Instead, we should rethink the representation of relations by moving from purely symbolic labels to natural-language descriptions, adopting hybrid KG designs that retain minimal structure while enabling more flexible, context-sensitive relational semantics."}}
{"id": "2601.09269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09269", "abs": "https://arxiv.org/abs/2601.09269", "authors": ["Wencheng Ye", "Liang Peng", "Xiaoyang Yuan", "Yi Bin", "Pengpeng Zeng", "Hengyu Jin", "Heng Tao Shen"], "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering", "comment": null, "summary": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.", "AI": {"tldr": "RISER is a plug-and-play, activation-space steering framework that adaptively improves LLM reasoning by learning to route and compose reusable reasoning vectors, achieving better zero-shot accuracy and token efficiency than CoT-style methods.", "motivation": "Existing domain-specific reasoning improvements for LLMs largely depend on fine-tuning or other parameter-updating methods, which are training- and resource-intensive. Activation steering offers a more parameter-efficient alternative, but current approaches use static, manually chosen intervention directions that cannot adapt to varying inputs or complex, multi-step reasoning. The paper aims to create a controllable, adaptive, and efficient way to steer LLM reasoning without modifying model weights or relying on hand-crafted interventions.", "method": "The authors introduce RISER, a plug-and-play framework that operates purely in activation space. First, they build a library of reusable \u201creasoning vectors\u201d (directions in activation space associated with particular reasoning behaviors or primitives). Then they add a lightweight Router module that, for each input, selects and composes these vectors on the fly. The Router is trained with reinforcement learning using task-level rewards (e.g., correctness of final answers) to learn which combinations of reasoning vectors improve performance. During inference, the Router dynamically activates and combines these vectors, thereby steering the LLM\u2019s internal computations toward better reasoning trajectories without altering the original model parameters.", "result": "On seven diverse benchmarks, RISER improves the base LLM\u2019s zero-shot accuracy by about 3.4\u20136.5% on average. It outperforms chain-of-thought (CoT) style prompting while using 2\u20133 times fewer tokens, indicating better efficiency. The method maintains robust accuracy gains across tasks, and the learned routing strategies demonstrate that the system can autonomously select and combine multiple reasoning vectors in a meaningful way.", "conclusion": "RISER shows that dynamic, RL-trained routing over reusable reasoning vectors can effectively and efficiently steer LLM reasoning in activation space, without updating model parameters. The approach yields consistent accuracy improvements and higher token efficiency compared to CoT prompting, and the resulting control strategies are interpretable and compositional. This suggests a promising direction for more controllable, modular, and resource-efficient reasoning enhancement in large language models."}}
{"id": "2601.09084", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09084", "abs": "https://arxiv.org/abs/2601.09084", "authors": ["Wilson Y. Lee"], "title": "How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation", "comment": null, "summary": "Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.", "AI": {"tldr": "The paper analyzes how many human preference judgments are needed to reliably detect small improvements between generative models and shows that typical evaluations are statistically underpowered.", "motivation": "Human preference evaluations are the de facto standard for comparing generative models, but practitioners lack guidance on the sample sizes needed to detect small but meaningful performance differences. As models become closer in quality, naive or ad\u2011hoc evaluation designs may wrongly conclude that models are equivalent when, in fact, the evaluation is just too small or too noisy.", "method": "The authors study preference evaluations through a statistical decision-theoretic lens. They characterize regimes where preference signals are either diffuse (all prompt types are similarly informative) or concentrated. Under assumptions of diffuse signal, they analytically show that proportional allocation of judgments across prompts is minimax-optimal, meaning no alternative allocation can substantially improve detectability. They then empirically analyze large-scale human preference datasets across multiple modalities (chat, images, code) to estimate preference margins, variance, and effect sizes, and to compare standard evaluation protocols with curated benchmarks that reduce prompt-induced variability.", "result": "Theoretical analysis shows that in the common case where preference signal is diffuse, spreading judgments proportionally across prompts is essentially optimal, so clever reallocation strategies do not dramatically reduce the number of required judgments. Empirically, most real-world model comparisons lie in this diffuse regime, with small preference margins and high variance, implying that reliable detection of improvements requires far more human judgments than commonly collected, even in large evaluations. These limitations hold across different tasks and evaluation protocols. However, curated benchmarks that reduce prompt-level variability achieve systematically larger preference margins and about a 1.5\u00d7 reduction in prompt-level variance, which in turn improves statistical power.", "conclusion": "Human preference evaluations of generative models are often severely underpowered for detecting the small gains that modern models achieve, so null or inconclusive results frequently reflect insufficient sample size rather than true model parity. In diffuse-signal settings, proportional allocation of judgments is near-optimal, meaning that power must primarily be increased by collecting more data rather than by reallocating it. Curating prompts to reduce variability can materially improve detectability by increasing effect sizes and lowering variance. Therefore, future evaluation practice should explicitly plan around effect size, variance, budget, and protocol design rather than relying on informal or fixed-size human preference studies."}}
{"id": "2601.09274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09274", "abs": "https://arxiv.org/abs/2601.09274", "authors": ["Jian Zhang", "Yu He", "Zhiyuan Wang", "Zhangqi Wang", "Kai He", "Fangzhi Xu", "Qika Lin", "Jun Liu"], "title": "$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "comment": null, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \\textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "AI": {"tldr": "A^3-Bench is a new benchmark to evaluate scientific reasoning in AI by focusing on memory-driven activation of prior knowledge, using anchors and attractors, a dual-scale evaluation framework, and a new utilization index metric.", "motivation": "Existing scientific reasoning benchmarks mostly check whether final answers are correct or whether intermediate reasoning steps are coherent. They largely ignore the underlying memory mechanisms that humans use, where prior knowledge is activated as anchors and attractors and then integrated into reasoning. The authors want a benchmark that explicitly targets and measures these memory-driven processes.", "method": "The authors construct A^3-Bench by annotating 2,198 science reasoning problems across domains using the SAPM process: subject, anchor & attractor, problem, and memory developing. They then design a dual-scale evaluation framework based on anchor and attractor activation and propose the AAUI (Anchor\u2013Attractor Utilization Index) to quantify how well models activate and utilize these memory structures. They run experiments using multiple base models and prompting paradigms to evaluate performance on the benchmark.", "result": "They obtain experimental results across various models and paradigms that show measurable differences in how models activate anchor and attractor memories, as captured by AAUI. The benchmark successfully reveals how memory activation patterns correlate with reasoning quality and performance in scientific problem solving.", "conclusion": "A^3-Bench provides a targeted way to assess memory-driven scientific reasoning in AI models, moving beyond simple answer accuracy or chain-of-thought coherence. By modeling and measuring anchor and attractor activation with a dual-scale framework and AAUI metric, the work offers new insights and tools for analyzing and improving memory-based reasoning mechanisms."}}
{"id": "2601.09089", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09089", "abs": "https://arxiv.org/abs/2601.09089", "authors": ["Shuyang Hou", "Yi Hu", "Muhan Zhang"], "title": "SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding", "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly enhanced their reasoning capabilities. However, they continue to struggle with basic character-level tasks, such as counting letters in words, a problem rooted in their tokenization process. While existing benchmarks have highlighted this weakness through basic character operations, such failures are often dismissed due to lacking practical relevance. Yet, many real-world applications, such as navigating text-based maps or interpreting structured tables, rely heavily on precise sub-token understanding. In this regard, we introduce SubTokenTest, a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks. Our benchmark includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. We provide a comprehensive evaluation of nine advanced LLMs. Additionally, we investigate the impact of test-time scaling on sub-token reasoning and explore how character-level information is encoded within the hidden states.", "AI": {"tldr": "They propose SubTokenTest, a benchmark to test LLMs on practical sub-token (character-level) understanding tasks, evaluate multiple models, test scaling, and analyze hidden states.", "motivation": "LLMs are good at reasoning but bad at basic character-level tasks due to tokenization; existing benchmarks are dismissed as toy tasks, yet many real-world applications need precise sub-token understanding (e.g., maps, tables). There is a need for a practically relevant, utility-driven benchmark to rigorously assess sub-token understanding.", "method": "Design SubTokenTest, a benchmark with 10 tasks across 4 domains that require sub-token understanding while minimizing confounds from higher-level reasoning. Use these tasks to systematically evaluate nine advanced LLMs, and conduct additional experiments on test-time scaling and representation analysis to see how character-level information is encoded in hidden states.", "result": "They obtain performance measurements of nine advanced LLMs on the ten sub-token tasks, revealing how and where LLMs fail at sub-token reasoning. They also characterize how test-time scaling affects sub-token reasoning and uncover patterns of how character-level information is represented within model hidden states.", "conclusion": "SubTokenTest demonstrates that LLMs still exhibit substantial weaknesses in sub-token understanding, even in practical settings. The benchmark isolates tokenization-related failures from general reasoning errors, providing a more realistic and diagnostic way to measure and study sub-token reasoning and representation in LLMs."}}
{"id": "2601.09278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09278", "abs": "https://arxiv.org/abs/2601.09278", "authors": ["Xiaohan Yu", "Chao Feng", "Lang Mei", "Chong Chen"], "title": "M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning", "comment": null, "summary": "Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.", "AI": {"tldr": "The paper introduces M^3Searcher, a modular multimodal information-seeking agent that improves autonomous web research beyond text-only settings, and a supporting dataset MMSearchVQA for training and evaluation.", "motivation": "DeepResearch-style agents are effective for autonomous information acquisition but are restricted to text-only environments. Extending them to multimodal (image + text) web settings is challenging due to (1) a specialization-generalization trade-off in training models for large-scale multimodal tool use, and (2) a lack of training data capturing complex, multi-step multimodal search trajectories. The work aims to overcome these limitations so that agents can robustly handle complex multimodal information-seeking tasks.", "method": "The authors design M^3Searcher, a modular multimodal information-seeking agent that separates the process of acquiring information (retrieval and tool use) from the process of deriving final answers (reasoning and synthesis). They optimize the agent via a retrieval-oriented multi-objective reward that simultaneously promotes factual accuracy, sound reasoning, and high-quality retrieval behavior. To support reinforcement-learning style training, they construct MMSearchVQA, a multimodal multi-hop dataset focused on retrieval-centric trajectories and questions.", "result": "Experiments show that M^3Searcher surpasses existing baselines on complex multimodal tasks, achieving better performance in factual correctness, reasoning quality, and retrieval effectiveness. The agent also demonstrates strong transfer ability to new tasks and settings, maintaining effective reasoning under varied multimodal conditions.", "conclusion": "Decoupling information acquisition from answer derivation, together with a retrieval-focused multi-objective reward and a dedicated multimodal multi-hop dataset, leads to significant gains in multimodal autonomous information seeking. M^3Searcher offers a more general and adaptable framework for multimodal DeepResearch-style agents and sets a new performance standard on complex multimodal search and reasoning benchmarks."}}
{"id": "2601.09119", "categories": ["cs.CL", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.09119", "abs": "https://arxiv.org/abs/2601.09119", "authors": ["Yongming Sun"], "title": "Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms", "comment": null, "summary": "Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.", "AI": {"tldr": "The paper proposes a zero-shot framework for mapping job advertisements to standardized skill taxonomies (ESCO) without labeled training data, using LLM-based synthetic data generation, hierarchical multi-skill generation, and a contrastive bi-encoder model, achieving strong performance on Chinese job ads.", "motivation": "Fine-grained labor market analysis needs automatic mapping of unstructured job ads to standardized skills (e.g., ESCO). Existing supervised XMLC approaches require large, expensive, taxonomy-aligned labeled datasets, which are particularly scarce in non-English languages where job ad language differs from formal skill definitions. The authors aim to remove this annotation bottleneck and enable scalable skill extraction across languages.", "method": "1) Use a Large Language Model to synthesize job-ad-like training instances from ESCO skill definitions. 2) Introduce hierarchy-conditioned, multi-skill text generation using ESCO Level-2 categories so that each synthetic job requirement segment is semantically coherent and reflects related skills. 3) Train a contrastive bi-encoder on the synthetic corpus to align job-ad sentences and ESCO skill descriptions in a shared embedding space; the encoder extends a BERT backbone with BiLSTM and attention pooling to better capture long, dense requirement statements. 4) Use an upstream RoBERTa-based binary classifier to filter out non-skill sentences from job ads, improving overall precision. 5) Apply the trained system in a zero-shot manner to real job ads to retrieve relevant ESCO skills.", "result": "Hierarchy-conditioned LLM generation yields more fluent and discriminative synthetic training data than unconstrained pairing. The multi-label contrastive model trained on this data transfers effectively to real Chinese job ads, achieving strong zero-shot skill retrieval performance, with F1@5 = 0.72, and outperforming TF\u2013IDF and standard BERT baselines.", "conclusion": "Synthetic, hierarchy-aware LLM data generation combined with a contrastive bi-encoder and a sentence-level skill filter provides an effective zero-shot pipeline for mapping job ads to ESCO skills. This approach reduces dependence on costly labeled data, generalizes to non-English labor markets, and offers a scalable solution for automated skill coding in labor economics and workforce analytics."}}
{"id": "2601.09281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09281", "abs": "https://arxiv.org/abs/2601.09281", "authors": ["Jingjing Zhou", "Gaoxiang Cong", "Li Su", "Liang Li"], "title": "STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.", "AI": {"tldr": "Proposes STaR, an inference-time framework to remove sensitive information from the entire reasoning chain of Large Reasoning Models, not just final answers, and introduces new metrics to evaluate privacy-preserving unlearning.", "motivation": "Existing unlearning methods for language models mainly modify final outputs, which is inadequate for Large Reasoning Models whose privacy-sensitive information can appear throughout multi-step Chain-of-Thought trajectories. There is a need for a method that protects privacy across all intermediate reasoning steps without retraining the model, and for better evaluation metrics of unlearning effectiveness in this setting.", "method": "STaR is a parameter-free, inference-time unlearning framework with four components: (1) semantic-aware detection to identify sensitive content in generated or candidate reasoning steps; (2) secure prompt prefix to inject global safety constraints into the model\u2019s reasoning; (3) trajectory-aware suppression that dynamically blocks or redirects generation away from sensitive content across the entire reasoning chain; and (4) token-level adaptive filtering that censors both exact sensitive tokens and paraphrased variants during generation. Additionally, the paper proposes two evaluation metrics: Multi-Decoding Consistency Assessment (MCS) to test whether unlearning holds under different decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation to measure privacy leakage at both final answers and intermediate reasoning steps.", "result": "On the R-TOFU benchmark, STaR provides strong and stable unlearning performance, effectively suppressing sensitive content along full reasoning trajectories while incurring only small losses in task utility. It outperforms existing unlearning baselines and maintains robustness across multiple decoding strategies, as quantified by the newly proposed MCS and MIA metrics.", "conclusion": "Inference-time, parameter-free regulation of reasoning trajectories can deliver comprehensive privacy protection for Large Reasoning Models, overcoming limitations of output-only unlearning methods. STaR sets a new performance bar on a dedicated privacy/unlearning benchmark and offers evaluation tools that better capture the trajectory-level nature of privacy leakage in reasoning models."}}
{"id": "2601.09120", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09120", "abs": "https://arxiv.org/abs/2601.09120", "authors": ["Chen-Wei Liang", "Bin Guo", "Zhen-Yuan Wei", "Mu-Jiang-Shan Wang"], "title": "Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment", "comment": "18 pages, 7 figures. Preprint", "summary": "Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\\% cross-jurisdictional performance retention versus 76.2\\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.", "AI": {"tldr": "The paper proposes a three-stage, relationship-aware, domain-adaptive framework that significantly improves automatic patent claim generation and its quality assessment across jurisdictions.", "motivation": "Existing patent claim generation systems generalize poorly across jurisdictions, fail to adequately model the semantic relationships between claims and prior art, and lack reliable, unified quality assessment, which limits their utility in real-world patent prosecution workflows.", "method": "The authors design a three-stage framework: (1) relationship-aware similarity analysis using multi-head attention with eight specialized heads to explicitly model relationships between prior art and claims; (2) domain-adaptive claim generation via curriculum learning and dynamic selection of LoRA adapters specialized for five patent domains; and (3) unified quality assessment using cross-attention mechanisms between different evaluation aspects to provide a holistic evaluation of generated claims.", "result": "On USPTO HUPD, EPO collections, and the Patent-CE benchmark, the approach yields large gains: +7.6 ROUGE-L over GPT-4o, +8.3% BERTScore over Llama-3.1-8B, and substantially higher correlation with human experts (0.847 vs. 0.623) for quality assessment models, while retaining 89.4% of performance when transferred across jurisdictions compared with 76.2% for baselines.", "conclusion": "The proposed framework offers a strong, generalizable solution for automated patent claim generation and evaluation, substantially improving semantic alignment with prior art, cross-jurisdiction robustness, and agreement with human experts, and thereby supporting end-to-end automated patent prosecution workflows."}}
{"id": "2601.09282", "categories": ["cs.AI", "cs.DC", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09282", "abs": "https://arxiv.org/abs/2601.09282", "authors": ["Leszek Sliwko", "Jolanta Mizeria-Pietraszko"], "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing", "comment": null, "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.", "AI": {"tldr": "The paper proposes an LLM-driven, natural-language-based scheduling approach for Kubernetes clusters, showing that semantic soft affinity expressed in plain English can be accurately interpreted and yields scheduling quality on par with or better than traditional, complex configs, though latency remains a deployment concern.", "motivation": "Configuring workload placement in Kubernetes clusters typically requires detailed, technical affinity/anti-affinity rules and resource constraints, which are hard for many users to express and maintain. As workloads and policies grow more complex\u2014especially with nuanced, soft preferences and trade-offs\u2014this configuration burden creates a usability gap and risks suboptimal scheduling. The authors are motivated to simplify how users express scheduling intent, allowing them to use natural language instead of low-level configuration primitives, and to test whether such an approach can maintain or improve scheduling quality.", "method": "The authors design a semantic, intent-driven scheduling paradigm by integrating a Large Language Model into the Kubernetes scheduling pipeline via a scheduler extender. Users attach natural-language allocation hints as annotations to workloads, describing soft affinity preferences (e.g., preferred regions, node types, or co-location constraints). A prototype is built with (1) a cluster state cache to expose current cluster information and (2) an intent analyzer using AWS Bedrock-hosted LLMs, which parses the natural language hints into structured soft-affinity constraints. Several top-tier LLMs (Amazon Nova Pro/Premier, Mistral Pixtral Large) are evaluated against a labeled ground-truth dataset to measure parsing accuracy, and the scheduler\u2019s placement performance is tested across six representative scenarios, including complex quantitative and conflicting-preference cases, compared against standard Kubernetes configurations and a baseline parsing engine.", "result": "The LLM-based intent analyzer attains high parsing performance, exceeding 95% Subset Accuracy on the evaluation dataset for leading models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. In end-to-end scheduling experiments over six scenarios, the prototype scheduler delivers equal or better workload placement compared to standard Kubernetes configurations. Its advantages are most pronounced in scenarios with complex, quantitative requirements and conflicting soft preferences, where traditional configuration becomes intricate. However, the experiments also reveal that synchronous LLM calls introduce noticeable latency, limiting immediate production applicability without architectural adjustments.", "conclusion": "The study concludes that semantic, intent-driven scheduling using LLMs is a viable approach to simplify workload orchestration in Kubernetes clusters. Natural-language soft affinity hints can be reliably translated into effective scheduling constraints, achieving scheduling quality that matches or surpasses traditional methods while greatly improving usability. At the same time, the work identifies key practical limitations\u2014most notably LLM latency\u2014indicating that asynchronous processing and further system optimizations are needed for production-ready deployments. Overall, the paper confirms the practicality and promise of LLM-powered semantic soft affinity for more accessible cluster scheduling."}}
{"id": "2601.09141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09141", "abs": "https://arxiv.org/abs/2601.09141", "authors": ["Miao Zhang", "Kelly Chen", "Md Mehrab Tanjim", "Rumi Chunara"], "title": "Identity-Robust Language Model Generation via Content Integrity Preservation", "comment": null, "summary": "Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.", "AI": {"tldr": "The paper shows that large language models treat users differently based on sociodemographic attributes even for objective questions, and proposes a training-free prompting framework that makes responses more identity-robust while preserving necessary semantic content.", "motivation": "Although prior work has largely focused on stereotypical or representational bias in LLMs, there is a less explored but crucial issue: core response quality (factual accuracy, utility, and safety) can degrade depending on user identity cues in the prompt, even when identity is irrelevant to the task. This undermines fairness and reliability of LLMs across different user groups. The authors are motivated to understand why this happens and to design a practical mitigation strategy that does not require retraining models.", "method": "The authors first empirically study how LLM responses vary with different sociodemographic identity cues in prompts, showing that quality differences stem from biased generation behavior rather than gaps in the model\u2019s underlying factual knowledge. They then introduce a lightweight, training-free framework for identity-robust generation that works at the prompting/processing level. The framework selectively strips or neutralizes non-essential identity-related information from the prompt while keeping semantically necessary attributes intact, so that the model\u2019s output content remains relevant but is less influenced by irrelevant identity cues.", "result": "Across four benchmarks and 18 different sociodemographic identities, the proposed framework substantially reduces identity-dependent bias in generation quality. Quantitatively, it achieves on average a 77% reduction in bias compared to standard (vanilla) prompting and a 45% reduction relative to existing prompt-based defense methods, while preserving the integrity and usefulness of the generated content.", "conclusion": "Identity cues in user prompts can cause significant, unfair variation in LLM response quality even for objective tasks, not because the model lacks knowledge but because of biased generation dynamics. A carefully designed, training-free prompting framework that neutralizes non-critical identity information can markedly improve identity robustness. This provides a practical path to fairer, more consistent LLM behavior across user groups and fills an important gap beyond prior work on stereotypical and representational bias."}}
{"id": "2601.09185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09185", "abs": "https://arxiv.org/abs/2601.09185", "authors": ["Zeqiang Wang", "Xinyue Wu", "Chenxi Li", "Zixi Chen", "Nishanth Sastry", "Jon Johnson", "Suparna De"], "title": "OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb", "comment": null, "summary": "Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $\u0394W = BA^\\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $\u0394W = B\u03a3A^\\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.", "AI": {"tldr": "The paper proposes OrthoGeoLoRA, a geometrically constrained variant of LoRA for parameter-efficient fine-tuning of language models, and shows it improves hierarchical concept retrieval performance under the same low-rank budget.", "motivation": "Fine-tuning large language models and text encoders for social science applications is computationally and energy intensive, which is a barrier for smaller institutions and non-profits. Existing PEFT methods like standard LoRA are cheaper but have geometric issues (gauge freedom, scale ambiguity, rank collapse) that may limit their effectiveness. There is also a need for realistic evaluation benchmarks in social science retrieval tasks.", "method": "The authors analyze the standard LoRA update \u0394W = BA^T and identify its geometric drawbacks. They then introduce OrthoGeoLoRA, which constrains the low-rank factors to be orthogonal and includes a diagonal scaling matrix \u03a3, yielding an SVD-like parameterization \u0394W = B\u03a3A^T on the Stiefel manifold. They implement a geometric reparameterization compatible with common optimizers (e.g., Adam) and existing fine-tuning pipelines. Additionally, they design a benchmark for hierarchical concept retrieval using the European Language Social Science Thesaurus (ELSST).", "result": "On experiments with a multilingual sentence encoder evaluated on the new ELSST-based benchmark, OrthoGeoLoRA achieves better ranking metrics than standard LoRA and several strong PEFT baselines, while using the same low-rank budget. This demonstrates improved parameter and compute efficiency in adapting foundation models.", "conclusion": "Imposing orthogonality and an SVD-like structure on LoRA\u2019s low-rank updates addresses key geometric issues and yields more effective parameter-efficient fine-tuning. OrthoGeoLoRA offers a practical, drop-in alternative to standard LoRA that integrates with existing training workflows and delivers better performance for resource-constrained settings, especially in social science information retrieval applications."}}
{"id": "2601.09353", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09353", "abs": "https://arxiv.org/abs/2601.09353", "authors": ["Ioannis Peridis", "Dimitrios Troullinos", "Georgios Chalkiadakis", "Pantelis Giankoulidis", "Ioannis Papamichail", "Markos Papageorgiou"], "title": "Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving", "comment": null, "summary": "Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.", "AI": {"tldr": "The paper proposes a Monte-Carlo Tree Search (MCTS) planner, enhanced with a neural network, for autonomous driving in lane-free traffic, analyzing safety, efficiency, and computation-quality trade-offs.", "motivation": "Traditional autonomous driving methods assume lane-based traffic, limiting lateral road usage and not fully exploiting the potential throughput of lane-free environments. Lane-free traffic is more complex and requires new planning methods that handle richer interactions among vehicles while ensuring safety and efficiency. There is also a need to combine search-based planning (like MCTS) with learned models (neural networks) to improve performance under computational constraints.", "method": "The authors formulate single-agent autonomous driving in lane-free traffic as a Markov Decision Process and apply Monte-Carlo Tree Search (MCTS) for planning. They integrate a pre-trained neural network to guide the MCTS selection phase, using the NN\u2019s predictions to bias the search toward promising actions and states. The method is evaluated via simulations using safety (collision rate) and efficiency (speed) metrics, examining the influence of isotropic state information, the impact of NN-guided MCTS, and the trade-off between computational budget and solution quality.", "result": "Experiments show that isotropic state information in a lane-free setting leads to a nudging behavior where vehicles adapt their policy in response to faster vehicles behind them. The NN-guided MCTS variant achieves faster performance improvements compared to plain MCTS, and the study characterizes how additional computation improves solution quality, identifying a trade-off between computational resources and planning performance.", "conclusion": "The study demonstrates that lane-free traffic requires specialized planning approaches and that combining MCTS with a guiding neural network can improve autonomous driving performance in such environments. Isotropic state representations induce useful interactive behaviors (like nudging), and NN-guided MCTS offers a practical balance between safety, efficiency, and computational cost, making it a promising direction for lane-free autonomous driving systems."}}
{"id": "2601.09195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09195", "abs": "https://arxiv.org/abs/2601.09195", "authors": ["Tao Liu", "Taiqiang Wu", "Runming Yang", "Shaoning Sun", "Junjie Wang", "Yujiu Yang"], "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection", "comment": null, "summary": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.", "AI": {"tldr": "The paper proposes ProFit, a supervised fine-tuning method that reduces overfitting to specific wordings by masking low-probability (surface-level) tokens, improving general and mathematical reasoning performance of LLMs.", "motivation": "Traditional supervised fine-tuning for LLM alignment uses a single reference answer, ignoring that many different responses can be equally valid. This can cause models to overfit to incidental phrasing instead of core semantics, and using multiple references to address this is too costly in data and computation. The authors want a cheaper way to avoid overfitting to non-essential expressions while preserving semantic alignment.", "method": "The authors first empirically analyze how using multiple reference answers affects SFT and show it mitigates overfitting but is expensive. They then study token probabilities during generation and establish that high-probability tokens encode the essential logical structure of answers, whereas low-probability tokens tend to be interchangeable surface expressions. Building on this, they design ProFit: during SFT, low-probability tokens in the reference answer are selectively masked, so the model is trained primarily on predicting high-probability, semantically important tokens and less on arbitrary wording. This makes the loss focus on core semantics instead of phrasing details.", "result": "Across multiple general reasoning and mathematical benchmarks, models fine-tuned with ProFit achieve higher performance than those trained with standard single-reference SFT. The gains are consistent, demonstrating better generalization and reduced overfitting to specific expressions, without needing expensive multi-reference datasets.", "conclusion": "Focusing SFT on high-probability, semantically central tokens is an effective way to reduce overfitting to surface forms when only single-reference answers are available. ProFit operationalizes this by masking low-probability tokens during training, offering a practical, compute- and data-efficient alternative to multi-reference SFT that improves reasoning ability of LLMs."}}
{"id": "2601.09382", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09382", "abs": "https://arxiv.org/abs/2601.09382", "authors": ["Qinglong Shi", "Donghai Wang", "Hantao Zhou", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments", "comment": "8 pages, 2 figures", "summary": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.", "AI": {"tldr": "The paper proposes proactive task-oriented LLM agents that can monitor for relevant external events based on user intent and then proactively re-engage the user when conditions are met. It introduces a data synthesis pipeline and a new dynamic-environment benchmark (ChronosBench), and shows a fine-tuned model achieving strong task completion rates on complex, shifting-intent tasks.", "motivation": "Existing LLM-based agents are mostly reactive: they only respond when prompted and within short, isolated sessions, making them poor at handling long-term user intents and dynamically changing environments. There is a need for agents that can maintain and act on persistent user goals, monitor changing external conditions, and proactively assist users when relevant events occur. Additionally, there is a lack of benchmarks and evaluation criteria for such proactive, long-term task-oriented interactions in dynamic environments.", "method": "1) Conceptual: They define a new interaction paradigm of proactive task-oriented agents with two formalized capabilities: (i) Intent-Conditioned Monitoring, where the agent autonomously derives trigger conditions from dialog history to watch for; and (ii) Event-Triggered Follow-up, where the agent initiates contact when monitored conditions are met due to environmental updates. 2) Data: They build a data synthesis pipeline to generate complex, multi-turn dialogs embedded in dynamic environments, including evolving user intents. 3) Benchmark: They design a new benchmark, ChronosBench, to evaluate task-oriented interactions in dynamic settings. 4) Modeling: They fine-tune an LLM with the synthetic data via supervised learning and compare it to leading closed- and open-source models on ChronosBench.", "result": "Experiments on the proposed ChronosBench benchmark show that existing leading closed- and open-source LLMs have significant weaknesses in long-term task-oriented interaction within dynamic environments. Their fine-tuned model, trained on the synthetic data pipeline, achieves an 85.19% task completion rate on complex tasks involving shifts in user intent, outperforming all baseline models evaluated.", "conclusion": "Proactive task-oriented LLM agents, endowed with intent-conditioned monitoring and event-triggered follow-up, are more effective than current reactive LLMs for long-term interaction in dynamic environments. The synthetic data pipeline and ChronosBench benchmark provide practical tools for training and evaluating such agents. The strong performance of their fine-tuned model validates the effectiveness of the proposed data-driven approach and suggests that proactivity and long-term intent tracking are key directions for improving LLM-based task-oriented systems."}}
{"id": "2601.09200", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09200", "abs": "https://arxiv.org/abs/2601.09200", "authors": ["Sung Jun Cheon", "Jaekyung Cho", "Seongho Choi", "Hyunjun Eun", "Seokhwan Jo", "Jaehyun Jun", "Minsoo Kang", "Jin Kim", "Jiwon Kim", "Minsang Kim", "Sungwan Kim", "Seungsik Kim", "Tae Yoon Kim", "Youngrang Kim", "Hyeongmun Lee", "Sangyeol Lee", "Sungeun Lee", "Youngsoon Lee", "Yujin Lee", "Seongmin Ok", "Chanyong Park", "Hyewoong Park", "Junyoung Park", "Hyunho Yang", "Subin Yi", "Soohyun Bae", "Dhammiko Arya", "Yongseok Choi", "Sangho Choi", "Dongyeon Cho", "Seungmo Cho", "Gyoungeun Han", "Yong-jin Han", "Seokyoung Hong", "Hyeon Hwang", "Wonbeom Jang", "Minjeong Ju", "Wonjin Jung", "Keummin Ka", "Sungil Kang", "Dongnam Kim", "Joonghoon Kim", "Jonghwi Kim", "SaeRom Kim", "Sangjin Kim", "Seongwon Kim", "Youngjin Kim", "Seojin Lee", "Sunwoo Lee", "Taehoon Lee", "Chanwoo Park", "Sohee Park", "Sooyeon Park", "Yohan Ra", "Sereimony Sek", "Seungyeon Seo", "Gun Song", "Sanghoon Woo", "Janghan Yoon", "Sungbin Yoon"], "title": "A.X K1 Technical Report", "comment": null, "summary": "We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.", "AI": {"tldr": "The paper presents A.X K1, a 519B-parameter Mixture-of-Experts language model with controllable reasoning modes and strong performance, especially for Korean.", "motivation": "To build a very large but inference-efficient language model that balances reasoning capability with deployment cost, particularly improving performance for Korean while following scaling laws under fixed compute budgets.", "method": "They design a 519B-parameter Mixture-of-Experts LM trained from scratch on ~10T curated tokens, use scaling laws to choose training configuration and vocabulary, and introduce a Think-Fusion training recipe that enables explicit user-controlled switching between thinking (reasoning) and non-thinking modes within a single model.", "result": "A.X K1 reaches competitive performance with top open-source LMs on general benchmarks and shows clear superiority on Korean-language benchmarks, while supporting explicit control over reasoning depth for efficiency and scalability.", "conclusion": "A.X K1 demonstrates that a large MoE model with a Think-Fusion training recipe can deliver competitive global performance, state-of-the-art Korean capability, and practical controllable reasoning for scalable real-world deployment under fixed compute constraints."}}
{"id": "2601.09215", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09215", "abs": "https://arxiv.org/abs/2601.09215", "authors": ["Feng Zhang", "Shijia Li", "Chunmao Zhang", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Jingwen Xu", "Han Liu"], "title": "UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning", "comment": null, "summary": "User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.", "AI": {"tldr": "They build a smarter, more strategic user simulator (UserLM-R1) that reasons, adapts to scenarios, and is harder for agents to exploit, outperforming prior simulators, especially in adversarial settings.", "motivation": "Existing user simulators are brittle: they use fixed, context-agnostic profiles that must be manually redesigned for each new scenario, which hurts scalability and generalization. They also fail to mimic human strategic thinking in negotiations, making them easy for trained agents to game. The authors want a simulator that generalizes across domains and behaves more like a real, strategically thinking user who can bargain and challenge the agent.", "method": "They propose UserLM-R1, a user language model equipped with explicit reasoning. First, they design richer user profiles that combine static role information with dynamic, scenario-specific goals so the same simulator can adapt to many tasks. Then they introduce a goal-driven decision-making policy: the model is guided to generate internal rationales (reasoning traces) before emitting user responses. Finally, they refine this reasoning process and strengthen strategic negotiation skills using supervised fine-tuning plus multi-reward reinforcement learning, where multiple reward signals shape behavior quality and robustness.", "result": "In experiments, UserLM-R1 consistently outperforms strong baseline user simulators. The gains are most pronounced on adversarial evaluation sets, where agents attempt to manipulate or exploit the simulator, indicating that UserLM-R1 better captures strategic, hard-to-game user behavior and yields higher-quality interactions for agent training and evaluation.", "conclusion": "A reasoning-augmented, goal-driven user language model with rich, adaptive profiles and multi-reward RL training can produce more realistic, strategically robust user simulations. This improves generalization across domains and resistance to agent manipulation, making UserLM-R1 a stronger interactive environment for post-training dialogue agents, especially in adversarial or negotiation-heavy scenarios."}}
{"id": "2601.09503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09503", "abs": "https://arxiv.org/abs/2601.09503", "authors": ["Siyuan Liu", "Hongbang Yuan", "Xinze Li", "Ziyue Zhu", "Yixin Cao", "Yu-Gang Jiang"], "title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "AI": {"tldr": "Proposes Task-to-Quiz (T2Q), a new way to evaluate LLM agents\u2019 true understanding of environments rather than just task success, revealing that current agents often solve tasks without grounded world models.", "motivation": "Existing LLM agent benchmarks mostly use trajectory-based metrics (e.g., task completion) that conflate successful execution with genuine understanding of the environment. This makes it unclear whether agents have learned a transferable, grounded model of the world or are just exploiting superficial cues. The authors aim to create an evaluation that isolates and measures environment understanding directly, across varied environments and difficulty levels.", "method": "Introduce Task-to-Quiz (T2Q), an automated, deterministic evaluation paradigm that converts interactive task environments into grounded question\u2013answer pairs, decoupling task performance from state understanding. They instantiate this as T2QBench, containing 30 environments and 1,967 QA pairs at multiple difficulty levels. Agents presumably interact with environments (or are given resulting states) and then answer structured questions that probe their internal model of the world state, independent of whether they completed the original task successfully.", "result": "Experiments on T2QBench show that high task success does not reliably indicate strong understanding of the environment. Agents can often complete tasks while lacking a robust, grounded state model. Moreover, existing memory mechanisms do not significantly improve this grounded understanding, highlighting a mismatch between current agent architectures and the requirements for generalizable world modeling.", "conclusion": "Task-to-Quiz exposes a gap between task performance and genuine environment understanding in LLM agents. The main limitations appear to be insufficient proactive exploration and coarse or inadequate state representations. T2Q and T2QBench provide a principled foundation and benchmark for developing future agent designs that learn more grounded, transferable models of their environments, with the goal of achieving more robust generalization across tasks and settings."}}
{"id": "2601.09241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09241", "abs": "https://arxiv.org/abs/2601.09241", "authors": ["Jing Ren", "Bowen Li", "Ziqi Xu", "Xinkun Zhang", "Haytham Fayek", "Xiaodong Li"], "title": "When to Trust: A Causality-Aware Calibration Framework for Accurate Knowledge Graph Retrieval-Augmented Generation", "comment": "Accepted by WWW 2026", "summary": "Knowledge Graph Retrieval-Augmented Generation (KG-RAG) extends the RAG paradigm by incorporating structured knowledge from knowledge graphs, enabling Large Language Models (LLMs) to perform more precise and explainable reasoning. While KG-RAG improves factual accuracy in complex tasks, existing KG-RAG models are often severely overconfident, producing high-confidence predictions even when retrieved sub-graphs are incomplete or unreliable, which raises concerns for deployment in high-stakes domains. To address this issue, we propose Ca2KG, a Causality-aware Calibration framework for KG-RAG. Ca2KG integrates counterfactual prompting, which exposes retrieval-dependent uncertainties in knowledge quality and reasoning reliability, with a panel-based re-scoring mechanism that stabilises predictions across interventions. Extensive experiments on two complex QA datasets demonstrate that Ca2KG consistently improves calibration while maintaining or even enhancing predictive accuracy.", "AI": {"tldr": "The paper introduces Ca2KG, a causality-aware calibration framework for knowledge-graph-based RAG to reduce overconfidence in LLM predictions.", "motivation": "Existing KG-RAG systems, while improving factual accuracy and explainability using knowledge graphs, tend to be overconfident even when the retrieved knowledge is incomplete or unreliable, which is dangerous in high-stakes domains.", "method": "Ca2KG combines counterfactual prompting to reveal retrieval-dependent uncertainties with a panel-based re-scoring mechanism that aggregates and stabilizes predictions across different interventions on the retrieved sub-graphs.", "result": "On two complex QA benchmarks, Ca2KG yields significantly better calibration metrics compared to standard KG-RAG baselines, while keeping predictive accuracy the same or slightly improving it.", "conclusion": "Incorporating causality-aware calibration via counterfactual interventions and re-scoring makes KG-RAG systems more reliable by aligning model confidence with actual correctness without sacrificing accuracy."}}
{"id": "2601.09536", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09536", "abs": "https://arxiv.org/abs/2601.09536", "authors": ["Dongjie Cheng", "Yongqi Li", "Zhixin Ma", "Hongru Cai", "Yupeng Hu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.", "AI": {"tldr": "The paper proposes a unified way for multimodal large language models to reason by generating intermediate images, and shows it works well even without manual multimodal annotations.", "motivation": "Existing MLLMs either reason only in text or, when they do use images, follow task-specific reasoning patterns. This makes them hard to generalize across many different multimodal tasks that require varied skills (e.g., zooming, pointing, marking regions). The authors want a single, general paradigm that can handle diverse multimodal reasoning behaviors in a unified way.", "method": "They introduce the concept of unified generative multimodal reasoning, where the model generates intermediate images as part of its reasoning trajectory. They implement this with Omni-R1, a two-stage framework combining supervised fine-tuning (SFT) and reinforcement learning (RL). The framework uses a perception alignment loss and a perception reward to encourage accurate, functional image generation during reasoning. They also propose Omni-R1-Zero, which removes the need for multimodal (image-based) annotations by bootstrapping stepwise visual visualizations from purely text-only reasoning traces.", "result": "Experiments show that Omni-R1 can perform unified generative reasoning across many different types of multimodal tasks rather than being limited to a single pattern. Surprisingly, Omni-R1-Zero, trained without explicit multimodal annotations, achieves performance comparable to or even better than Omni-R1 on average across evaluated tasks.", "conclusion": "Generating intermediate images as part of the reasoning process provides a powerful, unified way to endow MLLMs with diverse multimodal reasoning skills. Moreover, such generative multimodal reasoning can be effectively learned even from text-only supervision, pointing to an efficient and scalable direction for future multimodal model training."}}
{"id": "2601.09246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09246", "abs": "https://arxiv.org/abs/2601.09246", "authors": ["Xiangqian Wang", "Yifan Jia", "Yang Xiang", "Yumin Zhang", "Yanbin Wang", "Ke Liu"], "title": "TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding", "comment": null, "summary": "Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.", "AI": {"tldr": "TeachPro is a multi-label ML framework that analyzes students\u2019 open-ended teaching evaluations along five instructional dimensions using dimension-anchored evidence encoding and dual syntactic\u2013semantic graph representations of comments, achieving more fine-grained and robust diagnostic feedback than prior sentiment-based methods.", "motivation": "Standardized student evaluations are often unreliable and coarse, with limited response options and distortion. Current ML approaches to open-ended comments usually collapse nuanced feedback into simple positive/negative sentiment, failing to capture specific instructional issues (e.g., clarity, feedback timeliness, demeanor). There is a need for an automated system that can extract detailed, actionable diagnostic insights along multiple pedagogical dimensions from qualitative comments.", "method": "The authors propose TeachPro, a multi-label learning framework centered on five dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. They introduce a Dimension-Anchored Evidence Encoder that (1) uses a pre-trained text encoder to embed annotated qualitative feedback, (2) employs a prompt module to represent each teaching dimension as a learnable semantic anchor, and (3) applies cross-attention to align evidence with these dimensions. For student comments, they design a Cross-View Graph Synergy Network with two branches: a Syntactic Branch that uses parse trees to capture grammatical dependencies and a Semantic Branch that uses BERT-based similarity graphs to model latent conceptual relations. A BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer pushes the two views to learn complementary (disentangled) embeddings. A final cross-attention mechanism connects the dimension-anchored evidence with the multi-view comment representations. They also build a new benchmark dataset with expert annotations and multi-label scores.", "result": "Experiments on the new benchmark show that TeachPro outperforms existing methods in extracting multi-dimensional teaching evaluations from comments, achieving better diagnostic granularity and robustness under various evaluation scenarios. The model can reliably map qualitative feedback to multiple specific teaching dimensions instead of only predicting overall sentiment.", "conclusion": "TeachPro demonstrates that combining dimension-anchored evidence encoding with syntactic\u2013semantic graph modeling of comments enables fine-grained, multi-label assessment of teaching quality from open-ended student feedback. This approach overcomes the limitations of standard evaluations and sentiment-only models, offering richer, more actionable insights for instructional improvement, supported by a newly released annotated benchmark dataset."}}
{"id": "2601.09635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09635", "abs": "https://arxiv.org/abs/2601.09635", "authors": ["Kuo Liang", "Yuhang Lu", "Jianming Mao", "Shuyi Sun", "Chunwei Yang", "Congcong Zeng", "Xiao Jin", "Hanzhang Qin", "Ruihao Zhu", "Chung-Piaw Teo"], "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach", "comment": "Updated version of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5329027", "summary": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.", "AI": {"tldr": "Introduces LEAN-LLM-OPT, an agentic LLM-based framework that automatically formulates large-scale optimization models from problem descriptions and data, using a dynamic workflow of specialized LLM agents and tools, and validates it on new benchmarks and a real airline revenue management case.", "motivation": "Building large-scale optimization models is essential for modern business decisions but is currently manual, slow, and requires significant expert effort. Existing LLM-based approaches either lack scalability, struggle with complex data handling, or do not adequately structure the modeling process. The authors want to reduce the human effort in turning natural language problem descriptions and datasets into formal optimization formulations, while maintaining or improving modeling quality and scalability, and to provide benchmarks to systematically evaluate such systems.", "method": "They design LEAN-LLM-OPT, a lightweight agentic framework where multiple LLM agents collaborate. Two upstream agents first read the problem description and related datasets and then dynamically construct a step-by-step workflow for how to formulate an optimization model, guided by similar past problems and common modeling practices. This workflow decomposes the task into structured sub-tasks (e.g., identify decision variables, parameters, constraints, objective) and delegates mechanical data-handling and preprocessing to external tools. A downstream LLM agent then executes the workflow to generate the final optimization formulation, focusing on difficult modeling decisions rather than planning or data wrangling. The framework is instantiated with GPT-4.1 and an open-source 20B model (gpt-oss-20B) and evaluated via simulations and real-world use cases. They also construct two benchmarks, Large-Scale-OR and Air-NRM, for systematic evaluation of large-scale optimization auto-formulation.", "result": "In simulation experiments, LEAN-LLM-OPT with GPT-4.1 and gpt-oss-20B performs strongly on large-scale optimization modeling tasks and is competitive with, or comparable to, state-of-the-art methods. In a real-world Singapore Airlines choice-based revenue management application, the framework shows practical value, achieving leading performance across multiple scenarios. Additionally, the authors release two new benchmarks, Large-Scale-OR and Air-NRM, plus accompanying code and data, demonstrating that their approach generalizes across tasks and is evaluable in a standardized way.", "conclusion": "LEAN-LLM-OPT demonstrates that a lightweight, agentic, workflow-based decomposition of optimization modeling is an effective way to harness LLMs for automatic large-scale optimization formulation. By separating planning, data handling, and higher-level modeling decisions across specialized agents and tools, the system reduces modeling burden while remaining competitive with advanced baselines in both simulations and a real airline revenue management case. The introduction of the Large-Scale-OR and Air-NRM benchmarks, along with open-source code and data, provides a foundation for further research and comparison in LLM-assisted optimization modeling."}}
{"id": "2601.09250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09250", "abs": "https://arxiv.org/abs/2601.09250", "authors": ["Jing Ren", "Bowen Li", "Ziqi Xu", "Renqiang Luo", "Shuo Yu", "Xin Ye", "Haytham Fayek", "Xiaodong Li", "Feng Xia"], "title": "When to Invoke: Refining LLM Fairness with Toxicity Assessment", "comment": "Accepted by Findings of WWW 2026", "summary": "Large Language Models (LLMs) are increasingly used for toxicity assessment in online moderation systems, where fairness across demographic groups is essential for equitable treatment. However, LLMs often produce inconsistent toxicity judgements for subtle expressions, particularly those involving implicit hate speech, revealing underlying biases that are difficult to correct through standard training. This raises a key question that existing approaches often overlook: when should corrective mechanisms be invoked to ensure fair and reliable assessments? To address this, we propose FairToT, an inference-time framework that enhances LLM fairness through prompt-guided toxicity assessment. FairToT identifies cases where demographic-related variation is likely to occur and determines when additional assessment should be applied. In addition, we introduce two interpretable fairness indicators that detect such cases and improve inference consistency without modifying model parameters. Experiments on benchmark datasets show that FairToT reduces group-level disparities while maintaining stable and reliable toxicity predictions, demonstrating that inference-time refinement offers an effective and practical approach for fairness improvement in LLM-based toxicity assessment systems. The source code can be found at https://aisuko.github.io/fair-tot/.", "AI": {"tldr": "The paper presents FairToT, an inference-time framework that uses prompt-guided toxicity assessment and fairness indicators to improve the consistency and fairness of LLM-based toxicity judgments across demographic groups without retraining the model.", "motivation": "LLMs are widely used for toxicity detection in content moderation, but they often show inconsistent and biased judgments, especially for subtle or implicit hate speech and across different demographic groups. Existing methods mostly focus on model training or post-hoc adjustments without explicitly deciding when additional corrective mechanisms are necessary. There is a need for a practical, deployable method that can selectively refine LLM outputs at inference time to reduce group disparities while maintaining prediction reliability.", "method": "The authors propose FairToT, an inference-time framework that wraps around existing LLMs. It uses prompt-guided toxicity assessment and introduces two interpretable fairness indicators that flag instances where demographic-related variation or inconsistency is likely. When these indicators are triggered, FairToT invokes additional assessment procedures\u2014via tailored prompts or multiple evaluations\u2014to refine the toxicity judgement. Crucially, this is done without modifying model parameters, making it compatible with black-box LLMs.", "result": "On benchmark toxicity datasets, applying FairToT reduces disparities in toxicity predictions across demographic groups and improves the consistency of assessments in cases involving subtle or implicit hate speech. The framework maintains overall predictive performance and stability of toxicity scores despite these fairness improvements.", "conclusion": "Inference-time refinement via FairToT is an effective and practical way to improve fairness and consistency of LLM-based toxicity assessment systems without retraining. By selectively invoking corrective assessment based on fairness indicators, the framework achieves more equitable group-level outcomes while preserving reliable toxicity predictions, making it suitable for real-world moderation pipelines."}}
{"id": "2601.09636", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09636", "abs": "https://arxiv.org/abs/2601.09636", "authors": ["Yibo Lyu", "Gongwei Chen", "Rui Shao", "Weili Guan", "Liqiang Nie"], "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records", "comment": null, "summary": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.", "AI": {"tldr": "The paper proposes PersonalAlign, a new task and benchmark (AndroidIntent) for aligning GUI agents with users\u2019 implicit, long\u2011term preferences and routines, and introduces HIM-Agent, a hierarchical memory-based agent that significantly improves execution and proactive assistance performance.", "motivation": "GUI agents perform well when users give explicit, fully specified instructions, but real users often provide vague commands and have implicit preferences and recurring routines that are not directly stated. Existing benchmarks and agents don\u2019t adequately test or exploit long-term, user-specific records as persistent context. The authors aim to bridge this gap so agents can understand omitted preferences, infer latent routines, and proactively assist users based on their personal histories.", "method": "1) Define a new task: Hierarchical Implicit Intent Alignment for Personalized GUI Agents (PersonalAlign), where agents must (a) resolve vague instructions by inferring omitted user preferences from long-term records, and (b) provide proactive suggestions by detecting latent routines from user state and history. 2) Build AndroidIntent, a benchmark of ~20k long-term user records, with 775 annotated user-specific preferences and 215 annotated routines, used to evaluate both disambiguation of vague instructions and proactive assistance. 3) Design the Hierarchical Intent Memory Agent (HIM-Agent), which (a) maintains a continually updating personal memory, and (b) hierarchically organizes user preferences and routines to better reason about user intent. 4) Evaluate multiple GUI agents, including GPT-5, Qwen3-VL, and UI-TARS, on AndroidIntent, with and without HIM-Agent\u2019s personalization mechanism.", "result": "The authors obtain a curated dataset (AndroidIntent) capturing rich long-term user behavior with structured annotations of preferences and routines. Experiments show that incorporating the proposed HIM-Agent architecture leads to substantial gains on AndroidIntent relative to strong baselines like GPT-5, Qwen3-VL, and UI-TARS. Specifically, HIM-Agent improves task execution performance by 15.7% and proactive assistance performance by 7.3%, indicating better handling of vague instructions and better anticipation of user needs.", "conclusion": "Personalizing GUI agents through hierarchical implicit intent alignment and long-term personal memory is both feasible and beneficial. The AndroidIntent benchmark offers a standardized way to measure these capabilities. The HIM-Agent\u2019s hierarchical memory structure and continuous updating of user-specific preferences and routines yield significant performance improvements in both executing underspecified commands and providing proactive suggestions. This suggests that future GUI assistants should incorporate long-term, hierarchically organized user modeling to effectively align with real-world, implicit user intents."}}
{"id": "2601.09270", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09270", "abs": "https://arxiv.org/abs/2601.09270", "authors": ["Yexing Du", "Kaiyuan Liu", "Bihe Zhang", "Youcheng Pan", "Bo Yang", "Liangyu Huo", "Xiyuan Zhang", "Jian Xie", "Daojing He", "Yang Xiang", "Ming Liu", "Bin Qin"], "title": "MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus", "comment": null, "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA", "AI": {"tldr": "The paper introduces MCGA, a multi-task audio corpus in Classical Chinese literature, and shows that current MLLMs struggle on its tasks.", "motivation": "Multimodal large language models are advancing quickly and are being applied to Chinese Classical Studies, but the audio modality for this domain is underexplored and lacks a standardized, diverse benchmark for evaluating and improving models.", "method": "The authors construct MCGA, a Multi-task Classical Chinese Literary Genre Audio Corpus that covers diverse literary genres and defines six tasks (ASR, S2TT, SEC, SQA, SU, SR). They then evaluate ten existing MLLMs on this benchmark and design new evaluation metrics for Speech Emotion Captioning and for measuring consistency between a model's speech and text capabilities.", "result": "Experiments on the MCGA test set show that ten state-of-the-art MLLMs still perform poorly and face substantial challenges on the defined audio tasks. The newly proposed metrics reveal limitations in emotional understanding and modality consistency.", "conclusion": "MCGA exposes the current weaknesses of MLLMs in handling Classical Chinese audio across several tasks, and by releasing the corpus, code, and metrics, the authors aim to provide a foundation for future research to build more robust, multidimensional audio capabilities in Chinese Classical Studies."}}
{"id": "2601.09667", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09667", "abs": "https://arxiv.org/abs/2601.09667", "authors": ["Zhiyuan Hu", "Yunhai Hu", "Juncheng Liu", "Shuyue Stella Li", "Yucheng Wang", "Zhen Xu", "See-Kiong Ng", "Anh Tuan Luu", "Xinxing Xu", "Bryan Hooi", "Cynthia Breazeal", "Hae Won Park"], "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "comment": "Work in Progress", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "AI": {"tldr": "The paper proposes Multi-Agent Test-Time Reinforcement Learning (MATTRL), which improves LLM-based multi-agent reasoning at inference time by injecting structured textual experiences, avoiding expensive and unstable MARL training.", "motivation": "Traditional multi-agent reinforcement learning is powerful but difficult to train due to co-adaptation, non-stationarity, and sparse/high-variance rewards. Meanwhile, LLM-based multi-agent systems work well as collaborators but lack a principled way to improve through experience without retraining. The authors aim to get the robustness and adaptivity of MARL-style learning while avoiding its training cost and instability, especially under distribution shift.", "method": "MATTRL organizes a group of LLM specialists into a multi-agent, multi-turn discussion team. During inference, it (1) forms a multi-expert team, (2) conducts multi-turn deliberation, (3) performs credit assignment at the turn level to build an experience pool of helpful textual rationales or actions, (4) retrieves relevant experiences at test time, and (5) reinjects these experiences back into the dialogue so that the agents can refine their reasoning and reach a consensus decision. Different credit-assignment schemes for constructing and using the experience pool are designed and compared via ablations.", "result": "On challenging benchmarks in medicine, mathematics, and education, MATTRL improves task accuracy by 3.67 percentage points over a strong multi-agent baseline and by 8.67 percentage points over comparable single-agent LLM baselines. Ablation studies show how alternative credit-assignment strategies affect performance and stability, demonstrating that structured test-time experiences substantially enhance multi-agent reasoning quality.", "conclusion": "MATTRL provides a way to get the benefits of reinforcement-style learning\u2014adaptation from experience and robustness to distribution shift\u2014without expensive MARL training or model fine-tuning. By injecting and reusing structured textual experiences within multi-agent LLM deliberation at inference time, it yields a more stable, effective, and efficient approach to multi-agent reasoning that generalizes better under distribution shift."}}
{"id": "2601.09280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09280", "abs": "https://arxiv.org/abs/2601.09280", "authors": ["Chaerin Lee", "Sohee Park", "Hyunsik Na", "Daseon Choi"], "title": "ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering", "comment": "18 pages, 2 figures. Preprint", "summary": "Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.", "AI": {"tldr": "The paper proposes ReGraM, a region-first KG reasoning framework that builds a query-aligned subgraph and performs constrained, stepwise reasoning to improve factual accuracy and reduce hallucinations in medical QA.", "motivation": "Existing LLM+KG approaches for medical QA often traverse large parts of the knowledge graph or rely on broad retrieval, introducing noise and causing unstable multi-hop reasoning. The key problem is not accessing more knowledge but selecting and reasoning over the right subset of evidence per query, especially in domain-specific medical contexts where not all relations are equally relevant.", "method": "ReGraM constructs a localized, query-aligned subgraph (a \u201cregion\u201d) of the biomedical KG and then performs hop-wise, stepwise reasoning restricted to this region under multiple evidence-aware modes. This region-first design avoids assuming all relations are equally useful and focuses inference on the most relevant KG portion for the given question. The framework is evaluated against a strong baseline (KGARevion) on multiple medical QA benchmarks, with ablation and qualitative analyses to study the contributions of region construction and hop-wise reasoning alignment.", "result": "Across seven medical QA benchmarks, ReGraM consistently outperforms KGARevion, with an 8.04% absolute accuracy gain on multiple-choice questions (MCQ), a 4.50% gain on short-answer questions (SAQ), and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses indicate that the alignment between region construction and hop-wise reasoning is the main source of performance improvement.", "conclusion": "Region-first KG reasoning\u2014constructing query-aligned subgraphs and performing localized, stepwise inference\u2014is an effective paradigm for medical QA. By focusing on the most relevant KG region rather than exhaustive traversal, ReGraM improves factual accuracy, stability of multi-hop reasoning, and reduces hallucinations, highlighting the importance of evidence selection and structured reasoning in LLM+KG systems for medicine."}}
{"id": "2601.09680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09680", "abs": "https://arxiv.org/abs/2601.09680", "authors": ["Sara AlMahri", "Liming Xu", "Alexandra Brintrup"], "title": "Automating Supply Chain Disruption Monitoring via an Agentic AI Approach", "comment": null, "summary": "Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \\rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \\$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.", "AI": {"tldr": "Agentic AI framework using multiple LLM-based agents to automatically detect and respond to deep-tier supply chain disruptions, achieving very high task accuracy and dramatic reductions in response time and cost versus human analyst workflows.", "motivation": "Global supply chains face frequent disruptions from geopolitical, economic, and natural events, and most firms lack visibility beyond direct (Tier-1) suppliers, making deep-tier vulnerabilities hard to detect before they cascade downstream. Existing processes are slow, manual, and reactive. There is a need for an automated, scalable, and proactive approach that can continuously monitor unstructured information sources, map risks to multi-tier supplier networks, and quickly recommend mitigations.", "method": "The authors design a minimally supervised agentic AI architecture composed of seven specialized agents built on large language models and deterministic tools. These agents collaborate to: (1) continuously scan unstructured news for disruption signals; (2) classify and interpret disruption events; (3) map affected entities onto multi-tier supply networks; (4) analyze exposure using network structure and disruption characteristics; and (5) generate mitigation recommendations such as alternative sourcing options. They evaluate the framework on 30 synthetic scenarios designed around three automotive OEMs and five disruption types, measuring task-level performance, end-to-end latency, and cost. They also conduct a real-world case study on the 2022 Russia\u2013Ukraine conflict.", "result": "Across the 30 synthetic scenarios, the framework attains very high accuracy on core tasks, with F1 scores ranging from 0.962 to 0.991. End-to-end analyses are completed in an average of 3.83 minutes at a cost of $0.0836 per disruption. Compared with typical industry practice\u2014multi-day, analyst-led assessments\u2014the system reduces response times by over three orders of magnitude while remaining highly cost-effective. The Russia\u2013Ukraine case study shows that the approach can be applied to real geopolitical disruptions and practical supply networks.", "conclusion": "A multi-agent, LLM-powered framework can autonomously monitor and manage disruptions across deep-tier supply chains, shifting firms from reactive recovery to proactive resilience. The system demonstrates that minimally supervised agentic AI is capable of high-accuracy, low-latency, and low-cost disruption analysis, outperforming traditional analyst-driven methods by large margins in speed. This establishes a foundational approach for building more resilient, proactive, and autonomous supply chain risk management systems that operate over extended, multi-tier supplier networks."}}
{"id": "2601.09313", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09313", "abs": "https://arxiv.org/abs/2601.09313", "authors": ["Jonathan Drechsel", "Erisa Bytyqi", "Steffen Herbold"], "title": "Understanding or Memorizing? A Case Study of German Definite Articles in Language Models", "comment": null, "summary": "Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.", "AI": {"tldr": "The paper investigates whether language models apply abstract rules or rely on memorization for German definite article agreement, using a gradient-based interpretability method to analyze how article-related information is encoded in model neurons.", "motivation": "Although language models show high accuracy on grammatical agreement tasks, it is uncertain if their performance stems from genuine rule-like generalization or rote memorization of frequent patterns. German definite singular articles are an ideal testbed because their surface forms are tightly determined by gender and case, offering a controlled setting to disentangle rule-based encoding from memorized associations.", "method": "The authors introduce or apply GRADIEND, a gradient-based interpretability technique that learns parameter update directions specifically tailored to transitions between definite article forms corresponding to particular combinations of gender and case. They then analyze how these learned update directions influence model parameters and neurons across different gender-case configurations.", "result": "Parameter update directions optimized for one gender-case article transition substantially impact other, ostensibly unrelated gender-case configurations. Furthermore, there is a large overlap in the sets of neurons most strongly affected across different gender-case settings, showing that the underlying representations are not cleanly separated by grammatical rule dimensions such as gender and case.", "conclusion": "The observed cross-interference and neuron overlap indicate that language models do not encode German definite article agreement in a purely rule-based, compositional way. Instead, they appear to rely at least partially on memorized or entangled associations between forms, challenging the view that good performance on agreement tasks directly reflects abstract grammatical rule learning."}}
{"id": "2601.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09342", "abs": "https://arxiv.org/abs/2601.09342", "authors": ["Ewelina Gajewska", "Katarzyna Budzynska", "Jaros\u0142aw A Chudziak"], "title": "Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.", "AI": {"tldr": "They build a multi-agent, context-aware system to detect implicitly hateful speech more accurately and fairly than existing prompting-based methods.", "motivation": "Implicitly hateful speech is difficult to detect because it often relies on socio-cultural context and group-specific knowledge that general-purpose models and simple prompts lack. Existing moderation approaches can be inaccurate or unfair across demographic groups, and current evaluations often ignore the balance between catching harmful content and avoiding false accusations. The authors want a moderation framework that is identity-aware, context-sensitive, and evaluated with fairness in mind.", "method": "They design a multi-agent architecture with a central Moderator Agent and multiple Community Agents, each representing a specific demographic group. These Community Agents are dynamically constructed and draw on socio-cultural information from publicly available knowledge sources to interpret potentially hateful content in context. The system performs identity-aware moderation decisions and is evaluated on the challenging ToxiGen dataset. For evaluation, they prioritize balanced accuracy to capture the trade-off between true positive and true negative rates and to better measure fairness across groups. They compare their system against strong prompting baselines (zero-shot, few-shot, chain-of-thought) and other alternative approaches.", "result": "On the ToxiGen dataset, their contextualised multi-agent system outperforms state-of-the-art prompting methods and other baselines in detecting implicitly hateful speech. The approach yields higher overall accuracy and, importantly, better balanced accuracy, indicating improved fairness in how it treats positive (hateful) and negative (non-hateful) instances. The improvements hold consistently across all targeted demographic groups.", "conclusion": "A community-driven, multi-agent moderation framework that explicitly encodes socio-cultural and group-specific context can substantially improve the detection of implicitly hateful speech while enhancing fairness across demographic groups. Using balanced accuracy as a key metric provides a more rigorous and equitable evaluation of moderation systems. This suggests that identity-aware, consultative architectures are a promising direction for building fairer and more effective content moderation tools."}}
{"id": "2601.09365", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09365", "abs": "https://arxiv.org/abs/2601.09365", "authors": ["Biswesh Mohapatra", "Th\u00e9o Charlot", "Giovanni Duca", "Mayank Palan", "Laurent Romary", "Justine Cassell"], "title": "Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs", "comment": null, "summary": "Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.", "AI": {"tldr": "The paper studies how dialog systems can explicitly represent and reuse common ground in situated dialogue, beyond surface-level behaviors like acknowledgments or clarifications.", "motivation": "Although LLMs can appear grounded by asking for clarification or acknowledging information, it is unclear whether they actually build and store an explicit representation of common ground that can be reused later in conversation. There is a need to evaluate and improve how models establish, represent, and exploit shared references to entities and their relations in situated dialogues.", "method": "The authors design an evaluation focused on relational references to entities in shared situational dialogue contexts. They compare several alternative schemes for explicitly representing common ground during conversation. They then propose and test new approaches that aim to improve both the formation of this explicit common-ground representation and its use for later referencing in the dialogue.", "result": "The experiments show differential effectiveness of various representation methods for common ground and demonstrate that the proposed approaches enhance the model\u2019s ability to correctly establish and later exploit shared references to entities and their relations in situated dialogues.", "conclusion": "Explicit modeling and storage of common ground is crucial for dialog systems to truly understand and reuse shared conversational content, rather than merely simulating grounded behavior. Carefully designed representations and mechanisms can measurably improve both the establishment and later use of common ground in situated dialogue tasks."}}
{"id": "2601.09367", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09367", "abs": "https://arxiv.org/abs/2601.09367", "authors": ["Aidana Aidynkyzy", "O\u011fuz Dikenelli", "Oylum Alatl\u0131", "\u015eebnem Bora"], "title": "Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish", "comment": null, "summary": "The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.", "AI": {"tldr": "This paper creates and uses a new English\u2013Turkish parallel clinical relation extraction dataset to compare LLM prompting (with advanced in\u2011context retrieval) against fine\u2011tuned models, finding that retrieval\u2011enhanced prompting performs best, especially in English, and can help close resource gaps in clinical NLP.", "motivation": "There are very few annotated clinical information extraction datasets in non\u2011English languages, which makes it difficult to evaluate or deploy LLM-based methods beyond English. Clinical relation extraction, in particular, lacks bilingual benchmarks that would allow systematic comparison of methods and understanding of cross-lingual performance gaps. The authors aim to fill this gap for English and Turkish, and to test whether modern prompting and retrieval strategies can reduce reliance on supervised fine\u2011tuning in low-resource clinical settings.", "method": "1) Construct a parallel English\u2013Turkish clinical relation extraction dataset by translating and carefully curating instances from the 2010 i2b2/VA relation classification corpus. 2) Evaluate a range of LLMs on this task under different prompting regimes: standard prompts, multiple variants of in\u2011context learning (ICL), and Chain\u2011of\u2011Thought (CoT) reasoning prompts. 3) Compare these prompting-based approaches to strong fine\u2011tuned baselines, including the PURE model. 4) Introduce Relation-Aware Retrieval (RAR), a contrastive-learning-based method for selecting in\u2011context examples that capture both sentence-level and relation-level semantics. 5) Test RAR across LLMs, and further combine it with a structured reasoning prompt using DeepSeek\u2011V3.", "result": "Prompting-based LLM approaches outperform traditional fine\u2011tuned models like PURE on the clinical relation extraction task. Across all evaluated LLMs and prompting strategies, performance in English is higher than in Turkish, reflecting a cross-lingual performance gap. Among in\u2011context learning methods, the proposed Relation-Aware Retrieval (RAR) strategy yields the best results: with Gemini 1.5 Flash, it achieves micro\u2011F1 scores of 0.906 in English and 0.888 in Turkish. When RAR is combined with a structured reasoning prompt using DeepSeek\u2011V3, English performance further increases to a micro\u2011F1 of 0.918.", "conclusion": "High-quality selection of in\u2011context demonstrations is crucial for strong LLM performance on clinical relation extraction. The proposed Relation-Aware Retrieval method effectively leverages contrastive learning to retrieve demonstrations that capture nuanced sentence and relation semantics, enabling prompting-based LLMs to surpass fine\u2011tuned baselines. While English still outperforms Turkish, advanced retrieval and prompting techniques substantially improve performance in the lower-resource language, suggesting a promising path to mitigating resource disparities in clinical NLP via bilingual datasets and retrieval-augmented prompting, rather than relying solely on large supervised training sets."}}
{"id": "2601.09373", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09373", "abs": "https://arxiv.org/abs/2601.09373", "authors": ["Bolei Ma", "Yusuke Miyao"], "title": "The Imperfective Paradox in Large Language Models", "comment": null, "summary": "Do Large Language Models (LLMs) genuinely grasp the compositional semantics of events, or do they rely on surface-level probabilistic heuristics? We investigate the Imperfective Paradox, a logical phenomenon where the past progressive aspect entails event realization for activities (e.g., running $\\to$ ran) but not for accomplishments (e.g., building $\\nrightarrow$ built). We introduce ImperfectiveNLI, a diagnostic dataset designed to probe this distinction across diverse semantic classes. Evaluating state-of-the-art open-weight models, we uncover a pervasive Teleological Bias: models systematically hallucinate completion for goal-oriented events, often overriding explicit textual negation. Representational analyses show that while internal embeddings often distinguish process from result, inference decisions are dominated by strong priors about goal attainment. We further find that prompting-based interventions reduce hallucinated completions but also increase incorrect rejections of valid entailments. Our findings suggest that current LLMs lack structural aspectual awareness, operating as predictive narrative engines rather than faithful logical reasoners.", "AI": {"tldr": "The paper tests whether LLMs truly understand event aspect (especially the imperfective paradox) and finds that they systematically hallucinate that goal-oriented events are completed, revealing a lack of genuine compositional semantic understanding.", "motivation": "There is an open question whether LLMs perform genuine logical-semantic reasoning about events or just rely on surface statistical patterns. The imperfective paradox in aspectual semantics provides a precise, well-studied phenomenon to probe this: in natural language, the past progressive sometimes but not always entails event completion, depending on event type (activity vs accomplishment). Understanding this is important both for linguistic theory and for evaluating LLMs as semantic reasoners.", "method": "The authors construct ImperfectiveNLI, a diagnostic NLI-style dataset targeting the imperfective paradox across different event types and semantic classes. They evaluate multiple state-of-the-art open-weight LLMs on these examples, analyze their entailment judgments, and inspect internal representations/embeddings to see whether models encode distinctions between processes and results. They also test prompting-based interventions to see whether explicit instructions can mitigate systematic errors.", "result": "Across models, they find a strong Teleological Bias: models tend to infer that goal-directed or accomplishment events described in the past progressive were completed, even when texts explicitly deny completion. Internal representations do differentiate processes from results, but the models\u2019 actual inference outputs are dominated by priors that goal-oriented actions reach their goals. Prompting can reduce hallucinated completions but also causes the models to under-predict genuine entailments, trading one kind of error for another.", "conclusion": "Current LLMs lack robust, structural understanding of aspect and the imperfective paradox, behaving more like narrative predictors that assume teleological completion of goals than like logical reasoners that respect compositional aspectual semantics and negation. This raises doubts about their suitability for tasks requiring fine-grained, formal semantic reasoning about events, and shows that simple prompting is insufficient to fully correct these biases."}}
{"id": "2601.09398", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09398", "abs": "https://arxiv.org/abs/2601.09398", "authors": ["Songyao Jin", "Kun Zhou", "Wenqi Li", "Peng Wang", "Biwei Huang"], "title": "Ability Transfer and Recovery via Modularized Parameters Localization", "comment": null, "summary": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.", "AI": {"tldr": "The paper studies how specific abilities are localized in LLM parameters and proposes a method (ACT) to transfer or restore such abilities by copying only a small set of activation-identified channels between models.", "motivation": "Continuous pre-training or fine-tuning for specialization often causes catastrophic forgetting or degradation of other abilities in LLMs. There is a need to understand where abilities reside in model parameters and to find a way to add, restore, or merge capabilities across models without destructive interference.", "method": "The authors analyze module activations of closely related LLMs under different domain and language inputs, discovering that ability-related activations concentrate in a small subset of channels. Based on this, they develop ACT (Activation-Guided Channel-wise Ability Transfer), which (1) identifies ability-relevant channels via activation differences between specialized and base models, (2) selectively copies parameters for those channels, and (3) applies lightweight fine-tuning to ensure compatibility and integration of the transferred abilities.", "result": "Empirical results on multilingual math and scientific reasoning demonstrate that ACT can successfully recover previously forgotten abilities while keeping existing capabilities intact. It also enables merging several specialized models into one that combines their strengths with minimal negative interference.", "conclusion": "Abilities in LLMs are largely localized and disentangled at the channel level, allowing targeted parameter transfer. The proposed ACT method exploits this structure to restore and merge specialized abilities with limited side effects, suggesting a practical approach for modular capability management in large language models."}}
{"id": "2601.09402", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09402", "abs": "https://arxiv.org/abs/2601.09402", "authors": ["Xinze Li", "Zhenghao Liu", "Haidong Xin", "Yukun Yan", "Shuo Wang", "Zheni Zeng", "Sen Mei", "Ge Yu", "Maosong Sun"], "title": "Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.", "AI": {"tldr": "PAGER is a structured, page-driven RAG framework that guides LLMs to build organized, information-dense knowledge pages per question, leading to better answer quality than standard RAG.", "motivation": "Existing RAG systems, even with iterative retrieval and refinement, accumulate knowledge in an unstructured or loosely organized way. This lack of coherent organization makes it hard to form comprehensive, cohesive knowledge representations, causing weaker reasoning, conflict handling, and knowledge utilization.", "method": "PAGER uses an LLM to first create a structured cognitive outline for the input question, decomposed into multiple slots, each corresponding to a distinct knowledge aspect. For each slot, PAGER iteratively retrieves and refines relevant documents, filling that slot with curated content. The filled slots are assembled into a coherent page\u2014a structured knowledge representation\u2014which is then used as the context for answer generation by an LLM. The framework is evaluated on several knowledge-intensive benchmarks and with different backbone models.", "result": "Across multiple knowledge-intensive benchmarks and backbone LLMs, PAGER consistently outperforms baseline RAG approaches. The generated pages are empirically shown to be higher-quality and more information-dense than the representations produced by prior methods, and they help reduce knowledge conflicts while improving the effective use of retrieved information.", "conclusion": "Imposing a page-style, slot-based structure on iterative RAG enables LLMs to build more coherent, dense, and reliable knowledge representations for each query. This structured representation improves retrieval, conflict mitigation, and downstream answer generation, and is robust across tasks and base models."}}
{"id": "2601.09421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09421", "abs": "https://arxiv.org/abs/2601.09421", "authors": ["Filip Trhlik", "Andrew Caines", "Paula Buttery"], "title": "Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing", "comment": "21 pages, 18 figures", "summary": "Pre-trained language models (LMs) have, over the last few years, grown substantially in both societal adoption and training costs. This rapid growth in size has constrained progress in understanding and mitigating their biases. Since re-training LMs is prohibitively expensive, most debiasing work has focused on post-hoc or masking-based strategies, which often fail to address the underlying causes of bias. In this work, we seek to democratise pre-model debiasing research by using low-cost proxy models. Specifically, we investigate BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. We show that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. Furthermore, correlations between BabyLMs and BERT hold across multiple intra-model and post-model debiasing methods. Leveraging these similarities, we conduct pre-model debiasing experiments with BabyLMs, replicating prior findings and presenting new insights regarding the influence of gender imbalance and toxicity on bias formation. Our results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This provides a way to democratise pre-model debiasing research and enables faster, more accessible exploration of methods for building fairer LMs.", "AI": {"tldr": "The paper proposes using small, cheaply trained \u201cBabyLMs\u201d as proxies for large language models to study and reduce bias before full-scale pre-training, dramatically cutting compute costs while preserving similar bias patterns and learning dynamics.", "motivation": "Large pre-trained language models are widely used and very costly to train, which makes it hard to systematically study how their biases emerge and how to mitigate them during pre-training. Existing debiasing work mainly applies post-hoc or masking-based methods that do not address the root causes of bias. There is a need for an affordable, controllable setup that approximates the bias behavior of large models so researchers with limited resources can experiment with pre-model debiasing strategies.", "method": "The authors design and train compact BERT-like models, called BabyLMs, on small and easily modifiable corpora. They then compare these BabyLMs to standard BERT models along several dimensions: (1) patterns of intrinsic bias formation, (2) performance development during training, and (3) responses to multiple intra-model and post-model debiasing methods. Using these validated similarities, they run controlled pre-model (data-level and training-level) debiasing experiments on BabyLMs, manipulating factors such as gender imbalance and toxicity in the training data.", "result": "BabyLMs exhibit bias acquisition patterns and performance curves that closely track those of full-sized BERT models, despite being much smaller and cheaper to train. Correlations hold not only in raw bias metrics but also in how different debiasing interventions affect the models. Experiments on BabyLMs reproduce prior findings on bias formation and reveal new details about how gender imbalance and toxic content in the training corpus contribute to downstream biases. Crucially, the required compute drops from over 500 GPU-hours for standard models to under 30 GPU-hours for BabyLMs.", "conclusion": "Small, BERT-like BabyLMs can effectively approximate the bias dynamics of large language models and thus serve as a low-cost sandbox for pre-model debiasing research. This approach substantially lowers the computational barrier to studying and mitigating bias during pre-training, enabling broader participation and faster iteration on methods for building fairer language models."}}
{"id": "2601.09445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09445", "abs": "https://arxiv.org/abs/2601.09445", "authors": ["Minh Vu Pham", "Hsuvas Borkakoty", "Yufang Hou"], "title": "Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models", "comment": null, "summary": "In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.", "AI": {"tldr": "The paper studies where and how conflicting facts from pre\u2011training are internally stored in language models, and shows how to locate and intervene on these conflicts at inference time using mechanistic interpretability tools.", "motivation": "Large language models often contain mutually inconsistent information about the same event or fact, because they are trained on diverse and sometimes contradictory data. Most prior work tries to fix such conflicts by editing or fine\u2011tuning models, usually by aligning them with external knowledge. However, we still lack an understanding of *where* inside the model these conflicting memories reside and *how* they are represented. This gap prevents precise, minimal interventions and limits our ability to systematically control or debug model knowledge. The paper aims to fill this gap by localizing intra\u2011model knowledge conflicts within the model\u2019s own internal components.", "method": "The authors build a mechanistic\u2011interpretability\u2011based framework that probes the internal representations of a language model for conflicting knowledge originating from pre\u2011training. Concretely, they identify specific components (e.g., certain layers, heads, or neurons) that differentially activate for conflicting variants of the same event. They then perform causal interventions (such as activation patching, ablation, or targeted modification of activations) to test whether those components are necessary and sufficient for expressing each conflicting piece of knowledge. This combination of localization and causal testing provides a structured way to map conflicts to particular internal mechanisms.", "result": "They find that conflicting knowledge is not uniformly distributed across the network but is concentrated in particular internal components. These components can be selectively manipulated to favor one version of a fact over another at inference time. Their experiments show that intervening on the identified components can reliably control which conflicting knowledge the model exhibits, without broadly degrading overall performance. This supports the view that distinct, addressable subcircuits encode different, even contradictory, memories from pre\u2011training.", "conclusion": "The paper concludes that intra\u2011memory knowledge conflicts in LMs can be traced to specific internal structures, and that mechanistic interpretability offers practical tools for identifying and causally controlling these conflicts at inference. This advances both theoretical understanding\u2014by showing that conflicting knowledge is structurally localized rather than diffusely spread\u2014and practical capabilities, by enabling more targeted and reversible knowledge control than coarse editing or fine\u2011tuning. The work encourages further research into fine\u2011grained mechanistic maps of model knowledge and conflict, as a foundation for safer, more controllable language models."}}
{"id": "2601.09446", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09446", "abs": "https://arxiv.org/abs/2601.09446", "authors": ["Ramya Keerthy Thatikonda", "Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "title": "Improving Symbolic Translation of Language Models for Logical Reasoning", "comment": "The Third workshop of NeusymBridge @AAAI 2026 (Bridging Neurons and Symbols for NLP and Knowledge Graph Reasoning)", "summary": "The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.", "AI": {"tldr": "The paper improves the reliability of small language models for symbolic logical reasoning by fine-tuning them on synthetic data, decomposing the task into predicate generation and FOL translation (incremental inference), and adding a verification module to catch structural errors.", "motivation": "Translating natural language into first-order logic lets us do verifiable reasoning with external solvers, but small language models often fail at this translation due to formatting and logical-structure errors. Existing self-correction loops depend too much on model strength and are unreliable for smaller models, limiting practical, accessible symbolic reasoning systems.", "method": "1) Systematically categorize common NL\u2192FOL translation errors made by small LMs. 2) Use large LMs to synthesize training data reflecting these errors and fine-tune small LMs to avoid them. 3) Propose incremental inference: split reasoning into (a) predicate generation from natural language and (b) translation of those predicates into full FOL. 4) Introduce a verification module focusing on predicate-arity and related structural issues, automatically checking and correcting outputs. 5) Evaluate three model families on four logical reasoning datasets using the defined error categories and predicate-level metrics.", "result": "After fine-tuning with synthetic data, applying incremental inference, and adding verification, small LMs show lower symbolic translation error rates, higher predicate coverage, and better downstream logical reasoning accuracy across multiple datasets and model families.", "conclusion": "Decomposing NL\u2192FOL reasoning, systematically fine-tuning with targeted synthetic data, and adding structural verification significantly boost the reliability of small LMs for symbolic reasoning. This narrows the performance gap with larger models and is a step toward practical, verifiable, and accessible logical reasoning systems based on smaller models."}}
{"id": "2601.09487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09487", "abs": "https://arxiv.org/abs/2601.09487", "authors": ["Yunqiao Yang", "Wenbo Li", "Houxing Ren", "Zimu Lu", "Ke Wang", "Zhiyuan Huang", "Zhuofan Zong", "Mingjie Zhan", "Hongsheng Li"], "title": "SlidesGen-Bench: Evaluating Slides Generation via Computational and Quantitative Metrics", "comment": "37 pages, 34 figures", "summary": "The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments. In this paper, we introduce SlidesGen-Bench, a benchmark designed to evaluate slide generation through a lens of three core principles: universality, quantification, and reliability. First, to establish a unified evaluation framework, we ground our analysis in the visual domain, treating terminal outputs as renderings to remain agnostic to the underlying generation method. Second, we propose a computational approach that quantitatively assesses slides across three distinct dimensions - Content, Aesthetics, and Editability - offering reproducible metrics where prior works relied on subjective or reference-dependent proxies. Finally, to ensure high correlation with human preference, we construct the Slides-Align1.5k dataset, a human preference aligned dataset covering slides from nine mainstream generation systems across seven scenarios. Our experiments demonstrate that SlidesGen-Bench achieves a higher degree of alignment with human judgment than existing evaluation pipelines. Our code and data are available at https://github.com/YunqiaoYang/SlidesGen-Bench.", "AI": {"tldr": "Introduces SlidesGen-Bench, a unified, quantitative, and reliable benchmark for evaluating automated slide generation systems, aligned with human preferences.", "motivation": "Existing automated slide generation systems are diverse (code-based, image-centric, etc.), making fair comparison difficult. Current evaluation methods are often architecture-specific, subjective, or rely on uncalibrated human judgments, preventing reproducible and comparable evaluation across different systems.", "method": "Propose SlidesGen-Bench, an evaluation benchmark that: (1) grounds all systems in the visual domain by treating their outputs as rendered slides, independent of generation method; (2) uses computational metrics to evaluate three dimensions\u2014Content, Aesthetics, and Editability\u2014rather than purely subjective scores; and (3) constructs Slides-Align1.5k, a human preference dataset of slides from nine mainstream generation systems across seven scenarios, to calibrate and validate the evaluation metrics against human judgments.", "result": "Experiments show that SlidesGen-Bench\u2019s metrics correlate more strongly with human preferences than existing evaluation protocols, enabling more consistent and reliable comparison of heterogeneous slide generation systems.", "conclusion": "SlidesGen-Bench provides a universal, quantitative, and reliable framework for evaluating automated slide generation systems by focusing on visual outputs, defining metric-based evaluation over Content, Aesthetics, and Editability, and aligning these metrics with human preference data, outperforming previous evaluation pipelines in human alignment."}}
{"id": "2601.09504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09504", "abs": "https://arxiv.org/abs/2601.09504", "authors": ["Yinqi Liu", "Yueqi Zhu", "Yongkang Zhang", "Xinfeng Li", "Feiran Liu", "Yufei Sun", "Xin Wang", "Renzhao Liang", "Yidong Wang", "Cunxiang Wang"], "title": "MVSS: A Unified Framework for Multi-View Structured Survey Generation", "comment": null, "summary": "Scientific surveys require not only summarizing large bodies of literature, but also organizing them into clear and coherent conceptual structures. Existing automatic survey generation methods typically focus on linear text generation and struggle to explicitly model hierarchical relations among research topics and structured methodological comparisons, resulting in gaps in structural organization compared to expert-written surveys. We propose MVSS, a multi-view structured survey generation framework that jointly generates and aligns citation-grounded hierarchical trees, structured comparison tables, and survey text. MVSS follows a structure-first paradigm: it first constructs a conceptual tree of the research domain, then generates comparison tables constrained by the tree, and finally uses both as structural constraints for text generation. This enables complementary multi-view representations across structure, comparison, and narrative. We introduce an evaluation framework assessing structural quality, comparative completeness, and citation fidelity. Experiments on 76 computer science topics show MVSS outperforms existing methods in organization and evidence grounding, achieving performance comparable to expert surveys.", "AI": {"tldr": "MVSS is a framework that generates survey papers using multiple coordinated structures (trees, tables, text) rather than only linear text.", "motivation": "Automatic survey generation tools lack explicit hierarchical topic structures and systematic method comparisons, making their outputs less organized and less like expert-written surveys.", "method": "MVSS uses a structure-first, multi-view approach: (1) build a citation-grounded conceptual topic tree, (2) generate structured comparison tables guided by this tree, and (3) produce survey text constrained by both the tree and tables. An evaluation framework measures structural quality, comparative completeness, and citation fidelity. Experiments are run on 76 CS topics.", "result": "On 76 computer science topics, MVSS produces surveys that are better organized and more evidence-grounded than existing automatic methods, and its quality is comparable to expert-written surveys according to the proposed metrics.", "conclusion": "A multi-view, structure-first framework that jointly generates topic hierarchies, comparison tables, and narrative text can substantially narrow the gap between automatic and expert-written surveys in terms of organization and citation grounding."}}
{"id": "2601.09515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09515", "abs": "https://arxiv.org/abs/2601.09515", "authors": ["Chenglong Wang", "Canjia Li", "Xingzhao Zhu", "Yifu Huo", "Huiyu Wang", "Weixiong Lin", "Yun Yang", "Qiaozhi He", "Tianhua Zhou", "Xiaojia Chang", "Jingbo Zhu", "Tong Xiao"], "title": "SERM: Self-Evolving Relevance Model with Agent-Driven Learning from Massive Query Streams", "comment": null, "summary": "Due to the dynamically evolving nature of real-world query streams, relevance models struggle to generalize to practical search scenarios. A sophisticated solution is self-evolution techniques. However, in large-scale industrial settings with massive query streams, this technique faces two challenges: (1) informative samples are often sparse and difficult to identify, and (2) pseudo-labels generated by the current model could be unreliable. To address these challenges, in this work, we propose a Self-Evolving Relevance Model approach (SERM), which comprises two complementary multi-agent modules: a multi-agent sample miner, designed to detect distributional shifts and identify informative training samples, and a multi-agent relevance annotator, which provides reliable labels through a two-level agreement framework. We evaluate SERM in a large-scale industrial setting, which serves billions of user requests daily. Experimental results demonstrate that SERM can achieve significant performance gains through iterative self-evolution, as validated by extensive offline multilingual evaluations and online testing.", "AI": {"tldr": "They propose a self-evolving relevance model (SERM) for search systems that handles dynamic query streams by mining informative samples and generating reliable pseudo-labels via multi-agent modules, achieving strong offline and online gains in a large-scale industrial setting.", "motivation": "Real-world query streams change dynamically, making it hard for static relevance models to generalize. Existing self-evolution methods are attractive but, at industrial scale, it is hard to: (1) find truly informative samples in massive, mostly uninformative traffic; and (2) trust pseudo-labels from a single current model, which can be noisy or biased. The paper aims to solve these two bottlenecks so self-evolution becomes practical for billion-scale search systems.", "method": "They introduce SERM, a Self-Evolving Relevance Model framework with two multi-agent modules. The multi-agent sample miner monitors query distributions to detect shifts and selects informative samples for further training. The multi-agent relevance annotator then assigns labels to these samples using a two-level agreement mechanism across multiple agents/models to improve pseudo-label reliability. The model iteratively self-evolves by training on these mined and annotated samples over time.", "result": "In a large industrial search environment with billions of daily requests, SERM yields significant performance improvements. The gains are confirmed both in extensive offline multilingual experiments and in online tests, though specific metrics are not listed in the abstract.", "conclusion": "By coupling a multi-agent sample miner with a multi-agent relevance annotator and iteratively retraining, SERM effectively enables robust self-evolution of relevance models in dynamic, large-scale query streams. This mitigates issues of sparse informative samples and unreliable pseudo-labels, and demonstrably improves real-world search performance."}}
{"id": "2601.09555", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09555", "abs": "https://arxiv.org/abs/2601.09555", "authors": ["Manyi Zhang", "Ji-Fu Li", "Zhongao Sun", "Haoli Bai", "Hui-Ling Zhen", "Zhenhua Dong", "Xianzhi Yu"], "title": "Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats", "comment": null, "summary": "Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.", "AI": {"tldr": "Systematic study of post-training quantization (PTQ) methods for large language models under microscaling floating-point (MXFP) formats, revealing when and why they work, especially for MXFP8 vs MXFP4, and proposing a simple scaling optimization to improve MXFP4.", "motivation": "MXFP is a promising low-precision format for efficient LLM deployment, but most existing PTQ techniques were designed and evaluated for integer quantization. It is unclear how well they transfer to MXFP formats, which design choices matter, and what practical guidelines should be followed when using MXFP for LLMs and multimodal LLMs. This work aims to fill that gap with a broad, empirical analysis.", "method": "The authors perform a large-scale empirical evaluation of more than 7 representative PTQ algorithms across MXFP formats (MXFP8 and MXFP4), using 15 benchmarks and 3 different LLM families, including multimodal models. They compare algorithmic paradigms, study quantization sensitivity of different model components, and analyze error sources, especially the role of scaling factors in MXFP4. They also test a simple pre-scale optimization to reduce quantization errors.", "result": "They find that MXFP8 yields almost no performance loss across tasks and models, while MXFP4 leads to substantial degradation and is much harder to use effectively. The effectiveness of PTQ under MXFP is highly dependent on the compatibility between the PTQ algorithm and the MXFP format, with some paradigms consistently outperforming others. Quantization behavior and sensitivity patterns are very consistent across different LLM families and modalities; in multimodal LLMs, language model components dominate quantization sensitivity more than vision encoders. For MXFP4, errors are largely driven by the scaling factor, and a simple pre-scale optimization significantly improves performance.", "conclusion": "PTQ can be effectively adapted to MXFP, especially MXFP8, but MXFP4 remains challenging without careful handling of scaling. The study offers practical guidance: prefer MXFP8 for near-lossless compression; select PTQ algorithms that are format-compatible; focus quantization effort on the language model part in multimodal LLMs; and use pre-scale optimization to alleviate scaling-factor-induced errors in MXFP4. These insights help practitioners deploy LLMs more efficiently with MXFP quantization."}}
{"id": "2601.09570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09570", "abs": "https://arxiv.org/abs/2601.09570", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering", "comment": "16 pages, 9 Figures, Version submitted to IEEE for publication", "summary": "Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.", "AI": {"tldr": "The paper proposes Dialogue Telemetry, a turn-level measurement framework to monitor and improve information-gathering dialogues by estimating remaining information and detecting when questioning has stalled.", "motivation": "Autonomous, schema-grounded dialogue systems lack fine-grained, turn-level metrics to monitor how efficiently they are collecting required information and to detect when additional questions stop yielding useful new information. This \u201cinstrumentation gap\u201d makes it hard to recognize and mitigate stalled or unproductive questioning, especially when the root cause of degradation is unclear or cannot practically be diagnosed in real time.", "method": "The authors introduce Dialogue Telemetry (DT), which outputs two model-agnostic signals after each question-answer turn: (1) a Progress Estimator (PE) that estimates residual information potential per schema category, including a variant measured in bits; and (2) a Stalling Index (SI) that identifies a failure mode where the system keeps asking about the same category and receives semantically similar, low-information responses. DT is integrated into a reinforcement learning framework so that an RL policy can use these signals as features. The framework is evaluated in LLM-based simulated, SAR-inspired interviews, comparing efficient vs. stalled dialogues.", "result": "In LLM-based simulations of search-and-rescue style interviews, DT successfully distinguishes efficient information-gathering dialogues from stalled ones. Using PE and SI as additional signals in an RL policy improves policy performance when there are explicit costs associated with stalling or inefficient questioning, showing that the telemetry is both interpretable and operationally useful.", "conclusion": "Dialogue Telemetry offers interpretable, model-agnostic, turn-level instrumentation for schema-grounded information-gathering dialogues. By quantifying residual information and detecting stalling patterns without requiring causal explanations, DT enables better monitoring and control of dialogue behavior and can be used to improve learned dialogue policies, particularly in scenarios where inefficient questioning has real operational costs."}}
{"id": "2601.09609", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09609", "abs": "https://arxiv.org/abs/2601.09609", "authors": ["Qian Cao", "Yahui Liu", "Wei Bi", "Yi Zhao", "Ruihua Song", "Xiting Wang", "Ruiming Tang", "Guorui Zhou", "Han Li"], "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing", "comment": null, "summary": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.", "AI": {"tldr": "The paper introduces an RL framework for LLMs that uses structured, branching planning in Chain-of-Thought to explicitly encourage diverse generations, especially for creative writing, improving diversity without hurting quality.", "motivation": "Reinforcement learning fine-tuning of LLMs often reduces output diversity, which is harmful for open-ended, creative tasks. Existing RL methods focus on optimizing for reward and performance but lack explicit mechanisms to promote diverse exploration and generation. There is a need for an approach that can systematically guide diverse reasoning paths and outputs while maintaining quality.", "method": "The authors propose an RL framework organized around a semi-structured, long Chain-of-Thought, decomposing generation into explicit intermediate planning steps. They introduce a Diverse Planning Branching mechanism, which creates multiple divergent plans during the planning phase, guided by diversity variation metrics, so that different reasoning or narrative paths are explored. Additionally, they design a group-aware diversity reward that evaluates sets of trajectories jointly to encourage them to be distinct from each other rather than collapsing to similar outputs. This combination is trained in an RL setting for LLMs on creative tasks.", "result": "On creative writing benchmarks, the method yields significantly higher diversity among generated outputs compared to standard RL-based enhancement and other baselines, while preserving or not degrading generation quality. Quantitative metrics and possibly human evaluations (implied) show consistent improvements over competing methods.", "conclusion": "Explicitly structuring RL-based LLM training around a branching Chain-of-Thought planning phase, combined with a group-aware diversity reward, effectively mitigates diversity collapse. The approach can enhance creative writing performance by producing more varied yet high-quality outputs and may generalize to other open-ended tasks that benefit from diverse solution trajectories."}}
{"id": "2601.09631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09631", "abs": "https://arxiv.org/abs/2601.09631", "authors": ["Stergios Chatzikyriakidis"], "title": "LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation", "comment": null, "summary": "Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant \"Reasoning Gap\": while native-like models (Claude 3.7) perform intuitively (40\\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\\% valid poems), while our hybrid verification loop restores performance to 73.1\\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.", "AI": {"tldr": "The paper presents a hybrid system that combines large language models with deterministic phonological algorithms to accurately detect, analyze, and generate rhymes in Modern Greek, overcoming LLM weaknesses in phonology and achieving much higher generation validity than pure LLM approaches.", "motivation": "LLMs are strong at many NLP tasks but perform poorly on phonologically grounded tasks like rhyme detection and rhyme-based text generation, especially in lower-resource languages such as Modern Greek where resources and benchmarks are scarce. There is a need for accurate computational handling of complex rhyme structures and reliable poem generation that respects these constraints, as existing LLMs either reason poorly about phonology or hallucinate structurally invalid rhymes.", "method": "The authors design a hybrid architecture that integrates LLMs with deterministic phonological algorithms implementing a fine-grained taxonomy of Greek rhyme types (Pure, Rich, Imperfect, Mosaic, Identical Pre-rhyme Vowel). They test multiple LLMs (Claude, GPT-4o, Gemini, Llama, Mistral) under several prompting strategies (zero-shot, few-shot, Chain-of-Thought, RAG). For generation, they use an agent-style pipeline: the LLM proposes candidate rhymes/poems, a phonological module verifies rhyme correctness, and feedback/iteration is used to refine outputs. They systematically evaluate identification accuracy and the proportion of valid generated poems under these settings.", "result": "They find a notable 'Reasoning Gap' between models: more intuitive or native-like models (Claude 3.7) reach about 40% accuracy in rhyme identification, whereas models optimized for reasoning (Claude 4.5) can reach 54% when guided with Chain-of-Thought prompts. Pure LLM-based poem generation performs extremely poorly, with fewer than 4% of outputs satisfying rhyme constraints. When integrated into the hybrid verification loop, however, performance rises dramatically to 73.1% valid poems. They also construct and release a cleaned corpus of over 40,000 rhymes from Greek poetry sources for future research.", "conclusion": "The study concludes that LLMs alone are inadequate for reliable phonologically precise tasks such as rhyme detection and constrained poem generation, particularly in a lower-resource language like Modern Greek. Hybrid systems that pair LLM generative and reasoning abilities with explicit, deterministic phonological modules can bridge this gap, yielding state-of-the-art performance. The authors emphasize the importance of structured prompting (especially Chain-of-Thought) for reasoning-heavy models and provide tools and data resources to facilitate further work in computational phonology and rhyme-aware text generation."}}
{"id": "2601.09633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09633", "abs": "https://arxiv.org/abs/2601.09633", "authors": ["Sahil Mishra", "Srinitish Srinivasan", "Srikanta Bedathur", "Tanmoy Chakraborty"], "title": "TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion", "comment": "Accepted in The Web Conference (WWW) 2026", "summary": "Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric \"is-a\" relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies.", "AI": {"tldr": "TaxoBell introduces a Gaussian box embedding framework to better model hierarchical \"is-a\" relations for automatic taxonomy expansion, significantly outperforming prior methods.", "motivation": "Manual taxonomy expansion is slow and cannot keep up with emerging concepts, while existing automated methods using point embeddings fail to capture asymmetric hierarchical relations and box embeddings suffer from instability, lack of uncertainty modeling, and difficulty handling polysemy.", "method": "The authors propose TaxoBell, a Gaussian box embedding approach that maps box geometries to multivariate Gaussian distributions whose means represent semantic positions and covariances represent uncertainty. They use an energy-based optimization objective to train these embeddings, allowing stable training, uncertainty-aware modeling, and interpretable hierarchical reasoning.", "result": "On five benchmark datasets, TaxoBell outperforms eight state-of-the-art taxonomy expansion baselines, achieving about 19% improvement in Mean Reciprocal Rank (MRR) and around 25% improvement in Recall@k. Error analysis and ablations validate where the gains come from and highlight method strengths and weaknesses.", "conclusion": "TaxoBell effectively overcomes key limitations of previous point and box embedding methods for taxonomy expansion, enabling more accurate, stable, and interpretable modeling of hierarchical \"is-a\" relations, with strong empirical gains and detailed analysis of its behavior."}}
{"id": "2601.09648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09648", "abs": "https://arxiv.org/abs/2601.09648", "authors": ["Andrew Moore", "Paul Rayson", "Dawn Archer", "Tim Czerniak", "Dawn Knight", "Daisy Lal", "Gear\u00f3id \u00d3 Donnchadha", "M\u00edche\u00e1l \u00d3 Meachair", "Scott Piao", "Elaine U\u00ed Dhonnchadha", "Johanna Vuorinen", "Yan Yabo", "Xiaobin Yang"], "title": "Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation", "comment": "12 pages, 2 figures", "summary": "Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.", "AI": {"tldr": "The paper presents the first large-scale, multi-language evaluation of USAS-based semantic tagging, compares rule-based and neural approaches, and releases models, datasets (including a new Chinese set and a silver-labelled English set), and code.", "motivation": "While WSD and semantic tagging have been widely evaluated with resources like WordNet and BabelNet, the USAS framework lacks a broad, open, multi-language evaluation beyond basic lexical coverage or single-language studies. There is also a shortage of manually annotated training data for supervised models within this framework, limiting progress on data-driven approaches and cross-lingual semantic tagging. The authors aim to fill these gaps by providing a comprehensive evaluation and new resources to stimulate further research.", "method": "The authors conduct a large-scale evaluation of a rule-based semantic tagging system built on USAS across five languages, using four existing datasets and a new Chinese dataset. To address the lack of gold-standard training data, they create a silver-labelled English dataset with USAS tags. They then train multiple monolingual and multilingual neural models for semantic tagging, testing them in both monolingual and cross-lingual setups. Performance of these neural models is compared against the rule-based system, and the authors explore hybridization where the rule-based system is enhanced with neural models. All models, datasets, and code are released publicly.", "result": "The experiments demonstrate that neural models trained on the silver-labelled English data can effectively perform USAS-style semantic tagging and can be used in both mono and cross-lingual settings. In multiple configurations, the neural approaches are competitive with or improve upon the purely rule-based USAS system across the five evaluated languages. The study also shows that integrating neural models with the rule-based approach can further enhance overall tagging performance.", "conclusion": "USAS-based semantic tagging can benefit substantially from data-driven neural methods, even when training data is only silver-labelled. The authors provide the first extensive, multi-language evaluation of USAS semantic tagging, demonstrating the viability of neural and hybrid rule-neural approaches. By releasing trained models, datasets (including a new Chinese set) and code, they establish a foundation and toolkit for future research on multilingual semantic tagging within the USAS framework."}}
{"id": "2601.09688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09688", "abs": "https://arxiv.org/abs/2601.09688", "authors": ["Yibo Wang", "Lei Wang", "Yue Deng", "Keming Wu", "Yao Xiao", "Huanjin Yao", "Liwei Kang", "Hai Ye", "Yongcheng Jing", "Lidong Bing"], "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "comment": "Source code: https://github.com/Infinity-AILab/DeepResearchEval", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "AI": {"tldr": "The paper introduces DeepResearchEval, an automated framework to construct and evaluate deep multi-step web research tasks.", "motivation": "Evaluating deep research systems is hard because current benchmarks are costly to annotate, use fixed evaluation dimensions that may not fit each task, and struggle to verify factual accuracy when model outputs lack explicit citations.", "method": "They design an automated, persona-driven pipeline to generate realistic multi-step research tasks and filter them with Task Qualification and Search Necessity checks, ensuring tasks truly require multi-source retrieval and synthesis. For evaluation, they build an agentic pipeline with Adaptive Point-wise Quality Evaluation that dynamically derives tailored evaluation dimensions, criteria, and weights for each task, and an Active Fact-Checking component that automatically extracts claims from system reports and verifies them via web search, even when citations are absent.", "result": "The framework can automatically construct complex research tasks grounded in user personas and evaluate research agents by dynamically scoring their outputs on task-specific dimensions while verifying factual statements using external web search.", "conclusion": "DeepResearchEval offers a scalable, automated way to both generate realistic deep research tasks and to evaluate research agents with task-adaptive quality criteria and active fact-checking, addressing key limitations of prior static, annotation-heavy benchmarks."}}
{"id": "2601.09692", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09692", "abs": "https://arxiv.org/abs/2601.09692", "authors": ["Tianyi Niu", "Justin Chih-Yao Chen", "Genta Indra Winata", "Shi-Xiong Zhang", "Supriyo Chakraborty", "Sambit Sahu", "Yue Zhang", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection", "comment": "Code: https://github.com/tianyiniu/RoutingGenData", "summary": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.", "AI": {"tldr": "The paper studies how to train LLM routers using only synthetic (generated) data instead of labeled real data, and proposes a new, more robust routing method.", "motivation": "In many real-world scenarios we want a router to choose among multiple LLMs for each user query, but we lack ground-truth labeled datasets and the real query distribution is unknown and heterogeneous. Existing routing methods usually assume labeled data. The authors want to understand whether routers can be trained purely from synthetic data generated by LLMs given high-level task descriptions, and what properties this generated data must have to be effective.", "method": "They define a new setting called Routing with Generated Data (RGD), where a generator LLM produces both questions and answers from task descriptions. They compare two classes of routers: query-answer routers that use both queries and labels, and query-only routers that rely only on the queries. They evaluate these routers across four benchmarks and a pool of 12 models while varying generator quality. They analyze how generator properties affect routing performance, identify two key desiderata (self-consistency and performance differentiation), and use filtering based on these properties to improve synthetic data quality. They also introduce CASCAL, a query-only router that (1) estimates model correctness via consensus voting among models and (2) discovers model-specific skill niches using hierarchical clustering.", "result": "They find that as generator quality drops, query-answer routers suffer a larger performance degradation than query-only routers. Their analysis shows that effective generators must both answer their own questions correctly and produce questions that cause sufficiently diverse performance among the model pool. Filtering synthetic data using these criteria improves routing performance. CASCAL, their proposed query-only router, is significantly more robust to weak generators and achieves a 4.6% absolute accuracy gain over the best query-answer router when trained on data from a weak generator.", "conclusion": "Routers for LLM model selection can be trained from purely synthetic data if that data satisfies certain quality properties. Query-only routing, particularly the proposed CASCAL approach, is more robust than query-answer routing when generator quality is limited. Carefully designed generators and data filtering strategies are key to enabling effective routing without real labeled datasets."}}
{"id": "2601.09694", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09694", "abs": "https://arxiv.org/abs/2601.09694", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents", "comment": "17 Pages", "summary": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.", "AI": {"tldr": "The paper proposes an LLM-based agent that adaptively decides which layers of a large language model to prune, achieving ~45% sparsity while significantly reducing performance loss and factual knowledge degradation compared with existing structured pruning methods.", "motivation": "Post-training pruning of LLMs can greatly reduce computational costs, but current methods use rigid or hand-crafted per-layer sparsity schedules and cause severe degradation in factual knowledge and QA ability, especially for structured pruning. There is a need for a more adaptive, knowledge-preserving pruning strategy that decides where to prune based on the model\u2019s actual sensitivity and knowledge pathways, without requiring retraining.", "method": "The authors introduce agent-guided pruning, where a separate foundation model serves as an adaptive pruning agent. They compute layer-wise sensitivity profiles using Wanda-style weight-activation metrics combined with gradient-based importance scores, then normalize them as z-scores to make them comparable across layers and models. These statistics are passed to the LLM agent, which, using self-reflection over past pruning outcomes, iteratively decides which layers and how much to prune at each iteration. A checkpoint rollback mechanism reverts to the previous model checkpoint whenever perplexity deterioration exceeds a predefined threshold, enabling self-correction during the pruning process. The pipeline is model-agnostic and requires no retraining of the pruned model.", "result": "At approximately 45% sparsity on Qwen3 4B and 8B models, the method outperforms structured pruning baselines, achieving a 56% relative improvement in MMLU accuracy, 19x higher factual knowledge retention on FreebaseQA, and 69% less perplexity degradation. The pruning process converges efficiently, needing only 2\u20134 rollbacks over 21\u201340 pruning iterations, indicating stable and effective decision-making by the LLM agent.", "conclusion": "Agent-guided pruning shows that a foundation model can effectively act as a controller to compress other foundation models, producing highly sparse LLMs that better preserve general performance and factual knowledge compared to existing structured pruning methods. The approach is model-agnostic, requires no retraining, and leverages self-reflective, data-driven layer selection instead of heuristic sparsity schedules, offering a promising direction for practical and intelligent LLM compression."}}
{"id": "2601.09696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09696", "abs": "https://arxiv.org/abs/2601.09696", "authors": ["Shan Randhawa", "Agha Ali Raza", "Kentaro Toyama", "Julie Hui", "Mustafa Naseem"], "title": "Empathy Applicability Modeling for General Health Queries", "comment": "In Submission to ACL", "summary": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.", "AI": {"tldr": "The paper proposes a framework to anticipate when patient queries require empathy, builds an annotated benchmark, and shows classifiers can effectively predict empathy applicability, improving over heuristic and LLM baselines.", "motivation": "Large language models are being used in clinical workflows but often respond without sufficient clinical empathy, which is crucial for effective doctor-patient communication. Existing NLP work mainly labels empathy in already-produced responses and does not proactively determine when empathy is needed, especially in general health queries. There is a gap in theory-driven, anticipatory modeling to detect empathy needs from patient queries themselves, which is necessary for guiding future empathetic responses and improving asynchronous healthcare interactions.", "method": "The authors introduce the Empathy Applicability Framework (EAF), a theory-based scheme that categorizes patient queries according to whether emotional reactions and interpretations are applicable, using clinical, contextual, and linguistic cues. They compile a benchmark of real patient queries that are dual-annotated by human annotators and GPT-4o for empathy applicability. They analyze agreement between humans and GPT-4o, focusing on the subset with human consensus, and observe substantial alignment. Then they train supervised classifiers on human-only and GPT-only labels to predict empathy applicability and compare their performance against heuristic rules and zero-shot LLM baselines. They also conduct an error analysis to identify systematic failure modes.", "result": "Classifiers trained with the EAF labels achieve strong predictive performance for empathy applicability and outperform both heuristic baselines and zero-shot LLM approaches. In the subset of data where humans agree, GPT-4o annotations align well with human judgments, suggesting GPT-4o can be a useful annotator or co-annotator. The analysis uncovers specific difficult cases where models struggle, such as when distress is implicit rather than explicit, when clinical severity is ambiguous, or when broader contextual hardship influences empathy needs.", "conclusion": "The Empathy Applicability Framework offers a practical, theory-grounded way to detect when empathy is needed from patient queries before generating responses. By releasing a benchmark and demonstrating effective classifier performance, the work establishes a foundation for anticipatory empathy modeling. The identified challenges indicate that robust solutions will require multiple annotators, clinician input, and culturally diverse data. Overall, EAF can support more consistently empathetic communication in asynchronous digital healthcare settings by signaling empathy needs in advance."}}
{"id": "2601.09706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09706", "abs": "https://arxiv.org/abs/2601.09706", "authors": ["Andreea Dutulescu", "Stefan Ruseti", "Mihai Dascalu"], "title": "Value-Aware Numerical Representations for Transformer Language Models", "comment": null, "summary": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.", "AI": {"tldr": "The paper proposes a new value-aware numerical representation for transformer-based language models that explicitly encodes numerical magnitudes to improve arithmetic and numerical robustness.", "motivation": "Transformer LMs perform well on math reasoning benchmarks but remain brittle at basic numerical understanding because they treat numbers as ordinary tokens whose embeddings lack explicit information about numerical value. This symbolic treatment leads to systematic numerical errors, especially in arithmetic operations and across different numeric formats and operand lengths. The authors aim to address this representational limitation.", "method": "The authors augment standard tokenized inputs with an additional, dedicated prefix token for each number. The embedding of this prefix token is explicitly conditioned on the true underlying numerical value (e.g., its magnitude), thereby injecting value information directly into the model\u2019s input space while preserving compatibility with existing tokenizers and decoder-only Transformer architectures. They then train and evaluate models with this value-aware augmentation on a variety of arithmetic tasks and numeric formats, comparing against standard baselines.", "result": "Across a range of arithmetic tasks, numerical formats, and operand lengths, models that use the value-aware numerical representation outperform baseline models that rely solely on standard token embeddings. The improvements are consistent and highlight better robustness and accuracy in numerical and arithmetic reasoning.", "conclusion": "Making numerical value explicit in the input representation via a value-conditioned prefix token is an effective and efficient way to improve the numerical robustness of language models. This design can be integrated with existing tokenizers and Transformer architectures without major changes, suggesting a practical path toward more reliable numerical and arithmetic behavior in language models."}}
