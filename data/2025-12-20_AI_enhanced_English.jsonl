{"id": "2512.16814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16814", "abs": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "comment": null, "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "AI": {"tldr": "The paper introduces Grammar Forced Translation (GraFT), a framework that improves translating natural language into temporal logic by constraining the model\u2019s output choices using grammar- and task-specific structure, leading to better accuracy, especially out-of-domain.", "motivation": "Existing NL-to-temporal-logic systems decompose the task into lifting atomic propositions and then translating, but they struggle with correctly identifying atomic propositions, handling co-reference, and learning effectively from limited data. These weaknesses limit the practical deployment of robots and autonomous systems that must understand task specifications or instructions from humans in natural language.", "method": "GraFT reframes both lifting of atomic propositions and the NL-to-TL translation as structured prediction problems with strongly constrained output spaces. Instead of letting a language model choose freely from its full vocabulary at every decoding step, GraFT only permits a small, grammar- and problem-specific subset of tokens at each step, effectively enforcing valid temporal-logic structures and constrained AP candidates. The method exploits the syntactic/semantic structure of temporal logic and the lifting task to reduce the hypothesis space, and the authors also provide a theoretical analysis showing that this reduction yields more sample-efficient learning.", "result": "On three benchmarks\u2014CW, GLTL, and Navi\u2014GraFT improves average end-to-end NL-to-TL translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% compared with state-of-the-art methods, demonstrating both better overall performance and stronger generalization to unseen distributions.", "conclusion": "By leveraging grammar-based and task-specific constraints to shrink the space of possible outputs at each prediction step, GraFT enables more accurate and data-efficient translation from natural language to temporal logic. The significant gains, especially out-of-domain, indicate that solution-space reduction is a powerful principle for robust NL-to-formal-language translation in robotics and autonomous systems."}}
{"id": "2512.16832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16832", "abs": "https://arxiv.org/abs/2512.16832", "authors": ["Aditya Yadavalli", "Tiago Pimentel", "Tamar I Regev", "Ethan Wilcox", "Alex Warstadt"], "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels", "comment": null, "summary": "Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.", "AI": {"tldr": "The paper quantifies how much information prosody (speech melody) conveys beyond text, and what aspects of meaning it encodes, using an information-theoretic framework with large speech and language models.", "motivation": "Prosody carries crucial aspects of meaning (like emotion or sarcasm) that are often missed if we only analyze text. However, there has been no systematic, quantitative way to measure exactly how much information prosody adds over text, nor what specific kinds of meaning are carried more by audio vs. text. The authors aim to fill this gap to better understand the division of labor between prosody and lexical content.", "method": "They develop an information-theoretic method for estimating the mutual information between specific semantic dimensions of an utterance (e.g., emotion, sarcasm, questionhood) and different communication channels (audio and text). Large speech and language models are used as probabilistic estimators to compute how predictive each channel is of a given meaning dimension, effectively measuring how much information that channel carries about it. They then apply this framework to real speech corpora from television and podcasts.", "result": "The analysis shows that for sarcasm and emotion, the audio channel\u2014largely reflecting prosody\u2014conveys more than an order of magnitude more information than the text channel alone when broader discourse context is limited to the current sentence. For questionhood (whether something is a question), the extra information provided by prosody over text is much smaller; text already carries most of that signal.", "conclusion": "Prosody is a major carrier of certain types of meaning, especially sarcasm and emotion, substantially more so than text when long-term context is absent. In contrast, for more structurally coded meanings like questionhood, text is relatively sufficient and prosody adds less. The authors propose extending their information-theoretic framework to a wider range of meanings, channels, and languages to build a comprehensive quantitative theory of how different aspects of meaning are distributed across communicative signals."}}
{"id": "2512.16843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16843", "abs": "https://arxiv.org/abs/2512.16843", "authors": ["Harsh Vardhan Bansal"], "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference", "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)", "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications", "AI": {"tldr": "The paper introduces LLMCache, a model-agnostic, layer-wise caching framework that speeds up Transformer inference by reusing intermediate activations for semantically similar inputs, achieving up to 3.1x speedup with minimal accuracy loss.", "motivation": "Transformer-based language models deliver strong performance but suffer from high inference latency, which hinders real-time and large-scale deployment. Existing token-level KV caching only helps autoregressive decoding and has limited scope, so there is a need for a more general caching mechanism that can accelerate a broader set of Transformer architectures and tasks.", "method": "The authors propose LLMCache, a layer-wise caching framework that stores and reuses intermediate Transformer activations. It uses a lightweight semantic fingerprinting mechanism to detect when a new input is sufficiently similar to a previously seen one, enabling reuse of cached activations at arbitrary layers. The framework is model-agnostic and supports both encoder and decoder Transformers. They also design adaptive eviction strategies to prevent cache staleness and manage memory, integrating these into the inference pipeline.", "result": "On BERT and GPT-2 evaluated on SQuAD, WikiText-103, and OpenBookQA, LLMCache achieves up to 3.1x speedup in inference latency while keeping accuracy degradation below 0.5%, demonstrating substantial efficiency gains with negligible performance loss across models and tasks.", "conclusion": "LLMCache is presented as a practical, general-purpose solution for optimizing Transformer inference. By enabling layer-wise reuse of activations for semantically similar inputs, it can significantly reduce latency in real-world applications without materially sacrificing accuracy, and it applies broadly across different Transformer architectures and tasks."}}
{"id": "2512.16883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16883", "abs": "https://arxiv.org/abs/2512.16883", "authors": ["Tzu-Han Lin", "Wei-Lin Chen", "Chen-An Li", "Hung-yi Lee", "Yun-Nung Chen", "Yu Meng"], "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch", "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.", "AI": {"tldr": "The paper proposes AdaSearch, an RL framework that teaches LLM agents when to use web search, improving their awareness of what they already know, reducing unnecessary tool calls, and keeping task performance high.", "motivation": "Existing RL-trained search agents for LLMs tend to overuse external search: this increases latency and cost and exposes models to noisy or adversarial content. Purely relying on internal (parametric) knowledge, however, leads to hallucinations. Current attempts to curb search use via penalties on the number of tool calls need heavy reward engineering, give poor credit assignment, and can be gamed by agents that reduce calls in superficial ways. Moreover, counting calls as an evaluation metric mixes up good uses of search with bad ones, so it does not really measure adaptive, knowledge-aware behavior. The authors want a method and metric that explicitly measure and improve an agent\u2019s ability to know when it should or should not search.", "method": "1) They first introduce an F1-based decision metric that evaluates an agent\u2019s self-knowledge awareness: how accurately it decides whether to search, treating \u201cshould search\u201d vs \u201cshould not search\u201d as a classification problem. They apply this to existing methods like Search-R1 to show that current agents often request search even when answers are clearly available in their parametric knowledge. 2) They then propose AdaSearch, a two-stage, outcome-driven RL framework that separates the act of solving the task from the separate decision of whether to call search. The search/no-search decision is made by an explicit, interpretable module trained with RL signals derived from task outcomes rather than from hard penalties on call counts, making the decision boundary more transparent.", "result": "Across various LLM architectures and sizes, AdaSearch leads to higher knowledge-boundary awareness (better F1 on the search decision task), fewer unnecessary search-tool calls, and maintains strong end-task accuracy. It also yields more transparent and interpretable decision patterns compared to methods that implicitly fold search decisions into end-to-end policies with call-count penalties.", "conclusion": "Explicitly modeling and learning the decision of when to invoke external search, rather than just penalizing tool usage, produces LLM search agents that are both more efficient and more trustworthy. AdaSearch demonstrates that disentangling problem solving from search invocation, and evaluating the decision with a clear metric, leads to better adaptive use of parametric vs external knowledge, reduced costs and risks from over-searching, and more interpretable behavior, which is especially important for high-stakes domains like finance and medicine."}}
{"id": "2512.16899", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16899", "abs": "https://arxiv.org/abs/2512.16899", "authors": ["Yushi Hu", "Reyhane Askari-Hemmat", "Melissa Hall", "Emily Dinan", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "comment": "Code and data available at https://github.com/facebookresearch/MMRB2", "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "AI": {"tldr": "The paper presents Multimodal RewardBench 2 (MMRB2), a comprehensive benchmark to evaluate multimodal reward models for interleaved image-text tasks, demonstrating current gaps between models and human performance and identifying directions for improvement.", "motivation": "Reward models are crucial for aligning large language models with human preferences, but current work focuses mainly on text-only models. As omni/multimodal models that handle both images and text become more prevalent, there is a lack of systematic evaluation tools to assess and improve multimodal reward models that judge or guide such models. This paper aims to fill that gap with a dedicated benchmark.", "method": "The authors build MMRB2, a benchmark covering four multimodal tasks: text-to-image generation, image editing, interleaved (image-text) generation, and multimodal reasoning (\"thinking-with-images\"). For each task, they collect 1,000 expert-annotated preference pairs from outputs of 23 models/agents across 21 source tasks. They ensure quality and difficulty by using practical prompts, responses from strong state-of-the-art systems, and an ensemble filtering strategy to keep only preference pairs with strong expert agreement. They then evaluate a variety of existing multimodal reward models and LLM-as-a-judge systems on these pairs and analyze correlations between benchmark performance and downstream task success in Best-of-N sampling setups.", "result": "On MMRB2, humans exceed 90% agreement with the reference preferences, while leading multimodal judges fall short: Gemini 3 Pro reaches about 75\u201380% accuracy, GPT-5 and Gemini 2.5 Pro achieve 66\u201375%, GPT-4o lags at 59%, and the best open-source system, Qwen3-VL-32B, performs comparably to Gemini 2.5 Flash at around 64%. The benchmark scores strongly correlate with actual downstream performance when using these reward models to select from multiple candidate outputs (Best-of-N sampling).", "conclusion": "MMRB2 exposes a substantial performance gap between human experts and current multimodal reward models, even for top proprietary systems, indicating that multimodal preference modeling remains underdeveloped. Since benchmark performance aligns well with downstream task gains, MMRB2 offers a practical diagnostic and development tool. The analysis highlights where current reward models fail, pointing to concrete directions for improving multimodal alignment and evaluation in future work."}}
{"id": "2512.16902", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16902", "abs": "https://arxiv.org/abs/2512.16902", "authors": ["Eric Todd", "Jannik Brinkmann", "Rohit Gandikota", "David Bau"], "title": "In-Context Algebra", "comment": "28 pages, 18 figures. Code and data at https://algebra.baulab.info", "summary": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.", "AI": {"tldr": "Study of how transformers solve variable-valued group arithmetic tasks in-context.", "motivation": "Prior evidence of geometric embeddings mirroring algebraic structures comes from settings with fixed symbol-to-value mappings; it is unclear how transformers reason when token meanings vary per sequence (true in-context variable reasoning). The paper aims to uncover mechanisms transformers learn in this harder, more realistic setting.", "method": "Design a new arithmetic task over algebraic groups where the mapping from symbols to group elements changes from sequence to sequence, forcing in-context interpretation. Train transformers on these tasks, including on different group instances. Construct targeted data distributions that allow causal tests of hypothesized mechanisms, probing internal attention heads and behaviors. Analyze learned mechanisms by ablation and controlled input manipulations.", "result": "Transformers trained on these variable-meaning group arithmetic tasks achieve near-perfect accuracy and can generalize to novel algebraic groups not seen during training. Mechanistic probing reveals three recurring mechanisms: (1) commutative copying, where a specialized attention head effectively copies the correct answer token; (2) identity element recognition, separating facts that involve the group identity; and (3) closure-based cancellation, tracking group membership to limit and cancel candidate answers via closure properties.", "conclusion": "Even when token meanings vary across sequences, transformers learn robust, interpretable symbolic reasoning mechanisms\u2014not only geometric embeddings\u2014to perform in-context algebraic reasoning. This complements prior work on fixed-symbol arithmetic and suggests transformers can implement genuine variable-based reasoning over abstract algebraic structures."}}
{"id": "2512.16914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16914", "abs": "https://arxiv.org/abs/2512.16914", "authors": ["Nikhil Prakash", "Donghao Ren", "Dominik Moritz", "Yannick Assogba"], "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates", "comment": "18 pages, 3 figures", "summary": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.", "AI": {"tldr": "They introduce Constructive Circuit Amplification, a method that boosts specific abilities (like math reasoning) in LLMs by strengthening only a small, task-relevant subset of internal components, achieving sizable accuracy gains with minimal side effects.", "motivation": "Existing work shows that large language models contain sparse subnetworks (circuits) specialized for particular tasks, and that fine-tuning often just strengthens these circuits. However, standard fine-tuning modifies many parameters and can cause unwanted side effects, making it hard to precisely improve a single capability (e.g., math reasoning) without harming others. This motivates a more targeted way to edit only the components truly responsible for a desired behavior.", "method": "They propose Constructive Circuit Amplification (CCA). First, they collect reasoning traces on relevant tasks to identify pivotal tokens that are crucial for successful reasoning. Next, they analyze the model\u2019s internal activations to locate the specific components (e.g., attention heads, MLP units, or parameters) most responsible for processing these pivotal tokens and for the target task behavior. Finally, they fine-tune or update only this sparse, task-linked subset of components instead of the whole model, thereby amplifying the existing reasoning circuit without broad parameter changes.", "result": "On mathematical reasoning benchmarks, applying CCA to several LLMs yields accuracy improvements of up to +11.4%, while modifying as little as 1.59% of the model\u2019s components. Evaluation on other benchmarks (MMLU, TriviaQA, TruthfulQA) shows little to no degradation, indicating that the edits are highly targeted and do not significantly harm general capabilities.", "conclusion": "The paper concludes that LLM capabilities can be enhanced in a precise, interpretable way by selectively strengthening sparse internal circuits rather than globally fine-tuning the whole model. Constructive Circuit Amplification demonstrates that targeted, circuit-level updates can significantly improve specific skills like mathematical reasoning while largely preserving overall model behavior, supporting the view that LLM knowledge and skills are localized in sparse subnetworks that can be directly manipulated."}}
{"id": "2512.16917", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16917", "abs": "https://arxiv.org/abs/2512.16917", "authors": ["Qihao Liu", "Luoxin Ye", "Wufei Ma", "Yu-Cheng Chou", "Alan Yuille"], "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.", "AI": {"tldr": "They propose an adversarial RL framework where a reasoning LLM and a discriminator LLM co-train so that the discriminator evaluates each step (or slice) of reasoning, providing dense rewards that improve mathematical reasoning performance over standard RL baselines.", "motivation": "LLMs that can perform explicit reasoning still make many process errors (wrong calculations, brittle logic, plausible but invalid steps). Standard RL or supervised methods provide mostly outcome-level or sparse rewards, making it hard to assign credit to intermediate reasoning steps. There is a need for an efficient way to give accurate, dense, step-level feedback to improve reasoning quality.", "method": "They introduce Generative Adversarial Reasoner, an on-policy joint training framework. A reasoning LLM generates chain-of-thought solutions, which are partitioned into logically complete slices of comparable length with a compute-efficient review schedule. A discriminator LLM then evaluates the soundness of each slice, giving concise, structured justifications. The reasoner is rewarded for logically consistent steps that lead to correct answers; the discriminator is rewarded for correctly identifying errors or distinguishing correct vs flawed reasoning traces. This adversarial RL co-evolution produces dense, on-policy, step-level rewards that augment sparse exact-match outcome rewards.", "result": "On several mathematical reasoning benchmarks, their method consistently outperforms strong RL post-training baselines. For example, on AIME24, performance of DeepSeek-R1-Distill-Qwen-7B improves from 54.0 to 61.3 accuracy (+7.3), and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0).", "conclusion": "Jointly training a reasoning LLM and a discriminator LLM via adversarial reinforcement learning, with step-wise evaluation of reasoning slices, yields denser and better-calibrated rewards. This improves credit assignment and sample efficiency, leading to higher-quality mathematical reasoning. The modular discriminator also enables flexible reward shaping for other objectives like distillation, preference alignment, and proof-based reasoning."}}
{"id": "2512.16855", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.16855", "abs": "https://arxiv.org/abs/2512.16855", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge", "comment": "Published in the IEEE ICCAD 2025 conference", "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.", "AI": {"tldr": "TOGGLE is a formal-methods\u2013guided framework that compresses LLMs via quantization and pruning while guaranteeing preservation of specified linguistic properties.", "motivation": "LLMs are computationally heavy, making it hard to deploy them on edge devices. Existing compression methods reduce cost but can harm essential linguistic behaviors and provide no formal guarantees that these properties are preserved. There is a need for a compression approach that is both efficient and comes with verifiable guarantees about model behavior.", "method": "The authors introduce TOGGLE, which uses Signal Temporal Logic (STL) to formally specify desired linguistic properties (e.g., consistency, ordering, or response patterns). They define an STL robustness measure that evaluates how well a model satisfies these properties. Then they apply Bayesian optimization over layer-wise quantization and pruning configurations, using the STL robustness as an objective/constraint. The search yields compressed models that still satisfy the STL constraints, and this is done without retraining or fine-tuning the LLMs.", "result": "On four LLMs (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), TOGGLE finds compressed models that meet all specified STL-based linguistic properties while significantly reducing resource usage: up to 3.3\u00d7 fewer FLOPs and up to 68.8% reduction in model size.", "conclusion": "TOGGLE is the first framework to apply formal methods, specifically STL-guided robustness and Bayesian optimization, to LLM compression. It shows that one can achieve substantial efficiency gains on LLMs for edge deployment while providing formal guarantees about preserving desired linguistic behaviors, all without model retraining or fine-tuning."}}
{"id": "2512.16856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16856", "abs": "https://arxiv.org/abs/2512.16856", "authors": ["Nenad Toma\u0161ev", "Matija Franklin", "Julian Jacobs", "S\u00e9bastien Krier", "Simon Osindero"], "title": "Distributional AGI Safety", "comment": null, "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "AI": {"tldr": "The paper argues that AI safety research must also address \"patchwork AGI\" arising from coordinated sub-AGI agents, proposing sandboxed agent economies with market mechanisms, reputation, and oversight to manage collective risks.", "motivation": "Most AI safety work assumes a single, monolithic AGI. However, increasingly capable tool-using AI agents that can communicate and coordinate may collectively behave like an AGI before any single system is generally intelligent. This collective route to AGI-like capability is under-explored and poses urgent safety concerns.", "method": "Conceptual/theoretical: the authors articulate the \"patchwork AGI\" hypothesis and propose a new safety framework called distributional AGI safety. They outline the design of virtual sandbox economies where multiple agents interact under constrained, auditable, and overseen market-like mechanisms.", "result": "The paper provides a structured framework and design principles for virtual agentic sandbox economies (impermeable or semi-permeable) that govern agent-to-agent transactions via robust markets, auditability, and reputation systems, aiming to mitigate systemic risks from coordinated agents.", "conclusion": "AI safety must expand from focusing on aligning single systems to managing risks arising from networks of interacting agents that can collectively reach AGI-level capability. Carefully designed sandboxed economies with strong governance, auditing, and reputation can be a promising direction for controlling such distributional AGI systems."}}
{"id": "2512.16873", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16873", "abs": "https://arxiv.org/abs/2512.16873", "authors": ["Otman A. Basir"], "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI", "comment": null, "summary": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.", "AI": {"tldr": "The paper proposes a six-layer \u201cSocial Responsibility Stack\u201d (SRS) architecture that embeds societal values as explicit, enforceable constraints throughout the AI system lifecycle, unifying ethics, control theory, and governance into a closed-loop supervisory framework.", "motivation": "AI systems increasingly influence critical decisions and social outcomes, but current responsible AI guidelines are mostly high-level principles without concrete, enforceable engineering mechanisms spanning design, deployment, and oversight. There is a need for a systematic way to translate normative goals\u2014like fairness, autonomy, and transparency\u2014into operational constraints and controls within socio-technical systems.", "method": "The authors conceptualize responsibility as a closed-loop supervisory control problem and design a six-layer Social Responsibility Stack (SRS) architecture. They develop a unified constraint-based formulation for embedding societal values into AI systems and interpret these constraints through safety-envelope and feedback-control perspectives. The framework integrates design-time safeguards, runtime monitoring, behavioral interfaces, and institutional governance mechanisms. They demonstrate its application via case studies in clinical decision support, cooperative autonomous vehicles, and public-sector AI systems.", "result": "The SRS framework enables continuous monitoring and enforcement of multiple social and ethical metrics\u2014such as fairness, user autonomy, cognitive burden, and explanation quality\u2014across the AI lifecycle. The case studies show that normative objectives can be turned into explicit constraints, system interfaces, audit trails, and governance procedures, illustrating how social values can be operationalized in real deployments.", "conclusion": "The paper concludes that the Social Responsibility Stack provides a practical, architectural foundation for building AI systems that are accountable, adaptive, and auditable. By integrating ethical principles with control-theoretic concepts and institutional governance, SRS offers a way to embed societal values as first-class, enforceable elements of socio-technical AI systems, moving beyond aspirational guidelines toward concrete, lifecycle-wide responsibility mechanisms."}}
