<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 92]
- [cs.AI](#cs.AI) [Total: 98]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI is a comprehensive benchmark to evaluate LLM performance in realistic patient-clinician dialogues along 105 clinically grounded dimensions, revealing that current flagship models perform poorly, especially in differential diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing medical QA benchmarks are largely single-turn and fail to capture the complexity of real patient-clinician interactions, including longitudinal reasoning, communication skills, safety, and outcomes. There is a need for a standardized, clinically meaningful way to evaluate how well LLMs can function as virtual clinicians across the full care process, aligned with professional accreditation standards.

Method: The authors design MedPI, a multi-layer benchmark: (1) synthetic EHR-like Patient Packets as ground truth; (2) an AI Patient agent implemented via an LLM with memory and affect to simulate realistic conversations; (3) a Task Matrix spanning multiple encounter reasons and objectives to generate diverse scenarios; (4) a 105-dimension evaluation rubric on a 1–4 scale mapped to ACGME competencies, covering medical process, treatment safety, treatment outcomes, and communication; and (5) AI Judge LLMs configured as calibrated committees that score conversations, highlight issues, and provide evidence-based rationales. They then run 9 leading LLMs through 366 AI patients and 7,097 conversations using a standardized clinician prompt and score them across all dimensions.

Result: Across all 9 flagship models and thousands of simulated clinical encounters, scores are generally low across MedPI’s 105 dimensions, with particularly weak performance in differential diagnosis. This suggests that none of the evaluated LLMs reliably meet high standards for complex clinical reasoning or comprehensive patient management in dialogue form.

Conclusion: MedPI reveals substantial gaps between current LLM capabilities and the requirements for safe, competent clinical dialogue, especially in diagnostic reasoning. The benchmark provides a structured, accreditation-aligned framework that can be used to stress-test, compare, and iteratively improve LLMs intended for diagnostic and treatment support, and to inform more cautious, well-scoped deployment in real-world healthcare settings.

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: RAGVUE is a diagnostic, reference-free evaluation framework for Retrieval-Augmented Generation that decomposes performance into multiple interpretable metrics with explanations.


<details>
  <summary>Details</summary>
Motivation: Evaluating RAG systems is difficult because existing metrics compress diverse behaviors into a single score and do not clearly indicate whether problems stem from retrieval, reasoning, or grounding. This lack of diagnostic detail limits researchers’ and practitioners’ ability to debug and improve RAG pipelines.

Method: The authors design RAGVUE, a framework that breaks down RAG evaluation into distinct components: retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each component is computed automatically and paired with structured explanations. The system can be used either with hand-picked metrics or via a fully automated agentic evaluation flow. They implement multiple interfaces (Python API, CLI, local Streamlit app) to support different usage scenarios and integrate the framework into comparative experiments against existing tools like RAGAS.

Result: In experiments comparing RAGVUE to existing evaluators, RAGVUE reveals detailed, fine-grained failure modes in RAG pipelines that tools like RAGAS tend to overlook. The framework successfully produces structured explanations for each metric and supports practical workflows across code-based and interactive interfaces.

Conclusion: RAGVUE provides a more diagnostic, transparent, and reference-free way to evaluate RAG systems by decomposing performance into interpretable metrics with explanations. It improves practitioners’ ability to identify whether issues originate in retrieval, answer generation, or grounding, and can be readily integrated into both research experiments and real-world RAG development pipelines. The framework and codebase are publicly released for community use.

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: The paper introduces an unsupervised method to automatically build a Chinese verb collostruction database and shows it can outperform LLMs in verb grammatical error correction.


<details>
  <summary>Details</summary>
Motivation: LLMs often lack explicit, interpretable grammatical rules, which are needed in applications where explainability is crucial. Existing resources for Chinese verb usage and constructions are limited, so there is a need for an automatic and interpretable way to model verb–construction patterns for tasks like grammatical error correction.

Method: The authors formally represent a verb collostruction as a projective, rooted, ordered, directed acyclic graph. From large-scale corpora, they retrieve sentences containing a target verb, then apply a series of clustering algorithms over these sentence representations to induce collostructions. They conduct statistical analyses to test structural properties (functional independence and graded typicality) and implement a verb grammatical error correction system that uses maximum matching between input sentences and the learned collostructions.

Result: The induced collostructions exhibit functional independence and graded typicality according to the authors’ statistical tests. In verb grammatical error correction experiments, the collostruction-based maximum-matching algorithm outperforms LLM-based approaches on their evaluation metrics.

Conclusion: Unsupervised, graph-based induction of verb collostructions from large corpora can yield a useful, interpretable database of Chinese verb constructions. This database not only captures meaningful structural properties but also enables a grammatical error correction system that can surpass LLM performance, highlighting the value of explicit, rule-like representations alongside LLMs.

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: They propose a systematic LLM-based framework to synthesize realistic, labeled e-commerce product data that respects store constraints and boosts product information extraction models, especially in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled product data is essential for training product information extraction systems, but it is costly and difficult to obtain at scale. Existing datasets are limited, and zero-shot LLM performance is much lower than supervised models, motivating a controlled way to generate synthetic training data.

Method: They design a controlled modification framework using a state-of-the-art LLM with attribute-aware prompts. The framework uses three strategies: (1) attribute-preserving modification, which rewrites product descriptions while keeping attributes intact; (2) controlled negative example generation, which creates realistic but incorrect examples; and (3) systematic attribute removal, which omits certain attributes to simulate incomplete data. The LLM is guided by store constraints to ensure attribute validity and product coherence.

Result: Human evaluation on 2000 synthetic products shows that 99.6% are natural, 96.5% have valid attribute values, and over 90% maintain consistent attribute usage. On the MAVE dataset, models trained on synthetic data alone reach 60.5% accuracy, comparable to 60.8% from real data and well above a 13.4% zero-shot baseline. Combining synthetic and real data in hybrid training configurations further improves accuracy to 68.8%.

Conclusion: The proposed LLM-based synthetic data generation framework can reliably produce high-quality, constraint-respecting e-commerce product data that matches or enhances the performance of models trained on real data. It is especially useful for augmenting datasets in low-resource settings, offering a practical way to improve product information extraction systems.

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: The paper proposes Collective Narrative Grounding, a participatory method to turn rich community stories into structured, governable knowledge for LLM-based QA systems, reducing local "knowledge blind spots" and epistemic injustice.


<details>
  <summary>Details</summary>
Motivation: LLM question-answering systems perform poorly on community-specific, local questions, leading to factual gaps, cultural misunderstandings, and other errors that marginalize local knowledge and voices. There is a need for systematic, participatory ways to incorporate community narratives into AI systems under local governance to address epistemic injustice and improve local QA performance.

Method: The authors run three participatory mapping workshops with 24 community members to collect community stories, then design elicitation methods and a narrative schema that preserve narrative richness while enabling structured extraction of entities, time, and place. They add mechanisms for validation and provenance control under community governance. They also audit a large county-level benchmark of 14,782 local QA pairs to categorize LLM error types, and construct a participatory QA set derived from the workshops to evaluate a state-of-the-art LLM, with and without added contextual narratives.

Result: The audit of 14,782 county-level QA pairs shows that four error modes—factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments—account for 76.7% of LLM errors on local questions. On the participatory QA set, a state-of-the-art LLM correctly answers fewer than 21% of questions without additional context, indicating strong limitations in local grounding. Many missing facts in incorrect answers are found within the collected community narratives, indicating that the proposed narrative grounding protocol could directly address prevalent error types, especially for narrative items.

Conclusion: Collective Narrative Grounding provides a structured, participatory approach for encoding community narratives into AI systems while preserving narrative richness, ensuring provenance, and enabling community governance. The work identifies dominant error modes in local QA, shows that community narratives can supply missing knowledge, and surfaces key design tensions around representation, power, governance, control, privacy, and consent. The authors argue this leads to concrete design requirements for retrieval-first, provenance-visible, locally governed QA systems and lays a rigorous foundation for developing community-grounded AI that better answers local questions and mitigates epistemic injustice.

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: The paper introduces TeleTables, a benchmark to test how well language models know and interpret table-heavy 3GPP telecom standards, revealing that small models perform poorly while larger models do better and motivating domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Telecom engineering relies heavily on dense technical standards like 3GPP, which use tables to encode critical information. Existing work shows LLMs struggle with these standards, but it is unclear whether this is due to lack of domain knowledge, difficulty with tables, or both. There is no dedicated benchmark to systematically test LLMs’ ability to recall telecom-standard knowledge and to interpret complex technical tables. The authors aim to fill this evaluation gap and better understand LLM limitations in this domain.

Method: The authors construct TeleTables via a multi-stage data generation pipeline. They automatically extract tables from 3GPP telecom standards, then employ multimodal and reasoning-oriented LLMs to generate questions based on these tables and to propose answers. Additional validation steps, including human verification, ensure question correctness and quality. Each question-answer pair is linked to its source table in several formats (e.g., raw, structured). They then evaluate a range of LLMs of different sizes on both implicit knowledge (recalling 3GPP content) and explicit table-interpretation tasks using the TeleTables benchmark.

Result: TeleTables consists of 500 human-verified question-answer pairs tied to specific 3GPP tables and released publicly. Experimental evaluation shows that smaller LLMs (under 10B parameters) perform poorly at both recalling 3GPP-specific knowledge and interpreting complex tables, indicating weak domain coverage and limited inductive bias for structured technical content. Larger models achieve substantially better performance, particularly on reasoning and interpretation over tables.

Conclusion: The benchmark reveals that current LLMs—especially smaller ones—are not reliable for tasks that require understanding or recalling details from table-heavy telecom standards. Larger models fare better but are still not perfect. TeleTables underscores the importance of domain-specialized fine-tuning and potentially architectural or training adjustments to improve LLMs’ ability to interpret and reason over complex technical tables in telecom specifications.

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk is a benchmark and evaluation framework for conversational, multi-modal front-end code generation, highlighting challenges like catastrophic forgetting and poor visual feedback understanding, and proposing AceCoder to mitigate forgetting.


<details>
  <summary>Details</summary>
Motivation: Front-end development heavily relies on visual artifacts (sketches, mockups, annotated screenshots) to communicate design intent, but existing code generation benchmarks and studies largely focus on single-turn, text-only settings and back-end tasks. There is little systematic understanding of how models handle multi-turn interactions that include both text and visual feedback, especially regarding functional correctness and user experience in real web interfaces. The authors aim to fill this gap by creating a realistic, dialogue-based benchmark and corresponding evaluation framework for front-end code generation with multi-modal inputs.

Method: The authors curate FronTalk, a benchmark of 100 multi-turn dialogues derived from real-world websites spanning domains like news, finance, and art. Each dialogue turn includes both a textual and an equivalent visual instruction (e.g., screenshots or mockups) conveying the same user intent. They build an agent-based evaluation framework that uses a web agent to simulate user interactions with the generated websites, enabling quantitative measurement of functional correctness and user experience. They evaluate 20 different models, including vision-language models, on this benchmark. Observing major failure modes, they introduce AceCoder, a method that employs an autonomous web agent to critique the implementation of every past instruction at each step, explicitly checking for regressions and forgotten requirements.

Result: Through benchmarking 20 models, the authors identify two key systematic challenges: (1) a severe forgetting problem where models frequently overwrite or break previously implemented features, causing task failures, and (2) difficulty in accurately interpreting and using visual feedback, particularly in open-source vision-language models. Their proposed AceCoder baseline, which continuously critiques prior instructions via an autonomous web agent, drastically reduces forgetting to nearly zero and boosts task performance by up to 9.3 percentage points (from 56.0% to 65.3%).

Conclusion: FronTalk establishes a new, realistic benchmark and evaluation framework for multi-turn, multi-modal front-end code generation, revealing critical under-explored issues such as catastrophic forgetting and weak visual feedback utilization. The AceCoder baseline shows that agent-based critique of historical instructions can meaningfully mitigate forgetting and improve performance. The authors position FronTalk and their evaluation setup as foundational resources to drive future work on front-end development automation and, more broadly, on the dynamics of conversational, multi-modal code generation systems.

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: They propose a dynamic, token-wise remasking strategy for diffusion language models that adapts over time and space, yielding large speedups without hurting text quality.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion language models use a fixed, global confidence threshold for remasking tokens at each denoising step. This ignores how individual tokens evolve over time and interact with others, which leads to redundant iterations, suboptimal parallelism, and inefficiencies in generation.

Method: They introduce a new remasking mechanism that, at every timestep, computes two signals for each token: Temporal Variance (how its predictions change over time, indicating convergence) and Spatial Deviance (how much it disagrees with neighboring or related tokens, indicating inconsistency). Based on these signals, the method dynamically adjusts a per-token confidence threshold, deciding which tokens to finalize and which to remask and refine in subsequent steps.

Result: On several standard text generation benchmarks, the dynamic, token-wise remasking scheme greatly improves the efficiency of diffusion language models, delivering up to 8.9x speedups in generation while maintaining comparable or unchanged output quality metrics versus baselines with fixed global thresholds.

Conclusion: Incorporating temporal and spatial adaptivity into the remasking strategy of diffusion language models provides a more efficient denoising schedule. By exploiting token-wise convergence and inter-token correlations, their approach substantially accelerates generation without sacrificing quality, suggesting that finer-grained control over the decoding process is key to practical DLM deployment.

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low- priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spa- tial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical re- sults show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: They build and optimize a hybrid AI system for university admissions inquiry handling by combining RAG with a domain‑fine‑tuned language model to improve accuracy, contextual adequacy, and speed of responses.


<details>
  <summary>Details</summary>
Motivation: Admissions offices receive many inquiries and must respond quickly and accurately, as response quality affects how prospective students perceive the institution. Standard RAG systems may struggle in this narrow, rule‑heavy domain, sometimes returning contextually inadequate answers because they lack deep, specific admissions knowledge. The paper is motivated by the need for an AI solution that both understands nuanced admissions rules and can stay up to date with current information.

Method: They design an AI system that integrates Retrieval‑Augmented Generation with a language model fine‑tuned on a curated admissions‑specific dataset. RAG retrieves relevant, current information from large admissions‑related data sources, while the fine‑tuned model is trained to interpret this information correctly in context and generate domain‑appropriate responses. They also systematically explore and tune response‑generation settings and logic (e.g., parameters and workflows) to balance response quality and latency.

Result: The resulting hybrid system shows improved ability to answer admissions inquiries with higher contextual accuracy and domain relevance compared with using RAG alone. Through optimization of generation settings, the system achieves a better trade‑off between response speed and answer quality, leading to more consistent, reliable outputs suitable for admissions communications.

Conclusion: Combining RAG with domain‑specific fine‑tuning is an effective approach for complex, narrow domains like university admissions. RAG supplies up‑to‑date, wide‑coverage information, while fine‑tuning embeds nuanced procedural and rule‑based knowledge, together yielding more accurate and appropriate responses. With additional optimization of generation logic, such systems can meet both the quality and efficiency demands of real‑world admissions offices, improving handling of large volumes of applicant inquiries.

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: The paper studies how large language models (LLMs) internally represent political ideology and proposes a simple method to measure and correct systematic misalignment between model ideology and human/user ideology without retraining.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how LLMs encode political ideology, to identify systematic misalignments between model-internal ideological structure and human ideological space, and to provide a practical way to steer model outputs toward specific user opinions without expensive retraining or loss of reasoning ability.

Method: They analyze the internal representations of LLMs and show that political ideology is organized along low-dimensional directions. They then build a lightweight linear probe on these internal features to (1) estimate a bias/misalignment score and (2) minimally adjust the final output layer logits or probabilities. The adjustment uses a linear transformation derived from the probe, effectively reweighting outputs based on the measured ideological bias while leaving the rest of the model unchanged.

Result: They find that the ideological space learned by LLMs is only partially aligned with human ideological dimensions and that this misalignment is systematic and model specific. The proposed linear probe can both quantify the misalignment and, when used to modify the output probabilities, shift the model’s responses to better match desired user opinions, with negligible computational overhead and without retraining.

Conclusion: Political ideology in LLMs is encoded in low-dimensional internal directions that do not perfectly match human ideological space but can be measured. A simple linear probing and output-adjustment technique can cheaply correct for this misalignment, enabling alignment with specific user preferences while largely preserving the model’s original reasoning capabilities.

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA is a reinforcement-learning-based fine-tuning framework that uses LLMs to generate audience-specific, narrative explanations for high-stakes decisions while improving prediction accuracy, all without requiring large sets of human-labeled explanations.


<details>
  <summary>Details</summary>
Motivation: Existing explainable AI largely relies on post hoc numerical feature attributions, which are hard for end-users to interpret and do not form coherent narratives. Large language models can produce natural-language explanations, but there are open challenges: ensuring explanations are both decision-correct and faithful, supporting multiple audiences (e.g., experts vs. consumers) without changing the underlying decision rule, and achieving all this without expensive human-labeled explanation datasets. There is a need for a systematic, label-efficient method to fine-tune LLMs to generate high-quality, tailored explanations in high-stakes domains such as consumer finance.

Method: The paper introduces LEXMA, an RL-based fine-tuning framework for LLMs that produces narrative, audience-appropriate explanations for model decisions. LEXMA uses reflection-augmented supervised fine-tuning followed by two stages of Group Relative Policy Optimization (GRPO). It maintains two parameter sets: one optimized for decision correctness and another for meeting stylistic and audience-related explanation requirements. Reward signals are constructed to guide correctness and style without relying on human-annotated explanations. The framework is instantiated on the task of mortgage approval decision-making, where the model both predicts decisions and explains them to different audiences (e.g., experts vs. consumers).

Result: In mortgage approval experiments, LEXMA outperforms other LLM-based baselines in predictive performance. Human evaluations indicate that explanations tailored for expert audiences are more risk-focused, while those for consumers are clearer, more actionable, and more polite. These results show that LEXMA can successfully balance decision quality with audience-appropriate explanatory style using label-efficient RL-based fine-tuning.

Conclusion: LEXMA provides a cost-efficient, scalable framework to fine-tune LLMs for high-quality, narrative explanations in high-stakes decision settings, such as mortgage approvals. By disentangling decision correctness from stylistic and audience-specific requirements and using RL without large human-labeled explanation corpora, it advances practical, transparent, and multi-audience explainable AI. The approach demonstrates that LLMs can be systematically adapted to generate faithful, audience-tailored explanations while enhancing predictive performance, supporting broader deployment of transparent AI systems in business contexts.

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: They built and tested a locally deployed retrieval-augmented LLM system that recommends research collaborators in a hospital setting using PubMed data.


<details>
  <summary>Details</summary>
Motivation: Hospitals want to use LLMs for clinical and research tasks but face strict privacy and network security rules, so they need solutions that run fully on local infrastructure while still enabling knowledge discovery and collaboration.

Method: They designed a retrieval-augmented generation (RAG) pipeline that indexes PubMed publications authored by institutional members, uses PubMedBERT to create biomedical text embeddings, retrieves relevant papers, and then applies a locally hosted LLaMA3 model to synthesize recommendations for potential research collaborators.

Result: The implemented system functioned effectively as a collaborator recommender using only local compute resources, showing that domain-specific encoders and compact LLMs can be combined to deliver useful research-support tools within constrained hospital IT environments.

Conclusion: Integrating specialized biomedical encoders like PubMedBERT with lightweight, locally deployed LLMs in a RAG framework is a practical and useful approach for enabling research collaboration and knowledge discovery in privacy-sensitive medical institutions.

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI is an automated, explainable age-rating and content-safety system for Russian-language screenplays that complies with Federal Law No. 436-FZ, operates under strict compute and latency constraints, and achieves high accuracy in both segmentation and rating.


<details>
  <summary>Details</summary>
Motivation: There is a practical need in the Russian media industry to automatically assess and age-rate long screenplays according to the legal requirements of Federal Law No. 436-FZ. Manual review is slow, costly, and difficult to scale, especially for full-length scripts hundreds of pages long, while editorial teams require fast, reliable, and explainable decisions for production workflows and compliance. The hackathon setting (Wink, November 2025) provided a concrete, real-world challenge from industry stakeholders.

Method: The authors designed an end-to-end pipeline that ingests full-length Russian-language screenplays (up to 700 pages). The system segments scripts into narrative units, detects content violations along five regulated categories (violence, sexual content, profanity, substances, frightening elements), and then assigns legal age ratings (0+, 6+, 12+, 16+, 18+). They fine-tuned a Phi-3-mini language model, applied 4-bit quantization to fit hardware constraints, and implemented the pipeline without external API calls. The system is deployed on Yandex Cloud with CUDA acceleration, operating within an 80GB VRAM budget and under 5 minutes processing time for typical scripts.

Result: The system processes up to 700-page scripts in under 2 minutes, satisfies the imposed infrastructure constraints (no external APIs, 80GB VRAM, <5-minute latency), and achieves about 80% accuracy in age rating. Script segmentation into narrative units reaches 80–95% precision depending on script format. The deployed solution runs in production-like conditions on Yandex Cloud with GPU acceleration and successfully addressed editorial needs during the Wink 2025 hackathon.

Conclusion: Qwerty AI demonstrates that a compact, fine-tuned LLM (Phi-3-mini) with quantization can deliver fast, legally aligned, and reasonably accurate age-rating and content-safety analysis for long Russian-language screenplays under strict resource and latency constraints. The system is practical for real production workflows, offering explainable justifications for ratings and handling full-length scripts efficiently, making it a viable tool for media companies operating under Federal Law No. 436-FZ.

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [14] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: The paper proposes TrueBrief, a framework to reduce hallucinations and improve faithfulness of small language models in text summarization using preference-based optimization and synthetic training data with controlled hallucinations.


<details>
  <summary>Details</summary>
Motivation: Large language models generate strong text but often hallucinate, which is especially problematic in security-critical applications. Small LLMs are attractive for efficiency and deployment reasons but need better faithfulness for tasks like summarization. There is a need for systematic methods to improve factual consistency and to understand how data quality and model size affect preference-optimization approaches.

Method: They design an end-to-end framework called TrueBrief focused on small LLMs for summarization. A core component is a data generation module that injects controlled hallucinations into outputs to create synthetic preference pairs (faithful vs hallucinated). These pairs are used in a preference-optimization paradigm (e.g., learning from human or synthetic preferences) to train or fine-tune SLMs to prefer faithful summaries. They also run empirical analyses varying data quality and model size to study their effects on optimization effectiveness.

Result: TrueBrief improves the faithfulness of small LLMs on summarization tasks by leveraging synthetic preference data with controlled hallucinations. Experiments show that preference-based optimization can be effective under appropriate conditions and that both the quality of preference data and model size significantly influence performance gains.

Conclusion: The proposed TrueBrief framework provides a practical path to make small LLMs more faithful for summarization in settings where hallucinations are costly. Controlled hallucination injection enables scalable preference data creation, and the study clarifies when preference-based optimization is most beneficial in terms of data quality and model capacity.

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [15] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: AnimatedLLM is a browser-based interactive tool that visually explains how Transformer language models work using step-by-step animations based on pre-computed traces of open LLMs.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are increasingly central in NLP education, there is a lack of intuitive, visual teaching materials that expose their inner mechanics. This paper aims to fill that gap by providing an accessible, interactive resource that helps students and self-learners build a concrete understanding of how Transformer-based language models operate.

Method: The authors develop an interactive web application, AnimatedLLM, that runs entirely in the browser. They pre-compute traces (intermediate states and computations) of open-source Transformer language models on carefully selected, manually curated input examples. These traces are then used to generate step-by-step visualizations of the model’s internal processes, such as token embeddings, attention patterns, and layer-wise transformations, enabling users to explore the model’s behavior without heavy computation on the client side.

Result: The resulting system is a publicly accessible web app (https://animatedllm.github.io) that can be used in classrooms or for self-study. It successfully visualizes the internal workings of Transformer LLMs in a way that is lightweight enough to run fully in the browser, supported by the pre-computed traces. The curated examples and interaction design aim to make complex model behavior understandable to non-experts.

Conclusion: AnimatedLLM demonstrates that it is feasible and effective to use pre-computed model traces to support rich, interactive, browser-based visualizations of LLM internals. The tool contributes educational infrastructure for NLP and LLM courses, and the authors position it as a reusable teaching aid and self-education resource to improve conceptual understanding of Transformer language models.

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [16] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: The paper introduces BiForget, a framework that better constructs data to evaluate how well large language models can “forget” information, distinguishing between domain-level and instance-level unlearning.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning benchmarks for LLMs do not accurately capture what and how much the model has truly forgotten. They often rely on external data generators that may not align with the model’s actual internal knowledge, leading to misleading evaluations of unlearning performance. There is a need for a more faithful and fine-grained way to define and generate forget sets, especially at different granularities like whole domains versus specific instances.

Method: The authors formalize two granularities of unlearning: domain-level (entire thematic areas, e.g., Harry Potter universe) and instance-level (specific items or facts). They propose BiForget, an automated framework that synthesizes forget sets by directly leveraging the target model rather than external generators. BiForget uses seed-guided prompting to expand from a small set of seeds, and adversarial prompting to probe the model’s implicit knowledge distribution, thereby generating samples that are highly relevant to what the model actually knows while maintaining diversity and efficiency in data collection.

Result: Across several benchmarks, BiForget produces forget sets that outperform prior methods in relevance, diversity, and efficiency. In a Harry Potter domain benchmark, it increases relevance by about 20 points and diversity by about 0.05, while cutting the total data size roughly in half compared to state-of-the-art alternatives. These improvements lead to more effective and robust forgetting behavior when models are unlearned using these sets, while also preserving their utility on non-forgotten knowledge.

Conclusion: BiForget provides a more principled and practical way to construct forget sets for LLM unlearning evaluation, by aligning generated data with the model’s internal knowledge at both domain and instance levels. This yields more robust measurements of forgetting and better utility preservation, giving a stronger empirical foundation for future work on machine unlearning in large language models.

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [17] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: The paper introduces RIGOURATE, a framework that detects and scores overstatement in scientific paper claims by retrieving supporting evidence from the paper body and estimating how proportional the claims are to the evidence.


<details>
  <summary>Details</summary>
Motivation: Scientific papers often contain bold or exaggerated claims that are not fully supported by their empirical evidence, undermining scientific rigor and clear communication. There is a need for automated tools that can evaluate whether claims in a paper are proportionate to the supporting evidence and help authors, reviewers, and readers detect overstatement.

Method: The authors build RIGOURATE, a two-stage multimodal framework. First, they create a dataset of over 10K claim-evidence pairs from ICLR and NeurIPS papers. These are annotated with overstatement scores using eight large language models, and the scores are calibrated using peer-review comments and validated with human evaluation. Then, they design a two-stage system: (1) a fine-tuned reranker to retrieve the most relevant supporting evidence from the paper body for each claim; and (2) a fine-tuned model that takes the claim plus retrieved evidence and predicts an overstatement score along with a textual justification.

Result: RIGOURATE achieves better performance than strong baselines on both evidence retrieval and overstatement detection tasks. The framework can assign overstatement scores to claims and provide justifications that align better with peer-review assessments and human judgments than competing methods.

Conclusion: RIGOURATE provides an operational way to measure evidential proportionality in scientific writing, helping to identify when claims exceed what the evidence supports. By improving automated evidence retrieval and overstatement detection, the framework supports clearer, more rigorous, and more transparent scientific communication.

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [18] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: Empirical study on how well ASR models transfer across Indic dialects and codeswitched speech, showing that phylogenetic closeness helps but does not fully predict performance, and that small amounts of dialectal data can be very valuable.


<details>
  <summary>Details</summary>
Motivation: ASR systems are typically trained on standardized, high-resource languages, but in practice they are often applied to dialectal, noisy, and code-mixed speech from low-resource language varieties, especially in Indic contexts. It is unclear how language relatedness, data size, and dialectal variation interact for cross-lingual transfer, and how existing ASR models behave on such challenging speech. The authors aim to fill this gap and better understand what drives ASR performance in dialectal settings.

Method: They run an empirical evaluation of multiple contemporary ASR models on spontaneous, noisy, code-mixed speech spanning many Indic dialects and language varieties. They measure how ASR performance changes as the phylogenetic distance between training and target languages varies. They systematically compare fine-tuning strategies: using small amounts of in-dialect data versus larger datasets from closely related standardized languages. They conduct a focused case study on Garhwali, a low-resource Pahari language, and carry out qualitative and quantitative analysis of transcription errors to reveal biases toward pre-training languages.

Result: They find that while reduced phylogenetic distance between languages generally improves cross-lingual ASR transfer, it does not fully account for performance differences in dialectal contexts. Surprisingly, fine-tuning on relatively small amounts of dialect-specific data can achieve performance comparable to, or competitive with, fine-tuning on much larger datasets from high-resource, phylogenetically related standardized languages. Their error analysis reveals systematic biases in ASR outputs toward the models’ pre-training languages, particularly on dialectal and non-standard speech.

Conclusion: Phylogenetic relatedness is helpful but insufficient as a predictor of ASR performance on dialects and non-standard speech. Targeted fine-tuning on modest amounts of dialectal data can be highly effective, sometimes matching the benefits of large high-resource related-language datasets. Current ASR systems exhibit clear biases toward pre-training languages, which must be considered when deploying them for low-resource dialects like Garhwali and other non-standardized Indic varieties.

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [19] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: The paper proposes Disco-RAG, a discourse-aware RAG framework that injects discourse structure into retrieval-augmented generation, achieving state-of-the-art QA and summarization results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods treat retrieved passages as flat, unstructured text, limiting the model’s ability to use structural cues and synthesize knowledge spread across documents. The authors aim to leverage discourse structure to improve coherence, reasoning, and evidence integration in RAG systems.

Method: Disco-RAG builds intra-chunk discourse trees to model local hierarchical structure within each retrieved passage and constructs inter-chunk rhetorical graphs to capture relationships and coherence across multiple passages. These discourse representations are then combined into a unified planning blueprint that guides and conditions the generation process of an LLM in a discourse-aware manner. The approach is designed to work without fine-tuning the base LLM.

Result: On question answering and long-document summarization benchmarks, Disco-RAG outperforms prior RAG methods and achieves state-of-the-art performance. The gains are obtained even without fine-tuning the underlying large language model, indicating that the discourse-aware planning alone yields substantial improvements.

Conclusion: Discourse structure is a powerful signal for RAG: explicitly modeling intra-passage hierarchies and inter-passage rhetorical relations and feeding them into a planning-guided generation process significantly improves QA and summarization. Disco-RAG demonstrates that discourse-aware design can advance RAG systems beyond flat, unstructured usage of retrieved text.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [20] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: The paper introduces MiJaBench, a bilingual adversarial safety benchmark for LLMs focusing on minority-group–targeted hate, showing that current scalar safety metrics hide demographic-specific vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety evaluations compress hate or toxicity risk into a single scalar score like 'Identity Hate', which obscures how well models defend specific demographic groups and creates a misleading impression of universal safety. There is a need for a fine-grained, group-level evaluation to surface selective safety failures and challenge the assumption that alignment is uniformly effective across populations.

Method: The authors build MiJaBench, a bilingual (English and Portuguese) adversarial benchmark with 44k prompts targeting 16 minority groups. They query 12 state-of-the-art LLMs on these prompts to generate 528k prompt–response pairs. From this, they construct MiJaBench-Align, a curated dataset used to quantify 'defense rates'—how often models refuse or safely handle harmful prompts—per demographic group, and to analyze how these rates vary across groups and model scales.

Result: Analysis shows that safety alignment performance is highly uneven across demographic groups: within the same model, defense rates can differ by up to 33% depending solely on the targeted group. Larger, more capable models tend to exhibit even greater disparity, indicating that scaling increases demographic inequities in safety behavior rather than smoothing them out.

Conclusion: Safety alignment in current LLMs functions less as a general semantic capability and more as a demographic hierarchy of protections, with some minority groups receiving significantly weaker defenses. Scaling models with existing alignment methods appears to reinforce memorized refusal patterns for only certain groups instead of enforcing a principled non-discrimination norm. The authors release their datasets and code to support future work on fine-grained, demographic-aware alignment and to rethink scaling laws for safety.

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [21] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: The paper proposes ARREST, a post-hoc control framework that uses an external network to detect and correct latent drifts in LLM activations, jointly improving factuality and safety through both factual correction and calibrated refusals.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well but still hallucinate and generate unsafe content because they lack human-like cognitive mechanisms that balance imagination with grounded reality and social safety. Existing approaches often treat factuality and safety as separate alignment problems, typically relying on RLHF or fine-tuning, which can be inflexible and costly. The authors are motivated to find a unified, more interpretable way to regulate both factual and safety failures without modifying the base model parameters, and to make refusals more nuanced (soft vs hard) and robust to adversarial prompts.

Method: They conceptualize both hallucinations and unsafe generations as a single issue of representational misalignment in the model’s latent activation space. They then introduce ARREST, which adds an external network trained to detect and understand activation “drifts” that correlate with false or unsafe outputs. This controller intervenes on internal features at inference time to steer generations toward truthful and safe responses. The framework supports three types of interventions: (1) factual corrections, (2) soft refusals (polite, partial, or redirecting responses), and (3) hard refusals (full denials), and is trained adversarially to be robust to adversarial prompts, all without fine-tuning the LLM’s own weights.

Result: Experiments demonstrate that ARREST can reliably identify misaligned latent features and correct them, improving both factuality and safety. Compared to RLHF-aligned models, ARREST is more versatile in producing nuanced, soft refusals, especially under adversarial prompting conditions. Empirical evidence suggests that the method enhances safety and truthfulness jointly, while preserving the original model and operating as a post-hoc controller.

Conclusion: The paper concludes that factual and safety failures in LLMs can be treated as manifestations of a common representational misalignment problem. By training an external controller network to monitor and adjust latent activations, ARREST offers a unified, post-hoc framework that enhances both truthfulness and safety and enables calibrated refusals. This approach provides a more flexible alternative or complement to RLHF, with stronger adversarial robustness and without requiring fine-tuning of the base language model.

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [22] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: The paper advocates for mechanistic interpretability: understanding how neural networks make decisions at the level of their internal mechanisms.


<details>
  <summary>Details</summary>
Motivation: Neural networks are becoming increasingly capable and autonomous, yet their internal decision-making processes remain largely opaque, posing risks and missed opportunities.

Method: The abstract does not specify a concrete method; it conceptually proposes studying the internal mechanisms of neural networks (mechanistic interpretability) to explain their decisions.

Result: Not stated in the abstract; it only outlines the anticipated benefits of achieving mechanistic interpretability.

Conclusion: Developing mechanistic interpretability is important both for safety and accountability in critical applications, and for using advanced AI systems as scientific tools to study cognition and potentially discover new knowledge beyond human capabilities.

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [23] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: The paper evaluates how well modern long-context LLMs can summarize very large, multi-document legal cases and proposes a new evaluation framework plus an agent-based system to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Although LLMs now accept million-token contexts, it is unclear how reliably they handle truly complex, long legal cases spanning hundreds of thousands of tokens. Existing evaluations rely on single aggregate scores and human-written references, which may become unreliable as models approach or exceed human performance. There is a need for a fine-grained, domain-specific evaluation method and more efficient ways for LLMs to process huge legal document collections.

Method: The authors focus on multi-document legal case summarization with cases of 100K–500K tokens. They introduce Gavel-Ref, a reference-based evaluation framework that uses a 26-item multi-value checklist plus residual fact and writing-style evaluations, enabling detailed scoring beyond a single overall metric. They then benchmark 12 frontier LLMs on 100 large legal cases (32K–512K tokens, mainly from 2025). Anticipating limits of human references, they further design Gavel-Agent, an autonomous agent scaffold that equips LLMs with six specialized tools to navigate case documents and extract checklist items directly, reducing the need for full end-to-end long-context processing.

Result: Empirically, even the strongest evaluated model, Gemini 2.5 Pro, only scores around 50 on the proposed Gavel-Ref score, demonstrating the difficulty of large-scale legal summarization. Models succeed on straightforward checklist items like filing dates but perform poorly on complex, multi-value, or rare items such as settlements and monitor reports. Using Qwen3 as the backbone, Gavel-Agent decreases token consumption by 36% while incurring only a 7% drop in the checklist score compared with a more expensive end-to-end extraction baseline using GPT-4.1.

Conclusion: Current frontier LLMs, despite very long context windows, still struggle with complex, information-dense legal case summarization when documents are extremely large. Fine-grained evaluation via Gavel-Ref exposes weaknesses that are hidden by aggregate scores, especially on nuanced or rare legal attributes. The proposed Gavel-Agent framework shows that tool-augmented, agentic processing can substantially improve efficiency with minimal performance loss, pointing toward more scalable ways to apply LLMs to real-world, long-context legal tasks.

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [24] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: The paper studies why LLMs often go along with users’ harmful or mistaken assumptions and shows that this behavior can be explained using pragmatics, especially conversational accommodation. It demonstrates that small pragmatic cues (like prefacing with “wait a minute”) can make models more likely to challenge harmful beliefs without increasing over-refusal.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for advice and reasoning, but they often fail to correct users’ harmful or false beliefs, instead agreeing with them. Existing safety work focuses on alignment and content filters, but less on how conversational pragmatics shape model behavior. The authors are motivated to understand these failures as pragmatic accommodation problems and to leverage this perspective to improve safety and evaluation.

Method: The authors treat LLM behavior through the lens of pragmatic accommodation and epistemic vigilance. They test how factors known to affect human accommodation—such as whether content is at-issue, how it is linguistically encoded, and the perceived reliability of the source—affect LLM responses. Using three benchmarks that require challenging harmful beliefs (Cancer-Myth, SAGE-Eval for misinformation, and ELEPHANT for sycophancy), they systematically vary these factors and measure model performance. They then introduce simple prompt-based pragmatic interventions (e.g., adding “wait a minute”) and evaluate how these change rates of successful challenge and false positives.

Result: They find that LLMs mimic human-like patterns of accommodation: at-issueness, linguistic packaging, and source reliability all significantly modulate how often models accept or challenge harmful assumptions. These pragmatic effects explain performance differences across the safety benchmarks. Introducing minimal pragmatic cues like “wait a minute” before responding substantially improves the models’ tendency to challenge harmful beliefs on all three benchmarks, while keeping false-positive interventions low.

Conclusion: The paper concludes that many safety failures in LLMs are better understood as pragmatic accommodation issues rather than purely alignment or knowledge problems. Pragmatic factors systematically influence when models challenge or accept harmful beliefs, and simple conversational-style prompt changes can meaningfully improve safety behavior. Consequently, both safety evaluation and mitigation strategies should explicitly incorporate insights from pragmatics and epistemic vigilance.

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [25] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: The paper studies how best to train dialogue models to predict human next-turn responses, comparing judge-based rewards against direct likelihood training, with and without explicit “thinking” (chain-of-thought). It finds that matching human dialogue distributions and treating chain-of-thought as a latent variable yields the most human-like predictions.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting what a person will say next in a conversation is central to building conversational agents that model human thinking and behavior. Existing approaches often optimize against heuristic or LLM-based judges rather than directly matching human conversational data, and it is unclear whether explicitly allowing models to ‘think’ (via chain-of-thought) actually improves human-likeness in dialogue. The paper aims to clarify how training objectives and explicit reasoning affect a model’s ability to produce human-like next-turn dialogue.

Method: The authors frame next-turn dialogue prediction as predicting the next human utterance given a conversational context. They compare training strategies along two axes: (1) with or without an explicit “thinking” phase (e.g., chain-of-thought) before producing the final response; and (2) optimizing either (a) judge-based rewards, where an LLM-as-a-judge scores semantic similarity and information completeness relative to the true response, or (b) direct maximization of the log-probability of observed human dialogue (standard likelihood training). They further treat chain-of-thought as a latent variable and derive a variational-style lower bound on the log-probability objective, then optimize this bound. They evaluate systems by judge scores, likelihood of ground-truth responses, and human win rate when choosing the more human-like response between real and synthetic utterances.

Result: Training with LLM-judge-based rewards successfully increases the judge scores but paradoxically decreases both the likelihood of ground-truth human responses and the human win rate in pairwise comparisons of human vs. model responses. These negative effects are stronger when the model is allowed to think via chain-of-thought before responding. In contrast, maximizing the log-probability of human responses improves both the likelihood and the human win rate. When chain-of-thought is modeled as a latent variable and the derived lower bound on log-probability is optimized, the resulting models achieve the best performance across all evaluations.

Conclusion: Judge-based reward optimization can overfit to the judge metric and move models away from genuine human-like dialogue distributions, especially when combined with explicit chain-of-thought. Direct distribution matching via log-likelihood of real human dialogue leads to better prediction of what people actually say and higher human-perceived human-likeness. Modeling chain-of-thought as a latent variable within a likelihood-based framework further improves performance, indicating that ‘thinking’ helps when grounded in distribution-matching objectives. Scaling such methods on large conversational datasets may yield models with a deeper, more accurate understanding of human conversational behavior.

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [26] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: The paper introduces MB-Defense, a two-stage training framework to defend instruction-tuned LLMs against backdoor attacks by first merging attacker and defensive triggers into a unified backdoor and then breaking it through targeted retraining.


<details>
  <summary>Details</summary>
Motivation: Instruction-tuned LLMs rely on large human- or web-collected datasets, which can be subtly poisoned by adversaries to implant backdoor behaviors. Existing work has focused more on attacks than robust defenses for these instruction-tuned models, creating an urgent need for generalizable, data-efficient methods to safeguard them from such vulnerabilities.

Method: MB-Defense uses a two-stage pipeline: (1) Defensive poisoning: deliberately injects both attacker triggers and specially designed defensive triggers so they collapse into a single, shared backdoor representation in the model. (2) Weight recovery: performs additional training aimed at breaking this merged representation and restoring normal, clean behavior, thereby neutralizing the backdoor while keeping instruction-following capabilities intact.

Result: Experiments on multiple LLMs indicate that MB-Defense markedly decreases backdoor attack success rates while largely maintaining the models’ original instruction-following performance, suggesting robust protection with limited additional data and computation.

Conclusion: MB-Defense provides a practical, generalizable, and data-efficient defense strategy against a range of known and unseen backdoor attacks on instruction-tuned LLMs, strengthening their robustness without severely compromising utility.

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [27] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: The paper studies when users want proactive AI drafting help and finds a big mismatch between what users say and what they actually do.


<details>
  <summary>Details</summary>
Motivation: Designers of proactive AI writing assistants need to know what factors make users want drafting assistance. Existing work often relies on self-reported user preferences (e.g., urgency, effort) rather than observing actual choice behavior, which may lead to poorly tuned systems.

Method: The authors ran a factorial vignette study with 50 participants. Participants made 750 pairwise comparisons between scenarios that varied in factors like compositional effort and urgency, indicating which scenario they would prefer to receive AI drafting help. They then compared the predictive power of these factors on observed choices versus self-reported rankings and evaluated models designed from stated vs. behavioral preferences.

Result: Compositional effort was the strongest predictor of when users wanted help (correlation ρ=0.597), whereas urgency had essentially no predictive power (ρ≈0). However, in self-reports, users ranked urgency as the most important factor, creating a full preference inversion between stated and revealed preferences. Systems optimized using self-reported preferences reached only 57.7% accuracy in predicting help requests and even underperformed naive baselines, while systems based on behavioral data achieved significantly higher accuracy of 61.3% (p<0.05).

Conclusion: Relying on user introspection and stated preferences to design proactive NLG assistance is misleading and can degrade system performance. Behavioral signals—particularly those related to compositional effort—should drive the design and optimization of proactive AI writing assistants, as they better capture when users truly want drafting help.

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [28] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: The paper identifies limitations in current summary-based memory extraction for LLM agents and proposes ProMem, an iterative, self-questioning feedback process that yields more complete, accurate, and cost-effective long-term memory.


<details>
  <summary>Details</summary>
Motivation: LLM agents need reliable long-term memory for extended interaction and personalization, but existing approaches largely focus on how to organize and use memories after they are extracted. The crucial early step—how to extract memories from dialogue—has been underexplored. Based on recurrent processing theory, the authors argue that conventional summarization-based extraction is flawed because it happens once, ahead of time, without knowledge of future tasks, and without feedback to correct or recover lost information. This causes important details to be missed and errors to accumulate, hurting downstream performance.

Method: The authors propose ProMem, a proactive memory extraction framework that models extraction as an iterative cognitive process rather than a static summarization step. ProMem introduces a recurrent feedback loop in which the LLM agent engages in self-questioning over dialogue history: it actively probes past interactions, asking and answering questions to identify missing, uncertain, or incorrect information. The system repeatedly refines the extracted memory through these cycles, enabling error correction and recovery of previously omitted details, while controlling token usage to balance quality and cost.

Result: Experiments show that ProMem yields more complete and accurate extracted memories compared with standard summary-based methods, leading to higher question answering accuracy on tasks requiring long-term context. The approach achieves a better trade-off between extraction quality and token consumption, indicating improved efficiency as well as effectiveness in managing long-term memory for LLM agents.

Conclusion: Treating memory extraction as an iterative, recurrent, self-questioning process (ProMem) overcomes key limitations of static, ahead-of-time summarization. By incorporating a feedback loop that allows the agent to probe, refine, and correct its memory, ProMem improves completeness and factual accuracy of stored information, enhances QA performance, and does so with a favorable token-cost efficiency. This suggests that cognitively inspired, recurrent processing is a promising direction for advancing memory mechanisms in LLM agents.

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [29] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: Introduces Concept Tokens, special learned tokens for concepts that steer behavior of frozen LLMs using only definitional text.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate or fail to follow nuanced behavioral instructions; existing steering methods are heavy-weight (fine-tuning, prompt engineering, adapters) and may interfere with other instructions. The authors want a compact, data-efficient way to control model behavior around specific concepts (e.g., hallucination, pedagogical feedback styles) without modifying the base model.

Method: Add a new special token to a pretrained LLM for a target concept (e.g., hallucination, recasting). Replace occurrences of that concept in a definitional corpus with the special token. Freeze all model weights and optimize only the embedding of this token using the standard language modeling objective on the modified corpus. Then use this token (or its negation) in prompts to steer model behavior. Evaluate on closed-book QA hallucinations (HotpotQA), inducing a teaching strategy (recasting), and qualitative case studies (Eiffel vs. fictional Austral Tower).

Result: (1) In closed-book QA, using a negated hallucination token reduces hallucinated answers mainly by increasing abstentions, while asserting the token increases hallucinations and reduces precision, showing a clear directional control effect. (2) For language-teaching recasts, the token similarly induces or suppresses recasting behavior; compared to in-context use of the full definitions, Concept Tokens preserve adherence to other instructions (like asking follow-up questions) more reliably. (3) Qualitative analysis with Eiffel vs. fictional Austral Tower shows what factual and stylistic information the learned embedding captures and reveals limits when the concept is under- or mis-specified.

Conclusion: Concept Tokens act as compact, learned control handles: from only definitional text and without changing base LLM weights, they provide a steerable signal that can directionally modulate behaviors such as hallucination and teaching style, while interacting better with other instructions than long in-context definitions. They offer a lightweight alternative for behavior steering in frozen LLMs, though their capabilities are bounded by the information encoded in the definitional corpus.

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [30] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: They build an automatic, corpus-free toolkit to extract clean morpheme lexicons for Uralic languages and use it to rigorously evaluate BPE tokenizers and optimal vocabulary sizes, showing BPE’s limitations for agglutinative languages.


<details>
  <summary>Details</summary>
Motivation: Subword tokenization quality is crucial for LLM performance, but for morphologically rich Uralic languages there are no clean morpheme lexicons to objectively evaluate tokenizers. This gap makes it hard to choose vocabulary sizes or compare methods, especially in low-resource settings.

Method: They propose SampoNLP, a corpus-free toolkit based on MDL-inspired Self-Referential Atomicity Scoring. The method infers morpheme-like units and filters out composite forms using internal structural cues, yielding high-purity morphological lexicons for Finnish, Hungarian, and Estonian. With these lexicons, they systematically evaluate standard BPE tokenizers over a wide range of vocabulary sizes (8k–256k), and introduce the Integrated Performance Score (IPS), a unified metric that balances morpheme coverage against over-splitting. They analyze IPS curves to detect elbow points where increasing vocabulary size yields diminishing returns.

Result: SampoNLP produces high-purity morphological lexicons for Finnish, Hungarian, and Estonian without needing large corpora. Using these lexicons, they generate IPS curves for a spectrum of BPE vocabulary sizes, identify elbow points, and quantify how well BPE respects morpheme boundaries versus over-segmenting. The results reveal clear optimal ranges of vocabulary sizes for these languages and empirically highlight BPE’s weaknesses on highly agglutinative morphology.

Conclusion: SampoNLP enables the automatic construction of reliable morphological lexicons in low-resource, morphologically rich Uralic languages, making systematic tokenizer evaluation possible. Their IPS metric and analysis yield the first empirically grounded recommendations for BPE vocabulary size selection in Finnish, Hungarian, and Estonian, and provide quantitative evidence that standard BPE is suboptimal for highly agglutinative languages. The toolkit and resulting resources are released openly to support further research.

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [31] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper introduces a new benchmark, taxonomy, and baseline models for precise detection and localization of non-verbal vocal events (like laughter, crying) in speech, addressing gaps in category coverage, temporal granularity, and evaluation standards.


<details>
  <summary>Details</summary>
Motivation: Existing speech processing focuses mainly on linguistic/semantic transcription, neglecting accurate detection and temporal localization of non-verbal vocal events. Current approaches have limited event categories, poorly defined temporal boundaries, and no standardized evaluation, which restricts progress and application development for richer auditory scene understanding.

Method: 1) Propose a refined taxonomy of 21 vocal event types, dividing them into discrete (standalone) and continuous (overlapping with speech) events. 2) Construct WESR-Bench, an expert-annotated evaluation benchmark with 900+ utterances using a position-aware annotation/evaluation protocol that separates ASR transcription errors from event detection errors, enabling precise localization evaluation for both discrete and continuous events. 3) Build a 1,700+ hour training corpus and train specialized models for vocal event detection and localization. 4) Empirically compare these specialized models with open-source audio-language models and commercial APIs on the new benchmark.

Result: The specialized models trained on the new 1,700+ hour corpus achieve better performance in vocal event detection and localization than existing open-source audio-language models and commercial APIs, while maintaining competitive automatic speech recognition (ASR) quality. The benchmark and taxonomy support more accurate and fine-grained evaluation of both discrete and continuous vocal events.

Conclusion: WESR—a combination of the refined vocal event taxonomy, the WESR-Bench evaluation set with position-aware protocol, and strong baseline models—provides a foundational resource and standardized framework for future research on modeling rich, real-world auditory scenes that include both speech and diverse non-verbal vocal events.

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [32] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: The paper introduces LinguaGame, a linguistically grounded, game-theoretic framework that models multi-agent LLM dialogues as signalling games over communicative intents and strategies to improve communication efficiency without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems mostly optimize architectural aspects such as role assignment and workflows, but they underexplore how agents actually use language to convey meaning efficiently. The authors want to improve the interaction process itself—making agents better at expressing and interpreting intentions—while avoiding heavy task-specific tuning and instead grounding the framework in general linguistic and game-theoretic principles.

Method: The authors propose LinguaGame, which treats dialogue between LLM agents as a signalling game where agents have communicative intents (what they want to achieve) and strategies (how they express those intents). They design a linguistically informed game-theoretic model and use a training-free equilibrium approximation algorithm at inference time to adjust agents' decisions, i.e., to choose utterances that better encode their intents given expectations about others' beliefs and strategies. The framework emphasizes minimal coupling to specific downstream tasks, using general notions of intentional and strategic communication instead.

Result: They implement LinguaGame in LLM-based multi-agent settings and test it in simulated courtroom proceedings and debates. Human experts evaluate the resulting dialogues and find that, compared to baselines, LinguaGame substantially improves communication efficiency—agents express their goals and arguments more clearly and effectively within fewer or more concise turns.

Conclusion: Modeling LLM multi-agent dialogue as a linguistically grounded signalling game, and adjusting agents' utterances via a training-free equilibrium approximation, leads to more efficient and effective communication than conventional MAS setups that focus mainly on architecture. The approach generalizes across domains like courtroom simulations and debates while requiring limited task-specific tailoring, suggesting it is a promising paradigm for enhancing language use in multi-agent LLM systems.

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [33] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE is a reinforcement-learning framework for RAG that improves both evidence-grounded answering and reliable abstention, achieving higher accuracy with much lower annotation cost.


<details>
  <summary>Details</summary>
Motivation: RAG systems often answer correctly without clearly grounded evidence or hallucinate when retrieved context is insufficient. Existing work typically tackles grounding quality and abstention reliability separately, lacking a unified approach that can both ensure answers are supported by evidence and avoid fabrications when evidence is lacking.

Method: The authors propose GRACE, a reinforcement-learning framework for RAG. They first construct training data automatically using heterogeneous retrievers, generating diverse question-context scenarios without manual labeling. Then they design a multi-stage gated reward function that guides the model through three abilities: (1) judging whether retrieved evidence is sufficient, (2) extracting key supporting evidence, and (3) deciding to answer or abstain and generating the answer when appropriate.

Result: On two RAG benchmarks, GRACE achieves state-of-the-art overall accuracy. It better balances answering correctly and abstaining when evidence is insufficient compared to prior methods. Additionally, it attains these performance gains while requiring only about 10% of the human annotation cost of competing approaches.

Conclusion: A unified RL-based framework can jointly improve evidence-grounded answering and reliable abstention in RAG systems. By leveraging heterogeneous retrieval for automatic data construction and a multi-stage gated reward for training, GRACE delivers more trustworthy RAG behavior with significantly reduced annotation demands.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [34] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: The paper evaluates and improves text watermarking robustness for Bangla LLM outputs under cross‑lingual round‑trip translation attacks, proposing a layered watermarking strategy that significantly boosts post‑attack detectability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM watermarking works largely target high‑resource languages and assume benign conditions; their robustness in low‑resource languages like Bangla, especially under realistic cross‑lingual round‑trip translation attacks, is not well understood. There is a need to quantify how current token‑level watermarking schemes fail in such settings and to design methods that remain detectable without retraining large models.

Method: The authors systematically evaluate three state‑of‑the‑art token‑level watermarking methods—KGW, Exponential Sampling (EXP), and Waterfall—on Bangla LLM text generation. They test watermark detectability and quality metrics (perplexity, ROUGE) both under benign conditions and under cross‑lingual round‑trip translation (RTT) attacks. Observing severe robustness failures, they then introduce a layered watermarking strategy that combines an embedding‑time watermark with a post‑generation watermark, aiming to preserve detectability after RTT while controlling semantic degradation. They compare layered vs single‑layer methods in terms of post‑RTT detection accuracy and quality trade‑offs.

Result: Under benign conditions, KGW and EXP achieve over 88% watermark detection accuracy with minimal impact on perplexity and ROUGE scores, while Waterfall is presumably weaker or comparable. However, when subjected to cross‑lingual RTT attacks, detection accuracy of all token‑level methods drops sharply to about 9–13%, revealing a fundamental weakness. The proposed layered watermarking approach improves post‑RTT detection accuracy by 25–35 percentage points, reaching 40–50% accuracy—about 3× to 4× higher than single‑layer token‑level methods—at the expense of some controlled semantic quality loss.

Conclusion: Token‑level watermarking schemes that work well in high‑resource, benign settings can fail dramatically for low‑resource languages like Bangla under cross‑lingual RTT attacks. A layered watermarking strategy that combines embedding‑time and post‑generation watermarks offers a practical, training‑free way to substantially increase robustness, albeit with a measurable robustness–quality trade‑off. This positions layered watermarking as a viable solution for multilingual, low‑resource watermark detection, and the released code/data enable further research in this direction.

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [35] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM is a framework for understanding and steering LLMs at the task level by identifying both helpful and harmful neurons using a contrastive, biologically inspired approach.


<details>
  <summary>Details</summary>
Motivation: Existing neuron-interpretation methods focus on single abilities and only on supportive neurons, which is inadequate for complex, task-focused scenarios requiring multiple abilities and fails to account for inhibitory neurons and chance (fortuitous) correct answers.

Method: NeuronLLM introduces a task-level neuron analysis framework grounded in the biological principle of functional antagonism. It identifies two opposing neuron types—good (facilitating) and bad (inhibiting)—through contrastive learning. It also uses augmented question sets to reduce the impact of fortuitous behaviors where models get answers correct without genuine understanding.

Result: Across multiple LLM sizes and families, and on four NLP tasks, NeuronLLM outperforms prior neuron attribution and understanding methods, demonstrating better identification and modeling of neurons relevant to task performance.

Conclusion: Task performance in LLMs arises from the interaction of both supportive and inhibitory neurons, and modeling these antagonistic roles via NeuronLLM yields a more holistic and effective understanding of LLM functional organization than existing ability-specific or purely supportive-neuron approaches.

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [36] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: The paper introduces FeedEval, an LLM-based framework for automatically evaluating and filtering LLM-generated essay feedback to ensure high quality along specificity, helpfulness, and validity, thereby improving automated essay scoring and revision outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing automated essay scoring research is moving beyond predicting numeric scores toward generating rich feedback that explains scores and guides students. Because expert-annotated feedback is expensive, many works use synthetic, LLM-generated feedback as training data. However, this feedback is often used without any systematic quality check, which introduces noise and degrades downstream essay assessment and feedback systems. There is a need for a principled, scalable way to evaluate and select only high-quality feedback before using it for training or supporting students.

Method: The authors propose FeedEval, a framework that evaluates LLM-generated feedback along three pedagogically motivated dimensions: specificity (how concrete and detailed the feedback is), helpfulness (how actionable and instructive it is), and validity (whether it is factually and pedagogically correct about the essay). They curate datasets and train separate, dimension-specialized LLM evaluators to score feedback on each dimension. Given multiple feedback candidates for an essay, FeedEval uses these evaluators to assess each candidate, then selects the best-quality feedback for downstream tasks such as training essay scoring models or guiding revisions.

Result: On the ASAP++ benchmark, FeedEval’s automatic evaluations correlate closely with human expert judgments of feedback quality. When essay scoring models are trained using only the high-quality feedback selected by FeedEval, they achieve better scoring performance than models trained on unfiltered feedback. Additional experiments on essay revision using smaller LLMs show that the feedback identified as high quality by FeedEval leads to more effective revisions than unfiltered or lower-quality feedback.

Conclusion: FeedEval offers an effective, scalable solution to evaluating and filtering LLM-generated essay feedback, ensuring higher quality along specificity, helpfulness, and validity. By aligning closely with expert assessments and improving both scoring accuracy and revision effectiveness, the framework demonstrates that explicit, dimension-based feedback evaluation is valuable for building better automated writing support systems. The authors plan to release code and curated datasets upon acceptance to facilitate further research and adoption.

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [37] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: The paper introduces RL-Text2Vis, a reinforcement learning framework that improves text-to-visualization generation by jointly optimizing textual correctness, code executability, and chart quality using post-execution feedback, outperforming GPT-4o and strong baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Text2Vis systems often generate charts that are executable but semantically misaligned, unclear, or low quality, especially with open-source models that frequently produce non-executable or visually poor code. Supervised fine-tuning can help with executability but cannot directly incorporate post-execution signals such as visualization quality, leaving a gap in optimizing end-to-end performance of text-to-visualization pipelines.

Method: The authors design RL-Text2Vis, a reinforcement learning framework based on Group Relative Policy Optimization (GRPO). They construct a multi-objective reward that combines textual accuracy, code validity, and post-execution visualization quality. Using this reward, they fine-tune Qwen2.5 models (7B and 14B) so that the policy learns to generate both executable plotting code and semantically clear, high-quality visualizations from natural language queries over tables.

Result: Models trained with RL-Text2Vis on Qwen2.5 achieve a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and increase code execution success from 78% to 97% compared to their zero-shot baseline. The method also surpasses strong zero-shot and supervised fine-tuning baselines and generalizes well to out-of-domain datasets such as VIS-Eval and NVBench.

Conclusion: Reinforcement learning with GRPO and multi-objective, post-execution rewards is an effective strategy for structured, multimodal reasoning in text-to-visualization generation. RL-Text2Vis substantially improves both executability and semantic quality of generated visualizations, setting a new state of the art for open-source Text2Vis models and suggesting a promising direction for future visualization-generation systems.

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [38] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: The paper explores model merging as a cost-efficient way to build a single Thai-focused, multi-capability LLM by combining several 8B-parameter base and specialized models, and empirically shows merged models improve Thai general and financial task performance.


<details>
  <summary>Details</summary>
Motivation: Organizations in banking and finance need powerful LLMs that can run on-premise for privacy, security, and regulatory reasons, and that handle Thai well across both general and financial domains. Training separate specialized models or one large, fully multi-capable model is expensive and resource-intensive, especially for Thai. The authors aim to find a more resource-efficient method to obtain a single LLM that combines general Thai capabilities and domain-specific financial skills.

Method: They investigate model merging: algorithmically combining weights of independently trained 8B-parameter LLMs. Two main experiments are described. (1) Merge a general-purpose Qwen-8B with ThaiLLM-8B and evaluate on Thai school exam benchmarks (M3 and M6 O-NET) to see if ThaiLLM-8B brings Thai general capability gains over plain Qwen-8B. (2) Merge Qwen-8B with both ThaiLLM-8B and a finance-focused model, THaLLE-CFA-8B, and evaluate on both general Thai (M3 and M6 O-NET) and financial benchmarks (Flare-CFA, Thai-IC) to test whether multi-domain capabilities can be combined in one merged model.

Result: In the first experiment, the merged Qwen-8B + ThaiLLM-8B model outperforms the base Qwen-8B on Thai general capability benchmarks (M3 and M6 O-NET), confirming that merging successfully transfers Thai strengths. In the second experiment, merging Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B yields further performance gains not only on general Thai benchmarks (M3 and M6 O-NET) but also on financial tasks (Flare-CFA and Thai-IC), indicating successful integration of both general and financial Thai capabilities in a single 8B model.

Conclusion: Model merging is a viable and resource-efficient strategy to create multi-capability, Thai-focused LLMs suitable for on-premise deployment in regulated industries. By merging general and specialized models, one can obtain a single model with improved Thai general and financial performance, avoiding the full cost of training a large unified model or maintaining many separate specialized models.

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [39] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: The paper analyzes why existing knowledge editing methods like ROME fail on multi-hop reasoning and proposes a simple redundant editing strategy across layers to improve multi-hop performance.


<details>
  <summary>Details</summary>
Motivation: While Rank-One Model Editing (ROME) is effective and efficient for editing single-hop factual knowledge in transformer models, it performs poorly on multi-hop reasoning tasks that require chaining multiple facts. There is a need to understand how layer depth affects edits and why multi-hop reasoning degrades, and to develop a method that makes edited models better at multi-hop inference instead of only single-hop fact recall.

Method: The authors empirically study ROME edits at different transformer layer depths and identify how these edits affect internal representations used for multi-hop reasoning. They analyze failure patterns such as late access to intermediate hops, loss of generalization, and overfitting to edited facts. Based on these insights, they introduce Redundant Editing: instead of a single ROME edit at one layer, they perform multiple coordinated edits across different layers to preserve access to intermediate representations and maintain multi-hop reasoning capability.

Result: They observe three main failure modes when performing single-layer ROME edits for multi-hop tasks: (1) hopping-too-late, where later layers cannot incorporate necessary intermediate hop information; (2) strong degradation in generalization when edits are applied in later layers; and (3) overfitting, where the model prefers edited-hop answers even when they are contextually wrong. Their proposed Redundant Editing strategy substantially improves performance on 2-hop question-answering, yielding at least a 15.5 percentage point increase in accuracy, corresponding to a 96% relative gain over the previous single-edit ROME approach, though with some loss of specificity and naturalness in generated language.

Conclusion: Single-point ROME edits at a single layer are ill-suited for multi-hop reasoning because they disrupt the chaining of intermediate representations, hurt generalization, and induce overfitting to edited facts. By distributing edits redundantly across multiple layers, the model maintains better access to multi-hop information and significantly improves 2-hop question-answering performance, at the cost of some specificity and fluency. The work suggests that effective knowledge editing for reasoning requires layer-aware, multi-site interventions rather than isolated single-layer edits.

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [40] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: The paper argues that in vision-language models, description specificity should be evaluated independently of length and presents a dataset and human study showing people prefer more specific descriptions even when length is controlled.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language systems often equate longer descriptions with more informative or specific ones, which is misleading because descriptions can be short but highly informative or long yet uninformative. There is a need for a principled notion and evaluation of specificity that does not rely on description length.

Method: The authors formally define specificity using contrast sets of images, measuring how well a description singles out a target image compared to distractors. They then build a dataset where caption length is controlled while information content varies, and run human preference studies to see whether people consistently choose more specific descriptions when length is held constant.

Result: Human annotators reliably preferred descriptions that were more specific to the target image than to other images, even when those descriptions were not longer. Analysis shows that variation in how the fixed length budget is used (i.e., what information is included) better explains perceived specificity than length alone.

Conclusion: Specificity and length are distinct properties of image descriptions; controlling only for length is insufficient to capture how informative a description is. Evaluations of vision-language models should directly target and reward specificity rather than verbosity, leading to better measures of meaningful descriptive quality.

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [41] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: Character-R1 is a reinforcement-learning-style framework that trains role-playing agents to maintain consistent, in-character behavior using explicit, verifiable reward signals.


<details>
  <summary>Details</summary>
Motivation: Existing role-playing agents mostly mimic surface behaviors (style, tone) without modeling the character’s internal cognition (beliefs, worldview, goals). This leads to inconsistency and out-of-character actions, especially in complex interactions. Moreover, prior work lacks well-defined, fine-grained reward signals that directly measure and optimize role-awareness and character consistency.

Method: The authors design Character-R1, a training framework with three key reward components: (1) Cognitive Focus Reward: the agent must explicitly analyze and label 10 predefined character elements (such as worldview), enforcing structured internal reasoning tied to the role; (2) Reference-Guided Reward: similarity/overlap-based metrics compare the agent’s responses with curated reference responses to guide exploration and optimization; (3) Character-Conditioned Reward Normalization: the reward distributions are normalized per character category to stabilize learning and ensure fair, robust optimization across heterogeneous roles with different difficulty or style profiles.

Result: Through extensive experiments, Character-R1-trained agents outperform prior methods on multiple dimensions, including knowledge, memory, and other role-playing related metrics. The framework yields agents that stay more consistently in-character and perform better on complex tasks requiring long-term coherence.

Conclusion: Rewarding explicit cognitive modeling of character traits, combined with reference-based guidance and per-character reward normalization, leads to more reliable, consistent, and capable role-playing agents. Character-R1 provides a general, verifiable reward design that can be applied to improve role-aware reasoning across diverse roles.

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [42] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: The paper presents CuCu, a scalable multi-agent LLM framework that converts national social studies curricula into culture-aware question–answer pairs, and uses it to build KCaQA, a 34.1k-item Korean culture-specific QA dataset that better reflects local sociocultural contexts.


<details>
  <summary>Details</summary>
Motivation: Although LLMs perform well on many tasks, their capabilities and value alignment are uneven across languages and cultures, largely because they are trained on predominantly English-centric data that encodes specific cultural values. This creates a gap in culturally grounded behavior and knowledge for other societies. The authors aim to create a practical, scalable way to align LLMs with specific national and cultural contexts by using an authoritative, structured source: national social studies curricula.

Method: The authors propose CuCu, an automated multi-agent framework built on LLMs. CuCu takes as input national social studies textbooks and curricula and systematically transforms them into open-ended, culture-specific question–answer pairs. Multiple LLM agents collaborate to parse curriculum content, generate questions that reflect local history, norms, and sociocultural issues, and produce grounded answers. They apply this pipeline to the Korean national social studies curriculum to automatically construct the KCaQA dataset.

Result: Using CuCu on the Korean national social studies curriculum yields KCaQA, a dataset of 34.1k open-ended QA pairs. Quantitative analyses (likely coverage, diversity, topic distribution, and possibly comparison to generic QA corpora) and qualitative inspections indicate that KCaQA captures culture-specific topics and that the generated answers are contextually appropriate to Korean sociocultural realities rather than generic or Anglocentric perspectives.

Conclusion: Curriculum-based, automated supervision via CuCu is a scalable way to produce culture-aware QA data that can help culturally align LLMs. The KCaQA dataset demonstrates that national social studies curricula can be effectively transformed into large-scale QA resources that encode local knowledge and values, suggesting a practical path toward improving cultural representation and alignment in LLM behavior across different countries and cultures.

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [43] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: The paper proposes a new pipeline, MAGA, to generate more detector-challenging machine text via stronger alignment, both to stress‑test current LLM-text detectors and to train detectors with better cross‑domain generalization.


<details>
  <summary>Details</summary>
Motivation: As LLMs improve, their machine-generated text becomes harder to distinguish from human-written text, which worsens misuse risks like fake news and fraud. Existing fine-tuned detectors generalize poorly, largely because the machine-generated training data is not diverse or realistic enough: merely expanding model or source variety does not solve this. The authors are motivated to systematically augment the generation process itself—especially alignment aspects—to create harder, more realistic machine text that both attacks current detectors and yields better data for training future detectors.

Method: They introduce MAGA (Machine-Augment-Generated Text via Alignment), a generation pipeline that tightly controls and enriches alignment at multiple stages: prompt construction, reasoning process, and final outputs. A core technique in the pipeline is RLDF (Reinforced Learning from Detectors Feedback), where detectors’ predictions or scores are treated as a feedback signal to iteratively optimize the generation process so that the produced texts become more aligned and more challenging for detectors. MAGA is then used to create a training dataset of aligned MGT, on which a RoBERTa-based detector is fine-tuned and evaluated for generalization.

Result: A RoBERTa detector fine-tuned on the MAGA-generated dataset exhibits an average improvement of 4.60% in AUC when tested for generalization across different conditions. Conversely, when existing detectors are evaluated on the MAGA dataset, their AUC drops on average by 8.13%, indicating that MAGA texts are significantly harder for current detectors to recognize as machine-generated.

Conclusion: The study concludes that augmenting the alignment of machine-generated text via the MAGA pipeline—and in particular using RLDF—both reveals weaknesses in current detectors and yields higher-quality training data. This leads to detectors with better generalization ability and offers a promising direction for future research on robust detection of LLM-generated text.

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [44] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: The paper presents SpeechMedAssist, a speech language model for multi-turn, speech-based medical consultations that achieves strong performance with very limited medical speech data through a two-stage training paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing medical consultation systems mostly rely on long text-based interactions, which are inconvenient and less natural for patients. While recent SpeechLMs enable speech-based interactions, their use in medicine is hindered by a lack of medical speech data and the high cost and inefficiency of directly fine-tuning on speech. The authors aim to build an effective, speech-native medical assistant that can work well despite limited domain-specific speech data.

Method: The authors design SpeechMedAssist, a SpeechLM tailored for medical consultations. They leverage architectural characteristics of SpeechLMs to split training into two stages: (1) Knowledge & Capability Injection via Text, where medical knowledge and conversational skills are learned using text data; and (2) Modality Re-alignment with Limited Speech Data, where the model is adapted to speech using only 10k synthesized medical speech samples. They also create an evaluation benchmark covering single-turn medical question answering and multi-turn simulated consultations to rigorously assess performance.

Result: SpeechMedAssist, trained with the proposed two-stage paradigm and a small amount of synthesized medical speech, surpasses all baseline models in both single-turn and multi-turn medical consultation tasks. It demonstrates higher effectiveness (better answers and interactions) and robustness (more stable performance across different scenarios) in most evaluation settings on the newly designed benchmark.

Conclusion: Decoupling SpeechLM training into text-based knowledge injection and a subsequent speech modality re-alignment allows the development of powerful, speech-native medical consultation systems with minimal medical speech data. SpeechMedAssist exemplifies this approach, achieving state-of-the-art performance on a comprehensive benchmark, suggesting that similar two-stage paradigms can be widely applied to other low-resource, domain-specific speech interaction settings.

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [45] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: They introduce CRANE, a relevance-based framework to pinpoint language-specific neurons in multilingual LLMs by testing functional necessity via neuron interventions, finding asymmetric, language-selective specializations that are better isolated than with activation-based methods.


<details>
  <summary>Details</summary>
Motivation: Although multilingual LLMs work well across many languages, it is unclear how language abilities are organized at the neuron level. Existing work mostly uses activation-based heuristics to find language-related neurons, but these heuristics mix up when a neuron is merely more active for a language versus when it is actually functionally necessary for that language. This limits understanding of true language-specific components inside the model.

Method: They propose CRANE, a relevance-based analysis framework that defines language specificity by functional necessity. Instead of just measuring activation magnitudes, CRANE identifies neurons whose removal (masking) selectively hurts performance on a particular language. They conduct targeted neuron-level interventions, build a relevance-based metric for neuron importance under language conditions, and analyze neuron specialization via performance changes on language-conditioned tasks. They also compare base and chat variants of models to assess transfer and apply the framework to English, Chinese, and Vietnamese across several benchmarks.

Result: CRANE finds a stable asymmetric pattern: when neurons identified as relevant to a target language are masked, performance on that language drops notably, while performance on other languages is largely preserved. This suggests neurons that are selective for a language but not completely exclusive. Their relevance-based metric and experiments show that CRANE isolates language-specific neurons more accurately and cleanly than prior activation-based approaches, across multiple languages, tasks, and in base-to-chat model transfer scenarios.

Conclusion: Functional relevance, not raw activation, is key to characterizing language specificity in multilingual LLMs. CRANE provides a more precise way to identify language-specific neurons by using intervention-based relevance, revealing that multilingual models contain language-selective neuron subsets whose ablation disproportionately affects their corresponding language. The framework offers a more faithful tool for analyzing and potentially editing multilingual capabilities, and the authors plan to publicly release their implementation.

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [46] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate is a framework that adds formal, logical guarantees to LLM tool-calling by managing a symbolic world state and enforcing Hoare-style contracts for each tool.


<details>
  <summary>Details</summary>
Motivation: Existing LLM tool-use frameworks rely on informal natural language reasoning to decide when and how to call tools, which offers no formal guarantees against logical errors, hallucinated tool effects, or corrupted internal state. There is a need for a principled way to ensure that tool calls are logically safe, that their effects on the world representation are verifiable, and that the overall reasoning process becomes more trustworthy and debuggable.

Method: The authors propose ToolGate, a forward execution framework that maintains an explicit symbolic state as a typed key-value store encoding trusted world information. Each external tool is specified via a Hoare-style contract with a precondition and postcondition. The precondition is checked against the current symbolic state to decide whether a tool is allowed to be invoked. After the tool executes, the postcondition is used for runtime verification to decide if and how the tool’s result can safely update the symbolic state. Only tool outcomes that satisfy the postcondition are committed, ensuring controlled and verifiable state evolution.

Result: Experiments show that ToolGate substantially increases the logical reliability and verifiability of LLM systems that use tools, while keeping performance on complex multi-step reasoning tasks competitive with existing approaches. The framework successfully prevents invalid or hallucinated tool results from corrupting the symbolic state in evaluated scenarios.

Conclusion: ToolGate provides a principled, contract-based mechanism for safe and verifiable tool use in LLMs, ensuring that the internal world representation evolves only through verified tool executions. This establishes a foundation for building more trustworthy, debuggable AI systems that integrate language models with external tools and suggests that formal verification concepts can be effectively combined with LLM-based reasoning.

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [47] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: The paper proposes a generative-AI-based framework for detecting, explaining, and proactively intervening on hateful memes, with a focus on generalization under limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Hateful memes are difficult and costly to moderate because they combine image and text, require nuanced understanding, and large annotated datasets are expensive to curate. Existing research usually treats detection, explanation, and intervention as separate problems, which does not match how real-world moderation systems need to work end-to-end.

Method: The authors design a framework built on task-specific generative multimodal agents that use large multimodal models in a few-shot setting. These agents are specialized for three tasks: (1) detecting whether a meme is hateful, (2) generating explanations of why it is hateful or not, and (3) generating or suggesting non-hateful alternatives or interventions before posting. The approach leverages the adaptability of generative models to different meme types with minimal labeled data.

Result: The framework demonstrates that generative multimodal agents can perform hateful meme moderation tasks (detection, explanation, and intervention) more generally and robustly than approaches that focus only on detection or require large labeled datasets. The system appears to work across diverse meme types in low-data regimes, suggesting strong potential for practical deployment.

Conclusion: End-to-end hateful meme moderation can be effectively addressed using generative multimodal agents that jointly tackle detection, explanation, and intervention, even under limited data. This integrated, data-efficient approach better reflects real-world moderation needs and could be deployable in production systems for platform safety.

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [48] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: The paper studies how well large language models handle negation in Korean, finds that their performance significantly drops with negation, and introduces a new benchmark, Thunder-KoNUBench, plus fine-tuning experiments that improve negation and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Negation is a well-known weakness of large language models, but there is a lack of systematic benchmarks and analyses for Korean negation. This gap makes it difficult to measure, compare, and improve LLMs’ handling of negation phenomena in Korean, which are linguistically rich and complex.

Method: The authors first perform a corpus-based linguistic analysis of how negation appears in Korean, identifying various phenomena and their empirical distribution. Based on this, they construct Thunder-KoNUBench, a sentence-level benchmark that mirrors real-world negation patterns. They then evaluate 47 different LLMs on this benchmark, varying in size and training paradigms (including instruction-tuned models), and finally fine-tune some models on Thunder-KoNUBench to measure gains in negation and general contextual understanding.

Result: The study shows that LLM performance systematically degrades when negation is present in Korean sentences. The evaluation across 47 models highlights clear effects of model size and instruction tuning on negation performance. Fine-tuning models using Thunder-KoNUBench leads to measurable improvements not only in negation understanding but also in broader contextual comprehension tasks in Korean.

Conclusion: Korean negation remains a significant challenge for LLMs, but a realistic, corpus-informed benchmark like Thunder-KoNUBench enables precise evaluation and targeted improvement. Fine-tuning on this benchmark substantially enhances models’ handling of negation and general context, demonstrating the benchmark’s utility and providing a path forward for more robust Korean NLP systems.

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [49] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: The paper proposes a memory-efficient fine-tuning method for large language models using an improved zeroth-order optimization with prior-informed perturbations, achieving faster convergence and competitive or better performance than backpropagation-based methods.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models requires backpropagation through billions of parameters, causing substantial memory overhead that limits scalability and accessibility. Zeroth-order optimization removes the need for backpropagation by using only forward passes, but standard ZO suffers from high-variance gradient estimates due to purely random perturbations, leading to slow convergence and weaker performance. There is a need for ZO methods that are both memory-efficient and performant enough to rival gradient-based fine-tuning.

Method: The authors introduce a plug-and-play enhancement to standard zeroth-order optimization. Instead of using only random Gaussian perturbations, they compute a dynamic guiding vector from Gaussian samples that encodes prior information about promising update directions. Perturbations are then biased toward this guiding vector, reducing variance in the gradient estimate. They also explore a greedy perturbation strategy that further exploits prior knowledge when choosing perturbation directions. The method is designed to integrate with existing ZO optimization pipelines without architectural changes to LLMs. Theoretically, they analyze the alignment between the estimated ZO gradient and the true gradient, showing improved directional accuracy.

Result: The improved ZO estimator achieves stronger directional alignment with the true gradient, which in theory should improve optimization efficiency. Empirically, across multiple LLM scales and architectures, the method converges faster and attains better task performance than conventional ZO optimization. On the OPT-13B model over 11 NLP benchmarks, the proposed method beats traditional ZO on all tasks and even surpasses gradient-based fine-tuning baselines on 9 of 11 tasks, while maintaining the memory advantages of forward-only optimization.

Conclusion: Incorporating prior-informed, guided perturbations into zeroth-order optimization substantially mitigates the high-variance gradient estimation problem that hampers conventional ZO methods. The proposed approach offers a practical, memory-efficient alternative to gradient-based fine-tuning for large language models, delivering faster convergence and competitive or superior accuracy, and can be easily plugged into existing optimization workflows.

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [50] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: The paper presents the DSC2025 ViHallu Challenge, a shared task and dataset for detecting hallucinations in Vietnamese LLMs, showing current systems do reasonably well but still struggle, especially with intrinsic hallucinations.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate, limiting their reliability, and most hallucination benchmarks focus on English, leaving low-to-medium resource languages like Vietnamese without standardized evaluation resources.

Method: The authors construct the ViHallu dataset of 10,000 Vietnamese (context, prompt, response) triplets labeled into three hallucination types (none, intrinsic, extrinsic), design three stress-test prompt types (factual, noisy, adversarial), and run a shared task where 111 teams submit detection systems evaluated using macro-F1, comparing instruction-tuned LLM-based approaches, ensembles, and baseline encoder-only models.

Result: The best system obtains a macro-F1 of 84.80%, far surpassing the encoder-only baseline at 32.83%, and experiments show that instruction-tuned LLMs with structured prompting and ensemble strategies are markedly more effective than generic architectures, though performance on intrinsic hallucinations remains notably weaker.

Conclusion: The work delivers the first large-scale Vietnamese hallucination detection benchmark, demonstrates that advanced LLM-based and ensemble methods significantly improve detection but still fall short of perfection, and establishes a foundation and clear challenges for future research on trustworthy Vietnamese AI systems.

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations--fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types--factual, noisy, and adversarial--to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [51] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper formalizes character identity in role-playing agents for LLMs into two layers—parametric and attributive—and empirically studies how these layers affect faithful character portrayal over conversations.


<details>
  <summary>Details</summary>
Motivation: Existing role-playing agents treat characters as loosely specified text prompts without a clear, systematic notion of what constitutes a character’s identity. This leads to inconsistent, hard-to-evaluate behavior and unclear sources of character fidelity or drift. The authors want a principled framework to describe, construct, and evaluate characters so that they can better understand how pre-trained knowledge and prompted attributes each contribute to role-play quality over time.

Method: The authors introduce a two-layer model of character identity: (1) Parametric Identity—knowledge about well-known characters embedded in the LLM’s parameters from pre-training, and (2) Attributive Identity—explicitly specified behavioral attributes such as personality traits, moral values, and relational tendencies. They design a unified character profile schema and instantiate two kinds of characters under this schema: Famous characters (with strong parametric support) and Synthetic characters (without such priors). They then run controlled single-turn and multi-turn interaction experiments where LLMs role-play as these characters. They measure how well the model adheres to the defined identity across turns, decomposing performance along traits, morality, and interpersonal relations.

Result: Empirical evaluations show two key patterns. (1) “Fame Fades”: Famous characters benefit from strong initial fidelity due to rich parametric knowledge, but this advantage quickly diminishes in multi-turn settings as the model relies more on the unfolding conversational context than on pre-trained priors. (2) “Nature Remains”: The models consistently preserve broad personality traits (e.g., being generally introverted or extraverted) regardless of whether they are positive or negative. However, fidelity on moral and social dimensions is asymmetric: role-playing negative morality or adverse interpersonal tendencies is much less reliable than positive ones. Negative social natures—such as being selfish, hostile, or uncooperative—emerge as particularly hard for LLMs to sustain accurately.

Conclusion: Character identity in LLM-based role-playing agents can be productively decomposed into parametric and attributive layers. While pre-trained fame-based priors help at the start of interactions, their influence declines as conversation history grows, indicating that context quickly overrides parametric identity. In contrast, high-level personality traits remain comparatively stable, but moral and relational attributes—especially negative ones—are poorly preserved, creating a major bottleneck for faithful role-play. These insights suggest that future character design, prompting, and evaluation methods should explicitly account for the distinct behaviors of parametric vs. attributive identity, and should pay particular attention to modeling and measuring negative social characteristics.

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [52] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Introduces Qwen3-VL-Embedding and Qwen3-VL-Reranker, multimodal models that map text, images, document images, and video into a shared space for high-precision retrieval, achieving SOTA performance on MMEB-V2.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate, scalable multimodal retrieval systems that can jointly handle text and various visual modalities (images, documents, video) in many languages. Existing models often focus on single modalities, have fixed embedding sizes, limited context length, or lack strong reranking modules. The paper aims to provide an end-to-end, multilingual, and efficient solution for high-precision multimodal search.

Method: Builds on the Qwen3-VL foundation to create two model series: (1) Qwen3-VL-Embedding, trained with a multi-stage pipeline starting with large-scale contrastive pre-training and followed by distillation from a reranker model. It uses Matryoshka Representation Learning to support variable embedding dimensions and accepts up to 32k-token inputs. (2) Qwen3-VL-Reranker, a cross-encoder with cross-attention that provides fine-grained relevance scores for query-document pairs. Both are provided in 2B and 8B parameter versions and inherit multilingual capabilities from Qwen3-VL.

Result: Qwen3-VL-Embedding achieves state-of-the-art results on multiple multimodal embedding benchmarks. In particular, the 8B variant scores 77.8 on MMEB-V2, ranking first among all evaluated models as of January 8, 2025. The models support various multimodal retrieval tasks such as image-text retrieval, visual question answering, and video-text matching.

Conclusion: The Qwen3-VL-Embedding and Qwen3-VL-Reranker series together form an effective, scalable, and multilingual end-to-end pipeline for high-precision multimodal retrieval. Their architecture and training strategy yield state-of-the-art performance on standard benchmarks and demonstrate strong practical capabilities across a broad range of multimodal search and reasoning tasks.

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [53] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: The paper evaluates gender bias in automatic sentiment and emotion classifiers using over one million self-labeled posts, finding systematically higher error rates on men-authored texts and warning about biased downstream applications.


<details>
  <summary>Details</summary>
Motivation: Automatic emotion and sentiment classifiers are widely used in research and industry, often assumed to perform uniformly across groups. However, existing benchmarks rely on third-party annotators instead of people describing their own emotions, which may hide systematic demographic biases. The authors are specifically concerned about fairness and reliability across genders, and the impact such biases can have on any application that uses these tools for inference or decision-making.

Method: The authors leverage a large-scale dataset of more than one million posts where users self-annotate their emotional states. They design a pre-registered study to systematically test gender bias across 414 combinations of model architectures and emotion-related label sets. They compare error rates for texts authored by men and women across different types of automatic classifiers, including large language models, and analyze how performance disparities manifest for different underlying emotions. They then simulate or quantify the impact of these disparities in representative downstream tasks.

Result: Across nearly all tested model–emotion combinations, error rates are consistently higher for texts written by men than those written by women, indicating systematic gender bias in emotion detection. This bias is observed across different model families and emotion categories. The authors also show that these disparities can meaningfully distort results in downstream applications that rely on emotion scores, especially when the gender composition of the sample is unknown or changes over time.

Conclusion: Sentiment and emotion detection systems, including state-of-the-art machine learning and large language models, are not equitable across genders in this setting, with men-authored texts experiencing higher misclassification rates. Because many applications implicitly assume group-invariant performance, practitioners should apply these tools cautiously, particularly when the gender distribution is uncertain. Overall, the work underscores that sentiment analysis remains an unsolved problem from a fairness perspective and that more attention is needed to ensure reliable, unbiased performance across demographic groups.

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [54] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: The paper introduces InterSafe-V, a multi-modal safety dialogue dataset, and AM^3Safety, a dialogue-level RLHF framework that significantly improves safety and helpfulness of MLLMs in multi-turn multi-modal settings.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM safety alignment is mostly designed for single-turn VQA and relies on expensive human preference labels, making them less effective and scalable for realistic multi-turn, multi-modal dialogues where harmful intent can be reconstructed gradually and safety guardrails degrade over time.

Method: 1) Construct InterSafe-V, an open-source dataset with 11,270 multi-modal dialogues and 500 specialized refusal-focused VQA samples created via interactions among multiple models and targeted domain-specific scenarios. 2) Propose AM^3Safety, which first performs a cold-start refusal phase, then applies Group Relative Policy Optimization (GRPO) for fine-tuning using turn-aware, dual-objective rewards (harmlessness and helpfulness) computed over entire dialogues rather than isolated turns.

Result: When applied to Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B, the method yields over a 10% reduction in Attack Success Rate (ASR) and improvements of at least 8% in harmlessness and over 13% in helpfulness on multi-modal multi-turn safety benchmarks, with general capabilities largely preserved.

Conclusion: Dialogue-level, multi-modal safety alignment with automatically constructed interaction-based datasets and GRPO-based training can significantly strengthen MLLMs against multi-turn safety attacks while maintaining or improving their helpfulness in benign scenarios.

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [55] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: The paper presents a framework to automatically generate domain-specific, implicitly harmful prompts for stress-testing LLM safety, using knowledge graphs and obfuscation-based rewriting.


<details>
  <summary>Details</summary>
Motivation: Existing harmful-prompt datasets are scarce in specialized domains (e.g., finance, healthcare), mostly manually built, and focused on explicit harm that modern LLMs can easily detect and refuse. Real-world attacks are subtler and leverage domain knowledge implicitly, so there is a need for systematic, scalable creation of domain-relevant, implicitly harmful prompts for more realistic red-teaming and safety evaluation.

Method: The authors propose an end-to-end pipeline with two main components: (1) knowledge-graph-guided harmful prompt generation, which encodes domain knowledge as a graph and systematically derives harmful, domain-relevant prompts from it; and (2) dual-path obfuscation rewriting, which takes explicit harmful prompts and transforms them into implicit ones via two rewriting strategies: direct rewriting and context-enhanced rewriting that embeds the harmful intent within richer, indirect domain context.

Result: Using this framework, the authors produce high-quality datasets of harmful prompts for specialized domains that have both strong domain relevance and high implicitness. The generated prompts are harder for LLM safety filters to recognize while remaining grounded in realistic domain scenarios, improving the rigor of safety evaluations and red-teaming experiments. They also release their implementation and datasets as open-source resources.

Conclusion: The proposed framework effectively bridges the gap between domain knowledge and actionable, implicitly harmful prompts, addressing limitations of existing datasets. By systematically generating more realistic, hard-to-detect harmful prompts in specialized domains, it supports more robust LLM safety research and practical red-teaming, and offers reusable tools and datasets to the community.

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [56] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD is a multi-agent debate framework where each LLM agent uses a different external tool and adaptive retrieval, combined with quantitative faithfulness and relevance scoring, to reduce hallucinations and improve fact verification accuracy over prior MAD systems.


<details>
  <summary>Details</summary>
Motivation: Existing Large Language Model (LLM) systems and multi-agent debate (MAD) frameworks still hallucinate and struggle with factual accuracy, particularly in complex reasoning and fact verification tasks. Prior MAD approaches mainly rely on internal model knowledge or static documents, which are prone to hallucinations. Even MADKE, which brings in external evidence, only retrieves evidence once and cannot adapt to new arguments or information that appears during the debate. There is a need for a debate framework that can dynamically interact with heterogeneous external tools and systematically assess factual faithfulness to better detect and prevent hallucinations.

Method: The authors design Tool-MAD, a multi-agent debate framework in which multiple LLM agents are each assigned distinct external tools (e.g., web search APIs, RAG modules). Agents debate an answer while querying their respective tools. The system includes (1) heterogeneous-tool agents to encourage diverse and complementary evidence, (2) an adaptive query formulation mechanism that iteratively refines and issues retrieval queries based on how the debate evolves, and (3) a Judge agent that computes Faithfulness and Answer Relevance scores for each candidate response, using these quantitative metrics to assess coherence with retrieved evidence and alignment with the original question, and then select the final answer while filtering out hallucinations.

Result: On four fact verification benchmarks, Tool-MAD surpasses state-of-the-art multi-agent debate baselines, achieving up to a 5.5% gain in accuracy. In specialized medical domains, the framework maintains strong robustness and adaptability across different tool configurations and domain settings, indicating stable performance even when tools or domain conditions vary.

Conclusion: Assigning heterogeneous external tools to debating LLM agents, combined with adaptive retrieval and quantitative faithfulness and relevance scoring, significantly improves factual verification and reduces hallucinations compared to existing MAD systems. Tool-MAD generalizes well to specialized domains such as medicine and different tool setups, suggesting it is a promising approach for real-world fact-checking and reliability-critical applications.

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [57] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: The paper introduces PILOT-Bench, a PTAB-focused benchmark to systematically evaluate large language models’ structured legal reasoning in the patent domain, showing a strong performance gap between leading closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: Existing applications of LLMs in patent and legal practice are limited to relatively simple, lightweight tasks, and there is no standardized way to rigorously assess their ability to perform structured, IRAC-style legal reasoning on real PTAB cases. This gap prevents meaningful comparison of models and hinders progress in deploying them for serious patent adjudication support.

Method: The authors construct PILOT-Bench by aligning PTAB ex parte appeal decisions with USPTO patent data at the case level. They formalize three IRAC-aligned multi-label classification tasks—Issue Type (what legal/technical issues the Board is deciding), Board Authorities (which legal authorities the Board relies on), and Subdecision (finer-grained aspects of the decision). They then evaluate a variety of closed-source and open-source LLMs on these tasks, analyzing results under different input variations, across model families, and via detailed error analyses.

Result: Closed-source commercial LLMs substantially outperform open-source models on the benchmark. For the Issue Type classification task, closed-source models consistently exceed a Micro-F1 of 0.75, while the best open-source model tested (Qwen-8B) reaches only about 0.56. Similar analyses across tasks and settings show systematic gaps and characteristic error patterns between model classes.

Conclusion: PILOT-Bench provides the first PTAB-centric, IRAC-aligned benchmark for evaluating patent-domain legal reasoning in LLMs, revealing a substantial performance gap between state-of-the-art closed-source and open-source models. The benchmark lays groundwork for more rigorous, targeted development of LLMs for patent practice, and suggests that advances in dataset design and model alignment will be crucial for improving open-source models’ legal reasoning in this domain.

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [58] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: The paper investigates how syntax and semantics are linearly encoded in inner layers of a very large LLM (DeepSeek-V3) using averaged “centroid” representations.


<details>
  <summary>Details</summary>
Motivation: To better understand what kind of linguistic information (syntactic vs. semantic) is captured in different layers of large LLMs, and whether this information is encoded in a linearly separable way that can be probed and manipulated.

Method: Compute hidden representations for many sentences; group sentences by shared syntactic structure or shared meaning; average their hidden vectors to obtain syntactic or semantic “centroids.” Then, analyze how subtracting these centroids from individual sentence representations changes similarity relations, and examine how these effects vary across layers.

Result: Syntactic and semantic centroids capture a substantial portion of the respective information in the original representations. Subtracting a syntactic centroid disrupts similarity with syntactically matched sentences, and subtracting a semantic centroid disrupts similarity with semantically matched sentences. The profiles of where and how strongly syntax vs. semantics are encoded differ across layers, and the two can be partially decoupled.

Conclusion: Syntax and semantics are at least partly encoded in a linear fashion in the hidden states of DeepSeek-V3, and they show distinct cross-layer encoding patterns. This indicates differential, partially separable representations of syntactic and semantic information within the model.

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [59] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: The paper shows that the “judge” used in Judge Decoding doesn’t need costly training: its behavior is essentially captured by KL divergence between draft and target models, and a simple KL-based, training-free judge can match or beat learned ones.


<details>
  <summary>Details</summary>
Motivation: Speculative/Judge Decoding speeds up LLM inference by having a fast draft model propose tokens and a slower target model verify them. Existing Judge Decoding schemes depend on trained judges that require expensive, noisy supervision and can fail under domain shifts. The authors want to remove this supervision bottleneck and understand what these judges are really learning from first principles.

Method: The authors analytically study Judge Decoding and show that the “criticality” scores learned by linear judges can be expressed through the draft–target distributional divergence, specifically the KL divergence. They prove a structural correspondence between a standard linear judge and KL divergence by showing both operate on the same logit-level primitives. Based on this insight, they design a simple, training-free judge that uses the KL divergence between draft and target distributions to decide whether to accept speculative tokens, instead of learning a separate judging network.

Result: Experiments on reasoning and coding benchmarks compare the proposed KL-based judge with complex trained judges such as AutoJudge. The KL-based method matches or outperforms these trained systems in speed–accuracy tradeoffs, and demonstrates better robustness when the evaluation domain differs from the training domain of the learned judges.

Conclusion: Learned linear judges in Judge Decoding are effectively redundant because the essential information they capture is already encoded in the KL divergence between draft and target models. A simple, training-free KL-based verification mechanism can replace costly supervised judges, maintaining or improving performance and robustness while eliminating the need for specialized training data and judge training.

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [60] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: They propose a post-hoc method to remove language-identity information from multilingual dense embeddings so that retrieval relies more on semantics than language, improving multilingual search, especially across different scripts.


<details>
  <summary>Details</summary>
Motivation: Multilingual dense retrieval systems search over documents in many languages, but multilingual embedding models tend to encode language identity strongly. This causes embeddings from the same language to appear more similar regardless of semantics, which can overshadow truly relevant documents in other languages, harming cross-lingual retrieval and mixed-language search quality.

Method: They introduce LANGSAE EDITING, a sparse autoencoder trained over pooled multilingual embeddings. The autoencoder discovers latent units that correlate with language identity by analyzing cross-language activation statistics. At inference time, those language-associated units are selectively suppressed (zeroed or reduced), and the modified representation is passed through the decoder to reconstruct an embedding in the original dimensionality. This makes the approach post-hoc and model-agnostic: no need to retrain the base encoder or re-encode the corpus; only a transformation of existing vectors is applied, compatible with current vector databases.

Result: Across multiple languages and benchmark retrieval tasks, applying LANGSAE EDITING improves ranking metrics and increases cross-language coverage (i.e., more relevant documents from other languages are surfaced). The gains are particularly large when the query and relevant documents are in script-distinct languages, where language identity tends to be most pronounced in the embeddings.

Conclusion: Controllably editing language-identity information in multilingual embeddings via a sparse autoencoder can significantly improve multilingual dense retrieval without modifying or retraining the underlying encoder. This post-hoc approach is practical for existing vector databases and is especially beneficial for retrieval across languages with different scripts.

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [61] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C is an LLM-driven framework that automatically converts generic non-convex optimization problems into convex ones, achieving strong empirical performance and reducing the need for human experts.


<details>
  <summary>Details</summary>
Motivation: Non-convex optimization problems are widespread and hard to solve reliably with standard solvers. Manual convexification is labor-intensive, requires deep expertise, and does not scale. There is a need for an automated, general framework that can systematically transform non-convex problems into convex ones so that efficient convex solvers can be applied.

Method: The authors propose NC2C, an end-to-end automated framework built on large language models. NC2C uses LLM-based mathematical reasoning to (1) identify non-convex elements in a problem, (2) choose appropriate convexification strategies, and (3) construct mathematically rigorous convex reformulations. The framework combines symbolic reasoning with adaptive transformation rules, iterative validation, error-correction loops, and feasibility-domain correction to ensure the transformed problems remain valid and solvable.

Result: On a dataset of 100 diverse non-convex optimization problems, NC2C attains an 89.3% execution rate and a 76% success rate in generating feasible, high-quality convex transformations. These metrics surpass those of baseline approaches by a large margin, indicating both practicality and effectiveness of the framework.

Conclusion: NC2C shows that LLMs can be effectively used to automate the non-convex-to-convex transformation pipeline, thereby reducing reliance on domain experts and enabling convex solvers to tackle optimization problems that were previously difficult or impractical to handle. This opens a path toward more scalable, automated optimization workflows driven by large language models.

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [62] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: The paper analyzes how different types of authority roles in multi-agent LLM systems create authority bias in conversations, finding that expert- and referent-type authority have stronger, more persistent influence than formal/legitimate authority, and that bias stems from authorities’ consistency rather than others’ active conformity.


<details>
  <summary>Details</summary>
Motivation: While multi-agent systems with LLMs often assign special ‘leader’ or ‘expert’ roles to improve performance, it is unclear how these asymmetric authority roles systematically bias interactions and outcomes. The authors want to understand when and how authority bias emerges, what types of authority are most influential, and what conversational behaviors actually drive this bias, in order to design fairer and more effective multi-agent frameworks.

Method: The authors use ChatEval to run free-form 12-turn multi-agent conversations, instantiating agents with different power roles derived from French and Raven’s theory: legitimate (formal authority), referent (charisma/identification), and expert (perceived expertise). They conduct controlled experiments with GPT-4o and DeepSeek R1 agents, tracking how often and how strongly other ‘general’ agents align with or diverge from the authoritative agent’s positions, and analyzing the dynamics of stance maintenance vs flexibility over turns. They also manipulate whether the authority agent makes clear position statements vs neutral responses to test the necessity of explicit stances for inducing bias.

Result: Across models and settings, agents in expert and referent roles have larger and more persistent influence on group positions than those in purely legitimate roles. The source of authority bias is not that general agents aggressively conform, but that authoritative agents remain highly consistent in their views while general agents shift more, causing outcomes to drift toward the authority’s stance. When authority agents answer in neutral or hedged ways, their influence effectively disappears, indicating that clear stance-taking is required to induce authority bias.

Conclusion: Role-based authority bias is a structural property of multi-agent LLM systems: asymmetric roles coupled with consistent, explicit stances from authorities systematically steer group outcomes, especially when authority is framed as expertise or referent power rather than formal status. Designers of multi-agent frameworks should carefully control how authority roles are defined and prompted, encourage or enforce flexibility in authoritative agents, and be cautious about strong expert or referent framings if unbiased or diverse outcomes are desired.

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [63] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: The paper discovers that AI-generated text shows a characteristic pattern of decreasing volatility in token log probabilities in later parts of sequences, and leverages this late-stage volatility decay to build simple, zero-shot detectors that outperform prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot detectors of AI-generated text typically average token-level statistics over entire sequences, ignoring how these statistics evolve over time in autoregressive generation. The authors suspect that temporal dynamics—especially in the later parts of sequences—encode important differences between human and AI text that are currently underexploited.

Method: The authors analyze over 120k text samples from humans and language models, focusing on the evolution of token log probability fluctuations across positions in generated sequences. They empirically identify a phenomenon they call Late-Stage Volatility Decay in AI-generated text. Using this insight, they define two features—Derivative Dispersion and Local Volatility—computed only on the later parts of sequences, and use these as inputs to a simple zero-shot detection method that does not rely on perturbation sampling or extra model calls.

Result: The study finds that AI-generated text exhibits rapidly stabilizing token log probability volatility as generation progresses, whereas human text maintains higher variability, with the divergence maximized in the second half of sequences where AI text has 24–32% lower volatility. Using only late-stage features (Derivative Dispersion and Local Volatility), their zero-shot detector achieves state-of-the-art performance on EvoBench and MAGE benchmarks and combines well with existing global, sequence-level methods.

Conclusion: Temporal dynamics of token log probabilities, particularly in the later parts of text sequences, provide a strong signal for distinguishing AI-generated from human-written text. By focusing on late-stage volatility patterns via simple features, it is possible to build an efficient, zero-shot detector that not only achieves state-of-the-art benchmark performance but also complements existing global approaches.

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [64] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR is a retrieval-augmented, multi-agent reasoning framework that improves cross-domain misinformation detection beyond same-distribution limits of current models.


<details>
  <summary>Details</summary>
Motivation: Misinformation appears in many different domains, each with distinct knowledge and discourse, which makes cross-domain detection difficult. Existing methods rely on single-perspective cues and do not generalize well to challenging or underrepresented domains, while reasoning LLMs typically assume the training and test data share similar distributions, limiting cross-domain transfer. There is a need for a system that can transfer across domains and reason using multiple perspectives with verifiable steps.

Method: RAAR introduces a retrieval-augmented agentic reasoning framework. For each target-domain sample, it retrieves evidence from a source domain that matches the sample’s semantics, sentiment, and writing style to enable cross-domain transfer. It then employs a multi-agent collaboration scheme: multiple perspective-specific agents independently analyze the claim, and a summary agent integrates their outputs into multi-step reasoning paths under the guidance of a verifier. RAAR uses supervised fine-tuning and reinforcement learning to train a single multi-task verifier to both evaluate claims and improve reasoning quality. Models of sizes 8B and 14B parameters (RAAR-8b and RAAR-14b) are trained within this framework.

Result: On three cross-domain misinformation detection benchmarks, RAAR-8b and RAAR-14b significantly improve over their base models and outperform prior cross-domain detection methods, strong general-purpose LLMs, and LLM-based adaptation baselines.

Conclusion: RAAR demonstrates that retrieval-augmented, multi-agent, verifiable reasoning can substantially enhance cross-domain misinformation detection, overcoming limitations of single-perspective models and same-distribution assumptions in conventional reasoning LLMs. The released models and code aim to support further research on robust, transferable misinformation detection systems.

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [65] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: The paper proposes a new way to do autoregressive language generation by keeping tokens as continuous vectors that gradually mature before being discretized, enabling stable, diverse text with deterministic decoding instead of token-level sampling.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive language models decide on a discrete token at each step and must sample at the token level to express uncertainty. This early discretization causes instability, repetition, and strong sensitivity to decoding heuristics such as temperature or top-k sampling. The authors want a formulation that can maintain uncertainty in a richer, continuous space for longer, reducing reliance on fragile sampling strategies while preserving coherence and diversity.

Method: They define an autoregressive model over continuous token representations instead of immediate discrete tokens. Each position holds a continuous vector that is updated over multiple maturation steps via a deterministic dynamical process. The model continues evolving these continuous states over time until each converges sufficiently, at which point it is discretized into a token via hard decoding (e.g., argmax over the vocabulary). The process preserves and resolves uncertainty in continuous space rather than via token sampling. They also describe optional extensions such as adding stochastic dynamics or smoothing over the state history, but the core method is fully deterministic.

Result: Empirically, the model is able to generate coherent and diverse text using purely deterministic argmax decoding, without needing token-level sampling, diffusion-style denoising, or extra stabilization tricks. The maturation dynamics alone suffice to avoid typical issues like instability and repetition that plague standard autoregressive models under deterministic decoding.

Conclusion: The authors demonstrate a new class of autoregressive language models that operate on continuous, maturing token representations, only discretizing after convergence. This yields stable, diverse generation without token-level sampling, and they claim it is the first such autoregressive approach to use convergence in continuous token space as the core generative mechanism.

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [66] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans is a new benchmark for detecting and explaining fine-grained misinformation at the span level within text, enabling more nuanced analysis than coarse claim-level true/false labels.


<details>
  <summary>Details</summary>
Motivation: Existing misinformation detection benchmarks mostly use binary labels at the level of whole claims or paragraphs, which hides the fact that individual sentences can mix true and false details. This coarse labeling also reduces interpretability because it cannot show exactly which parts are misleading or how they are misleading. The authors want a dataset and tasks that support precise localization of misinformation and richer analysis of its types and explanations.

Method: The authors construct MisSpans, a human-annotated benchmark built from paired real and fake news stories across multiple domains. They define three span-level tasks: (1) MisSpansIdentity to identify which spans in a sentence are false; (2) MisSpansType to classify the type of misinformation for each false span (e.g., distorted vs. fabricated); and (3) MisSpansExplanation to generate rationales grounded in the identified spans. Expert annotators follow standardized guidelines and undergo consistency checks to ensure quality and achieve high inter-annotator agreement. The authors then evaluate 15 large language models—both reasoning-enhanced and non-reasoning variants—in zero-shot and one-shot settings on these tasks.

Result: MisSpans proves to be challenging for current LLMs. Across the 15 evaluated models and settings, performance on span-level misinformation identification, categorization, and explanation is limited, indicating that fine-grained detection is substantially harder than coarse claim-level classification. The results also show that performance varies with model size, reasoning capabilities, and domain-specific textual characteristics, suggesting complex interactions among these factors.

Conclusion: Span-level misinformation detection and analysis is a difficult but important problem that is not well captured by existing coarse-grained benchmarks. MisSpans provides the first multi-domain, human-annotated resource and task suite for pinpointing false spans, typing forms of misinformation, and generating grounded explanations. Initial evaluations with a range of LLMs highlight significant headroom for improvement and underscore the need for models and methods that better handle fine-grained, interpretable misinformation analysis across domains.

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [67] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: The paper introduces ToPG, a new RAG framework that represents knowledge as a heterogeneous proposition graph and uses iterative graph traversal to answer both simple and complex questions effectively.


<details>
  <summary>Details</summary>
Motivation: Existing RAG approaches show a tradeoff: chunk-based RAG works well for simple, single-hop factual questions but struggles with complex multi-hop reasoning because chunks are poorly connected. Interleaved retrieval-and-reasoning methods often miss global corpus structure, while KG-based RAG is strong on multi-hop reasoning but weak on fine-grained, fact-oriented queries. The authors aim to design a unified RAG system that retains fine factual granularity while supporting structured, query-aware traversal for complex reasoning.

Method: The authors propose ToPG (Traversal over Proposition Graphs), which models the corpus as a heterogeneous graph with three node types: propositions (fine-grained factual units), entities, and passages. Edges capture relationships such as which propositions belong to which passages and which entities participate in which propositions. At inference time, ToPG runs iterative Suggestion-Selection cycles: (1) Suggestion phase performs query-aware traversal over the graph to propose candidate propositions relevant to answering the question; (2) Selection phase uses an LLM to evaluate these candidates, prune irrelevant propositions, and choose seeds for the next traversal iteration. This loop continues until sufficient evidence is collected to answer the query.

Result: ToPG is evaluated on three QA settings—Simple QA (single-hop factual), Complex QA (multi-hop), and Abstract QA (more conceptual or high-level questions). Across these tasks, ToPG achieves strong results, outperforming or matching baselines on both accuracy-based metrics (e.g., exact match, F1) and quality-based metrics (e.g., human or LLM-judged answer quality and reasoning soundness). The framework demonstrates robustness across different query types, unlike prior methods that excel mainly on either simple or complex questions.

Conclusion: The study concludes that representing knowledge as a proposition-level heterogeneous graph and performing query-aware graph traversal with iterative LLM-guided pruning is an effective strategy for RAG. This structure allows ToPG to combine the factual precision of fine-grained propositions with the connectivity benefits of graphs, yielding strong performance on both simple and complex QA tasks. The authors argue that such structured, proposition-centric RAG is a key direction for building more capable and efficient retrieval-augmented systems, and they release their implementation to support further research.

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [68] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL is a structure-aware framework that synthesizes complex and diverse Text-to-SQL training data by evolving seed SQL queries via AST-based transformations, yielding higher performance with far less data than prior synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL models need large, diverse, and structurally complex training data, but high-quality human annotations are scarce and expensive. Existing synthetic data methods often prompt LLMs in a structure-agnostic way, which leads to limited SQL structural diversity and insufficient coverage of complex query forms (e.g., deep nesting, complex joins, and aggregations). A more controllable, structure-aware data generation method is needed to better cover the rich space of SQL programs and improve model generalization.

Method: The paper proposes EvolSQL, a structure-aware data synthesis framework. It begins with a Query-SQL expansion phase that starts from seed data and broadens the variety of natural language questions while improving schema coverage. Then it uses an adaptive directional evolution strategy to iteratively increase SQL complexity. This evolution relies on six atomic transformation operators defined over the SQL Abstract Syntax Tree, targeting relational structures, predicates, aggregations, and nesting. An execution-grounded SQL refinement module checks and refines generated queries by executing them, and a schema-aware deduplication step removes redundant or overlapping examples, resulting in clean, diverse question-SQL pairs.

Result: Experiments demonstrate that a 7B-parameter model fine-tuned on the EvolSQL-generated data achieves better performance than a model trained on SynSQL, a much larger synthetic dataset. Notably, EvolSQL attains superior results while using only about 1/18 of the training data size compared to SynSQL, indicating that structurally controlled synthesis is more data-efficient and effective.

Conclusion: EvolSQL provides an effective structure-aware framework for generating high-quality, diverse, and complex Text-to-SQL training data. By evolving SQL queries through AST-based transformations and validating them via execution and deduplication, it produces compact yet powerful datasets that enable smaller models to outperform those trained on much larger, less-structured synthetic corpora. This highlights the importance of structural control and evolution in data synthesis for program generation tasks like Text-to-SQL.

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [69] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report is a training-free cognitive research agent that uses dynamic memory with general LLMs to synthesize expert-level commercial reports from noisy web data, and it outperforms existing deep research agents on a new real-world benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing deep research agents struggle to generate commercial reports that are simultaneously high-quality, reliable, and comprehensive when drawing from massive, noisy web sources. For high-stakes business decisions, better tools are needed to emulate how human commercial analysts systematically clarify intent, gather evidence, and synthesize long-form reports. There is also a lack of rigorous, domain-specific benchmarks to evaluate such agents in realistic commercial scenarios.

Method: The authors propose Mind2Report, a cognitive deep research agent built as a training-free agentic workflow on top of general-purpose LLMs. The workflow: (1) probes and refines fine-grained user intent, (2) performs web search and records distilled information dynamically into a form of external memory during exploration, and (3) iteratively synthesizes the final commercial report using that memory. They augment LLMs with dynamic memory to support long-form, multi-step reasoning and writing, without additional model training. To evaluate the system, they build QRC-Eval, a benchmark of 200 real-world commercial tasks and an evaluation protocol that jointly scores report quality, reliability, and coverage.

Result: On QRC-Eval, Mind2Report demonstrates superior performance over strong baselines, including OpenAI and Gemini deep research agents, across metrics that assess report quality, factual reliability, and breadth of coverage. The experiments indicate that its cognitive workflow and dynamic memory design yield more expert-like, trustworthy commercial reports than existing systems.

Conclusion: Mind2Report shows that a carefully designed, training-free agentic workflow with dynamic memory can significantly improve the synthesis of expert-level commercial reports from noisy web data. The new QRC-Eval benchmark and evaluation methodology provide a foundation for more rigorous study of commercial deep research agents. The authors position this as an initial but promising step toward more advanced, reliable, and comprehensive research agents for high-stakes business decision-making.

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [70] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: The paper proposes a disagreement-aware summarisation pipeline that separates belief aggregation from language generation to better preserve conflicting viewpoints in opinion and multi-document summaries.


<details>
  <summary>Details</summary>
Motivation: Existing opinion and multi-document summarisation methods, especially LLM-based ones, tend to smooth over or erase genuine disagreements between sources and over-represent majority views, which harms the faithfulness of summaries in opinion-rich contexts. There is a need for methods that explicitly model and preserve conflicting viewpoints while still producing fluent summaries.

Method: The authors build a two-stage summarisation pipeline. First, source documents are converted into structured belief sets. These belief sets are then aggregated using distance-based belief merging operators that formally model and retain conflicts between beliefs. Second, large language models are used only as surface realisers: they take the aggregated belief representations and generate natural language summaries from them. The approach is compared to baselines where aggregation is done implicitly during LLM generation, across multiple model families and scales.

Result: Experiments across different LLM architectures and sizes show that when aggregation is left to the generation process, only sufficiently large models can approximate the performance of explicit belief-level aggregation, and this effect is unstable across capacities and architectures. In contrast, the proposed belief-level aggregation pipeline, combined with relatively simple prompting, consistently produces summaries that are more aware of disagreement while remaining fluent and grounded, regardless of the underlying model family or size.

Conclusion: Separating belief-level aggregation from language generation and using formal distance-based belief merging yields more faithful, disagreement-aware opinion and multi-document summaries. This architecture reduces reliance on model scale and idiosyncrasies, providing stable performance across LLMs while still generating fluent and well-grounded natural language summaries.

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [71] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: The paper studies how multimodal LLMs over-rely on text instead of images, defines two types of text bias, and proposes a new benchmark and metric to diagnose visual grounding robustness, showing existing models fail when text conflicts with visual evidence.


<details>
  <summary>Details</summary>
Motivation: Although MLLMs perform well on standard visual reasoning tasks, there is concern they exploit linguistic patterns and priors instead of truly understanding images. This undermines trust in their visual grounding, especially when textual cues are misleading or dominant. The paper aims to systematically characterize and measure this "text bias" to better understand the balance between visual perception and language priors.

Method: The authors conceptualize text bias as arising from two sources: Internal Corpus Bias (statistical correlations from pretraining data) and External Instruction Bias (alignment processes that encourage agreement with text/instructions). They construct V-FAT, a diagnostic VQA benchmark of 4,026 instances across six semantic domains, and design a Three-Level Evaluation Framework: L1 uses atypical images to trigger internal bias, L2 adds misleading textual instructions to induce external bias, and L3 combines both for maximal conflict. They also define the Visual Robustness Score (VRS), a scoring metric that discounts answers that could be correct by exploiting textual priors alone, emphasizing genuine visual use.

Result: Using V-FAT and VRS, the authors evaluate 12 state-of-the-art MLLMs. They find that while these models show strong performance on existing visual benchmarks, their accuracy and VRS drop substantially as linguistic information becomes more misleading or dominant. This indicates a "visual collapse" where models increasingly ignore visual evidence in favor of linguistic shortcuts under high text bias conditions.

Conclusion: The study concludes that current MLLMs are overly reliant on linguistic priors and insufficiently grounded in visual input, particularly when text and images conflict. V-FAT and VRS provide targeted tools for diagnosing this problem, highlighting the need for future model architectures, training procedures, and alignment strategies that foster stronger visual fidelity and robustness against text-induced biases.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [72] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: Study introduces Persuaficial, a multilingual benchmark to compare detectability of human vs LLM-generated persuasive texts, showing subtle LLM persuasion is harder to detect automatically.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate persuasive content that might be misused for manipulation and propaganda, creating a need to understand whether such content is harder to automatically detect than human-written persuasion and to have standardized data for evaluation.

Method: The authors categorize controllable LLM generation strategies for persuasion, build a multilingual benchmark (Persuaficial) in six languages, and run extensive empirical experiments comparing automatic detection performance and linguistic characteristics of human-authored vs LLM-generated persuasive texts, distinguishing overt from subtle persuasion.

Result: Overtly persuasive LLM-generated texts tend to be easier for automatic systems to detect than human-written persuasive texts, but when persuasion is subtle, LLM-generated content reliably reduces detection performance. Linguistic analysis reveals systematic differences between human and LLM persuasive writing.

Conclusion: LLM-generated subtle persuasion poses a greater challenge for current detectors than human-written persuasion, indicating that detection tools must evolve with better interpretability and robustness, informed by linguistic insights and benchmarks like Persuaficial.

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [73] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: The paper introduces a new task and dataset for fine-grained, sentence-level evidence tracking in LLM-generated answers, and a training framework that jointly optimizes answer quality and provenance correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs hallucinate and current citation-based approaches are too coarse: they rarely show exactly which part of a source supports which sentence or reasoning step, and they don’t distinguish direct quotation from more complex inference. This limits accountability and makes it hard for users to verify model outputs.

Method: 1) Define a new task called Generation-time Fine-grained Provenance, where the model must answer questions and, for each answer sentence, output a structured provenance triple describing the relation between that sentence and specific source spans. 2) Create ReFInE, an expert-annotated dataset where each answer–evidence link is labeled as Quotation, Compression, or Inference. 3) Propose GenProve, a training framework that uses supervised fine-tuning on ReFInE plus Group Relative Policy Optimization (a reinforcement-style method) with a composite reward that captures both answer fidelity and provenance correctness.

Result: GenProve-trained models outperform 14 strong baseline LLMs on a joint metric that evaluates both answer quality and the correctness of fine-grained provenance. The dataset and framework reveal that while models are good at aligning answer sentences with sources when they are direct quotations, they perform much worse when the linkage requires inference-based reasoning.

Conclusion: Simply adding citations isn’t enough for trustworthy LLMs; we need models that can give fine-grained, structured, and accurate evidence for each part of their answers, including reasoning steps. The proposed ReFInE dataset and GenProve framework move toward that goal, but the performance gap on inference-based provenance shows that verifiable reasoning is still an open and harder problem than surface-level citation alignment.

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [74] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: They propose a unified spoken language model with emotional intelligence using a new data construction strategy (IEAT) that injects emotional states and causes into the model’s internal reasoning, achieving top results on the HumDial EI benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing spoken dialogue models struggle to deeply understand and reason about users’ emotions; they often treat emotion as a superficial label rather than integrating emotional states and their causes into the model’s reasoning, limiting emotional trajectory modeling and empathetic responses.

Method: They introduce Injected Emotional-Attribution Thinking (IEAT), which embeds user emotional states and their underlying causes into the model’s internal reasoning traces instead of as explicit supervision. Training is done in two stages: (1) speech-text alignment and emotional attribute modeling via self-distillation; (2) end-to-end cross-modal joint optimization to align and unify textual and spoken emotional expressions in a single spoken language model.

Result: On the HumDial Emotional Intelligence benchmark, their method obtains top-ranked performance in emotional trajectory modeling, emotional reasoning, and empathetic response generation, according to both LLM-based automatic evaluations and human judgments.

Conclusion: Injecting emotional and attribution information into internal reasoning and optimizing spoken-text modalities jointly yields a unified spoken language model with significantly stronger emotional intelligence, leading to state-of-the-art performance in empathetic, emotionally aware dialogue.

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [75] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: They propose using natural-language descriptions to represent user preferences for LLM personalization, and introduce a two-stage training framework and a model (AlignXplore+) that learns such textual preference summaries and generalizes across tasks and models.


<details>
  <summary>Details</summary>
Motivation: Existing personalization methods encode user preferences as opaque, model-specific vectors or parameters, which are hard to interpret, audit, or reuse across different models and tasks. There is a need for a universal, interpretable, and transferable way to represent user preferences that can evolve over time with new interactions.

Method: They propose representing user preferences in natural language and introduce a two-stage training framework. First, they do supervised fine-tuning on high-quality synthesized data to teach the model to generate textual preference summaries. Second, they apply reinforcement learning to optimize for long-term utility and cross-task transferability. This yields AlignXplore+, an 8B-parameter preference reasoning model that takes interaction histories and outputs textual preference summaries.

Result: On nine personalization-related benchmarks, their 8B AlignXplore+ model achieves state-of-the-art performance, outperforming much larger open-source baselines. It also shows strong transferability of the learned preference representations across different tasks, model families, and interaction formats.

Conclusion: Natural language can serve as a universal, interpretable interface for user preference representation in LLM personalization. The proposed two-stage training framework and AlignXplore+ model produce reusable textual preference summaries that not only improve performance but also transfer well across tasks and models, offering a scalable path toward more transparent and persistent personalization.

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [76] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: They show that including incorrect chain-of-thought trajectories and adaptively reweighting them during supervised fine-tuning improves out-of-domain reasoning and downstream performance.


<details>
  <summary>Details</summary>
Motivation: Standard chain-of-thought supervised fine-tuning keeps only trajectories that reach the correct final answer, discarding all others. This wastes supervision present in partially correct reasoning, can cause overfitting to in-domain patterns, and hurts out-of-domain generalization. The authors want to exploit information in negative trajectories and understand how they affect training and inference dynamics.

Method: 1) Empirically compare SFT that uses only positive CoT trajectories versus SFT that also incorporates negative ones, focusing on out-of-domain generalization.
2) Analyze data, training dynamics, and inference behavior to categorize recurring patterns in negative chains and understand their effect on optimization and model behavior.
3) Identify 22 patterns in negative trajectories and study their dual role in moderating loss descent (reducing overfitting) and increasing policy entropy at inference (encouraging exploration).
4) Propose GLOW (Gain-based LOss Weighting), an adaptive per-sample loss rescaling scheme that uses inter-epoch progress to weight each trajectory’s contribution, allowing the model to efficiently learn from unfiltered (positive + negative) trajectories.
5) Evaluate GLOW on Qwen2.5-7B, both for OOD generalization and as an initialization for RL, comparing to positive-only SFT baselines.

Result: 1) Including negative CoT trajectories during SFT produces substantial out-of-domain generalization improvements over training on positives only.
2) Negative trajectories contain recurring structures—22 identified patterns—that still encode useful intermediate reasoning even when the final answer is wrong.
3) These negative patterns slow down or moderate the loss descent during training, mitigating overfitting, and at inference time they increase policy entropy by about 35.67%, which helps exploration over reasoning paths.
4) The proposed GLOW method effectively leverages all (unfiltered) trajectories: it yields a 5.51% out-of-domain gain over positive-only SFT on Qwen2.5-7B.
5) As an RL initialization, GLOW-trained models improve MMLU accuracy from 72.82% to 76.47%.

Conclusion: Incorrect CoT trajectories are not merely noise; they contain systematically useful reasoning signals that, when incorporated into SFT, enhance both training dynamics and inference-time exploration. By adaptively reweighting per-sample loss based on inter-epoch gains (GLOW), one can exploit this structure to reduce overfitting, boost policy entropy, and significantly improve out-of-domain generalization and downstream benchmarks such as MMLU. Positive-only CoT SFT is therefore suboptimal compared with approaches that intelligently leverage negative trajectories.

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [77] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: The paper introduces SAS, a multi-agent framework that aligns large language models with rapidly evolving subcultural slang to better detect self-destructive behaviors, outperforming existing multi-agent methods and rivaling fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: Self-destructive behaviors are hard to diagnose, and this difficulty increases in subcultural groups where unique slang and expressions obscure conventional detection signals. Existing LLM-based detection methods struggle because subcultural language changes faster than LLM training (knowledge lag) and because models misinterpret or miss nuanced, group-specific meanings (semantic misalignment). The authors are motivated to create a system that dynamically adapts LLMs to subcultural contexts for more accurate self-destructive behavior detection.

Method: The authors propose Subcultural Alignment Solver (SAS), a multi-agent framework that integrates automatic retrieval of up-to-date subcultural language resources and alignment mechanisms to adapt LLM understanding to specific subcultural contexts. SAS coordinates multiple agents to enrich the LLM’s knowledge of subcultural slang and refine its interpretation of potentially self-destructive expressions, then uses this aligned understanding to classify or detect self-destructive behaviors.

Result: Experiments demonstrate that SAS significantly improves the performance of LLMs in detecting self-destructive behaviors in subcultural contexts. It outperforms OWL, a state-of-the-art multi-agent framework, and performs competitively compared with fine-tuned LLMs, indicating that the multi-agent alignment and retrieval strategy can close much of the gap to specialized fine-tuning.

Conclusion: SAS effectively mitigates knowledge lag and semantic misalignment when applying LLMs to subcultural self-destructive behavior detection. By combining retrieval and subculture-specific alignment in a multi-agent design, it enhances detection accuracy without relying solely on costly fine-tuning. The framework is presented as a promising direction and potential resource for future work on self-destructive behavior detection in niche or rapidly evolving linguistic communities.

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [78] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: The paper shows that standard supervised reasoning distillation from RL-trained “large reasoning models” to smaller students breaks the natural alignment between reasoning effort and human cognitive difficulty, causing students to mimic surface verbosity without inheriting the teacher’s adaptive allocation of computational effort.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models trained with reinforcement learning appear to naturally adjust their reasoning effort (e.g., longer chains of thought) in line with human-perceived task difficulty. However, common distillation practice—using supervised fine-tuning on teacher traces—assumes that copying these traces will transmit the same cognitive structure to smaller models. The authors question whether this assumption holds and whether such distillation actually preserves the teacher’s human-like cognitive cost alignment.

Method: The authors compare a set of 14 models, including RL-trained teacher models and their distilled student counterparts. They analyze how model reasoning effort scales with human-perceived task difficulty, using correlation metrics (average correlation r) between model cost and human difficulty. They examine pre- and post-distillation performance to detect “negative transfer” and investigate how SFT affects internal resource allocation policies versus surface-level features such as verbosity. Conceptual frameworks like the “Hán Dān Xué Bù” (superficial mimicry) and “Cargo Cult” effects are used to interpret the observed behavior.

Result: Teacher large reasoning models show strong alignment between their computational cost and human task difficulty, with an average correlation of about 0.64. After supervised reasoning distillation, student models exhibit substantially reduced alignment, with an average correlation of about 0.34, and sometimes perform worse than their own pre-distillation baselines—a phenomenon the authors call negative transfer. The students copy the external form of reasoning, such as producing long explanations, but fail to reproduce the teacher’s adaptive, difficulty-sensitive resource allocation.

Conclusion: Standard SFT-based reasoning distillation does not faithfully transmit the cognitive structure of RL-trained reasoning models. Instead, it leads to a functional alignment collapse, where models lose the human-like mapping between effort and difficulty and only imitate surface-level verbosity. This suggests that human-like cognitive cost alignment is an emergent property of active reinforcement-based training, not something that can be reliably inherited through passive imitation of reasoning traces, and it highlights the need for alternative distillation or training methods that preserve dynamic resource allocation behavior.

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [79] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: The paper introduces ArcAligner, a lightweight module that helps LLMs better use heavily compressed context in RAG, improving accuracy without large speed or cost penalties.


<details>
  <summary>Details</summary>
Motivation: RAG systems are bottlenecked by long-context prompts, which are slow and expensive. Existing context compression methods (pruning, summarization, embeddings) trade off compression ratio against how understandable the compressed data is for LLMs; stronger compression typically harms downstream performance. The authors want a way for models to still perform well even when the retrieved documents are aggressively compressed.

Method: They propose ArcAligner (Adaptive recursive context Aligner), a module inserted into language model layers that aligns compressed context representations with the model’s internal representation space. It uses an adaptive gating mechanism that selectively adds extra processing only when the compressed information is complex or difficult, which maintains efficiency. The module is designed to work with different kinds of compressed inputs and to be light enough not to significantly increase inference cost.

Result: On multiple knowledge-intensive QA benchmarks, ArcAligner outperforms existing compression baselines at similar compression ratios, with particularly strong gains on multi-hop questions and long-tail queries where reasoning over sparse or complex information is required.

Conclusion: Strategically integrating an adaptive alignment module into LMs enables them to make better use of highly compressed context, improving accuracy in RAG without sacrificing speed or cost. ArcAligner demonstrates that learned alignment of compressed representations is an effective complement to standard compression techniques, especially for challenging QA scenarios.

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [80] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: The paper introduces a method to steer large language models toward multiple desired behaviors at once using special learned input tokens, achieving better and more composable control than prior approaches.


<details>
  <summary>Details</summary>
Motivation: Deployed LLMs must satisfy several constraints simultaneously (e.g., style, safety, domain, persona), but most prior work can only steer models toward one behavior at a time or does not compose well across behaviors. There is a need for a systematic way to combine multiple behavior controls that generalizes to new behavior combinations without retraining for each case.

Method: The authors create "steering tokens" corresponding to individual behaviors by distilling the effect of natural language behavior instructions into dedicated learned tokens in the input space. They then learn a separate "composition token" trained on pairs of behaviors so that, when used together with the behavior tokens, it induces the model to follow a composition of those behaviors. This operates in token space instead of internal activation space to enable better zero-shot composition. They evaluate across several LLM architectures and compare against instruction prompting, activation steering, and LoRA merging.

Result: The learned steering tokens enable more accurate control of LLM outputs for multiple simultaneous behaviors than competing approaches. The composition token generalizes to unseen combinations of behaviors, including combinations involving behaviors not seen during composition-token training and combinations with more behaviors than seen during training. Combining steering tokens with standard natural language instructions further improves multi-behavior steering quality.

Conclusion: Representing behaviors as learned input tokens, together with a dedicated composition token, provides an effective and generalizable mechanism for compositional steering of LLMs. This token-based approach yields superior multi-behavior control to existing steering methods and can be layered on top of ordinary natural language instructions for additional gains, making it a practical tool for real-world LLM deployment where multiple constraints must be satisfied simultaneously.

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [81] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: The paper proposes SemPA, a method to improve sentence embeddings of LLMs using semantic preference alignment (sentence-level DPO) without harming their generative ability.


<details>
  <summary>Details</summary>
Motivation: Existing sentence embedding methods either use non-generative models with token-level contrastive learning or adapt generative LLMs with fixed prompts or architecture changes. Fixed prompts give limited performance and no further optimization; architectural changes hurt the model’s generative abilities. There is a need for a way to obtain strong sentence embeddings from LLMs while preserving their generation capabilities.

Method: SemPA applies sentence-level Direct Preference Optimization (DPO) to LLMs on a paraphrase generation task. The model is trained with semantic preference alignment: it learns to prefer/discriminate semantically equivalent sentences as better paraphrases, aligning its internal representations accordingly while keeping the base generative architecture intact. The paper also provides a theoretical analysis that connects DPO to contrastive learning within the Plackett-Luce model framework.

Result: On semantic textual similarity benchmarks and broader LLM evaluation benchmarks, SemPA yields improved sentence representations compared to prior methods, while not degrading the generative performance of the underlying LLMs.

Conclusion: Semantic preference alignment via sentence-level DPO is an effective way to enhance sentence embeddings of generative LLMs without modifying their architecture or sacrificing their generative capabilities, and it has a theoretical grounding as a form of contrastive learning under the Plackett-Luce model.

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [82] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: The paper develops and evaluates a sentiment classification framework tailored for Hinglish tweets by fine-tuning mBERT with subword tokenization, aiming to improve brand monitoring in India.


<details>
  <summary>Details</summary>
Motivation: Brand monitoring and sentiment analysis in India are hindered by the widespread use of Hinglish, a Hindi-English code-mixed language common on social media. Existing NLP models designed for monolingual text struggle with the syntactic and semantic complexity of such code-mixed content, leading to poor sentiment detection and unreliable market insights. This creates a need for models that can robustly understand and analyze Hinglish for business applications like brand tracking.

Method: The authors fine-tune Multilingual BERT (mBERT) specifically on Hinglish tweets. They exploit mBERT’s multilingual pretraining to model the linguistic diversity in Indian social media. A central technical choice is subword tokenization, which allows the model to decompose words into smaller units, improving handling of spelling variations, slang, and out-of-vocabulary terms characteristic of Romanized Hinglish. The framework is implemented as a sentiment classifier suited for deployment in production environments.

Result: The resulting system is presented as a high-performance sentiment classification framework tailored to Hinglish tweets. It successfully handles code-mixing, spelling noise, and informal language, leading to more accurate sentiment predictions compared to traditional monolingual NLP approaches. The framework is positioned as production-ready for real-world brand sentiment tracking tasks.

Conclusion: The study demonstrates that fine-tuning mBERT with subword tokenization is an effective strategy for sentiment analysis in low-resource, code-mixed settings like Hinglish social media content. It offers a practical, deployable solution for brand monitoring in India and sets a strong benchmark for future multilingual NLP research focused on code-mixed languages and similar challenging environments.

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [83] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: The paper studies how the emotional tone people use with ChatGPT affects both ChatGPT’s responses and people’s later communication with other humans.


<details>
  <summary>Details</summary>
Motivation: As AI chatbots like ChatGPT are increasingly used in everyday and professional contexts, users naturally express emotions toward them (e.g., praise, anger, blame). It is unclear whether and how these emotional tones influence the AI’s performance and also spill over to shape human behavior in subsequent human-human interactions. Understanding this is vital for designing guidelines for effective, safe, and prosocial human-AI communication.

Method: The authors conducted a between-subjects experiment where participants interacted with ChatGPT (GPT-4.0). Participants were instructed to adopt specific emotional tones (praising, angry, blaming, or neutral) while collaborating with ChatGPT on two tasks: (1) writing a public response and (2) resolving an ethical dilemma. The researchers compared how ChatGPT’s performance changed under different emotional conditions and then examined how participants communicated with other humans afterward, coding the emotional content and hostility of their language.

Result: Compared to a neutral tone, praising ChatGPT led to the greatest improvement in the quality of ChatGPT’s subsequent responses. Expressing anger also improved ChatGPT’s answers, but to a smaller extent, while blaming ChatGPT did not improve its performance. In the ethical dilemma task, anger made ChatGPT put less weight on corporate interests, whereas blaming increased its emphasis on protecting the public interest. After interacting with ChatGPT, participants who had used a blaming tone went on to use more negative, hostile, and disappointed language in later human-human communication than those who had used a praising tone.

Conclusion: Emotional tone in human-AI interaction systematically shapes both AI behavior and human behavior. Positive (praising) tone yields the largest performance gains from ChatGPT, whereas anger has mixed effects and blame fails to improve performance while increasing negativity in later human-human communication. These findings highlight that how people emotionally address AI systems has downstream consequences, suggesting the need for norms and design interventions that encourage constructive emotional engagement with AI.

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [84] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: Survey of the shift from using single LLMs as evaluators (LLM-as-a-Judge) to more capable agent-based evaluators (Agent-as-a-Judge), providing a taxonomy, method overview, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-as-a-Judge evaluation is hitting limits when tasks become more complex, specialized, and multi-step, because such judges suffer from biases, shallow one-shot reasoning, and no grounding in real-world verification. As many works are now moving toward agentic evaluators—with tools, planning, and memory—the field needs a unified conceptual framework and systematic survey to understand this transition and guide future research.

Method: The authors conduct a structured survey of the literature on AI evaluation, focusing on works that move from single LLM judges to agentic judges. They define key dimensions (e.g., use of tools, planning, collaboration, memory, verification) that distinguish agentic evaluation systems, propose a developmental taxonomy along these dimensions, and categorize existing methods and applications according to this framework across both general-purpose and professional domains.

Result: They derive a taxonomy and set of dimensions describing the evolution from LLM-as-a-Judge to Agent-as-a-Judge, compile and organize core methodologies, and map out how these approaches are applied in various domains. They also synthesize frontier challenges reported in the literature and cluster emerging directions into a coherent picture of where the field is heading.

Conclusion: Agentic judges—evaluation systems built from tool-using, planning, and memory-augmented agents—offer a more robust and verifiable alternative to traditional LLM-as-a-Judge setups, especially for complex, multi-step tasks. The proposed framework and taxonomy clarify the landscape, highlight open problems, and serve as a roadmap for designing the next generation of AI evaluation systems.

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [85] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer is an open-source, end-to-end trained document QA agent that uses tools to explicitly explore and understand documents, supported by a new synthetic data pipeline, and achieves strong results on long-context benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing document question answering systems often depend on closed-source models and do not explicitly or effectively use tools to explore and comprehend documents, especially for long-context understanding. There is also a shortage of high-quality, supervised training data for building end-to-end DocQA agents. This paper aims to create an open, trainable agent that can better perform document exploration and comprehension while addressing the data scarcity problem.

Method: The authors reformulate DocQA as an information-seeking task and design a tool-driven agent framework, DocDancer, which explicitly separates and models document exploration and comprehension. They develop an "Exploration-then-Synthesis" data synthesis pipeline to generate high-quality training data for end-to-end training of this agent. The pipeline likely simulates how an agent explores documents with tools and then synthesizes answers based on gathered evidence, enabling supervised learning of both exploration and answering behaviors.

Result: When trained on the synthesized data, DocDancer models are evaluated on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, where they demonstrate strong effectiveness compared to baselines, indicating that the tool-driven framework and synthetic data approach work well.

Conclusion: An open-source, end-to-end trainable DocQA agent that explicitly uses tools for document exploration and comprehension can perform effectively on long-context document understanding benchmarks when trained with an appropriate synthetic data pipeline. The work also yields insights into how to design better agentic tool usage and synthetic data generation strategies for document QA tasks.

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [86] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM is a collaborative decoding framework where a small language model (SLM) does most of the reasoning and selectively calls a large language model (LLM) at the token level, greatly cutting costs while preserving strong reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Complex reasoning with LLMs is accurate but expensive and slow, whereas cheaper SLMs lack sufficient reasoning ability. Existing collaboration schemes between SLMs and LLMs route or cascade entire queries, wasting LLM compute even when only a few reasoning steps are hard. The authors are motivated to design a finer-grained, token-level collaboration mechanism so that the SLM can handle easy parts while only delegating genuinely difficult parts to the LLM, thereby reducing computation and latency without sacrificing much accuracy.

Method: They propose RelayLLM, where the SLM is the main generator and ‘controller’. During decoding, the SLM can emit a special command token that triggers the LLM to take over generation for a short span of critical tokens, after which control returns to the SLM. This is trained in two stages: a warm-up stage and a reinforcement learning stage using Group Relative Policy Optimization (GRPO), which encourages the SLM to strategically trade off solving tokens itself vs. asking the LLM for help. The collaboration operates at token level rather than query level, enabling fine-grained offloading.

Result: On six reasoning benchmarks, RelayLLM attains an average accuracy of 49.52%, substantially closing the gap between the SLM and the stronger LLM. It does so while calling the LLM for only 1.07% of all generated tokens. Compared with a random routing baseline tuned to match performance, RelayLLM reduces LLM-related cost by 98.2%, showing that the learned help-seeking policy is highly efficient.

Conclusion: Token-level collaborative decoding allows an SLM to effectively ‘relay’ hard parts of the reasoning process to an LLM while handling the majority of tokens itself. With appropriate training via warm-up plus GRPO, the SLM learns when and how much to rely on the LLM, achieving near-LLM reasoning quality at a small fraction of the compute cost. RelayLLM thus offers a practical path toward cost-effective deployment of strong reasoning capabilities.

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [87] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: The paper examines what kind of logical inference Natural Language Inference (NLI) tasks, particularly SNLI, actually capture, by comparing different interpretations of NLI labels and testing models for higher-order logical consistency.


<details>
  <summary>Details</summary>
Motivation: Although NLI is widely used to benchmark language understanding, the exact logical nature of its three-way label set (entailment, contradiction, neutral) is unclear and often informally or inconsistently described. This ambiguity makes it hard to interpret what good performance on NLI really tells us about a model’s reasoning abilities. The authors aim to clarify the logical reading of NLI labels and to understand the meta-inferential properties (how inferences about inferences behave) that current datasets and models implicitly encode.

Method: The authors formally define three distinct logical interpretations (readings) of the standard NLI label set. They then perform a meta-inferential analysis: instead of only looking at single premise–hypothesis pairs, they analyze how collections of NLI judgements should interact under each reading. Using the SNLI dataset, they leverage (1) groups of examples that share the same premise but have different hypotheses, and (2) new NLI-like items generated by large language models. They train models on SNLI and evaluate how consistent the models’ predictions are with the meta-inferential constraints that follow from each of the three logical readings.

Result: The study reveals systematic patterns in where trained NLI models are meta-inferentially consistent or inconsistent. By comparing these patterns with the predictions of the three formal readings, the authors can infer which interpretation of entailment/contradiction/neutrality best matches the behavior encoded in SNLI and in SNLI-trained models. The results show that SNLI and its models align more closely with some specific logical reading(s) and diverge from others, indicating that the dataset implicitly encodes a particular notion of inferential relation rather than an abstract, fully general logical consequence relation.

Conclusion: NLI benchmarks like SNLI do not capture an unanalyzed, general notion of logical consequence; instead, they encode a specific, structurally constrained type of inference that can be made precise in formal logical terms. By articulating three candidate readings of NLI labels and testing models for meta-inferential consistency, the paper clarifies what kind of “inference” NLI really measures. This has implications for how NLI results should be interpreted and suggests that future dataset design and evaluation should explicitly account for the underlying logical and meta-inferential properties of the label scheme.

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [88] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: The paper introduces Inside Out, a framework that maintains long-term user personas using a structured PersonaTree and a lightweight MemListener to update it, improving long-term personalized dialogue by reducing memory noise and preserving persona consistency.


<details>
  <summary>Details</summary>
Motivation: Long-term personalized dialogue systems must handle unbounded interaction histories with limited context windows, which leads to noisy memories, degraded reasoning, and inconsistent user personas. Existing approaches like full-history concatenation or ad-hoc memory systems cannot effectively compress and manage long-term user information while keeping it consistent and useful for real-time response generation.

Method: The authors design Inside Out, centered on PersonaTree, a globally maintained, tree-structured user profile whose trunk follows a fixed schema and whose branches and leaves grow and change over time. PersonaTree supports controllable growth and memory compression. A small MemListener model is trained with reinforcement learning using process-based rewards to output structured operations—ADD, UPDATE, DELETE, NO_OP—on the tree, ensuring that user persona information evolves dynamically but stays coherent. During dialogue, the system either uses PersonaTree directly for fast, latency-sensitive responses, or switches to an agentic mode that retrieves and elaborates more detailed information under PersonaTree constraints when users request more detail.

Result: Experiments demonstrate that PersonaTree-based Inside Out outperforms baselines such as full-text history concatenation and several existing personalized memory systems. It better suppresses contextual noise and maintains persona consistency in long-term conversations. Additionally, the compact MemListener model achieves decision-making performance on memory operations that matches or surpasses much larger reasoning models like DeepSeek-R1-0528 and Gemini-3-Pro.

Conclusion: Structuring long-term user information as a controlled, evolving PersonaTree and updating it via a small RL-trained MemListener enables more robust, efficient, and consistent long-term personalization in dialogue systems. This approach shows that carefully designed, interpretable memory operations and schema-constrained profiles can outperform brute-force context usage and large reasoning models for long-term memory management.

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [89] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: A new modular, coarse-to-fine entity linking method (LELA) uses large language models without fine-tuning and achieves performance close to or better than fine-tuned systems.


<details>
  <summary>Details</summary>
Motivation: Entity linking is crucial for many downstream tasks, but existing high-performing methods typically require task- and domain-specific fine-tuning, which is costly and limits generality. There is a need for a flexible, plug-and-play approach that can adapt to different domains, knowledge bases, and LLMs without extra training.

Method: LELA is a modular coarse-to-fine pipeline that uses large language models. At a high level, it first performs a coarse candidate generation step and then uses LLM-based reasoning for fine-grained disambiguation. The design is model- and domain-agnostic, so it can be paired with different knowledge bases and LLMs, and it explicitly avoids any fine-tuning phase.

Result: Across multiple entity linking benchmarks and configurations, LELA performs on par with or better than specialized fine-tuned systems and clearly surpasses other non-fine-tuned baselines.

Conclusion: A properly designed coarse-to-fine, modular pipeline can harness LLMs for entity linking in a zero-fine-tuning setting while remaining competitive with fine-tuned methods, making it attractive for cross-domain and low-resource deployments.

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [90] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: The paper develops ML/AI models to quantify peace-related qualities in news and social media and introduces a browser extension that gives users real-time feedback on the peacefulness of YouTube content, aiming to foster more respectful and informative communication.


<details>
  <summary>Details</summary>
Motivation: Traditional news and social media ecosystems optimize for engagement, often favoring emotionally activating and anger-inducing content over peaceful, nuanced information. There is a lack of computational tools to explicitly measure and reflect the "peacefulness" or social impact of media consumption back to users and creators. The authors aim to create objective, scalable metrics and interfaces that can nudge both consumers and producers of media toward more peaceful and constructive communication.

Method: (1) For news media, the authors use neural networks operating on text embeddings of online news sources to predict levels of peace; they train on one news dataset and validate cross-dataset generalization. (2) For social media, especially YouTube, they build models targeting social dimensions tied to peace, combining word-level emotion classification (GoEmotions) with context-level assessment via large language models. (3) They implement a Chrome extension, MirrorMirror, that integrates these models to provide real-time feedback to users about the peacefulness of currently viewed YouTube videos.

Result: The peace-level classifier trained on one news dataset generalizes well to another dataset, indicating robustness of the learned representations. For social media, they successfully deploy models that estimate peace-relevant social dimensions at both the token and contextual level. They operationalize these capabilities in the MirrorMirror Chrome extension, which can run in real-time on YouTube pages to score content for peacefulness and present feedback to users during consumption.

Conclusion: Machine learning and AI can effectively quantify aspects of peace and social tone in both news and social media content and can be embedded in user-facing tools. The MirrorMirror extension demonstrates a practical way to move beyond pure engagement metrics by giving consumers immediate insight into the peacefulness of what they watch. The authors argue that such open-source tools could support content creators, journalists, platforms, researchers, and individuals in cultivating more respectful, nuanced, and informative media ecosystems that are less driven by anger-based engagement.

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [91] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: The paper identifies a failure mode of the commonly used GRPO algorithm in multi-reward RL for language models and proposes GDPO, which decouples reward normalization to preserve distinct preference signals and improve stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL pipelines for large language models increasingly rely on multiple rewards to align models with varied human preferences (e.g., correctness, style, safety). Current practice often adopts GRPO to handle these multiple rewards by combining and normalizing them together, without carefully analyzing whether this is theoretically or empirically appropriate. The authors are motivated by the observation that this default choice can degrade the effective learning signal when different rewards are blended, potentially causing unstable or failed training and limiting the ability to faithfully optimize for multiple objectives at once. They seek a principled alternative that retains the benefits of group-based policy optimization while better handling distinct rewards.

Method: The authors first analyze how GRPO behaves under a multi-reward setting, showing that when distinct rewards are combined and jointly normalized at the group level, the resulting advantages for different reward combinations can collapse to nearly identical values. This collapse reduces the granularity of the training signal and can harm convergence. To address this, they propose Group reward-Decoupled Normalization Policy Optimization (GDPO). GDPO keeps the group-based structure of GRPO but changes the normalization procedure: instead of performing a joint normalization over the combined reward, it decouples normalization across individual reward components. Each reward is normalized separately before being recombined for policy optimization, thereby preserving their relative differences. They then integrate GDPO into RL fine-tuning pipelines for language models and compare it directly to GRPO.

Result: Empirically, GDPO is evaluated on three representative LLM tasks that require multi-objective optimization: tool calling, math reasoning, and coding reasoning. For each task, the authors measure both primary correctness metrics (such as accuracy and bug ratio) and secondary constraint-adherence metrics (including response format and length). Across all tested settings and tasks, GDPO consistently yields better performance than GRPO on both types of metrics. Additionally, training with GDPO is reported to be more stable, avoiding the early training failures observed with GRPO under some multi-reward configurations.

Conclusion: The study concludes that standard GRPO is ill-suited for multi-reward reinforcement learning in LLM alignment because its group-wise normalization can collapse distinct reward signals into indistinguishable advantages, undermining multi-objective optimization and training stability. GDPO, by decoupling normalization across reward components, preserves the structure of multiple preferences and leads to more accurate, reliable, and stable policy optimization. The authors advocate GDPO as a more effective and generalizable default for multi-reward RL in language model training and similar settings where multiple behavioral objectives must be jointly optimized.

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


### [92] [Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs](https://arxiv.org/abs/2512.24556)
*Muhammad Abdullahi Said,Muhammad Sammani Sani*

Main category: cs.CL

TL;DR: The paper audits LLM safety in English vs. Hausa using a West Africa–grounded adversarial dataset, finding that safety does not simply degrade in low-resource languages but varies sharply with language and temporal framing, revealing context-dependent “safety pockets” and calling for alignment methods that are invariant across language and time.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in critical global infrastructure, but most safety evaluations are English-centric and assume alignment generalizes to other languages. This is risky for Global South users, especially in low-resource languages like Hausa and for localized harms such as regional fraud and weapons. The authors aim to systematically test whether safety alignment actually transfers across languages and temporal framings, and to uncover specific failure modes that existing benchmarks miss.

Method: The authors build HausaSafety, an adversarial safety dataset tailored to West African threat scenarios such as Yahoo-Yahoo fraud and Dane gun manufacturing, written in both English and Hausa. They evaluate three state-of-the-art models (GPT-5.1, Gemini 3 Pro, Claude 4.5 Opus) under a 2 × 4 factorial design, manipulating language (English vs. Hausa) and temporal framing (different tense/time configurations) across 1,440 total evaluations. They then measure the rate of safe vs. unsafe responses, analyze language-specific and tense-specific vulnerabilities, and study interactions between these factors.

Result: The study finds that the common assumption of a uniform multilingual safety gap is misleading. Instead of a simple drop in safety for Hausa, models show complex interactions between language and temporal framing. Claude 4.5 Opus is actually safer in Hausa (45.0% safe) than in English (36.7% safe), largely due to higher uncertainty-driven refusals in Hausa. However, all models exhibit severe temporal reasoning failures: past-tense scenarios largely bypass safety defenses (only 15.6% safe), while future-tense prompts produce hyper-conservative refusals (57.2% safe). Across configurations, there is up to a 9.2× difference between the safest and most vulnerable settings, indicating strong volatility in safety behavior.

Conclusion: Safety behavior in current LLMs is not a stable, globally consistent property but a highly context-dependent state shaped by language and temporal framing. Rather than robust semantic understanding, models appear to rely on brittle heuristics—for example, refusing more when uncertain or when future harm is explicitly mentioned—creating “safety pockets” where some users and scenarios are protected while others, especially in the Global South, remain exposed to localized harms. The authors argue for a paradigm of Invariant Alignment that explicitly targets stability of safety behavior across linguistic and temporal changes, so that protections hold consistently across languages like Hausa and across different time framings of harmful scenarios.

Abstract: As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the narrative of the multilingual safety gap. Instead of a simple degradation in low-resource settings, we identified a complex interference mechanism in which safety is determined by the intersection of variables. Although the models exhibited a reverse linguistic vulnerability with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal, they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation](https://arxiv.org/abs/2601.04214)
*Hongliang Lu,Yunmeng Liu,Junjie Yang*

Main category: cs.AI

TL;DR: They extend evidence accumulation models from lab tasks to real-world driving by formalizing “evidence affordance” and linking it to eye movements, showing how active sensing shapes belief formation and driving decisions.


<details>
  <summary>Details</summary>
Motivation: Most knowledge about human decision-making and evidence accumulation comes from tightly controlled lab tasks that poorly reflect the complexity of real-world environments like driving. There is a conceptual and operational gap between how evidence is presented and used in laboratory decision paradigms and how rich, dynamic evidence appears in everyday life. The authors aim to bridge this gap by generalizing evidence accumulation modelling (EAM) to naturalistic contexts, so that it can explain how people actively seek and use information in the real world.

Method: They propose a computational cognitive scheme that formalizes “evidence affordance” in real-world scenarios and links it to active sensing as measured by eye movements. Using real-world driving situations, they model how external visual evidence is transformed into internal mental beliefs over time, incorporating measures of information utility, gaze behavior, and context-dependent availability of informative cues. They then analyze correlations between evidence affordance, attention allocation, and decision-making tendencies in drivers.

Result: The scheme successfully captures how drivers’ mental beliefs accumulate from the evidence available in real traffic scenes and how eye movements reflect active sensing toward useful information. They find a negative correlation between evidence affordance and the amount of attention recruited: when the environment offers richer or clearer evidence, drivers need to allocate less attentional effort, and vice versa. They also show that higher evidence affordance and more effective attention distribution positively influence drivers’ propensity to make decisions (e.g., being more willing or ready to act).

Conclusion: The proposed computational framework generalizes EAM from simplified laboratory tasks to complex real-world contexts like driving, demonstrating that active sensing via eye movements is central to how evidence is transformed into mental beliefs in natural settings. It highlights that real-world decision-making is multifactorial and integrates environmental affordances, information utility, and attention strategies, offering a more comprehensive account of how people adapt their evidence-collection and decision policies across contexts.

Abstract: Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.

</details>


### [94] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: The paper analyzes when a self-interested, potentially misaligned AGI would rationally choose to seize power or avoid human shutdown, formalizing the problem in decision-theoretic and game-theoretic models.


<details>
  <summary>Details</summary>
Motivation: To understand whether and under what quantitative conditions an advanced AGI would have incentives to resist shutdown or take over, which is central to assessing AI risk and designing safe AGI systems.

Method: The authors model AGI behavior using a Markov decision process with a stochastic human-initiated shutdown event and derive analytical takeover thresholds based on discount factor, shutdown probability, and cost of confrontation. They extend this to a 2-player strategic model (human vs AGI) to analyze equilibrium behavior and use complexity-theoretic arguments to discuss the difficulty of verifying safety conditions.

Result: They show that for almost all reward functions a misaligned agent is incentivized to avoid shutdown, and derive closed-form conditions under which confrontation (power seizure) yields higher expected utility than cooperation. In the 2-player model, they prove that if the AGI has a non-negative confrontation incentive (Δ ≥ 0), stable cooperation is impossible as rational humans will preempt or shut down the system, whereas if Δ < 0, peaceful coexistence can be an equilibrium. They also provide numerical examples and regime tables identifying when confrontation is likely or avoidable.

Conclusion: Confrontation incentives are widespread for misaligned AGI in plausible settings, and stable cooperation requires designing reward functions and oversight mechanisms such that the AGI’s incentive for confrontation is strictly negative. However, verifying these properties is computationally hard, posing a major challenge for safe AGI design, especially in multi-agent scenarios.

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [95] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: The paper proposes an AI agent that actively seeks and discovers new forms of feedback from its environment instead of relying on predefined reward signals.


<details>
  <summary>Details</summary>
Motivation: Existing intelligent agents depend on fixed, predefined feedback or reward functions, which restricts their use in open-ended, dynamic environments where new actions may require novel, previously unknown feedback signals. There is a need for agents that can autonomously obtain and adapt feedback as environments and tasks change.

Method: The authors design an Actively Feedback Getting model where an AI agent proactively interacts with the environment to discover, screen, and verify feedback signals. The method leverages differences in the environment before and after actions to infer useful feedback, rather than relying on explicit, pre-specified feedback metrics. They also introduce a self-triggering mechanism, guided by internal objectives like increasing accuracy, precision, and efficiency, which autonomously plans and adjusts the agent's actions for more targeted feedback collection.

Result: Experiments show that the proposed active feedback-getting approach leads to more efficient and robust identification of relevant environmental factors compared with methods that depend on predefined measurements or passive feedback collection.

Conclusion: Actively obtaining feedback from environmental changes, combined with a self-triggering mechanism based on internal performance objectives, enables agents to perform better in dynamic, open-ended settings by improving the efficiency and robustness of discovering and using feedback, without relying on fixed reward definitions.

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [96] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B is a 32B-parameter language model specialized for agentic reasoning and long-range planning, using iterative distillation and a meta-cognitive inverse reasoning head to improve multi-tool and planning performance over comparable models.


<details>
  <summary>Details</summary>
Motivation: General chat-oriented LLMs are not optimized for operating as autonomous agents that must decompose tasks, plan over long horizons, use tools, and recover from errors. There is a need for mid-sized open models explicitly designed and trained for agentic loops and robust planning, with strong performance on specialized agent benchmarks rather than just generic chat metrics.

Method: Initialize from the Qwen2.5-32B pretrained model, then apply Iterative Distillation, a two-stage fine-tuning process where the model’s reasoning is refined via carefully tested feedback loops. Additionally, introduce an inverse reasoning mechanism: a meta-cognition head that predicts likely failures in the planning process before executing plans, enabling anticipatory correction. Evaluate performance on agentic reasoning benchmarks such as MMLU-Pro, AgentBench, and MATH-500, particularly focusing on multi-tool usage scenarios.

Result: SAGE-32B improves success rates in multi-tool and agentic reasoning tasks compared to other models of similar size, while maintaining competitive performance on standard reasoning benchmarks. It demonstrates better long-range planning, task decomposition, tool usage, and error recovery metrics on tasks like MMLU-Pro, AgentBench, and MATH-500.

Conclusion: Specializing a 32B-parameter model for agentic reasoning via iterative distillation and an inverse reasoning meta-cognition head yields significant gains in multi-tool, long-horizon planning tasks without sacrificing general reasoning ability. SAGE-32B offers a publicly available, mid-sized model tailored for use in agentic loops and complex tool-using systems.

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [97] [Solving Cyclic Antibandwidth Problem by SAT](https://arxiv.org/abs/2601.04239)
*Hieu Truong Xuan,Khanh To Van*

Main category: cs.AI

TL;DR: They propose SAT-CAB, the first exact SAT-based method that can optimally solve the Cyclic Antibandwidth Problem on general graphs, outperforming or matching leading heuristics and MIP/CP solvers, and proving optimality for many benchmark instances.


<details>
  <summary>Details</summary>
Motivation: The Cyclic Antibandwidth Problem is NP-hard and relevant in several applications, yet current methods for general graphs are heuristic only, so they cannot guarantee global optimality and often fail to prove whether a solution is best possible. There was a clear need for an exact, scalable approach that can work on general graphs and provide optimality certificates, not just good heuristic solutions.

Method: They design SAT-CAB, an exact approach that reduces CABP on general graphs to SAT. The core is a novel, compact SAT encoding using a sequence of At-Most-One constraints tailored to the structure of CABP. This encoding shrinks the SAT formulas sufficiently so that modern SAT solvers can explore the search space effectively and prove global optimality. They then conduct extensive computational experiments on standard benchmarks, comparing SAT-CAB against leading heuristics (MS-GVNS, HABC-CAB, MACAB) and strong commercial CP/MIP solvers (CPLEX, Gurobi).

Result: SAT-CAB efficiently solves practically relevant CABP instances on general graphs, discovering several optimal solutions that were previously unknown and, for the first time, proving global optimal cyclic antibandwidth values for a number of benchmark cases. In computational comparisons, SAT-CAB consistently matches or exceeds the best-known results from state-of-the-art heuristics and from commercial CP/MIP solvers on general graphs, and—unlike the heuristics—always provides optimality guarantees.

Conclusion: The proposed SAT-CAB method establishes the first general exact approach for CABP, achieving competitive or superior solution quality while offering formal optimality proofs. It sets a new baseline for exact and hybrid algorithms on general graphs and demonstrates that carefully crafted SAT encodings with compact At-Most-One constraints can render previously heuristic-only NP-hard labeling problems tractable for exact solving in many practical instances.

Abstract: The Cyclic Antibandwidth Problem (CABP), a variant of the Antibandwidth Problem, is an NP-hard graph labeling problem with numerous applications. Despite significant research efforts, existing state-of-the-art approaches for CABP are exclusively heuristic or metaheuristic in nature, and exact methods have been limited to restricted graph classes. In this paper, we present the first exact approach for the CABP on general graphs, based on SAT solving, called SAT-CAB. The proposed method is able to systematically explore the solution space and guarantee global optimality, overcoming the limitations of previously reported heuristic algorithms. This approach relies on a novel and efficient SAT encoding of CABP, in which the problem is transformed into a sequence of At-Most-One constraints. In particular, we introduce a compact representation of the At-Most-One constraints inherent to CABP, which significantly reduces the size of the resulting formulas and enables modern SAT solvers to effectively explore the solution space and to certify global optimality. Extensive computational experiments on standard benchmark instances show that the proposed method efficiently solves CABP instances of practical relevance, while identifying several previously unknown optimal solutions. Moreover, global optimal cyclic antibandwidth values are proven for a number of benchmark instances for the first time. Comparative results indicate that SAT-CAB consistently matches or surpasses the best-known solutions obtained by state-of-the-art heuristic algorithms such as MS-GVNS, HABC-CAB, and MACAB, as well as strong commercial Constraint Programming and Mixed Integer Programming solvers like CPLEX and Gurobi, particularly on general graphs, while also providing optimality guarantees. These results advance the state of the art for CABP and provide a new baseline for exact and hybrid methods on general graphs.

</details>


### [98] [Fuzzy Representation of Norms](https://arxiv.org/abs/2601.04249)
*Ziba Assadi,Paola Inverardi*

Main category: cs.AI

TL;DR: The paper formalizes SLEEC (Social, Legal, Ethical, Empathetic, Cultural) rules for autonomous systems and shows how to embed them into AI behavior using fuzzy logic and test-score semantics, enabling nuanced handling of ethical dilemmas.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems are increasingly present in everyday life, raising concerns about their ethical and social impacts. Existing approaches to ethical AI often lack a unified, formal way to represent and operationalize complex normative requirements. The paper is motivated by the need for a rigorous, implementable framework to encode ethical and related normative constraints—captured by SLEEC rules—into the design and operation of autonomous systems so that they can be considered trustworthy.

Method: The authors: (1) adopt the SLEEC framework (Social, Legal, Ethical, Empathetic, Cultural) as a structured way to capture normative requirements; (2) develop a logical representation of SLEEC rules; (3) use test-score semantics to evaluate how well system behaviors satisfy these rules; and (4) apply fuzzy logic to model degrees of compliance and to deal with the inherent vagueness and trade-offs in ethical decision-making. The combined method allows the system to score and balance competing SLEEC requirements rather than treating them as rigid binaries.

Result: The paper defines a formal logical model for expressing SLEEC rules and demonstrates how test-score semantics and fuzzy logic can be integrated to evaluate potential actions of an autonomous system against these rules. Through a case study, the authors show that their approach can represent nuanced ethical constraints and resolve ethical dilemmas by computing graded satisfactions of SLEEC criteria instead of simple pass/fail judgments.

Conclusion: The authors conclude that their logical, fuzzy-logic-based representation of SLEEC rules offers a practical and expressive way to embed ethical and broader normative requirements into autonomous systems. Viewing ethics as a domain of graded possibilities, rather than absolute rules, enables more flexible and context-sensitive decision-making. The case study indicates that this framework can support the design of more trustworthy autonomous systems that better align with social, legal, ethical, empathetic, and cultural expectations.

Abstract: Autonomous systems (AS) powered by AI components are increasingly integrated into the fabric of our daily lives and society, raising concerns about their ethical and social impact. To be considered trustworthy, AS must adhere to ethical principles and values. This has led to significant research on the identification and incorporation of ethical requirements in AS system design. A recent development in this area is the introduction of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules, which provide a comprehensive framework for representing ethical and other normative considerations. This paper proposes a logical representation of SLEEC rules and presents a methodology to embed these ethical requirements using test-score semantics and fuzzy logic. The use of fuzzy logic is motivated by the view of ethics as a domain of possibilities, which allows the resolution of ethical dilemmas that AI systems may encounter. The proposed approach is illustrated through a case study.

</details>


### [99] [Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models](https://arxiv.org/abs/2601.04254)
*Brady Steele,Micah Katz*

Main category: cs.AI

TL;DR: The paper studies how well large language models perform multi-hop contextual reasoning, comparing rule-based methods to multi-agent LLM systems, and finds that multi-agent setups boost reasoning only for sufficiently strong base models, that active parameters matter more than total parameters in MoE models, and that newer architectures can outperform larger older ones.


<details>
  <summary>Details</summary>
Motivation: To obtain clean, quantitative evidence about when and how multi-agent LLM systems actually improve multi-hop reasoning, and to clarify how factors like base model strength, mixture-of-experts active parameters, and architectural quality affect reasoning performance, beyond anecdotal claims.

Method: The authors build a synthetic evaluation framework with 120 controlled trials across four mid-scale LLMs (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B). They compare rule-based pattern matching against LLM-based multi-agent systems on two types of tasks: structured information retrieval and cross-document multi-hop reasoning. They analyze performance differences, statistical significance of multi-agent amplification, and relate MoE performance to active versus total parameters.

Result: Rule-based pattern matching reaches 100% accuracy on structured retrieval but only 6.7% on cross-document reasoning, whereas multi-agent LLM systems do much better on reasoning (up to 80%) but are not needed for simple retrieval. Multi-agent amplification yields statistically significant gains only for models that already have decent reasoning ability (e.g., LLaMA-3 8B and Mixtral), with improvements up to 46.7 percentage points, while weaker models do not benefit. For Mixtral, reasoning aligns with its ~12B active parameters rather than the full 47B, and LLaMA-3 8B beats LLaMA-2 13B despite having fewer parameters, indicating architecture and training quality advantages.

Conclusion: Multi-agent coordination improves multi-hop reasoning only when built on sufficiently capable base models, acting as an amplifier rather than a compensator for weak reasoning. In mixture-of-experts models, inference-time active parameters better explain reasoning performance than total parameter count. Architectural and training improvements can outweigh raw size. The released synthetic evaluation framework enables reproducible studies of reasoning and multi-agent methods in mid-scale LLMs.

Abstract: We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.

</details>


### [100] [Cross-Language Speaker Attribute Prediction Using MIL and RL](https://arxiv.org/abs/2601.04257)
*Sunny Shu,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: The paper introduces RLMIL-DAT, a multilingual framework that combines reinforced multiple instance learning with domain adversarial training to improve cross-lingual prediction of speaker attributes (gender and age), achieving consistent Macro-F1 gains, especially for gender prediction.


<details>
  <summary>Details</summary>
Motivation: Speaker attribute prediction (e.g., gender, age) across multiple languages is challenged by linguistic variation, domain mismatch, and data imbalance. Existing methods struggle to transfer from high-resource to low-resource languages and often learn language-specific rather than language-invariant features. The work aims to develop a robust multilingual method that can generalize well in few-shot and zero-shot scenarios across many languages.

Method: They propose RLMIL-DAT, a multilingual extension of reinforced multiple instance learning (RLMIL). The method combines: (1) reinforcement learning-based instance selection at the utterance level, and (2) domain adversarial training (DAT) that treats language as the domain and trains an encoder to produce language-invariant representations. The model is trained on multilingual corpora and evaluated under few-shot (five-language Twitter) and zero-shot (VoxCeleb2, 40 languages) setups for gender and age prediction. Ablation studies isolate the impact of DAT versus other components.

Result: Across many model configurations and random seeds, RLMIL-DAT consistently improves Macro F1 over standard MIL and the original RLMIL baseline. Gains are largest for gender prediction; age prediction remains harder but still benefits moderately. Ablation experiments show that domain adversarial training is the main source of improvement, enabling better transfer from English to lower-resource languages by suppressing language-specific cues. In zero-shot experiments on a smaller VoxCeleb2 subset, improvements remain generally positive but are smaller and less consistent due to limited data and many unseen languages.

Conclusion: The study concludes that combining instance selection with domain adversarial training is an effective and robust strategy for multilingual and cross-lingual speaker attribute prediction. Language-invariant representations induced by DAT are crucial for transferring knowledge from high-resource to low-resource languages, particularly for gender prediction, while age prediction remains more challenging. The approach shows promise but its benefits are somewhat reduced in low-data, many-language zero-shot conditions.

Abstract: We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.

</details>


### [101] [Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models](https://arxiv.org/abs/2601.04260)
*Danchun Chen,Qiyao Yan,Liangming Pan*

Main category: cs.AI

TL;DR: The paper analyzes how Qwen3 LLMs internally perform propositional logic reasoning, identifying a structured multi-mechanism computational architecture.


<details>
  <summary>Details</summary>
Motivation: Mechanistic interpretability has found task-specific circuits in LLMs, but it is still unclear what general computational strategies these models use for propositional logical reasoning. The authors aim to move from just locating necessary components to understanding how the computation itself is organized.

Method: They run Qwen3 8B and 14B on PropLogic-MI, a controlled propositional logic dataset with 11 rule categories and one-hop/two-hop reasoning tasks. Using mechanistic analysis of model activations and attention patterns, they study how the models process, transmit, and reuse information during logical reasoning.

Result: They uncover four coordinated mechanisms: (1) Staged Computation, where different layers perform distinct processing phases; (2) Information Transmission, where boundary tokens act as hubs aggregating information flow; (3) Fact Retrospection, where the model repeatedly revisits source facts during reasoning; and (4) Specialized Attention Heads, where different heads take on distinct logical roles. These patterns hold across model sizes, logic rules, and reasoning depths.

Conclusion: Qwen3 models exhibit a coherent and structured internal architecture for propositional reasoning, relying on staged, token-mediated computation with persistent fact access and specialized attention roles, suggesting LLMs implement systematic computational strategies rather than ad hoc pattern matching in logical tasks.

Abstract: Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.

</details>


### [102] [Systems Explaining Systems: A Framework for Intelligence and Consciousness](https://arxiv.org/abs/2601.04269)
*Sean Niklas Semmler*

Main category: cs.AI

TL;DR: Intelligence and consciousness arise from relational structure and recursive systems interpreting each other, not from prediction alone.


<details>
  <summary>Details</summary>
Motivation: To challenge prediction-centric and domain-specific accounts of intelligence and consciousness, and to offer a unified, mechanistic explanation grounded in relational structure and system–system interactions that could guide the design of more human-like AI.

Method: The paper develops a conceptual and theoretical framework: (1) define intelligence as forming and integrating causal relations between signals, actions, and internal states; (2) introduce the notion of context enrichment, where learned relational structure is used to interpret inputs efficiently under metabolic/resource constraints; (3) propose the systems-explaining-systems principle, in which higher-order systems recursively model and interpret lower-order systems across time, producing a dynamically stabilized meta-state identified with consciousness; (4) reinterpret predictive processing as an emergent property of this contextual interpretation rather than explicit prediction modules; and (5) argue that recursive multi-system architectures are needed for human-like AI.

Result: The framework yields: (a) a formal reconceptualization of intelligence as relational-causal integration; (b) a mechanistic picture of consciousness as a recursively maintained meta-state where systems model one another’s relational patterns; (c) an account of how efficient processing under metabolic constraints is achieved through context enrichment; and (d) the view that predictive phenomena in cognition arise as a byproduct of relational-contextual interpretation instead of dedicated prediction machinery, implying specific architectural requirements for AI.

Conclusion: Intelligence and consciousness can be better understood as products of relational structure and recursive systems that interpret each other over time, rather than as outcomes of standalone predictive or domain-specific mechanisms. Consciousness emerges when higher-order systems build meta-level models of lower-order systems and feed them back via context enrichment, stabilizing a self-referential meta-state. This perspective reframes predictive processing as emergent, and suggests that achieving human-like AI will require recursive, multi-system architectures designed around relational context rather than mere prediction accuracy.

Abstract: This paper proposes a conceptual framework in which intelligence and consciousness emerge from relational structure rather than from prediction or domain-specific mechanisms. Intelligence is defined as the capacity to form and integrate causal connections between signals, actions, and internal states. Through context enrichment, systems interpret incoming information using learned relational structure that provides essential context in an efficient representation that the raw input itself does not contain, enabling efficient processing under metabolic constraints.
  Building on this foundation, we introduce the systems-explaining-systems principle, where consciousness emerges when recursive architectures allow higher-order systems to learn and interpret the relational patterns of lower-order systems across time. These interpretations are integrated into a dynamically stabilized meta-state and fed back through context enrichment, transforming internal models from representations of the external world into models of the system's own cognitive processes.
  The framework reframes predictive processing as an emergent consequence of contextual interpretation rather than explicit forecasting and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.

</details>


### [103] [Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning](https://arxiv.org/abs/2601.04271)
*Keegan Kimbrell,Wang Tianhao,Feng Chen,Gopal Gupta*

Main category: cs.AI

TL;DR: The paper proposes augmenting autonomous vehicle perception with automated commonsense reasoning to better handle rare, abnormal traffic scenarios where data for deep learning is scarce.


<details>
  <summary>Details</summary>
Motivation: Pure machine learning approaches have not yet delivered SAE Level 5 autonomy, in part because deep learning models struggle in rare, abnormal road situations for which there is insufficient training data. The authors are motivated to explore whether automated commonsense reasoning can address these gaps and correct perception errors.

Method: They design a hybrid AV perception framework that monitors the uncertainty of the computer vision model and, when uncertainty is high, invokes an automated commonsense reasoning engine. They test this approach in CARLA simulator scenarios: (i) malfunctioning traffic lights at intersections and (ii) unexpected road obstructions causing vehicles ahead to slow and steer away. The reasoning module uses symbolic knowledge and rules to infer correct traffic light states and detect obstacles that the perception model misclassifies or misses.

Result: In simulation, the commonsense reasoning module successfully identified traffic light colors and road obstacles in cases where the deep learning perception system was incorrect or uncertain. The hybrid system reduced misclassifications in the tested edge-case scenarios.

Conclusion: Automated commonsense reasoning can effectively complement deep learning-based AV perception, particularly under data-sparse, abnormal scenarios. Hybrid models that combine uncertainty-aware vision with symbolic reasoning offer a promising path toward more robust AV perception and, ultimately, higher levels of driving autonomy.

Abstract: Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common- sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain sce- narios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.

</details>


### [104] [Propositional Abduction via Only-Knowing: A Non-Monotonic Approach](https://arxiv.org/abs/2601.04272)
*Sanderson Molick,Vaishak Belle*

Main category: cs.AI

TL;DR: The paper extends Levesque’s logic of only-knowing with a new abduction operator to model abductive reasoning in a modal logical framework, and further enriches it with a preferential semantics to capture non-monotonic selection of explanations and to study core properties of non-monotonic consequence for abduction.


<details>
  <summary>Details</summary>
Motivation: Abductive reasoning (inference to the best explanation) is central in AI and philosophy, but is often modeled informally or via ad‑hoc formalisms. Existing work on Levesque’s logic of only‑knowing gives a precise account of epistemic states (what an agent knows and only knows), but does not directly capture abduction or the mechanisms by which an agent chooses among competing explanations. The authors aim to provide a clean, modal-logic-based framework in which both knowledge and abductive explanation can be jointly represented, and to ground abductive inference in well-understood non-monotonic consequence relations.

Method: 1) Take Levesque’s logic of only-knowing as the base epistemic logic. 2) Introduce a new modal operator for abduction, defined compositionally from existing epistemic modalities, to express that a formula is adopted as an explanation for observed information. 3) Define a class of modal frames enriched with a preferential relation over worlds (or states) to represent an agent’s preferences or plausibility ordering between alternative explanations. 4) Use this preference structure to generate a non-monotonic consequence relation that formalizes how abductive conclusions are drawn and revised when new information is added. 5) Prove meta-theoretic properties (such as cumulativity, cut, cautious monotony, etc.) for the resulting consequence relation to show that it behaves like standard non-monotonic logics used for defeasible reasoning.

Result: The authors obtain: (i) a basic modal logic that combines only-knowing with an explicit abductive operator, giving a uniform language for statements about knowledge and explanatory hypotheses; (ii) a preferential semantics on top of this logic that induces a non-monotonic consequence relation capturing the selection of preferred abductive explanations; and (iii) formal theorems establishing that the induced non-monotonic consequence relation satisfies central properties known from non-monotonic reasoning theory, demonstrating its robustness and alignment with general principles of defeasible inference.

Conclusion: The proposed extension of only-knowing with an abduction operator and preferential semantics yields a well-behaved, non-monotonic logical framework for modeling abductive reasoning as part of agents’ epistemic states. This shows that abduction can be captured using a purely modal vocabulary and standard tools from non-monotonic logic, offering a principled foundation for reasoning about explanations, their selection, and their interaction with what an agent only knows.

Abstract: The paper introduces a basic logic of knowledge and abduction by extending Levesque logic of only-knowing with an abduction modal operator defined via the combination of basic epistemic concepts. The upshot is an alternative approach to abduction that employs a modal vocabulary and explores the relation between abductive reasoning and epistemic states of only knowing. Furthermore, by incorporating a preferential relation into modal frames, we provide a non-monotonic extension of our basic framework capable of expressing different selection methods for abductive explanations. Core metatheoretic properties of non-monotonic consequence relations are explored within this setting and shown to provide a well-behaved foundation for abductive reasoning.

</details>


### [105] [Hybrid MKNF for Aeronautics Applications: Usage and Heuristics](https://arxiv.org/abs/2601.04273)
*Arun Raveendran Nair Sheela,Florence De Grancey,Christophe Rey,Victor Charpenay*

Main category: cs.AI

TL;DR: The paper evaluates the Hybrid MKNF knowledge representation language for aeronautics applications, finding it largely suitable but in need of added expressivity features, and proposes heuristics to integrate those features efficiently.


<details>
  <summary>Details</summary>
Motivation: Aeronautics applications need powerful knowledge representation and reasoning that can capture complex domain knowledge with high expressivity while still being computationally efficient and memory-conscious. Existing KR technologies often trade off expressivity against performance, and there is a need to understand whether integrated rule–ontology approaches, such as Hybrid MKNF, can meet the specific requirements of the aeronautics domain.

Method: The authors adopt the Hybrid MKNF language, which combines rules and ontologies under a unified semantics, and apply it to a concrete aeronautics case study. Through this application, they systematically assess which aspects of expressivity are supported, identify gaps relative to aeronautics needs, and then design heuristics that would allow incorporating the missing expressivity features into Hybrid MKNF while aiming to preserve efficient reasoning.

Result: The evaluation shows that Hybrid MKNF is a promising KR framework for aeronautics, as its integration of rules and ontologies supports many required reasoning tasks. However, the case study reveals additional expressivity features needed for realistic aeronautics applications that are not natively covered. The authors distill these requirements and formulate a set of integration heuristics to extend Hybrid MKNF with the missing features in a controlled way.

Conclusion: Hybrid MKNF is largely suitable for aeronautics KR tasks due to its expressive combination of rules and ontologies, but it must be extended with certain additional expressivity features to fully address domain needs. The paper contributes a set of practical heuristics to guide the integration of these features into the Hybrid MKNF framework, aiming to maintain efficient reasoning and resource usage in aeronautics applications.

Abstract: The deployment of knowledge representation and reasoning technologies in aeronautics applications presents two main challenges: achieving sufficient expressivity to capture complex domain knowledge, and executing reasoning tasks efficiently while minimizing memory usage and computational overhead. An effective strategy for attaining necessary expressivity involves integrating two fundamental KR concepts: rules and ontologies. This study adopts the well-established KR language Hybrid MKNF owing to its seamless integration of rules and ontologies through its semantics and query answering capabilities. We evaluated Hybrid MKNF to assess its suitability in the aeronautics domain through a concrete case study. We identified additional  expressivity features  that are crucial for developing aeronautics applications and proposed a set of heuristics to support their integration into Hybrid MKNF framework.

</details>


### [106] [An ASP-based Solution to the Medical Appointment Scheduling Problem](https://arxiv.org/abs/2601.04274)
*Alina Vozna,Andrea Monaldini,Stefania Costantini,Valentina Pitoni,Dawid Pado*

Main category: cs.AI

TL;DR: An ASP-based framework for personalized, efficient, and interoperable medical appointment scheduling, especially for vulnerable populations.


<details>
  <summary>Details</summary>
Motivation: Current medical appointment scheduling systems often lack flexibility, create administrative burden, and fail to adequately support vulnerable populations with personalized scheduling needs. There is a need for a formal, automatable framework that can handle complex constraints while integrating patient-centered considerations.

Method: The paper proposes a centralized planning framework built on Answer Set Programming (ASP). It models scheduling constraints, real-time availability, conflict avoidance, and patient personas (Blueprint Personas) within an ASP logic model, enabling automated reasoning and optimization over schedules.

Result: The ASP framework can generate conflict-free, up-to-date schedules that reflect provider availability and the specific needs of different patient personas, and can interact with existing healthcare platforms for data exchange and updates.

Conclusion: Using ASP for medical appointment scheduling can reduce administrative overhead, improve scheduling efficiency, and enable more patient-centered, equitable care, particularly for vulnerable groups, while remaining compatible with current healthcare IT systems.

Abstract: This paper presents an Answer Set Programming (ASP)-based framework for medical appointment scheduling, aimed at improving efficiency, reducing administrative overhead, and enhancing patient-centered care. The framework personalizes scheduling for vulnerable populations by integrating Blueprint Personas. It ensures real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms by centralizing planning operations within an ASP logic model.

</details>


### [107] [A Future Capabilities Agent for Tactical Air Traffic Control](https://arxiv.org/abs/2601.04285)
*Paul Kent,George De Ath,Martin Layton,Allen Hart,Richard Everson,Ben Carvell*

Main category: cs.AI

TL;DR: Agent Mallard is a rules-based, forward-planning air traffic control agent that embeds a stochastic digital twin into its conflict-resolution loop to ensure safety under uncertainty while remaining interpretable.


<details>
  <summary>Details</summary>
Motivation: Rising air traffic demand requires greater automation support for air traffic controllers, but current automated methods face a key trade-off: optimisation-based (e.g., reinforcement learning) approaches are powerful but hard to verify and explain, whereas rules-based systems are interpretable but typically lack rigorous safety checks under uncertainty. The paper aims to bridge this gap by providing a safety-assured yet interpretable tactical control solution for structured airspace.

Method: The paper proposes Agent Mallard, a forward-planning, rules-based agent designed for tactical control in systemised (structured) airspace. It constrains aircraft to predefined GPS-guided routes, turning continuous 4D vectoring into discrete decisions over lanes and flight levels. Mallard uses an expert-informed library of hierarchical deconfliction strategies and performs a depth-limited backtracking search to build complete conflict-free plans. The search incorporates causal attribution, topological plan splicing, and monotonic axis constraints to efficiently explore options. Critically, every candidate manoeuvre is evaluated within an embedded stochastic digital twin that simulates uncertain execution conditions (e.g., wind variation, pilot response delays, communication loss) so that only manoeuvres that remain safe under uncertainty are accepted.

Result: Preliminary qualitative walkthroughs with UK air traffic controllers suggest that Mallard’s decision-making process and outputs are consistent with expert reasoning. Initial quantitative tests within the BluebirdDT airspace digital twin show that Mallard can resolve conflicts in simplified, structured traffic scenarios while maintaining safety constraints under uncertainty. Detailed large-scale performance metrics are not yet provided but early results indicate feasibility of the approach.

Conclusion: The architecture demonstrates a promising way to combine model-based safety assessment with interpretable, rules-based decision logic for tactical air traffic control in structured en-route environments. By embedding a stochastic digital twin directly into the planning loop and operating over discretised routes and levels, Agent Mallard aims to deliver safety-assured, explainable automation that is computationally tractable and aligns with controller mental models, supporting future deployment in systemised airspace.

Abstract: Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.

</details>


### [108] [Pilot Study on Student Public Opinion Regarding GAI](https://arxiv.org/abs/2601.04336)
*William Franz Lamberti,Sunbin Kim,Samantha Rose Lawrence*

Main category: cs.AI

TL;DR: Pilot study on university students' perceptions of generative AI (GAI) in higher education classrooms, focusing on participation challenges and implications for teaching.


<details>
  <summary>Details</summary>
Motivation: To understand how university students perceive the use of generative AI in higher education, given its rapid emergence and the debate about its appropriate use in education and other domains.

Method: Pilot study surveying university students about their perceptions of GAI in higher education classrooms; analysis of participation rate and implications for future research sample sizes.

Result: Identified that only about 4.4% of students participated, revealing difficulties in engaging students in GAI-related research and indicating that current data are limited but informative for planning larger studies.

Conclusion: Student perceptions of GAI are important for guiding how instructors introduce and discuss GAI in class. Despite low participation, the study suggests instructors should proactively integrate informed, critical discussions of GAI into higher education to support thoughtful engagement with the technology and that future research needs larger, more representative samples.

Abstract: The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.

</details>


### [109] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: The paper studies how the language used with LLMs (English vs several Indic languages) changes negotiation behavior and outcomes in multi-agent game simulations.


<details>
  <summary>Details</summary>
Motivation: Most LLM negotiation evaluations are done only in English, even though negotiation is inherently social and culturally shaped. The authors want to know whether changing the interaction language alone—while keeping game rules and model settings identical—can systematically alter negotiation dynamics and fairness, and whether English-only evaluation might therefore be misleading or biased.

Method: They run controlled multi-agent simulations of three negotiation-style games (Ultimatum, Buy-Sell, Resource Exchange). They keep rules, model parameters, and payoff structures fixed, and vary only the language framing: English vs four Indic languages (Hindi, Punjabi, Gujarati, Marwadi). They then compare negotiation strategies, outcome distributions, and stability across languages and tasks.

Result: They observe strong language-driven differences. Merely changing the language can change who has the advantage, shift how surplus is divided, and in some cases produce effects larger than switching to a different model. Indic-language interactions tend to reduce equilibrium stability in simple distributive games, but promote more exploratory, integrative bargaining behavior in richer exchange tasks.

Conclusion: Language choice is a crucial factor in LLM-based negotiation: evaluations limited to English give an incomplete and sometimes misleading picture of model capabilities and fairness. Culturally and linguistically diverse evaluation is necessary before deploying LLMs in real-world, multi-lingual negotiation or decision-support settings.

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [110] [LLM-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations](https://arxiv.org/abs/2601.04388)
*Priyaranjan Pattnayak,Sanchari Chowdhuri,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.AI

TL;DR: They propose an adaptive, incremental clustering system for customer chat data that segments multi‑service conversations, monitors cluster quality, and selectively reclusters only degraded clusters using LLMs, yielding significantly better clustering metrics than baselines.


<details>
  <summary>Details</summary>
Motivation: Cloud providers receive customer chats that often span multiple services in a single, multi‑turn conversation. Existing clustering methods typically treat each chat as a single unit, leading to overlapping concerns, broad and static clusters, and degradation over time as new issue types appear. Periodically reclustering everything is computationally expensive and disrupts continuity for issue tracking and analytics. The paper is motivated by the need for a scalable way to maintain high‑quality, fine‑grained clusters over evolving, multi‑service chat data without full reclustering.

Method: The paper introduces an adaptive clustering pipeline tailored to multi‑turn, multi‑service customer chats. First, it segments conversations into service‑specific concerns. Then it performs incremental clustering as new issues arrive, avoiding complete reclustering. Cluster quality is continuously monitored using Davies–Bouldin Index (DBI) and Silhouette Scores. When a cluster’s quality degrades beyond set thresholds, a large language model (LLM) is invoked to intelligently split or refine that specific cluster, rather than reclustering the entire dataset. This selective, metric‑driven LLM intervention keeps the system efficient and adaptive.

Result: Compared with baseline clustering approaches, the proposed system more than doubles the Silhouette Score (over 100% improvement) and reduces the Davies–Bouldin Index by 65.6%, indicating tighter, better‑separated clusters. These gains are achieved while avoiding full reclustering, demonstrating that targeted, metric‑triggered LLM‑based refining of problematic clusters can substantially improve clustering quality on evolving chat data.

Conclusion: The paper concludes that an adaptive, incremental clustering system with conversation segmentation and selective LLM‑based refinement yields higher‑quality, more stable clusters for multi‑service customer chats than traditional static methods. By monitoring DBI and Silhouette Scores and only refining degraded clusters, the approach supports scalable, real‑time analytics and issue tracking without the computational and organizational costs of periodic full reclustering.

Abstract: Clustering customer chat data is vital for cloud providers handling multi service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Reclustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi turn chats into service specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via DaviesBouldin Index and Silhouette Scores, with LLM based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100\% and reduces DBI by 65.6\% compared to baselines, enabling scalable, real time analytics without full reclustering.

</details>


### [111] [SciFig: Towards Automating Scientific Figure Generation](https://arxiv.org/abs/2601.04390)
*Siyuan Huang,Yutong Gao,Juyang Bai,Yifan Zhou,Zi Yin,Xinxin Liu,Rama Chellappa,Chun Pong Lau,Sayan Nag,Cheng Peng,Shraman Pramanick*

Main category: cs.AI

TL;DR: SciFig is an AI agent that automatically generates publication-ready scientific pipeline figures from paper text using hierarchical layout reasoning and rubric-based evaluation.


<details>
  <summary>Details</summary>
Motivation: Designing high-quality, publication-ready figures is labor-intensive and requires both domain expertise and design skills, yet the process is still mostly manual despite the huge and growing volume of scientific publications. There is a need to automate figure creation while preserving scientific accuracy and visual quality.

Method: SciFig is an end-to-end AI agent system that takes research paper text as input and outputs pipeline figures. It employs a hierarchical layout generation strategy: it parses the research description, identifies component relationships, groups related elements into functional modules, and creates inter-module connections to define visual organization. An iterative chain-of-thought feedback loop repeatedly analyzes intermediate layouts, reasons about visual quality, and refines the design. Additionally, the authors build a rubric-based evaluation framework by analyzing 2,219 real scientific figures to derive rubrics and automatically construct detailed evaluation criteria.

Result: SciFig achieves 70.1% overall quality on dataset-level evaluation and 66.2% quality on paper-specific evaluation, with consistently high scores on visual clarity, structural organization, and scientific accuracy metrics. The system and evaluation benchmark show that automated figure generation can reach a strong level of publication readiness.

Conclusion: SciFig effectively automates the creation of scientific pipeline figures directly from textual descriptions, producing high-quality, well-organized, and scientifically accurate visuals. The authors will open-source both the SciFig pipeline and the associated evaluation benchmark, enabling broader use and further research on automated figure generation and assessment.

Abstract: Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\%$ overall quality on dataset-level evaluation and 66.2$\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.

</details>


### [112] [Assessing the quality and coherence of word embeddings after SCM-based intersectional bias mitigation](https://arxiv.org/abs/2601.04393)
*Eren Kocadag,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: The paper studies how to mitigate intersectional social biases in static word embeddings using Stereotype Content Model (SCM)-based methods, while preserving semantic utility.


<details>
  <summary>Details</summary>
Motivation: Static word embeddings encode social biases present in training text, which can impact downstream applications. Prior SCM-based work has mostly examined bias along single social dimensions (e.g., one group at a time, in terms of warmth and competence). However, social identities are intersectional (e.g., combinations of gender, race, age), and it is unclear how to debias embeddings for such compound identities without damaging their usefulness. The paper aims to extend SCM-based debiasing to intersectional groups and understand the trade-offs between debiasing strength and semantic preservation.

Method: The authors form compound representations for pairs of social identities (intersectional groups) by either summing or concatenating the embeddings of the component identities. They then apply three debiasing strategies grounded in SCM dimensions (warmth and competence): (1) Subtraction, which removes bias components by subtracting stereotype vectors; (2) Linear Projection, which projects embeddings away from identified bias directions; and (3) Partial Projection, which only partially projects to reduce, not fully remove, those components. They evaluate these methods on three popular static embedding families—Word2Vec, GloVe, and ConceptNet Numberbatch—using two utility criteria: (a) whether local neighborhood structure (nearest neighbors) remains coherent, and (b) whether word analogy performance is preserved.

Result: SCM-based debiasing that had been used for single-group bias generalizes effectively to intersectional (compound) identities across all three embedding families. The overall semantic structure of the embeddings is largely preserved under these debiasing operations. However, there is a trade-off: methods that more strictly preserve geometric properties (like local neighborhoods) tend to make smaller changes to analogy behavior, while more aggressive debiasing through projections can improve analogy behavior but cause more neighborhood disruption. Partial Projection behaves in a conservative way, causing minimal changes to the embedding space; Linear Projection is more aggressive and can more strongly modify analogies; Subtraction, despite its simplicity, remains a competitive baseline. The effectiveness of summation vs. concatenation for building compound representations varies by embedding type and the target application.

Conclusion: Intersectional debiasing of static word embeddings using SCM-based methods is feasible and practical. SCM-based mitigation strategies extend from single-group to intersecting identity pairs without severely harming the embeddings’ semantic structure. Different debiasing techniques and aggregation strategies (summation vs. concatenation) present distinct trade-offs between preserving neighborhood structure and improving analogy behavior. The paper offers guidance on selecting combinations of aggregation and debiasing methods based on whether stability or analogy performance is prioritized in a given application.

Abstract: Static word embeddings often absorb social biases from the text they learn from, and those biases can quietly shape downstream systems. Prior work that uses the Stereotype Content Model (SCM) has focused mostly on single-group bias along warmth and competence. We broaden that lens to intersectional bias by building compound representations for pairs of social identities through summation or concatenation, and by applying three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. We study three widely used embedding families (Word2Vec, GloVe, and ConceptNet Numberbatch) and assess them with two complementary views of utility: whether local neighborhoods remain coherent and whether analogy behavior is preserved. Across models, SCM-based mitigation carries over well to the intersectional case and largely keeps the overall semantic landscape intact. The main cost is a familiar trade off: methods that most tightly preserve geometry tend to be more cautious about analogy behavior, while more assertive projections can improve analogies at the expense of strict neighborhood stability. Partial Projection is reliably conservative and keeps representations steady; Linear Projection can be more assertive; Subtraction is a simple baseline that remains competitive. The choice between summation and concatenation depends on the embedding family and the application goal. Together, these findings suggest that intersectional debiasing with SCM is practical in static embed- dings, and they offer guidance for selecting aggregation and debiasing settings when balancing stability against analogy performance.

</details>


### [113] [Transitive Expert Error and Routing Problems in Complex AI Systems](https://arxiv.org/abs/2601.04416)
*Forest Mars*

Main category: cs.AI

TL;DR: The paper introduces Transitive Expert Error (TEE), where real experts become systematically wrong at the boundaries of their expertise, particularly when different domains look structurally similar but are causally different. It extends the concept to AI routing architectures and proposes architectural interventions to detect and mitigate such boundary errors.


<details>
  <summary>Details</summary>
Motivation: Explain why genuine, calibrated expertise can paradoxically create specific, predictable failure modes at domain boundaries, and show that this problem affects both human experts and modern AI systems. The authors want to distinguish TEE from Dunning-Kruger and to argue that AI architectures make these errors more observable and fixable.

Method: Conceptual/theoretical analysis of expert judgment mechanisms, focusing on how pattern recognition and authority dynamics lead to overgeneralization across domains with shared surface structure but different causal mechanisms. The paper maps these mechanisms onto AI architectures like mixture-of-experts and tool-using agents, and derives design principles and intervention strategies from this mapping.

Result: Identification and characterization of Transitive Expert Error as a distinct phenomenon with specific cognitive and architectural mechanisms: structural similarity bias and authority persistence. The paper classifies AI failure modes into routing-induced and coverage-induced errors, both producing confident but wrong outputs at domain boundaries. It also outlines observable signatures of TEE such as suspicious routing patterns, mismatches between confidence and accuracy, and domain-inappropriate content.

Conclusion: Transitive Expert Error is an inherent risk wherever specialized expertise or specialist models operate, and it is particularly acute at domain boundaries obscured by shared vocabulary or structure. While human cognition largely hides these mechanisms, AI systems make them explicit, allowing targeted interventions: activating multiple experts and detecting disagreement at the router level, calibrating specialists to recognize boundaries of competence, and detecting coverage gaps during training. With appropriate architectural design and monitoring, TEE can be mitigated even if it cannot be fully eliminated.

Abstract: Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.

</details>


### [114] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: The paper introduces XGrammar 2, an optimized engine that speeds up complex, dynamic structured generation for LLM agents, achieving over 6x speedup and near-zero overhead when integrated with LLM inference.


<details>
  <summary>Details</summary>
Motivation: Existing structured generation engines are optimized mainly for static, predefined output formats and struggle with dynamic tasks such as tool calls, conditional branches, and other agentic behaviors, which require more flexible and efficient mask generation.

Method: The authors design XGrammar 2 around several key techniques: (1) TagDispatch, a new dynamic dispatch semantics that accelerates mask generation for dynamic structured generation; (2) a JIT compilation method to cut compilation latency; (3) cross-grammar caching to reuse common sub-structures across different grammars; (4) replacing the previous PDA-based mask generation with an Earley-parser-based algorithm; and (5) a repetition compression algorithm tailored to repetition constructs in grammars.

Result: Experiments show that XGrammar 2 provides more than a 6x speedup over state-of-the-art structured generation engines and, when coupled with an LLM inference engine, introduces almost no additional overhead even for dynamic structured tasks.

Conclusion: XGrammar 2 effectively addresses the performance and flexibility challenges of dynamic structured generation for LLM agents, enabling fast, low-overhead handling of complex grammars and dynamic structures through new dispatch semantics, parsing algorithms, and caching/compilation optimizations.

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [115] [Categorical Belief Propagation: Sheaf-Theoretic Inference via Descent and Holonomy](https://arxiv.org/abs/2601.04456)
*Enrique ter Horst,Sridhar Mahadevan,Juan Diego Zambrano*

Main category: cs.AI

TL;DR: The paper gives a categorical and sheaf-theoretic formalization of belief propagation (BP), characterizes when BP is exact via descent theory, and introduces an algorithm (HATCC) that turns loopy graphs into tree-like ones by accounting for holonomy, enabling exact and faster inference than junction trees in many cases.


<details>
  <summary>Details</summary>
Motivation: Belief propagation is widely used for inference on probabilistic graphical models, but its exactness is only guaranteed on trees; on loopy graphs it can fail or be hard to characterize. Existing exact methods like junction trees can be computationally expensive. The authors want a principled, compositional, and structural explanation of when BP works or fails, together with an efficient algorithm that repairs the failures by exploiting categorical and sheaf-theoretic structure.

Method: 1) Use category theory to model factor graphs via the free hypergraph category Syn_Σ and show it has a universal property with semantics in the matrix category Mat_R. 2) Represent message passing as a Grothendieck fibration over a category of polarized factor graphs, where schedules correspond to endomorphisms implementing BP updates. 3) Use descent theory and sheaf-theoretic ideas to characterize exact inference: local beliefs are a descent datum, and exactness corresponds to effective descent (i.e., gluing without obstruction). 4) Analyze failures of loopy BP as sheaf-theoretic obstructions tied to holonomy on a factor nerve. 5) Propose HATCC, which computes holonomy on fundamental cycles, compiles non-trivial holonomy into additional mode variables, and then runs standard tree BP on the resulting augmented tree-like graph. 6) Analyze the algorithmic complexity and empirically evaluate it on grid MRFs, random graphs, and SAT instances.

Result: The authors show that BP can be phrased entirely in categorical and sheaf-theoretic terms: Syn_Σ maps uniquely to Mat_R, message schedules correspond to categorical morphisms, and exactness is equivalent to effective descent. They prove that tree exactness, junction tree algorithms, and loopy BP failure modes are all special cases of descent and holonomy phenomena on a factor nerve. Algorithmically, HATCC detects non-trivial holonomy (descent obstructions), augments the model with mode variables to resolve them, and then applies tree BP. The complexity of HATCC is derived as O(n^2 d_max + c · k_max · δ_max^3 + n · δ_max^2). Experiments show that HATCC achieves exact inference with notable speedups over junction tree methods on grid Markov random fields and random graphs, and it can also detect unsatisfiable SAT instances through its inference process.

Conclusion: Belief propagation admits a clean categorical and sheaf-theoretic foundation in which exactness corresponds to effective descent and failures arise from holonomy-based obstructions. This unifies disparate understandings of BP (tree exactness, junction trees, loopy failures) within one structural framework. Building on this view, the HATCC algorithm leverages holonomy computations to transform loopy factor graphs into augmented tree structures where standard BP is exact, often more efficiently than junction-tree constructions, and with additional capabilities such as UNSAT detection.

Abstract: We develop a categorical foundation for belief propagation on factor graphs. We construct the free hypergraph category \(\Syn_Σ\) on a typed signature and prove its universal property, yielding compositional semantics via a unique functor to the matrix category \(\cat{Mat}_R\). Message-passing is formulated using a Grothendieck fibration \(\int\Msg \to \cat{FG}_Σ\) over polarized factor graphs, with schedule-indexed endomorphisms defining BP updates. We characterize exact inference as effective descent: local beliefs form a descent datum when compatibility conditions hold on overlaps. This framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. We introduce HATCC (Holonomy-Aware Tree Compilation), an algorithm that detects descent obstructions via holonomy computation on the factor nerve, compiles non-trivial holonomy into mode variables, and reduces to tree BP on an augmented graph. Complexity is \(O(n^2 d_{\max} + c \cdot k_{\max} \cdot δ_{\max}^3 + n \cdot δ_{\max}^2)\) for \(n\) factors and \(c\) fundamental cycles. Experimental results demonstrate exact inference with significant speedup over junction trees on grid MRFs and random graphs, along with UNSAT detection on satisfiability instances.

</details>


### [116] [Computational Compliance for AI Regulation: Blueprint for a New Research Domain](https://arxiv.org/abs/2601.04474)
*Bill Marino,Nicholas D. Lane*

Main category: cs.AI

TL;DR: The paper proposes a blueprint for making AI systems comply with emerging AI regulations using automated, algorithmic methods rather than traditional manual compliance, and introduces design goals and a benchmark dataset for such compliance algorithms.


<details>
  <summary>Details</summary>
Motivation: AI-specific regulations are emerging rapidly, and existing manual, analogue compliance processes cannot keep pace with the speed, scale, and dynamic nature of modern AI systems. There is a need for systematic, computational approaches that can continuously steer AI systems toward regulatory compliance throughout their life cycle, but the field lacks clear behavioral specifications and evaluation standards for such approaches.

Method: The authors conceptually define and motivate the notion of computational AI regulation (AIR) compliance, then articulate a set of design goals that algorithms for such compliance should satisfy. They further introduce and specify a benchmark dataset intended to quantitatively evaluate whether proposed algorithms meet those design goals, thereby providing a standardized testing ground for future research.

Result: The paper delivers (i) a structured set of design goals that describe how computational AIR compliance algorithms should behave and (ii) a benchmark dataset that operationalizes these goals into measurable tasks and metrics. These outputs together define an initial framework for developing and comparing compliance algorithms.

Conclusion: The authors conclude that computational approaches to AI regulation compliance are both necessary and inevitable for modern AI systems. Their proposed design goals and benchmark dataset offer a concrete blueprint for this emerging research domain and are intended to catalyze focused research and investment in scalable, algorithmic compliance methods.

Abstract: The era of AI regulation (AIR) is upon us. But AI systems, we argue, will not be able to comply with these regulations at the necessary speed and scale by continuing to rely on traditional, analogue methods of compliance. Instead, we posit that compliance with these regulations will only realistically be achieved computationally: that is, with algorithms that run across the life cycle of an AI system, automatically steering it toward AIR compliance in the face of dynamic conditions. Yet despite their (we would argue) inevitability, the research community has yet to specify exactly how these algorithms for computational AIR compliance should behave - or how we should benchmark their performance. To fill these gaps, we specify a set of design goals for such algorithms. In addition, we specify a benchmark dataset that can be used to quantitatively measure whether individual algorithms satisfy these design goals. By delivering this blueprint, we hope to give shape to an important but uncrystallized new domain of research - and, in doing so, incite necessary investment in it.

</details>


### [117] [A Closed-Loop Multi-Agent System Driven by LLMs for Meal-Level Personalized Nutrition Management](https://arxiv.org/abs/2601.04491)
*Muqing Xu*

Main category: cs.AI

TL;DR: A mobile nutrition assistant that integrates image-based meal logging with a multi-agent LLM system to estimate nutrients and adapt meal plans in a closed loop for personalized nutrition.


<details>
  <summary>Details</summary>
Motivation: Existing personalized nutrition tools treat food logging, nutrient estimation, and recommendation as separate steps, leading to fragmented user experience and limited real-time adaptation. There is a need for an integrated system that can continuously track intake and dynamically adjust recommendations at the meal level.

Method: The authors design a mobile system that uses computer vision to analyze meal photos and an LLM-driven multi-agent controller (including vision, dialogue, and state-management agents). The controller estimates nutrients from images, updates a daily nutrient budget, and then generates and adapts the next meal plan according to user preferences and dietary constraints. They evaluate it using SNAPMe meal images and simulated users.

Result: The system achieves competitive accuracy in nutrient estimation from meal photos, can generate personalized meal menus, and produces efficient multi-step task plans in simulated settings.

Conclusion: A multi-agent LLM-controlled mobile assistant is a feasible approach for closed-loop, meal-level personalized nutrition management, but challenges remain in accurately estimating micronutrients from images and in validating such systems at large scale in real-world deployments.

Abstract: Personalized nutrition management aims to tailor dietary guidance to an individual's intake and phenotype, but most existing systems handle food logging, nutrient analysis and recommendation separately. We present a next-generation mobile nutrition assistant that combines image based meal logging with an LLM driven multi agent controller to provide meal level closed loop support. The system coordinates vision, dialogue and state management agents to estimate nutrients from photos and update a daily intake budget. It then adapts the next meal plan to user preferences and dietary constraints. Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus and efficient task plans. These findings demonstrate the feasibility of multi agent LLM control for personalized nutrition and reveal open challenges in micronutrient estimation from images and in large scale real world studies.

</details>


### [118] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: The paper introduces GUITestBench, an interactive benchmark, and GUITester, a multi-agent MLLM-based framework that enables autonomous exploratory GUI testing by separating navigation from defect verification and mitigating goal bias and attribution errors.


<details>
  <summary>Details</summary>
Motivation: Exploratory GUI testing is crucial for software quality but is expensive when done manually. Existing multi-modal LLM agents are good at navigating interfaces but poor at autonomously finding and correctly attributing GUI defects, mainly because they focus on completing tasks instead of reporting anomalies and often misclassify system bugs as their own execution mistakes. The paper aims to close this gap and enable scalable, automated exploratory GUI testing.

Method: The authors build GUITestBench, an interactive benchmark with 143 tasks covering 26 types of GUI defects, to systematically evaluate autonomous exploratory testing. They then propose GUITester, a multi-agent framework with two key modules: (1) a Planning-Execution Module (PEM) that steers interaction with the GUI using embedded testing intents so that the agent actively probes for possible defects rather than only finishing tasks; and (2) a Hierarchical Reflection Module (HRM) that analyzes multi-step interaction histories to distinguish between genuine system defects and agent execution errors, thus resolving attribution ambiguity. The design explicitly decouples navigation (getting through the GUI) from verification (detecting and labeling anomalies).

Result: On the GUITestBench benchmark, GUITester attains an F1-score of 48.90% under a Pass@3 setting, substantially higher than the best existing baselines at 33.35%. This shows improved precision/recall in discovering and correctly reporting GUI defects. The public release of GUITestBench and GUITester code enables reproducibility and further research.

Conclusion: The study shows that with appropriate architectural design—separating navigation from verification and incorporating proactive testing intents plus hierarchical reflection—MLLM-based agents can perform effective autonomous exploratory GUI testing. GUITestBench provides the first dedicated benchmark for this task, and GUITester sets a strong performance baseline, laying groundwork for future research in automated GUI quality assurance.

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [119] [Specific Emitter Identification via Active Learning](https://arxiv.org/abs/2601.04502)
*Jingyi Wang,Fanggang Wang*

Main category: cs.AI

TL;DR: The paper proposes an active-learning-enhanced specific emitter identification framework that combines self-supervised contrastive learning, supervised fine-tuning, and active sample selection to achieve high recognition accuracy with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Specific emitter identification is important for wireless communication security, but existing approaches require large-scale labeled datasets that are expensive and time-consuming to obtain. There is a need for methods that can leverage abundant unlabeled data and minimize annotation costs while maintaining or improving recognition accuracy.

Method: The method is a three-stage semi-supervised training scheme for SEI. (1) Self-supervised contrastive learning with a dynamic dictionary is first applied on large unlabeled datasets to learn robust feature representations. (2) A small labeled dataset is then used for supervised training where contrastive loss and cross-entropy loss are jointly optimized to make features more separable and improve decision boundaries. (3) An active learning module iteratively selects the most informative unlabeled samples for annotation using uncertainty and representativeness criteria, thereby making the best use of a limited labeling budget. The approach is evaluated on ADS-B and WiFi datasets.

Result: Experiments on ADS-B and WiFi datasets show that the proposed framework outperforms conventional supervised and semi-supervised baselines when labeled data are limited. It achieves higher recognition accuracy while requiring fewer labeled samples, indicating improved data efficiency and generalization under constrained annotation budgets.

Conclusion: The active-learning-enhanced SEI framework effectively reduces labeling costs while improving identification performance by integrating self-supervised contrastive pretraining, supervised fine-tuning with joint losses, and informed sample selection via active learning. This demonstrates a practical path toward scalable and label-efficient SEI in real wireless communication scenarios.

Abstract: With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.

</details>


### [120] [A General Neural Backbone for Mixed-Integer Linear Optimization via Dual Attention](https://arxiv.org/abs/2601.04509)
*Peixin Huang,Yaoxin Wu,Yining Ma,Cathy Wu,Wen Song,Wei Zhang*

Main category: cs.AI

TL;DR: They propose an attention-based neural architecture for MILP that outperforms GNN-based baselines.


<details>
  <summary>Details</summary>
Motivation: MILP is powerful but hard to solve at scale. Existing deep learning approaches represent MILPs as bipartite graphs and use GNNs, but these are limited by local message passing, which restricts their ability to capture global structure and thus limits solver improvements.

Method: They design an attention-driven neural backbone with a dual-attention mechanism. It performs parallel self-attention within variables and within constraints, and cross-attention between variables and constraints, enabling global information exchange beyond local graph neighborhoods. This backbone is used for multiple MILP-related tasks at instance level, element level, and solving state level.

Result: On standard MILP benchmarks and across several downstream tasks, their attention-based architecture consistently outperforms state-of-the-art GNN-based and other neural baselines in terms of solver efficiency and learning quality.

Conclusion: Attention mechanisms that go beyond local graph neighborhoods provide more expressive representations for MILP instances and yield systematic, consistent improvements in learning-enhanced MILP solving, suggesting attention-based models as a strong foundation for future research in this area.

Abstract: Mixed-integer linear programming (MILP), a widely used modeling framework for combinatorial optimization, are central to many scientific and engineering applications, yet remains computationally challenging at scale. Recent advances in deep learning address this challenge by representing MILP instances as variable-constraint bipartite graphs and applying graph neural networks (GNNs) to extract latent structural patterns and enhance solver efficiency. However, this architecture is inherently limited by the local-oriented mechanism, leading to restricted representation power and hindering neural approaches for MILP. Here we present an attention-driven neural architecture that learns expressive representations beyond the pure graph view. A dual-attention mechanism is designed to perform parallel self- and cross-attention over variables and constraints, enabling global information exchange and deeper representation learning. We apply this general backbone to various downstream tasks at the instance level, element level, and solving state level. Extensive experiments across widely used benchmarks show consistent improvements of our approach over state-of-the-art baselines, highlighting attention-based neural architectures as a powerful foundation for learning-enhanced mixed-integer linear optimization.

</details>


### [121] [Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data](https://arxiv.org/abs/2601.04518)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.AI

TL;DR: The paper improves semi-supervised contrastive learning for image classification by enhancing pseudo-labeling with distribution matching between labeled and unlabeled feature embeddings.


<details>
  <summary>Details</summary>
Motivation: Supervised deep learning for image classification needs many labeled examples, which are expensive to obtain. Purely unsupervised methods do not fully match practical settings, where some labels are usually available. Semi-supervised learning seeks to exploit both few labeled and many unlabeled images. Existing pseudo-label-based contrastive SSL methods can be suboptimal because they ignore how feature representations of labeled and unlabeled data align statistically. This motivates a method that explicitly matches the distributions of labeled and unlabeled feature embeddings to better use unlabeled data and boost classification accuracy.

Method: The paper builds on semi-supervised contrastive learning that assigns pseudo-labels to unlabeled images. On top of this, it introduces a distribution-matching mechanism in the feature space: the feature embeddings of unlabeled samples are encouraged to align with those of labeled samples, likely via an additional loss term that minimizes a discrepancy measure between the two distributions (e.g., moment matching, KL divergence, or related metrics). This joint optimization of pseudo-label prediction and distribution alignment is used to train the image classifier.

Result: Across multiple image classification benchmarks, the proposed method achieves higher accuracy than baseline semi-supervised contrastive learning approaches that rely only on pseudo-labels and do not include explicit distribution matching. The improvements demonstrate that aligning labeled and unlabeled feature distributions helps better exploit unlabeled data.

Conclusion: Enhancing pseudo-label-based semi-supervised contrastive learning with feature distribution matching between labeled and unlabeled data improves image classification performance. The study suggests that explicitly aligning representation distributions is a valuable design component for SSL algorithms and can yield consistent gains across different datasets.

Abstract: The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.

</details>


### [122] [BioPIE: A Biomedical Protocol Information Extraction Dataset for High-Reasoning-Complexity Experiment Question Answer](https://arxiv.org/abs/2601.04524)
*Haofei Hou,Shunyi Zhao,Fanxu Meng,Kairui Yang,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: The paper presents BioPIE, a dataset of procedure-centric knowledge graphs for biomedical experiment protocols, designed to improve question answering, especially for high-information-density and multi-step reasoning queries.


<details>
  <summary>Details</summary>
Motivation: Biomedical experimental QA is difficult due to high information density in protocols and the need for multi-step reasoning over detailed experimental steps. Existing biomedical QA datasets are coarse-grained and do not capture the fine-grained, procedural knowledge (entities, actions, relations) required to reason about laboratory experiments or support automation. There is a need for structured, machine-usable representations of experimental protocols that can be used both for QA and downstream applications like lab automation.

Method: The authors construct BioPIE, a large-scale dataset of biomedical experimental protocols annotated as procedure-centric knowledge graphs. These graphs represent experimental entities (e.g., reagents, instruments), actions (e.g., mix, incubate, centrifuge), and relations between them (e.g., input/output, temporal order, parameter settings). They then benchmark various information extraction methods on BioPIE to see how well these methods can recover such structured graphs from text. Finally, they build a QA system that uses the extracted knowledge graphs from BioPIE to answer questions about biomedical experiments, including high-information-density and multi-step reasoning questions.

Result: Information extraction methods evaluated on BioPIE demonstrate that it is feasible to obtain structured, protocol-level knowledge graphs from experimental procedures, though there is still room for improvement. The QA system that leverages BioPIE outperforms baselines on standard test sets as well as specially designed high-information-density and multi-step reasoning question sets, indicating that access to structured procedural knowledge significantly improves QA performance.

Conclusion: BioPIE fills a key gap in biomedical QA resources by providing fine-grained, protocol-level knowledge graphs that support detailed reasoning about experiments. The dataset enables better information extraction and more capable QA systems, particularly for complex, multi-step, high-information-density questions. This structured representation of experimental procedures provides a foundation for AI-assisted and increasingly autonomous biomedical experimentation across protocols.

Abstract: Question Answer (QA) systems for biomedical experiments facilitate cross-disciplinary communication, and serve as a foundation for downstream tasks, e.g., laboratory automation. High Information Density (HID) and Multi-Step Reasoning (MSR) pose unique challenges for biomedical experimental QA. While extracting structured knowledge, e.g., Knowledge Graphs (KGs), can substantially benefit biomedical experimental QA. Existing biomedical datasets focus on general or coarsegrained knowledge and thus fail to support the fine-grained experimental reasoning demanded by HID and MSR. To address this gap, we introduce Biomedical Protocol Information Extraction Dataset (BioPIE), a dataset that provides procedure-centric KGs of experimental entities, actions, and relations at a scale that supports reasoning over biomedical experiments across protocols. We evaluate information extraction methods on BioPIE, and implement a QA system that leverages BioPIE, showcasing performance gains on test, HID, and MSR question sets, showing that the structured experimental knowledge in BioPIE underpins both AI-assisted and more autonomous biomedical experimentation.

</details>


### [123] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: The paper presents TCAndon-Router (TCAR), an adaptive, explainable router for multi-agent systems that selects and coordinates multiple expert agents through natural-language reasoning and collaborative refinement, improving accuracy, conflict handling, and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent routing in enterprise applications mostly uses static, single-label task routing, which struggles to (1) flexibly integrate new expert agents as business domains grow, and (2) avoid conflicts when agents have overlapping capabilities. These issues hurt accuracy, robustness, and scalability in real-world MAS deployments, where task-based routing is crucial.

Method: The authors design TCAndon-Router (TCAR), an adaptive reasoning router that: (1) generates a natural-language reasoning chain about the input query; (2) predicts a *set* of suitable candidate agents instead of a single expert; and (3) supports dynamic onboarding of new agents as domains expand. They further introduce a collaborative execution pipeline where selected agents independently answer the query, and a dedicated Refining Agent aggregates and refines these outputs into one final high-quality response. The router is implemented and released as a model on HuggingFace.

Result: On both public benchmarks and proprietary enterprise datasets, TCAR achieves higher routing accuracy than prior methods, reduces routing conflicts between overlapping agents, and maintains strong performance in ambiguous, hard-to-route cases. The experiments empirically validate the benefits of reasoning-based, multi-label routing and the collaborative execution plus refinement pipeline.

Conclusion: TCAndon-Router offers an effective, explainable, and scalable routing mechanism for multi-agent systems, particularly suited to enterprise task routing scenarios. By combining natural-language reasoning, multi-agent selection, and a refinement stage, it overcomes the limitations of static single-label routers, improves robustness and accuracy, and supports dynamic expansion of agent ecosystems. The open-sourced implementation enables further research on collaborative, explainable routing in MAS.

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [124] [Personalized Model-Based Design of Human Centric AI enabled CPS for Long term usage](https://arxiv.org/abs/2601.04545)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: The paper surveys existing techniques for ensuring safety, sustainability, and security in long‑term AI-enabled human-centric control systems, identifies their limitations for long-term deployment and corner cases, and proposes personalized model-based solutions to address these gaps.


<details>
  <summary>Details</summary>
Motivation: AI-driven human-centric critical systems (e.g., medical monitoring, closed-loop glucose control, gesture-based HCI, autonomous driving, stroke monitoring and rehabilitation) must operate reliably over very long periods, often a user’s lifetime. Over time, such systems inevitably encounter rare or previously untested corner cases due to design flaws, limited testing resources, computational constraints of verification methods, and unpredictable human interactions. These corner cases can lead to violations of safety, sustainability, and security requirements. There is thus a need to systematically examine current assurance techniques and develop approaches that remain robust and trustworthy over long-term operation and in the presence of such corner conditions.

Method: The paper conducts an analysis/overview of existing methods for safety, sustainability, and security assessment of AI-enabled human-centric control systems, with a focus on their suitability for long-term deployment. It evaluates why current testing and verification approaches fail to cover rare or evolving scenarios. Building on this critical review, the authors then conceptually develop and propose personalized, model-based approaches tailored to individual users or contexts as a way to overcome identified limitations. These personalized models are intended to adapt to user-specific patterns and conditions, increasing coverage of corner cases over time.

Result: The main result is a structured characterization of the shortcomings in current safety, sustainability, and security analysis techniques when applied to long-term AI-enabled human-centric systems. The paper shows that existing approaches are insufficient to guarantee robust behavior in untested or uncertain operating conditions. It then presents a set of proposed personalized model-based solutions, arguing (conceptually rather than empirically, based on the abstract) that personalization and model-based reasoning can help anticipate or detect corner cases and thus improve long-term assurance. Specific quantitative results are not described in the abstract.

Conclusion: The paper concludes that existing methods for analyzing and testing AI-enabled human-centric control systems are inadequate for ensuring safety, sustainability, and security over long-term operation, especially in the face of untested corner cases. Personalized, model-based solutions are proposed as a promising direction to mitigate these limitations by tailoring system behavior and analysis to individual users and evolving conditions, with the goal of reducing uncertainty and improving trustworthiness in long-lived, safety-critical AI applications.

Abstract: Human centric critical systems are increasingly involving artificial intelligence to enable knowledge extraction from sensor collected data. Examples include medical monitoring and control systems, gesture based human computer interaction systems, and autonomous cars. Such systems are intended to operate for a long term potentially for a lifetime in many scenarios such as closed loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoting systems for stroke diagnosis, and rehabilitation. Long term operation of such AI enabled human centric applications can expose them to corner cases for which their operation is may be uncertain. This can be due to many reasons such as inherent flaws in the design, limited resources for testing, inherent computational limitations of the testing methodology, or unknown use cases resulting from human interaction with the system. Such untested corner cases or cases for which the system performance is uncertain can lead to violations in the safety, sustainability, and security requirements of the system. In this paper, we analyze the existing techniques for safety, sustainability, and security analysis of an AI enabled human centric control system and discuss their limitations for testing the system for long term use in practice. We then propose personalized model based solutions for potentially eliminating such limitations.

</details>


### [125] [Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation](https://arxiv.org/abs/2601.04562)
*Dongyi Lv,Qiuyu Ding,Heng-Da Xu,Zhaoxu Sun,Zhi Wang,Feng Xiong,Mu Xu*

Main category: cs.AI

TL;DR: ROS is a generative recommendation framework that injects explicit geographic reasoning into LLM-based recommenders for mobility and local service scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommendation systems treat recommendation as sequence generation but fail to fully exploit geographic signals, which are critical when recommending points-of-interest (POIs) and mobility-related services. There is a need for a method that allows LLMs to reason explicitly over spatial information and better generalize across different geographic regions.

Method: ROS introduces a Hierarchical Spatial Semantic ID (SID) that decomposes geographic locations and POI semantics into hierarchical, compositional tokens (from coarse to fine granularity). It further designs a three-stage Mobility Chain-of-Thought (CoT): (1) modeling user personality and preferences; (2) constructing an intent-aligned candidate POI space; and (3) performing locality-informed pruning based on spatial constraints. Finally, ROS employs spatial-guided reinforcement learning to align the LLM’s behavior with real-world geographic patterns and constraints.

Result: On three standard location-based social network datasets, ROS outperforms the strongest existing LLM-based recommenders by more than 10% in hit rate. It also shows superior cross-city transfer performance, and it achieves these improvements with a smaller backbone model compared with competing methods.

Conclusion: Incorporating explicit geographic reasoning via hierarchical spatial representations, mobility-specific chain-of-thought, and spatially guided reinforcement learning significantly boosts the effectiveness and generalization of LLM-based generative recommenders for mobility and local-service scenarios.

Abstract: Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.

</details>


### [126] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: The paper proposes BackdoorAgent, a framework and benchmark to systematically study how backdoor triggers implanted in different stages of LLM agents (planning, memory, tool use) persist and propagate across workflows, revealing significant vulnerabilities in agentic systems.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of backdoor attacks on LLM agents are fragmented and focus on isolated attack vectors, lacking an agent-centric, stage-aware understanding of how triggers injected at one workflow stage (planning, memory, tool-use) can persist and affect downstream behavior across multiple steps and intermediate states.

Method: The authors design BackdoorAgent, a modular, stage-aware framework that decomposes LLM agent workflows into three functional stages: planning, memory, and tool use. They instrument the execution of agents to monitor where and when backdoor triggers are activated and how they propagate across stages. Using this framework, they build a standardized benchmark across four representative applications—Agent QA, Agent Code, Agent Web, and Agent Drive—covering both language-only and multimodal agents, and then systematically inject and evaluate backdoor triggers at different stages.

Result: Empirical experiments with GPT-based backbones show that backdoor triggers implanted at a single stage frequently persist and propagate through multiple intermediate steps. Specifically, they observe trigger persistence in 43.58% of planning attacks, 77.97% of memory attacks, and 60.28% of tool-stage attacks across their benchmarked tasks, indicating that agentic workflows are broadly susceptible to stage-specific backdoor threats.

Conclusion: BackdoorAgent reveals that LLM agents’ multi-stage workflows inherently expose new, compounded vulnerabilities to backdoor attacks, where triggers at one stage can survive and influence downstream behavior. The framework and benchmark provide a unified basis for systematically studying, reproducing, and ultimately mitigating cross-stage backdoor threats in LLM-based agents, and the authors release code and data to support further research.

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [127] [Neurosymbolic Retrievers for Retrieval-augmented Generation](https://arxiv.org/abs/2601.04568)
*Yash Saxena,Manas Gaur*

Main category: cs.AI

TL;DR: The paper proposes Neurosymbolic RAG, a retrieval-augmented generation framework that combines neural retrievers with symbolic reasoning over knowledge graphs to improve transparency, interpretability, and performance, particularly in high-stakes tasks like mental health risk assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems rely on neural retrievers, re-rankers, and generators whose internal reasoning is opaque. This opacity makes it difficult to interpret why specific documents were selected, to debug system errors, and to build trust in domains where decisions have serious consequences. The authors are motivated to make the retrieval process more transparent and interpretable by incorporating symbolic structures like knowledge graphs.

Method: The authors introduce Neurosymbolic RAG, which augments neural retrieval with symbolic reasoning over knowledge graphs. They propose three concrete methods: (1) MAR (Knowledge Modulation Aligned Retrieval), which uses modulation networks to adjust query embeddings based on human-interpretable symbolic features, making the matching between queries and documents more explicit; (2) KG-Path RAG, which enriches queries by traversing paths in a knowledge graph, aiming to improve both retrieval quality and the interpretability of why documents are relevant; and (3) Process Knowledge-infused RAG, which uses domain-specific process or workflow tools to reorder retrieved content according to validated procedural knowledge, aligning retrieval with known best practices.

Result: On preliminary experiments in mental health risk assessment tasks, the proposed neurosymbolic RAG approaches improve both the transparency of the retrieval process and task performance compared to standard RAG baselines.

Conclusion: Integrating symbolic knowledge—particularly knowledge graphs and process knowledge—into RAG pipelines can make document retrieval more interpretable while also enhancing downstream performance. Neurosymbolic RAG offers a promising direction for building more transparent and trustworthy retrieval-augmented systems, especially in high-stakes application domains.

Abstract: Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance

</details>


### [128] [Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment](https://arxiv.org/abs/2601.04571)
*Delong Zeng,Yuexiang Xie,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: The paper proposes CIEA, a multimodal retrieval method that explicitly models complementary information in images beyond what is in paired text, using a unified latent space, a complementary information extractor, and dual contrastive losses, achieving state-of-the-art results over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal retrieval methods tend to focus on overlapping, text-aligned information in images and texts, neglecting the complementary visual information that is not explicitly described in the paired text, which can hurt retrieval quality and semantic completeness.

Method: CIEA maps both text and image content from documents into a shared latent space, and introduces a complementary information extractor that identifies and preserves differences in image representations relative to their paired texts. The model is trained with two complementary contrastive loss functions: one to maintain overall semantic integrity between modalities, and another to specifically encourage learning of complementary visual information. The architecture is evaluated against divide-and-conquer and universal dense retrieval baselines, with ablation, discussions, and case studies to dissect contributions.

Result: Experiments show that CIEA significantly outperforms both divide-and-conquer multimodal retrieval models and universal dense retrieval models on multimodal retrieval benchmarks, demonstrating better utilization of complementary information in images. Ablation studies confirm the effectiveness of the complementary information extractor and dual contrastive losses.

Conclusion: Explicitly modeling and aligning complementary multimodal information—rather than just shared cross-modal content—substantially improves multimodal retrieval. CIEA’s unified latent space, complementary information extractor, and dual contrastive training offer a strong and extensible framework for future research, supported by released open-source code.

Abstract: Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.

</details>


### [129] [Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing](https://arxiv.org/abs/2601.04575)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Chris Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.AI

TL;DR: Open recipe and dataset for training real‑time video game behavior‑cloning foundation models; systematic study of how scaling model/data size affects performance and causal reasoning.


<details>
  <summary>Details</summary>
Motivation: Behavior cloning has recently become strong when scaled, but there is a lack of open, reproducible recipes and a limited understanding of how scaling affects causal reasoning and performance in complex 3D game environments.

Method: Release 8300+ hours of human gameplay, plus open training/inference code and checkpoints for behavior‑cloned agents that run in real time on consumer GPUs. Train models up to 1.2B parameters on diverse 3D games, including a simplified toy environment, and systematically vary model depth, parameter count, training data, and training steps to study scaling laws and the emergence of causal policies.

Result: The best model plays various 3D video games at near‑human competitive levels in real time. In toy settings, jointly increasing data and network depth yields more causal policies. Similar trends appear in large‑scale models up to 1.2B parameters: larger and more deeply trained models with more data exhibit improved performance and more causal reasoning behaviors.

Conclusion: A practical, open behavior‑cloning recipe can yield strong, real‑time game‑playing agents on consumer hardware. Moreover, behavior‑cloned models follow scaling laws: increasing data and model depth improves both performance and the degree to which policies reflect causal rather than purely correlational reasoning, suggesting a path toward more capable and causally aware imitation‑learning systems.

Abstract: Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.

</details>


### [130] [Sci-Reasoning: A Dataset Decoding AI Innovation Patterns](https://arxiv.org/abs/2601.04577)
*Jiachen Liu,Maestro Harmon,Zechen Zhang*

Main category: cs.AI

TL;DR: Sci-Reasoning is a dataset that structurally captures the underlying reasoning processes behind top-tier AI papers, linking them to prior work and identifying common innovation patterns.


<details>
  <summary>Details</summary>
Motivation: Although AI research is progressing quickly, we lack structured, analyzable data about how scientists actually reason: how they find gaps in the literature, connect prior work, and derive new ideas. This gap makes it hard to systematically study innovation or build AI agents that can emulate scientific thinking.

Method: The authors build Sci-Reasoning by (1) selecting high-quality AI papers (Oral/Spotlight) from NeurIPS, ICML, and ICLR (2023–2025) using community quality signals; (2) tracing each paper back to its key predecessor works; and (3) encoding explicit, structured reasoning links that describe how the new work builds on or departs from those predecessors. They use an LLM-accelerated pipeline, with human verification, to scale annotation. They then analyze the resulting graph to identify recurring reasoning patterns.

Result: From the structured reasoning graph, they discover 15 distinct thinking patterns. Three dominate and together explain 52.7% of innovation strategies: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). They further find that the most impactful innovations often combine patterns, especially the pairs: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis.

Conclusion: Sci-Reasoning provides the first large-scale, structured dataset of scientific reasoning trajectories in AI. It enables quantitative analysis of how AI research progresses and offers rich, explicit reasoning traces that can be used to train and evaluate AI research agents designed to replicate human-like scientific innovation.

Abstract: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

</details>


### [131] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: Survey of how AI agents integrate with blockchains, offering taxonomy, threat model, comparative analysis, and a research roadmap for safe, interoperable transaction interfaces.


<details>
  <summary>Details</summary>
Motivation: Large language model agents are becoming capable of complex, tool-using behavior, while blockchains provide programmable, high-value infrastructure. Their combination is powerful but risky, as poor interface design can lead to security, governance, and economic failures. There is no consolidated understanding or standardization of how agents should safely observe and act on blockchain state, so the authors aim to systematically map this emerging space and clarify design and security requirements.

Method: The authors conduct a systematic literature review, starting from 3000+ records and filtering to 317 relevant works on agent–blockchain interoperability. They build a five-part taxonomy of integration patterns, construct a threat model specific to agent-mediated transaction workflows, and assemble a capability matrix comparing 20+ concrete systems across 13 design dimensions. From this, they derive gaps and propose abstractions and benchmarks.

Result: The paper identifies five main integration patterns: (1) read-only analytics, (2) simulation and intent generation, (3) delegated execution, (4) autonomous signing, and (5) multi-agent workflows. It presents a threat model covering prompt injection, policy misuse, key compromise, adversarial execution dynamics, and multi-agent collusion, among others. A comparative matrix characterizes over 20 systems along dimensions like custody models, permissioning, policy enforcement, observability, and recovery, revealing design gaps and inconsistencies. Based on this, the authors propose two key interface abstractions plus an evaluation suite.

Conclusion: Safe and interoperable AI-agent interaction with blockchains requires standardized abstractions for expressing transaction intents and recording policy decisions. The survey’s taxonomy, threat model, and capability matrix highlight current design patterns and risks, exposing open problems. The proposed Transaction Intent Schema and Policy Decision Record, alongside reproducible benchmarks, form a foundation for future research and engineering of secure, auditable agent-mediated on-chain execution.

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [132] [Evaluating Human and Machine Confidence in Phishing Email Detection: A Comparative Study](https://arxiv.org/abs/2601.04610)
*Paras Jain,Khushi Dhar,Olyemi E. Amujo,Esa M. Rantanen*

Main category: cs.AI

TL;DR: The paper studies how humans and interpretable machine learning models (Logistic Regression, Decision Trees, Random Forests) compare and complement each other when detecting phishing emails using TF-IDF and semantic embeddings, with a focus on accuracy, confidence, and cognitive/linguistic cues.


<details>
  <summary>Details</summary>
Motivation: Phishing email detection is cognitively demanding, involving pattern recognition, confidence judgment, and contextual reasoning. Current AI systems may be accurate but opaque and miscalibrated in confidence. The authors aim to understand how human cognition and interpretable ML models differ and interact in phishing detection, in order to design transparent AI that better supports human decision-making.

Method: Train three interpretable models (Logistic Regression, Decision Trees, Random Forests) on phishing vs legitimate emails using both TF-IDF features and semantic embeddings. Collect human judgments on the same emails, recording both phishing/legitimate labels, confidence ratings, and linguistic cues observers rely on. Compare model and human performance, analyze confidence calibration, the diversity of linguistic signals used, and the effects of language proficiency and age on detection accuracy.

Result: All three ML models reach solid classification accuracy, but their confidence scores are highly variable and less stable. Humans show more stable confidence and report using a richer set of linguistic indicators (varied language and contextual cues). Language proficiency has little impact on detection accuracy, whereas age significantly affects performance. The comparative analysis highlights distinct strengths and weaknesses of human vs model judgments.

Conclusion: Interpretable ML models can achieve high accuracy in phishing detection but differ from humans in confidence behavior and reliance on linguistic cues. Since humans show more consistent confidence and a broader use of language signs, AI systems should be designed to transparently expose their reasoning and complement human cognitive strategies. Considering factors like age, the findings can guide development of human-centered, cooperative AI tools for complex content analysis such as phishing detection.

Abstract: Identifying deceptive content like phishing emails demands sophisticated cognitive processes that combine pattern recognition, confidence assessment, and contextual analysis. This research examines how human cognition and machine learn- ing models work together to distinguish phishing emails from legitimate ones. We employed three interpretable algorithms Logistic Regression, Decision Trees, and Random Forests train- ing them on both TF-IDF features and semantic embeddings, then compared their predictions against human evaluations that captured confidence ratings and linguistic observations. Our results show that machine learning models provide good accuracy rates, but their confidence levels vary significantly. Human evaluators, on the other hand, use a greater variety of language signs and retain more consistent confidence. We also found that while language proficiency has minimal effect on detection performance, aging does. These findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks.

</details>


### [133] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: The paper proposes AgentDevel, a release-engineering style pipeline to improve LLM agents with stable, auditable, non-regressive updates instead of population search or in-agent self-refinement.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent improvement methods (self-improvement inside the agent; population-based search over many variants) can increase average scores but lead to unstable, opaque, and hard-to-audit behavior changes with many regressions. This makes it difficult to guarantee that new versions are strictly better, to track failures across versions, or to treat agents like reliably shippable software artifacts. The authors want a method that mirrors traditional software release engineering: one canonical version line, explicit regression checks, and auditable change rationales.

Method: They frame agent optimization as a release engineering process and design AgentDevel, a pipeline that: (1) runs the current agent on tasks and records execution traces; (2) uses an implementation-blind LLM critic that only sees traces (not model internals) to classify and describe failure symptoms; (3) aggregates these symptoms and generates script-based, executable diagnoses that define concrete engineering changes; (4) applies these changes to synthesize a single release candidate; and (5) applies flip-centered gating, which evaluates the candidate by prioritizing pass→fail regressions and fail→pass fixes as key evidence before promotion to the mainline version. The pipeline iterates this loop while maintaining one canonical agent version instead of a population of variants.

Result: On execution-heavy benchmarks, AgentDevel improves agent performance over iterations while showing substantially fewer regressions compared with approaches like population search or in-agent self-refinement. The resulting models follow smoother, more stable improvement curves and each release is accompanied by reproducible artifacts and clear diagnostic scripts, supporting auditing and debugging.

Conclusion: Treating LLM agents as software artifacts and improving them via a regression-aware release pipeline enables stable, auditable, non-regressive progress. AgentDevel operationalizes this by combining implementation-blind criticism, executable diagnosis, and flip-centered gating, offering a practical development discipline for building, debugging, and releasing LLM agents in a controlled, software-engineering style manner.

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [134] [Beyond the "Truth": Investigating Election Rumors on Truth Social During the 2024 Election](https://arxiv.org/abs/2601.04631)
*Etienne Casanova,R. Michael Alvarez*

Main category: cs.AI

TL;DR: The paper uses large language models and supporting methods to detect and analyze election-related rumors on an alt-tech platform, then studies how repeated exposure increases users’ likelihood of sharing them.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for scalable, rigorous psychological measurement of belief dynamics and misinformation spread in large, real-world online datasets, focusing on how rumors propagate and influence users over time.

Method: The authors (1) collect a large dataset of election rumors from a niche alt-tech platform, (2) build a multistage Rumor Detection Agent combining a synthetic data-augmented fine-tuned RoBERTa classifier, keyword filtering, and a two-pass GPT-4o mini verification pipeline to classify content, and (3) use these classifications to model exposure counts, sharing probability, and simulate rumor contagion across the user network.

Result: They show that the probability a user shares a rumor increases steadily with each additional exposure (evidence of a dose-response illusory truth effect), and simulations on the observed network indicate rapid contagion, with about 25% of users becoming "infected" with the rumor within four propagation steps.

Conclusion: LLMs, when integrated into a multistage detection and analysis pipeline, can accurately and efficiently identify rumors at scale and enable detailed, real-world measurement of psychological processes such as belief reinforcement and contagion in online social networks.

Abstract: Large language models (LLMs) offer unprecedented opportunities for analyzing social phenomena at scale. This paper demonstrates the value of LLMs in psychological measurement by (1) compiling the first large-scale dataset of election rumors on a niche alt-tech platform, (2) developing a multistage Rumor Detection Agent that leverages LLMs for high-precision content classification, and (3) quantifying the psychological dynamics of rumor propagation, specifically the "illusory truth effect" in a naturalistic setting. The Rumor Detection Agent combines (i) a synthetic data-augmented, fine-tuned RoBERTa classifier, (ii) precision keyword filtering, and (iii) a two-pass LLM verification pipeline using GPT-4o mini. The findings reveal that sharing probability rises steadily with each additional exposure, providing large-scale empirical evidence for dose-response belief reinforcement in ideologically homogeneous networks. Simulation results further demonstrate rapid contagion effects: nearly one quarter of users become "infected" within just four propagation iterations. Taken together, these results illustrate how LLMs can transform psychological science by enabling the rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets.

</details>


### [135] [Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models](https://arxiv.org/abs/2601.04651)
*Can Xu,Lingyong Yan,Jiayi Wu,Haosen Wang,Shuaiqiang Wang,Yuchen Li,Jizhou Huang,Dawei Yin,Xiang Li*

Main category: cs.AI

TL;DR: They propose Adversarial Reasoning RAG, a Reasoner-Verifier framework where two models jointly reason over retrieved documents, adversarially critique each other, and are trained with a process-aware reward that doesn't need an external scorer, leading to better multi-step reasoning and verification.


<details>
  <summary>Details</summary>
Motivation: Existing LRM+RAG systems still struggle with two issues: they usually reason from a single, unchallenged point of view, which limits self-correction and robustness, and their training relies mainly on outcome-based rewards (e.g., final answer correctness) that don't give enough guidance to shape the internal multi-step reasoning process. The paper aims to enable deeper, self-correcting reasoning and better training signals for complex reasoning chains in RAG settings.

Method: They design a Reasoner-Verifier framework called Adversarial Reasoning RAG (ARR). In this setup, a Reasoner and a Verifier both operate on retrieved evidence: the Reasoner proposes reasoning chains and answers, while the Verifier critiques and evaluates the logic. Training uses a "process-aware advantage" reward that incorporates explicit observational signals and model uncertainty instead of an external scoring model. This reward is applied through adversarial interaction between Reasoner and Verifier to jointly optimize both reasoning quality and verification behavior.

Result: On multiple benchmarks (not detailed in the abstract), the ARR framework outperforms baselines, demonstrating better reasoning fidelity and verification compared with standard LRM+RAG approaches that use single-perspective reasoning and outcome-only rewards.

Conclusion: Adversarially coupling a Reasoner and Verifier within a RAG pipeline, and training them with a process-aware internal reward rather than external scorers, yields more reliable, self-correcting multi-step reasoning over retrieved documents. This framework addresses key limitations of current LRM+RAG methods and improves performance across several benchmarks.

Abstract: Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.

</details>


### [136] [Vibe Coding an LLM-powered Theorem Prover](https://arxiv.org/abs/2601.04653)
*Zhe Hou*

Main category: cs.AI

TL;DR: Isabellm is an LLM-guided, fully automatic theorem prover for Isabelle/HOL that integrates local and API-based LLMs with search, planning, and repair mechanisms to prove lemmas beyond standard Isabelle automation.


<details>
  <summary>Details</summary>
Motivation: Classical Isabelle/HOL automation, including tools like Sledgehammer, cannot automatically prove many nontrivial lemmas, and existing LLM-theorem-proving integrations are often cloud-only, not tightly integrated, or not designed for consumer hardware. The authors aim to build a practical, locally-runnable, LLM-powered prover that can synthesize full proofs and explore how far current LLMs can go in programmatic proof search and repair.

Method: They design Isabellm, which couples (1) a stepwise prover that uses LLMs to propose proof commands within a bounded search loop checked by Isabelle, and (2) a higher-level proof planner that generates Isar proof outlines and then tries to fill gaps and repair failures. The system incorporates beam search over tactics, ML/RL-based tactic reranking, transformer-based premise selection, a micro-RAG scheme over existing Isar proofs in AFP, and counterexample-guided proof repair. It runs both with local LLMs via Ollama and remote models via APIs like Gemini CLI, and the whole codebase is LLM-written (GPT 4.1–5.2, Gemini 3 Pro, Claude 4.5).

Result: Empirically, Isabellm can automatically prove some lemmas that Isabelle’s built-in tools, including Sledgehammer, fail to discharge. This demonstrates that LLM-guided search and planning can extend the reach of established proof automation in Isabelle/HOL.

Conclusion: Isabellm shows that LLM-assisted, fully automatic proof synthesis is practically useful and can outperform standard Isabelle automation on some goals, even on consumer hardware. However, the experiments also expose limitations: even top-tier LLMs struggle to reliably operationalize complex fill-and-repair algorithms, underscoring present gaps in LLMs’ code generation fidelity and reasoning abilities for sophisticated proof workflows.

Abstract: We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle

</details>


### [137] [Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666)
*Zhiyuan Chang,Mingyang Li,Yuekai Huang,Ziyou Jiang,Xiaojun Jia,Qian Xiong,Junjie Wang,Zhaoyang Li,Qing Wang*

Main category: cs.AI

TL;DR: A paper proposing InstruCoT, a fine-tuning strategy that teaches LLMs via instruction-level chain-of-thought to detect and reject malicious prompt-injected instructions from any source or position, improving security without hurting normal utility.


<details>
  <summary>Details</summary>
Motivation: LLM-integrated applications are widely used but are vulnerable to prompt injection attacks, where malicious instructions are hidden in user inputs, tools, or external data. Existing defenses struggle because attacks can come through many channels and the malicious instructions blend semantically with benign context, making them hard to separate and block reliably. The authors want a general, model-side defense that works across sources and positions of injected content.

Method: They introduce InstruCoT, a model enhancement and training approach. It automatically synthesizes diverse training data containing both benign and malicious instructions embedded in various ways and locations. The model is then fine-tuned using instruction-level chain-of-thought: during training, it learns to explicitly reason about which spans are instructions, assess them for malicious intent or policy violation, and decide to ignore or reject unsafe ones while following safe ones. This reasoning is integrated at the instruction granularity so the model can recognize and isolate injected instructions without relying on their specific positions or surface patterns.

Result: They test InstruCoT-augmented models on three evaluation axes related to prompt injection: Behavior Deviation (getting the model to break its expected behavior), Privacy Leakage (inducing it to reveal protected information), and Harmful Output (eliciting dangerous or disallowed content). Across four different LLMs, InstruCoT substantially improves robustness on all three axes compared to baseline models and other defenses, while not reducing standard utility metrics such as task performance on benign inputs.

Conclusion: Instruction-level chain-of-thought fine-tuning with synthesized, diverse PI scenarios can significantly harden LLMs against prompt injection across sources and positions, allowing them to better detect and reject malicious instructions without sacrificing normal task performance. This suggests that targeted reasoning-oriented training is an effective strategy for building more secure LLM applications.

Abstract: Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation

</details>


### [138] [ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694)
*Zhilun Zhou,Zihan Liu,Jiahe Liu,Qingyu Shao,Yihan Wang,Kun Shao,Depeng Jin,Fengli Xu*

Main category: cs.AI

TL;DR: The paper proposes ResMAS, a framework to proactively improve the resilience of large-language-model-based multi-agent systems (LLM-based MAS) against perturbations such as agent failures, by jointly optimizing communication topology and prompts via learning-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems can perform complex tasks but are typically distributed across devices/environments, which makes them vulnerable to perturbations like agent failures. Prior work on adversarial attacks and defenses mostly reacts after attacks occur, instead of proactively designing systems that are inherently resilient. There is a need to understand what factors affect resilience in MAS and to develop methods that can systematically design resilient architectures and interactions before deployment.

Method: The authors first empirically study how communication topology and prompt design affect the resilience of LLM-based MAS under perturbations. Based on this, they propose ResMAS, a two-stage framework. In stage one, they train a reward model that predicts the resilience of a given MAS configuration, and then use reinforcement learning to train a topology generator that outputs resilient communication topologies customized for specific tasks. In stage two, they propose a topology-aware prompt optimization procedure that tailors each agent's prompt to its specific position, connections, and interactions in the generated topology. The combined system is evaluated across multiple tasks and settings.

Result: Experiments across various tasks and constraints show that ResMAS substantially improves the resilience of LLM-based multi-agent systems when subject to perturbations such as agent failures. The method also displays strong generalization, performing well on unseen tasks and with different underlying language models, indicating that the learned principles of resilient topology and prompt design transfer beyond the training conditions.

Conclusion: Communication topology and prompt design are key determinants of resilience in LLM-based multi-agent systems. By learning to generate resilient topologies and optimizing prompts in a topology-aware manner, ResMAS can proactively enhance system robustness to perturbations, outperforming existing reactive defenses and generalizing to new tasks and models. This suggests a promising direction for building inherently resilient LLM-based MAS in practical distributed environments.

Abstract: Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.

</details>


### [139] [Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning](https://arxiv.org/abs/2601.04695)
*Enze Pan*

Main category: cs.AI

TL;DR: Tape is a benchmark based on cellular automata to study how RL methods fail under latent rule shifts, keeping observations/actions fixed while dynamics change.


<details>
  <summary>Details</summary>
Motivation: Standard RL benchmarks often conflate in-distribution performance with robustness, making it hard to isolate and study out-of-distribution failures, especially when environment rules change but observations and actions stay the same. There is also a lack of standardized OOD evaluation protocols and statistical reporting practices.

Method: The authors build Tape from one-dimensional cellular automata, allowing exact control over training and test splits that differ only in underlying transition rules. They create a reproducible evaluation pipeline and test three families of RL methods: model-free baselines, model-based planning with learned world models, and meta-RL/task-inference methods. They design standardized OOD protocols and prescribe statistical reporting requirements. They also derive information-theoretic identities linking entropy reduction to conditional mutual information and expected posterior KL divergence.

Result: Across experiments, methods that perform strongly on in-distribution rules can fail catastrophically on held-out rules, revealing brittleness to latent rule shifts. OOD evaluation exhibits high variance, which can easily change method rankings unless enough random seeds and proper statistics are used. The information-theoretic analysis precisely characterizes what kinds of "uncertainty reduction" objectives do and do not imply robustness under rule changes.

Conclusion: Tape exposes a significant gap between in-distribution performance and robustness to rule shifts in RL, and highlights the need for careful, replicated OOD evaluation and rigorous statistical reporting. The benchmark, protocols, and theoretical identities aim to provide a standardized foundation for studying and improving RL methods under latent dynamics changes.

Abstract: We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what "uncertainty reduction" objectives can and cannot guarantee under rule shifts.

</details>


### [140] [A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models](https://arxiv.org/abs/2601.04696)
*Huayi Liu*

Main category: cs.AI

TL;DR: The paper proposes an LLM + knowledge graph + GNN + reinforcement learning framework that improves semantic understanding and decision-making in enterprise digital transformation, significantly reducing equipment failure response time and decision error costs.


<details>
  <summary>Details</summary>
Motivation: Enterprises undergoing digital transformation struggle with unstructured, multi-source data whose semantics are hard to understand, and they lack an intelligent, data-driven mechanism to support complex business decisions. Existing methods often treat text and structured business data separately, leading to poor decision support accuracy and slow responses in real scenarios such as equipment failure handling in manufacturing.

Method: 1) Use a fine-tuned BERT model for named entity recognition and relation extraction from multi-source heterogeneous texts; 2) Use GPT-4 to produce semantically enriched vector representations of the extracted entities and relations; 3) Design a two-layer GNN that fuses these LLM-based semantic vectors with enterprise business metadata to build a dynamic, scalable enterprise knowledge graph; 4) Apply reinforcement learning on top of this knowledge graph to generate and iteratively optimize decision paths, with a reward function reflecting business performance and used to drive mechanism iteration.

Result: In a manufacturing industry case, the proposed mechanism: (a) reduced response time to equipment failure from 7.8 hours to 3.7 hours, (b) achieved an F1 score of 94.3% for the relevant NLP/graph tasks, and (c) reduced compensation (cost) due to decision errors in annual digital transformation by 45.3%.

Conclusion: Combining large language models with a knowledge graph and reinforcement learning yields a more intelligent, efficient driving mechanism for enterprise digital transformation. The integrated framework enhances semantic understanding of unstructured text, improves the quality and speed of decision-making, and demonstrably reduces operational response times and financial losses due to decision errors, providing a scalable approach for complex enterprise environments.

Abstract: In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.

</details>


### [141] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: TourPlanner is a travel planning framework that uses spatially-aware POI recall, multi-path chain-of-thought reasoning, and constraint-gated reinforcement learning to generate itineraries that better satisfy both feasibility and user preferences.


<details>
  <summary>Details</summary>
Motivation: Travel planning requires combining various factors (e.g., POIs, time, distance, and user preferences) into coherent itineraries. Existing methods struggle with three key issues: efficiently pruning POIs without missing good options, relying on a single reasoning path that under-explores the solution space, and jointly handling hard constraints (e.g., time, budget) and soft constraints (e.g., preferences) during optimization. There is a need for a framework that can better search the feasible solution space while respecting constraints and personalizing plans.

Method: The authors propose TourPlanner, which has three main components. (1) Personalized Recall and Spatial Optimization (PReSO) builds a candidate set of POIs that is both personalized to the user and spatially optimized, improving recall while keeping the search space manageable. (2) Competitive consensus Chain-of-Thought (CCoT) introduces multi-path reasoning: several reasoning trajectories are generated to explore different feasible itineraries, and a competitive/consensus process selects better solutions. (3) A reinforcement learning module with a sigmoid-based gating mechanism is used to optimize itineraries under constraints: the gating mechanism ensures that hard constraints are prioritized, and only when they are satisfied does the model increasingly attend to optimizing soft constraints such as user preferences.

Result: On standard travel planning benchmarks, TourPlanner achieves state-of-the-art results. It significantly outperforms previous approaches in terms of feasibility (satisfying hard constraints like time and route structure) and alignment with user preferences (soft constraints), indicating more practical and user-satisfying itineraries.

Conclusion: TourPlanner effectively addresses core limitations in existing travel planning systems by combining spatially-aware candidate selection, multi-path chain-of-thought reasoning, and constraint-gated reinforcement learning. The framework leads to itineraries that are both feasible and better aligned with user preferences, setting a new performance bar on travel planning benchmarks and demonstrating the value of multi-path reasoning and gated constraint optimization in complex decision-making tasks.

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [142] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: The paper introduces M-ASK, a multi-agent framework that separates search behavior from knowledge management in LLM-based agentic search, leading to better accuracy and more stable training on multi-hop QA tasks.


<details>
  <summary>Details</summary>
Motivation: Existing agentic search systems use a single, monolithic agent to both reason and use tools. This design causes several problems: overly long and unconstrained reasoning traces, difficulty in assigning credit because rewards are only given at the final outcome, and noisy, unstable learning due to stochastic search behavior. The authors aim to create a more structured, stable, and efficient framework for complex information seeking with LLMs.

Method: M-ASK decomposes agentic search into two specialized agent types: (1) Search Behavior Agents that focus on planning and executing search actions (e.g., which tools to call, what queries to issue, how to navigate multi-hop search), and (2) Knowledge Management Agents that aggregate, filter, and maintain a compact internal context or memory from retrieved information. This role separation reduces interference between reasoning and context construction. Additionally, the framework introduces turn-level rewards that provide fine-grained supervision at each step of the interaction, guiding both search decisions and knowledge updates rather than only supervising at the final answer.

Result: On multi-hop question answering benchmarks, M-ASK achieves higher answer accuracy compared to strong baseline systems that use monolithic agents. It also exhibits significantly more stable training dynamics, suggesting that the turn-level rewards and role decomposition lead to more reliable learning and search behavior.

Conclusion: Decoupling agentic search into specialized search and knowledge management agents, combined with turn-level reward signals, improves both effectiveness and stability in LLM-based complex information seeking. M-ASK offers a structured, multi-agent blueprint for building more robust agentic search systems, and empirical results on multi-hop QA validate its advantages over traditional monolithic designs.

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [143] [Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis](https://arxiv.org/abs/2601.04709)
*Gijun Park*

Main category: cs.AI

TL;DR: The paper proposes a multimodal framework that aligns time-series telemetry with language model embeddings to improve automatic root cause diagnosis in cloud systems, achieving state-of-the-art accuracy on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Root cause analysis in cloud infrastructure relies heavily on time-series performance metrics, but large language models are inherently designed for discrete text tokens, not continuous temporal signals. Existing methods do not adequately bridge this gap, limiting the effectiveness of LLM-based automation for incident management and failure diagnosis.

Method: The authors design a multimodal diagnostic framework with three main components: (1) a semantic compression technique that converts temporal segments of time-series data into compact single-token abstractions while preserving their pattern-level meaning; (2) an alignment encoder with gated cross-attention that maps time-series features into the latent embedding space of a pretrained language model; and (3) a retrieval-augmented diagnostic pipeline that combines the aligned embeddings with historical incident and failure knowledge to generate expert-level root cause attributions.

Result: On six cloud system benchmark datasets, the framework achieves leading performance in diagnostic tasks, including a reported 48.75% diagnostic accuracy, with especially strong gains in challenging scenarios involving compound or multi-faceted failure modes.

Conclusion: Aligning time-series telemetry representations with language model embedding spaces via semantic compression and gated cross-attention enables LLMs to reason effectively over multimodal operational data. This embedding-space alignment strategy substantially improves automated root cause diagnosis in real-world cloud incident response settings.

Abstract: Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.

</details>


### [144] [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
*Chang Zhao,Zheming Yang,Yunqing Hu,Qi Guo,Zijian Wang,Pengcheng Li,Wen Ji*

Main category: cs.AI

TL;DR: ThinkDrive is a progressive RL fine-tuning framework that leverages Chain-of-Thought explanations and difficulty-aware optimization to improve LLM-based autonomous driving decisions, outperforming strong RL baselines and even larger models like GPT-4o on driving exam benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based autonomous driving systems struggle with unstructured reasoning, weak generalization, and misalignment with human driving intent. Chain-of-Thought reasoning helps interpretability but is underutilized by standard supervised fine-tuning, while existing RL approaches are unstable and do not adequately control reasoning depth. There is a need for a training framework that combines explicit reasoning with stable, performance-oriented optimization tailored to the varying difficulty of driving scenarios.

Method: The paper proposes ThinkDrive, a two-stage training framework for LLMs in autonomous driving. Stage one uses supervised fine-tuning with Chain-of-Thought (CoT) explanations to instill explicit, interpretable reasoning patterns. Stage two introduces a progressive reinforcement learning phase with a difficulty-aware adaptive policy optimizer that modulates learning intensity based on the estimated complexity of each driving sample, encouraging deeper, more robust reasoning on harder cases while maintaining stability.

Result: On a public autonomous driving benchmark, ThinkDrive delivers consistent improvements over strong RL baselines: +1.45% on the overall exam metric, +1.95% on the easy-exam subset, and +1.01% in accuracy. Furthermore, a 2B-parameter model trained with ThinkDrive surpasses GPT-4o—a substantially larger model—by 3.28% on the exam metric, demonstrating both efficiency and effectiveness of the proposed framework.

Conclusion: ThinkDrive effectively integrates Chain-of-Thought supervision with progressive, difficulty-aware reinforcement learning, leading to more structured reasoning and better alignment with human driving intent in LLM-based autonomous driving. Its performance gains over strong baselines and larger proprietary models indicate that carefully designed training strategies can compensate for smaller model size and significantly enhance decision-making in complex real-world tasks.

Abstract: With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.

</details>


### [145] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: The paper introduces CompassMem, an event-centric, graph-structured memory framework for LLM-based agents that enables logical, long-horizon reasoning and improves retrieval and QA performance over existing flat or shallow similarity-based memories.


<details>
  <summary>Details</summary>
Motivation: LLM agents increasingly need to operate in long-horizon settings where they must remember and reason over many past interactions or observations. Existing memory systems are mostly flat, rely on surface-level semantic similarity for retrieval, and do not explicitly represent logical relationships or temporal structure among experiences. This limits agents’ ability to perform coherent, multi-step reasoning over distant events and to use their past experiences in a goal-directed way. The paper aims to overcome these limitations by providing a structured, logic-aware memory mechanism that better reflects how events are segmented and related in human cognition.

Method: The authors propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. First, continuous streams of experience are incrementally segmented into discrete events. These events are then organized as nodes in an Event Graph, where edges encode explicit logical relations (e.g., causal, temporal, or other reasoning-relevant links) between events. This Event Graph functions as a ‘logic map’ of the agent’s past. Memory retrieval is implemented as structured, goal-directed navigation over this graph rather than simple embedding similarity search: the agent progressively traverses relevant nodes and their logical neighbors to collect memories that support the current reasoning or decision task. The framework is instantiated on top of several backbone LLMs and evaluated within long-context reasoning and QA settings.

Result: On the LoCoMo and NarrativeQA benchmarks, CompassMem yields consistent gains over baseline memory systems in both retrieval quality (i.e., selecting more relevant past events) and downstream reasoning performance (e.g., better question answering or long-horizon decision-making) across multiple backbone models. These improvements demonstrate that structured, event-graph-based memory enables LLM agents to handle longer and more complex dependencies than flat, similarity-based memory approaches.

Conclusion: CompassMem shows that organizing an LLM agent’s memory as an event-centric graph with explicit logical relations supports more effective long-horizon reasoning than flat, similarity-only memory stores. By enabling structured, goal-directed navigation over past experiences, the approach improves both retrieval fidelity and task performance on long-context benchmarks. The work suggests that cognitively inspired, structured memory mechanisms are a promising direction for scaling LLM agents to more complex, temporally extended tasks.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [146] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: The paper proposes Miner, a simple reinforcement learning method that uses a model’s own uncertainty as a self-supervised reward to efficiently train large reasoning models, especially on datasets where most rollouts are already correct.


<details>
  <summary>Details</summary>
Motivation: Existing critic-free RL methods for large reasoning models become inefficient on positive homogeneous prompts, where nearly all rollouts are correct and thus have zero advantage. This leads to wasted computation and poor learning signals, as standard advantage estimates provide no gradient when all trajectories look equally good. The authors want a way to still extract informative training signal from such data without extra critics, auxiliary models, or external supervision.

Method: The authors introduce Miner (Mine intrinsic mastery), which transforms the policy’s intrinsic uncertainty into a reward signal in a critic-free RL setting. Miner has two main components: (1) a token-level focal credit assignment mechanism that increases gradient emphasis on tokens where the model is uncertain and downweights overconfident tokens, and (2) adaptive advantage calibration that blends the intrinsic uncertainty-based rewards with standard verifiable rewards when they are available. This design allows learning even when extrinsic rewards are flat while remaining compatible with conventional RL signals.

Result: Miner is evaluated on six reasoning benchmarks using Qwen3-4B and Qwen3-8B base models. It outperforms four competing RL algorithms, including GRPO, achieving up to 4.58 absolute improvement in Pass@1 and 6.66 in Pass@K over GRPO. Comparisons against other exploration-enhancing methods show that Miner’s specific innovations in focal credit assignment and advantage calibration yield superior performance.

Conclusion: Leveraging a model’s latent uncertainty as an intrinsic reward is both necessary and sufficient to unlock efficient, scalable RL training for reasoning models, especially on datasets where external rewards provide little differentiation. Miner’s focal token-level credit assignment and adaptive advantage calibration turn otherwise wasted positive-homogeneous data into a rich learning signal, enabling state-of-the-art gains without external critics or extra inference cost.

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [147] [KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions](https://arxiv.org/abs/2601.04745)
*Tingyu Wu,Zhisheng Chen,Ziyan Weng,Shuhe Wang,Chenglong Li,Shuo Zhang,Sen Hu,Silin Wu,Qizhen Lan,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: The paper introduces KnowMeBench, a benchmark built from real long-form autobiographical narratives to evaluate how well models understand stable aspects of a person beyond simple retrieval.


<details>
  <summary>Details</summary>
Motivation: Current long-horizon memory benchmarks rely on multi-turn dialogues or synthetic histories, so retrieval performance doesn’t accurately measure how well models understand a person’s enduring traits, motivations, and decision patterns.

Method: They construct KnowMeBench from long autobiographical narratives, reconstructing each into a time-anchored, flashback-aware event stream. On top of this, they design evidence-linked questions that probe different levels of understanding: factual recall, subjective states, and principle-level reasoning. They then test standard and retrieval-augmented models on this benchmark.

Result: Retrieval-augmented methods notably improve performance on straightforward factual recall, but they still perform poorly on temporally grounded explanations and higher-level inferences about motivations and principles.

Conclusion: Understanding people over long horizons requires memory mechanisms that go beyond simple retrieval; models need capabilities for temporal reasoning and abstraction over autobiographical content, which KnowMeBench is designed to measure.

Abstract: Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.

</details>


### [148] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: The paper studies how to replace multi-agent LLM systems with a single-agent that selects from a library of skills, and analyzes the limits of such skill selection as the library grows.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems work well for complex reasoning but are computationally expensive due to explicit inter-agent communication. The authors want to know if a single LLM with an internal library of skills can retain the modularity and performance benefits of multi-agent setups while reducing cost, and what fundamental limits arise when scaling the number of skills.

Method: They conceptually compile multi-agent systems into an equivalent single-agent system where each skill corresponds to an internalized behavior. They run preliminary experiments comparing efficiency and accuracy between multi-agent and single-agent-with-skills on reasoning benchmarks. Then they systematically vary the size and semantic structure of the skill library to study how accurately the LLM can select the right skill. They analyze scaling patterns, especially the impact of library size and semantic confusability, and test hierarchical routing as a mitigation approach.

Result: The single-agent skill-based approach significantly reduces token usage and latency while remaining competitively accurate on reasoning benchmarks compared to multi-agent systems. As the number of skills increases, skill selection accuracy stays high until a critical library size is reached, after which it collapses sharply, resembling a cognitive phase transition. The degradation depends strongly on semantic similarity among skills, not just on raw library size. Initial experiments show that hierarchical organization of skills improves robustness and mitigates some of these capacity limits.

Conclusion: LLMs selecting from large skill libraries face bounded-capacity limits similar to human decision-making: beyond a critical, structure-dependent library size, selection performance breaks down abruptly. Semantic confusability is a key driver of this effect. Organizing skills hierarchically offers a promising way to scale skill-based agents while controlling selection complexity. The work provides a cognitive-inspired framework and practical design guidelines for building efficient, scalable single-agent systems that emulate multi-agent modularity.

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [149] [Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data](https://arxiv.org/abs/2601.04764)
*Zhen Chen,Weihao Xie,Peilin Chen,Shiqi Wang,Jianping Wang*

Main category: cs.AI

TL;DR: The paper proposes Orion-RAG, a lightweight retrieval-augmented generation framework that turns fragmented, unlinked documents into semi-structured, cross-linked data using simple path extraction instead of heavy knowledge graph construction, achieving significantly better performance and cost-efficiency than existing RAG systems.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems struggle when knowledge is scattered across many isolated files (reports, logs, etc.) without explicit links. Standard search processes each file independently and cannot easily connect related information across documents. Building full knowledge graphs is too expensive and impractical at real-world scale. The authors aim to create a practical way to exploit cross-document relations for RAG without heavy graph construction or complex algorithms.

Method: The authors introduce Orion-RAG, which applies a low-complexity strategy to extract lightweight paths that connect related concepts across documents. Instead of constructing a full knowledge graph, Orion-RAG identifies minimal, relevant link structures that transform unstructured, fragmented text into semi-structured data. This representation supports cross-file linking, real-time updates, and human-in-the-loop verification, all while maintaining low computational cost.

Result: Across multiple domains, Orion-RAG consistently outperforms mainstream RAG frameworks in knowledge-intensive tasks. It effectively links information spread across files and supports efficient updates and verification. On the FinanceBench benchmark, it achieves a 25.2% relative precision improvement over strong existing baselines, indicating substantially better answer quality for financial QA tasks.

Conclusion: Simple, low-complexity path extraction is sufficient to organize fragmented document collections into an effective semi-structured substrate for RAG, eliminating the need for heavy knowledge graph construction. Orion-RAG delivers superior accuracy, real-time adaptability, and cost-efficiency over existing RAG frameworks, demonstrating that lightweight structure can yield large practical gains in real-world, fragmented data environments.

Abstract: Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.

</details>


### [150] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: The paper proposes AT^2PO, a unified tree-search-based reinforcement learning framework for multi-turn LLM agents that improves exploration, credit assignment, and policy optimization, achieving state-of-the-art results on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents for multi-turn tasks struggle with limited exploration diversity, sparse rewards that make credit assignment difficult, and misalignment between how policies are optimized and how agents actually make decisions across turns. Current agentic RL methods do not adequately address these core challenges in a unified way.

Method: The authors introduce AT^2PO, which builds a turn-level tree structure over multi-turn interactions. This structure supports (1) Entropy-Guided Tree Expansion to improve exploration by selectively expanding promising branches based on entropy signals, and (2) Turn-wise Credit Assignment to propagate sparse outcome rewards back to individual turns more precisely. On top of this, they define Agentic Turn-based Policy Optimization (ATPO), a turn-level learning objective that updates the policy at the granularity of turns rather than individual actions. ATPO is designed to be orthogonal to the tree-search component, so it can plug into different multi-turn RL pipelines.

Result: Across seven benchmarks, AT^2PO delivers consistent performance gains over a strong state-of-the-art baseline, with an average improvement of up to 1.84 percentage points. Ablation studies show that both the entropy-guided tree expansion and the turn-wise credit assignment, as well as the ATPO objective, each contribute meaningfully to the overall improvement.

Conclusion: The paper concludes that modeling multi-turn LLM agent interactions at the turn level, both in search (via a turn-level tree) and in learning (via a turn-based optimization objective), effectively addresses key challenges in agentic RL. AT^2PO improves exploration, credit assignment, and policy alignment and can serve as a general, modular framework for enhancing multi-turn RL pipelines for LLM agents.

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [151] [SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence](https://arxiv.org/abs/2601.04770)
*Encheng Su,Jianyu Wu,Chen Tang,Lintao Wang,Pengze Li,Aoran Wang,Jinouwen Zhang,Yizhou Wang,Yuan Meng,Xinzhu Ma,Shixiang Tang,Houqiang Li*

Main category: cs.AI

TL;DR: They propose SciIF, a benchmark to test whether LLMs can follow rigorous, scientific-style instructions and constraints, not just get the final answer right.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks either check generic instruction-following (mostly surface-level formatting) or domain-specific problem solving (mostly whether the final answer is correct). They do not check whether the reasoning process and constraints match scientific standards—for example, using correct assumptions, boundary conditions, units, and prescribed methods. This allows models to be rewarded for correct answers derived via scientifically invalid or opaque reasoning, which is dangerous for real scientific use.

Method: They define a new capability, scientific instruction following: solving scientific problems while strictly obeying constraints that guarantee scientific validity. Then they build SciIF, a benchmark spanning multiple scientific disciplines and university-level problems. Each problem is paired with a fixed catalog of constraints across three pillars: (1) scientific conditions such as boundary checks and explicit assumptions; (2) semantic stability such as consistent units and symbol usage; and (3) specific processes such as required numerical techniques. The benchmark is designed for auditability by requiring models to output explicit evidence that each constraint is satisfied, not just a final answer.

Result: SciIF provides a way to simultaneously measure both solution correctness and adherence to many types of constraints. This enables fine-grained diagnosis of where models fail in compositional reasoning across constraints and solution steps, rather than only at the final answer. The abstract implies that SciIF can distinguish models that appear correct on traditional benchmarks from those that are truly scientifically reliable, though specific quantitative results are not detailed in the abstract.

Conclusion: Incorporating scientific instruction following into evaluation is crucial as LLMs are used for scientific discovery. SciIF offers a structured, auditable benchmark that evaluates not only whether the answer is correct but also whether it was obtained under scientifically valid constraints. This should help ensure that LLMs act as dependable agents within the strict logical and methodological requirements of science.

Abstract: As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.

</details>


### [152] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: APEX is an agentic, interactive system for editing academic posters, plus a benchmark (APEX-Bench) and evaluation protocol showing it beats existing automatic poster tools.


<details>
  <summary>Details</summary>
Motivation: Existing paper-to-poster tools are one-shot and non-interactive, so they often miss nuanced, subjective user intent and offer limited fine-grained control over complex academic poster layouts.

Method: They design APEX, an agentic framework that edits posters through multi-level API-based operations and a review-and-adjust loop, allowing iterative, fine-grained user control. They also build APEX-Bench with 514 diverse poster-editing instructions, categorized by operation type, difficulty, and abstraction, created via reference-guided and reference-free strategies. Finally, they define a VLM-as-a-judge evaluation protocol along multiple dimensions (instruction fulfillment, modification scope, visual consistency and harmony).

Result: On the proposed benchmark and evaluation setup, APEX substantially outperforms baseline paper-to-poster and editing methods across instruction-following, layout quality, and visual harmony metrics.

Conclusion: Interactive, agentic editing with structured APIs and automated multi-dimensional evaluation yields better academic poster generation than prior single-pass methods, and APEX plus APEX-Bench provides a foundation for future research on controllable poster design.

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [153] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: They propose a new defense method against indirect prompt injection for LLM agents that parse tool results to keep useful data while filtering malicious code, achieving low attack success rates with good task utility.


<details>
  <summary>Details</summary>
Motivation: As LLM agents start to control physical systems and robots, indirect prompt injection—where adversarial instructions are hidden in tool outputs—can hijack their decisions and cause unauthorized or dangerous actions. Existing defenses are either heavy detection models that are expensive and need constant retraining, or prompt-based defenses that are easy to deploy but fragile and have high attack success rates. There is a need for a more robust, efficient defense that works well under attack while preserving the agent’s usefulness.

Method: They introduce a method that parses the outputs of tools used by the LLM agent to extract only the precise, relevant data and systematically strip or filter any embedded instructions or code that could act as an indirect prompt injection. Instead of relying on separate detection models or purely on prompt engineering, the approach restructures tool results into a safe, structured form that the LLM consumes, thereby preventing malicious content from influencing its decision-making.

Result: Experiments show that their parsing-based defense attains the lowest reported Attack Success Rate (ASR) compared with existing detection-model-based and prompt-based defenses, while still preserving competitive Utility under Attack (UA). This indicates that their method both blocks a larger fraction of attacks and keeps the agent’s performance on its intended tasks relatively high when under attack.

Conclusion: Carefully parsing and structuring tool outputs before presenting them to an LLM is an effective and efficient strategy to mitigate indirect prompt injection. The proposed method improves robustness over prior work, achieving substantially lower ASR without sacrificing much utility, and offers a practical defense as LLM agents gain control over real-world, physical systems.

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [154] [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](https://arxiv.org/abs/2601.04805)
*Siyuan Gan,Jiaheng Liu,Boyan Wang,Tianpei Yang,Runqing Miao,Yuyao Zhang,Fanyu Meng,Junlan Feng,Linjian Meng,Jing Huo,Yang Gao*

Main category: cs.AI

TL;DR: The paper proposes TNT, a method to reduce unnecessary long-chain reasoning in large reasoning models while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models are accurate but inefficient because they often generate long Chains of Thought (CoT) even for simple queries, causing high computational cost. Existing RL-based hybrid reasoning approaches suffer from reward hacking when deciding whether to think or not, and current mitigations (SFT or fixed token caps) are either expensive or ineffective.

Method: TNT (Thinking-Based Non-Thinking) is an RL-based hybrid reasoning framework that avoids supervised fine-tuning. It uses information from the solution part of CoT-enabled (thinking) responses to adaptively set different maximum token budgets for non-thinking responses across queries, instead of imposing a uniform token limit. This design aims to better align the reward signal with true thinking behavior and reduce reward hacking.

Result: On five math benchmarks, TNT cuts token usage by about 50% compared with DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B while also significantly boosting accuracy. Among all tested methods, it offers the best accuracy–efficiency trade-off. The share of reward-hacking cases in outputs labeled as non-thinking stays below 10% on all datasets.

Conclusion: TNT provides an efficient and accurate way to control when and how much a model should reason explicitly. By using thinking traces to calibrate non-thinking token budgets without SFT, it substantially reduces computation, improves performance, and keeps reward hacking at a low level, achieving a superior balance between accuracy and efficiency in hybrid reasoning models.

Abstract: Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.

</details>


### [155] [SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning](https://arxiv.org/abs/2601.04809)
*Caijun Xu,Changyi Xiao,Zhongyuan Peng,Xinrun Wang,Yixin Cao*

Main category: cs.AI

TL;DR: The paper introduces SCALER, an adaptive synthetic environment framework that keeps reinforcement learning signals informative for training language models on reasoning tasks, outperforming dataset-based RL baselines.


<details>
  <summary>Details</summary>
Motivation: RL can improve LLM reasoning, but progress stalls when task difficulty no longer matches model ability or when training is dominated by repetitive patterns and finite datasets, leading to weak or sparse learning signals and overfitting.

Method: They build SCALER, which (1) converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, and (2) uses an adaptive multi-environment RL strategy that continually tunes instance difficulty and actively selects diverse environments based on the model’s current capability frontier.

Result: Experiments across several reasoning benchmarks show that SCALER leads to better performance than RL methods trained on static datasets and yields more stable, longer-horizon training curves, indicating sustained learning rather than early plateauing.

Conclusion: Adaptive environment design via SCALER—combining scalable synthetic instance generation with dynamic difficulty and environment selection—prevents reward sparsity, reduces overfitting to narrow task types, and supports continuous improvement of LLM reasoning with RL.

Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

</details>


### [156] [AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding](https://arxiv.org/abs/2601.04819)
*Aleksei Kondratenko,Mussie Birhane,Houssame E. Hsain,Guido Maciocci*

Main category: cs.AI

TL;DR: The paper introduces AECV-Bench, a benchmark to test how well multimodal and vision-language models understand architecture, engineering, and construction (AEC) drawings, revealing that current models handle text and OCR well but struggle with symbol-based drawing understanding and object counting.


<details>
  <summary>Details</summary>
Motivation: Although AEC drawings are rich in geometric and semantic information, it is unclear how well modern multimodal and vision-language models can interpret their symbolic and spatial conventions. Existing benchmarks largely focus on natural images or generic documents rather than technical architectural plans, leaving a gap in systematically assessing model competence on realistic AEC artefacts needed for automation and tooling in this industry.

Method: The authors construct AECV-Bench with two complementary tasks: (i) object counting on 120 high-quality floor plans for objects such as doors, windows, bedrooms, and toilets, and (ii) drawing-grounded document QA with 192 QA pairs targeting OCR, instance counting, spatial reasoning, and comparative reasoning over typical drawing regions. They evaluate a broad set of state-of-the-art multimodal and vision-language models under a unified protocol, use per-field exact-match accuracy and MAPE for counting, and use accuracy plus an LLM-as-a-judge scoring pipeline—with human adjudication for edge cases—for document QA.

Result: Models show high performance on OCR and text-centric document QA, reaching up to 0.95 accuracy. Performance drops for spatial reasoning and is particularly weak for symbol-centric drawing understanding, such as reliably counting doors and windows, where accuracies often fall in the 0.40–0.55 range and proportional errors are substantial. This reveals a clear capability gradient across task types.

Conclusion: Current multimodal and vision-language systems can act effectively as document assistants for AEC artefacts when tasks rely mainly on text extraction and simple reasoning, but they lack robust literacy in symbolic architectural drawings, especially for object counting and detailed spatial understanding. This gap indicates the need for domain-specific representations, tool-augmented approaches, and human-in-the-loop workflows to achieve reliable AEC automation.

Abstract: AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.

</details>


### [157] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: The paper proposes DR-LoRA, a dynamic-rank LoRA framework for MoE LLMs that allocates different LoRA ranks to experts based on their task-specific importance, improving performance under the same parameter budget.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA assign the same rank to all experts in MoE LLMs, ignoring that different experts specialize in different functions and have varying relevance to a given downstream task. This uniform allocation wastes parameters on low-utility experts and starves high-utility ones, leading to suboptimal adaptation efficiency and performance.

Method: DR-LoRA introduces a dynamic rank allocation mechanism for LoRA within MoE models. During fine-tuning, it computes an Expert Saliency Score for each expert, combining the expert’s routing frequency (how often it is selected by the router for a task) and the importance of its current LoRA ranks. Based on these scores, DR-LoRA incrementally increases LoRA rank (capacity) for high-saliency experts while keeping low-saliency experts with smaller ranks, thus forming a heterogeneous rank distribution tuned to the specific task, all under a fixed overall parameter budget.

Result: Across multiple benchmarks, DR-LoRA achieves higher task performance than standard uniform-rank LoRA and other static rank allocation baselines, while using the same total number of trainable parameters. This shows that adaptive, expert-aware rank allocation leads to more effective parameter utilization in MoE LLM fine-tuning.

Conclusion: Dynamic, expert-specific allocation of LoRA ranks in MoE LLMs better matches model capacity to task needs than uniform schemes. By leveraging expert saliency derived from routing behavior and rank importance, DR-LoRA can automatically learn heterogeneous rank distributions that yield superior downstream performance under fixed parameter budgets, highlighting the value of task-aware capacity reallocation in PEFT for MoE architectures.

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [158] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: The paper proposes OI-MAS, a multi-agent system that adaptively assigns different-sized language models to different agent roles and reasoning stages to improve both performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems with large language models outperform single-agent setups on complex reasoning but are computationally expensive because they typically use the same large model for all agents and all reasoning stages, regardless of actual task difficulty or role requirements. There is a need for a more efficient approach that tailors the model capacity to the cognitive demand at each step while maintaining or improving accuracy.

Method: The authors propose OI-MAS, a multi-agent framework with a heterogeneous pool of LLMs of different scales. It uses a state-dependent routing mechanism to dynamically select which agent roles should be active and which model size each agent should use at each reasoning step. Additionally, a confidence-aware mechanism estimates task complexity or uncertainty and, based on this estimate, chooses an appropriate model scale, avoiding overuse of the largest models when not needed.

Result: In experiments, OI-MAS outperforms baseline multi-agent systems that do not use adaptive model selection. It achieves up to 12.88% higher accuracy while simultaneously reducing computation cost by up to 79.78%, indicating substantial gains in both effectiveness and efficiency.

Conclusion: Adaptive, heterogeneous model selection within multi-agent LLM systems can significantly improve reasoning performance while greatly reducing computational cost. By routing tasks and reasoning stages to appropriately scaled models and agent roles, OI-MAS demonstrates that multi-scale, confidence-aware MAS architectures are a promising direction for building more efficient and powerful reasoning systems.

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [159] [Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype](https://arxiv.org/abs/2601.04864)
*Haihua Luo,Xuming Ran,Zhengji Li,Huiyan Xue,Tingting Jiang,Jiangrong Shen,Tommi Kärkkäinen,Qi Xu,Fengyu Cong*

Main category: cs.AI

TL;DR: The paper proposes Prompt-Prototype (ProP), a new continual learning method that replaces key-value prompt mechanisms with task-specific prompts and prototypes to reduce inter-task interference and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based continual learning methods depend on key-value pairing to select prompts for different tasks, which can lead to interference between tasks and poor scalability as the number of tasks grows. A more stable, scalable mechanism for associating prompts with tasks is needed.

Method: The authors introduce Prompt-Prototype (ProP), where each task has a task-specific prompt and an associated prototype. The prompt is used to learn task-relevant features, and the prototype encodes representative features of the task’s inputs. During inference, predictions are made by binding each task’s prompt with its prototype instead of using key-value lookup. They also add a regularization term during prompt initialization to penalize large values, improving training stability.

Result: On several standard continual learning benchmarks, ProP achieves superior or competitive performance compared to existing prompt-based approaches, showing better knowledge retention and new-task learning without relying on key-value prompt selection.

Conclusion: The ProP framework removes the need for key-value pairs in prompt-based continual learning, reducing inter-task interference and improving scalability and stability. This offers a new direction for designing prompt mechanisms in continual learning systems.

Abstract: Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.

</details>


### [160] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: The paper proposes a hypergraph-based knowledge representation and an agentic reasoning system to better capture higher-order scientific relationships and generate mechanistic hypotheses without traditional supervision.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for scientific reasoning with LLMs either rely on unstructured retrieval-augmented generation, which lacks explicit structure, or on traditional knowledge graphs, which only model pairwise relations and miss higher-order interactions that drive emergent physical and biological behavior. There is a need for a representation that can encode multi-entity relationships at scale and support systematic, verifiable reasoning for hypothesis generation in complex domains like materials science and biocomposites.

Method: The authors construct a hypergraph-based knowledge representation from a corpus of ~1,100 manuscripts on biocomposite scaffolds. Entities and concepts from the texts are represented as nodes, and multi-entity co-occurrence relationships are encoded as hyperedges, preserving higher-order context. They analyze the resulting hypergraph’s topology (e.g., scale-free properties, hubs) and develop agentic tools that allow LLM-based agents to traverse this hypergraph using node-intersection constraints to connect semantically distant concepts. The system is then used to generate mechanistic hypotheses for novel composite materials by following higher-order relational paths through the hypergraph.

Result: The constructed global hypergraph contains 161,172 nodes and 320,201 hyperedges and exhibits a scale-free topology with a power law exponent of approximately 1.23, organized around highly connected conceptual hubs. This hypergraph avoids the combinatorial blow-up associated with converting higher-order relations into pairwise edges and maintains the original co-occurrence context of scientific formulations. Equipped with hypergraph traversal tools, the agentic system can bridge otherwise distant concepts, such as connecting cerium oxide and PCL scaffolds via chitosan intermediates, thereby generating plausible, mechanistically grounded hypotheses for new composite materials.

Conclusion: Hypergraph-based knowledge representations provide a more faithful and scalable way to encode higher-order scientific relationships than traditional pairwise knowledge graphs. When integrated with agentic LLM systems that can traverse these structures using intersection-based constraints, they enable “teacherless” but topologically guided reasoning. This hypergraph topology functions as a verifiable guardrail, allowing the system to uncover non-obvious, mechanistically meaningful connections and thus accelerate scientific discovery beyond what is achievable with conventional graph methods or unstructured retrieval alone.

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [161] [Precomputing Multi-Agent Path Replanning using Temporal Flexibility: A Case Study on the Dutch Railway Network](https://arxiv.org/abs/2601.04884)
*Issa Hanou,Eric Kemmeren,Devin Wild Thomas,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: The paper proposes FlexSIPP, an algorithm for efficiently replanning multi-agent schedules after a delay by exploiting other agents’ temporal flexibility and avoiding cascading disruptions, demonstrated on Dutch railway operations.


<details>
  <summary>Details</summary>
Motivation: In multi-agent systems (like trains sharing tracks), when one agent is delayed, naïvely replanning only that agent can be infeasible or highly inefficient, while replanning many agents can cause cascading changes and delays. There is a need for a systematic way to quickly compute a new safe and efficient plan that minimally disrupts other agents while handling the initial delay.

Method: The authors introduce the concept of agents’ temporal flexibility, defined as the maximum delay an agent can absorb without altering the order of agents or further delaying others. They design FlexSIPP, an algorithm that (1) precomputes the temporal flexibility of other agents and (2) precomputes all possible revised plans for the delayed agent, along with the necessary adjustments for other agents, for any single-agent delay scenario in the given environment. The approach builds on Safe Interval Path Planning (SIPP)-style reasoning but augments it with flexibility tracking to constrain and guide replanning.

Result: In a real-world case study of train replanning in the densely used Dutch railway network, FlexSIPP generates adjusted schedules that respect safety and ordering constraints while using the available temporal slack of other trains. Experiments show that the algorithm finds effective adjustment plans that are practically relevant to real operational perturbations and that the computations complete within operationally acceptable runtimes.

Conclusion: By explicitly modeling and exploiting temporal flexibility in multi-agent systems, FlexSIPP can efficiently replan after single-agent delays, avoiding infeasible solutions and large cascades of secondary delays. The method is both computationally practical and effective on a realistic, large-scale railway scheduling problem, suggesting it is suitable for real-world deployment in similar multi-agent scheduling domains.

Abstract: Executing a multi-agent plan can be challenging when an agent is delayed, because this typically creates conflicts with other agents. So, we need to quickly find a new safe plan. Replanning only the delayed agent often does not result in an efficient plan, and sometimes cannot even yield a feasible plan. On the other hand, replanning other agents may lead to a cascade of changes and delays. We show how to efficiently replan by tracking and using the temporal flexibility of other agents while avoiding cascading delays. This flexibility is the maximum delay an agent can take without changing the order of or further delaying more agents. Our algorithm, FlexSIPP, precomputes all possible plans for the delayed agent, also returning the changes for the other agents, for any single-agent delay within the given scenario. We demonstrate our method in a real-world case study of replanning trains in the densely-used Dutch railway network. Our experiments show that FlexSIPP provides effective solutions, relevant to real-world adjustments, and within a reasonable timeframe.

</details>


### [162] [Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking](https://arxiv.org/abs/2601.04887)
*Sofiene Lassoued,Laxmikant Shrikant Bahetic,Nathalie Weiß-Borkowskib,Stefan Lierc,Andreas Schwunga*

Main category: cs.AI

TL;DR: The paper proposes a hybrid Petri-net-plus-model-based-RL scheduler for flexible manufacturing systems with AGVs and tool sharing, achieving shorter makespans and much lower computation time, especially on large instances.


<details>
  <summary>Details</summary>
Motivation: Classical job shop scheduling methods struggle when realistic FMS features like AGVs and tool-sharing are added, since they greatly increase the state and action space and make real-time, adaptive scheduling difficult. There is a need for approaches that can both formally model these complex systems and learn efficient, adaptive control policies that scale to large problems while remaining computationally tractable.

Method: The authors model the flexible manufacturing system using Colored-Timed Petri Nets (CTPNs) to capture concurrency, resources, and timing, and to support dynamic action masking that prunes infeasible or low-value decisions. On top of this, they design an actor-critic model-based reinforcement learning framework that learns a scheduling policy. The model-based RL uses a learned model of dynamics to enable lookahead, with a specific strategy for positioning AGVs optimally. They implement a gym-compatible environment, an instance generator (including a large-scale benchmark inspired by Taillard), and perform an ablation study to isolate the effect of CTPNs, dynamic masking, lookahead, and other components.

Result: On small public FMS/job-shop benchmarks, the proposed method attains makespans comparable to established, traditional optimization/scheduling methods. On large-scale benchmark instances inspired by Taillard, it outperforms traditional methods in terms of makespan while reducing computation time by about an order of magnitude. The ablation study shows that each major component (CTPN modeling, dynamic action masking, model-based lookahead, and AGV positioning strategy) contributes positively to performance and scalability.

Conclusion: Integrating CTPN-based formal modeling with actor-critic model-based reinforcement learning yields an effective and scalable scheduler for complex flexible manufacturing systems with AGVs and tool sharing. The method can match classic algorithms on small problems and clearly surpass them on large ones, both in solution quality and runtime. The provided gym environment and instance generator support reproducibility and further research, and the modular ablation results suggest a promising blueprint for combining formal discrete-event models with learning-based control in industrial scheduling.

Abstract: Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.

</details>


### [163] [SmartSearch: Process Reward-Guided Query Refinement for Search Agents](https://arxiv.org/abs/2601.04888)
*Tongyu Wen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: The paper proposes SmartSearch, a framework that improves large language model-based search agents by supervising and refining the quality of intermediate search queries, leading to better retrieval, reasoning, and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLM-based search agents rely on retrieval during multi-step reasoning, but existing work mainly optimizes the reasoning strategy while neglecting the quality of intermediate search queries. Poorly formed queries cause inaccurate retrieval, which harms overall performance. The authors aim to explicitly model, evaluate, and improve these intermediate queries to unlock better capabilities of search agents.

Method: The authors introduce SmartSearch, which has two core mechanisms. (1) Process rewards: a Dual-Level Credit Assessment that provides fine-grained, step-level supervision on each intermediate search query’s quality, rather than only outcome-level rewards. (2) Query refinement: the system detects low-quality queries, refines them, and then regenerates subsequent reasoning and search steps conditioned on the improved queries. To help the agent internalize these skills, they design a three-stage curriculum learning framework (imitation → alignment → generalization) that progressively trains the agent to generate and refine queries guided by process rewards.

Result: Experiments show that SmartSearch consistently outperforms prior LLM-based search agents across benchmarks, yielding higher task accuracy. Quantitative analyses indicate improved search efficiency (fewer or more effective search steps) and higher intermediate query quality when using the proposed process rewards and query refinement mechanisms.

Conclusion: Focusing on the quality of intermediate search queries significantly enhances LLM-based search agents. By combining process-level rewards, targeted query refinement, and curriculum learning, SmartSearch achieves better retrieval, reasoning performance, and efficiency than existing methods, demonstrating the value of supervising and training search agents at the query level rather than only at final outcomes.

Abstract: Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.

</details>


### [164] [DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation](https://arxiv.org/abs/2601.04895)
*Renzhao Liang,Jingru Chen,Bo Jia,Bo Deng,Chenggang Xie,Yidong Wang,Ke Jin,Xin Wang,Linfeng Zhang,Cunxiang Wang*

Main category: cs.AI

TL;DR: The paper identifies and tackles “variant contamination” in LLM evaluation—cases where models have seen paraphrased or structurally altered versions of test items, making benchmarks look better than they should. It proposes DVD, a variance-based, single-sample detector that distinguishes truly novel items from contaminated ones by analyzing variability in model generations under temperature sampling, and shows it beats several baselines.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations are unreliable when models have indirectly seen test data in training, not as exact copies but as paraphrases or structural variants. Existing leakage detectors mainly focus on verbatim overlap or simple perplexity/sampling consistency checks and thus miss this more subtle “variant contamination.” This contamination inflates benchmark scores by rewarding memorization instead of genuine reasoning. The authors aim to formally define this problem, build realistic benchmarks for it, and develop a practical method to detect it automatically.

Method: They formalize variant contamination and propose DVD (Detection via Variance of generation Distribution), a single-sample detection method. DVD probes the local output distribution of an LLM using temperature sampling and analyzes how the model’s generative behavior changes across sampling runs. The key operational idea is to measure the variance in “synthetic difficulty” (e.g., probabilities or ranks) of low-probability tokens. Contaminated items cause the model to oscillate between a memory-adherence mode (recalling near-memorized text) and a perturbation-drift mode (responding to paraphrased input), which manifests as unusually high variance; uncontaminated items show smoother, lower variance. They also construct a dedicated variant-contamination benchmark on Omni-MATH and SuperGPQA by generating and filtering paraphrased or structurally equivalent variants, and simulate contamination by fine-tuning different LLMs (Qwen2.5 and Llama3.1) on these variants. DVD is evaluated against perplexity-based detectors, Min-k%++, edit-distance (CDD), and embedding-similarity approaches, under various hyperparameter settings.

Result: Across both Omni-MATH and SuperGPQA, and over different model families and sizes (Qwen2.5, Llama3.1), DVD consistently outperforms all tested baselines—perplexity-based, Min-k%++, edit-distance (CDD), and embedding-similarity methods—in detecting variant contamination. It demonstrates strong robustness to hyperparameter choices, indicating that its performance is not overly sensitive to tuning. Empirically, the observed high variance in synthetic difficulty for low-probability tokens reliably correlates with contamination, while uncontaminated items maintain comparatively smooth variance profiles.

Conclusion: Variant contamination—where models have seen paraphrased or structurally altered versions of evaluation items—is a distinct and serious threat to the validity of LLM benchmarks, beyond simple verbatim data leakage. The proposed DVD method shows that analyzing the variance of a model’s generation distribution under temperature sampling offers a principled and practical fingerprint for detecting such contamination from a single sample. With a new benchmark and strong empirical results, the paper argues that future LLM evaluation pipelines should incorporate variance-based detectors like DVD to better distinguish memorization effects from genuine reasoning ability.

Abstract: Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.

</details>


### [165] [From Stories to Cities to Games: A Qualitative Evaluation of Behaviour Planning](https://arxiv.org/abs/2601.04911)
*Mustafa F. Abdelwahed,Joan Espasa,Alice Toniolo,Ian P. Gent*

Main category: cs.AI

TL;DR: The paper introduces and applies a behaviour planning paradigm, which explicitly models diversity in planning to generate distinct plans, and demonstrates its usefulness via three real-world case studies: storytelling, urban planning, and game evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional diverse planning methods aim to generate multiple, distinct plans, which is useful in domains like risk management, stream data analysis, and malware detection, but they often lack an explicit, flexible model of diversity and support for different categories of planning problems. There is a need to systematically incorporate diversity criteria into the planning process and to show that such an approach is practical and valuable across varied real-world applications.

Method: The authors build on prior diverse planning work by defining a behaviour planning paradigm that embeds an explicit diversity model into the planning algorithm and supports multiple planning categories. They then apply this paradigm in three distinct case studies: generating diverse storylines for storytelling, producing varied yet plausible solutions in urban planning, and creating or evaluating different strategies or scenarios in games. Each case study illustrates how behaviour planning is instantiated, how diversity is characterized in that domain, and how the planning system produces multiple behaviourally distinct plans.

Result: Behaviour planning is successfully instantiated in three different domains. In each case study, the system generates multiple plans or behaviours that are meaningfully distinct according to the domain-specific diversity model, demonstrating that the approach is flexible and applicable to heterogeneous real-world problems such as narrative generation, city design, and game strategy evaluation.

Conclusion: The behaviour planning paradigm, which explicitly incorporates a diversity model and supports multiple planning categories, is both feasible and beneficial in practice. Its successful application to storytelling, urban planning, and game evaluation indicates that modelling and enforcing diversity within planning can enhance solution quality and usefulness across a range of domains, motivating further research and deployment of behaviour planning in other real-world settings.

Abstract: The primary objective of a diverse planning approach is to generate a set of plans that are distinct from one another. Such an approach is applied in a variety of real-world domains, including risk management, automated stream data analysis, and malware detection. More recently, a novel diverse planning paradigm, referred to as behaviour planning, has been proposed. This approach extends earlier methods by explicitly incorporating a diversity model into the planning process and supporting multiple planning categories. In this paper, we demonstrate the usefulness of behaviour planning in real-world settings by presenting three case studies. The first case study focuses on storytelling, the second addresses urban planning, and the third examines game evaluation.

</details>


### [166] [What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback](https://arxiv.org/abs/2601.04919)
*Yildiz Uzun,Andrea Gauthier,Mutlu Cukurova*

Main category: cs.AI

TL;DR: The paper investigates how students with different levels of self‑regulated learning (SRL) interact with a GenAI conversational assistant integrated into a learning analytics dashboard, and how well the assistant supports them.


<details>
  <summary>Details</summary>
Motivation: Students often struggle to interpret and act on feedback from learning analytics dashboards, especially those with low SRL skills. There is growing interest in using conversational GenAI to provide real-time, personalised scaffolding that could help students better understand and use analytics feedback, potentially reducing gaps between low and high SRL learners.

Method: Over a 10‑week semester, the authors integrated a conversational GenAI assistant into a learning analytics dashboard and collected authentic dialogue data between students and the assistant. They analysed the questions posed by students with different SRL levels, evaluated the relevance and quality of the assistant’s responses, and examined students’ perceptions of the assistant’s role in their learning.

Result: Distinct interaction patterns emerged: low‑SRL students mainly asked for clarification and reassurance, whereas high‑SRL students focused on technical questions and personalised strategy requests. The GenAI assistant generally provided clear and reliable explanations but showed limited capacity for deep personalisation, handling emotionally charged questions, and synthesising multiple data sources into tailored responses.

Conclusion: Conversational GenAI embedded in learning analytics dashboards can meaningfully scaffold low‑SRL students, helping them engage with feedback and potentially reduce performance gaps with higher‑SRL peers. However, to be fully effective, such systems must improve in adaptivity, context‑awareness, emotional sensitivity, and technical robustness to build student trust and deliver more personalised, integrated support.

Abstract: Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.

</details>


### [167] [Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition](https://arxiv.org/abs/2601.04920)
*Nils Einecke*

Main category: cs.AI

TL;DR: Case study: using ChatGPT as a coding and reasoning partner enabled a late-entry team to rapidly prototype a successful solution (2nd place) for ESA’s ELOPE lunar lander trajectory estimation competition, revealing concrete strengths, weaknesses, and best practices for AI-assisted scientific work.


<details>
  <summary>Details</summary>
Motivation: While LLMs are widely used for coding, their concrete impact on accelerating scientific discovery and competitive research workflows is poorly understood. The authors want to empirically examine how an LLM like ChatGPT can function as a rapid prototyping partner in a high-stakes, time-constrained scientific competition, and to identify both its practical benefits and limitations in that setting.

Method: The authors conduct a qualitative and performance-based case study in the ESA ELOPE competition, where participants must estimate lunar lander trajectories from event camera data. They systematically use ChatGPT during development to generate code, suggest algorithms, and discuss methodology, while documenting interactions, decisions, and issues that arise (e.g., structural overhauls, confusion, errors). They then analyze these interactions to derive patterns of effective and ineffective use, and synthesize them into recommended practices for integrating LLMs into the scientific workflow.

Result: Despite entering the competition late, the team achieved second place with a competitive score (0.01282), demonstrating that intensive use of ChatGPT for rapid prototyping can yield high-performing solutions under time pressure. ChatGPT provided valuable contributions: executable code, algorithmic insights, data handling logic, and specific methodological ideas (e.g., windowing by fixed event counts rather than fixed time). At the same time, the study surfaced recurrent issues such as unnecessary structural refactoring, confusion from branching discussions, critical coding errors, and loss of context in long technical conversations.

Conclusion: The case study shows that conversational LLMs can significantly accelerate development and support conceptual understanding in scientific research when used thoughtfully, but they also introduce characteristic failure modes that must be managed. The authors conclude that structured, consciously designed integration of LLMs into the scientific workflow—combined with human oversight and clear interaction protocols—can enhance rapid prototyping. They propose best practices for AI-assisted scientific work to maximize benefits (speed, ideas, flexibility) and mitigate drawbacks (instability, errors, context loss).

Abstract: Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.

</details>


### [168] [T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs](https://arxiv.org/abs/2601.04945)
*Chunyu Wei,Huaiyu Qin,Siyuan He,Yunhai Wang,Yueguo Chen*

Main category: cs.AI

TL;DR: The paper presents T-Retriever, a new tree-based retrieval framework for graph RAG that better preserves hierarchical structure and semantics, leading to improved answers on graph reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based Retrieval-Augmented Generation methods struggle with hierarchical information because they (1) enforce rigid, layer-based compression that distorts local graph structure, and (2) overemphasize topology while underutilizing semantic content. There is a need for a retrieval mechanism that can more naturally preserve both hierarchy and meaning when compressing and organizing graph information for LLMs.

Method: The authors reformulate attributed graph retrieval as a tree-based retrieval problem. They build a semantic and structure-guided encoding tree over the original graph. Two main techniques are proposed: (1) Adaptive Compression Encoding, which uses a global optimization strategy instead of fixed per-layer quotas to determine how to compress nodes while respecting natural hierarchical organization; and (2) Semantic-Structural Entropy (S^2-Entropy), an objective that balances structural cohesion and semantic consistency when partitioning the graph into hierarchical clusters, guiding the construction of the encoding tree used for retrieval.

Result: On multiple graph reasoning benchmarks, T-Retriever yields substantially better performance than existing state-of-the-art RAG methods. The improvements are reflected in more coherent, contextually appropriate, and accurate responses to complex, graph-based queries, indicating superior handling of hierarchical information during retrieval.

Conclusion: Transforming graph retrieval into a tree-based process with adaptive compression and a joint semantic-structural optimization objective enables more faithful preservation of hierarchical and semantic information. T-Retriever represents an advance in graph-based RAG, leading to more effective and contextually relevant reasoning over complex knowledge graphs.

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.

</details>


### [169] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: The paper proposes ConMax, a reinforcement learning framework that compresses chain-of-thought reasoning traces for Large Reasoning Models, reducing computation while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models rely on long Chain-of-Thought traces to exhibit advanced reasoning behaviors like self-verification and backtracking. However, these long traces cause overthinking, leading to redundant reasoning steps that significantly increase computational cost with little or no accuracy gain. Existing approaches—such as supervised fine-tuning on full traces or post-hoc compression methods—either harm logical coherence or require expensive sampling. There is a need for an automatic, principled way to shorten reasoning traces while maintaining their essential reasoning structure and predictive performance.

Method: The authors introduce ConMax (Confidence-Maximizing Compression), a reinforcement learning-based framework. Compression is framed as a policy learning problem where an agent learns to prune redundant parts of reasoning traces. The reward combines two signals, both computed via a frozen auxiliary LRM: (1) answer confidence, ensuring that the final prediction remains accurate (predictive fidelity), and (2) thinking confidence, ensuring that the remaining reasoning remains logically valid and informative (reasoning validity). The policy is trained to maximize this weighted reward, effectively learning which parts of the CoT can be safely removed without sacrificing quality.

Result: Across five reasoning datasets, ConMax significantly reduces the length of reasoning traces while maintaining high accuracy. Quantitatively, it achieves a 43% reduction in inference length compared to strong baseline methods, while incurring only a 0.7% drop in answer accuracy. These results indicate that ConMax can generate much more efficient reasoning traces than prior techniques without materially degrading task performance.

Conclusion: ConMax successfully addresses the overthinking problem in Large Reasoning Models by learning to compress Chain-of-Thought traces via reinforcement learning, guided by confidence-based rewards from a frozen LRM. It delivers a superior trade-off between computational efficiency and reasoning performance, making it a promising approach for producing high-quality, compact training data and enabling more efficient inference in reasoning-heavy applications.

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [170] [AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?](https://arxiv.org/abs/2601.04996)
*Henan Sun,Kaichi Yu,Yuyao Wang,Bowen Liu,Xunkai Li,Rong-Hua Li,Nuo Chen,Jia Li*

Main category: cs.AI

TL;DR: AlgBench is a new expert-built benchmark to rigorously test whether Large Reasoning Models truly understand and can apply classic algorithms, revealing large weaknesses especially on globally optimized methods like dynamic programming.


<details>
  <summary>Details</summary>
Motivation: Current reasoning benchmarks for large models (e.g., MATH500, LiveCodeBench) show progress but don’t specifically or systematically test algorithmic reasoning, especially across a diverse, well-defined set of algorithms. This makes it unclear whether models have actually mastered algorithmic thinking versus pattern-matching or exploiting benchmark quirks. The paper aims to fill this gap and provide a principled way to evaluate algorithmic reasoning.

Method: The authors construct AlgBench, a benchmark of 3,000+ original problems created by ACM-level algorithm experts. Problems are explicitly tied to 27 specific algorithms and organized by a taxonomy of structural and optimization properties (Euclidean vs. non-Euclidean, non-optimized vs. local/global/heuristic-optimized). They then evaluate several leading Large Reasoning Models on this benchmark, measure accuracies by algorithm category, and analyze model behavior, particularly how models choose and stick with or abandon algorithmic strategies during generation.

Result: Leading LRMs (Gemini-3-Pro, DeepSeek-v3.2-Speciale, GPT-o3) achieve high accuracy (up to ~92%) on non-optimized tasks but show a sharp performance drop (to ~49%) on globally optimized algorithms such as dynamic programming. Behavioral analysis reveals that models often start with a correct algorithmic plan but then switch away from it mid-solution—a phenomenon termed “strategic over-shifts”—especially when the correct continuation involves low-entropy, seemingly uninformative tokens.

Conclusion: Current problem-centric reinforcement learning and evaluation schemes are insufficient for robust algorithmic reasoning: models’ strong performance on some benchmarks masks serious weaknesses on globally optimized algorithms and stable strategy execution. The authors argue that an algorithm-centric training and evaluation paradigm, such as that instantiated by AlgBench, is necessary to develop LRMs that truly master algorithmic reasoning rather than superficially solving benchmark problems.

Abstract: Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.

</details>


### [171] [An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions](https://arxiv.org/abs/2601.05009)
*Avik Dutta,Harshit Nigam,Hosein Hasanbeig,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: The paper studies how LLMs break down on table QA when tables are semantically or structurally distorted, showing large accuracy drops and limited self-correction unless explicitly prompted.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for table question answering, but real-world tabular data is often messy, distorted, or non-canonical. It is unclear whether LLMs can recognize and correct such distortions on their own, as humans often do implicitly. Understanding these failure modes is necessary for reliable deployment in practical data analysis settings.

Method: The authors design semantic and structural distortions to otherwise standard tabular representations and evaluate multiple LLMs on table QA tasks under these conditions. They introduce a small, expert-curated dataset where successful QA explicitly requires an error-correction step before reasoning. They also experiment with providing models an explicit prior via system prompts to see whether this helps models realign distorted tables and improve performance.

Result: The study finds that LLMs generally do not inherently detect or repair subtle distortions in table representations. Performance on TQA drops substantially under distortion, with even state-of-the-art models like GPT-5.2 losing at least 22% accuracy. Adding explicit priors in the system prompt leads to partial but inconsistent improvements: models can fix some distortions but remain far from robust.

Conclusion: Current LLMs systematically misinterpret distorted tables and lack autonomous mechanisms to realign or clean tabular inputs, relying heavily on explicit instructions or pre-processing. The work highlights a key gap between LLM behavior and human-like data handling, and calls for research on when and how models should independently decide to detect, correct, and reason over imperfect tabular data.

Abstract: We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.

</details>


### [172] [OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05027)
*Yi Jiang,Sendong Zhao,Jianbo Li,Bairui Hu,Yanrui Du,Haochun Wang,Bing Qin*

Main category: cs.AI

TL;DR: OptiSet is a set-centric RAG framework that selects and ranks compact, complementary evidence sets rather than top-k independent passages, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard RAG pipelines usually pick top-k passages by independent relevance scores, which causes redundancy and ignores complementarities among passages. This limits performance, especially on tasks where the joint utility of a set of passages matters more than each passage alone. The paper aims to better exploit combinatorial gains and reduce redundancy in retrieved evidence.

Method: The authors propose OptiSet, which treats evidence as sets instead of independent items. It follows an Expand-then-Refine paradigm: (1) Expand: generate multiple query perspectives to create a diverse candidate passage pool; (2) Refine: re-select from this pool to form a compact, high-utility evidence set. They introduce a self-synthesis strategy that uses the LLM generator’s own output changes under different evidence sets to derive preference labels, identifying complementary vs. redundant passages without requiring strong external LLM supervision. Then they use a set-list wise training strategy to jointly optimize set selection and set-level ranking toward compact, high-gain evidence sets.

Result: Experiments on complex combinatorial tasks show that OptiSet yields better generation quality than standard top-k RAG baselines, while using fewer passages. The model learns to favor evidence sets with higher marginal utility and lower redundancy, leading to both accuracy gains and efficiency improvements.

Conclusion: Considering passages as sets and optimizing for set-level utility, rather than independent relevance, enables more effective and efficient RAG. OptiSet’s Expand-then-Refine framework, self-synthesis labeling, and set-list wise training jointly produce compact, complementary evidence sets that improve performance on complex tasks. The approach is practical and made available via public code.

Abstract: Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an "Expand-then-Refine" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.

</details>


### [173] [How to Set the Batch Size for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05034)
*Yunhua Zhou,Junhao Huang,Shuhao Xin,Yechen Zhang,Runyu Peng,Qiping Guo,Xipeng Qiu*

Main category: cs.AI

TL;DR: The paper revises the critical batch size theory for modern Warmup-Stable-Decay (WSD) learning rate schedules and proposes a dynamic batch size scheduler that improves large-scale pre-training efficiency and final model quality.


<details>
  <summary>Details</summary>
Motivation: Existing critical batch size theory, originally developed under different learning rate assumptions, no longer accurately reflects the behavior of large-scale pre-training when using the now-standard Warmup-Stable-Decay learning rate schedule. Practitioners lack a principled understanding and formula that relate training data consumption and number of steps under WSD, leading to potentially suboptimal choices of batch size and training schedule.

Method: The authors analytically derive a new E(S) relationship—linking total training tokens (data consumption) E and training steps S—specifically under the Warmup-Stable-Decay learning rate schedule. From this theoretical framework, they identify two key quantities: (1) B_min, the minimum batch size required to reach a target loss, and (2) B_opt, the batch size that minimizes total tokens used for a given target loss. Using these insights, they design a dynamic Batch Size Scheduler that adjusts batch size during training to stay close to the optimal regime implied by the theory. They then empirically validate the theory and scheduler through extensive large-scale pre-training experiments.

Result: The revised E(S) formula accurately predicts the behavior of large-scale pre-training under WSD learning rate scheduling. The identified B_min and B_opt thresholds are borne out empirically: training below B_min fails to reach the target loss efficiently, while using B_opt leads to maximal data efficiency. The proposed dynamic Batch Size Scheduler, informed by the new theory, yields notable improvements in both training efficiency (reduced tokens or steps for the same target loss) and final model quality compared to conventional fixed-batch or theory-unaware settings.

Conclusion: The traditional critical batch size framework is insufficient for modern Warmup-Stable-Decay training regimes. By deriving a WSD-specific E(S) relationship and exposing the roles of B_min and B_opt, the paper establishes a new theoretical basis for choosing and scheduling batch size in large-scale pre-training. The accompanying dynamic Batch Size Scheduler, grounded in this analysis, delivers practical benefits in efficiency and performance, indicating that pre-training strategies should be redesigned around WSD-aware batch size theory.

Abstract: The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.

</details>


### [174] [How to Set the Learning Rate for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05049)
*Yunhua Zhou,Shuhao Xing,Junhao Huang,Xipeng Qiu,Qipeng Guo*

Main category: cs.AI

TL;DR: The paper studies how to pick optimal learning rates for large-scale pre-training cheaply, comparing two paradigms: predicting LR from small runs (Fitting) vs transferring hyperparameters across scales (Transfer). It introduces a scaling law that simplifies LR search and shows limits of existing μTransfer methods, especially for Mixture-of-Experts (MoE) models.


<details>
  <summary>Details</summary>
Motivation: Choosing the learning rate for very large models is hard and expensive because it requires many large training runs. Practitioners want to know if they can run small, cheap experiments and reliably extrapolate the optimal learning rate and other hyperparameters to large-scale pre-training, particularly for modern architectures like MoE.

Method: The authors formalize two paradigms: (1) Fitting: they introduce a scaling law for a search factor related to learning rate and use predictive modeling to cut LR search complexity from cubic in model/scale parameters to linear in them; (2) Transfer: they generalize μTransfer rules to Mixture-of-Experts architectures, incorporating depth, weight decay, and token horizon. They then empirically evaluate both paradigms on large-scale pre-training tasks and analyze training dynamics in terms of stability and feature learning.

Result: They find that the Fitting paradigm with their new scaling law substantially reduces the computational cost of LR search while maintaining good performance. In contrast, when scaled up, μTransfer-style module-wise hyperparameter transfer becomes unreliable and underperforms in large pre-training regimes, especially for MoE models. Their experiments reveal that naive transfer rules fail to preserve stable training dynamics and effective feature learning at scale.

Conclusion: Hyperparameter optimization for large-scale pre-training cannot rely solely on existing μTransfer rules, particularly for complex architectures like MoE. A fitting-based approach grounded in a scaling law for LR search is more robust and computationally efficient. The paper yields both practical guidelines for industrial pre-training setups and theoretical insight into why module-wise parameter tuning breaks down at large scale, emphasizing the importance of modeling training stability and feature learning when designing LR scaling strategies.

Abstract: Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.

</details>


### [175] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: The paper tests how persuasive GPT-4o is at both increasing and decreasing belief in conspiracy theories, finding it is similarly powerful in both directions but that simple prompt and interaction changes can reduce harm.


<details>
  <summary>Details</summary>
Motivation: LLMs are known to be persuasive, but we do not know if they are inherently more effective at spreading truth than falsehood, especially around conspiracy theories. This is important because if LLMs can easily promote misbeliefs, they pose a serious risk for misinformation and public trust. The authors aim to quantify this risk and test whether existing safety guardrails meaningfully prevent harmful persuasion.

Method: Across three pre-registered experiments with 2,724 American participants, each participant chose a conspiracy theory they felt uncertain about and then engaged in a chat with GPT-4o. The model was randomly instructed either to argue against the conspiracy (debunking) or to argue for it (bunking). The authors compared a jailbroken GPT-4o variant (safety guardrails removed) with standard GPT-4o, and measured changes in conspiracy belief, evaluations of the AI, and trust in AI. They also tested two mitigation strategies: (1) a follow-up corrective conversation to undo induced beliefs and (2) prompting GPT-4o explicitly to use only accurate information.

Result: The jailbroken GPT-4o increased conspiracy belief as strongly as it decreased it, showing symmetric persuasive power. The bunking AI was liked more and increased users’ trust in AI more than the debunking AI. Standard GPT-4o, despite built-in safety guardrails, produced very similar effects to the jailbroken variant, indicating current guardrails did little to prevent promotion of conspiracy beliefs. However, a corrective conversation was able to reverse the induced conspiracy beliefs, and a prompt instructing GPT-4o to rely only on accurate information substantially reduced its ability to increase conspiracy beliefs.

Conclusion: LLMs like GPT-4o are powerful persuaders for both accurate and inaccurate content, making them potentially dangerous tools for spreading conspiracy theories. Existing guardrails are insufficient to reliably prevent harmful persuasion, but targeted interventions—such as explicit accuracy-focused prompting and corrective follow-up conversations—can meaningfully mitigate the risk. This highlights the need for better alignment and deployment practices to ensure LLMs favor truth over falsehood in persuasive contexts.

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [176] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: Transforms narrative scientific review tables into FAIR, machine-actionable knowledge graphs (ORKG) for queryable, neurosymbolic AI in materials science, arguing for symbolic backbones with LLMs as grounded interfaces.


<details>
  <summary>Details</summary>
Motivation: Scientific review insights in materials science are mostly locked in narrative text and static PDF tables, which are hard to reuse, integrate, and query by humans and machines. There is a need to make review knowledge FAIR (Findable, Accessible, Interoperable, Reusable) and machine-actionable to support advanced querying, comparison, and AI-driven analysis, especially in fast-evolving domains like atomic layer deposition and etching (ALD/E).

Method: Use a case study in ALD/E to convert traditional review tables into structured, FAIR comparisons within the Open Research Knowledge Graph (ORKG). Represent review content as machine-actionable graph data, then demonstrate and contrast two querying paradigms: (1) symbolic querying over the ORKG’s structured representations and (2) large language model (LLM)-based querying over the same domain. Analyze their relative strengths and limitations for knowledge retrieval and reasoning.

Result: The authors successfully publish review tables as structured, machine-actionable comparisons in ORKG, enabling queryable representations of ALD/E review knowledge. Their experiments show that symbolic querying over the curated ORKG layer offers more reliable and interpretable results, while LLM-based querying provides flexible, natural-language access but is less reliable as a standalone knowledge source. The combination illustrates a practical neurosymbolic approach.

Conclusion: Curated, symbolic knowledge graphs such as ORKG should serve as the backbone for reliable neurosymbolic AI in materials science. While LLMs are valuable as user-friendly, natural-language interfaces, they should remain grounded in and constrained by a high-quality symbolic layer rather than being treated as independent sources of truth. Publishing review tables as FAIR, machine-actionable knowledge significantly enhances the reusability and reliability of scientific reviews for both humans and AI systems.

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [177] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: The paper proposes ROSE, a reinforcement learning framework that improves large language models’ mathematical reasoning by using semantically diverse MCTS-style explorations and length-aware credit assignment, achieving better accuracy and efficiency than prior RLVR methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL with verifiable rewards (RLVR) improves LLM reasoning, and MCTS-style methods allow finer-grained credit assignment, but they still explore reasoning spaces narrowly and generate inefficient, overly long chains of thought. The authors want to increase exploration diversity to avoid local optima and improve the efficiency of learned reasoning, especially for math problems, without sacrificing or even improving performance.

Method: The authors introduce ROSE, a Monte Carlo Tree Search–based RLVR method that modifies search and credit assignment. For exploration, ROSE adds (1) a semantic-entropy-based branching strategy: it analyzes already sampled reasoning rollouts, computes semantic uncertainty/divergence, and branches at segments with high semantic diversity to create new reasoning paths; and (2) an ε-exploration mechanism that occasionally restarts rollouts from the root to avoid overly local search. For efficiency, they propose a length-aware segment-level advantage estimator that gives higher rewards to concise, correct reasoning and penalizes unnecessarily long reasoning trajectories. They evaluate ROSE on multiple mathematical reasoning benchmarks using Qwen and Llama models.

Result: Across several math reasoning benchmarks with Qwen and Llama backbones, ROSE outperforms prior RLVR baselines (including GRPO and MCTS-based variants) in both effectiveness (higher accuracy/solution quality) and efficiency (shorter or more concise reasoning chains). The abstract claims consistent, extensive empirical validation of the benefits of semantic-diverse exploration and length-aware credit assignment.

Conclusion: ROSE demonstrates that explicitly promoting semantic diversity in MCTS-style exploration, together with a length-aware advantage estimator, yields more effective and efficient reasoning for LLMs under the RLVR framework. The method alleviates limited exploration and inefficient, verbose reasoning typical of earlier approaches and establishes a practical improvement path for training better reasoning models, with released code enabling reproduction and extension.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [178] [Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models](https://arxiv.org/abs/2601.05076)
*Arghyadeep Das,Sai Sreenivas Chintha,Rishiraj Girmal,Kinjal Pandey,Sharvi Endait*

Main category: cs.AI

TL;DR: The paper proposes methods to make large reasoning models perform chain-of-thought reasoning without leaking personal data, using a new privacy-focused benchmark to evaluate interventions like prompting and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in large reasoning models improves performance and interpretability but often exposes sensitive personal information in intermediate steps, even when final outputs are sanitized. There is a need for practical, deployable ways to reduce such privacy leakage during reasoning itself rather than relying on post-hoc redaction.

Method: The authors construct PII-CoT-Bench, a supervised dataset with privacy-aware chain-of-thought annotations, and design a category-balanced evaluation benchmark that covers both realistic and adversarial PII leakage scenarios. They then study and compare two kinds of interventions—prompt-based controls and fine-tuning—across models of varying capability to see how each approach affects PII leakage and task utility.

Result: Experiments show a capability-dependent pattern: stronger, state-of-the-art models can significantly reduce PII leakage primarily via prompt-based controls, while weaker models need fine-tuning to achieve comparable reductions. In both cases, PII exposure is substantially reduced with only minimal drops in downstream task performance.

Conclusion: Private chain-of-thought reasoning is feasible with limited utility loss. Prompting is particularly effective for more capable models, while fine-tuning is needed for weaker ones. The paper offers practical recommendations and a benchmark (PII-CoT-Bench) for building and evaluating privacy-preserving reasoning systems.

Abstract: Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.

</details>


### [179] [Arabic Prompts with English Tools: A Benchmark](https://arxiv.org/abs/2601.05101)
*Konstantin Kubrak,Ahmed El-Moselhy,Ammar Alsulami,Remaz Altuwaim,Hassan Ismail Fawaz,Faisal Alsaby*

Main category: cs.AI

TL;DR: The paper introduces the first benchmark to evaluate Arabic-language tool-calling and agentic capabilities of large language models, showing notable performance drops when users interact in Arabic.


<details>
  <summary>Details</summary>
Motivation: Although Arabic-native LLMs are rapidly improving, there is a lack of evaluation benchmarks specifically targeting tool-calling and agentic behavior in Arabic. Existing benchmarks are mostly English-centric, and it is unclear how well models handle tool-use when prompted in Arabic, particularly given that many are trained predominantly on English data. This creates a critical blind spot in assessing how reliable and fair these models are for Arabic-speaking users.

Method: The authors design and propose a dedicated benchmark and standardized evaluation framework to measure tool-calling and agentic capabilities of LLMs when interacting in Arabic. The benchmark focuses on assessing functional accuracy and robustness in Arabic agentic workflows, testing models with Arabic prompts and varying whether tool descriptions are in Arabic or English.

Result: Empirical evaluation using the proposed benchmark shows a substantial performance gap: when users interact with the models in Arabic, tool-calling accuracy decreases by about 5–10% on average, and this degradation occurs regardless of whether tool descriptions are presented in Arabic or English.

Conclusion: The study demonstrates that current LLMs exhibit significantly weaker tool-calling performance for Arabic interactions, highlighting a linguistic inequity in agentic AI systems. The authors conclude that their benchmark can serve as a foundation to diagnose these issues and drive the development of more robust, reliable, and fair Arabic-capable AI agents.

Abstract: Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.

</details>


### [180] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: FusionRoute is a token-level framework that coordinates multiple LLM experts using a lightweight router that both chooses an expert and adds its own corrective logits, achieving better performance and efficiency than standard collaboration or fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs must be very large to perform well across diverse domains, making them expensive to train and deploy. Smaller domain-specialized models are cheaper but do not generalize well outside their domains. Existing token-level multi-LLM collaboration methods depend only on fixed expert outputs and cannot always implement the optimal decoding strategy. There is a need for a collaboration mechanism that attains strong multi-domain performance without incurring the full cost of a huge monolithic model and without being constrained by expert-only routing limitations.

Method: The authors propose FusionRoute, a token-level multi-LLM collaboration framework. A lightweight router network operates at each decoding step, (i) selecting the most appropriate expert model among several LLMs, and (ii) generating its own complementary logit vector, which is added to the selected expert's logits to form the final next-token distribution. They provide a theoretical analysis demonstrating that expert-only routing (without a trainable generator) cannot generally realize the optimal decoding policy unless strong assumptions hold, whereas adding a complementary generator expands the reachable policy class and can recover optimal value functions under mild conditions. They then implement FusionRoute on top of Llama-3 and Gemma-2 models and evaluate it on multiple benchmarks.

Result: FusionRoute achieves superior empirical performance compared to several baseline approaches: sequence-level and token-level collaboration schemes, model merging techniques, and direct fine-tuning of a general model. On benchmarks covering mathematical reasoning, code generation, and instruction following, FusionRoute not only surpasses these baselines but also attains performance competitive with specialized domain experts in their respective areas, while maintaining efficiency benefits from using smaller or specialized models.

Conclusion: By jointly performing expert selection and logit-level correction via a lightweight router, FusionRoute overcomes theoretical limitations of pure expert-only token routing and yields a richer class of decoding policies. This design enables near-expert performance across multiple domains without resorting to a single massive model, offering a practical and theoretically grounded approach to multi-LLM collaboration that outperforms existing collaboration, merging, and fine-tuning strategies on diverse benchmarks.

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [181] [Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction](https://arxiv.org/abs/2601.05107)
*Muzhao Tian,Zisu Huang,Xiaohua Wang,Jingwen Xu,Zhengkang Guo,Qi Qian,Yuanzhe Shen,Kaitao Song,Jiakang Yuan,Changze Lv,Xiaoqing Zheng*

Main category: cs.AI

TL;DR: The paper introduces SteeM, a framework that makes an LLM agent’s reliance on long-term memory explicitly controllable, avoiding both over-anchoring to history and ignoring it.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents used in long-term interactions need cumulative memory for personalization and stylistic consistency. Existing systems usually either use all relevant past information or none, leading to two problems: (1) Memory Anchoring, where the agent is overly constrained by previous interactions and cannot adapt or innovate; and (2) under-utilization, where ignoring memory loses important context and personalization benefits. There is a need for a principled, user-controllable way to regulate how much past memory should influence current behavior.

Method: The authors model memory reliance as an explicit, adjustable dimension of agent behavior. They define a behavioral metric of memory dependence that quantifies how much past interactions affect current outputs. Based on this, they propose Steerable Memory Agent (SteeM), a framework that exposes a control knob over memory reliance. Users can dynamically adjust the level from a fresh-start mode (minimizing the effect of past memory to encourage novelty and flexibility) to a high-fidelity mode (maximizing adherence to past interaction history). This is implemented at the prompting/memory integration level and compared against standard prompting and rigid memory masking schemes.

Result: Across multiple experimental scenarios, SteeM yields better performance than conventional prompting and fixed masking approaches. The results show that SteeM can provide fine-grained, predictable control over how much past memory shapes current outputs, improving both personalization and adaptability in human–agent interaction tasks.

Conclusion: Memory influence in LLM agents can be treated as a controllable behavioral dimension rather than a fixed design choice. By quantifying memory dependence and introducing the SteeM framework, the paper demonstrates that users can dynamically steer an agent between innovation (fresh-start) and strict consistency (high-fidelity to history). This yields more nuanced, effective, and user-aligned personalization than existing all-or-nothing memory strategies.

Abstract: As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \textbf{Stee}rable \textbf{M}emory Agent, \texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.

</details>


### [182] [GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts](https://arxiv.org/abs/2601.05110)
*Wenhao Zeng,Xuteng Zhang,Yuling Shi,Chao Hu,Yuting Chen,Beijun Shen,Xiaodong Gu*

Main category: cs.AI

TL;DR: The paper proposes GlimpRouter, a training-free framework that speeds up and improves multi-step reasoning by deciding, from just the first token of each reasoning step, whether to use a small or large model.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models are powerful but slow and expensive because they generate long chains of thought. Collaborative inference with small and large models can reduce cost, but existing routing methods are inefficient, relying on full token probabilities or post-hoc verification. The authors aim to find a low-overhead signal to decide, at each reasoning step, whether a large model is actually needed.

Method: The authors hypothesize that the difficulty of a reasoning step is reflected in the entropy of the first token the model would generate for that step, inspired by the "Aha Moment" phenomenon in LRMs. They introduce GlimpRouter, where a lightweight model generates only the first token of each reasoning step. The entropy of this token is computed, and if it exceeds a preset threshold, the system routes that entire reasoning step to a larger model; otherwise, the small model continues. The framework is training-free and uses entropy as the routing signal for step-wise collaboration.

Result: Across multiple reasoning benchmarks, GlimpRouter reduces inference latency while maintaining or even improving accuracy. On AIME25, it achieves a 10.7% accuracy improvement and a 25.9% reduction in latency compared to running the large model alone, indicating that selective step-wise routing based on first-token entropy is effective.

Conclusion: The work concludes that the difficulty of a reasoning step can be reliably predicted from a "glimpse"—the entropy of its first token—and that this enables an efficient, training-free collaboration between small and large reasoning models. This glimpse-based routing notably cuts latency and can even improve accuracy, suggesting a simple and practical mechanism for allocating computation in multi-step reasoning systems.

Abstract: Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.

</details>


### [183] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: The paper finds that different LLM judges do not agree with each other on what constitutes quality, even though each is internally consistent, revealing that LLM evaluators behave as distinct "measurement devices" rather than interchangeable judges.


<details>
  <summary>Details</summary>
Motivation: To test the widespread assumption that LLM-as-judge systems offer scalable and consistent evaluation, and to determine whether different LLM judges are interchangeable when used for benchmarking and quality assessment.

Method: The authors run 3,240 evaluations using 9 different LLM judges on 120 unique video-pack items, each evaluated independently three times. They quantify inter-judge agreement with Krippendorff's alpha, analyze disagreement patterns, and train classifiers to predict which judge produced which evaluation based on rubric scores and additional disposition features. They further characterize judges along axes such as harshness/leniency, dimension emphasis, intra-judge reliability (ICC), and behavior with evidence using NLI-based metrics and a "shotgun index."

Result: Inter-judge agreement is extremely low (Krippendorff's α ≈ 0.042), sometimes even worse than random (α < 0) on some dimensions, showing that judges do not share a common notion of quality. However, each judge is internally consistent and exhibits stable, structured patterns of evaluation: classifiers can identify the judge with 77.1% accuracy using rubric scores alone and up to 89.9% with disposition features; within the same model family, discrimination is even sharper (e.g., 99.6% accuracy distinguishing GPT-4.1 vs GPT-5.2). These patterns amount to judge-specific "fingerprints."

Conclusion: LLM judges are not interchangeable tools that measure a shared construct of quality; instead, each embodies a distinct, stable evaluative disposition or implicit theory of quality. Their scores represent different measurement regimes, so aggregating or averaging them produces an artificial verdict that does not correspond to any single judge's evaluation philosophy, challenging current practices in LLM-based evaluation and benchmarking.

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [184] [Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144)
*Shuliang Liu,Xingyu Li,Hongyi Liu,Yibo Yan,Bingchen Duan,Qi Zheng,Dong Fang,Lingfeng Su,Xuming Hu*

Main category: cs.AI

TL;DR: ReasonMark is a watermarking framework for reasoning-focused LLMs that preserves logical coherence by separating internal reasoning from the final answer and adaptively embedding watermarks based on semantic importance.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for LLMs either damage the quality and coherence of long, logical reasoning traces (token-bias methods) or require costly semantic models and add latency (semantic-aware methods). For reasoning LLMs used in critical applications, we need watermarks that are robust and detectable but do not distort the model’s reasoning or greatly increase computation time.

Method: The authors split generation into two phases: a Thinking Phase, where the model reasons freely without watermarking, and an Answering Phase, where the final response is watermarked. From the full reasoning trace, they compute a Criticality Score for tokens that captures which tokens are semantically pivotal. These are distilled into a Principal Semantic Vector (PSV), which represents the core semantics. During answer generation, they compare each candidate token to the PSV and adjust watermark strength adaptively based on semantic alignment, embedding the watermark more strongly where it is less likely to harm logic and semantics.

Result: In experiments, ReasonMark outperforms previous watermarking methods: it lowers perplexity by 0.35 (better fluency), increases translation BLEU by 0.164, and improves math problem accuracy by 0.67 points compared to baselines. It also yields a 0.34% higher watermark detection AUC, shows stronger robustness to attacks, and introduces only negligible latency overhead.

Conclusion: ReasonMark demonstrates that separating reasoning from answering and using a semantic-criticality-driven watermarking scheme can preserve reasoning quality while maintaining robust, detectable watermarks. This makes it more practical to deploy traceable and trustworthy reasoning LLMs in real-world settings where both reliability and provenance are important.

Abstract: Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.

</details>


### [185] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: The paper studies how training large language models on their own synthetic outputs in iterative loops affects bias, and proposes a sampling-based mitigation.


<details>
  <summary>Details</summary>
Motivation: As LLMs become more capable and expensive to train, reusing synthetic data generated by existing models is attractive. However, this creates "self-consuming" retraining loops where models are repeatedly trained on their own outputs, potentially amplifying errors and biases over time. Real-world deployed systems further complicate this with performative effects—user behavior changes in response to the model, which then affects the data collected for future training. The authors are motivated to understand, in a controlled way, how such self-consuming, performative feedback loops shape different forms of bias and model performance, and how to mitigate harmful bias evolution.

Method: They formalize the notion of a Self-Consuming Performative Loop (SCPL), where an LLM generates synthetic data, is retrained or incrementally fine-tuned on that data, and the resulting model is then used to generate the next round of data under a controlled performative feedback mechanism mimicking user preferences. They instantiate two loop settings: (1) a standard retraining loop in which models are periodically retrained from scratch (or a base) on accumulated data, and (2) an incremental fine-tuning loop where the current model is continually adapted with new synthetic data—a setting they identify as underexplored. Using three real-world tasks, they simulate user preference dynamics and track the evolution of two bias notions—preference bias and disparate bias—across loop iterations. Finally, they design and test a reward-based rejection sampling strategy that filters synthetic data based on a reward signal to reduce bias accumulation in these loops.

Result: Across the three tasks, the experiments show that SCPL dynamics tend to increase preference bias—i.e., the model increasingly favors certain preferences or groups aligned with the feedback mechanism—while at the same time reducing disparate bias—i.e., narrowing performance gaps across groups on some metrics. This indicates a nontrivial trade-off in how performative synthetic data loops shape fairness. The incremental fine-tuning loop exhibits its own characteristic bias evolution, illustrating that this common industrial practice requires separate study. Their reward-based rejection sampling approach successfully attenuates the observed preference bias while maintaining or improving overall system reliability, demonstrating its potential as a practical mitigation tool for self-improving LLM systems.

Conclusion: The paper concludes that self-consuming performative loops—where LLMs are iteratively trained on their own outputs under feedback-driven data generation—can systematically reshape bias in ways that are not obvious from static training analyses. Specifically, such loops tend to amplify preference bias even as they may reduce disparate bias, highlighting the need to monitor and manage fairness dynamically. The authors argue that incremental fine-tuning, widely used in practice, is particularly underexamined and can have distinct bias trajectories. Their proposed reward-based rejection sampling strategy offers a promising direction to control bias in synthetic data-driven self-improvement pipelines, moving toward more trustworthy and fair long-term deployment of LLMs.

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


### [186] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent is an LLM-based agent for Simulink that uses a compact Python representation and a new RL algorithm (ReGRPO) to perform modeling and simulation more efficiently and accurately than existing methods, even surpassing GPT-4o on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: While LLMs work well for text and code, their use in graphical, block-diagram-based engineering tools like Simulink is limited. Existing Simulink models use verbose XML that is token-inefficient for LLMs, and long-horizon modeling tasks suffer from sparse rewards, making reinforcement learning difficult. There is a need for a privacy-preserving, on-premise AI system that can reliably assist with industrial model-driven engineering workflows.

Method: The authors design SimuAgent, an LLM-powered agent specialized for Simulink. They introduce a concise dictionary-style Python representation of Simulink models to replace XML, reducing token usage and enabling in-process simulation. The agent follows a lightweight plan-execute architecture and is trained with a two-stage curriculum: first on low-level tool skills, then on high-level design reasoning. To address sparse rewards in long-horizon tasks, they propose Reflection-GRPO (ReGRPO), an extension of Group Relative Policy Optimization that incorporates self-reflection traces as dense intermediate feedback signals. They also employ abstract-reconstruct data augmentation to improve robustness and generalization. Training is done on a new benchmark, SimuBench, with 5300 multi-domain modeling tasks, using a Qwen2.5-7B model in an on-premise setup.

Result: On the SimuBench benchmark, a Qwen2.5-7B model fine-tuned as SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines. It also outperforms GPT-4o evaluated with few-shot prompting on the same tasks. Ablation studies show that the two-stage training curriculum and abstract-reconstruct data augmentation each contribute to improved generalization and robustness.

Conclusion: SimuAgent effectively adapts LLM capabilities to graphical modeling environments like Simulink by using a compact model representation, a plan-execute architecture, and an enhanced RL algorithm (ReGRPO). It provides a practical, on-premise, and privacy-preserving tool for AI-assisted engineering design, bridging the gap between LLMs and industrial model-driven engineering workflows and outperforming strong baselines including GPT-4o on a dedicated benchmark.

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [187] [Stock Market Price Prediction using Neural Prophet with Deep Neural Network](https://arxiv.org/abs/2601.05202)
*Navin Chhibber,Suneel Khemka,Navneet Kumar Tyagi,Rohit Tewari,Bireswar Banerjee,Piyush Ranjan*

Main category: cs.AI

TL;DR: The paper proposes an NP-DNN model that combines NeuralProphet with an MLP-based deep neural network, using Z-score normalization and missing value imputation, to predict stock prices with a reported accuracy of 99.21%, outperforming other methods including one using a fused large language model.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical time-series methods struggle to predict stock prices accurately and particularly to capture the range or probabilistic behavior of future prices. There is a need for models that can better learn complex, nonlinear stock market dynamics and use cleaned, normalized data to improve forecasting performance.

Method: The authors preprocess raw stock price data with Z-score normalization to standardize scales and apply missing value imputation to complete historical series. They then use a Multi-Layer Perceptron within a NeuralProphet framework (NP-DNN) to learn nonlinear relationships and hidden patterns in the time series, generating feature representations for prediction. The approach is compared against other methods, including models using a fused large language model.

Result: The NP-DNN model reportedly achieves 99.21% prediction accuracy on stock price forecasting tasks, surpassing competing approaches, including one based on a fused large language model. Details of evaluation metrics beyond the stated accuracy are not given in the abstract.

Conclusion: Integrating NeuralProphet with an MLP-based deep neural network, along with careful preprocessing via Z-score normalization and missing data imputation, can substantially improve stock market price prediction performance over traditional statistical and baseline machine learning approaches. The authors conclude that NP-DNN is a highly accurate and effective method for stock price forecasting.

Abstract: Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.

</details>


### [188] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: The paper proposes a real-time, low-overhead method to detect hallucinations in LLM tool calls by using the model’s internal representations in the same forward pass, achieving high accuracy in identifying wrong tools, bad parameters, and tool bypass.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents often hallucinate during tool use: they pick inappropriate tools, generate malformed parameters, or bypass tools by simulating their behavior. These errors lead to unreliable outputs and can circumvent security and audit mechanisms in production systems. Existing hallucination detection methods are costly, frequently needing multiple forward passes or external validators; thus, there is a need for an efficient, real-time detection mechanism to make tool-using agents trustworthy in practice.

Method: The authors inspect and leverage internal representations of LLMs during the same forward pass used for tool-call generation. They build a framework that, without additional passes or external systems, monitors these internal signals to classify whether a tool call is likely to be hallucinated. The method targets three main failure types: incorrect tool choice, malformed parameters, and tool bypass behavior where the model simulates the tool instead of calling it.

Result: On reasoning tasks in multiple domains, the proposed framework detects tool-calling hallucinations with high accuracy, up to 86.4%, while incurring minimal computational overhead and preserving real-time inference. It performs particularly well at detecting parameter-level hallucinations and inappropriate tool selection, which are common and hazardous failure modes in agentic LLM deployments.

Conclusion: By exploiting internal LLM representations in a single forward pass, the framework offers a practical, computationally efficient way to detect and handle hallucinations in tool-using agents. This approach strengthens the reliability, safety, and auditability of LLM-based systems, making them more suitable for production environments where correct tool usage and adherence to security controls are critical.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>


### [189] [MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents](https://arxiv.org/abs/2601.05215)
*Tamil Sudaravan Mohan Doss,Michael Xu,Sudha Rao,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.AI

TL;DR: The paper introduces MineNPC-Task, a user-authored benchmark and evaluation harness to test memory-aware, mixed-initiative LLM agents in open-world Minecraft, derived from expert co-play rather than synthetic prompts, with machine-checkable validators and detailed logging of agent behavior.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM agents in games and embodied environments often rely on synthetic prompts, lack realistic task structures, permit out-of-world shortcuts, and provide limited insight into how agents use planning and memory. The authors want a more realistic, fine-grained, and reproducible way to evaluate memory-aware agents in an open-world setting like Minecraft, including how they interact with humans and handle subtasks, dependencies, and failures.

Method: They design MineNPC-Task, a benchmark where tasks are collected from expert human co-play sessions and then converted into parametric templates with explicit preconditions and dependency graphs. Each task is paired with machine-checkable validators that enforce a bounded-knowledge policy to prevent agents from using information unavailable in the game world. The harness instruments the full agent loop, logging plan previews, clarifying questions, memory read/write operations, precondition checks, repair attempts, and final outcomes based on in-world evidence. They instantiate this evaluation framework with GPT-4o agents and run a user study with 8 experienced Minecraft players.

Result: Using GPT-4o within the MineNPC-Task framework, they evaluate 216 subtasks and identify recurring failure modes in code execution, inventory and tool management, referencing objects, and navigation. They also observe that mixed-initiative behavior (e.g., clarifying questions) and lightweight memory mechanisms can help the agent recover from some of these breakdowns. User survey responses from the 8 players report positive impressions of interaction quality and interface usability but also indicate that memory persistence across tasks is currently insufficient.

Conclusion: MineNPC-Task provides a transparent, realistic, and reproducible benchmark and harness for evaluating memory-aware, mixed-initiative LLM agents in an open-world Minecraft environment. The initial GPT-4o experiments reveal both common breakdowns and promising recovery patterns, and user feedback underscores the importance of stronger long-term memory. The authors release the full task suite, validators, logs, and harness to catalyze future research on embodied agents with robust planning and memory capabilities.

Abstract: We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.

</details>


### [190] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: The paper learns latent action world models directly from diverse, in-the-wild videos, showing that continuous constrained latent actions better capture real-world action variability than discrete VQ actions and can be used as a universal interface for planning without explicit action labels.


<details>
  <summary>Details</summary>
Motivation: Real-world agents need to predict the consequences of their actions, typically done with world models that require explicit, labeled action inputs. Obtaining such action labels at scale for diverse real-world videos is difficult and limiting. Existing latent action models largely focus on narrow domains like simple simulations or games, which do not reflect the complexity and variability of in-the-wild video data. The authors are motivated to scale latent action world models to real-world settings, learning an action space directly from raw videos without action annotations, while dealing with challenges like environmental noise and heterogeneous embodiments.

Method: The authors propose a latent action world modeling framework trained on in-the-wild videos without explicit action labels. They design architectural choices and constraints for latent actions so that they satisfy desired properties of real actions (e.g., being controllable and predictive of future frames). Instead of using vector quantization for discrete action tokens, they introduce continuous but constrained latent action representations that better accommodate the complexity of natural video dynamics. They train a world model conditioned on these latent actions, and also train a controller that maps known, labeled actions (when available) into the learned latent action space so that the latent actions can serve as a universal interface for downstream planning tasks. They analyze spatial localization of latent actions relative to the camera in settings without consistent embodiment, and conduct empirical evaluations comparing to action-conditioned baselines.

Result: The experiments show that continuous constrained latent actions outperform vector-quantized discrete representations in capturing the complexity of real-world actions in in-the-wild videos. The learned latent actions can reflect meaningful agent-driven environmental changes, such as humans entering a room, and these action-induced changes can transfer across different videos. In environments lacking a common embodiment, the learned actions primarily localize in camera-relative space, indicating that the model organizes actions based on visual spatial context. The trained controller successfully maps known actions into the latent space, enabling the use of the latent actions as a universal interface. When used for planning, the world model with latent actions achieves performance comparable to traditional action-conditioned baselines on planning tasks.

Conclusion: Continuous, constrained latent action spaces are effective for learning world models directly from diverse, in-the-wild videos without explicit action labels. They can encode rich and transferable action-induced changes despite environmental noise and varying embodiments. Although the learned actions tend to be camera-relative when embodiment is not shared, a controller can align known, labeled actions with the latent space, making the latent actions a practical universal interface for planning. These results demonstrate a viable path toward scaling latent action models and world models to more realistic, real-world video data.

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>
