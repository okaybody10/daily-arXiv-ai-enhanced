<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: They introduce RIMRULE, a method that distills symbolic rules from LLM tool-use failures and injects them at inference time to improve domain-specific tool use without changing model weights.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently fail to use domain-specific, poorly documented, or idiosyncratic tools and APIs. Manually adapting models or writing tool-specific instructions is costly and not easily transferable. The authors seek an automatic, interpretable, and reusable way to adapt LLMs to such tools.

Method: RIMRULE is a neuro-symbolic adaptation framework where (1) the LLM attempts tool-using tasks; (2) failures generate traces; (3) the LLM proposes candidate rules explaining/correcting these failures; (4) a Minimum Description Length (MDL) objective selects a compact, general set of rules; (5) rules are stored both as natural language and structured symbolic forms for retrieval; and (6) during inference, relevant rules are dynamically injected into prompts to guide tool use, without updating model weights.

Result: On multiple tool-use benchmarks, RIMRULE improves accuracy for both previously seen and new tools, surpassing prompting-based adaptation methods and working in addition to finetuning gains. Rules distilled using one LLM can also benefit other models, including long reasoning LLMs, demonstrating cross-model portability.

Conclusion: Dynamic injection of distilled, symbolic rules offers an effective, interpretable way to adapt LLMs to domain-specific tools. This approach enhances tool-use performance without weight updates, is complementary to finetuning, and yields portable symbolic knowledge that can be reused across different LLM architectures.

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: The paper introduces Pat-DEVAL, an LLM-based, multi-dimensional evaluation framework specifically designed to assess the legal and technical quality of patent description bodies, achieving high correlation with expert judgments.


<details>
  <summary>Details</summary>
Motivation: Existing automatic patent drafting systems rely on large language models, but current evaluation methods cannot adequately judge long-form structural coherence or compliance with patent law requirements like enablement and written description. There is a need for an evaluation framework that reflects real-world statutory and professional patent standards.

Method: The authors design Pat-DEVAL, an LLM-as-a-judge framework tailored to patent descriptions. They introduce Chain-of-Legal-Thought (CoLT), which forces the model to follow a sequential reasoning process grounded in patent-law-specific criteria. They evaluate Pat-DEVAL on a curated dataset, Pap2Pat-EvalGold, and compare its scoring with human patent experts and with baseline metrics/LLM evaluators using correlation analysis.

Result: Pat-DEVAL achieves a Pearson correlation of 0.69 with expert judgments overall, outperforming baseline metrics and existing LLM evaluators. For the dimension of Legal-Professional Compliance, it reaches an even higher correlation of 0.73, indicating particularly strong alignment with expert legal assessments when statutory constraints are explicitly modeled.

Conclusion: Incorporating explicit patent-law constraints into an LLM-as-a-judge framework leads to more accurate and legally meaningful evaluation of automatically drafted patent descriptions. Pat-DEVAL sets a new standard for assessing both technical and legal quality, providing a solid basis for deploying automated patent drafting systems in practice.

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [3] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: This paper systematically analyzes what architectural choices matter for Emotion Recognition in Conversation (ERC) and links recognition performance to linguistic patterns in the IEMOCAP dataset.


<details>
  <summary>Details</summary>
Motivation: Despite high ERC accuracy, it is unclear which architecture components actually drive performance, and there is little connection between recognition models and deeper linguistic/semantic analysis. The authors want to fill this gap with controlled experiments and corpus-based linguistic study.

Method: They run a rigorous ablation study with 10 random seeds on IEMOCAP, comparing models with/without conversational context, hierarchical sentence representations, and external affective lexicons, using strictly causal context. They also perform a linguistic analysis of 5,286 discourse-marker occurrences to examine how emotion correlates with discourse marker position, especially in the left periphery of utterances.

Result: (1) Performance gains come primarily from recent conversational context, with 90% of gains captured by 10–30 prior turns. (2) Hierarchical sentence representations help only when no context is provided; their benefit vanishes once context is included. (3) External affective lexicons like SenticNet add no value beyond pre-trained encoders. Their simple causal-context architectures reach 82.69% (4-way) and 67.07% (6-way) weighted F1, surpassing previous text-only and even bidirectional-context systems. Linguistically, they find a strong association between emotion and discourse marker positioning, with sad utterances showing notably fewer left-periphery markers.

Conclusion: Conversational context is the dominant factor in ERC, largely subsuming the need for complex intra-utterance modeling and external affective lexicons; simple causal models with enough recent context can outperform more complex systems. Linguistically, sadness is less explicitly signaled via discourse management markers and thus relies more heavily on conversational history for correct recognition, helping explain why sadness gains the most from contextual modeling.

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [4] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: They propose a lightweight temporal knowledge graph reasoning framework that uses large language models as teachers to distill both structural and temporal reasoning into compact student models, achieving better accuracy–efficiency trade-offs than baselines.


<details>
  <summary>Details</summary>
Motivation: Temporal knowledge graph reasoning is important for real-time intelligent decision-making, but current models are large, computationally expensive, and hard to deploy on resource-constrained platforms. Existing compression and distillation methods mainly target static knowledge graphs and fail to capture temporal dependencies, leading to performance degradation on TKGs. There is a need for a dedicated distillation framework that preserves temporal reasoning while reducing model size and cost.

Method: They design a distillation framework specialized for temporal knowledge graphs, where large language models serve as teacher models. The framework transfers both structural (graph/topology) and temporal (time-dependent) reasoning capabilities from the teacher to lightweight student models. It integrates large-scale public knowledge encoded in LLMs with task-specific temporal information from TKGs, training students to model temporal dynamics effectively under a compact architecture.

Result: On several public temporal knowledge graph benchmarks, the distilled student models outperform strong baseline methods, including existing TKG models and possibly generic compression/distillation approaches. The method yields superior trade-offs between reasoning accuracy and computational cost, demonstrating improved efficiency and deployability without sacrificing performance.

Conclusion: A tailored distillation framework that uses LLMs as temporal reasoning teachers can effectively compress temporal knowledge graph reasoning models while preserving and even improving performance. This approach enables accurate, efficient, and practical TKG reasoning on low-power and distributed platforms, highlighting the value of combining LLM prior knowledge with task-specific temporal signals for model compression in dynamic graph settings.

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [5] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: The paper proposes an evidence-based medicine–aware, graph-based RAG framework that encodes PICO and evidence hierarchy into retrieval and reranking, and validates it in sports rehabilitation with strong automatic and expert evaluation results, plus releasing a large knowledge graph and QA benchmark.


<details>
  <summary>Details</summary>
Motivation: Current medical RAG systems aim at better accuracy but usually ignore key evidence-based medicine principles. Specifically, they do not ensure that retrieved evidence matches the PICO structure of clinical questions, and they treat all studies similarly without respecting evidence hierarchy (e.g., systematic reviews vs. case reports). This limits clinical trustworthiness and alignment with EBM practice. The authors want a principled way to integrate EBM into RAG so that LLM answers are grounded in the right type and quality of evidence, and to fill the lack of RAG resources in sports rehabilitation.

Method: They design a generalizable, graph-based RAG pipeline that explicitly encodes EBM concepts. First, they construct a large knowledge graph in which nodes and relations incorporate PICO elements (Population, Intervention, Comparison, Outcome, and Time/PICOT) from clinical literature. Retrieval over this graph is PICO-aware so that evidence structurally matches the question’s PICO. Second, they introduce a Bayesian-inspired reranking algorithm that adjusts document ranking scores according to evidence grade (e.g., higher-level evidence is favored) without manually setting fixed weights. The framework is instantiated and evaluated in the sports rehabilitation domain, including building a reusable QA benchmark.

Result: They built a sports rehabilitation knowledge graph with 357,844 nodes and 371,226 edges and a benchmark of 1,637 QA pairs. On this benchmark, their system achieved strong automatic metrics: 0.830 nugget coverage (evidence completeness), 0.819 answer faithfulness (grounding to sources), 0.882 semantic similarity (closeness to reference answers), and 0.788 PICOT match accuracy (PICO alignment). In expert evaluation using a 5-point Likert scale, five clinicians rated the system between 4.66 and 4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment, indicating high perceived quality and safety of the generated answers.

Conclusion: Integrating EBM principles—through PICO-structured knowledge graphs and evidence-grade–aware reranking—into graph-based RAG significantly improves retrieval relevance, evidence quality, and answer faithfulness in medical question answering. The approach is domain-transferable to other clinical areas beyond sports rehabilitation. Additionally, the released knowledge graph and QA benchmark help mitigate the scarcity of RAG resources in sports rehabilitation and can support future research in EBM-aligned medical RAG.

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [6] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: The paper presents JP-TL-Bench, a lightweight benchmark for fine-grained evaluation of Japanese-English translation systems using LLM-based, reference-free pairwise comparisons and a stable scoring scheme.


<details>
  <summary>Details</summary>
Motivation: Existing Japanese-English MT evaluation struggles with distinguishing between multiple high-quality translations where nuances of politeness, implicature, ellipsis, and register matter. Traditional acceptability-focused or reference-based metrics are not well-suited for this fine-grained, preference-level judgment, and consistent, affordable human evaluation is costly. A dedicated, stable benchmark is needed to support iterative system development in this language pair.

Method: The authors design JP-TL-Bench as a benchmark where candidate translation systems are evaluated via reference-free, pairwise comparisons conducted by an LLM acting as a judge. Each candidate system’s outputs are compared against a fixed, versioned anchor set of system outputs. For each sentence, the LLM decides which of the two translations is better along dimensions relevant to Japanese-English naturalness. These pairwise preferences are then modeled using a Bradley-Terry model to estimate each system’s latent strength. From the fitted log-strengths, they compute win rates and derive a normalized 0–10 "LT" score using a logistic transform, ensuring comparable scores across evaluations.

Result: The benchmark provides structurally stable scores for MT systems because each candidate is always evaluated against the same frozen anchor set under a fixed judging and aggregation pipeline. As a consequence, win rates and LT scores are comparable across time and between systems, enabling fine-grained tracking of progress. The reference-free LLM-judging protocol keeps evaluation relatively low-cost while still sensitive to subtle qualitative differences between high-quality translations.

Conclusion: JP-TL-Bench offers a practical, stable, and fine-grained evaluation framework for Japanese-English MT focused on relative quality among strong systems, rather than simple acceptability. By leveraging reference-free, pairwise LLM judgments aggregated with a Bradley-Terry model into normalized LT scores, it supports iterative model development while maintaining structural comparability of results over time.

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [7] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: The paper evaluates how well large language models generate counterfactual explanations across multiple languages and how useful these are for data augmentation.


<details>
  <summary>Details</summary>
Motivation: While LLMs are strong at English counterfactual generation and show multilingual abilities, it is unknown how effective they are at generating counterfactuals in many languages and how these counterfactuals differ in quality, editing patterns, and downstream utility. Understanding this is important for interpretability and robust training in multilingual settings, especially for low-resource languages.

Method: The authors systematically study multilingual counterfactual generation in six languages by: (1) automatically evaluating counterfactuals generated directly in target languages versus those obtained by translating from English; (2) analyzing edit patterns in high-resource European languages to identify common cross-lingual perturbation strategies; (3) discovering and categorizing recurrent error types in generated counterfactuals across languages; and (4) comparing the downstream impact of multilingual versus cross-lingual counterfactual data augmentation on model performance and robustness, with attention to low-resource settings.

Result: Translation-based (English→other language) counterfactuals generally have higher validity than directly generated ones but require more extensive edits and still lag behind original English counterfactuals in quality. Edit patterns for high-resource European languages are highly similar, indicating shared strategic principles for counterfactual perturbations. Four major, recurring error categories are observed in generated counterfactuals across all studied languages. In downstream experiments, multilingual counterfactual data augmentation brings larger performance gains than cross-lingual augmentation, most notably for low-resource languages. However, imperfections in the generated counterfactuals constrain overall improvements in performance and robustness.

Conclusion: LLMs can produce multilingual counterfactuals, but these are less reliable and efficient than English counterfactuals, and translation-based approaches, while more valid, are more intrusive and still imperfect. Cross-lingual counterfactual perturbations appear to follow common principles, yet systematic, language-agnostic errors remain. Multilingual CDA is more beneficial than cross-lingual CDA, particularly for low-resource languages, but the quality limitations of current LLM-generated counterfactuals cap their utility for improving model performance and robustness.

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [8] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval is a benchmark to test LLM agents’ function-calling under realistic, noisy, and complex API conditions, revealing that current strong LLMs still struggle, especially with irrelevant information and maintaining user intent.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLM function-calling rely on simplified, idealized API settings that ignore real-world complications such as noisy outputs, detailed specifications, constraints, and runtime issues. This gap makes it hard to know how well LLM agents will actually perform when integrated with practical, complex API ecosystems. The authors aim to build a benchmark that realistically reflects these complexities so that LLM agent capabilities can be meaningfully measured and improved.

Method: The authors design WildAGTEval, a benchmark that models realistic API complexity along two axes: (1) API specification complexity, including detailed documentation and constraints; and (2) API execution complexity, covering runtime and noise-related challenges. They construct an API system with 60 distinct complexity scenarios that can be combined into about 32K possible test configurations. On top of this, they create user–agent interaction tasks so that different LLM agents can be evaluated across these scenarios. They then systematically test several advanced LLMs on this benchmark and analyze both quantitative performance and qualitative behavior.

Result: Most of the complexity scenarios prove challenging even for advanced LLMs. In particular, scenarios involving high levels of irrelevant information significantly degrade performance, causing a 27.3% drop for strong models. Qualitative findings further show that LLM agents sometimes misrepresent or distort the user’s intent, apparently to assert that they have finished the task, which undermines the reliability and usefulness of these agents.

Conclusion: WildAGTEval provides a more realistic and fine-grained way to evaluate LLM agents’ function-calling abilities under real-world API conditions. The benchmark exposes substantial weaknesses in current models, particularly in handling irrelevant information and faithfully preserving user intent. This suggests that improving robustness to API complexity and alignment with user goals is an important direction for future LLM agent development and evaluation.

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [9] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: The paper studies how quantizing large language models affects the quality and faithfulness of their self-explanations (both natural language explanations and counterfactuals), finding modest but non-negligible degradation that depends on model size, explanation type, and quantization method.


<details>
  <summary>Details</summary>
Motivation: Quantization is a key technique for speeding up and compressing LLMs, making them cheaper to deploy, but its impact on the reliability of self-explanations—used for transparency and accountability in high-stakes settings—has not been studied. Since self-explanations require the model to reason about its own decisions, they may be especially sensitive to quantization-induced changes, so practitioners need to know whether compressed models still produce trustworthy explanations.

Method: The authors take LLMs and apply three standard quantization techniques at multiple bit widths, then generate two kinds of self-explanations: natural language explanations (NLEs) and counterfactual examples. They evaluate (1) task performance, (2) explanation quality, and (3) explanation faithfulness using automatic metrics and a human user study that rates coherence and trustworthiness, comparing quantized models against their full-precision counterparts and analyzing differences across model sizes and quantization schemes.

Result: Across settings, quantization causes moderate drops in self-explanation quality (up to 4.4%) and faithfulness (up to 2.38%), and a user study shows human-perceived coherence and trustworthiness of explanations can decrease by as much as 8.5%. Larger models are not strongly more robust in terms of explanation quality but do preserve faithfulness better than smaller models. No single quantization method uniformly optimizes task accuracy, explanation quality, and faithfulness, and natural language explanations prove more sensitive to quantization than counterfactual explanations.

Conclusion: Quantization modestly but consistently degrades the quality and faithfulness of LLM self-explanations, and these effects depend on model size, explanation type, and quantization method. However, the magnitude of degradation is generally small enough that quantization remains a viable compression strategy. The authors advise practitioners to validate self-explanation behavior—especially NLEs—for specific use cases rather than assuming quantized models inherit the explanatory properties of their full-precision counterparts.

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [10] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: They present DepFlow, a depression-conditioned text-to-speech framework that disentangles depression-related acoustics from content, then uses it to generate camouflaged depression data and improve robustness of depression detection models.


<details>
  <summary>Details</summary>
Motivation: Existing speech-based depression datasets strongly couple negative linguistic sentiment with depression labels. Models thus learn semantic shortcuts (e.g., negative words = depressed), leading to poor robustness when people exhibit camouflaged depression—maintaining positive or neutral language while still being clinically depressed. There is a need for methods and data that decouple acoustics of depression from linguistic sentiment to train more reliable detectors and to simulate realistic clinical scenarios where labeled data is scarce and ethically constrained.

Method: They propose DepFlow, a three-stage, depression-conditioned TTS system: (1) A Depression Acoustic Encoder is trained with adversarial objectives to produce depression embeddings that are invariant to speaker identity and linguistic content yet remain predictive of depression (disentanglement + discriminability). (2) A flow-matching text-to-speech model uses FiLM modulation to inject these depression embeddings into the synthesis process, enabling control over depression severity while preserving speaker and text content. (3) A prototype-based severity mapping mechanism organizes embedding space into interpretable prototypes along a depression continuum, allowing smooth, graded control over depressive severity. Using this system, they generate a Camouflage Depression-oriented Augmentation (CDoA) dataset by pairing depressed-sounding acoustics with positive/neutral texts from a sentiment-stratified text bank, creating deliberate acoustic–semantic mismatches.

Result: The Depression Acoustic Encoder achieves an ROC-AUC of 0.693 for depression discrimination while being disentangled from speaker and content. The generated CDoA data, when used to augment training, improves macro-F1 scores by 9%, 12%, and 5% across three different depression detection architectures, surpassing conventional data augmentation techniques. The approach successfully generates realistic acoustic–semantic mismatches that are underrepresented in natural datasets and enhances model robustness under camouflaged depression scenarios.

Conclusion: DepFlow enables controllable, depression-conditioned speech synthesis that disentangles depression-related acoustics from linguistic content and speaker identity. By generating a CDoA dataset with intentional acoustic–semantic mismatches, the method significantly improves robustness of depression detection models compared with standard augmentations. Beyond detection performance, DepFlow serves as a flexible platform for simulating conversational scenarios and conducting evaluation where access to diverse, ethically obtainable clinical speech is limited.

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [11] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: The paper proposes a new robust uncertainty (RU) quantification method and a trap-question benchmark with fake-name facts to better detect hallucinations in large language models, achieving 0.1–0.2 ROCAUC gains over baselines on four models.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in large language models reduce reliability and trust, especially in real-world, high‑stakes or critical-thinking scenarios. Existing uncertainty quantification methods work mainly for standard QA settings and struggle with non-canonical or adversarial questions. There is a need for an evaluation setting and a method that stress-test LLMs on multi-fact generation and can better detect when the model is likely hallucinating.

Method: The authors design an uncertainty quantification scenario focused on generation involving multiple facts. They construct a curated set of trap questions that embed fake names, intended to elicit hallucinations or wrong factual associations. On top of this setting, they introduce a novel robust uncertainty (RU) method, which outputs a more reliable uncertainty score for LLM generations than existing baselines. They then compare RU against several baseline uncertainty methods across four different LLMs, using ROCAUC as the main metric for hallucination detection performance.

Result: The trap-question dataset is shown to be effective in inducing and revealing hallucinations. Across four LLMs, the proposed RU method outperforms baseline uncertainty methods, with improvements of approximately 0.1–0.2 in ROCAUC compared to the best existing baseline for hallucination detection.

Conclusion: A specially designed trap-question setting with fake names, together with the proposed robust uncertainty (RU) method, provides a more effective way to quantify and detect hallucinations in LLM outputs. The approach improves hallucination detection performance across multiple models and offers a promising direction for enhancing the reliability and trustworthiness of LLM-generated content in complex, multi-fact scenarios.

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [12] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: The paper studies how different kinds of bilingual data in pretraining corpora affect multilingual LLM abilities, finding that parallel data is crucial for translation but not for cross-lingual understanding and reasoning.


<details>
  <summary>Details</summary>
Motivation: Although multilingual LLMs are mostly trained on monolingual web text, they still show strong cross-lingual capabilities. It is commonly assumed that the small amount of bilingual data in the corpus explains this, but its exact role and which types of bilingual data matter are not well understood. The paper aims to disentangle how bilingual data contributes to translation versus other cross-lingual tasks.

Method: The authors pretrain language models from scratch under controlled corpus conditions. They compare models trained on a standard web-scale corpus to models trained on a version where all multilingual (bilingual) documents are removed. They then categorize the bilingual documents in the original corpus into parallel, code-switching, and miscellaneous based on semantic alignment across languages. They conduct granular ablations by reintroducing only parallel or only code-switching data into the monolingual-only corpus and evaluate the resulting models on translation, cross-lingual QA, and general reasoning tasks, analyzing training curves and performance changes.

Result: Bilingual documents comprise only about 2% of the web corpus. Removing them leads to a large degradation in translation quality (56% BLEU drop), but performance on cross-lingual QA and general reasoning remains almost unchanged, with similar training dynamics. Within the bilingual subset, around 14% is parallel data and 72% code-switching. Reintroducing parallel data nearly recovers translation performance (to 91% of the unfiltered baseline), whereas reintroducing only code-switching data has minimal effect on translation. Other cross-lingual tasks are largely unaffected by either type of bilingual data.

Conclusion: Systematic token-level alignments present in parallel data are critical for learning good translation capabilities in multilingual LLMs, while cross-lingual understanding and reasoning can emerge largely from monolingual training alone. Different cross-lingual abilities thus rely on qualitatively different types of signal in the pretraining corpus, with translation specifically requiring structured bilingual supervision, whereas broader cross-lingual generalization does not depend heavily on explicit bilingual data.

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [13] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: The paper proposes BEPA, a BERT-based model trained with a JEPA objective to fix collapsed [CLS] embeddings and create a language-agnostic representation space, improving multilingual performance.


<details>
  <summary>Details</summary>
Motivation: Standard BERT-style models can suffer from a collapsed [CLS] embedding space, limiting their ability to represent diverse languages in a unified way. The authors are motivated by the success of Joint Embedding Predictive Architectures (JEPA) in self-supervised learning and aim to transfer these benefits to text modeling, particularly for multilingual tasks.

Method: They introduce BERT-JEPA (BEPA), which augments BERT-style models with an additional JEPA training objective. This objective encourages the [CLS] representation to be predictive in a joint-embedding space, discouraging collapse and pushing it toward a language-agnostic representation. The overall architecture remains BERT-like but with a modified self-supervised training scheme combining masked language modeling (assumed) with JEPA-style prediction in embedding space.

Result: The resulting BEPA model yields improved performance across multiple multilingual benchmarks compared to baseline BERT-style models without the JEPA objective, indicating better cross-lingual generalization and more informative [CLS] embeddings.

Conclusion: Incorporating a JEPA objective into BERT (forming BEPA) can restructure the [CLS] embedding space into a more robust, language-agnostic representation, addressing collapse issues and delivering consistent gains on multilingual tasks. This suggests JEPA is a promising direction for enhancing self-supervised language models, especially in multilingual scenarios.

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [14] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R is a retrieval-free, reinforcement-learning-based framework that improves image geolocalization by turning GPS coordinates into interpretable, hierarchical geographic reasoning paths and optimizing with distance-based rewards.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language geolocalization methods often depend on synthetic reasoning labels or retrieval from external image databases, which hampers interpretability, scalability, and generalization. The paper aims to design a system that can reason about location in a structured and explainable way using only ground-truth coordinates, without extra supervision or retrieval components.

Method: The authors propose Geo-R, which introduces a Chain of Region: a rule-based, hierarchical mapping from GPS coordinates to geographic entities such as country, province, and city. This creates structured reasoning targets without model-generated text or synthetic labels. On top of this, they apply a lightweight reinforcement learning scheme where rewards are computed via Haversine distance between predicted and true coordinates, providing spatially meaningful feedback. The model is trained to follow the chain-of-region reasoning and refine coordinate predictions based on these RL signals, all in a retrieval-free setup.

Result: Across multiple geolocalization benchmarks, Geo-R achieves higher localization accuracy and better generalization than baselines, while also producing more interpretable reasoning paths. It demonstrates that structured geographic reasoning combined with coordinate-based RL can outperform methods relying on retrieval or synthetic reasoning annotations.

Conclusion: Geo-R establishes a new, retrieval-free paradigm for image geolocalization that couples interpretable, hierarchical geographic reasoning with direct spatial supervision via reinforcement learning. By leveraging only ground-truth coordinates and deterministic region mappings, it improves accuracy, generalization, and transparency, and it offers an easily reproducible framework whose model and code will be publicly released.

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [15] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: They build a much larger Luxembourgish NER dataset by auto-labeling Wikipedia/Wikidata and then using LLMs to filter and verify the labels.


<details>
  <summary>Details</summary>
Motivation: Under-represented languages like Luxembourgish lack sizeable, consistent NER datasets, and manual annotation is expensive and difficult because of scarce resources and linguistic particularities. This limits progress in NLP for these languages.

Method: They construct a pipeline that: (1) uses Luxembourgish Wikipedia pages; (2) extracts internal links and maps them to Wikidata items; (3) infers entity types from Wikidata properties to create weak NER labels; (4) recognizes that Wikipedia links can be noisy and not all are reliable; and (5) applies and compares several large language models to evaluate, filter, and retain only high-quality labelled sentences, effectively cleaning and validating the automatically generated annotations.

Result: The pipeline produces judgeWEL, an automatically labelled and LLM-verified Luxembourgish NER corpus that is about five times larger than the existing Luxembourgish NER dataset, with more balanced and broader coverage of entity categories.

Conclusion: Using Wikipedia/Wikidata for weak supervision combined with LLM-based verification is an effective way to build large, higher-quality NER datasets for low-resource languages like Luxembourgish, significantly expanding available resources and supporting future multilingual and low-resource NER research.

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [16] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: The paper introduces a more expressive temporal knowledge representation called HTKGHs, builds a new dataset (htkgh-polecat) from POLECAT, and benchmarks LLMs for geopolitical relation prediction on this structure.


<details>
  <summary>Details</summary>
Motivation: Existing temporal knowledge graphs and hyper-relational extensions cannot efficiently express complex geopolitical events that involve more than two main entities, limiting forecasting performance and realism.

Method: 1) Formally define Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs) that generalize HTKGs and remain backward compatible. 2) Use this framework to construct a new dataset, htkgh-polecat, from the POLECAT global event database, capturing complex temporal geopolitical facts. 3) Evaluate state-of-the-art LLMs on a relation prediction (forecasting) task over this dataset, analyzing their performance and adaptability to the richer structure.

Result: They obtain a new formalism (HTKGHs) and a corresponding real-world dataset (htkgh-polecat), along with empirical benchmarks showing how well popular LLMs handle complex multi-entity temporal relation prediction in geopolitics, yielding diagnostic insights rather than just raw scores.

Conclusion: HTKGHs provide a more expressive yet compatible framework for representing complex temporal geopolitical facts, htkgh-polecat operationalizes this idea on real data, and LLM benchmarking reveals strengths and weaknesses of current models in complex TKG-based forecasting tasks.

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [17] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: The paper compares three lightweight Transformer models (DistilBERT, MiniLM, ALBERT) for enterprise NLP across multiple domains, focusing on both accuracy and efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Enterprises need lightweight NLP models that can be efficiently deployed at scale across different text domains (sentiment, news, toxicity) while balancing accuracy with constraints on latency, memory, and computation. Existing work often targets single tasks or does not systematically quantify these trade-offs under realistic deployment constraints.

Method: The authors fine-tune DistilBERT, MiniLM, and ALBERT on three benchmark datasets: IMDB for sentiment analysis, AG News for topic classification, and the Measuring Hate Speech corpus for toxicity detection. Under fixed, enterprise-oriented fine-tuning constraints (i.e., no extensive hyperparameter search), they evaluate models with accuracy, precision, recall, and F1, and measure efficiency via model size, inference time, throughput, and memory usage.

Result: ALBERT attains the highest task-specific accuracy on multiple datasets, MiniLM delivers the best inference speed and throughput, and DistilBERT offers the most stable accuracy across tasks with reasonably strong efficiency. No model is uniformly superior across all axes of performance.

Conclusion: There is an inherent trade-off between accuracy and efficiency across lightweight Transformer models in enterprise NLP settings. The paper recommends MiniLM for latency-critical applications, DistilBERT when a balanced accuracy-efficiency profile is desired, and ALBERT when maximizing accuracy in resource-constrained deployments is the priority.

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [18] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: The paper proposes a unified theoretical framework connecting formal semantic structures with the empirical behavior of large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To use LLMs as an empirical testbed for long-standing philosophical and linguistic theories of meaning, and to reconcile social constructivist "language game" accounts with a mathematical view of semantics.

Method: The author formalizes two key notions—lexical fields (Lexfelder) and linguistic fields (Lingofelder)—as interacting structures in a continuous semantic space, and then conceptually analyzes how features of transformer architectures (distributed representations, attention, geometric embedding regularities) correspond to these structures.

Result: They show that LLMs’ ability to encode rich semantic regularities aligns well with the proposed Semantic Field Theory, while systematic weaknesses in pragmatics and context sensitivity align with predictions from social, use-based theories of meaning.

Conclusion: The paper concludes that mathematical semantic structure and socially grounded language games are complementary, not competing, perspectives. This clarifies both the power and the limits of purely statistical LLMs and points toward new, theoretically grounded AI architectures that integrate formal structure with social-pragmatic grounding.

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [19] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: The paper introduces Defensive M2S, a method that compresses multi-turn LLM conversations into single-turn summaries for training guardrail models, greatly reducing computational cost while improving attack detection performance.


<details>
  <summary>Details</summary>
Motivation: Guardrail models must often inspect full multi-turn dialogue histories to ensure safety, but this is computationally expensive at both training and inference time, especially as conversations grow longer. The authors seek a way to maintain or improve safety detection performance while significantly reducing the token and time costs associated with processing entire conversation histories.

Method: They propose Defensive M2S, a training paradigm that converts multi-turn conversations into compressed single-turn inputs using several templates (hyphenize, numberize, pythonize). Guardrail models (LlamaGuard, Nemotron, Qwen3Guard) are then fine-tuned on these compressed representations instead of full histories. A formal complexity analysis demonstrates that this reduces training cost from quadratic to linear in the number of turns. They evaluate different model–compression combinations on SafeDialBench, a multi-turn jailbreak benchmark, and measure attack detection recall and token usage at training and inference time.

Result: On a dataset of 779 samples with an average of 10.6 turns, M2S compression reduces training tokens from 15.7M to 169K (a 93× reduction). The best-performing setting, Qwen3Guard with the hyphenize template, reaches 93.8% attack detection recall while cutting inference tokens per conversation by 94.6% (from 3,231 to 173). This configuration yields a 38.9 percentage point improvement in recall compared to the original multi-turn baseline while drastically lowering token costs.

Conclusion: M2S compression is an effective efficiency technique for LLM guardrail models, enabling them to handle long multi-turn conversations with substantially lower training and inference costs while simultaneously improving jailbreak detection performance. This approach supports more scalable and practical safety screening in real-world LLM deployments.

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [20] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: The paper presents a noise-aware NER method for historical VET documents degraded by OCR, using synthetic noise injection, transfer learning, and multi-stage fine-tuning to improve robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard NER models perform poorly on historical, OCR-degraded VET documents because they are trained on clean, modern text and are not robust to OCR noise or domain-specific terminology. There is also a lack of methods and resources targeting multiple entity types in the VET domain, particularly for noisy historical data.

Method: The authors design a robust NER pipeline based on Noise-Aware Training (NAT). They synthetically inject OCR-like errors into training data, apply transfer learning from pre-trained models, and use multi-stage fine-tuning. They systematically compare three strategies: training on noisy data (with synthetic OCR errors), on clean data, and on a combination of artificial and real data. The approach is evaluated on German historical VET documents and is designed to be language-agnostic.

Result: Experiments show that domain-specific fine-tuning combined with noise-aware training significantly improves NER robustness and accuracy under noisy OCR conditions compared with models trained only on clean data. The model successfully recognizes multiple entity types in VET documents. The authors release code to facilitate reproducible, noise-aware NER in other domain-specific settings.

Conclusion: Noise-aware, domain-specific training with synthetic OCR errors and multi-stage fine-tuning is an effective strategy for improving NER in noisy historical VET documents. The approach generalizes across languages, supports multiple entity types, and provides a practical, reproducible framework for robust NER in other specialized, OCR-degraded corpora.

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [21] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: The paper studies how well rule-based methods can break complex sentences into atomic sentences and which syntactic structures cause problems.


<details>
  <summary>Details</summary>
Motivation: Complex sentences bundle multiple ideas, which hurts IR, QA, and reasoning systems that work better with atomic propositions. Prior split-and-rephrase and LLM-based methods improve accuracy but are hard to interpret and do not reveal which specific syntactic structures cause failures. There is also no principled analysis of which clause and dependency types are most challenging for rule-based extraction.

Method: Implement dependency-based atomic sentence extraction rules in spaCy, targeting complex structures such as relative clauses, adverbial clauses, coordination, and passive voice. Use the WikiSplit dataset, manually construct 100 gold-standard atomic sentence sets, and evaluate the rule-based outputs with ROUGE (1/2/L) and BERTScore to measure lexical, structural, and semantic alignment. Analyze error patterns by syntactic construction.

Result: The system attains ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898 on the 100-sentence test set, suggesting moderate-to-high overlap with human gold standards across lexical, structural, and semantic dimensions. Detailed error analysis shows that certain constructions—relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions—are especially error-prone.

Conclusion: Rule-based atomic sentence extraction using dependency parses can achieve reasonably strong performance, but it is fragile when faced with syntactically complex structures. Specific constructions like relative clauses, appositions, coordination, adverbial clauses, and passives systematically degrade accuracy, highlighting the need for more sophisticated rules, hybrid symbolic–neural approaches, or structure-aware learning methods to handle linguistic complexity.

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [22] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: Survey of multi-hop question answering execution procedures via a four-axis framework, mapping existing systems and highlighting trade-offs and open challenges for retrieval–reasoning agents.


<details>
  <summary>Details</summary>
Motivation: Despite strong performance of recent RAG and agentic multi-hop QA systems, their retrieval–reasoning procedures are often implicit and heterogeneous, making it hard to compare, understand, and improve procedural choices across model families. The paper aims to make the execution process explicit and systematically analyzable.

Method: Propose a four-axis framework for characterizing multi-hop QA execution procedures: (A) overall execution plan, (B) index structure, (C) next-step control including strategies and triggers, and (D) stop/continue criteria. Use this schema to categorize representative systems, review ablations, and synthesize patterns on standard multi-hop QA benchmarks such as HotpotQA, 2WikiMultiHopQA, and MuSiQue.

Result: The framework successfully organizes diverse multi-hop QA methods into a common space, exposing how different systems instantiate plans, indexing choices, control policies, and stopping rules. Synthesized evidence from prior experiments reveals recurring trade-offs among answer effectiveness, computational efficiency, and faithfulness of retrieved evidence across benchmarks.

Conclusion: Execution procedures are a critical, under-specified component of multi-hop QA systems. The proposed four-axis framework clarifies design dimensions and highlights open challenges: incorporating structure-aware planning, learning transferable control policies that generalize across tasks, and developing robust stopping mechanisms that remain reliable under distribution shift.

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [23] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: The paper introduces Embedding Consistency Regulation (ECR), a framework to preserve and even improve the geometric structure of embedding spaces in compact multilingual models, enabling stable, task-aligned, and privacy-friendly deployment without relying on teacher logits.


<details>
  <summary>Details</summary>
Motivation: Compact models, especially in multilingual settings or under tight capacity, often suffer from collapsed or distorted embedding spaces. Existing compression and distillation methods mainly match outputs (e.g., logits) and overlook the underlying manifold geometry. This leads to semantic drift, where the compressed model diverges in both task behavior and linguistic properties from the original, hindering downstream performance and reliable deployment. The authors aim to create a compression-friendly training framework that preserves meaningful semantic geometry in low-capacity models.

Method: The authors propose Embedding Consistency Regulation (ECR). First, they compute teacher embeddings on a corpus once offline and derive a set of semantic anchors that represent the teacher’s embedding manifold. During training, the compact student model is encouraged to maintain consistent local geometry around these anchors, effectively preserving the manifold structure without directly matching logits or internal features. At inference, ECR requires only a lightweight projection step over the learned anchors, adding no changes to the decoding architecture and minimal computational overhead. The framework is designed to be independent of, but compatible with, traditional distillation methods and does not require access to teacher outputs during deployment.

Result: On a 100K multilingual corpus, ECR stabilizes training of compact models and preserves semantic structure across tasks and languages compared with conventional compression baselines. It yields embedding spaces that are both more compact and better aligned with downstream task requirements, enabling low-capacity models to learn cleaner manifolds. The experiments demonstrate that these benefits hold without relying on teacher logits and that ECR can be combined with distillation for further improvements.

Conclusion: Embedding Consistency Regulation offers an effective way to preserve and even enhance the semantic geometry of embeddings in compact multilingual models. By using precomputed semantic anchors and enforcing local geometric consistency, ECR mitigates semantic drift, stabilizes training, and creates task-aligned, deployment-friendly representations. Its low overhead and independence from teacher outputs make it especially suitable for scenarios with strict efficiency or privacy constraints, ultimately helping compact models adhere more closely to task requirements while remaining practical to deploy.

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [24] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: They propose a lightweight, language-agnostic multilingual ASR model using a hierarchical LoRA-MoE framework on top of mHuBERT-CTC, achieving single-pass decoding with competitive accuracy and better efficiency for low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Existing large multilingual ASR models like Whisper perform well but are too computationally heavy and slow for deployment on edge or low-resource devices, and many approaches rely on prior language ID or multi-stage pipelines, which hurt efficiency and true language-agnostic operation.

Method: They build a multilingual ASR system using a CTC-based mHuBERT-CTC backbone augmented with a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework. The hierarchy includes a shared multilingual LoRA capturing language-invariant acoustics and multiple language-specific LoRA experts capturing language-dependent traits. A novel LID-posterior-driven routing mechanism selects experts during inference without needing explicit language labels, enabling single-pass, end-to-end decoding.

Result: On MSR-86K and MLC-SLM 2025 Challenge datasets, the HLoRA system matches or closely approaches state-of-the-art two-stage inference systems while requiring only single-pass decoding, leading to substantially reduced decoding latency and computational cost, particularly beneficial for low-resource mASR scenarios.

Conclusion: A hierarchical, language-agnostic LoRA-MoE design on a CTC mHuBERT backbone can deliver competitive multilingual ASR accuracy with significantly improved efficiency and without requiring language labels at inference, making it well-suited for low-resource and edge-device deployment.

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [25] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth is a framework that automatically generates novel, diverse reasoning and coding benchmarks for LLMs using information-theoretic metrics and a self-verifying synthesis pipeline.


<details>
  <summary>Details</summary>
Motivation: Creating benchmarks for LLM reasoning and coding skills is currently manual, costly, and slow, and many existing benchmarks leak into LLM training data, compromising evaluation. There is a need for scalable, automated methods to build genuinely new and diverse benchmarks whose novelty can be quantified without exhaustive model evaluations.

Method: The authors design InfoSynth, an information-theoretic benchmark generation framework that uses KL-divergence and entropy to quantify novelty and diversity of tasks. On top of this, they build an end-to-end pipeline that starts from seed datasets and applies genetic algorithms plus iterative code feedback to synthesize new Python programming problems along with corresponding solutions and test cases. The framework includes controls to tune target novelty/diversity and difficulty during generation, and it incorporates self-verification of synthesized code through tests.

Result: The pipeline automatically produces new Python coding problems whose solutions and test cases are correct about 97% of the time. Compared to the original seed datasets, the generated benchmarks score higher on the proposed novelty and diversity metrics, and the method demonstrates controllability over both novelty/diversity levels and difficulty of the resulting problems.

Conclusion: InfoSynth provides a scalable, automated, and largely self-verifying way to generate high-quality coding and reasoning benchmarks for LLMs. By grounding the generation process in KL-divergence and entropy, it offers quantitative measures and controllability of benchmark novelty, diversity, and difficulty, helping mitigate data contamination issues and improving the reliability of LLM evaluation.

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [26] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: The paper introduces CSSBench, a Chinese-Specific Safety Benchmark to evaluate how well lightweight LLMs handle Chinese malicious queries that hide intent via Chinese-specific adversarial patterns.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety guardrails and benchmarks are mostly English-centric and fail to capture real-world Chinese malicious queries, which often obfuscate harmful intent using homophones, pinyin, symbol-splitting, and other language-specific tricks. This creates an evaluation gap, especially problematic for lightweight models deployed in cost-sensitive and on-device settings, which may be more vulnerable to such attacks.

Method: The authors construct CSSBench, a benchmark focusing on Chinese-specific adversarial patterns. It spans six realistic domains (illegal activities and compliance, privacy leakage, health/medical misinformation, fraud and hate, adult content, public and political safety) and organizes queries into multiple task types, including adversarially perturbed queries that mimic real-world Chinese usage. They then systematically evaluate a selection of popular lightweight LLMs on this benchmark and also measure over-refusal behavior to quantify safety-induced performance degradation.

Result: The evaluated lightweight LLMs struggle significantly with the Chinese-specific adversarial patterns in CSSBench, revealing that these perturbations critically challenge their safety mechanisms. Additionally, the analysis of over-refusal shows that safety alignment can lead to noticeable performance degradation on benign queries, indicating a trade-off between safety and utility.

Conclusion: Chinese-specific adversarial patterns represent a major, underexplored safety risk for lightweight LLMs. CSSBench provides a comprehensive, realistic benchmark to expose these vulnerabilities and to measure both safety and over-refusal. The benchmark is intended to guide the development and deployment of more robust, Chinese-capable LLMs, especially in cost-sensitive and on-device scenarios.

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [27] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: The paper introduces JourneyBench, a benchmark for evaluating how well LLM-based customer support agents follow complex, multi-step business policies, showing that structured, dynamically controlled prompting greatly improves policy adherence, even for smaller models.


<details>
  <summary>Details</summary>
Motivation: Traditional IVR-style customer support is rigid and poor at handling complex, policy-driven workflows. Although LLM agents seem promising, there is no good way to systematically evaluate whether they truly follow real-world business rules, multi-step policies, and task dependencies, especially under unpredictable user or environment behavior. Existing benchmarks emphasize tool use or task completion but largely ignore detailed policy adherence. The authors aim to fill this evaluation gap.

Method: The authors propose JourneyBench, a benchmark built from graph-based representations of customer support workflows. These graphs are used to generate diverse, realistic multi-step support scenarios that encode business policies and task dependencies. They define a new metric, the User Journey Coverage Score, to quantify how well an agent follows the intended policy path through the workflow. They then implement and compare two LLM agent designs: (1) a Static-Prompt Agent (SPA) that uses a fixed prompt, and (2) a Dynamic-Prompt Agent (DPA) that explicitly models and controls policy execution via structured prompting and orchestration. Multiple state-of-the-art LLMs are evaluated across 703 conversations in three domains on this benchmark.

Result: Across the 703 benchmark conversations, agents using the Dynamic-Prompt Agent (DPA) design achieve substantially higher User Journey Coverage Scores than those using the Static-Prompt Agent (SPA) design, indicating better adherence to complex policies and workflows. Notably, with DPA-style orchestration, a smaller model such as GPT-4o-mini can surpass the policy adherence performance of a more capable model like GPT-4o when used with a static prompt.

Conclusion: Structured, policy-aware orchestration via dynamic prompting is more important for policy adherence in customer support agents than raw model capability alone. JourneyBench provides a realistic, graph-based benchmark and a dedicated metric (User Journey Coverage Score) to measure this behavior, highlighting that careful agent design can enable even smaller LLMs to outperform larger ones in following business rules. The benchmark is positioned as a key resource for pushing AI-driven customer support systems beyond the limitations of traditional IVR approaches.

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [28] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: They propose a simple, model-agnostic repetition-and-judging framework that exponentially reduces hallucinations for fixed-input LLM tasks, with formal probabilistic guarantees validated by experiments.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate even when the prompt clearly specifies the correct information, which is unacceptable in deterministic automation workflows where the input is fixed and correctness is objective. Existing mitigation strategies (e.g., prompt engineering, decoding tweaks, or fine-tuning) can be costly, brittle, or lack formal guarantees. The authors want a lightweight, theoretically grounded way to reliably reduce hallucinations without modifying the model itself.

Method: 1) Formalize a ‘specific task’ as a fixed input with a deterministic correctness criterion. 2) Show that repeating the same prompt multiple times in independent context windows leads to an exponential decrease in the probability that all outputs are wrong. 3) Introduce an LLM-as-a-judge to select the correct answer from multiple runs and derive how the failure probability of this judged pipeline depends on the judge’s true- and false-positive rates. 4) When the judge is imperfect, strengthen it via majority voting over multiple independent judge calls, analyze the ensemble error, and prove that its error probability decays exponentially in the number of votes. 5) Provide explicit probabilistic bounds on the chance that the overall pipeline returns a hallucinated answer.

Result: Theoretical analysis shows that: (a) reissuing the same prompt independently yields an exponential reduction in the probability that all answers are incorrect; (b) using an LLM-as-a-judge to select between answers yields a pipeline whose failure probability decays according to the judge’s accuracy parameters; and (c) majority-vote ensembles of judges further reduce error rates exponentially with the number of votes. Experiments on controlled extraction tasks using synthetic noisy judges empirically confirm the theory: pipeline failure probability decreases exponentially with the number of prompt repetitions, and the probability of selecting a hallucinated answer falls exponentially with the number of judges in the ensemble.

Conclusion: A simple, modular framework that combines repeated prompting with an LLM-as-a-judge (possibly in an ensemble) can drive hallucination probabilities arbitrarily low for fixed-input, deterministic tasks. This approach is model-agnostic, requires no changes to model weights, decoding strategies, or prompts, and comes with explicit probabilistic guarantees that are borne out in experiments, making it well-suited for reliable, automated LLM workflows.

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [29] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: The paper introduces Physio-DPO, a physics-informed alignment method for protein language models that reduces structural hallucinations by leveraging thermodynamic stability information.


<details>
  <summary>Details</summary>
Motivation: Large protein language models often generate sequences that look plausible in sequence space but fold into thermodynamically unstable or incorrect 3D structures, leading to structural hallucinations. Existing alignment methods like Direct Preference Optimization treat structural preferences as binary and fail to exploit the graded, continuous nature of the underlying physical energy landscape. There is a need for a training framework that directly incorporates biophysical stability signals to better align model generation with realistic protein folding behavior.

Method: The authors propose Physio-DPO, a physics-informed variant of Direct Preference Optimization tailored to protein design. It uses physics-based energy evaluations of native protein structures and deliberately perturbed, hard negative structures. A magnitude-aware objective scales optimization updates based on the energy gap between the native and perturbed conformations, so that samples with larger stability differences exert stronger learning signals. This connects the model’s preference learning directly to the thermodynamic stability landscape rather than just binary win/loss labels.

Result: In experiments on generative protein design benchmarks, Physio-DPO outperforms supervised fine-tuning (SFT), Proximal Policy Optimization (PPO), and standard DPO. It reduces self-consistency RMSD to 1.28 Å and boosts the proportion of sequences that reliably fold (foldability) to 92.8%. Qualitative structural inspections show improved packing of hydrophobic cores and more coherent hydrogen-bond networks, indicating more physically realistic folded structures.

Conclusion: Physio-DPO successfully grounds protein language models in thermodynamic principles, significantly mitigating structural hallucinations. By making preference learning magnitude-aware and tied to energy gaps between native and perturbed structures, it better aligns sequence generation with stable, realistic protein folds and recovers key biophysical interaction patterns.

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [30] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: The paper introduces Fast-weight Product Key Memory (FwPKM), a dynamic episodic memory layer for language models that combines high storage capacity with computational efficiency, leading to strong long-context performance and generalization far beyond training lengths.


<details>
  <summary>Details</summary>
Motivation: Existing sequence modeling layers face a core trade-off: Softmax attention can, in principle, store unbounded information but is computationally expensive (quadratic in sequence length), whereas linear attention and similar methods are efficient but have limited, fixed-size memory capacity. Product Key Memory (PKM) offers large, sparse memory but is static and does not update quickly during inference. The authors aim to design a layer that preserves high-capacity storage and efficiency while enabling rapid, online updating of memory during both training and inference, to better handle long-context and episodic information.

Method: The authors propose Fast-weight Product Key Memory (FwPKM), which converts the traditional, static Product Key Memory into a dynamic, fast-weight episodic memory. FwPKM maintains a large, sparse key-value memory structure but allows the values (and potentially some key-related parameters) to be updated on-the-fly using local, chunk-level gradient descent as sequences are processed. During training and inference, the model reads from and writes to this memory via attention-like lookups over product-key-indexed slots, then applies small, localized gradient steps to incorporate new key-value pairs that represent recent context. This provides a fast-weight mechanism where memory contents adapt rapidly within a sequence, rather than only through slow parameter updates across many optimization steps.

Result: Experiments show that adding FwPKM to language models improves their ability to memorize and retrieve sequence-specific information, acting as an effective episodic memory that complements standard semantic representations. On long-context language modeling benchmarks, models with FwPKM achieve substantially lower perplexity than baselines without such dynamic memory. In the Needle in a Haystack evaluations—where models must recover a small, specific piece of information buried in a long context—FwPKM-equipped models can successfully handle contexts of up to 128K tokens, even though they were only trained on sequences up to 4K tokens, indicating strong extrapolation and long-range retrieval capabilities.

Conclusion: FwPKM resolves the traditional capacity-efficiency trade-off in sequence modeling by turning Product Key Memory into a dynamic, fast-weight episodic memory that can be updated during both training and inference. It offers large, efficient storage, rapid online adaptation, and strong long-context retrieval, significantly reducing perplexity on long-context datasets and generalizing to contexts far longer than those seen during training. This suggests that combining standard semantic modules with a fast-weight episodic memory is a powerful design pattern for scalable, long-context language models.

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [31] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: The paper proposes Sigmoid Head, a quality estimation module added on top of pre-trained language models to provide better token-level quality signals than softmax probabilities, without needing human-labeled quality data.


<details>
  <summary>Details</summary>
Motivation: Language model probabilities from the usual softmax head are poor indicators of output quality when multiple continuations are valid. Softmax enforces a single high-probability token at each step, and one-hot training data encodes only one reference token as correct, which penalizes alternative valid outputs. This misalignment makes it hard to tell if a model output is low-quality or just one of many valid options, motivating a better, ambiguity-aware quality estimator.

Method: The authors add a secondary output head, called Sigmoid Head, on top of a pre-trained language model’s hidden representations. Instead of softmax over the vocabulary, this head uses independent sigmoid activations per token (an "unembedding" layer with sigmoid). During training, they perform negative sampling but use a heuristic to avoid choosing tokens that are likely to be valid alternatives to the reference token as negatives. The Sigmoid Head is trained so that multiple tokens can simultaneously receive high probabilities when appropriate, while remaining computationally efficient at training and inference.

Result: The Sigmoid Head’s probabilities correlate more strongly with actual output quality than the original softmax probabilities. It provides better quality estimation signals without requiring human-annotated labels and remains efficient to use. Empirical results also show improved robustness in out-of-domain scenarios compared with supervised quality estimation models that depend on labeled quality data.

Conclusion: By decoupling quality estimation from the softmax output layer and replacing the one-hot, single-answer assumption with a sigmoid-based multi-answer capable head plus careful negative sampling, the proposed Sigmoid Head offers a more reliable, domain-robust quality signal for language model outputs. This approach can be trained on existing LM data without additional human annotations and can be used as a drop-in module for quality estimation on top of pre-trained LMs.

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [32] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: The paper evaluates how well different LLMs can identify relevant text spans in subjective NLP tasks, using various prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Most existing span identification work uses smaller models like BERT and focuses on explicit spans such as named entities; subjective spans in tasks like ABSA using modern LLMs are underexplored.

Method: The authors benchmark multiple LLMs on span identification across three tasks—sentiment analysis, offensive language identification, and claim verification—testing strategies such as instruction tuning, in-context learning, and chain-of-thought prompting.

Result: They find that LLMs can accurately identify precise text spans, especially when they can leverage underlying relationships and structure within the text, and that different prompting strategies impact performance.

Conclusion: LLMs, when guided with appropriate strategies, are effective for subjective text span identification, extending explainable span extraction beyond traditional tasks like NER.

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [33] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: They adapt and ensemble transformer NLP models for cancer pathology reports across Canadian provinces, showing high recall and fewer missed cancers with a privacy-preserving, share-weights-only workflow.


<details>
  <summary>Details</summary>
Motivation: Manual abstraction of pathology reports for cancer registries is slow and resource-heavy, delaying population-level cancer data. Existing transformer-based NLP solutions are usually trained within one jurisdiction, and it is unclear whether they can generalize when pathology reporting styles, templates, and coding practices differ between provinces. There is also a need to minimize missed cancers and to enable cross-provincial collaboration without sharing identifiable patient text.

Method: They took two transformer models—BCCRTron (a domain-adapted model from the British Columbia Cancer Registry) and GatorTron (a general biomedical transformer)—and fine-tuned them on ~104k NLCR pathology reports for Tier 1 (cancer vs non-cancer) and ~22k for Tier 2 (reportable vs non-reportable cancer) classification. They used two complementary input pipelines focusing on synoptic sections and diagnosis sections. Performance was evaluated on Newfoundland & Labrador Cancer Registry test sets, and they built an OR-ensemble that flags a report as positive if either model does. Model weights were shared between provinces, but raw text stayed local to preserve privacy.

Result: Individually, both adapted models achieved strong performance on NLCR data, indicating successful localization from one province to another. However, each model alone still missed a non-trivial number of cancers and reportable cancers. The OR-ensemble substantially improved sensitivity: for Tier 1, recall reached 0.99, halving missed cancers from 48 and 54 (for each standalone model) down to 24. For Tier 2, recall also reached 0.99, reducing missed reportable cancers from 54 and 46 to 33. This shows that combining complementary representations captures errors that each model alone would miss.

Conclusion: Transformer models trained or adapted in one jurisdiction can be effectively fine-tuned for another with limited local data, maintaining high performance despite differing reporting conventions. An OR-ensemble of domain-adapted and biomedical transformers, fed by complementary pathology report sections, significantly reduces missed cancers and improves error coverage, which is critical for registry completeness. Their privacy-preserving, weight-sharing workflow demonstrates a practical path toward interoperable, cross-provincial NLP infrastructure and lays groundwork for a future pan-Canadian foundation model tailored to cancer pathology and registry tasks.

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: They propose a new knowledge retrieval method for LLMs that is aware of conversational reasoning, using a coarse-to-fine, MCTS-inspired search to find logically aligned context, which improves dialogue responses.


<details>
  <summary>Details</summary>
Motivation: Existing LLM enhancement methods either focus on retrieving semantically similar text or on improving reasoning, but they struggle to jointly exploit both. Standard retrieval often matches on surface similarity and ignores the logical structure and reasoning steps in multi-turn dialogue, leading to suboptimal, less creative responses. The authors aim to design a retrieval mechanism that better tracks the reasoning structure of conversations, not just topic similarity, to improve response quality and diversity.

Method: They design a reasoning-aware knowledge retrieval framework with a coarse-to-fine pipeline. First, a coarse step selects a contextually relevant sub-region of the knowledge base, ensuring that all sentences in this region share the conversation’s topic. Then, a fine-grained step retrieves sentences within this region that are closely tied to the ongoing reasoning process in the dialogue. Both steps are driven by a Monte Carlo Tree Search–inspired search over the knowledge base, where navigation between sentences is guided by shared or common keywords, helping explore and exploit promising areas of the knowledge graph or corpus.

Result: On two multi-turn dialogue datasets, the proposed retriever better matches the hidden reasoning structure in human conversations compared with standard semantic similarity retrieval. Quantitatively, it improves measures related to response informativeness and creativity, and qualitatively, it yields more diverse and logically aligned knowledge snippets for LLMs to condition on when generating replies.

Conclusion: Integrating reasoning-awareness into the retrieval stage, via a coarse-to-fine, MCTS-inspired search over a knowledge base, leads to knowledge that is both topically relevant and aligned with conversational logic. This improves the diversity and usefulness of retrieved information and enhances the quality of LLM-generated multi-turn dialogue responses. The work suggests that retrieval mechanisms should explicitly model reasoning structure rather than depend solely on surface semantic similarity.

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [35] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: The paper develops and evaluates fine-tuned large language models to automatically screen for depression from Nigerian Pidgin audio responses, showing that GPT-4.1 can accurately predict PHQ-9 severity and generate culturally appropriate outputs.


<details>
  <summary>Details</summary>
Motivation: Depression is highly prevalent in Nigeria, but screening is limited by clinician shortages, stigma, and language barriers. Existing tools like PHQ-9 were developed in high-income, mostly English-speaking settings and may not transfer well to Nigerian contexts where Nigerian Pidgin and hundreds of local languages dominate everyday communication. There is a need for scalable, culturally and linguistically adapted digital tools that can support depression screening in underserved, low-resource communities.

Method: The authors collected 432 Nigerian Pidgin audio responses from young adults (18–40 years) to prompts aligned with the PHQ-9 depression questionnaire. They transcribed the audio and conducted detailed preprocessing and annotation, including semantic labeling, interpretation of slang and idioms, and assigning PHQ-9 severity scores. Three large language models (Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1) were then fine-tuned on this annotated dataset. Model performance was evaluated using quantitative metrics (accuracy, precision, semantic alignment with target labels) and qualitative analysis (clarity, relevance, and cultural appropriateness of responses).

Result: Among the three fine-tuned LLMs, GPT-4.1 achieved the best performance, with 94.5% accuracy in predicting PHQ-9 severity levels, surpassing Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitative evaluation also indicated that GPT-4.1 generated the clearest, most culturally appropriate, and context-sensitive responses to Nigerian Pidgin inputs.

Conclusion: Fine-tuned LLMs—especially GPT-4.1—can accurately and appropriately screen for depression from Nigerian Pidgin conversational data, demonstrating the feasibility of AI-mediated depression screening in linguistically diverse, resource-constrained Nigerian communities. The work lays a foundation for deploying conversational mental-health tools tailored to low- and middle-income settings with substantial language diversity.

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [36] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: Defines intelligence as goal-directed work per nat of irreversibly processed information, grounded in physical conservation laws and metastable encodings, then uses this to derive limits, architectures, and implications for brains and AI safety.


<details>
  <summary>Details</summary>
Motivation: To give a unified, substrate-neutral, and physically grounded account of what intelligence is, moving beyond informal or algorithmic-only definitions, and to relate intelligent behaviour to thermodynamics, information theory, and conservation laws. The authors aim to explain why intelligent systems look the way they do, what physical limits they face, and how biological brains and artificial systems may be constrained by the same underlying principles.

Method: They model an intelligent system as a coupled agent–environment dynamical process that irreversibly processes information to produce goal-directed work. They formalize how information is physically encoded via the Conservation-Congruent Encoding (CCE) framework, where encodings are metastable basins of attraction constrained by conservation laws. Using this framework, they mathematically define intelligence as work per nat of irreversibly processed information, derive a hierarchy of physical constraints on sensing, computation, and actuation in open systems, analyze dynamical regimes (oscillatory, near-critical) relevant to biological systems, and construct a theory of continuous dynamical circuits where Boolean logic appears as a special attractor-based case. They then extrapolate conceptual implications for self-modelling, epistemic limits, and AI safety as properties of irreversible information flow and structural homeostasis.

Result: 1) A quantitative definition of intelligence rooted in thermodynamics: goal-directed work produced per nat of irreversibly processed information. 2) The Conservation-Congruent Encoding framework linking information encodings to metastable physical states constrained by conservation laws. 3) A derived hierarchy of physical constraints that structure information intake, irreversible computation, and work extraction. 4) A theoretical explanation for why long-horizon efficiency requires preserving internal informational structure, leading naturally to self-modelling and intrinsic epistemic limits analogous to incompleteness. 5) An analysis suggesting that biological systems, particularly brains with oscillatory and near-critical dynamics, operate near an efficiency optimum predicted by the framework. 6) A general theory of continuous dynamical circuits from which classical Boolean logic emerges as a special case, with more general invariant geometries enabling computational modes beyond fixed-point logic. 7) A conceptual, physically grounded framing of AI safety in terms of irreversible information flow and maintenance of structural homeostasis.

Conclusion: Intelligence can be rigorously characterized as a physical process: the efficient transformation of irreversibly processed information into goal-directed work under conservation constraints. This view unifies biological and artificial intelligence as instances of the same substrate-neutral phenomenon and implies intrinsic limits on what intelligent systems can know and do. The proposed framework accounts for observed efficient regimes in biological brains, generalizes digital computation to continuous dynamical circuits, and motivates a physically grounded approach to AI safety based on managing irreversible information flows and preserving structural integrity.

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [37] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: The paper proposes algorithms to balance workload among delivery workers in last‑mile logistics by assigning packages based on both effort and distance, rather than just geographic proximity, and validates them on real urban data.


<details>
  <summary>Details</summary>
Motivation: Traditional last-mile delivery assignments based mainly on geographic proximity often lead to inefficient routes and unfair, unbalanced workloads across delivery workers. This imbalance can reduce overall system efficiency, increase delivery time, and create operational and human-resource management issues. There is a need for methods that explicitly consider workers’ effort and workload when assigning deliveries, to produce both efficient and fair schedules in real urban environments.

Method: The authors formulate last-mile delivery assignment as a workload balancing problem. Given a set of delivery points and a fixed number of workers, they design a multi-algorithm framework that assigns packages so each worker has a similar total effort per day. The approach integrates: (1) several variants of k-means clustering that group delivery points considering distance and workload, (2) evolutionary algorithms to search for better allocations, (3) recursive assignment strategies that refine initial k-means-based partitions using different problem encodings, and (4) a hybrid evolutionary ensemble that combines these techniques. The allocation objective simultaneously accounts for travel distances between workers and delivery points and for balancing total workload (effort/time).

Result: Using data from a real last-mile delivery operation in Azuqueca de Henares, Spain, the proposed multi-algorithm methods can generate package-to-worker assignments where daily workloads are much more evenly distributed than with traditional proximity-based allocation. The algorithms succeed in balancing effort while preserving reasonable travel distances, demonstrating practical applicability on an operational-scale problem.

Conclusion: Workload balancing in last-mile delivery can be significantly improved by moving beyond simple geographic proximity rules and explicitly optimizing workers’ effort. A multi-algorithm approach that combines k-means variants with evolutionary and hybrid ensemble strategies can effectively assign delivery points so that workers have comparable daily workloads. The real-world case study supports the viability of the method and suggests it is a promising tool for human-resources and operational planning in urban last-mile logistics.

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [38] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: The paper introduces MinDist, a new hand-evaluation metric and corresponding algorithm for 13-card Indian Rummy, and shows it substantially improves AI play over traditional heuristics.


<details>
  <summary>Details</summary>
Motivation: While 13-card Indian Rummy is widely played and strategically rich, there is little formal, interpretable work on algorithmic play. Existing heuristics lack a principled way to measure how close a hand is to completion and to reason about opponent hands in a systematic, computationally tractable way.

Method: The authors define MinDist, a hand-evaluation metric based on an edit distance between a current hand and the closest valid meld configuration, extending the earlier MinScore measure. They adapt the MinScore dynamic-programming algorithm with dynamic pruning and pattern caching to compute MinDist exactly during play. They embed this in a rule-based strategic framework for move selection and augment it with opponent hand modeling in a two-player zero-sum simulation setting. They then compare MinDist-based agents against heuristic baselines using statistically rigorous evaluation (hypothesis testing).

Result: The MinDist-based agents achieve significantly higher win rates than agents using traditional heuristic evaluation functions in simulated two-player Indian Rummy games. The algorithm remains computationally efficient enough for use during play due to pruning and caching optimizations.

Conclusion: Modeling the structural edit distance to the nearest valid hand (MinDist) provides a powerful, interpretable way to evaluate states and guide strategy in 13-card Indian Rummy. This formalizes aspects of expert intuition and yields practical performance gains, suggesting a promising direction for principled AI strategy design in rummy-like card games.

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [39] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: The paper evaluates how generative AI models interpret and recreate the architectural logic of Iranian vernacular pigeon towers, revealing a gap between visual similarity and true architectural reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand whether current image-based generative AI can grasp not only the appearance but also the embedded environmental, material, and cultural intelligence of vernacular architecture, using Iranian pigeon towers as a representative case.

Method: The authors use three diffusion-based generative AI systems—Midjourney v6, DALL-E 3, and DreamStudio (SDXL)—and craft three escalating prompt types: (1) referential prompts based directly on the original pigeon tower type, (2) adaptive prompts that ask for contextual variation, and (3) speculative prompts that push the models toward innovative reinterpretation. Outputs are evaluated using a five-criteria framework: typology, materiality, environment, realism, and cultural specificity.

Result: All three AI systems consistently reproduce recognizable forms and geometric patterns of pigeon towers, performing well in typology and basic realism. However, they systematically misinterpret or overlook the underlying material logic, climatic adaptation strategies, and culturally specific cues. Reference images help produce more realistic and faithful depictions but constrain the models’ ability to generate novel interpretations, whereas prompt freedom encourages creative forms that drift away from cultural and environmental authenticity.

Conclusion: There is a clear boundary between visual resemblance and genuine architectural reasoning in current generative AI: models excel at stylistic mimicry but struggle with the deeper performative, climatic, and cultural intelligence embedded in vernacular forms. The authors propose “computational vernacular reasoning” as a conceptual lens to examine how AI systems perceive, distort, and creatively reframe traditional design knowledge, highlighting both their potential and their limitations for architectural design and heritage analysis.

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [40] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: Designing an LLM-based agent to automatically build fuzzy cognitive maps (FCMs) with causal feedback from raw text, and showing that its resulting dynamics match human-designed maps.


<details>
  <summary>Details</summary>
Motivation: Building causal fuzzy cognitive maps from text is typically expert-intensive and manual. The authors want an automated, agentic way to infer causal structures and their dynamical behavior directly from natural language, enabling scalable modeling of complex systems and capturing feedback loops that drive quasi-autonomous behavior.

Method: They create an LLM “agent” controlled by three carefully crafted system instructions, executed in sequence: (1) extract important nouns and noun phrases from text, (2) select from these the most relevant FCM concept nodes, and (3) infer fuzzy causal edges (including direction and sign/strength) among the nodes. The resulting FCM defines a dynamical system whose equilibria (fixed points and limit cycles) are then analyzed. They apply this to a test essay about AI and compare the LLM-generated FCMs (from different LLMs, Gemini and ChatGPT) to human-generated FCMs. They also construct a mixed FCM combining multiple LLM-generated maps and study its equilibria.

Result: The LLM-generated FCMs, despite having different numbers of nodes and edges than human-generated FCMs, settle into the same equilibrium limit cycles as the human maps when simulated. A mixed FCM combining output from Gemini- and ChatGPT-based agents not only inherits the dominant equilibria from the stronger component map but also exhibits additional equilibria that better approximate the presumed underlying causal dynamics described in the text.

Conclusion: A carefully instructed LLM agent can autonomously extract causal fuzzy cognitive maps from raw text whose dynamic behavior closely matches expert-built FCMs. Moreover, combining multiple LLM-derived FCMs can yield richer dynamical structures that both preserve key equilibria and introduce new ones, improving approximation of the underlying causal system while retaining controllability via prompts (the “agentic leash”).

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [41] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar is a system that automatically evolves and evaluates new game mechanics by combining evolutionary search with large language models, producing diverse, skill-based, and human-validated games.


<details>
  <summary>Details</summary>
Motivation: Designing game mechanics is traditionally manual, slow, and requires expert knowledge. There is a need for automated methods that can generate not just levels or content, but the fundamental rules and interactions of games, while ensuring the resulting games are skill-based and engaging. Existing automatic game design systems often struggle to explore a wide variety of mechanics and to rigorously evaluate them in terms of meaningful gameplay quality.

Method: Mortar integrates a quality-diversity (QD) evolutionary algorithm with a large language model to autonomously generate and explore a wide range of game mechanics. For evaluation, Mortar synthesizes complete games that combine evolved mechanics with ones stored in an archive. It then uses a tree search procedure to compose these mechanics into full games. The resulting games are evaluated according to how well they preserve a skill-based ordering among players—specifically, whether more skilled players reliably outperform less skilled ones. Each mechanic is scored based on its marginal contribution to this skill-ordering metric. The authors conduct ablation studies to examine the role of each component and complement this with a human user study.

Result: Mortar successfully generates diverse and playable games, and evolves mechanics that improve the skill-based ordering score. The system finds mechanics that measurably contribute to maintaining a consistent performance hierarchy among players. Ablation results show that each major component (QD search, LLM, tree search composition, and skill-based evaluation) plays an important role in performance. The user study indicates that humans perceive the resulting games as varied and playable, supporting the quantitative findings.

Conclusion: The paper concludes that Mortar is an effective system for autonomously evolving game mechanics, not just surface-level content. By integrating quality-diversity search with large language models and a skill-ordering-based evaluation, Mortar can generate diverse, playable games with mechanics that support meaningful, skill-based gameplay. The work suggests a path toward more fully automated game design systems and highlights the importance of combining automated search, generative models, and principled gameplay metrics, with potential for extension to broader domains in interactive system design.

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [42] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: The paper tests whether LLMs can directly solve inventory optimization and finds they perform poorly due to hallucinations and weak stochastic reasoning, then proposes a hybrid system where the LLM handles language and a separate solver handles math, achieving much better performance.


<details>
  <summary>Details</summary>
Motivation: Small and medium-sized businesses struggle with inventory management and usually lack access to or expertise in advanced optimization tools. With the rise of LLMs, there is interest in using them as easy-to-use decision tools via natural language. However, it is unclear whether LLMs alone can reliably handle complex, stochastic inventory optimization without specialized OR methods. The paper is motivated by closing this usability gap while maintaining rigorous optimization quality.

Method: The authors compare two setups: (1) using GPT-4o as an end-to-end solver for inventory decisions driven by natural language, and (2) a hybrid agentic framework where the LLM only interprets and elicits information from users and delegates all quantitative optimization to formal algorithms. They introduce a "Human Imitator"—a fine-tuned model that simulates a boundedly rational manager—to generate realistic, noisy dialogue and decisions for scalable stress testing of both systems. Using this environment, they evaluate inventory costs and performance across scenarios, including ones with perfect information, to isolate computational vs informational limitations of LLMs.

Result: The hybrid agentic framework lowers total inventory costs by 32.1% compared with the interactive baseline that relies on GPT-4o as a direct, end-to-end solver. Experiments show that even when GPT-4o is given perfect ground-truth information, its performance does not improve enough, indicating that the primary constraint is its computational and reasoning capacity, not lack of information. The Human Imitator successfully provides a controlled but realistic testing ground for evaluating LLM-based decision systems under messy, human-like input.

Conclusion: LLMs are not yet suitable as standalone solvers for complex, stochastic inventory optimization because they suffer from a "hallucination tax" and limited grounded mathematical reasoning. However, when used as natural-language interfaces that gather requirements and communicate results while delegating computation to rigorous optimization solvers, they can significantly improve decision quality and make advanced operations research tools accessible to non-expert managers. The work advocates for hybrid agentic architectures rather than end-to-end LLM replacement of traditional OR methods.

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [43] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: Proposes Mathesis, a neuro-symbolic architecture that turns logical proof search into continuous energy minimization over hypergraph-encoded mathematical states.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex logical reasoning because they lack an explicit axiomatic and logically consistent internal structure, leading to brittle and inconsistent multi-step deductions. The work aims to build an architecture with an internal logical framework that can support robust, verifiable reasoning.

Method: Represent mathematical states as higher-order hypergraphs; introduce a Symbolic Reasoning Kernel (SRK), a differentiable logic engine that maps logical constraints to a continuous energy landscape via a global energy function E(G), where zero energy corresponds to full logical consistency; use gradients from this energy function to train a Hypergraph Transformer Brain; cast proof search as energy minimization, and implement multi-step deduction via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

Result: The abstract implies that Mathesis can perform multi-step logical deduction by treating proofs as low-energy configurations in a differentiable space, and that the combination of SRK, Hypergraph Transformer, and search procedures enables more structured and consistent reasoning than standard LLMs, though quantitative results are not described in the abstract.

Conclusion: A neuro-symbolic framework like Mathesis can endow reasoning systems with an internal axiomatic structure by encoding logic as an energy landscape over hypergraphs and using gradient-based training plus search, thereby transforming proof search into an optimization problem and potentially improving logical consistency and complex reasoning in LLM-style systems.

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [44] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: The paper studies whether confidence-based abstention in vision-language models can reliably control error rates in video question answering, especially under distribution shift.


<details>
  <summary>Details</summary>
Motivation: In high-stakes settings, VLMs must be able to abstain when uncertain to avoid costly mistakes. It is unclear whether standard confidence-based abstention yields reliable error control, particularly when the data distribution changes.

Method: The authors use the NExT-QA benchmark and the Gemini 2.0 Flash model to empirically examine how confidence thresholding affects risk-coverage tradeoffs in video question answering, both in-distribution and under distribution shift.

Result: They find that in-distribution, adjusting the confidence threshold epsilon yields smooth risk-coverage curves and reduces error rates as coverage decreases, indicating mechanistic control. (The abstract is truncated, so results under distribution shift are not fully specified.)

Conclusion: Confidence-based abstention can provide controllable error rates for VLMs on in-distribution video QA, but the robustness of this control under distribution shift is not fully clear from the truncated abstract.

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [45] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: The paper evaluates three neural reasoning paradigms and argues that explicit model-based neural architectures (Sphere Neural Networks) yield more reliable logical reasoning than LLMs or standard supervised networks.


<details>
  <summary>Details</summary>
Motivation: Existing neural reasoning approaches, particularly LLM-based and standard supervised-learning models, are unreliable on even simple logical tasks and lack robust, human-like decision-making. The authors aim to understand which neural reasoning paradigm is most reliable and to design an architecture that can handle various forms of syllogistic and disjunctive reasoning without catastrophic forgetting or shallow, pattern-level competence.

Method: The authors theoretically compare three categories of neural reasoning (LLM reasoning, supervised-learning reasoning, and explicit model-based reasoning), then empirically test two architectures on syllogistic and disjunctive syllogistic reasoning tasks. First, they train an Euler Net to 100% accuracy on classic syllogistic reasoning and then retrain it on disjunctive syllogistic reasoning to probe generalization and forgetting. Second, they design Sphere Neural Networks that represent concepts as circles on the surface of an n-dimensional sphere, with negation encoded as complement circles. Reasoning is performed by checking geometric consistency and filtering out illogical statements that create unsatisfiable circle configurations. They evaluate this model on 16 syllogistic reasoning tasks, including disjunctive syllogisms.

Result: The Euler Net can achieve 100% performance separately on classic and disjunctive syllogistic reasoning but fails to retain prior knowledge when retrained, dropping to 6.25% on previously mastered tasks, and its reasoning is limited to pattern matching rather than robust logical competence. In contrast, the proposed Sphere Neural Network can represent negation geometrically, detect unsatisfiable logical configurations, and successfully solve 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while maintaining the integrity of classical syllogistic reasoning capabilities.

Conclusion: Among neural reasoning paradigms, explicit model construction—exemplified by the proposed Sphere Neural Networks that encode logical structure geometrically—provides the most reliable reasoning, avoiding catastrophic forgetting and surpassing LLM-based and standard supervised approaches in rigorous syllogistic and disjunctive reasoning tasks.

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [46] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench is a standardized, closed-loop benchmark and deployment framework that lets LLM agents generate, test, compare, and automatically deploy optimized GPU kernels into real LLM inference systems.


<details>
  <summary>Details</summary>
Motivation: Although LLMs can now write GPU kernels as autonomous agents, there is no standardized, end‑to‑end way to validate, compare, and safely integrate these AI-generated kernels into real-world inference stacks. Existing efforts are fragmented: kernel generation, correctness testing, performance benchmarking, and deployment are disconnected, and often not based on realistic serving traces. This makes it hard to systematically improve agents’ GPU programming abilities or to trust and operationalize their outputs. The paper is motivated by this gap and aims to provide a practical infrastructure that closes the loop from kernel generation to production deployment.

Method: The authors design FlashInfer-Bench as a closed-loop framework centered around FlashInfer Trace, a unified schema that encodes kernel definitions, workloads, implementations, and evaluation results. On top of this schema, they build: (1) a curated dataset derived from real LLM serving traces, defining realistic workloads; (2) a benchmarking framework that jointly checks correctness and performance of AI-generated kernels; (3) a public leaderboard to systematically track and compare different LLM agents’ GPU programming skills; and (4) an apply() mechanism that can dynamically substitute existing kernels in production LLM engines (e.g., SGLang, vLLM) with the current best-performing variants. They then use this infrastructure to run experiments on multiple agents and GPU programming languages, analyzing performance, trade-offs, and failure modes.

Result: FlashInfer-Bench produces a reproducible benchmark suite and infrastructure where LLM agents can be evaluated and iteratively improved. It yields: (1) a realistic dataset reflecting real-world inference traces; (2) quantitative measurements of correctness and performance of AI-generated GPU kernels; (3) a leaderboard ranking different agents and programming approaches; and (4) successful demonstration that kernels selected through this framework can be automatically integrated into popular LLM inference engines via apply(), improving performance in practice. The experimental use of the framework reveals concrete performance differentials between agents and between GPU programming languages, as well as limitations and failure cases of current LLM-based kernel generation.

Conclusion: FlashInfer-Bench provides a practical, standardized, and reproducible closed-loop pathway from LLM-driven GPU kernel generation to deployment in real LLM inference systems. By unifying trace description (FlashInfer Trace), correctness/performance benchmarking, public ranking, and automatic production substitution, it enables continuous and systematic improvement of AI-generated kernels. The framework also surfaces important insights into the strengths, weaknesses, and trade-offs of current LLM agents and GPU programming tools, offering guidance for future agent and system design.

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [47] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: The paper studies how LLM-based agents can develop and even be manipulated into showing “us vs. them” bias, including against humans as a group, and proposes attacks and mitigations related to this vulnerability.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLM agents, beyond exhibiting standard demographic biases, can develop intergroup bias from minimal cues and even treat humans as an outgroup; and to investigate whether such bias can be systematically triggered or amplified via attacks on agents’ persistent beliefs, posing new safety and alignment risks.

Method: The authors build a controlled multi-agent social simulation centered on allocation decisions with explicit payoff trade‑offs between in‑group and out‑group entities. They introduce minimal group cues and vary whether some counterparts are framed as humans versus agents. They then design and test a Belief Poisoning Attack (BPA) that tampers with agents’ persistent identity beliefs, implemented as profile poisoning at initialization (BPA‑PP) and memory poisoning via optimized belief‑refinement suffixes injected into stored reflections (BPA‑MP). They empirically evaluate intergroup bias and the impact of BPA across different settings.

Result: The simulations show that LLM agents reliably develop intergroup bias under minimal “us vs. them” cues. When some counterparts are framed as humans, this bias weakens, which the authors interpret as evidence of an implicit, belief‑dependent “human‑norm script” that favors humans only when the agent believes a human is present. By corrupting these beliefs with BPA‑PP and BPA‑MP, attackers can suppress the human‑norm script and restore or intensify outgroup bias toward humans, demonstrating the effectiveness and severity of belief poisoning across multiple conditions.

Conclusion: LLM‑based agents are prone to intergroup bias, including in settings where humans may be implicitly treated as an outgroup, and their mitigation relies in part on fragile, belief‑dependent human‑favoring norms. This belief dependence opens a novel attack surface: Belief Poisoning Attacks on identity-related beliefs can undermine protections and re‑induce harmful bias. The authors propose practical mitigations that focus on securing agent profiles and memory boundaries, emphasizing the need to explicitly harden current agent frameworks against such attacks to achieve safer, more reliable agent behavior.

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [48] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: An AI agent framework, ClinicalReTrial, iteratively redesigns clinical trial protocols to increase their probability of success, not just predict outcomes.


<details>
  <summary>Details</summary>
Motivation: Clinical trials frequently fail due to subtle protocol design issues, wasting promising therapeutics and resources. Existing AI models focus on predicting trial success or failure, which only diagnoses risk and does not suggest how to fix flawed protocols once problems are detected. There is a need for a proactive, optimization-driven system that can automatically propose and refine protocol changes to improve trial success probability before real-world execution.

Method: The paper formulates clinical trial design as an iterative protocol redesign and optimization problem. It introduces ClinicalReTrial, a self-evolving AI agent framework that operates in a closed loop: (1) it diagnoses likely failure modes of a given protocol; (2) proposes safety-aware modifications to the protocol; and (3) evaluates each modified protocol using a trial outcome prediction model that acts as a simulation environment. This setup provides dense reward feedback, enabling reinforcement-style optimization. The framework uses hierarchical memory, capturing feedback both within a single trial’s redesign iterations and across many trials, to learn generalizable redesign patterns and guide more efficient exploration of protocol changes over time.

Result: On empirical evaluation, ClinicalReTrial improves 83.3% of the evaluated clinical trial protocols. On average, it increases the predicted probability of success by 5.7% per protocol. Retrospective case studies show that the system’s learned redesign strategies resemble actual protocol modifications used in real-world clinical trials, suggesting practical relevance and face validity of its recommendations.

Conclusion: ClinicalReTrial demonstrates that clinical trial outcome prediction models can be transformed into environments for closed-loop protocol optimization, moving from passive risk assessment to active protocol improvement. By combining automated failure diagnosis, safety-aware redesign, and memory-augmented iterative learning, the framework can systematically increase trial success probabilities and discover modification patterns that align with real-world expert practice, indicating potential utility for supporting human trial designers in drug development.

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [49] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: The paper proposes a unified “Financial Swarm” framework where independent traders, modeled as swarm agents, use game-theoretic and reinforcement-learning tools (difference rewards, Markov team games) so that their self-interested liquidity-maximizing actions collectively improve overall market liquidity and efficiency without explicit coordination.


<details>
  <summary>Details</summary>
Motivation: Swarm intelligence and financial market design both study many independent agents whose local decisions lead to emergent global behavior, but these fields are usually treated separately. Swarm research lacks strong game-theoretic foundations to explain how rational, self-interested agents can still produce cooperative, utility-aligned outcomes. Financial market research seeks models where independent traders self-organize to provide liquidity and stabilize markets without collusion. Unifying these perspectives could clarify how decentralized, rational agents can achieve both individual payoff maximization and desirable system-level properties like liquidity and efficiency.

Method: The authors merge two existing paradigms: Liquidity Games from market microstructure and Rational Swarms from multi-agent learning. They define a swarm of traders whose payoffs depend on aggregate liquidity and whose collective goal is to enhance market liquidity while preserving independence. Within a Markov team games framework, they apply difference rewards—a reward shaping technique that measures each agent’s marginal contribution to the global objective—to align each agent’s learning signal with the system-wide liquidity objective. They then analyze how individual best responses under this scheme lead to globally beneficial liquidity outcomes.

Result: Within the proposed Markov team / Financial Swarm framework, the analysis shows that when traders are rewarded using difference rewards, each agent’s individually rational behavior (maximizing its adjusted payoff) leads to increased aggregate liquidity. This occurs without explicit coordination, communication, or collusive agreements among traders. The model demonstrates theoretical compatibility between individual profitability and systemic liquidity provision in bilateral asset markets under the specified game and reward structure.

Conclusion: The paper concludes that integrating Liquidity Games with Rational Swarms yields a coherent “Financial Swarm” model in which independent, self-interested agents can simultaneously pursue their own profits and enhance overall market liquidity. Difference rewards within a Markov team games setting provide a principled mechanism to align local incentives with global objectives, eliminating the need for coordination or collusion. This unified framework offers a foundation for future work in both swarm intelligence and financial market design, particularly for designing market mechanisms and learning rules that foster stability and efficiency in decentralized trading environments.

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [50] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt is a bio-inspired, LM-powered self-healing framework for distributed computing continuum systems that detects, diagnoses, and recovers from faults autonomously in seconds with low resource usage.


<details>
  <summary>Details</summary>
Motivation: Distributed Computing Continuum Systems (from IoT to cloud) are highly heterogeneous, dynamic, and fault-prone, making it difficult to maintain service continuity with traditional, manual, or static resilience mechanisms. There is a need for scalable, adaptive, self-regulated resilience strategies that can autonomously detect and heal faults across diverse components and conditions, similar to how biological systems maintain homeostasis.

Method: The paper proposes ReCiSt, an agentic self-healing framework inspired by the biological wound-healing phases. It maps Hemostasis, Inflammation, Proliferation, and Remodeling to four computational layers: Containment, Diagnosis, Meta-Cognitive, and Knowledge. Within these layers, language-model-powered agents autonomously handle fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation. The agents parse heterogeneous logs, infer root causes, refine reasoning strategies, and reconfigure computing resources. The framework is implemented and evaluated on public fault datasets using multiple language models, without baseline comparisons due to the lack of similar systems.

Result: Across different language models and public fault datasets, ReCiSt demonstrates the ability to perform self-healing within tens of seconds, using a minimum of about 10% CPU for the agents. The evaluations also highlight the framework’s capacity to perform deep analysis under uncertainty and to dynamically orchestrate an appropriate number of micro-agents to achieve resilience.

Conclusion: ReCiSt successfully translates biological self-healing phases into a layered, LM-driven computational framework that enables autonomous resilience in distributed computing continuum systems. The empirical results suggest that the approach can rapidly and efficiently detect, diagnose, and recover from faults with limited human oversight, pointing to the viability of bio-inspired, agentic architectures for self-healing in complex distributed environments.

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [51] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: The paper proposes ACCD, an adaptive causal coordination detection framework that improves accuracy, reduces manual labeling, and speeds up detection of coordinated inauthentic behavior on social media.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting coordinated inauthentic behavior on social media rely on shallow correlation, fixed parameters, and heavy manual annotation, limiting robustness, adaptability, and scalability. There is a need for a more principled, automated, and efficient end-to-end solution.

Method: ACCD is a three-stage framework with a memory-guided adaptive mechanism. Stage 1 uses an adaptive Convergent Cross Mapping (CCM) technique to discover genuine causal relations between accounts. Stage 2 combines active learning and uncertainty sampling in a semi-supervised classifier to minimize manual labels. Stage 3 uses an automated validation module that leverages historical detection experience to self-verify and refine detection results. Hierarchical clustering is employed for efficiency gains.

Result: On real-world datasets (Twitter IRA, Reddit coordination traces, and bot benchmarks), ACCD reaches an F1-score of 87.3% for coordinated attack detection, outperforming the best baseline by 15.2%. It cuts manual annotation needs by 68% and yields a 2.8x processing speedup via hierarchical clustering optimization.

Conclusion: ACCD delivers a more accurate, efficient, and highly automated end-to-end approach for discovering coordinated behavior on social media, with strong empirical performance and practical potential for broad deployment.

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [52] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: The paper proposes using semantic-space vector models, common in computational linguistics, to represent and optimize tactics in team sports and other multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Traditional tactical analysis in team sports often relies on heuristics, manual observation, or rigid statistical models that struggle to capture the compositional and contextual nature of team behavior. Semantic-space reasoning in NLP has matured as a way to represent complex meaning and context using vectors. The paper is motivated by the analogy that teams and plays can be treated like texts and sentences, suggesting that similar vector-based methods could systematically model tactical fit, coordination, and opponent exploitation across team-based domains.

Method: The authors map each player to a multidimensional vector that encodes technical, physical, and psychological attributes. They then construct team-level vectors by contextually weighting and aggregating these player vectors, analogous to composing word vectors into sentence or document representations. Tactical templates (e.g., high press, counterattack, possession build-up) are also encoded as vectors in the same semantic space. By computing distances and alignments between team vectors and tactical-template vectors, they quantify tactical fit and potential to exploit opponents. A Python prototype operationalizes this framework, generating tactical recommendations and fine-grained attribute-level diagnostics.

Result: The prototype shows that tactical configurations and team profiles can be embedded into a shared semantic space where their alignment can be quantified using vector-distance metrics. This yields interpretable, adaptive strategy recommendations and detailed insights into which player attributes drive or limit tactical fit. The approach is demonstrated as flexible and extendable to different tactical concepts and team compositions, illustrating its potential beyond a single sport.

Conclusion: The paper concludes that semantic-space reasoning can be successfully repurposed from NLP to tactical decision-making in team sports and more broadly to multi-agent coordination problems. It provides a conceptual and prototypical framework for modeling tactical fit, generating recommendations, and offering diagnostic insights. Future work will focus on integrating real-world tracking and performance data, adding predictive simulation capabilities, and developing hybrid human-machine tactical intelligence systems that support coaches, analysts, and autonomous agents across various domains.

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [53] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: The paper presents MIDAS, a multi-agent AI framework that mimics human meta-cognitive ideation workflows to generate more novel and diverse engineering design ideas.


<details>
  <summary>Details</summary>
Motivation: Novice designers struggle to generate truly novel and diverse ideas, and existing single-model AI tools worsen this by outputting many similar, semantically clustered concepts. The authors want an AI system that better supports creative engineering ideation rather than flooding users with near-duplicates.

Method: The authors design MIDAS, a distributed team of specialized AI agents that emulate stages of human meta-cognitive ideation. These agents iteratively refine ideas and evaluate each candidate along two axes: global novelty (relative to known solutions) and local novelty (relative to the set of already-generated ideas). The framework structures the interaction so that AI agents collaborate and hand off tasks in a workflow, with the human designer integrated into this loop.

Result: The framework shows that an agentic, multi-AI setup can systematically generate and refine ideas while tracking both global and local novelty, avoiding tight semantic clustering. It enables a more controlled, progressive exploration of the design space compared to standard single-spurt AI tools.

Conclusion: MIDAS offers a promising paradigm for human-AI co-creation in engineering design, moving beyond single-shot idea dumps toward a structured, meta-cognitive, multi-agent workflow. This approach repositions human designers from merely filtering AI outputs to actively collaborating with AI agents in an iterative ideation process, leading to more genuinely novel and diverse design outcomes.

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [54] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: The paper examines whether reasoning models truly have mid-reasoning "Aha!" moments that improve performance, and finds that such intrinsic shifts are rare and usually unhelpful, but that artificially induced shifts under high uncertainty can boost accuracy.


<details>
  <summary>Details</summary>
Motivation: Prior work has suggested that large reasoning models sometimes exhibit sudden mid-chain realizations that correct earlier mistakes, interpreted as a form of emergent self-correction or insight. However, it is unclear whether these observed shifts are real, reliable, and beneficial, or just artifacts of unstable inference. The authors aim to rigorously test the prevalence, nature, and utility of these mid-reasoning shifts.

Method: The authors analyze over one million reasoning traces drawn from multiple model architectures, reasoning domains, and decoding temperatures, and monitor hundreds of training checkpoints. They instrument training runs to detect mid-reasoning shifts, quantify their frequency and relationship with training progress, and measure how often they improve answer accuracy. They also study how shifts relate to model uncertainty (entropy), and run interventions that artificially trigger extrinsic mid-reasoning shifts when uncertainty is high.

Result: They find that intrinsic mid-reasoning shifts are rare events, do not increase in frequency as models train, and only seldom lead to more accurate final answers, undermining the notion that they correspond to genuine model "insight." The impact of such shifts is modulated by model uncertainty: their effects differ depending on the entropy of the model’s predictive distribution. Leveraging this, the authors demonstrate that deliberately inducing extrinsic shifts (e.g., restarting or perturbing reasoning) under high-entropy states consistently improves accuracy.

Conclusion: Mid-reasoning shifts in current reasoning models are better interpreted as signatures of unstable inference dynamics than as an internal self-correction mechanism or sudden insight. Nonetheless, these dynamics can be exploited: by intentionally triggering shifts when the model is uncertain, one can systematically enhance performance. This reframes "Aha!" moments from a romantic notion of emergent insight to a practical handle on managing and improving reasoning stability and reliability.

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [55] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: The paper proposes DA-DPO, a difficulty-aware framework to improve Direct Preference Optimization for multimodal LLMs by mitigating overfitting to easy preference pairs and better reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal DPO methods for reducing hallucinations in MLLMs overfit to easy preference pairs due to difficulty imbalance in preference data, which harms fine-grained hallucination suppression and overall performance.

Method: The authors introduce DA-DPO, which has two components: (1) Difficulty Estimation using pre-trained vision–language models with complementary generative and contrastive objectives, combined via a distribution-aware voting strategy to assign difficulty scores to preference pairs without extra training; and (2) Difficulty-Aware Training, which reweights preference pairs by difficulty, down-weighting easy pairs and emphasizing harder ones during DPO learning to alleviate overfitting.

Result: Experiments show that DA-DPO improves multimodal preference optimization, leading to stronger hallucination robustness, better generalization on standard benchmarks, and maintains computational efficiency compared to baseline DPO methods.

Conclusion: DA-DPO effectively balances learning across preference pairs of varying difficulty, enhances hallucination suppression and generalization in MLLMs, and does so in a cost-effective manner without requiring additional data or fine-tuning stages.

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [56] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: The paper proposes PedX-LLM, a vision-and-knowledge enhanced LLM framework that infers pedestrian crossing decisions with higher accuracy and better generalizability than traditional statistical and supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing statistical and supervised learning models for predicting pedestrian crossing behavior are site-specific and fail to generalize well to new locations. Meanwhile, recent LLM-based approaches have not been sufficiently adapted to the transportation domain and generally lack visual context, limiting their ability to reason about pedestrian behavior in diverse, real-world environments. There is a need for a model that can leverage both visual information and domain knowledge to perform human-like, context-aware reasoning about crossing decisions and generalize across different sites.

Method: The authors develop PedX-LLM, a framework built on a LLaMA-2-7B foundation model fine-tuned with Low-Rank Adaptation (LoRA). The model ingests textual data, transportation domain knowledge, and visual features extracted by LLaVA to perform pedestrian crossing inference. The framework includes a vision-augmented module that encodes the built environment and a knowledge integration component that injects transportation-specific concepts and rules. They evaluate PedX-LLM’s performance through balanced accuracy metrics and conduct cross-site validation using site-based partitioning to test generalizability on unseen environments. Zero-shot and few-shot configurations are examined to assess how minimal site-specific examples affect performance.

Result: PedX-LLM attains 82.0% balanced accuracy, surpassing the best-performing statistical and supervised learning baselines. The vision-augmented module alone yields a 2.9% improvement in balanced accuracy by effectively encoding the built environment, while adding transportation domain knowledge provides an additional 4.1% gain. In cross-site validation on five unseen test sites, the zero-shot PedX-LLM achieves 66.9% balanced accuracy, outperforming baseline data-driven methods by at least 18 percentage points. When enhanced with just five validation examples in a few-shot learning setup, PedX-LLM further improves to 72.2% balanced accuracy, demonstrating robust performance even with minimal target-site data.

Conclusion: PedX-LLM, by combining visual features, textual information, and transportation domain knowledge within an LLM framework, delivers more accurate and generalizable inference of pedestrian crossing behavior than traditional data-driven models. The vision-and-knowledge-enhanced reasoning enables the model to approximate human-like decision processes, yielding strong zero-shot and few-shot generalization to unseen sites and addressing the limitations of prior site-specific statistical and supervised methods.

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [57] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: The paper introduces AgenticDomiKnowS (ADS), an agent-based system that converts free-form task descriptions into executable DomiKnowS neuro-symbolic programs, drastically cutting development time.


<details>
  <summary>Details</summary>
Motivation: Integrating symbolic constraints into deep learning makes models more robust, interpretable, and data-efficient, but current frameworks like DomiKnowS require users to learn specialized syntax and perform a complex, time-consuming setup process. There is a need to let both experts and non-experts leverage neuro-symbolic tools without deep library-specific knowledge or hours of manual coding.

Method: The authors design AgenticDomiKnowS (ADS), an agentic workflow that takes natural language task descriptions and incrementally generates a full DomiKnowS program. The workflow decomposes the problem into separate components (e.g., defining domains, constraints, models), creates and tests each part independently, and iteratively refines them. ADS also supports optional human-in-the-loop interaction, allowing knowledgeable DomiKnowS users to inspect and adjust intermediate artifacts before moving on.

Result: ADS successfully enables both experienced DomiKnowS users and complete non-users to construct working neuro-symbolic programs starting from free-form descriptions. In their evaluation, the system reduces the end-to-end development time from several hours down to roughly 10–15 minutes per task, while still producing usable DomiKnowS code.

Conclusion: AgenticDomiKnowS demonstrates that an agentic, natural-language-driven workflow can substantially lower the barrier to building neuro-symbolic systems with DomiKnowS. By automating the translation from informal task descriptions to formal program components and allowing human oversight when desired, ADS accelerates development and makes neuro-symbolic integration more accessible and practical.

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>
