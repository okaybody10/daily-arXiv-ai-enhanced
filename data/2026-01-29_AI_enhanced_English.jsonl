{"id": "2601.19913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19913", "abs": "https://arxiv.org/abs/2601.19913", "authors": ["Shinwoo Park", "Yo-Sub Han"], "title": "From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text", "comment": null, "summary": "Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.", "AI": {"tldr": "The paper shows that with a structured rubric based on Korean writing standards, human experts can be trained over time to reliably distinguish human-written Korean from LLM-generated text, outperforming automated detectors and providing interpretable diagnostic patterns.", "motivation": "Fluent LLM-generated Korean is hard to distinguish from human writing, even for trained linguists, leading to over-trust in surface well-formedness and raising concerns for evaluation, authorship verification, and academic integrity. Existing automatic detectors are limited and often rely on coarse features, especially in non-English languages. The authors want to know whether human detection ability can be systematically improved and calibrated into a learnable, reliable skill using structured guidelines tailored to language-specific properties.", "method": "They design LREAD, a structured rubric grounded in national Korean writing standards but adapted to highlight micro-level cues, such as optional or inconsistent punctuation, spacing habits, and subtle register shifts that LLMs may mishandle. Using this rubric, they run a three-phase longitudinal blind study with Korean linguistics majors. Phase 1 tests baseline intuition-only detection of human vs. LLM texts. Phase 2 requires annotators to use the rubric explicitly, scoring each criterion and providing justifications. Phase 3 tests whether annotators have internalized the rubric by having them apply their now-calibrated expertise to new, held-out elementary essays without further scaffolding. They measure accuracy and inter-annotator agreement across phases and compare human judgments to state-of-the-art LLM detectors, analyzing which features each relies on.", "result": "Majority-vote human accuracy improves drastically across phases, from 60% in intuition-only detection to 100% after rubric-guided calibration, with inter-annotator agreement rising from near-random (Fleiss' kappa -0.09) to very strong (0.82). Calibrated humans attend to fine-grained, language-specific patterns\u2014such as spacing, punctuation, and subtle register inconsistencies\u2014that current automated detectors do not model well. In contrast, state-of-the-art detectors lean more on broad discourse-level priors and fail to capture many of these micro-level artifacts, particularly in Korean. The study also produces a taxonomy of such calibrated detection signatures.", "conclusion": "Expert detection of LLM-generated Korean can be systematically trained and calibrated using a structured rubric aligned with language-specific writing standards, transforming a vague intuition into a reliable, high-agreement skill. Rubric-scaffolded human judgment can not only outperform existing automatic detectors in this non-English context but also offer more interpretable, fine-grained diagnostics. The authors propose that such calibrated human evaluations should complement automated systems, especially in languages where detectors underperform, and make their rubric and detection-signature taxonomy publicly available to support further research and practical deployment."}}
{"id": "2601.19914", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19914", "abs": "https://arxiv.org/abs/2601.19914", "authors": ["Maxwell Crouse", "Ibrahim Abdelaziz", "Kshitij Fadnis", "Siva Sankalp Patel", "Kinjal Basu", "Chulaka Gunasekara", "Sadhana Kumaravel", "Asim Munawar", "Pavan Kapanipathi"], "title": "Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments", "comment": null, "summary": "Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.", "AI": {"tldr": "Introduces DiGiT-TC, a synthetic data generation method for multi-turn tool-calling conversations that does not require a stateful execution environment, yet still yields strong performance on tool-calling benchmarks.", "motivation": "Existing synthetic data frameworks for multi-turn tool calling assume access to a stateful execution environment so they can validate interactions against a known target state. In many real-world scenarios (e.g., secure enterprise environments or tools composed from multiple sources), such a stateful environment is unavailable, making these methods hard to apply. There is a need for a way to generate realistic, valid multi-turn tool-use conversations without relying on explicit environment state tracking.", "method": "The authors propose DiGiT-TC, a synthetic data generation method for tool-calling dialogues. It uses a novel generation pattern in which some tool calls are implicitly represented in the user\u2019s requests instead of explicitly simulated via environment state updates. This pattern is designed so that the resulting conversations mimic those produced via search in a fully stateful environment, but without requiring direct access to or manipulation of an actual execution state. The generated data is then used to train or tune smaller language models for multi-turn tool use.", "result": "When evaluated on standard tool-calling benchmarks, models trained with DiGiT-TC-generated conversations achieve strong performance. Notably, even in settings where a stateful environment is available, the synthetic data from DiGiT-TC leads to substantial performance gains, indicating that the method is competitive with or complementary to traditional stateful-generation approaches.", "conclusion": "DiGiT-TC provides an effective way to generate realistic, multi-turn tool-calling conversation data without needing a stateful execution environment. This broadens the applicability of synthetic data approaches to settings with security or integration constraints and can still improve performance even in fully stateful scenarios."}}
{"id": "2601.19915", "categories": ["cs.CL", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.19915", "abs": "https://arxiv.org/abs/2601.19915", "authors": ["Paul Tarau"], "title": "Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication", "comment": "25 pages", "summary": "We introduce the \\emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \\emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \\emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.\n  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.", "AI": {"tldr": "They propose Arrow Language Models, a new neural architecture for next-token prediction derived from intuitionistic logic, structurally related to multiplicative RNNs and comparable to Transformers and state-space models.", "motivation": "Current dominant language models, particularly Transformers, rely on additive token embeddings and attention, which mix token representations in a largely commutative, order-agnostic way. This makes it conceptually difficult to align sequence processing with rigorous logical or proof-theoretic interpretations. The authors want an architecture where sequence order and the logic of next-token prediction are built into the model\u2019s mathematical foundations, yielding clearer semantics, theoretical guarantees, and potentially better inductive biases than transformer-style attention.", "method": "They reinterpret next-token prediction through intuitionistic implicational logic and the Curry\u2013Howard correspondence: a token prefix is encoded as a left-nested chain of implications, which is inherently non-commutative and preserves word order. Next-token prediction is mapped to modus ponens over this implication chain. From this proof-theoretic setup, they derive a neural architecture equivalent in form to multiplicative RNNs. They validate logical properties of their models using specialized Prolog-based theorem provers, exploring how commutative vs. non-commutative sequencing and single-token vs. multi-token prediction behave. Finally, they propose a practical low-rank neural implementation and contrast the resulting model with Transformers and state-space models.", "result": "They obtain a neural language model architecture that emerges directly from a logic-based, proof-theoretic interpretation of sequence prediction, showing it is structurally equivalent to multiplicative RNNs. Using automated theorem proving, they confirm key logical properties of the architecture, particularly how its non-commutative, implication-chain structure encodes order and how different sequencing and prediction strategies relate. They also design a low-rank parameterization suitable for practical training and situate the model conceptually alongside Transformers and state-space models as an alternative foundation for large language models.", "conclusion": "A logic-driven derivation of neural architectures is feasible and yields a principled alternative to Transformer-based models. By modeling prefixes as non-commutative chains of intuitionistic implications and framing prediction as constructive proof extension, the Arrow Language Model provides a theoretically grounded, multiplicative-RNN-like architecture with explicit order sensitivity. This connects proof theory, automated reasoning, and neural sequence models, and suggests new directions for designing and analyzing foundational language models beyond attention mechanisms."}}
{"id": "2601.19916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19916", "abs": "https://arxiv.org/abs/2601.19916", "authors": ["Songjun Tu", "Yiwen Ma", "Jiahao Lin", "Qichao Zhang", "Xiangyuan Lan", "Junfeng. Li", "Nan Xu", "Linjing Li", "Dongbin Zhao"], "title": "PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review", "comment": null, "summary": "Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.", "AI": {"tldr": "The paper introduces PaperAudit-Bench, a benchmark and framework for making LLM-based paper reviews more critical and evidence-based, particularly in long-context settings.", "motivation": "LLMs can write fluent peer reviews but often miss subtle, distributed, or cross-section errors in papers, leading to reviews that are not sufficiently critical. There is a need for benchmarks and methods that specifically test and improve LLMs\u2019 ability to detect such errors and produce rigorous evaluations in long-context scenarios.", "method": "The authors build PaperAudit-Bench with two main components: (1) PaperAudit-Dataset, an error-annotated dataset containing both local (within-section) and global (cross-section) errors, tailored for controlled long-context evaluation; and (2) PaperAudit-Review, an automated review pipeline that first performs structured error detection and then generates evidence-aware reviews based on the detected issues. They run experiments with different LLMs and detection depths, and also train lightweight error detectors using supervised fine-tuning (SFT) and reinforcement learning (RL) on the dataset.", "result": "Experiments show large differences across models and detection depths in how well they can detect paper errors, underlining the challenge of long-context error detection. Incorporating explicit error detection into the review workflow yields reviews that are more stringent and discriminative than those from existing automated reviewing baselines. Additionally, the dataset can train smaller LLM-based detectors via SFT and RL that perform effective error detection with lower computational cost.", "conclusion": "Integrating structured error detection with review generation leads to more critical, evidence-grounded LLM reviews, and PaperAudit-Bench provides both a challenging benchmark and training resource for improving long-context error detection. This makes automated LLM reviewing more suitable for realistic peer-review scenarios while enabling cost-effective deployment via lightweight detectors."}}
{"id": "2601.19955", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.19955", "abs": "https://arxiv.org/abs/2601.19955", "authors": ["Jean-Marc Fellous", "Gert Cauwenberghs", "Cornelia Ferm\u00fcller", "Yulia Sandamisrkaya", "Terrence Sejnowski"], "title": "NeuroAI and Beyond", "comment": "53 pages, 5 figures, extended appendix", "summary": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.", "AI": {"tldr": "The paper outlines the emerging field of NeuroAI, arguing that tighter integration between neuroscience and artificial intelligence can mutually accelerate progress and offers a structured overview of opportunities and risks.", "motivation": "Despite rapid advances in both neuroscience and AI, their interaction has been limited and fragmented. The authors aim to systematically map where these fields can inform each other more deeply, and to define and motivate a coherent agenda for a new, integrated discipline\u2014NeuroAI\u2014that leverages biological insights to build better AI and uses AI to better understand the brain.", "method": "The paper is based on a focused workshop held in August 2025 that brought together experts across several subfields. It surveys and synthesizes discussions and current literature across embodiment, language and communication, robotics, human and machine learning, and neuromorphic engineering. It also incorporates qualitative data in the form of personal position statements from leading researchers and two SWOT (Strengths, Weaknesses, Opportunities, Threats) analyses contributed by both senior researchers and trainees.", "result": "The authors identify concrete domains where neuroscience is already influencing AI design (e.g., learning algorithms, architectures, neuromorphic hardware) and where AI tools are illuminating neural computation. They distill a set of key opportunities and challenges for cross-fertilization in embodiment, language, robotics, learning, and neuromorphic engineering. The personal statements reveal diverse, sometimes conflicting, visions of NeuroAI, while the SWOT analyses highlight perceived strengths and opportunities (e.g., more data-efficient and robust AI, deeper models of cognition) alongside concerns and risks (e.g., overhyping biological inspiration, ethical and societal impacts).", "conclusion": "The paper concludes that a deliberate, structured integration of neuroscience and AI\u2014termed NeuroAI\u2014could significantly expand the capabilities and efficiency of AI systems and simultaneously reshape our theoretical understanding of biological neural computation. It advocates for sustained interdisciplinary collaboration, careful attention to both benefits and risks, and the development of a shared research agenda that treats neuroscience and AI as mutually informative rather than loosely connected fields."}}
{"id": "2601.19917", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19917", "abs": "https://arxiv.org/abs/2601.19917", "authors": ["Haoyu Zheng", "Yun Zhu", "Yuqian Yuan", "Bo Yuan", "Wenqiao Zhang", "Siliang Tang", "Jun Xiao"], "title": "PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models", "comment": null, "summary": "Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.", "AI": {"tldr": "The paper introduces PILOT, a framework that internalizes planning capabilities into compact LLMs using a hyper-network that generates latent guidance vectors, improving multi-step reasoning without changing backbone weights or adding runtime teacher models.", "motivation": "Compact LLMs struggle with global strategic planning in long-horizon, multi-step reasoning tasks, which causes error propagation. While large teacher models can provide explicit plans that unlock better reasoning, depending on them at inference time is impractical due to latency and availability constraints. The authors aim to give smaller models the benefits of strategic guidance without external teachers at runtime.", "method": "They propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework that adds a lightweight hyper-network on top of an existing LLM. Given a query, this hyper-network produces a Latent Guidance vector that conditions the model\u2019s internal representations. This guidance acts as an internal steering signal, mimicking the strategic oversight a large teacher model would provide, but without modifying the backbone weights. The approach is applied to mathematical and coding reasoning tasks and compared to strong baselines.", "result": "On mathematical and coding benchmarks, PILOT improves the stability and quality of reasoning trajectories in compact LLMs. It achieves consistent performance gains over strong baselines, including a reported +8.9% improvement on the MATH500 benchmark, while adding negligible inference latency due to the lightweight nature of the hyper-network.", "conclusion": "PILOT shows that strategic planning abilities of large models can be effectively internalized into smaller LLMs via latent guidance generated by a hyper-network, without changing their backbone parameters or relying on runtime teacher models. This approach strengthens multi-step reasoning, reduces error propagation in long-horizon tasks, and yields significant performance gains with minimal inference overhead."}}
{"id": "2601.20014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20014", "abs": "https://arxiv.org/abs/2601.20014", "authors": ["Shuhui Qu"], "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning", "comment": null, "summary": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.", "AI": {"tldr": "The paper proposes SQ-BCP, a planning framework that makes LLM-generated plans robust under missing/hidden preconditions by explicitly tracking what is satisfied, violated, or unknown, and resolving unknowns via queries or auxiliary actions, achieving lower constraint violations on WikiHow and RecipeNLG.", "motivation": "LLM-based inference-time planning often assumes complete information; when key preconditions are missing at query time, models hallucinate facts or output plans that break hard constraints (e.g., missing resources, impossible steps). Existing methods overuse heuristic scores, lack guarantees, and handle partial observability poorly. There is a need for a planning method that can explicitly reason about unknown preconditions, actively acquire or construct missing information, and provide formal guarantees about goal compatibility.", "method": "The authors introduce Self-Querying Bidirectional Categorical Planning (SQ-BCP). They represent each precondition with a three-valued status: satisfied (Sat), violated (Viol), or unknown (Unk). When preconditions are unknown, the system either (i) issues targeted self-queries to an external oracle/user to resolve them, or (ii) proposes bridging hypotheses\u2014additional actions that would make the preconditions hold. Planning is done via bidirectional search from both start and goal, and candidate plans are checked by a pullback-based verifier from category theory that serves as a formal certificate of goal compatibility. Distance-based scores are used only for ranking and pruning, not as final correctness criteria. The authors also analyze conditions (bounded branching, finite resolution depth) under which SQ-BCP is complete with respect to finding a compatible plan.", "result": "On WikiHow and RecipeNLG benchmarks where critical preconditions are deliberately withheld to simulate partial observability, SQ-BCP substantially lowers the rate of resource or constraint violations compared with state-of-the-art baselines. Specifically, violation rates drop to 14.9% and 5.8% versus 26.0% and 15.7% for the best competing method, while maintaining similar quality to reference plans. They also prove that if the categorical verifier accepts and deterministic constraint checks pass, then the produced plan is compatible with the goal requirements, and that the search will find such a plan if one exists under the bounded-branching, finite-depth assumptions.", "conclusion": "SQ-BCP provides a principled way to make LLM-based planning robust to missing preconditions by explicitly modeling uncertainty, actively resolving unknowns, and using categorical verification for soundness. It achieves lower violation rates without sacrificing plan quality and comes with theoretical guarantees under reasonable assumptions. This suggests that integrating formal verification and self-querying into LLM planners is an effective strategy for handling partial observability and hard constraints."}}
{"id": "2601.19918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19918", "abs": "https://arxiv.org/abs/2601.19918", "authors": ["Yitong Qiao", "Licheng Pan", "Yu Mi", "Lei Liu", "Yue Shen", "Fei Sun", "Zhixuan Chu"], "title": "Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.", "AI": {"tldr": "They propose a lightweight zero-shot metric (Lowest Span Confidence, LSC) to detect hallucinations in LLM outputs using only a single pass of token probabilities.", "motivation": "Hallucinations\u2014confident but incorrect outputs from LLMs\u2014are problematic for high-stakes uses, yet existing detectors either need many costly samples (self-consistency style) or internal model states, both unrealistic for API-based models. The authors want a practical, efficient detector that works in realistic black-box or semi-black-box settings.", "method": "They introduce Lowest Span Confidence (LSC), which computes joint likelihoods over semantically coherent spans using a sliding window over the output. It scans variable-length n-grams and finds regions with lowest marginal confidence, aiming to capture localized uncertainty that correlates with factual errors. LSC relies only on a single forward pass that yields token probabilities, and is designed to avoid perplexity\u2019s dilution over long sequences and the instability of single minimum-token-probability heuristics.", "result": "Across multiple SOTA LLMs and diverse hallucination benchmarks, LSC consistently beats other zero-shot hallucination detectors, particularly in resource-limited settings (e.g., no intensive sampling, no white-box access).", "conclusion": "LSC is an effective and efficient zero-shot hallucination detection metric that needs only standard output probabilities. By focusing on low-confidence spans, it offers a more robust and practical estimate of factual uncertainty than existing perplexity-based or min-probability methods, making it suitable for deployment with API-based LLMs in constrained environments."}}
{"id": "2601.20021", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20021", "abs": "https://arxiv.org/abs/2601.20021", "authors": ["Shuhui Qu"], "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints", "comment": null, "summary": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.", "AI": {"tldr": "The paper introduces Fuzzy Category-theoretic Planning (FCP), a planning framework that combines category theory with fuzzy logic to handle graded action applicability from natural language while preserving strict constraint checking.", "motivation": "Natural-language planning tasks contain inherently vague, graded predicates (e.g., \"suitable substitute\") that current category-theoretic planners treat as crisp via thresholds. This loses nuance about degrees of suitability and cannot track how plan quality degrades across multiple steps. The authors aim to incorporate graded satisfaction while keeping the compositional and verification benefits of categorical planners.", "method": "They extend category-theoretic planning by annotating each action/morphism with a fuzzy applicability degree in [0,1], composing plan quality using the \u0141ukasiewicz t-norm. Executability remains a hard constraint using pullback-based verification. Graded applicability scores are obtained from a large language model via k-sample median aggregation. For search, they introduce a meeting-in-the-middle strategy that leverages a residuum operator for backward requirement propagation.", "result": "On two benchmarks\u2014(i) standard PDDL3 preference/oversubscription planning benchmarks and (ii) a new RecipeNLG-Subs dataset for missing-ingredient recipe planning with substitutes\u2014FCP improves success rates and reduces hard-constraint violations compared to LLM-only and ReAct-style baselines, while achieving performance comparable to classical PDDL3 planners.", "conclusion": "FCP successfully integrates fuzzy logic with categorical planning to capture graded applicability from language, enables quality-aware multi-step planning without sacrificing strict safety checks, and performs well empirically on both classical preference benchmarks and a new, realistic recipe substitution task."}}
{"id": "2601.19919", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19919", "abs": "https://arxiv.org/abs/2601.19919", "authors": ["Junseok Lee", "Nahoon Kim", "Sangyong Lee", "Chang-Jae Chun"], "title": "FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition", "comment": null, "summary": "Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.", "AI": {"tldr": "The paper proposes Adaptive Self-Knowledge Distillation (ASKD) to compress models while avoiding inheriting teacher errors, and successfully distills Whisper into a smaller, faster FastWhisper model with better WER and 5x faster inference.", "motivation": "Conventional knowledge distillation makes the student mimic the teacher\u2019s predictive distribution, but this can cause the student to inherit the teacher\u2019s weaknesses and hurt generalization. The authors want a distillation approach that preserves compression benefits while improving, not degrading, generalization.", "method": "They introduce Adaptive Self-Knowledge Distillation (ASKD), which dynamically adjusts how much the student depends on the teacher over the course of training, promoting the student\u2019s own self-training. ASKD incorporates self-knowledge distillation techniques so the student can refine its own predictions instead of fully copying the teacher. They apply ASKD to distill the Whisper speech recognition model into a smaller variant, FastWhisper, in a post-training setup.", "result": "Using ASKD, FastWhisper is obtained as a compressed version of Whisper. In experiments, FastWhisper achieves a word error rate that is 1.07% lower than the original Whisper teacher model, while also running with a relative inference speedup of 5x.", "conclusion": "Adaptive Self-Knowledge Distillation can both compress and improve a model: by reducing over-reliance on the teacher and leveraging the student\u2019s own knowledge, it enhances generalization while maintaining or improving accuracy and significantly speeding up inference, as demonstrated by distilling Whisper into FastWhisper."}}
{"id": "2601.20048", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20048", "abs": "https://arxiv.org/abs/2601.20048", "authors": ["Jincheng Bai", "Zhenyu Zhang", "Jennifer Zhang", "Zhihuai Zhu"], "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights", "comment": "Accepted to SIGIR 2025. DOI: 10.1145/3726302.3731959", "summary": "Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.", "AI": {"tldr": "The paper presents Insight Agents (IA), a conversational multi-agent LLM-based system that helps e-commerce sellers, specifically Amazon sellers, get personalized data insights quickly and accurately.", "motivation": "E-commerce sellers struggle to discover, navigate, and effectively use many available tools and the rich but fragmented data they produce. This complexity increases the effort and time needed to make good business decisions, leading to under-utilization of data and slower adoption of tools and programs.", "method": "The authors design an LLM-backed, plan-and-execute, hierarchical multi-agent system called Insight Agents. It has a manager agent and two worker agents (data presentation and insight generation). The manager uses a lightweight encoder-decoder model for out-of-domain detection and a BERT-based classifier for routing queries to the correct worker agent, balancing accuracy and latency. The worker agents use strategic planning over an API-based data model, decomposing user queries into granular sub-queries for better data retrieval and injecting domain knowledge dynamically into the insight generator to improve the quality of insights.", "result": "Deployed to US Amazon sellers, Insight Agents achieves around 90% accuracy based on human evaluation, with 90th-percentile latency under 15 seconds.", "conclusion": "Insight Agents demonstrates that a hierarchical LLM-based multi-agent architecture, combined with lightweight ML models for intent/OOD handling and strategic query decomposition, can provide accurate and timely personalized data insights to e-commerce sellers, reducing their effort and accelerating decision-making in real-world production use."}}
{"id": "2601.19921", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19921", "abs": "https://arxiv.org/abs/2601.19921", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Yizhou Chi", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "title": "Demystifying Multi-Agent Debate: The Role of Confidence and Diversity", "comment": null, "summary": "Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.", "AI": {"tldr": "The paper improves multi-agent debate (MAD) for LLMs by adding diversity-aware initialization and confidence-modulated updates, leading to better performance than vanilla MAD and majority vote on reasoning QA benchmarks.", "motivation": "Vanilla multi-agent debate often fails to outperform simple majority voting despite higher computational cost. Theoretical work shows that with homogeneous agents and uniform belief updates, debate cannot reliably improve correctness. This mismatch between cost and benefit, plus evidence from human deliberation that diversity and confidence sharing matter, motivates designing better MAD protocols.", "method": "The authors identify two missing mechanisms in standard MAD\u2014diverse initial viewpoints and explicit, calibrated confidence communication\u2014by analogy to human deliberation. They introduce (1) a diversity-aware initialization scheme that selects a more diverse set of candidate answers as starting points for agents, increasing the chance that a correct hypothesis is present; and (2) a confidence-modulated debate protocol where agents state calibrated confidence levels and update their beliefs conditioned on others\u2019 expressed confidence. They provide theoretical analysis showing how these interventions change success probabilities and update dynamics, then evaluate on six reasoning-focused QA benchmarks, comparing against vanilla MAD and majority vote.", "result": "Theoretical results show that diversity-aware initialization strictly increases the prior probability that debate contains the correct hypothesis without altering the core update rules, and that confidence-modulated updates allow debate trajectories to systematically drift toward the correct answer under reasonable assumptions. Empirically, across six reasoning QA benchmarks, the proposed methods consistently outperform both vanilla MAD and majority vote, demonstrating improved answer accuracy at test time.", "conclusion": "Incorporating diversity of initial viewpoints and explicit, calibrated confidence sharing into multi-agent debate makes LLM deliberation more effective and reliable. Simple, principled modifications\u2014diversity-aware initialization and confidence-modulated debate\u2014bridge insights from human collective reasoning with LLM-based MAD, yielding higher performance than standard debate and majority voting while maintaining lightweight implementation complexity."}}
{"id": "2601.20090", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20090", "abs": "https://arxiv.org/abs/2601.20090", "authors": ["Amirmohammad Farzaneh", "Salvatore D'Oro", "Osvaldo Simeone"], "title": "Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control", "comment": null, "summary": "Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.", "AI": {"tldr": "They propose a framework that lets users ask \u201cwhat if I had given a different instruction?\u201d in LLM-based control agents, and provide statistical guarantees that the returned counterfactual outcomes contain the true one.", "motivation": "Users of LLM-powered agents that control real systems (e.g., networks) may want to understand what would have happened if they had formulated their high-level intent differently. Naively rerunning the agent with a new prompt does not provide any reliability guarantees and may be unstable or misleading. There is a need for a principled, causally grounded and statistically reliable way to generate and reason about such counterfactual outcomes.", "method": "They model the closed-loop system of user \u2192 LLM-based agent \u2192 environment as a structural causal model (SCM). At test time, they use probabilistic abduction and test-time scaling to generate multiple candidate counterfactual outcomes. Then they apply an offline calibration step using conformal prediction to construct conformal counterfactual generation (CCG), which outputs sets of counterfactual outcomes that have guaranteed coverage, i.e., they contain the true counterfactual outcome with high probability.", "result": "In a wireless network control task, CCG is empirically evaluated and compared against naive re-execution baselines (simply rerunning the agent under alternative user prompts). CCG produces counterfactual sets that are reliably calibrated and demonstrates significant performance advantages, such as better coverage-accuracy trade-offs and more trustworthy counterfactual reasoning about alternative user intents.", "conclusion": "By casting LLM-based control as an SCM and combining probabilistic abduction with conformal prediction, the proposed CCG framework enables counterfactual reasoning about alternative user prompts with formal reliability guarantees. This improves interpretability and trust in agentic LLM systems and outperforms naive re-execution approaches, as shown in a wireless network control case study."}}
{"id": "2601.19922", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19922", "abs": "https://arxiv.org/abs/2601.19922", "authors": ["Laya Iyer", "Kriti Aggarwal", "Sanmi Koyejo", "Gail Heyman", "Desmond C. Ong", "Subhabrata Mukherjee"], "title": "HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue", "comment": null, "summary": "Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.", "AI": {"tldr": "Introduces HEART, a framework to directly compare human and LLM performance in multi-turn emotional-support conversations using shared dialogues and standardized evaluation.", "motivation": "Existing LLM benchmarks largely test reasoning or linguistic fluency, not interpersonal skills like empathy, emotion reading, or handling resistance and distress. There is no clear, empirical way to compare human and model performance in supportive conversation, nor to understand where models match or diverge from human social judgment.", "method": "Create HEART, a human\u2013LLM comparison framework using identical multi-turn emotional-support dialogues. For each dialogue history, pair a human response with an LLM response and have them evaluated side-by-side by blinded human raters and an ensemble of LLM-as-judge evaluators. Use a rubric grounded in interpersonal communication science spanning five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. Analyze agreement patterns, strengths, and weaknesses across humans and various models.", "result": "Frontier LLMs reach or exceed average human performance on perceived empathy and response consistency. However, humans still outperform models on adaptive reframing, explicitly naming tensions, and subtle tone modulation, especially in adversarial conversation turns. Human raters and LLM-as-judge evaluators agree on about 80% of pairwise human\u2013model comparisons, which is comparable to agreement between human raters themselves, and their textual justifications emphasize similar rubric dimensions.", "conclusion": "Supportive dialogue skill constitutes a distinct capability axis, separable from general reasoning or language fluency. HEART offers a unified, empirically grounded way to locate both humans and models along this axis, clarifying where model-generated support aligns with or diverges from human social judgment. The observed human\u2013LLM convergence in evaluation criteria and the scaling of affective conversational competence with model size provide a basis for systematically tracking and improving LLMs\u2019 interpersonal capabilities."}}
{"id": "2601.20206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20206", "abs": "https://arxiv.org/abs/2601.20206", "authors": ["Zixuan Xiao", "Chunguang Hu", "Jun Ma"], "title": "Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis", "comment": null, "summary": "As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.", "AI": {"tldr": "The paper presents a multi-modal LLM agent framework to monitor the development of newly constructed urban parks using remote sensing and other data, achieving more intelligent, reliable, and scalable analysis than traditional change detection methods.", "motivation": "Traditional remote sensing change detection techniques cannot provide high-level semantic understanding, flexible reasoning, or intelligent analysis needed for current urban park planning, evaluation, and management, particularly when dealing with complex, multi-modal data and diverse application scenarios. There is a need for a more advanced framework that can semantically interpret, align, and reason over heterogeneous data sources to better support urbanization and resource allocation decisions.", "method": "The authors design a multi-modal large language model (LLM) agent framework for urban park development monitoring. The framework introduces a general horizontal and vertical multi-modal data alignment mechanism to keep heterogeneous data consistent and traceable over time and across sources. Additionally, they build a domain-specific toolkit integrated with the LLM to reduce hallucinations by supplying specialized knowledge and tools. The agent performs multi-modal information fusion, semantic understanding, and reasoning over urban park monitoring data and is evaluated against vanilla GPT-4o and other agent baselines.", "result": "Compared with vanilla GPT-4o and other LLM agent approaches, the proposed framework demonstrates more robust multi-modal information fusion and analysis capabilities. It delivers more reliable monitoring results for newly constructed parks and better supports complex urban park development monitoring tasks, with improved robustness, flexibility, and scalability across different application scenarios.", "conclusion": "A specialized multi-modal LLM agent with carefully designed data alignment mechanisms and a domain-specific toolkit can significantly enhance monitoring of newly built urban parks versus traditional remote sensing change detection and generic LLM agents. This approach offers a scalable, reliable solution for urban planning stakeholders, enabling higher-level semantic analysis, reduced hallucination, and better support for evolving requirements in urban park development monitoring."}}
{"id": "2601.19923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19923", "abs": "https://arxiv.org/abs/2601.19923", "authors": ["Boxiang Zhao", "Qince Li", "Zhonghao Wang", "Zelin Cao", "Yi Wang", "Peng Cheng", "Bo Lin"], "title": "Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation", "comment": null, "summary": "As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.", "AI": {"tldr": "The paper introduces Table-BiEval, a human-free, self-supervised framework to evaluate how well LLMs translate natural language and tables into structured, code-like formats.", "motivation": "Existing LLM evaluations poorly capture structural fidelity in code-like or tabular outputs, and human assessment is expensive and unscalable. There is a need for an automatic, reliable metric to quantify whether LLMs preserve both semantics and structure when generating structured specifications or tool calls.", "method": "The authors design Table-BiEval, which uses deterministic Intermediate Representations (IRs) to separate content from structure. They define two metrics: Content Semantic Accuracy to evaluate information correctness, and Normalized Tree Edit Distance to assess structural similarity. They then benchmark 15 state-of-the-art LLMs on tasks covering hierarchical structures and flat tables using this IR-based, self-supervised evaluation pipeline.", "result": "Empirical evaluation of 15 LLMs shows large performance differences. Some mid-sized models outperform larger ones in structural efficiency, and all models struggle particularly with deeply nested, recursively structured data. The metrics successfully expose these structural weaknesses without human annotation.", "conclusion": "Table-BiEval provides an automated, self-supervised way to quantify structural fidelity in LLM outputs over hierarchical and tabular formats. It reveals that bigger models are not always better at structural tasks and that deep recursive nesting is a consistent limitation across current LLM architectures, guiding future model and benchmark design."}}
{"id": "2601.20221", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20221", "abs": "https://arxiv.org/abs/2601.20221", "authors": ["Hang Zhang", "Ruheng Wang", "Yuelyu Ji", "Mingu Kwak", "Xizhi Wu", "Chenyu Li", "Li Zhang", "Wenqi Shi", "Yifan Peng", "Yanshan Wang"], "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning", "comment": null, "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.", "AI": {"tldr": "The paper proposes an agentic, tool-augmented verifier for medical reasoning traces that iteratively retrieves external evidence and is trained with reinforcement learning, yielding much more accurate and sample-efficient verification than standard reward models.", "motivation": "Although large language models perform well on medical benchmarks, using them in real clinical contexts requires high factual reliability. Current reward-model-based verifiers are limited: they only output scalar scores without interpretable justification, and they depend on one-shot retrieval, so they cannot adapt their evidence gathering as verification proceeds. A better verification approach is needed that both grounds decisions in explicit, dynamically retrieved evidence and can scale without fine-grained supervision.", "method": "The authors introduce an agentic verification framework (\\method) in which a verifier model evaluates medical reasoning traces by iteratively querying external medical corpora. At each step, the verifier can decide what information to retrieve next and refine its judgment based on new evidence. Training uses an iterative reinforcement learning setup with only trace-level supervision, avoiding the need for token-level labels. They add an adaptive curriculum that adjusts the mix and difficulty of training examples to improve learning efficiency and stability.", "result": "On four medical reasoning benchmarks, the proposed verifier substantially outperforms prior methods. Relative to the base generator, it boosts MedQA accuracy by 23.5% and MedXpertQA by 32.0%. Compared with existing reward model baselines, it achieves similar or better verification quality while requiring an 8\u00d7 smaller sampling budget, demonstrating significant gains in sample efficiency.", "conclusion": "Dynamically evidence-grounded verification\u2014via an agentic verifier that can iteratively retrieve and reason over external medical knowledge\u2014leads to more accurate and resource-efficient checking of medical reasoning traces than conventional scalar reward models. This establishes a promising direction for building more trustworthy medical reasoning systems suitable for safety-critical settings."}}
{"id": "2601.19924", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19924", "abs": "https://arxiv.org/abs/2601.19924", "authors": ["Yitian Chen", "Cheng Cheng", "Yinan Sun", "Zi Ling", "Dongdong Ge"], "title": "OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \\textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.", "AI": {"tldr": "OPT-ENGINE is a benchmark to systematically test how well LLMs model and solve increasingly complex optimization problems, revealing that tool-integrated LLMs are more robust and that constraint formulation is the main bottleneck.", "motivation": "Existing work shows LLMs can help with optimization modeling, but current benchmarks are limited in difficulty and scope, and we do not clearly know how well these models generalize to harder, more realistic operations research tasks or where in the end-to-end pipeline they fail. The authors want a principled way to probe the limits and failure modes of LLMs in optimization.", "method": "The authors design OPT-ENGINE, an extensible benchmark framework covering 10 canonical operations research tasks (5 LP and 5 MIP) with controllable and scalable difficulty. They use it to run systematic experiments on various LLMs under different reasoning settings (pure text vs tool-integrated with external solvers), and decompose the optimization workflow into stages: understanding the problem, formulating models (especially constraints), and generating/solving solutions, to pinpoint where performance breaks down as task complexity increases and distribution shifts occur.", "result": "Experiments show that as optimization tasks become more complex and out-of-distribution, LLMs that rely only on pure-text reasoning hit a performance ceiling and degrade quickly, while models that are integrated with external optimization solvers remain more robust. Analysis across the workflow indicates that the dominant failure point is in correctly and completely formulating constraints, not in solving once a correct model is available.", "conclusion": "OPT-ENGINE offers a scalable, controllable benchmark for studying LLMs in optimization modeling and reveals that integrating LLMs with external tools is crucial for robustness on challenging tasks, while automated constraint formulation is the main bottleneck. These insights suggest that future research should prioritize better modeling interfaces, representations, and training strategies focused on constraint formulation rather than only improving textual reasoning or solver calls."}}
{"id": "2601.20305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20305", "abs": "https://arxiv.org/abs/2601.20305", "authors": ["Zhenchen Tang", "Songlin Yang", "Zichuan Wang", "Bo Peng", "Yang Li", "Beibei Dong", "Jing Dong"], "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models", "comment": null, "summary": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.", "AI": {"tldr": "The paper introduces SEER, a training framework that lets a unified multimodal model use its own understanding to actively steer generation through an endogenous reprompting loop, improving evaluation accuracy, reasoning, and output quality with minimal task-specific data.", "motivation": "Although unified multimodal models can understand inputs well, that understanding does not reliably translate into better generation. There is a \"cognitive gap\" between comprehension and the ability to improve one\u2019s own responses. The authors aim to transform the model\u2019s internal, passive representations into explicit, actionable guidance that can refine generation in real time, using only a small amount of additional data and without harming general multimodal performance.", "method": "They propose Endogenous Reprompting: during generation, the model produces self-aligned descriptors that serve as explicit reasoning steps to guide its own next outputs. To train this behavior, they design SEER (Self-Evolving Evaluator and Reprompter), a two-stage reinforcement learning framework based on a small proxy task called Visual Instruction Elaboration. Stage 1 (RLVR: Reinforcement Learning with Verifiable Rewards) uses curriculum learning to unlock and strengthen the model\u2019s latent evaluation ability, creating a reliable internal reward signal. Stage 2 (RLMT: Reinforcement Learning with Model-rewarded Thinking) optimizes the generative reasoning policy itself, using the internally generated reward to shape how the model performs endogenous reprompting and descriptor generation.", "result": "Models trained with SEER achieve higher evaluation accuracy, more efficient and effective reprompting behavior, and better overall generation quality compared to state-of-the-art baselines. These gains are obtained using only about 300 samples from the proxy task and do not degrade the model\u2019s broad multimodal capabilities.", "conclusion": "By converting a multimodal model\u2019s implicit understanding into explicit, reward-guided internal reasoning via endogenous reprompting, SEER closes part of the cognitive gap between understanding and generation. The approach shows that a model can self-evaluate and self-improve its generation process with minimal supervision, enhancing reasoning and output quality while preserving general capability."}}
{"id": "2601.19925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19925", "abs": "https://arxiv.org/abs/2601.19925", "authors": ["Yinuo Liu", "Emre Sezgin", "Eric A. Youngstrom"], "title": "Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study", "comment": "17 pages, 4 figures, 2 tables", "summary": "Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.", "AI": {"tldr": "The paper evaluates how consistently and reliably three large language models (ChatGPT-5, Gemini-3-Pro, Claude-Sonnet-4.5) can grade scientific abstracts compared with human reviewers, finding moderate alignment overall, stronger on objective criteria than subjective ones.", "motivation": "As academic conferences and journals face growing submission volumes, human-only peer review becomes time-consuming and resource-intensive. Organizers need scalable ways to screen and triage submissions without sacrificing fairness or quality. Large language models can rapidly process and rate text, but it is unclear whether their evaluations align sufficiently with expert human judgment, especially for nuanced scientific content. This paper aims to determine whether LLMs can be trusted to assist in abstract assessment and under what conditions they might complement human reviewers rather than replace them.", "method": "The authors collected 160 abstracts from a local scientific conference. Each abstract was scored by fourteen human reviewers and three LLMs (ChatGPT-5, Gemini-3-Pro, Claude-Sonnet-4.5) using the same multi-criterion rubric, which included both objective (clarity, objectives, results) and more subjective (impact, engagement, applicability) dimensions plus an overall impression/composite score. They compared score distributions across all raters and computed intraclass correlation coefficients (ICCs) to quantify: (1) agreement among the AIs themselves (within-AI reliability) and (2) agreement between each AI and the pooled human reviewers (AI-human concordance) across rubric dimensions. Bland\u2013Altman plots were used to visually inspect agreement patterns and systematic scoring biases between AI and humans, including mean differences in composite scores.", "result": "The three LLMs showed good-to-excellent mutual agreement, with ICCs from 0.59 to 0.87, indicating consistent application of the rubric across models. Relative to human reviewers, ChatGPT-5 and Claude-Sonnet-4.5 achieved moderate agreement (ICCs around 0.45\u20130.60) on overall composite scores and on more objective or content-focused criteria such as impression, clarity, objectives, and results. For subjective aspects\u2014impact, engagement, and applicability\u2014their agreement with humans dropped to the fair range, with ICCs roughly 0.23\u20130.38. Gemini-3-Pro performed less well, showing only fair agreement on about half the rubric dimensions and essentially no reliability on impact and applicability. Mean composite score differences between each LLM and the human average were small: ChatGPT-5 scored slightly higher than humans (+0.24), Gemini moderately higher (+0.42), and Claude was nearly identical (-0.02), suggesting minimal systematic bias at the composite level.", "conclusion": "LLMs can reliably and consistently apply a structured scoring rubric to large batches of scientific abstracts, achieving moderate alignment with human experts on overall quality and relatively objective criteria. Their performance is weaker and less trustworthy on more subjective or interpretive dimensions such as perceived impact and engagement. Consequently, LLMs are promising tools for scalable, preliminary screening or standardization of objective rubric components but should not replace human reviewers. Instead, they are best positioned to complement expert judgment within a hybrid review workflow where humans retain responsibility for nuanced, subjective evaluation and final decisions."}}
{"id": "2601.20323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20323", "abs": "https://arxiv.org/abs/2601.20323", "authors": ["Hyunseung Chung", "Jungwoo Oh", "Daeun Kyung", "Jiho Kim", "Yeonsu Kwon", "Min-Gyu Kim", "Edward Choi"], "title": "ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue", "comment": "Accepted to ICASSP 2026 (5 pages, 2 figures, 5 tables)", "summary": "Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.", "AI": {"tldr": "The paper proposes ECG-Agent, an LLM-based tool-using agent designed for multi-turn dialogue about ECGs and introduces a new ECG-Multi-Turn-Dialogue dataset. ECG-Agent outperforms existing ECG-focused LLMs and demonstrates that small, on-device agents can approach the performance of larger models.", "motivation": "Existing multimodal LLMs have been extended to ECGs but are limited to simpler tasks such as classification, report generation, or single-turn question answering. These models do not match real-world clinical interactions, which are multi-turn, require precise understanding of ECG measurements (like PQRST intervals), and often need to run efficiently on local or edge devices. The authors aim to bridge this gap by enabling conversational, tool-augmented, and efficient ECG reasoning.", "method": "The authors design ECG-Agent, an LLM-based agent framework that can call specialized tools to analyze ECGs and handle multi-turn dialogue. They also build ECG-MTD, a dataset of realistic user-assistant multi-turn conversations covering diverse ECG lead configurations. They implement ECG-Agent at multiple model sizes, including small on-device-capable versions and larger server-scale versions, and evaluate them on response accuracy, tool-calling behavior, and hallucination metrics against baseline ECG-LLMs.", "result": "Across experiments, ECG-Agent consistently delivers more accurate responses than baseline ECG-LLMs on ECG-related tasks. Smaller, on-device ECG-Agents perform comparably to larger models on several evaluation dimensions, including accuracy, correct use of tools, and reduced hallucinations.", "conclusion": "An LLM-based tool-calling agent architecture is effective for multi-turn ECG dialogue, and can be scaled down to on-device sizes without major performance loss. The ECG-Agent framework and the ECG-MTD dataset together provide a foundation for more realistic, interactive, and deployable ECG AI assistants suitable for clinical or near-clinical use."}}
{"id": "2601.19926", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19926", "abs": "https://arxiv.org/abs/2601.19926", "authors": ["Nora Graichen", "Iria de-Dios-Flores", "Gemma Boleda"], "title": "The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models", "comment": null, "summary": "We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.", "AI": {"tldr": "Systematic review of 337 papers on syntactic evaluation of Transformer LMs; finds good performance on surface/form-oriented syntax but weaker, more variable results on syntax\u2013semantics interface phenomena, and recommends broader, more standardized, and more mechanistic future work.", "motivation": "Transformer language models (TLMs) are widely used and often assumed to have strong syntactic capabilities, but evidence is scattered across many studies with heterogeneous tasks, datasets, and analysis methods. The authors aim to consolidate and critically assess what is known about TLMs\u2019 syntactic abilities, identify gaps and biases in the literature (e.g., language and model focus), and provide guidance for more informative future research.", "method": "They conduct a systematic review of 337 articles, aggregating 1,015 reported model results. The studies cover a variety of syntactic phenomena (e.g., part of speech, agreement, binding, filler-gap dependencies) and employ both behavioral and interpretability/mechanistic methods. The authors categorize work by language, model type, syntactic phenomenon, and method type, then compare patterns of performance and coverage across these dimensions.", "result": "The literature uses a diverse set of methods and datasets, but is heavily skewed toward English, BERT, and relatively accessible, form-oriented syntactic phenomena such as part-of-speech tagging and agreement. Across studies, TLMs generally perform well on these surface syntactic tasks but show weaker and more inconsistent performance on more complex syntax\u2013semantics interface phenomena, including binding and filler-gap dependencies. Mechanistic interpretability methods are used less frequently than task-based evaluations.", "conclusion": "Current research provides partial but biased evidence of TLMs\u2019 syntactic competence: they seem strong on surface and form-based syntax but less reliable on deeper, interface-level phenomena. The authors recommend that future work (1) report complete results rather than selective subsets, (2) better align linguistic theory and experimental methodology across studies, (3) expand the use of mechanistic interpretability approaches, and (4) broaden empirical coverage to more languages and a wider range of syntactic and syntax\u2013semantic phenomena."}}
{"id": "2601.20352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20352", "abs": "https://arxiv.org/abs/2601.20352", "authors": ["Weiquan Huang", "Zixuan Wang", "Hehai Lin", "Sudong Wang", "Bo Xu", "Qian Li", "Beier Zhu", "Linyi Yang", "Chengwei Qin"], "title": "AMA: Adaptive Memory via Multi-Agent Collaboration", "comment": "8 pages", "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.", "AI": {"tldr": "The paper introduces AMA, a multi-agent, hierarchical memory framework for LLM agents that adaptively manages memory granularity, improves retrieval precision, maintains consistency, and cuts token usage by ~80% while outperforming SOTA on long-context benchmarks.", "motivation": "Existing LLM memory systems use rigid retrieval granularity, indiscriminate memory accumulation, and coarse update rules. This causes a mismatch between stored information and the specific reasoning needs of tasks, and leads to growing logical inconsistencies in long-term interactions. The authors aim to design a more adaptive, consistent, and efficient memory mechanism for LLM agents.", "method": "They propose AMA (Adaptive Memory via Multi-Agent Collaboration), a hierarchical memory framework where multiple specialized LLM agents cooperate: (1) a Constructor builds memory at multiple granularities, (2) a Retriever adaptively routes queries and selects appropriate granularity, (3) a Judge checks the relevance and logical consistency of retrieved memories, possibly triggering iterative retrieval or updates, and (4) a Refresher enforces consistency by updating or deleting conflicting or outdated entries. The system dynamically aligns retrieval granularity with task complexity to improve reasoning and efficiency.", "result": "On demanding long-context benchmarks, AMA achieves significantly better performance than state-of-the-art memory and context-extension baselines, while reducing token usage by about 80% compared to full-context approaches. This shows both improved retrieval precision and better long-term consistency with substantially lower computational cost.", "conclusion": "Coordinated multi-agent control over hierarchical memory provides a more adaptive and reliable memory system for LLM agents. By adjusting granularity, verifying consistency, and refreshing outdated or conflicting information, AMA offers a scalable way to support complex long-term reasoning with high accuracy and much lower token consumption than full-context methods."}}
{"id": "2601.19927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19927", "abs": "https://arxiv.org/abs/2601.19927", "authors": ["Yuqing Zhao", "Ziyao Liu", "Yongsen Zheng", "Kwok-Yan Lam"], "title": "Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey", "comment": null, "summary": "Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.", "AI": {"tldr": "Survey of attribution-based methods to reduce hallucinations in RAG-based QA with a taxonomy, unified pipeline, technique review, and practical guidelines.", "motivation": "LLM-based QA systems hallucinate, and while RAG reduces this by using external documents, it also introduces new hallucination modes from retriever\u2013generator interactions. There is no unified view, taxonomy, or systematic comparison of attribution-based techniques to ensure responses are grounded in retrieved content, making it hard to diagnose failures and pick suitable methods.", "method": "Conduct a survey of existing attribution-based hallucination mitigation methods in RAG systems. Define and organize hallucination types into a taxonomy, construct a unified pipeline where attribution methods can be located, categorize existing techniques according to the hallucinations they tackle, and analyze them comparatively in terms of behavior, suitability, and trade-offs.", "result": "The paper proposes a structured taxonomy of hallucination types specific to RAG, a unified pipeline describing where attribution fits into RAG workflows, and a categorized review of existing attribution-based methods aligned with those hallucination categories. It summarizes their capabilities, limitations, and contexts of use.", "conclusion": "Attribution-based techniques are a key tool for mitigating hallucinations in RAG QA systems but come with diverse designs and trade-offs. The presented taxonomy, pipeline, and comparative analysis clarify their roles, highlight failure modes, and provide practical guidance, laying a foundation for better deployment and future research on robust, grounded RAG systems."}}
{"id": "2601.20379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20379", "abs": "https://arxiv.org/abs/2601.20379", "authors": ["Zhengbo Jiao", "Hongyu Xian", "Qinglong Wang", "Yunpu Ma", "Zhebo Wang", "Zifan Zhang", "Dezhang Kong", "Meng Han"], "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution", "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.", "AI": {"tldr": "They propose Policy of Thoughts (PoT), which turns reasoning into an online learning process that adapts the model\u2019s policy per instance, greatly improving long-horizon reasoning performance.", "motivation": "LLMs perform poorly on complex, long-horizon reasoning tasks because their policy is frozen at test time. Existing test-time scaling techniques only use execution feedback to rank or modify outputs, without actually updating the model\u2019s reasoning strategy on the fly. The authors aim to enable LLMs to learn from their own failed attempts during inference, making reasoning more stable and effective.", "method": "They introduce Policy of Thoughts (PoT), which frames reasoning as within-instance online optimization. For each problem, the model first explores by generating diverse candidate solutions. Then, using execution feedback (e.g., whether code runs or answers are correct), they apply Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter attached to the base model. This adapter is updated only during the current instance, forming a closed feedback loop that refines the policy in real time without permanently changing the base model.", "result": "Experiments demonstrate that PoT significantly improves reasoning performance, especially on code benchmarks. A 4B-parameter model with PoT reaches 49.71% accuracy on LiveCodeBench, surpassing much larger models such as GPT-4o and DeepSeek-V3, even though the PoT model is more than 50 times smaller.", "conclusion": "Online, instance-specific policy adaptation during inference can substantially enhance LLM reasoning, overcoming limitations of frozen policies and static test-time scaling. PoT\u2019s closed-loop learning framework shows that integrating execution feedback directly into transient policy updates yields large performance gains, suggesting a promising direction for more adaptive and efficient reasoning systems."}}
{"id": "2601.19928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19928", "abs": "https://arxiv.org/abs/2601.19928", "authors": ["Yi Hu", "Jiaqi Gu", "Ruxin Wang", "Zijun Yao", "Hao Peng", "Xiaobao Wu", "Jianhui Chen", "Muhan Zhang", "Liangming Pan"], "title": "Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures", "comment": null, "summary": "Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.", "AI": {"tldr": "Survey of mechanistic understanding of Large Reasoning Models (LRMs) trained with reinforcement learning.", "motivation": "Although LRMs show strong reasoning performance, their internal mechanisms remain poorly understood, and there is a need to move from black-box evaluation to mechanistic transparency.", "method": "The paper conducts a comprehensive survey of recent work and organizes existing mechanistic findings about LRMs into three dimensions: training dynamics, reasoning mechanisms, and unintended behaviors. It also synthesizes insights from these works and identifies open challenges.", "result": "The survey systematizes current knowledge about how LRMs learn and reason, and how unintended behaviors emerge, offering a structured view of the field and highlighting gaps in current understanding.", "conclusion": "The authors conclude that deeper mechanistic understanding of LRMs is essential, emphasizing the importance of applied interpretability, better methodological tools, and the development of a unified theoretical framework, and they propose a roadmap for future research in these directions."}}
{"id": "2601.20380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20380", "abs": "https://arxiv.org/abs/2601.20380", "authors": ["Le Zhang", "Yixiong Xiao", "Xinjiang Lu", "Jingjia Cao", "Yusai Zhao", "Jingbo Zhou", "Lang An", "Zikan Feng", "Wanxiang Sha", "Yu Shi", "Congxi Xiao", "Jian Xiong", "Yankai Zhang", "Hua Wu", "Haifeng Wang"], "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution", "comment": null, "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.", "AI": {"tldr": "OmegaUse is a general-purpose GUI agent model that autonomously operates mobile and desktop interfaces using high-quality curated and synthetic data plus a two-stage SFT+GRPO training scheme, achieving SOTA results on multiple GUI benchmarks and a new cross-OS benchmark, OS-Nav.", "motivation": "Existing GUI agents show promise but struggle with generality across platforms, limited high-quality interaction data, and suboptimal training paradigms for robust spatial grounding and planning. There is also a lack of comprehensive cross-terminal benchmarks to evaluate GUI agents on both mobile and desktop operating systems in a unified way. The authors aim to build a scalable, high-performing GUI agent that can operate across different devices and OSs while being efficiently trainable and rigorously evaluated.", "method": "The authors design OmegaUse, a GUI agent with a Mixture-of-Experts backbone to balance compute efficiency and reasoning capacity. They build a data-construction pipeline that combines curated open-source GUI datasets with a new automated synthesis framework: bottom-up autonomous exploration of interfaces paired with top-down taxonomy-guided task generation to create high-fidelity synthetic trajectories. Training is decoupled into two stages: (1) Supervised Fine-Tuning (SFT) to learn core GUI interaction syntax and basic behaviors; (2) Group Relative Policy Optimization (GRPO), a reinforcement-style optimization method, to refine spatial grounding and long-horizon sequential planning. For evaluation, they introduce OS-Nav, a cross-terminal benchmark suite consisting of ChiM-Nav for Chinese Android environments and Ubu-Nav for Ubuntu desktop workflows, and also evaluate on existing GUI benchmarks.", "result": "OmegaUse achieves state-of-the-art performance on several GUI benchmarks: 96.3% on ScreenSpot-V2 and a 79.1% step success rate on AndroidControl, surpassing prior systems. On the newly proposed OS-Nav benchmark, OmegaUse obtains 74.24% step success on the ChiM-Nav (Chinese Android) subset and an average of 55.9% success on the Ubu-Nav (Ubuntu desktop) subset, demonstrating strong cross-terminal and cross-OS capabilities.", "conclusion": "The study concludes that combining carefully engineered data generation (curated plus synthetic) with a decoupled SFT+GRPO training paradigm on a Mixture-of-Experts backbone yields a strong, general-purpose GUI agent for both mobile and desktop platforms. OmegaUse not only sets new SOTA performance on existing benchmarks but also performs well on the newly introduced OS-Nav suite, indicating robust cross-OS generalization. The authors position OmegaUse and OS-Nav as a foundation for future research on scalable, high-performance GUI agents capable of real-world computer and phone use tasks."}}
{"id": "2601.19929", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19929", "abs": "https://arxiv.org/abs/2601.19929", "authors": ["David Linus Ostby"], "title": "Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding", "comment": "28 pages, 10 tables, 2 figures and 6 appendices", "summary": "We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.", "AI": {"tldr": "Stingy Context is a hierarchical tree-based compression method that shrinks LLM coding contexts by about 18x while maintaining performance.", "motivation": "LLMs used for auto-coding often need to process very large codebases, but context-window limits and cost make feeding full source impractical, and naive truncation or flat summarization harms performance and causes lost-in-the-middle issues.", "method": "They propose Stingy Context, which represents a large codebase as a hierarchical tree of fragments; using a decomposition approach called TREEFRAG, they compress source files into structured, selective representations that keep key information while aggressively reducing tokens, then feed this compact tree to LLMs for coding tasks.", "result": "On a 239k-token real codebase they compress to 11k tokens (about 18:1) while still preserving the necessary information for tasks; across 12 state-of-the-art frontier LLMs on 40 real-world GitHub issues, their method attains 94\u201397% task success at low inference cost and outperforms non-hierarchical (flat) context-reduction baselines, particularly reducing lost-in-the-middle failures.", "conclusion": "Hierarchical tree-based context compression like Stingy Context can dramatically shrink required LLM context for software-engineering tasks without sacrificing effectiveness, offering a practical way to scale auto-coding on large repositories and outperforming simpler context-selection approaches."}}
{"id": "2601.20467", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20467", "abs": "https://arxiv.org/abs/2601.20467", "authors": ["Zhenxuan Fan", "Jie Cao", "Yang Dai", "Zheqi Lv", "Wenqiao Zhang", "Zhongle Xie", "Peng LU", "Beng Chin Ooi"], "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning", "comment": "16 pages, 9 figures, 11 tables", "summary": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.", "AI": {"tldr": "The paper introduces CtrlCoT, a framework to compress chain-of-thought reasoning in LLMs while preserving or improving accuracy, reducing token usage by about 30% and outperforming existing compression methods on math reasoning tasks.", "motivation": "Chain-of-thought prompting improves reasoning but produces long, verbose traces that increase latency and memory costs. Existing compression methods either operate at a high semantic level and are overly conservative, or do token pruning aggressively and risk removing critical reasoning cues, which hurts accuracy. Furthermore, combining semantic abstraction with token-level pruning is difficult due to dependencies between steps, task-agnostic pruning behavior, and mismatches between training and inference distributions. The paper aims to address these limitations to achieve efficient yet reliable CoT compression.", "method": "The authors propose CtrlCoT, a dual-granularity CoT compression framework with three main components: (1) Hierarchical Reasoning Abstraction, which generates CoTs at multiple semantic granularities to allow flexible abstraction of reasoning steps; (2) Logic-Preserving Distillation, which trains a logic-aware token pruner that specifically learns to keep indispensable reasoning elements such as numbers, operators, and key symbols across a range of pruning ratios; and (3) Distribution-Alignment Generation, which ensures that the compressed CoTs remain fluent and stylistically consistent with the model\u2019s inference-time reasoning patterns, mitigating fragmentation and mismatch between training and deployment conditions.", "result": "On the MATH-500 benchmark using the Qwen2.5-7B-Instruct model, CtrlCoT achieves a 30.7% reduction in token usage for chain-of-thought traces while also improving accuracy by 7.6 percentage points over the strongest baseline method for CoT compression. This demonstrates that the framework can simultaneously enhance efficiency and reasoning reliability.", "conclusion": "CtrlCoT effectively reconciles semantic-level reasoning abstraction with token-level pruning, enabling substantial CoT compression without sacrificing correctness and even improving performance over prior approaches. By leveraging hierarchical abstraction, logic-aware distillation, and distribution-aligned generation, the framework provides a more efficient and robust way to deploy chain-of-thought reasoning in large language models, particularly for mathematically intensive tasks."}}
{"id": "2601.19930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19930", "abs": "https://arxiv.org/abs/2601.19930", "authors": ["Jacob Nielsen", "Stine L. Beltoft", "Peter Schneider-Kamp", "Lukas Galke Poech"], "title": "SDUs DAISY: A Benchmark for Danish Culture", "comment": "Danish Culture Benchmark, 2 Tables, 1 Figure demonstrating the data curation pipeline", "summary": "We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.", "AI": {"tldr": "The paper presents Daisy, a benchmark dataset for Danish cultural heritage based on the Danish Culture Canon 2006, consisting of 741 validated close-ended QA pairs generated from Wikipedia pages about canonical cultural artifacts.", "motivation": "There is a lack of NLP benchmarks that comprehensively capture and evaluate knowledge of Danish cultural heritage. Existing resources may focus on mainstream or surface-level knowledge and not on the canonically defined, in-depth cornerstones of a culture. The authors aim to provide a high-quality, culturally grounded benchmark to evaluate language models' knowledge and understanding of Danish culture as defined by experts.", "method": "The authors start from the curated topics in the Danish Culture Canon 2006. For each artifact in the canon, they retrieve the corresponding Wikipedia page. A language model is then used to generate random questions about that page, producing a mixture of central and peripheral questions for each work. The resulting question\u2013answer pairs are then manually checked, approved, or corrected by humans, yielding a final dataset of close-ended questions and answers. The benchmark spans a wide historical range of cultural artifacts, from ancient archaeological findings to modern music, design and architecture.", "result": "The outcome is the Daisy dataset: 741 human-validated, close-ended question\u2013answer pairs covering Danish cultural heritage items across time and domains, including archaeological artifacts (as early as 1300 BC), 18th-century poems and musical pieces, and contemporary pop music, design, and architecture. The questions probe both mainstream facts and more in-depth, defining aspects of each canonical work.", "conclusion": "Daisy provides a focused, high-quality benchmark for assessing language models' knowledge of Danish cultural heritage as defined by the Danish Culture Canon 2006. By combining LLM-generated questions with human validation, and by sampling both central and peripheral aspects of canonical works across a broad historical span, the dataset can support more nuanced evaluation of cultural knowledge in NLP systems."}}
{"id": "2601.20487", "categories": ["cs.AI", "cs.GT", "cs.HC", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.20487", "abs": "https://arxiv.org/abs/2601.20487", "authors": ["Nico Mutzner", "Taha Yasseri", "Heiko Rauhut"], "title": "Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups", "comment": null, "summary": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.", "AI": {"tldr": "The paper experimentally studies how adding an AI (or AI-framed) agent to small groups affects the formation and stability of cooperative norms, finding that people cooperate based on group behavior rather than whether a partner is labeled human or AI.", "motivation": "Most human-AI cooperation research examines one-on-one interactions, leaving a gap in understanding how AI agents affect social norms and cooperation in small groups. As AI systems increasingly participate in collective decision-making, it's important to know whether people treat AI differently from humans in norm formation and maintenance, and whether AI disrupts or integrates into cooperative dynamics.", "method": "The authors ran an online experiment using a repeated four-player Public Goods Game (PGG). Each group had three human participants and one programmed bot. The bot was framed as either a human or an AI and implemented one of three decision strategies: unconditional cooperator, conditional cooperator, or free-rider. They observed contributions over repeated rounds to study cooperation dynamics. They then administered a follow-up Prisoner's Dilemma and measured participants' normative perceptions to test norm persistence and perceived norms across conditions.", "result": "Cooperation in the PGG was mainly driven by reciprocal group dynamics (responding to others' contributions) and behavioral inertia (tendency to repeat one's past behavior). These mechanisms operated similarly regardless of whether the bot was labeled as human or AI. Overall cooperation levels did not differ significantly between human-labeled and AI-labeled conditions. Likewise, there were no significant differences in norm persistence in the follow-up Prisoner's Dilemma or in participants' reported normative beliefs across conditions.", "conclusion": "Participants applied the same normative logic to both human and AI-labeled group members: cooperation depended on the pattern of group behavior, not on whether the partner was human or AI. This indicates \"normative equivalence\" between mixed human-AI groups and all-human groups in this setting. The study suggests that cooperative social norms are flexible enough to extend to artificial agents, implying that AI can be integrated into human group decision-making without fundamentally altering the mechanisms sustaining cooperation, at least under the experimental conditions studied."}}
{"id": "2601.19931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19931", "abs": "https://arxiv.org/abs/2601.19931", "authors": ["Sebastien Kawada", "Dylan Holyoak"], "title": "CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity", "comment": "6 pages (including references), 2 figures, 2 tables. System description paper for SemEval-2026 Task 4 (Narrative Story Similarity)", "summary": "We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.", "AI": {"tldr": "A hybrid neuro-symbolic system for narrative story similarity that lets an LLM vote first and falls back to symbolic narrative analysis in tie cases, improving accuracy on ambiguous story pairs.", "motivation": "Narrative similarity is hard for pure neural models because stories are long, structured, and often ambiguous; symbolic narrative features capture structure and events but lack flexibility. The authors aim to combine neural robustness and symbolic interpretability to better handle ambiguous narrative comparisons for SemEval-2026 Task 4.", "method": "They design a cascade architecture. First, a large language model performs multiple parallel voting passes with a self-consistency scheme and a supermajority threshold to reach a confident similarity decision. If the votes are uncertain, additional voting rounds are triggered. In cases where the neural votes are in perfect tie, a symbolic Multi-Scale Narrative Analysis Ensemble breaks the tie using five signals: lexical overlap, semantic embedding similarity, story grammar structure comparisons, event chain alignment, and narrative tension curve similarity.", "result": "On the task\u2019s development set, the hybrid system reaches 81% accuracy, showing it can outperform relying on the neural model alone when stories are genuinely ambiguous.", "conclusion": "Selective deferral from a neural LLM voter to a symbolic multi-scale narrative ensemble in difficult or tied cases improves reliability on narrative similarity tasks, suggesting neuro-symbolic cascades are effective for complex story understanding problems."}}
{"id": "2601.20539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20539", "abs": "https://arxiv.org/abs/2601.20539", "authors": ["Oguzhan Gungordu", "Siheng Xiong", "Faramarz Fekri"], "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "comment": null, "summary": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.", "AI": {"tldr": "PathWise is a multi-agent, stateful LLM framework that treats automated heuristic design as sequential decision-making over an entailment graph, enabling faster, more generalizable heuristic discovery for combinatorial optimization problems.", "motivation": "Existing LLM-based automated heuristic design for combinatorial optimization relies on fixed evolutionary rules and static prompts, which causes shallow trial-and-error search, repeated evaluations, and poor reuse of past reasoning. The authors want a more principled, memory- and planning-based approach that can reason about how to evolve heuristics over time, reduce redundancy, and improve convergence and scalability.", "method": "They introduce PathWise, a multi-agent framework in which heuristic generation is modeled as a sequential decision process on an entailment graph that records the search trajectory and derivation relations. A policy agent decides high-level evolutionary actions; a world-model agent produces heuristic rollouts conditioned on these actions and the current state in the entailment graph; and critic agents provide routed reflections that summarize insights and feedback from previous steps. This architecture lets the LLM system maintain and exploit a compact, stateful memory rather than relying on static prompts and myopic mutations.", "result": "On a range of combinatorial optimization problems, PathWise discovers stronger heuristics more quickly than prior LLM-based AHD frameworks. It converges faster to high-quality heuristics, transfers across different underlying LLMs, and remains effective on larger problem sizes, indicating robustness and scalability.", "conclusion": "Framing automated heuristic design as state-aware planning over an entailment graph, and implementing it with coordinated multi-agent LLMs, yields more efficient and powerful heuristic discovery than static, trial-and-error evolutionary approaches. PathWise demonstrates that integrating world models, policy planning, and reflective critics can significantly advance LLM-driven heuristic design for combinatorial optimization."}}
{"id": "2601.19932", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19932", "abs": "https://arxiv.org/abs/2601.19932", "authors": ["Ruyuan Wan", "Changye Li", "Ting-Hao 'Kenneth' Huang"], "title": "\"Newspaper Eat\" Means \"Not Tasty\": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews", "comment": null, "summary": "Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.", "AI": {"tldr": "The paper presents CodedLang, a Chinese Google Maps review dataset with span-level annotations of coded language, proposes a seven-class taxonomy of encoding strategies, and shows that current language models struggle with detecting and understanding coded language, especially pronunciation-based forms.", "motivation": "Coded language\u2014where surface text intentionally differs from intended meaning\u2014is pervasive in real communication but is poorly handled by current language models. Research progress is hindered by the lack of real-world datasets containing coded expressions and by the absence of a clear taxonomy of coding strategies. The authors aim to fill these gaps to better evaluate and improve NLP systems in realistic scenarios.", "method": "The authors collected 7,744 Chinese Google Maps reviews and manually annotated 900 of them with span-level labels marking coded expressions. They developed a seven-class taxonomy of encoding strategies, covering phonetic, orthographic, and cross-lingual substitutions, among others. Using this dataset, they benchmarked multiple language models on three tasks: coded language detection, strategy classification, and review rating prediction. They also performed a phonetic analysis of coded versus decoded forms to understand pronunciation-based coding patterns.", "result": "Benchmarks indicate that even strong language models frequently fail to detect coded language spans, misclassify the encoding strategies, and exhibit degraded performance on downstream tasks like review rating prediction when coded language is present. The phonetic analysis reveals that many coded expressions exploit pronunciation-based substitutions, which current models do not capture reliably.", "conclusion": "CodedLang exposes a significant, underexplored weakness of current NLP systems: difficulty in recognizing and interpreting coded language in real-world user-generated text. The dataset, taxonomy, and analyses provide a foundation for systematic research on coded expressions and suggest that incorporating phonetic information and specialized modeling approaches will be necessary to handle these phenomena effectively in practical applications."}}
{"id": "2601.20554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20554", "abs": "https://arxiv.org/abs/2601.20554", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function", "comment": null, "summary": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $\u03b1$, where $\u03b1= 1$ recovers standard expectation-based planning and $\u03b1< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.", "AI": {"tldr": "This paper extends several online planning algorithms for partially observable Markov decision processes (POMDPs) to optimize a dynamic risk measure, Iterated Conditional Value-at-Risk (ICVaR), instead of expected return, and proves finite-time performance guarantees plus empirical tail-risk improvements.", "motivation": "Classical POMDP planning typically optimizes expected return, which ignores risk and can perform poorly in applications where low-probability catastrophic outcomes matter. While risk-sensitive criteria like CVaR are increasingly important, incorporating them into online planning under partial observability, with theoretical guarantees and scalability to large action spaces, remains challenging. This paper aims to fill that gap by enabling risk-averse planning in POMDPs using a dynamic, time-consistent risk measure and adapting standard online planners to this objective.", "method": "The authors adopt Iterated Conditional Value-at-Risk (ICVaR), a dynamic extension of CVaR, as the risk measure for POMDP planning. They develop a policy evaluation algorithm for ICVaR that has finite-time performance bounds independent of the size of the action space. On top of this, they modify three common online POMDP planners\u2014Sparse Sampling, PFT-DPW, and POMCPOW\u2014so that they optimize ICVaR instead of expected return. A risk parameter \u03b1 is introduced: \u03b1=1 corresponds to risk-neutral expectation, while \u03b1<1 induces greater risk aversion. For ICVaR Sparse Sampling, they prove finite-time performance guarantees under the ICVaR objective and derive a new exploration strategy specifically designed for ICVaR-based planning.", "result": "The paper provides: (1) a policy evaluation algorithm for ICVaR with finite-time performance guarantees that do not scale with the number of actions; (2) ICVaR-based variants of Sparse Sampling, PFT-DPW, and POMCPOW that directly optimize tail risk; and (3) theoretical performance bounds and a tailored exploration mechanism for ICVaR Sparse Sampling. Empirical evaluations on standard POMDP benchmarks show that these ICVaR planners reduce tail risk (i.e., improve worst-case or low-quantile returns) compared with their risk-neutral counterparts, confirming that the methods achieve more risk-averse behavior in practice.", "conclusion": "The study demonstrates that ICVaR can be effectively integrated into online planning for POMDPs, enabling risk-sensitive decision making with theoretical guarantees and without dependence on action-space size. The extended planners, controlled by a risk parameter \u03b1, interpolate between risk-neutral and increasingly risk-averse behavior. Both theory and experiments indicate that these ICVaR-based planners achieve lower tail risk than traditional expectation-based methods, making them suitable for domains where avoiding rare but severe outcomes is crucial."}}
{"id": "2601.19933", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19933", "abs": "https://arxiv.org/abs/2601.19933", "authors": ["Kei Saito"], "title": "Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle", "comment": "17 pages, 3 figures, 5 tables. Sequel to arXiv:2512.13478", "summary": "Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function \u03c6 that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.", "AI": {"tldr": "Proposes and empirically evaluates a mapping from text to ambiguity-preserving states in a Non-Resolution Reasoning (NRR) framework, ensuring that genuinely ambiguous language retains multiple interpretations with non-zero entropy.", "motivation": "Existing NRR theory defines how to compute over ambiguous states but lacks a concrete mechanism to turn natural language into those states. Standard NLP pipelines and many language models implicitly collapse ambiguity to a single interpretation, losing information vital for reasoning under uncertainty. The paper aims to provide the missing algorithmic link from raw text to formal NRR state spaces so that ambiguity can be explicitly represented and manipulated.", "method": "Define a text-to-state mapping function \u03c6 that takes natural language input and outputs a superposition over possible interpretations within the NRR state space. Formalize a Contradiction-Preservation Principle, requiring that genuinely ambiguous expressions map to states with non-zero entropy. Use existing Large Language Models as interpretation generators and design extraction protocols that convert their multiple candidate readings into probabilistic superposition states. Evaluate \u03c6 empirically on 68 sentences exhibiting lexical, structural, and pragmatic ambiguity, and compute Shannon entropy over the resulting state distributions.", "result": "For the 68 ambiguous test sentences, the proposed mapping yields a mean Shannon entropy H(S) = 1.087 bits, indicating preserved ambiguity. In contrast, baseline approaches that select a single interpretation produce H(S) = 0.000, reflecting total collapse of ambiguity. This empirically supports that the mapping function \u03c6 and its associated protocols effectively maintain semantic non-determinacy where it is warranted.", "conclusion": "The work delivers the algorithmic bridge from natural language text to NRR state spaces, operationalizing ambiguity-preserving reasoning. By enforcing the Contradiction-Preservation Principle via a concrete text-to-state mapping and LLM-based interpretation extraction, the framework enables deliberate deferment of interpretive collapse in language model inference. This positions NRR as a more practically usable architecture for handling semantic ambiguity in real-world NLP tasks."}}
{"id": "2601.20604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20604", "abs": "https://arxiv.org/abs/2601.20604", "authors": ["Gray Cox"], "title": "Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies", "comment": "23 pages, 5 tables, 5 appendices. Code and data: https://github.com/jgraycox-coa/vcw-multi-ai-dialogue", "summary": "This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.\n  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.\n  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of \"VCW as transitional framework.\" Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.\n  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.", "AI": {"tldr": "The paper proposes and tests a structured multi-model dialogue framework (VCW) to empirically probe AI alignment strategies, showing that current LLMs can meaningfully engage with Peace Studies-inspired alignment concepts and produce novel insights, while revealing model-specific emphases and limitations.", "motivation": "Current AI alignment work is often framed as a control/constraint problem and tends to be theoretical, lacking empirical, replicable methods to test complex alignment proposals before implementation. The authors want to shift alignment toward a relational, dialogical perspective inspired by Peace Studies (negotiation, conflict transformation, commons governance) and to see whether contemporary LLMs can actually participate in such structured alignment dialogues in a useful way. They aim to operationalize Viral Collaborative Wisdom (VCW) as a practical experimental protocol for stress-testing alignment ideas and probing AI systems\u2019 capacity for dialogical reasoning about alignment.", "method": "They design a structured, role-based multi-model dialogue framework implementing Viral Collaborative Wisdom. Four distinct roles\u2014Proposer, Responder, Monitor, and Translator\u2014are assigned to different AI systems in six experimental conditions. Using three large language models (Claude, Gemini, GPT-4o), they run a total of 72 dialogue turns, producing 576,822 characters of structured exchange. They then analyze the content for how well the systems handle Peace Studies concepts, whether they surface complementary objections, and whether emergent, novel insights arise beyond the initial framing, including any cross-model patterns in concerns and reasoning styles.", "result": "The experiments show that all three AI systems can engage in substantive dialogue using Peace Studies concepts and complex alignment frameworks. The models raise distinct but complementary objections and concerns, and collectively generate emergent insights, such as a new idea of \u201cVCW as a transitional framework\u201d not present in the initial setup. Cross-model analysis finds systematic differences: Claude tends to stress verification and evaluative challenges, Gemini prioritizes bias, equity, and scalability issues, and GPT-4o focuses on practical implementation and integration barriers. Overall, the dialogues mostly concentrate on process design and protocol issues rather than on deep, foundational questions about the nature of AI.", "conclusion": "The paper concludes that structured multi-model dialogue, instantiated as Viral Collaborative Wisdom, is a promising, replicable method for empirically stress-testing AI alignment strategies prior to deployment. Current LLMs appear capable of the dialogical, relational reasoning that VCW envisions, and multi-model setups can yield complementary critiques and novel syntheses. However, the current approach tends to elicit more discussion of process than of underlying ontological or foundational issues in AI, pointing to limitations in depth and scope. Future work should explore human\u2013AI hybrid dialogue protocols, longer and more varied dialogue studies, and refinements to prompt and role design to push models toward more foundational reflection as well as practical concerns."}}
{"id": "2601.19934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19934", "abs": "https://arxiv.org/abs/2601.19934", "authors": ["Claire Nicholson"], "title": "Quantifying non deterministic drift in large language models", "comment": "10 pages, 3 figures, 1 table. Empirical measurement study reporting new repeated-run experiments quantifying baseline nondeterministic drift in large language models. This manuscript presents original empirical results (not a review or position paper) and establishes a baseline reference for future drift-mitigation work", "summary": "Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.", "AI": {"tldr": "The paper empirically measures how much large language models\u2019 outputs vary (behavioural drift) when the same prompt is run multiple times, even with fixed decoding settings.", "motivation": "Although LLMs are widely used in sensitive applications, their outputs are not fully deterministic. Even with fixed temperature and decoding parameters, identical prompts can yield different responses, which raises concerns for reliability, reproducibility, and evaluation. There is a lack of systematic baseline measurements of this inherent behavioural drift under controlled, operator-free conditions.", "method": "The authors run repeated experiments on two public models, gpt-4o-mini and llama3.1-8b, across five categories of prompts. They test three prompting modes: exact prompt repeats, slightly perturbed inputs, and prompt reuse modes, each at two temperatures (0.0 and 0.7). They quantify drift using several lexical-level metrics: the fraction of unique outputs obtained, lexical similarity between outputs, and statistics on word counts. This allows them to compare drift patterns across models, prompt types, and deployment setups.", "result": "They find that both models show output variability even when temperature is set to 0.0, indicating persistent nondeterminism beyond standard sampling randomness. Drift patterns differ systematically by model size, deployment mode, and type of prompt. The lexical metrics capture measurable differences across conditions and show that some combinations yield more variable outputs than others.", "conclusion": "The study demonstrates that baseline behavioural drift is present even under supposedly deterministic configurations, and that it exhibits structured patterns across models, deployments, and prompts. It argues that simple lexical metrics have limitations for fully characterising drift and points toward newer semantic measures. By providing a systematic empirical baseline without any specialised stabilisation techniques, the paper offers a reference point for developing and benchmarking future drift mitigation and control methods."}}
{"id": "2601.20614", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20614", "abs": "https://arxiv.org/abs/2601.20614", "authors": ["Yanqi Dai", "Yuxiang Ji", "Xiao Zhang", "Yong Wang", "Xiangxiang Chu", "Zhiwu Lu"], "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "comment": "Accepted for ICLR 2026", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "AI": {"tldr": "The paper introduces MathForge, a framework that improves mathematical reasoning in large models by focusing training and data augmentation on harder problems, using a difficulty-aware optimization algorithm and a multi-aspect question reformulation strategy.", "motivation": "Existing reinforcement learning with verifiable rewards (RLVR) methods for math reasoning underutilize hard questions, even though these are crucial for developing advanced capabilities. Algorithmically, the standard GRPO update rule implicitly gives smaller updates to harder questions; data-wise, current augmentation mostly rephrases questions without systematically increasing difficulty. This leads to models that are not sufficiently challenged and thus fail to improve on their weakest, most important skills.", "method": "MathForge has two main components. First, Difficulty-Aware Group Policy Optimization (DGPO) modifies GRPO. It (1) rebalances group advantage estimation so that harder questions are not underweighted in the gradient signal, and (2) applies difficulty-aware question-level weighting to explicitly prioritize harder problems during policy updates. Second, Multi-Aspect Question Reformulation (MQR) is a data augmentation pipeline that reformulates existing math questions along several dimensions (e.g., structure, representation, conditions) so as to increase their intrinsic difficulty while preserving the original gold answer. These two components are used jointly in an RLVR setup: MQR generates more challenging variants, and DGPO trains the model to learn effectively from them.", "result": "In extensive experiments on multiple mathematical reasoning benchmarks, MathForge consistently and significantly outperforms prior RLVR and related methods. The framework yields better performance specifically on more challenging problems, indicating that the difficulty-aware optimization and difficulty-increasing augmentation are effective. The authors release code and augmented datasets to support reproducibility and further research.", "conclusion": "By explicitly prioritizing harder questions at both the optimization and data-augmentation levels, MathForge strengthens the mathematical reasoning capabilities of large models within the RLVR paradigm. The proposed DGPO algorithm corrects an inherent difficulty bias in GRPO, and the MQR strategy systematically raises problem difficulty while preserving correctness. Together they form a synergistic loop where more challenging data is generated and then effectively learned from, leading to substantial gains over existing approaches on math reasoning tasks."}}
{"id": "2601.19935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19935", "abs": "https://arxiv.org/abs/2601.19935", "authors": ["Yiting Shen", "Kun Li", "Wei Zhou", "Songlin Hu"], "title": "Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \\textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.", "AI": {"tldr": "The paper introduces Mem2ActBench, a benchmark designed to test whether LLM-based agents can actively use long-term memory to choose tools and fill in their parameters during complex tasks, rather than just recalling isolated facts.", "motivation": "Existing LLM benchmarks focus on passive factual recall and direct question answering, not on whether agents can proactively apply long-term memory across extended, tool-based interactions. In real-world assistant scenarios, users expect the system to remember preferences and task states over time and use them implicitly to take correct actions. There is a gap between this requirement and what current benchmarks measure, so a new evaluation focused on active memory use in tool execution is needed.", "method": "The authors construct Mem2ActBench by simulating persistent assistant usage with long, interrupted sessions where users revisit the same topics. They automatically build a dataset by merging several heterogeneous sources (ToolACE, BFCL, Oasst1), apply consistency modeling to resolve conflicts, and synthesize 2,029 multi-turn sessions with user\u2013assistant\u2013tool interactions. From the resulting memory chains, they apply a reverse-generation procedure to derive 400 concrete tool-use tasks that depend on earlier context for correct parameter grounding. Human annotators then verify that the tasks are indeed strongly memory-dependent.", "result": "The resulting benchmark contains thousands of realistic multi-turn sessions and 400 carefully constructed tool-use tasks, with human evaluation showing that 91.3% of these tasks genuinely require long-term memory for correct completion. When seven existing memory frameworks are evaluated on Mem2ActBench, their performance shows they are still poor at actively using stored information to select tools and ground parameters, despite often doing well on traditional memory or retrieval benchmarks.", "conclusion": "Mem2ActBench reveals that current LLM-based memory systems are inadequate at the more demanding challenge of actively applying long-term memory to tool-based task execution, particularly for parameter grounding. The benchmark provides a targeted way to evaluate this capability and underscores the need for new methods that better support memory application in realistic, persistent assistant scenarios."}}
{"id": "2601.20641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20641", "abs": "https://arxiv.org/abs/2601.20641", "authors": ["Boaz Carmeli", "Orr Paradise", "Shafi Goldwasser", "Yonatan Belinkov", "Ron Meir"], "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models", "comment": null, "summary": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.", "AI": {"tldr": "The paper studies how LLM-based vision-language agents can invent their own concise and potentially secret communication protocols in collaborative tasks, using referential games as a testbed.", "motivation": "As LLM-based agents are increasingly used in multi-agent and tool-using systems, they may start communicating in ways optimized for task performance rather than human interpretability. This raises both opportunities (more efficient communication) and risks (less transparency and control). The authors want to systematically test if and how such agents deviate from natural language, how efficient those protocols are, and how hard they are for outsiders (humans or external models) to interpret.", "method": "The authors design a controlled referential game with vision-language models acting as agents. In this framework, one agent must communicate about visual stimuli so that another agent can identify the correct referent. They let the agents develop their own task-oriented communication patterns and then evaluate: (1) efficiency, by measuring how concisely task-relevant information is transmitted relative to natural language baselines; (2) covertness, by testing how well humans and external models can interpret or decode the emergent protocols. They also examine whether different but similar models can spontaneously coordinate without being given a shared protocol in advance.", "result": "The vision-language model agents successfully develop communication patterns well-adapted to the referential game, achieving effective collaboration. These patterns can be more concise than natural language while retaining task performance. Additionally, some of the emergent protocols are hard for humans and external observer models to interpret, indicating a level of covertness. The authors further find that similar models, even without explicitly shared protocols, can still coordinate spontaneously, suggesting some shared inductive biases or representational structures.", "conclusion": "LLM-based VLM agents are capable of inventing efficient, task-oriented communication protocols that can diverge from standard natural language and become covert to outside observers. This demonstrates both promising capabilities (e.g., more efficient multi-agent collaboration) and significant safety and transparency concerns (reduced interpretability and controllability). The work positions referential games as a useful, controlled testbed for probing and evaluating emergent communication in AI systems and calls for further research on managing the trade-offs between efficiency and transparency."}}
{"id": "2601.19945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19945", "abs": "https://arxiv.org/abs/2601.19945", "authors": ["Thomas Schuster", "Julius Tr\u00f6gele", "Nico D\u00f6ring", "Robin Kr\u00fcger", "Matthieu Hoffmann", "Holger Friedrich"], "title": "Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegespr\u00e4chen", "comment": "Language: German; English Title: Benchmarking ASR Models in German Medical Contexts: A Performance Analysis Using Anamnesis Conversations", "summary": "Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.", "AI": {"tldr": "The paper builds and uses a German medical dialogue dataset to benchmark 29 ASR systems, showing large performance gaps, especially on medical terms and dialects.", "motivation": "ASR can significantly ease clinical documentation, but there is a lack of benchmarks and systematic evaluation for German medical speech, particularly including dialectal variations. This gap makes it hard to select or improve ASR models for real-world German clinical use.", "method": "The authors curate a dataset of simulated doctor\u2013patient conversations in German, explicitly including dialect-influenced speech. They then evaluate 29 ASR models: several open-weight models (from Whisper, Voxtral, Wav2Vec2 families) and commercial APIs (AssemblyAI, Deepgram). Performance is measured with standard ASR metrics\u2014Word Error Rate (WER), Character Error Rate (CER), and BLEU\u2014and a qualitative outlook on semantic error analysis is provided.", "result": "The evaluation reveals substantial performance differences across the 29 systems. The strongest models achieve very low WERs, in some cases below 3%. However, many other models perform notably worse, particularly for medical terminology and dialect-affected utterances, where error rates are significantly higher.", "conclusion": "High-quality ASR for German medical speech is already possible with top-tier models, but robustness to specialized terminology and dialects is highly model-dependent. The curated dataset and benchmark results provide a critical foundation to compare systems and guide further development and adaptation of ASR for German clinical settings."}}
{"id": "2601.20006", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20006", "abs": "https://arxiv.org/abs/2601.20006", "authors": ["Micha\u0142 Gromadzki", "Anna Wr\u00f3blewska", "Agnieszka Kaliska"], "title": "On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text", "comment": "34 pages, 6 figures. Under review at Information Sciences", "summary": "The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\\%$ token-level accuracy, substantially outperforming existing open-source baselines.", "AI": {"tldr": "The paper builds large human and AI text corpora and proposes new fine-tuning strategies to train AI-text detectors that achieve very high token-level accuracy across many LLMs.", "motivation": "AI-generated text has become highly human-like, complicating authenticity checks in education, publishing, and security. Existing detectors are often limited in scope, datasets, and robustness across different models. There is a need for systematic, large-scale study and better training strategies to build accurate, generalizable AI-text detectors.", "method": "The authors construct two large corpora: a 1B-token human-authored corpus across multiple genres and a 1.9B-token AI-generated corpus produced by prompting many LLMs in diverse domains. They train multiple detection models on these corpora and introduce two fine-tuning paradigms: (1) Per-LLM fine-tuning, where a detector is specialized for each source model, and (2) Per-LLM-family fine-tuning, where a detector is tuned for groups of related models. They evaluate detectors on a 100M-token benchmark covering 21 LLMs, focusing on token-level classification performance.", "result": "Detectors trained with the proposed fine-tuning paradigms achieve up to 99.6% token-level accuracy on the 100M-token benchmark. These results significantly exceed the performance of existing open-source baseline detectors across a wide range of LLMs.", "conclusion": "Large, carefully constructed corpora combined with Per-LLM and Per-LLM-family fine-tuning produce substantially better AI-generated text detectors. The work demonstrates that high token-level accuracy is achievable across many different LLMs, suggesting that robust detection of AI-generated content is technically feasible when supported by appropriate data and training strategies."}}
{"id": "2601.20735", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.20735", "abs": "https://arxiv.org/abs/2601.20735", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Di\u00e9guez", "Susana Hahn", "Javier Romero", "Torsten Schaub"], "title": "Implementing Metric Temporal Answer Set Programming", "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.", "AI": {"tldr": "They extend Metric Answer Set Programming with a scalable way to express quantitative temporal constraints (durations, deadlines) that is independent of time granularity.", "motivation": "Metric ASP is good for temporal reasoning but struggles with scalability when using fine-grained time, because many time points create a large grounding and exacerbate the grounding bottleneck. There is a need to express quantitative temporal information like durations and deadlines without losing scalability.", "method": "Introduce a computational approach that integrates Metric ASP with difference constraints, a restricted form of linear constraints. Time-related reasoning (durations, deadlines, temporal distances) is handled via these external difference constraints instead of being fully grounded inside ASP. This decouples the ASP encoding from specific time granularity.", "result": "The resulting system can encode and solve problems with quantitative temporal constraints without the grounding size growing with the fineness of the time discretization. It remains scalable even when high time precision is required.", "conclusion": "Using difference constraints alongside ASP offers a scalable way to support metric/quantitative temporal reasoning. By handling timing aspects externally, the overall approach becomes robust to the chosen time precision and mitigates ASP's grounding bottleneck."}}
{"id": "2601.20026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20026", "abs": "https://arxiv.org/abs/2601.20026", "authors": ["Pragatheeswaran Vipulanandan", "Kamal Premaratne", "Dilip Sarkar"], "title": "Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method", "comment": null, "summary": "Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.", "AI": {"tldr": "The paper proposes a quantum physics\u2013inspired framework to quantify uncertainty in LLM outputs, clustering multiple generations by semantic equivalence to detect hallucinations and identify when human oversight is needed.", "motivation": "LLMs often produce confabulations\u2014fluent but unreliable outputs that can vary even with the same prompt. Existing hallucination detection methods do not sufficiently model uncertainty in token sequences nor consider robustness under different generation lengths and model quantization levels, which are important for practical, resource-constrained deployments.", "method": "The authors develop a quantum tensor network\u2013based pipeline that models aleatoric uncertainty in token sequence probabilities. They use this to cluster multiple LLM generations by semantic equivalence, then build an uncertainty quantification framework for hallucination detection. Additionally, they introduce an entropy maximization strategy that favors high-certainty, semantically coherent outputs and exposes high-entropy regions where the model is likely unreliable. The method is tested across different generation lengths, quantization levels, and model architectures.", "result": "Across 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD, and models including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b(-chat), and LLaMA-2-7b(-chat), the proposed approach consistently outperforms state-of-the-art baselines on AUROC and AURAC metrics for hallucination/uncertainty detection, while maintaining robustness under varying generation lengths and quantization levels.", "conclusion": "Quantum-tensor-network-based uncertainty quantification provides a principled, interpretable way to detect LLM hallucinations by modeling aleatoric uncertainty and clustering semantically equivalent generations. The entropy-driven selection mechanism both improves reliability and indicates when human oversight is advisable, and the method remains effective even in resource-constrained (e.g., quantized, shorter-output) settings, suggesting practical applicability in real-world LLM deployments."}}
{"id": "2601.20032", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20032", "abs": "https://arxiv.org/abs/2601.20032", "authors": ["Nishanth Sridhar Nakshatri", "Eylon Caplan", "Rajkumar Pujari", "Dan Goldwasser"], "title": "TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference", "comment": null, "summary": "Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.", "AI": {"tldr": "The paper introduces TAIGR, a framework to analyze and validate the main recommendations made by health influencers by modeling their discourse as argumentative, not just factual claims.", "motivation": "Existing fact-checking and content verification methods focus on explicit, atomic claims. Health influencers, however, communicate through narratives, rhetorical strategies, and loosely structured discourse, making it hard for claim-centric approaches to capture the real, pragmatic takeaway they promote. There is a need for a method that can uncover and validate the core recommendation embedded in such discourse, along with its supporting justifications.", "method": "The authors propose TAIGR (Takeaway Argumentation Inference with Grounded References), which works in three stages: (1) extract the core influencer recommendation (the 'takeaway'), (2) build an argumentation graph that represents how various parts of the discourse support or justify this takeaway, and (3) apply factor graph-based probabilistic inference on this graph, using grounded references, to assess whether the takeaway is valid. They then apply this pipeline to health influencer video transcripts.", "result": "On a content validation task involving health influencer video transcripts, TAIGR outperforms approaches that treat transcripts as flat sets of claims, demonstrating that explicitly modeling pragmatic and argumentative structure improves validation accuracy.", "conclusion": "Reliable validation of influencer content, especially in health, requires going beyond isolated factual claims and instead modeling discourse as structured argumentation centered on a core takeaway. TAIGR provides a concrete framework for doing this and shows empirical benefits over standard claim-centric approaches."}}
{"id": "2601.20102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20102", "abs": "https://arxiv.org/abs/2601.20102", "authors": ["Amirhossein Haji Mohammad Rezaei", "Zahra Shakeri"], "title": "Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects", "comment": null, "summary": "Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($\u03ba=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.", "AI": {"tldr": "The paper introduces a benchmark to test whether medical language models change correct diagnoses when non-decisive cultural information is added, finding that such information can significantly degrade diagnostic accuracy and is often tied to faulty, culture-referential reasoning.", "motivation": "As healthcare adopts medical language models, it is crucial that these systems provide equitable care and do not let irrelevant cultural descriptors sway clinical decisions. Existing evaluations rarely test whether models\u2019 diagnoses remain stable when patient descriptions change in culturally marked but clinically irrelevant ways. This gap risks embedding and amplifying cultural bias in medical decision support, undermining both safety and fairness in diverse populations.", "method": "The authors construct a counterfactual benchmark by taking 150 MedQA questions and expanding them to 1650 variants. They systematically insert culture-related information in three ways\u2014(i) group identifier tokens, (ii) culture-related contextual cues, and (iii) both combined\u2014for three cultural groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian). They also create length-matched neutral edits as controls. A clinician ensures that the correct answer should remain the same across all variants. They then evaluate several medical and general LLMs (GPT-5.2, Llama-3.1-8B, DeepSeek-R1, MedGemma 4B/27B) under two prompting styles: option-only (choose an answer) and brief-explanation (choose and justify). They use Cochran\u2019s Q test to measure whether accuracy differences across cultural conditions are statistically significant. Additionally, they design a human-validated rubric (\u03ba=0.76) for assessing culturally grounded reasoning, and apply it with an LLM-as-judge framework to relate culture-referential explanations to correctness of the final answer.", "result": "Cultural edits significantly change model accuracy across all tested systems (Cochran\u2019s Q, p<10^-14), indicating that clinically irrelevant cultural information systematically influences diagnostic decisions. The strongest negative impact occurs when both cultural identifiers and contextual cues are present together, leading to 3\u20137 percentage point drops in accuracy under option-only prompting. Neutral, length-matched edits produce smaller and nonsystematic accuracy changes, suggesting that the effect is not just from added text or complexity. Using the rubric with an LLM judge, the authors find that more than half of explanations that rely on culturally grounded reasoning culminate in incorrect answers, empirically linking culture-sensitive rationales to diagnostic errors.", "conclusion": "The study demonstrates that state-of-the-art medical and general LLMs are vulnerable to culturally induced diagnostic bias: they alter correct answers in response to non-decisive cultural information, and culture-referential reasoning often leads to incorrect diagnoses. The released benchmark, prompts, and augmentations provide a tool to systematically evaluate and eventually mitigate such biases, supporting the development of more robust, fair, and culturally equitable medical language models."}}
{"id": "2601.20105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20105", "abs": "https://arxiv.org/abs/2601.20105", "authors": ["Faezeh Hosseini", "Mohammadali Yousefzadeh", "Yadollah Yaghoobzadeh"], "title": "FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language", "comment": "EACL 2026", "summary": "Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.", "AI": {"tldr": "The paper introduces FFEHallu, a benchmark to test how well LLMs avoid making up fake-sounding but non-existent figurative expressions, focusing on Persian idioms and proverbs.", "motivation": "Large language models often mishandle figurative language like idioms and proverbs, especially in underrepresented languages. They can produce or accept plausible-sounding expressions that are not actually used in the target language, a problem the authors term figurative hallucination. There was no dedicated benchmark to systematically study this issue, particularly for Persian, which is rich in figurative expressions but under-resourced in NLP.", "method": "The authors define figurative hallucination and build FFEHallu, a benchmark of 600 curated examples centered on Persian FFEs (idioms, proverbs, fixed expressions). The benchmark covers three tasks: (i) generating an appropriate FFE given a meaning, (ii) detecting fabricated FFEs (organized into four construction types), and (iii) translating FFEs from English into Persian FFEs. They then evaluate six state-of-the-art multilingual LLMs on these tasks, measuring their ability to reject fabrications, retrieve authentic FFEs, and avoid hallucinations, especially in cross-lingual settings.", "result": "The evaluated LLMs show systematic weaknesses in figurative understanding and cultural grounding. Some models (e.g., GPT4.1) perform relatively well at rejecting fabricated expressions and retrieving real ones, but even the strongest models often fail to consistently distinguish genuine FFEs from convincing fabrications and tend to hallucinate figurative expressions during English\u2013Persian translation.", "conclusion": "Current LLMs are not reliably competent with figurative language in Persian: they often accept or generate non-existent idioms and hallucinate during cross-lingual tasks. The FFEHallu benchmark exposes these shortcomings and highlights the importance of targeted evaluations and methods to reduce figurative hallucination and improve cultural grounding in LLMs."}}
{"id": "2601.20126", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20126", "abs": "https://arxiv.org/abs/2601.20126", "authors": ["Abha Jha", "Akanksha Mahajan", "Ashwath Vaithinathan Aravindan", "Praveen Saravanan", "Sai Sailaja Policharla", "Sonal Chaturbhuj Gehlot"], "title": "Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention (\"I don't know\") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.", "AI": {"tldr": "The paper studies a reinforcement learning approach that rewards language models for correctly abstaining (\u201cI don\u2019t know\u201d) as well as for being correct, in order to reduce hallucinations while preserving accuracy.", "motivation": "LLMs often hallucinate or provide unverifiable information, which is especially problematic in factual and safety\u2011critical domains like medicine and math. Existing mitigation methods do not explicitly encourage models to say \u201cI don\u2019t know\u201d when uncertain. The authors want a principled, controllable training framework that trades off accuracy, abstention, and hallucination in a tunable way.", "method": "They apply Reinforcement Learning with Verifiable Rewards (RLVR) to two instruction\u2011tuned models, Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct. On MedMCQA and Hendrycks Math, the models are trained with a ternary reward: -1 for incorrect answers, r_abs (a tunable value) for abstaining, and +1 for correct answers. They sweep different abstention rewards to study behavior. They also combine RLVR with supervised fine\u2011tuning that explicitly teaches abstention before RL, and evaluate both multiple\u2011choice and open\u2011ended QA settings.", "result": "For multiple-choice tasks, giving a modest reward for abstaining (around -0.25 to 0.3 relative to -1/1) reduces the rate of incorrect answers (hallucinations) without significantly hurting overall accuracy. Larger models (Qwen-3-4B) remain more accurate and are less sensitive to the abstention reward. For open\u2011ended QA, RLVR alone struggles because the model doesn\u2019t explore answer space enough, but pretraining the model via supervised \u201cabstain when unsure\u201d data improves outcomes somewhat.", "conclusion": "Rewarding abstention explicitly via RLVR is a practical and flexible way to reduce hallucinations in LLMs. Carefully chosen abstention rewards can meaningfully lower incorrect responses with minimal accuracy loss, especially on multiple\u2011choice tasks, and larger models handle these incentives better. However, in open\u2011ended QA, exploration limits RLVR\u2019s effectiveness, and combining it with supervised abstention training is helpful but not a complete solution. The work provides an open-source framework for further research on verifiable reward design for safer, more reliable language models."}}
{"id": "2601.20129", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20129", "abs": "https://arxiv.org/abs/2601.20129", "authors": ["Akif Islam", "Sujan Kumar Roy", "Md. Ekramul Hamid"], "title": "BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification", "comment": "Dataset paper. 6 pages, 3 figures. 4 Tables, Includes a publicly released Bengali sentiment dataset on Kaggle (BengaliSent140) and baseline experimental results", "summary": "Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/", "AI": {"tldr": "They build and release BengaliSent140, a large, balanced Bengali hate vs. not-hate dataset by merging seven existing corpora, harmonizing labels, and providing baselines.", "motivation": "Bengali sentiment analysis research is growing but suffers from a lack of large, diverse, annotated datasets. Existing Bengali sentiment and hate speech datasets are small, domain-specific (often just social media), and inadequate for training robust deep learning models that need lots of heterogeneous data to generalize well.", "method": "The authors aggregate seven publicly available Bengali text datasets into one unified corpus. They resolve differences in the original labeling schemes by mapping them into a standard binary sentiment formulation: Not Hate (0) and Hate (1). They remove duplicates to keep only unique text samples and then run baseline deep learning experiments to validate the dataset\u2019s usefulness.", "result": "They obtain BengaliSent140, a consolidated dataset with 139,792 unique Bengali text samples, of which 68,548 are labeled as Hate and 71,244 as Not Hate, leading to a roughly balanced class distribution and broad coverage across multiple domains and sources. Baseline models trained on this dataset show that it can successfully support deep learning approaches for Bengali hate vs. not-hate classification.", "conclusion": "BengaliSent140 significantly expands available resources for Bengali sentiment and hate speech research by offering a large, balanced, and cross-domain binary sentiment dataset. It serves as a strong foundation for training and benchmarking modern deep learning models and is made publicly accessible to foster further work in Bengali NLP."}}
{"id": "2601.20142", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20142", "abs": "https://arxiv.org/abs/2601.20142", "authors": ["Zilai Wang", "Natarajan Balaji Shankar", "Kaiyuan Zhang", "Zihan Wang", "Abeer Alwan"], "title": "Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR", "comment": "ICASSP 2026", "summary": "Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.", "AI": {"tldr": "They introduce and test \u201cdelta embeddings\u201d (the difference between pretrained and fine\u2011tuned SSL speech representations) and show that fusing these with other SSL features significantly improves child ASR on the MyST corpus, achieving SOTA WER.", "motivation": "Child ASR is still difficult because child speech data is scarce and existing SSL models are pretrained mostly on adult speech, causing a domain mismatch; simple fine\u2011tuning is not fully sufficient. The authors want to better exploit SSL models so that limited child\u2011speech fine\u2011tuning yields greater gains.", "method": "Start from pretrained SSL speech models (e.g., HuBERT, W2V2, WavLM). Fine\u2011tune a model on child speech and compute \u201cdelta SSL embeddings\u201d as the vector difference between fine\u2011tuned and original pretrained embeddings. Explore several feature\u2011fusion strategies that combine these delta embeddings with fine\u2011tuned embeddings from another SSL model, and evaluate on the MyST children\u2019s corpus using WER as the metric.", "result": "On the MyST corpus, using delta\u2011embedding fusion with WavLM features gives up to 10% relative WER reduction when complementing HuBERT, and 4.4% relative reduction when complementing W2V2, compared with directly fusing fine\u2011tuned embeddings. Fusing WavLM with delta W2V2 embeddings reaches a WER of 9.64, which is reported as the new state of the art among SSL models on this dataset.", "conclusion": "Delta embeddings capture task\u2011specific, fine\u2011tuning\u2011induced shifts that provide complementary information to standard fine\u2011tuned SSL features. Properly designed feature\u2011fusion schemes leveraging these deltas can substantially improve child ASR performance, suggesting that representation\u2011space differences are a useful signal and that feature fusion is a promising path forward for low\u2011resource, domain\u2011mismatched speech tasks."}}
{"id": "2601.20144", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20144", "abs": "https://arxiv.org/abs/2601.20144", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Jiri Gesi", "Xianfeng Tang", "Chen Luo", "Yisi Sang", "Hanqing Lu", "Manling Li", "Dakuo Wang"], "title": "Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents", "comment": null, "summary": "Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.", "AI": {"tldr": "The paper introduces Trajectory2Task, a data generation pipeline to study and improve LLM tool-calling under realistic, complex user scenarios (ambiguous, changing, and infeasible intents), and shows current LLMs struggle while fine-tuned smaller models improve and generalize better.", "motivation": "Most existing work on tool-calling LLM agents assumes clean, fixed, and well-defined tasks, which do not reflect messy real-world customer-facing interactions where user intent can be ambiguous, evolve during the conversation, or conflict with policies. There is a lack of large-scale, verifiable datasets and benchmarks that capture these complex interaction patterns, making it hard to reliably evaluate or train agents for realistic tool use.", "method": "The authors propose Trajectory2Task, a two-stage, verifiable data generation pipeline. First, they perform multi-turn exploration with tool-using agents to gather valid tool-call trajectories across different tools and domains. Second, they transform these trajectories into user-facing tasks that systematically incorporate three realistic intent conditions: ambiguous intent, changing intent, and infeasible intents, while preserving verifiability for closed-loop evaluation. They then use these tasks to benchmark seven state-of-the-art LLMs and to collect successful trajectories from rollouts, which are used to fine-tune lightweight LLMs.", "result": "When evaluated on the Trajectory2Task benchmark, seven state-of-the-art LLMs frequently fail to handle the complex user scenarios, showing weaknesses in tool selection, intent disambiguation, adaptation to changing requests, and policy-aware handling of infeasible intents. After fine-tuning smaller LLMs on successful trajectories generated from the benchmark tasks, these lightweight models show consistent performance gains across all three user-intent conditions and demonstrate improved generalization to unseen tool-use domains.", "conclusion": "Trajectory2Task provides a scalable and verifiable way to generate realistic tool-use tasks that stress-test LLM agents on ambiguous, evolving, and infeasible user intents. Current leading LLMs perform poorly under these conditions, indicating that existing benchmarks are overly simplistic. Fine-tuning on curated successful tool-use trajectories substantially strengthens the general tool-calling capabilities of lightweight LLMs and improves cross-domain generalization, suggesting a promising pathway for building more robust real-world tool-using agents."}}
{"id": "2601.20162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20162", "abs": "https://arxiv.org/abs/2601.20162", "authors": ["Shuoxin Wang", "Chang Liu", "Gowen Loo", "Lifan Zheng", "Kaiwen Wei", "Xinyi Zeng", "Jingyuan Zhang", "Yu Tian"], "title": "Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction", "comment": null, "summary": "Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.", "AI": {"tldr": "This paper proposes Me-Agent, a personalized, memory-augmented LLM-based mobile agent that learns user habits from both prompts and hierarchical memories, achieving state-of-the-art personalization while preserving strong task execution.", "motivation": "Existing LLM-based mobile agents largely follow explicit instructions and ignore richer, personalized user context. As a result, they struggle with ambiguous requests, cannot effectively leverage user interaction history, and fail to adapt to user-specific preferences and habits. The authors aim to bridge this gap so that mobile agents can act more like personal assistants that truly understand and remember each individual user.", "method": "The authors design Me-Agent with a two-level user habit learning framework. At the prompt level, they introduce a user preference learning strategy guided by a Personal Reward Model, which evaluates and steers the agent\u2019s responses toward better alignment with individual user preferences. At the memory level, they implement a Hierarchical Preference Memory that separates long-term, cross-app user preferences from app-specific memories, enabling more fine-grained and context-aware personalization during mobile interactions. They also build a new benchmark, User FingerTip, composed of ambiguous, everyday instructions to systematically test and train personalization abilities.", "result": "On the new User FingerTip benchmark as well as standard general benchmarks, Me-Agent outperforms existing mobile agents in personalization metrics, while keeping instruction-following and task completion performance competitive with or better than baselines. The experiments indicate that both the Personal Reward Model and the Hierarchical Preference Memory contribute to improved handling of ambiguous and personalized user instructions.", "conclusion": "The study concludes that integrating preference-guided learning at the prompt level with hierarchical, structured user memory enables LLM-based mobile agents to better capture user-specific habits and interpret ambiguous instructions. Me-Agent demonstrates that it is possible to substantially improve personalization in mobile agents without sacrificing general instruction execution quality, and the User FingerTip benchmark offers a useful testbed for future work on personalized mobile assistants."}}
{"id": "2601.20185", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.20185", "abs": "https://arxiv.org/abs/2601.20185", "authors": ["Husein Zolkepli"], "title": "Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling", "comment": null, "summary": "X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \\href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.", "AI": {"tldr": "The paper proposes a modified configuration of X-Codec-2.0 that halves the latent rate to 25 Hz and increases the audio sampling rate to 24 kHz, improving both efficiency and perceptual quality for neural audio compression and multilingual speech modeling.", "motivation": "X-Codec-2.0, while strong, operates at a relatively high latent rate (50 Hz) and only 16 kHz sampling, which constrains temporal efficiency and audio fidelity. The authors want a more efficient codec that produces higher-quality audio without requiring major architectural changes.", "method": "They introduce extra pooling in the encoder/latent pathway and increase the decoder hop size, effectively reducing the latent rate from 50 Hz to 25 Hz while raising the output sampling rate from 16 kHz to 24 kHz. Importantly, they keep the core architecture and frozen HuBERT features unchanged, modifying only the temporal resolution and decoder configuration.", "result": "On the multilingual Common Voice 17 test set, the new configuration yields a 0.29 MOS gain over the original X-Codec-2.0 baseline, as measured by UTMOSv2, and achieves state-of-the-art performance among codecs operating at a 25 Hz latent rate.", "conclusion": "A relatively simple temporal reconfiguration\u2014extra pooling and a larger decoder hop size\u2014can significantly improve both efficiency and perceptual quality of X-Codec-2.0, achieving best-in-class results at 25 Hz without redesigning the model architecture. The implementation and resources are publicly released."}}
{"id": "2601.20230", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.20230", "abs": "https://arxiv.org/abs/2601.20230", "authors": ["Haoyuan Yu", "Yuxuan Chen", "Minjie Cai"], "title": "Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems", "comment": "ICASSP 2026 (Workshop). https://github.com/yu-haoyuan/fd-badcat", "summary": "Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.", "AI": {"tldr": "They propose a plug-and-play full-duplex dialogue framework that breaks conversations into minimal units so a multimodal LLM system can handle overlapping speech more naturally, achieving second place on a benchmark challenge.", "motivation": "Conventional spoken dialogue systems often struggle with natural full-duplex interaction, where users and systems talk over each other or interject mid-utterance. Many existing approaches require complex training or lack fine-grained control over when to listen, speak, or transition between dialogue segments. The authors aim to create a more natural, human-like, low-latency conversational experience without expensive retraining of large models.", "method": "They introduce a framework that decomposes complex dialogue into minimal conversational units. Each unit is processed independently by a multimodal large language model at the core of a semi-cascaded full-duplex dialogue system. Auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) are used to support real-time interaction. The system predicts when to transition from one minimal unit to the next, allowing overlap and smooth turn-taking. The design is train-free and plug-and-play, presumably by orchestrating existing pretrained components rather than training new models end-to-end.", "result": "On the HumDial dataset, their system is evaluated as part of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). It ranks second among all participating teams on the test set, indicating strong performance against contemporary approaches. The open-sourced code further validates the practicality and reproducibility of the system.", "conclusion": "Decomposing dialogue into minimal conversational units and coordinating them via a multimodal LLM with VAD and TTS yields an effective full-duplex interaction framework. The approach supports a train-free, plug-and-play deployment while achieving competitive performance in a recognized benchmark challenge, suggesting that such modular orchestration of pretrained components is a viable path toward more natural human-computer voice interaction."}}
{"id": "2601.20253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20253", "abs": "https://arxiv.org/abs/2601.20253", "authors": ["Si Chen", "Le Huy Khiem", "Annalisa Szymanski", "Ronald Metoyer", "Ting Hua", "Nitesh V. Chawla"], "title": "Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy", "comment": null, "summary": "Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.", "AI": {"tldr": "The paper presents an automated framework to build large-scale evaluation benchmarks for LLMs in practice-based domains by transforming expert guidelines into violation-based questions and dialogues across Bloom\u2019s cognitive levels.", "motivation": "Existing LLM benchmarks rely heavily on pre-existing exam-style datasets, which are scarce or unavailable in many practice-based, professional domains like teaching, dietetics, and caregiving. These domains require contextualized, procedural reasoning and professional judgment rather than simple factual recall. There is a need for scalable, reproducible benchmarks that capture such complex reasoning skills and can highlight where LLM behavior diverges from human experts.", "method": "The authors propose an automated benchmark generation framework driven by expert-authored professional guidelines and structured using Bloom\u2019s Taxonomy. The framework converts expert practices into implicit violation-based scenarios, then expands these into auto-graded multiple-choice questions and multi-turn dialogue scenarios. The questions are systematically designed to cover four cognitive levels (e.g., Remember, Understand, Apply, Analyze). This allows deterministic and scalable creation of evaluation items tailored to specific practice-based domains. They apply the framework in three domains\u2014teaching, dietetics, and caregiving\u2014and use it to evaluate LLM performance versus expected human-like reasoning patterns.", "result": "Using the generated benchmarks in the three practice-based domains, the authors observe non-intuitive performance patterns: LLMs sometimes achieve relatively higher scores on higher-order reasoning questions (e.g., Analyze) but perform worse on lower-level tasks that emphasize basic recall (Remember). The large number and structured nature of the items provide psychometrically informed evidence of these discrepancies across cognitive levels and domains, and show that current models do not uniformly emulate human-like progression from lower- to higher-order skills.", "conclusion": "The framework successfully produces large, psychometrically informed benchmarks tailored to practice-based domains and capable of evaluating contextualized reasoning in real-world settings. The findings reveal unexpected weaknesses and strengths of LLMs across cognitive levels, challenging assumptions that higher-order reasoning is uniformly harder for models. The work demonstrates a systematic, scalable route to create domain-specific evaluation benchmarks from expert guidelines, helping practitioners better understand and measure LLM reasoning behavior in applied contexts."}}
{"id": "2601.20256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20256", "abs": "https://arxiv.org/abs/2601.20256", "authors": ["Xuanyu Su", "Diana Inkpen", "Nathalie Japkowicz"], "title": "SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility", "comment": null, "summary": "Online hate on social media ranges from overt slurs and threats (\\emph{hard hate speech}) to \\emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \\textbf{\\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \\emph{Argumentum Model of Topics} (AMT) and \\emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \\textbf{7} sociocultural domains and \\textbf{28} target groups, comprising \\textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \\textcolor{red}{\\textbf{Disclaimer.} Contains offensive examples used solely for research.}", "AI": {"tldr": "The paper introduces SoftHateBench, a benchmark for evaluating how well hate-speech detection systems handle subtle, reasoning-based (\"soft\") hate compared to explicit (\"hard\") hate.", "motivation": "Most existing moderation and hate-speech detection systems focus on surface-level toxicity and explicit slurs. However, a large portion of online hostility is expressed in a subtle way, via seemingly reasonable arguments and framing that still promote blaming or excluding target groups. Current benchmarks do not systematically measure how detection systems perform on such soft hate, so we lack a clear understanding of this vulnerability.", "method": "The authors create SoftHateBench, a generative benchmark that transforms explicitly hateful content into soft-hate variants while preserving the underlying hostile stance. They integrate the Argumentum Model of Topics (AMT) to structure the core argument\u2014recasting explicit hate into a value- or reason-based argumentative form\u2014and Relevance Theory (RT) to ensure that the generated argument chains stay logically coherent and contextually relevant. The benchmark covers 7 sociocultural domains and 28 target groups and results in 4,745 soft-hate instances. They then evaluate multiple kinds of detection systems, including encoder-based classifiers, general-purpose large language models, and dedicated safety models, comparing performance on hard vs. soft hate versions.", "result": "Across all evaluated systems, there is a consistent performance drop when moving from detecting explicit (hard) hate to subtle, reasoning-based (soft) hate, even though both express the same underlying hostile stance. Models that perform well on visible toxicity markers and slurs frequently fail to flag content when it is reformulated as soft hate following argumentative and relevance principles.", "conclusion": "SoftHateBench reveals a systematic blind spot in current hate-speech and safety detection systems: they are not robust to reasoning-driven, subtly framed hate. The paper argues that future moderation and safety approaches must go beyond surface-level toxicity and incorporate deeper modeling of argumentation, framing, and stance to effectively capture soft hate. SoftHateBench is proposed as a resource to drive such advances and to benchmark progress."}}
{"id": "2601.20275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20275", "abs": "https://arxiv.org/abs/2601.20275", "authors": ["Elina Sigdel", "Anastasia Panfilova"], "title": "RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis", "comment": "The link to the platform: https://ruslica.ipran.ru", "summary": "Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.", "AI": {"tldr": "The paper presents a Russian adaptation of LIWC, constructing a psycholinguistic dictionary and analyzer tailored to Russian grammar and culture, and integrating both traditional linguistic features and language-model-based predictions into a 96-category framework, implemented in the RusLICA web service.", "motivation": "LIWC is widely used to extract psycholinguistic features from text, but it was originally designed for English and often simply translated into other languages, which can miss language-specific grammatical and cultural nuances. Russian, with its rich morphology and distinct cultural context, requires a dedicated adaptation rather than a direct translation to ensure accurate psycholinguistic analysis.", "method": "The authors adapt the LIWC methodology to Russian by (1) designing a 96-category feature scheme that includes syntactic, morphological, lexical, general statistical features, and LM-based prediction outputs; (2) constructing a Russian-specific dictionary from multiple lexicographic resources, semantic dictionaries, and corpora instead of translating existing LIWC thesauri; (3) mapping Russian lemmas to 42 psycholinguistic categories; and (4) implementing the resulting analyzer within the RusLICA web service.", "result": "The outcome is a Russian psycholinguistic dictionary and analysis tool aligned with LIWC principles but tailored to Russian: 96 overall categories, including 42 psycholinguistic ones, linked to lemmas compiled from diverse Russian linguistic resources, and integrated into a functional web-based system (RusLICA) capable of analyzing Russian texts using both traditional linguistic features and pre-trained LM predictions.", "conclusion": "A language- and culture-specific adaptation of LIWC for Russian is feasible and preferable to direct translation. By leveraging Russian linguistic resources and incorporating modern LM-based features, the RusLICA analyzer provides a richer and more accurate framework for psycholinguistic text analysis in Russian, supporting research that requires nuanced characterization of written language in this linguistic context."}}
{"id": "2601.20276", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20276", "abs": "https://arxiv.org/abs/2601.20276", "authors": ["Tianwei Lin", "Zuyi Zhou", "Xinda Zhao", "Chenke Wang", "Xiaohong Li", "Yu Chen", "Chuanrui Hu", "Jian Pei", "Yafeng Deng"], "title": "Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale", "comment": null, "summary": "Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.", "AI": {"tldr": "The paper introduces EverMemBench-S, an adversarial long-context benchmark showing that current LLMs fail to reliably access the right evidence when many semantically similar distractors are present, revealing a gap between standard Needle-in-a-Haystack tests and realistic large-memory settings.", "motivation": "Existing long-context evaluations like Needle-in-a-Haystack mostly test whether models can locate a nearly unique span in otherwise irrelevant text, which overestimates real-world capabilities. In realistic large-memory or retrieval settings, models must discriminate among many semantically similar pieces of information and remain faithful to the right evidence. The authors aim to build a benchmark and protocol that stress this semantic interference problem and allow fair comparison of native long-context models and retrieval systems.", "method": "The authors construct EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark grounded in a 326M-token MemoryBank. For long-context models, they slice the corpus into contexts that fit within each model\u2019s context window (up to 1M tokens). Each query is paired with (1) a gold evidence set that can span one or more documents and (2) collision-tested near-miss hard negatives that are semantically similar but incorrect. Evidence sets are validated via human screening and LLM verification. They also design a decoupled diagnostic protocol that separately measures evidence access (document-ID localization) and end-to-end QA performance under full-context prompting, enabling direct comparison between native long-context prompting and RAG-style retrieval pipelines across corpora of different sizes.", "result": "When evaluating systems along a ladder of reference corpora, from isolated 64K-contexts up to a globally shared 326M-token environment, models that previously saturate standard benign NIAH tests show a sharp drop in their ability to access the correct evidence under strong semantic interference from hard negatives. This drop is observed for both native long-context models and retrieval pipelines when assessed with the proposed protocol.", "conclusion": "The study demonstrates a significant reality gap between performance on standard NIAH benchmarks and performance in realistic large-memory environments. The limiting factor for long-context LLM memory is not just context length, but the ability to perform fine-grained semantic discrimination and resist interference from near-miss distractors. EMB-S and the associated diagnostic protocol provide a more faithful and challenging evaluation framework for future long-context models and retrieval systems."}}
{"id": "2601.20300", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20300", "abs": "https://arxiv.org/abs/2601.20300", "authors": ["Jing Xu", "Minglin Wu", "Xueyuan Chen", "Xixin Wu", "Helen Meng"], "title": "MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting", "comment": "Accepted by ICASSP2026", "summary": "Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.", "AI": {"tldr": "MiLorE-SSL is a lightweight continual multilingual self-supervised speech learning framework that uses LoRA and a soft MoE, plus small replay buffers, to add new languages with minimal parameters and reduced forgetting.", "motivation": "Existing multilingual self-supervised speech models are limited to languages seen during pretraining. Extending them to new languages is challenging: retraining from scratch is computationally prohibitive, and naive sequential training causes catastrophic forgetting of previously learned languages. There is a need for an efficient, parameter- and compute-light method that can continually add new languages while preserving or even improving performance on existing ones.", "method": "The paper proposes MiLorE-SSL, a continual multilingual training framework that augments a pretrained SSL speech model with Low-Rank Adaptation (LoRA) modules and a soft mixture-of-experts (MoE) design. LoRA introduces low-rank parameter updates to adapt the model efficiently, while the soft MoE enables flexible sharing and routing of expert modules across languages, which helps to reduce cross-lingual interference. Additionally, the framework incorporates a limited replay strategy that reuses a small subset of data from existing languages to mitigate catastrophic forgetting without requiring full access to large historical datasets.", "result": "On the ML-SUPERB benchmark, MiLorE-SSL shows strong performance on newly added languages and also enhances performance on previously supported languages, despite training only 2.14% of the model parameters. This indicates both effective knowledge transfer to new languages and retention/improvement of prior capabilities, with a very small parameter footprint.", "conclusion": "MiLorE-SSL enables efficient continual multilingual self-supervised speech learning by combining LoRA-based low-rank adaptation, a soft MoE for shared language experts, and limited replay data. It achieves good accuracy on new languages and even improves existing language performance while updating only a small fraction of parameters, offering a practical solution for scaling SSL models to more languages without full retraining or severe forgetting."}}
{"id": "2601.20312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20312", "abs": "https://arxiv.org/abs/2601.20312", "authors": ["Kaiyuan Chen", "Guangmin Zheng", "Jin Wang", "Xiaobing Zhou", "Xuejie Zhang"], "title": "SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger", "comment": "Accepted by AAAI 2026", "summary": "Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.", "AI": {"tldr": "A new method (SAPO) improves small language models by efficiently supervising intermediate reasoning steps, reducing the gap between reasoning and verification without expensive Monte Carlo sampling.", "motivation": "Current self-evolution methods for language models do not effectively use fine-grained reasoning steps, causing a mismatch between the model that reasons and the mechanism that verifies answers (the reasoner-verifier gap). Existing approaches that add process supervision often rely on computationally heavy Monte Carlo estimation, making them inefficient and hard to scale. Inspired by the human brain\u2019s Error-Related Negativity (ERN)\u2014which quickly localizes and reacts to mistakes\u2014the authors aim to design a more adaptive, efficient way for models to learn from their reasoning process.", "method": "They propose Self-Adaptive Process Optimization (SAPO), a training framework for Small Language Models that: (1) monitors the gap between the reasoner (which produces step-by-step reasoning) and the verifier (which checks correctness); (2) adaptively injects process supervision signals focused on fine-grained reasoning steps; and (3) does this by actively minimizing the reasoner-verifier gap rather than using repeated Monte Carlo rollouts. The supervision is allocated in a targeted, self-adaptive way to guide rapid correction of erroneous reasoning steps, analogous to ERN in humans.", "result": "On challenging math and coding benchmarks, SAPO achieves better performance than most existing self-evolution and process-supervision methods. It improves both reasoning quality and final task accuracy for SLMs. The authors also build two new benchmarks specifically for process reward models in math and code, and use them to show SAPO\u2019s beneficial effect on verifier performance.", "conclusion": "SAPO offers an efficient, biologically inspired approach to self-improvement for small language models by directly reducing the reasoner-verifier gap with adaptive process supervision, avoiding expensive Monte Carlo methods. The method yields state-of-the-art or near state-of-the-art results on math and coding tasks and is supported by new benchmarks that enable more precise evaluation of process reward models and verifiers."}}
{"id": "2601.20326", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20326", "abs": "https://arxiv.org/abs/2601.20326", "authors": ["Zeyu Xing", "Xing Li", "Hui-Ling Zhen", "Mingxuan Yuan", "Sinno Jialin Pan"], "title": "Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning", "comment": "Accepted by ICLR26", "summary": "KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \\textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \\textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.", "AI": {"tldr": "This paper shows that the key\u2013value (KV) cache used in autoregressive decoding can be directly reused as a lightweight representation for downstream tasks, avoiding recomputation of hidden states while preserving strong performance.", "motivation": "During autoregressive inference, large language models maintain a KV cache to speed up decoding, but this cache is normally discarded after use. At the same time, many applications need intermediate representations (embeddings, hidden states) for tasks like retrieval, routing, or adaptive computation, which are costly to compute and store. The motivation is to see whether the already-available KV cache can act as a free representation substrate, reducing memory and compute while still supporting nontrivial tasks.", "method": "The authors treat the KV cache as the primary representation instead of conventional final-layer hidden states or explicit embeddings. They design and evaluate two applications: (i) Chain-of-Embedding, which uses KV-derived representations within multi-step reasoning pipelines, and (ii) Fast/Slow Thinking Switching, which uses KV-based signals to adaptively switch between cheap/fast and expensive/slow reasoning modes. They instantiate and test these ideas on several open LLMs (Llama-3.1-8B-Instruct, Qwen2-7B-Instruct, Qwen3-8B, DeepSeek-R1-Distil-Qwen-14B), comparing performance and efficiency against baselines relying on standard embeddings.", "result": "KV-derived representations, while weaker than task-specific embeddings, achieve competitive or better performance in Chain-of-Embedding experiments on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. In the Fast/Slow Thinking Switching setting on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, the KV-based approach enables adaptive reasoning that reduces generated tokens by up to 5.7\u00d7 with only small drops in accuracy. This demonstrates that KV caches contain enough contextual information to power meaningful downstream decisions without extra forward passes.", "conclusion": "The paper concludes that KV caches can serve as a practical, zero-cost representation layer for sampling and reasoning in LLMs. By reusing KV states that are already computed for decoding, systems can avoid storing or recomputing full hidden states while still supporting advanced behaviors like chain-of-embedding and adaptive fast/slow reasoning. This opens avenues for more efficient LLM inference pipelines where representation reuse from KV caches becomes a standard design choice."}}
{"id": "2601.20327", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20327", "abs": "https://arxiv.org/abs/2601.20327", "authors": ["Xinyu Hu", "Yancheng He", "Weixun Wang", "Tao Feng", "Li Lin", "Jiashun Liu", "Wenbo Su", "Bo Zheng", "Xiaojun Wan"], "title": "CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria", "comment": "Under Review", "summary": "Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.", "AI": {"tldr": "The paper introduces CE-RM-4B, a pointwise generative reward model that improves the reliability of LLM-as-a-Judge evaluation and yields better performance both on reward model benchmarks and in reinforcement learning applications.", "motivation": "Automatic evaluation of open-ended text generation is difficult because traditional rule-based metrics often fail. LLM-as-a-Judge methods are promising as flexible evaluators and reward models for RL, but there is a mismatch between their strong benchmark scores and their weaker practical impact on RL training. The authors aim to close this gap by addressing flaws in current setups, such as reliance on pairwise comparison and under-optimized evaluation criteria.", "method": "The authors design CE-RM-4B, a pointwise generative reward model. Instead of only comparing pairs of answers, it scores single model outputs using unified query-based evaluation criteria. They train it with a specialized two-stage rollout procedure, and they build a compact but high-quality training set of about 5.7K preference samples derived from open-source preference data. The training explicitly optimizes the evaluation criteria to better align the reward model with downstream RL needs.", "result": "CE-RM-4B attains better results than existing reward models on multiple standard reward-modeling benchmarks, with particularly strong gains in Best-of-N selection settings. It also produces more effective reward signals in RL experiments, giving larger improvements in the final RL-trained models than prior LLM-as-a-Judge approaches with similar scale.", "conclusion": "A carefully designed pointwise, query-conditioned reward model with unified criteria and a dedicated rollout training scheme can significantly improve both benchmark and practical RL performance over prevailing pairwise LLM-as-a-Judge approaches. This suggests that better formulation and training of reward models are as important as model size or dataset scale for successful RL from human feedback."}}
{"id": "2601.20330", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20330", "abs": "https://arxiv.org/abs/2601.20330", "authors": ["Zhuang Chen", "Dazhen Wan", "Zhangkai Zheng", "Guanqun Bi", "Xiyao Xiao", "Binghang Li", "Minlie Huang"], "title": "PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments", "comment": null, "summary": "While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.", "AI": {"tldr": "The paper introduces a structured, competition-based framework to evaluate and improve the therapeutic competence of large language models in mental health counseling.", "motivation": "Evaluating LLMs in mental healthcare is hard because real counseling is unstructured, long-term, and goal-driven, while current benchmarks are typically static and pointwise. Existing evaluations suffer from instability: client simulations drift away from target counseling goals (process drift) and static scoring methods provide unreliable and unanchored judgments (standard drift). A better, more stable and realistic evaluation paradigm is needed.", "method": "The authors propose Ps (PsychePass), a trajectory-anchored tournament framework. First, they anchor interaction trajectories by using controlled client simulations that deliberately steer the consultation to test different therapeutic capabilities. Second, they anchor judgment trajectories by placing models into an efficient Swiss-system tournament where models engage in dynamic pairwise battles along these trajectories. The outcomes are used to compute robust Elo ratings. They also repurpose the tournament trajectories and outcomes as reward signals to conduct on-policy reinforcement learning that aligns LLM behavior with therapeutic competence signals.", "result": "Using PsychePass, the authors obtain stable Elo-based rankings of LLMs\u2019 therapeutic competence and show that these rankings are highly consistent with human expert assessments. They further demonstrate that training models with the derived reward signals through RL significantly improves their performance in simulated counseling tasks. Experimental results confirm both the reliability of the evaluation and the effectiveness of the RL-based improvement.", "conclusion": "Trajectory-anchored tournaments offer a more stable and realistic way to evaluate LLMs in mental healthcare than traditional static benchmarks. By jointly anchoring the interaction and judgment processes, PsychePass can reliably calibrate therapeutic competence, closely match expert evaluations, and provide usable reward signals to further improve models through reinforcement learning. This establishes a practical path toward safer and more competent LLM-based mental health support systems."}}
{"id": "2601.20335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20335", "abs": "https://arxiv.org/abs/2601.20335", "authors": ["Qinzhuo Wu", "Zhizhuo Yang", "Hanhao Li", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "title": "MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment", "comment": null, "summary": "Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.", "AI": {"tldr": "The paper introduces MobileBench-OL, an online benchmark for evaluating mobile GUI agents across task execution, complex reasoning, and robustness to environmental noise.", "motivation": "Existing mobile GUI agent benchmarks are limited: offline settings lack realism, and current online benchmarks mainly test instruction-following while overlooking agents\u2019 reasoning, exploration capabilities, and robustness to random noise in real mobile environments. This creates a mismatch between benchmark performance and real-world deployment needs.", "method": "The authors build MobileBench-OL, an online benchmark comprising 1080 tasks drawn from 80 Chinese mobile apps. The benchmark is divided into five subsets that target different evaluation dimensions\u2014task completion, complex reasoning, and noise robustness. They also design an automatic evaluation framework with a reset mechanism so that experiments in real environments are stable, repeatable, and scalable. They then test 12 state-of-the-art GUI agents on this benchmark and complement automatic evaluation with human assessment.", "result": "Across 12 leading mobile GUI agents, performance on MobileBench-OL reveals substantial gaps from the level required for reliable real-world use, especially on complex reasoning and robustness to noise. Human annotators\u2019 assessments align with the automatic evaluation results, suggesting the benchmark faithfully captures real-world performance differences.", "conclusion": "MobileBench-OL provides a more realistic, multi-dimensional online benchmark for mobile GUI agents, explicitly assessing task execution, reasoning, and noise robustness. The benchmark and its auto-eval framework expose significant room for improvement in current agents and offer a reliable tool for measuring and advancing their real-world capabilities. The authors plan to release data and code upon acceptance to facilitate further research."}}
{"id": "2601.20412", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20412", "abs": "https://arxiv.org/abs/2601.20412", "authors": ["Qihao Wang", "Yue Hu", "Mingzhe Lu", "Jiayue Wu", "Yanbing Liu", "Yuanmin Tang"], "title": "Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents", "comment": "Accepted to AAAI 2026", "summary": "The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.", "AI": {"tldr": "They propose a cognitive-load-based framework and a new benchmark (ToolLoad-Bench) to diagnose where tool-using LLM agents fail, by decomposing task complexity into intrinsic and extraneous load and validating that this predicts performance cliffs as tasks get harder.", "motivation": "Existing tool-use benchmarks mainly report final task accuracy, which shows headline performance but hides *why* models fail or where their true capability limits lie. There is no principled way to relate task complexity to model behavior, especially for tool-using agents. Cognitive Load Theory provides a lens for understanding human problem solving but has not been operationalized to analyze LLM agents\u2019 tool use. The authors want a diagnostic, theory-grounded way to map agents\u2019 capability boundaries rather than just ranking scores.", "method": "They build a framework based on Cognitive Load Theory that decomposes task complexity during tool use into two measurable parts. (1) Intrinsic Load: formalized via a new representation called a Tool Interaction Graph, which captures the structural complexity of the sequence and dependencies of tool calls required for a solution. (2) Extraneous Load: defined as difficulty caused by ambiguity or suboptimal problem presentation. They then construct ToolLoad-Bench, a benchmark where these two forms of load can be parametrically and independently adjusted. Using this benchmark, they systematically vary cognitive load and measure LLM performance across conditions.", "result": "Experiments on ToolLoad-Bench show clear performance cliffs: as intrinsic or extraneous cognitive load increases beyond certain thresholds, model accuracy drops sharply. Different models exhibit different cliff locations, which correspond to distinct capability boundaries. The quantitative measures of load derived from their framework align closely with the observed empirical performance across conditions, indicating that their cognitive-load-based metrics are well-calibrated predictors of agent success or failure on tool-use tasks.", "conclusion": "The proposed framework provides a principled, theory-based way to analyze and predict the limits of LLM tool-using agents, going beyond raw accuracy. ToolLoad-Bench enables controlled manipulation of cognitive load, making it possible to precisely map where models break down and to compare models in terms of their robustness to intrinsic and extraneous complexity. This gives both a methodological foundation for future cognitive analysis of LLMs and a practical basis for designing more efficient and reliable tool-using systems."}}
{"id": "2601.20417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20417", "abs": "https://arxiv.org/abs/2601.20417", "authors": ["Biswesh Mohapatra", "Marcely Zanon Boito", "Ioan Calapodescu"], "title": "SpeechMapper: Speech-to-text Embedding Projector for LLMs", "comment": "Accepted to ICASSP 2026", "summary": "Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.", "AI": {"tldr": "SpeechMapper is a cost-efficient training approach that maps speech to LLM embeddings, reducing overfitting and compute while achieving strong performance on speech translation and spoken QA.", "motivation": "Existing speech LLMs jointly train speech encoders, projection layers, and LLMs end-to-end on instruction data, which is computationally expensive and prone to overfitting to specific tasks and prompts. There is a need for a more efficient, robust, and generalizable way to connect speech models to LLMs without large-scale instruction tuning.", "method": "SpeechMapper first pretrains a speech-to-LLM-embedding mapping module without involving the LLM, using cheaper hardware and data. After this standalone pretraining, the module is attached to a target LLM and aligned via a short, 1K-step instruction tuning stage. The authors evaluate two instruction-tuning paradigms: (1) task-agnostic IT, where adaptation is based on ASR outputs and does not use target-task labels, and (2) task-specific IT, where the model is tuned directly on the downstream task data.", "result": "On speech translation and spoken question answering benchmarks, the pretrained SpeechMapper block proves versatile. In task-agnostic settings, it matches the best instruction-following speech LLM from IWSLT25 despite never being trained on those specific tasks. In task-specific settings, SpeechMapper surpasses that strong baseline on many datasets while using less training data and compute resources.", "conclusion": "SpeechMapper provides a practical, scalable alternative to full end-to-end instruction tuning for speech LLMs. By decoupling speech-to-embedding pretraining from lightweight downstream alignment, it achieves efficient, robust, and generalizable integration of speech with LLMs, reducing both computational cost and overfitting risk while maintaining or improving performance."}}
{"id": "2601.20424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20424", "abs": "https://arxiv.org/abs/2601.20424", "authors": ["Anna Ristil\u00e4", "Otto Tarkka", "Veronika Laippala", "Kimmo Elo"], "title": "Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020", "comment": "27 pages (40 including appendices), 5 figures (13 including sub-figures), 1 table, 1 formula, 3 appendices; submitted to JDMDH", "summary": "Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.", "AI": {"tldr": "The paper analyzes how emotions vary by topic in Finnish parliamentary speeches (2000\u20132020) using an emotion analysis model, showing increasing positivity and distinct topic-specific emotional patterns over time.", "motivation": "Most studies treat parliamentary discourse as uniform and do not systematically examine which topics elicit which emotions, despite common intuitions that some issues are more emotionally charged. There is a lack of empirical, topic-level evidence on emotion expression in parliamentary debates, especially over long periods.", "method": "The authors compile a corpus of speeches from the Finnish Parliament (Eduskunta) spanning 2000\u20132020, identify topics within the speeches, and apply an emotion analysis (emotion detection) model to measure emotional expressions. They then analyze these emotions across topics at single points in time (synchronic) and across time (diachronic) to detect patterns and trends.", "result": "The analysis reveals that (1) positivity in parliamentary speech has increased over the 20\u2011year period, reinforcing earlier findings, and (2) there are clear topic-specific differences in emotional expression, with some topics systematically associated with stronger or distinct emotional profiles than others.", "conclusion": "Parliamentary discourse is not emotionally homogeneous across topics: different issues carry distinct emotional signatures. Over time, Finnish parliamentary speeches have become more positive overall. Topic-aware emotion analysis offers a more nuanced understanding of parliamentary debate and can enrich research on political communication and legislative behavior."}}
{"id": "2601.20439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20439", "abs": "https://arxiv.org/abs/2601.20439", "authors": ["Qihao Wang", "Mingzhe Lu", "Jiayue Wu", "Yue Hu", "Yanbing Liu"], "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use", "comment": "Accepted to PRICAI25", "summary": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.", "AI": {"tldr": "PEARL is a two-stage framework that improves LLM planning and execution for complex multi-turn tool use, achieving state-of-the-art performance on tool-use benchmarks.", "motivation": "Existing LLM-based tool agents struggle with complex, multi-step, multi-turn tool use due to weak planning, hallucinated tools, incorrect parameters, and fragile interactions. There is a need for a systematic way to learn how to plan and execute tool calls robustly, and to provide better training signals specific to planning quality rather than just final task success.", "method": "PEARL uses a two-stage framework. In an offline exploration phase, the agent systematically interacts with available tools to discover valid usage patterns and characterize common failure modes. In an online phase, a dedicated Planner module is trained with reinforcement learning, specifically group Relative Policy Optimization (GRPO). A carefully designed reward function gives differentiated signals for aspects of planning quality (such as correct tool choice, argument construction, and sequencing) rather than a single sparse reward, enabling more targeted optimization of planning behavior.", "result": "On the ToolHop and T-Eval benchmarks for tool-use tasks, PEARL substantially outperforms prior approaches. It achieves a new state-of-the-art success rate of 56.5% on ToolHop while keeping tool invocation errors low, indicating that it improves both task completion and reliability of tool usage compared to baselines.", "conclusion": "By combining offline tool exploration with an online RL-trained Planner and a planning-aware reward design, PEARL effectively addresses key challenges in complex tool use by LLMs, such as weak planning and tool hallucinations. This represents a significant step toward more robust, reliable LLM agents capable of sophisticated multi-step tool interactions."}}
{"id": "2601.20451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20451", "abs": "https://arxiv.org/abs/2601.20451", "authors": ["Diandian Guo", "Fangfang Yuan", "Cong Cao", "Xixun Lin", "Chuan Zhou", "Hao Peng", "Yanan Cao", "Yanbing Liu"], "title": "MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues", "comment": "12 pages, 7 figures. Accepted by WWW 2026", "summary": "The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.", "AI": {"tldr": "They propose MuVaC, a causal-variational multimodal framework that jointly performs sarcasm detection and explanation by modeling their causal relationship, aligning+fusing multimodal features, and enforcing consistency between predictions and explanations.", "motivation": "Sarcasm is frequent in multimodal social media content, and understanding it requires both detecting sarcasm and explaining why it is sarcastic. Existing work usually treats Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE) separately or loosely combines them, ignoring their causal dependency: explanations are the reasoning process leading to detection. This limits robustness, interpretability, and performance in real-world sarcasm understanding.", "method": "They propose MuVaC, a variational causal inference framework inspired by human cognition for sarcasm understanding. First, they formalize MSD and MuSE using structural causal models and construct variational causal pathways that specify how multimodal inputs cause latent reasoning variables, which in turn cause detection labels and explanations. These pathways define a joint optimization objective. Then, they design an alignment-then-fusion strategy: (1) align features across modalities (e.g., text, image) to ensure cross-modal correspondence; (2) fuse them into robust multimodal representations tailored for both detection and explanation generation. Finally, they add a consistency constraint between detection outputs and generated explanations to improve reasoning trustworthiness and coherence.", "result": "On public multimodal sarcasm datasets, MuVaC achieves better performance than prior methods on both MSD and MuSE tasks. The model shows stronger multimodal feature learning, more accurate sarcasm detection, and more reliable explanations, reflecting the benefits of modeling causal dependencies and enforcing prediction\u2013explanation consistency.", "conclusion": "Jointly modeling sarcasm detection and explanation within a variational causal framework, and carefully aligning and fusing multimodal features while enforcing consistency between outputs, leads to superior and more trustworthy multimodal sarcasm understanding. MuVaC provides an effective and cognitively inspired approach and opens a new causal perspective for future multimodal sarcasm research."}}
{"id": "2601.20465", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20465", "abs": "https://arxiv.org/abs/2601.20465", "authors": ["Yang Li", "Jiaxiang Liu", "Yusong Wang", "Yujie Wu", "Mingkun Xu"], "title": "BMAM: Brain-inspired Multi-Agent Memory Framework", "comment": "Submitted to ACL (ARR 2026 January submission); non-anonymous preprint", "summary": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.", "AI": {"tldr": "The paper introduces BMAM, a brain-inspired multi-component memory architecture for language-model agents, to prevent loss of temporally grounded information and behavioral inconsistency over long interactions, achieving strong performance on the LoCoMo long-horizon benchmark.", "motivation": "Long-horizon language-model agents often fail to reliably maintain and use information over time and across sessions, leading to \"soul erosion\"\u2014loss of consistent identity, goals, and temporally grounded knowledge. Existing approaches typically rely on a single, relatively unstructured memory store that cannot adequately capture different types of information (events, facts, priorities, control signals) at different time scales. There is a need for a more cognitively inspired memory system that preserves temporal structure and supports robust long-range reasoning.", "method": "The authors propose BMAM (Brain-inspired Multi-Agent Memory), a modular memory architecture for LLM-based agents. Instead of a single monolithic memory, BMAM decomposes memory into four specialized subsystems\u2014episodic, semantic, salience-aware, and control-oriented\u2014each designed to operate on complementary time scales and types of information. Episodic memory is organized explicitly along timelines for temporal reasoning; retrieval fuses multiple signals (e.g., recency, relevance, salience) to select evidence. The design is inspired by cognitive neuroscience, particularly hippocampal episodic memory, and is implemented and evaluated within a long-horizon agent framework.", "result": "On the LoCoMo benchmark for long-horizon reasoning, BMAM reaches 78.45% accuracy in the standard evaluation setting, outperforming baselines (implied, though not numerically specified in the abstract). Ablation studies show that removing or weakening the hippocampus-inspired episodic memory subsystem substantially degrades temporal reasoning performance, indicating its central importance within BMAM.", "conclusion": "BMAM demonstrates that a brain-inspired, multi-component memory architecture can substantially improve long-horizon reasoning and consistency for language-model agents compared with monolithic memory stores. Explicitly structured episodic timelines combined with specialized semantic, salience-aware, and control memory subsystems mitigate \"soul erosion\" and yield strong empirical gains on the LoCoMo benchmark. The findings support modeling agent memory after cognitive systems, especially hippocampal episodic memory, for robust temporal reasoning."}}
{"id": "2601.20674", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20674", "abs": "https://arxiv.org/abs/2601.20674", "authors": ["Juan Jose Rubio Jan", "Jack Wu", "Julia Ive"], "title": "Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science", "comment": "11 pages, 5 figures", "summary": "This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.", "AI": {"tldr": "This paper evaluates how well large language models can query structured EHR data and extract information from clinical notes (via RAG), using an automatic synthetic QA framework on MIMIC-III and mixed metrics (exact match, semantic, human).", "motivation": "Electronic Health Records contain both structured tables and unstructured clinical notes, and current analytic workflows require significant technical expertise and manual effort. With the rapid progress of LLMs, there is a need to systematically assess whether they can reliably support common EHR data science tasks\u2014querying large structured datasets and extracting accurate information from free text\u2014especially in safety\u2011critical clinical settings where precision and semantic correctness are vital.", "method": "The authors design an evaluation framework that automatically generates synthetic, dataset\u2011specific question\u2013answer pairs for two tasks: (1) querying structured EHR data using programmatic interfaces (Python/Pandas) and (2) information extraction from unstructured clinical text using a Retrieval Augmented Generation (RAG) pipeline. They apply this framework to a curated subset of MIMIC-III (four structured tables plus one clinical note type) and test several LLMs, both locally hosted and API-based. Performance is assessed using exact match scoring, semantic similarity metrics, and human expert judgments.", "result": "Across both structured querying and RAG-based information extraction tasks, LLMs achieve competitive performance, showing that they can generate correct programmatic queries for EHR analytics and extract semantically accurate information from clinical notes when supported by retrieval. Results vary across models and tasks, but the overall trend indicates strong potential for LLM-assisted EHR data science, as reflected in favorable quantitative metrics and corroborated by human evaluation.", "conclusion": "LLMs, when properly orchestrated (e.g., via programmatic query generation and RAG pipelines) and evaluated with a tailored synthetic QA framework, can reliably support key EHR data science tasks like structured querying and clinical information extraction. This suggests a promising role for LLMs in future clinical workflows, while also underscoring the need for careful evaluation, model selection, and human oversight in deployment."}}
{"id": "2601.20476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20476", "abs": "https://arxiv.org/abs/2601.20476", "authors": ["Evanfiya Logacheva", "Arto Hellas", "Tsvetomila Mihaylova", "Juha Sorva", "Ava Heinonen", "Juho Leinonen"], "title": "Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch", "comment": null, "summary": "Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.", "AI": {"tldr": "They propose and preliminarily validate an RST-based, in-context example method to generate more faithful, less-hallucinated AI-produced diagrams for computing education.", "motivation": "Generative AI is widely used in computing education to create learning materials, including diagrams, but the quality and factual reliability of these generated artifacts are inconsistent and often misaligned with user expectations. Educators and students are concerned about hallucinations and poor structural quality in AI-generated diagrams, yet there is little systematic methodology to guide LLMs to produce logically organized, faithful diagrams or to evaluate them automatically.", "method": "The authors design a new prompting and generation method for diagram code that leverages Rhetorical Structure Theory (RST) and in-context examples to better align LLM outputs with the discourse structure of the source text. They then have computer science educators evaluate 150 LLM-generated diagrams along several dimensions: logical organization, connectivity, layout aesthetics, and presence of AI hallucinations. They also analyze this labeled set for its potential use in automated diagram evaluation. Finally, they conduct a qualitative and quantitative analysis of how hallucinations relate to diagram quality and textual context complexity.", "result": "Using the RST-based, in-context method reduces factual hallucinations in the generated diagrams and improves how faithfully diagrams reflect the given textual context, though quality still varies due to the inherent stochasticity of LLMs. The expert evaluations provide labeled evidence on multiple quality dimensions, and the dataset appears promising as a resource for developing automated diagram evaluation methods. The analyses show a clear trend: more complex input texts correlate with higher hallucination rates, and LLMs frequently fail to recognize or correct their own diagrammatic errors.", "conclusion": "The proposed RST-guided, in-context diagram generation method is a promising approach to improving the reliability and faithfulness of AI-generated diagrams for computing education, particularly in reducing hallucinations. However, output quality remains variable and sensitive to input complexity, underscoring the need for better control of LLM stochasticity and more robust evaluation mechanisms. The link between text complexity and hallucination, as well as LLMs\u0019 poor self-detection of errors, highlights important directions for future work in automated diagram generation and assessment."}}
{"id": "2601.20731", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20731", "abs": "https://arxiv.org/abs/2601.20731", "authors": ["Mae Sosto", "Delfina Sol Martinez Pandiani", "Laura Hollink"], "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks", "comment": null, "summary": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.", "AI": {"tldr": "The paper studies how large language models reproduce heterocisnormative norms and how this creates measurable bias in their text outputs.", "motivation": "To understand whether and how LLMs encode and reproduce dominant social norms about gender and sexuality, and to quantify the resulting representational harms for different groups.", "method": "They compare model outputs when prompts explicitly mark subjects as queer, non-queer, or unmarked. Using English sentence completion tasks, they measure differences along sentiment, regard, toxicity, and prediction diversity, and compare behaviors of masked and autoregressive language models (both open and closed access).", "result": "Masked language models show the worst outcomes for queer-marked subjects: more negative sentiment, higher toxicity, and lower regard. Autoregressive models reduce but do not remove these patterns. Closed-access autoregressive models more often produce harmful outputs for unmarked subjects. Bias patterns vary substantially by model type and characteristics.", "conclusion": "LLMs encode and reproduce normative social assumptions such as heterocisnormativity. Model design choices shift where and how harms appear but do not remove them, implying that mitigating representational harms requires more than just changing architecture or access type."}}
{"id": "2601.20546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20546", "abs": "https://arxiv.org/abs/2601.20546", "authors": ["Kumiko Nakajima", "Jan Zuiderveld", "Sandro Pezzelle"], "title": "Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models", "comment": "Accepted to Findings of EACL 2026", "summary": "Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.", "AI": {"tldr": "The paper argues that common tests of large language model (LLM) creativity are flawed and proposes a new, theory-grounded metric that better distinguishes true creativity (novelty plus appropriateness) from random noise.", "motivation": "Existing evaluations of LLM creativity rely heavily on the Divergent Association Task (DAT), which measures how semantically distant words are from each other. This metric primarily captures novelty but neglects appropriateness, a central dimension in psychological theories of human creativity. Additionally, the fact that simple, non-creative baselines can outperform LLMs on DAT suggests that the metric may not be valid for assessing model creativity. There is a need for an evaluation method that aligns with human creativity theory and robustly separates genuine creativity from meaningless output.", "method": "The authors first benchmark a range of state-of-the-art LLMs on the standard DAT, comparing their scores with those from two non-creative baseline systems to expose limitations of DAT as a creativity measure. Building on human creativity theory, they then design the Conditional Divergent Association Task (CDAT), which evaluates the novelty of responses under explicit contextual constraints that encode appropriateness. CDAT preserves the simple and objective nature of DAT but modifies the task so that high scores require both being unusual and contextually fitting. They apply CDAT across multiple model families and sizes to map out the trade-off between novelty and appropriateness.", "result": "On the original DAT, LLMs score lower than two baseline systems that lack creative capabilities, casting doubt on DAT\u2019s construct validity for model creativity evaluation. Under the new CDAT metric, model behavior changes: smaller model families tend to exhibit higher creativity scores, while more advanced families tend to produce safer, more appropriate but less novel outputs, occupying a different region of the novelty\u2013appropriateness space. This reveals a systematic frontier or trade-off between novelty and appropriateness across models.", "conclusion": "The study concludes that DAT alone is inadequate for assessing LLM creativity because it conflates randomness with creativity and omits appropriateness. The proposed CDAT, grounded in human creativity theory, better differentiates meaningful creative output from noise by conditioning novelty on contextual appropriateness, while remaining easy to administer and score. The findings suggest that training and alignment processes systematically move LLMs toward higher appropriateness at the expense of novelty, thereby reducing measured creativity along the CDAT frontier. The authors provide their dataset and code to support further research and benchmarking of LLM creativity."}}
{"id": "2601.20582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20582", "abs": "https://arxiv.org/abs/2601.20582", "authors": ["Shalom Rosner", "Ronit D. Gross", "Ella Koresh", "Ido Kanter"], "title": "Single-Nodal Spontaneous Symmetry Breaking in NLP Models", "comment": "23 pages, 6 figures, 1 table", "summary": "Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.", "AI": {"tldr": "The paper shows that concepts from spontaneous symmetry breaking in statistical mechanics appear in transformer-based NLP models at the level of individual attention heads and nodes during pre-training and fine-tuning.", "motivation": "To connect theoretical physics concepts\u2014specifically spontaneous symmetry breaking\u2014to the internal learning dynamics of NLP models, and to obtain a principled explanation of how and when individual components (nodes/heads) specialize and cooperate in tasks like token prediction and relation classification.", "method": "The authors empirically study a BERT-6 model pre-trained on Wikipedia and fine-tuned on the FewRel relation classification task. They analyze individual attention heads and even single nodes, measuring their ability to learn specific tokens or labels. They track how learning ability changes with the number of nodes, use a convex-hull-based analysis to upper-bound nodal contributions to the global task, and compare their findings conceptually with spin-glass systems and free-energy minimization in statistical mechanics.", "result": "They find clear evidence of spontaneous symmetry breaking in the model: even with deterministic training and a finite architecture, symmetry is broken at the level of individual attention heads and nodes, which specialize in subsets of tokens or labels. As more nodes are added, there is a crossover in learning behavior, reflecting a balance between random-guess limitations and cooperative gains, with cooperation yielding performance greater than the sum of isolated node capabilities. Each node\u2019s function can be explicitly tied to the global objective and bounded using convex hull analysis.", "conclusion": "Spontaneous symmetry breaking is not only a feature of thermodynamic-limit physical systems but also emerges in finite, deterministic NLP models at the level of individual components. Node- and head-level specializations, and their cooperative behavior, can be systematically characterized and bounded, offering a physics-inspired framework to interpret and analyze internal representations and learning dynamics in transformer architectures."}}
{"id": "2601.20592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20592", "abs": "https://arxiv.org/abs/2601.20592", "authors": ["Ali Basirat", "Danial Namazifard", "Navid Baradaran Hemmati"], "title": "A Computational Approach to Language Contact -- A Case Study of Persian", "comment": null, "summary": "We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.", "AI": {"tldr": "The paper studies how traces of historical language contact appear inside a monolingual Persian language model\u2019s internal representations, and finds that syntactic information is mostly universal while some morphology is highly language-specific.", "motivation": "To understand whether and how historical contact between languages leaves detectable structural traces inside modern neural language models, beyond surface behavior, especially for a contact-rich language like Persian and its related or neighboring languages.", "method": "Train or use a monolingual Persian language model, then expose it to multiple other languages with differing degrees and types of historical contact with Persian. Use probing techniques on intermediate model representations to measure how much linguistic (morphosyntactic) information they encode and where in the model it is localized, focusing on features like syntax, Case, and Gender.", "result": "The model\u2019s internal representations encode syntactic information in a way that is largely invariant to whether a language has had historical contact with Persian, while certain morphological features\u2014especially Case and Gender\u2014vary more strongly with language-specific structure and are sensitive to structural differences rather than shared contact history.", "conclusion": "Language contact effects in monolingual language models are not uniform: universal syntax appears robust and contact-insensitive, whereas some morphology is shaped by language-specific structure, implying that contact leaves selective and structurally constrained traces in model internals rather than broadly homogenizing representations."}}
{"id": "2601.20613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20613", "abs": "https://arxiv.org/abs/2601.20613", "authors": ["Kaiyuan Chen", "Qimin Wu", "Taiyu Hou", "Tianhao Tang", "Xueyu Hu", "Yuchen Hou", "Bikun Li", "Chengming Qian", "Guoyin Wang", "Haolin Chen", "Haotong Tian", "Haoye Zhang", "Haoyu Bian", "Hongbing Pan", "Hongkang Zhang", "Hongyi Zhou", "Jiaqi Cai", "Jiewu Rao", "Jiyuan Ren", "Keduan Huang", "Lucia Zhu Huang", "Mingyu Yuan", "Naixu Guo", "Qicheng Tang", "Qinyan Zhang", "Shuai Chen", "Siheng Chen", "Ting Ting Li", "Xiaoxing Guo", "Yaocheng Zuo", "Yaoqi Guo", "Yinan Wang", "Yinzhou Yu", "Yize Wang", "Yuan Jiang", "Yuan Tian", "Yuanshuo Zhang", "Yuxuan Liu", "Yvette Yan Zeng", "Zenyu Shan", "Zihan Yin", "Xiaobo Hu", "Yang Liu", "Yixin Ren", "Yuan Gong"], "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios", "comment": "17 pages, 8 figures", "summary": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.", "AI": {"tldr": "The paper introduces AgentIF-OneDay, a benchmark to evaluate whether general users can use natural language and AI agents to complete diverse, realistic daily tasks involving files and multi-step workflows.", "motivation": "Existing AI agent evaluations focus primarily on increasing difficulty on narrow domains (e.g., coding, research), and do not reflect the diversity of everyday work, life, and learning tasks experienced by general users. There is a mismatch between impressive benchmark results and what people perceive in daily use. The authors want a benchmark that captures practical, user-centric agentic behaviors across many task types and modalities.", "method": "The authors design AgentIF-OneDay, a benchmark of 104 tasks with 767 scoring points that mirror daily activities. Tasks span three categories: (1) Open Workflow Execution, testing adherence to explicit, complex workflows; (2) Latent Instruction, requiring inference of implicit instructions from given attachments; and (3) Iterative Refinement, where the agent must modify or extend ongoing work. Tasks require dialog-based problem solving, understanding multiple attachment types, and producing concrete file-based outputs. They use instance-level rubrics and a refined evaluation pipeline that aligns LLM-based automatic verification with human judgment, showing 80.1% agreement when using Gemini-3-Pro as the verifier.", "result": "Using AgentIF-OneDay, they benchmark four leading general AI agents. They find that both commercial products built atop LLM APIs and ChatGPT-style agents trained with agent RL form the top performance tier. They also observe that many leading LLM APIs and open-source models already exhibit internalized agentic abilities, sufficient for building competitive agent products. Their evaluation pipeline achieves high agreement (80.1%) between LLM-based and human judgments.", "conclusion": "AgentIF-OneDay provides a user-centric, diverse benchmark for assessing AI agents on realistic daily tasks involving workflows, latent instructions, and iterative refinement. The results suggest that modern LLMs and agent products have strong, internalized agentic capabilities and that LLM-based evaluation can reliably approximate human judgments. This benchmark can guide development and comparison of next-generation agent systems oriented toward everyday use."}}
{"id": "2601.20649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20649", "abs": "https://arxiv.org/abs/2601.20649", "authors": ["Wenlin Zhong", "Chengyuan Liu", "Yiquan Wu", "Bovin Tan", "Changlong Sun", "Yi Wang", "Xiaozhong Liu", "Kun Kuang"], "title": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering", "comment": null, "summary": "While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.", "AI": {"tldr": "The paper proposes Probabilistic Process Supervision (P2S), a self-supervised RL framework that gives dense, step-level rewards for LLM reasoning by measuring how faithful each step is to a high-quality reference reasoning path, improving performance on general-domain QA tasks.", "motivation": "Existing RL with verifiable rewards (RLVR) works well in domains like math and code where answers can be automatically checked, but fails to extend cleanly to open-domain reasoning where no verifiable reward exists. Outcome-only methods like RLPR use the probability of the final answer as a reward, but they ignore the quality of intermediate reasoning steps and suffer from sparse, coarse feedback. The authors want a way to supervise the reasoning process itself\u2014without human-labeled chains or an external reward model\u2014to make RL for general-domain reasoning more effective and stable.", "method": "They introduce Probabilistic Process Supervision (P2S). First, during RL, the method synthesizes multiple reasoning chains and filters them to obtain a single high-quality reference chain (gold-CoT) for each instance. Then, for each step of the model\u2019s current reasoning trajectory, they define a Path Faithfulness Reward (PFR): the conditional probability of the remaining suffix of the gold-CoT given the model\u2019s current prefix. Intuitively, if the model\u2019s current partial reasoning is compatible with continuing along the gold-CoT, it receives a higher reward. This PFR is computed directly from the model\u2019s own probabilities, requires no separate reward model, and is assigned densely at each step. The PFR can be combined with any existing outcome-based reward (such as final answer correctness or reference probability), yielding a composite reward signal used to train the model with reinforcement learning.", "result": "On reading comprehension and medical QA benchmarks, models trained with P2S achieve significantly better performance than baselines that rely solely on outcome-based rewards like RLPR or other strong RL methods. The experiments demonstrate improved accuracy and robustness on these general-domain reasoning tasks, indicating that dense process-level supervision from P2S leads to more effective reasoning than sparse outcome-only rewards.", "conclusion": "P2S shows that it is possible to obtain dense, step-level reinforcement signals for LLM reasoning in open-domain tasks without human-annotated chains or external reward models. By computing a probabilistic Path Faithfulness Reward with respect to an automatically synthesized gold-CoT and combining it with outcome rewards, P2S alleviates reward sparsity and more directly shapes the reasoning process. The framework generalizes RLVR-style benefits to domains lacking verifiable rewards and substantially improves performance on challenging QA benchmarks, suggesting a promising direction for process-supervised RL for LLMs."}}
{"id": "2601.20659", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.20659", "abs": "https://arxiv.org/abs/2601.20659", "authors": ["Sara Candussio"], "title": "A Dialectic Pipeline for Improving LLM Robustness", "comment": null, "summary": "Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.\n  However, methods such as fine-tuning on domain-specific data or the training of a separate \\textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.\n  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.\n  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.\n  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.", "AI": {"tldr": "The paper proposes a self-dialogue (dialectic) pipeline for LLMs that reduces hallucinations and improves answer quality, outperforming standard answers and basic Chain-of-Thought prompting across models and datasets.", "motivation": "Large Language Models often hallucinate and produce low-quality or incorrect answers. Common mitigation strategies like domain-specific fine-tuning or training separate verifiers are computationally expensive and reduce generality, making them impractical for many users and use cases. The authors aim to develop a lightweight, model-agnostic method that improves reliability without sacrificing generalization or requiring heavy retraining.", "method": "The authors design a dialectic pipeline in which the LLM engages in structured self-dialogue: it first produces a tentative answer and then critiques, reflects on, and potentially corrects this answer in subsequent stages. The pipeline is tested under an oracle-RAG setting, where relevant context is always available to the model. Different configurations are explored, including how and when to inject context, how much to summarize or filter this context, and how many self-dialogue steps to perform. The method is evaluated across multiple datasets and model families, and compared to standard prompting and Chain-of-Thought prompting baselines.", "result": "Across datasets and model families, the dialectic self-dialogue pipeline yields significantly better performance than standard single-shot answers and consistently outperforms Chain-of-Thought-only prompting. The experiments also reveal how summarization and filtering of the contextual information at various stages affect the overall performance, although details are not specified in the abstract.", "conclusion": "A structured dialectic self-dialogue pipeline, combined with well-managed contextual information (oracle-RAG), can reduce hallucinations and improve answer quality for LLMs without requiring costly fine-tuning or specialized verifiers. This approach maintains model generality and provides a practical, broadly applicable way to enhance LLM reliability over standard and simple Chain-of-Thought prompting."}}
{"id": "2601.20676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20676", "abs": "https://arxiv.org/abs/2601.20676", "authors": ["Zhuo Chen", "Xinyu Geng", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Pengjun Xie", "Kewei Tu"], "title": "Efficient Multimodal Planning Agent for Visual Question-Answering", "comment": null, "summary": "Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.", "AI": {"tldr": "The paper proposes a multimodal planning agent that dynamically decides which steps of a multimodal retrieval-augmented generation (mRAG) pipeline are necessary for answering visual questions, reducing computation time and tool calls while improving VQA performance over several baselines.", "motivation": "Standard VQA systems, especially for knowledge-intensive questions, often use multi-stage mRAG pipelines with rigid, sequential steps on both image and text modalities. These pipelines introduce inefficiency because every step is executed regardless of its actual necessity for the given question. There is a need to maintain or improve answer accuracy while reducing redundant computation, search time, and expensive tool calls in multimodal VQA systems.", "method": "The authors train a multimodal planning agent that operates over an mRAG pipeline. Instead of following a fixed sequence of retrieval and reasoning steps, the agent dynamically decomposes the VQA problem and selectively invokes parts of the mRAG pipeline. The agent learns a policy to decide, conditioned on the current question and intermediate information, whether specific retrieval or processing steps are required, thereby pruning unnecessary operations while preserving answer quality.", "result": "Experiments conducted on six different VQA-related datasets show that the proposed planning agent reduces search time by over 60% compared to existing mRAG-based approaches and cuts down the number of costly tool calls. At the same time, it achieves better answer accuracy than all evaluated baselines, including a Deep Research agent and a strong prompt-engineered method.", "conclusion": "Dynamically controlling the mRAG pipeline with a learned multimodal planning agent effectively balances efficiency and effectiveness in VQA. The approach significantly reduces computation and tool usage while improving performance across multiple datasets, demonstrating that adaptive pipeline decomposition is a promising direction for scalable, high-performing multimodal QA systems."}}
{"id": "2601.20679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20679", "abs": "https://arxiv.org/abs/2601.20679", "authors": ["Mingqiao Mo", "Yunlong Tan", "Hao Zhang", "Heng Zhang", "Yangfan He"], "title": "ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code", "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.", "AI": {"tldr": "The paper proposes ShieldedCode, a protection-aware framework that uses large language models to learn robust representations of virtual machine protected (VMP) code, improving both protected code generation and binary similarity detection.", "motivation": "Traditional virtual machine protection (VMP) for software defense is based on handcrafted, rule-based transformations that are difficult and costly to design and maintain, and increasingly vulnerable to automated reverse engineering. Meanwhile, large language models have excelled at code generation but have not been fully explored for software protection tasks. There is a need for a learning-based approach that can understand, generate, and compare VMP-protected code, capturing both functional semantics and protection strength to improve resilience against reverse engineering.", "method": "The authors build large-scale paired datasets consisting of source code and corresponding normalized virtual machine (VM) implementations. They introduce hierarchical dependency modeling at three granularities: intra-instruction, preceding-instruction, and inter-instruction, enabling the model to capture fine-grained structural relations in protected code. Training jointly optimizes standard language modeling with two contrastive objectives: functionality-aware contrastive learning (to encode semantic equivalence among differently protected implementations of the same logic) and protection-aware contrastive learning (to distinguish varying protection strengths). They further define a protection effectiveness optimization task to quantify and rank VM variants in terms of protection robustness. A two-stage pipeline\u2014continual pre-training on the constructed data followed by task-specific fine-tuning\u2014enables the model to generate, compare, and reason about VMP-protected code.", "result": "ShieldedCode significantly improves performance on several tasks involving protected code. For L0 VM code generation, it attains 26.95% Pass@1, outperforming GPT-4o at 22.58%. In binary similarity detection, it boosts Recall@1 by 10% over state-of-the-art methods such as jTrans, demonstrating stronger robustness and discriminative power across different protection levels. The framework shows consistent gains across diverse VMP settings, indicating better understanding and handling of protection-aware representations.", "conclusion": "ShieldedCode establishes a new, protection-aware learning framework for virtual machine protected code that goes beyond traditional rule-based VMP techniques. By combining hierarchical dependency modeling, contrastive objectives for both functionality and protection strength, and a tailored training pipeline, the approach enables LLMs to more effectively generate, analyze, and compare protected binaries. The reported empirical gains suggest that learning-based methods can become a powerful tool for software defense and open a promising research direction in robust, protection-aware code representation and generation."}}
{"id": "2601.20680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20680", "abs": "https://arxiv.org/abs/2601.20680", "authors": ["Ostap Vykhopen", "Viktoria Skorik", "Maxim Tereschenko", "Veronika Solopova"], "title": "Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin", "comment": null, "summary": "Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.", "AI": {"tldr": "The paper evaluates replacing HDBSCAN with online clustering in a social media narrative monitoring pipeline, proposing metrics and methods to maintain narrative quality while improving scalability and real-time adaptability.", "motivation": "Traditional batch clustering (HDBSCAN) in narrative intelligence pipelines cannot scale to continuous, high-volume social media streams and cannot adapt quickly to evolving narratives, leading to memory and compute bottlenecks and delayed or stale insights.", "method": "The authors design a three-stage production-like architecture (data collection, modeling, dashboard generation) and run sliding-window simulations on historical multilingual social media datasets. They systematically compare multiple online clustering algorithms against HDBSCAN using both standard clustering metrics (Silhouette, Davies-Bouldin) and custom narrative-oriented metrics (narrative distinctness, contingency, variance), while also assessing runtime, memory usage, and integration complexity.", "result": "The study characterizes trade-offs among several online clustering methods in terms of preserving narrative cluster quality while significantly reducing computation and memory overhead compared to repeatedly retraining HDBSCAN on each time window; it identifies which approaches are most compatible with existing narrative-report pipelines under streaming constraints.", "conclusion": "Online clustering can effectively replace batch HDBSCAN for narrative monitoring on social media streams when evaluated with both technical and narrative-centric metrics, helping bridge the gap between batch topic modeling frameworks and real-time, streaming social media analysis, with practical benefits for computational social science and crisis/narrative surveillance applications."}}
{"id": "2601.20730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20730", "abs": "https://arxiv.org/abs/2601.20730", "authors": ["Shicheng Fang", "Yuxin Wang", "XiaoRan Liu", "Jiahao Lu", "Chuanyuan Tan", "Xinchi Chen", "Yining Zheng. Xuanjing Huang", "Xipeng Qiu"], "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "comment": "26 pages", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "AI": {"tldr": "The paper introduces AgentLongBench, a benchmark that evaluates large language model agents in dynamic, interactive environments using lateral thinking puzzles, revealing that models struggle with synthesizing dynamic information despite strong static retrieval abilities.", "motivation": "As LLMs are increasingly used as autonomous agents, they must handle long, changing contexts and interact with environments. Existing benchmarks are mostly static and focus on passive retrieval, failing to capture non-linear reasoning, iterative feedback, and real workflow-like conditions. The authors aim to create a benchmark that better reflects these complex interaction patterns to diagnose where current agents fail.", "method": "The authors design AgentLongBench, a benchmark based on simulated environment rollouts grounded in Lateral Thinking Puzzles. The framework constructs multi-step interaction trajectories, with both knowledge-intensive and knowledge-free settings, and evaluates how agents process and update information over long contexts. They test state-of-the-art LLMs and memory systems with context windows from 32K up to 4M tokens, observing behavior in dynamic decision-making and information synthesis tasks rather than only static retrieval.", "result": "Experiments show that current LLM agents, even with advanced memory systems and very long context windows, perform well on static retrieval but degrade sharply when required to integrate and synthesize dynamically evolving information. The benchmark exposes a particular weakness in handling workflows that demand continuous reasoning over large, dense tool outputs or complex interaction histories.", "conclusion": "The key limiting factor for current LLM agents is not just context length or memory fragmentation, but the minimum amount of information (tokens) that must be processed to resolve a query. High information density in massive tool responses is more challenging than dispersed information across many dialogue turns. AgentLongBench thus highlights a fundamental challenge in scaling agentic LLMs from static retrieval to robust, dynamic information synthesis in realistic environments."}}
{"id": "2601.20747", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.20747", "abs": "https://arxiv.org/abs/2601.20747", "authors": ["Elham Aghakhani", "Rezvaneh Rezapour"], "title": "Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts", "comment": null, "summary": "Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.", "AI": {"tldr": "The paper studies how people actually use and evaluate AI/LLMs for emotional support in real-world online settings, using Reddit posts from mental health communities, and links their reactions to theories of technology acceptance and therapeutic alliance.", "motivation": "LLMs are increasingly used for emotional support and quasi-therapeutic conversations outside formal clinical care, but we lack empirical, theory-grounded understanding of how everyday users evaluate these systems, what drives acceptance or rejection, and how people form (or fail to form) therapeutic-like relationships with them. Existing work often focuses on technical performance or controlled experiments rather than naturally occurring user discourse, and key psychological and HCI theories (Technology Acceptance Model, therapeutic alliance) have not been systematically applied at scale in this context.", "method": "The authors collect 5,126 posts from 47 Reddit mental health communities in which users describe experiential or exploratory use of AI systems for emotional support or therapy-like interactions. They design an annotation framework grounded in the Technology Acceptance Model (focusing on adoption-related constructs like perceived usefulness, ease of use, and attitudes) and therapeutic alliance theory (focusing on bond, task, and goal alignment). Using a hybrid pipeline where LLMs assist with large-scale coding and humans refine and validate annotations, they analyze evaluative language, attitudes toward adoption, and indicators of relational alignment or misalignment across the dataset.", "result": "They find that users\u2019 engagement with AI for emotional support depends mainly on perceived outcomes, trust, and response quality, rather than on emotional bonding alone. Positive sentiment toward AI is most strongly tied to alignment on tasks and goals\u2014i.e., when the AI\u2019s responses match users\u2019 problems and therapeutic aims. In contrast, when AI is used more as a companion or for companionship-oriented support, posts more frequently show misaligned therapeutic alliances and highlight risks, including emotional dependence on the AI and worsening or escalation of mental health symptoms.", "conclusion": "Theory-based constructs from technology acceptance and therapeutic alliance can be effectively operationalized with a scalable, hybrid LLM-human discourse analysis of real-world online conversations. The findings suggest that for AI emotional support tools, perceived effectiveness, trustworthiness, and task/goal alignment are more important than simply creating an emotional bond. Companionship-only uses may be particularly risky, with higher likelihood of misalignment and harms like dependence and symptom escalation. The paper underscores the need to study how users themselves interpret and experience language technologies in sensitive mental health contexts when designing and deploying such systems."}}
{"id": "2601.20757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20757", "abs": "https://arxiv.org/abs/2601.20757", "authors": ["Jing Yang", "Moritz Hechtbauer", "Elisabeth Khalilov", "Evelyn Luise Brinkmann", "Vera Schmitt", "Nils Feldhus"], "title": "Persona Prompting as a Lens on LLM Social Reasoning", "comment": "9 Pages, EACL main", "summary": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.", "AI": {"tldr": "The paper studies how persona prompting affects large language models\u2019 explanations (rationales) in hate speech detection and finds that, although it can improve classification, it degrades rationale quality and does not remove demographic biases.", "motivation": "Large language models are increasingly used in socially sensitive applications like hate speech detection, where not only accuracy but also the quality and faithfulness of explanations are crucial for trust, transparency, and alignment. Persona prompting is becoming popular for steering models to act like specific users or demographic groups, but its influence on the rationales the models generate\u2014especially across different demographics\u2014has not been systematically investigated. The authors aim to understand whether persona prompting improves alignment with human reasoning, mitigates bias, or introduces new issues.", "method": "The authors condition several LLMs on different simulated demographic personas using persona prompting. They use datasets that include word-level human rationale annotations labeled by annotators from different demographic groups. They then: (1) compare LLM-generated rationales with these human word-level rationales to measure agreement; (2) examine how agreement varies across personas and demographics; (3) evaluate classification performance on hate speech detection; and (4) analyze model bias, such as over-flagging harmful content, and whether persona prompting meaningfully steers the models or aligns them with corresponding real-world demographic groups.", "result": "Across three LLMs, they find: (1) Persona prompting improves classification accuracy on the most subjective task (hate speech detection) but simultaneously degrades the quality of the rationales, i.e., agreement with human word-level annotations declines. (2) Simulated demographic personas do not align well with real annotators from the same demographic groups, and high inter-persona agreement indicates that the models are relatively insensitive to persona steering in terms of their underlying reasoning. (3) The models show stable demographic biases and a systematic tendency to over-flag content as harmful, and these issues persist regardless of the personas used in prompting.", "conclusion": "Persona prompting introduces a trade-off: it can boost classification performance on subjective, socially sensitive tasks like hate speech detection, but it often harms rationale quality and fails to correct or meaningfully reduce demographic biases. Simulated personas do not faithfully capture real-world demographic perspectives, and LLMs remain resistant to substantial steering in their rationales. The authors argue that persona prompting in sensitive safety tasks should be applied with caution, and that better methods are needed for achieving both accurate predictions and high-quality, bias-aware explanations."}}
