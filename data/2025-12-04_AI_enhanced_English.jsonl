{"id": "2512.03047", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03047", "abs": "https://arxiv.org/abs/2512.03047", "authors": ["Samih Fadli"], "title": "Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models", "comment": "6 pages. Companion paper to \"The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems\". Code and tools: https://github.com/AerisSpace/EthicalEntropyKit", "summary": "Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.", "AI": {"tldr": "The paper introduces an operational framework to measure and monitor \"ethical entropy\"\u2014a notion of alignment degradation\u2014in large language models over time and under stress, enabling run-time detection of value drift.", "motivation": "Static safety benchmarks fail to capture dynamic alignment failures in large language models, such as value drift under distribution shift, exploitation via jailbreaks, and gradual degradation of behavior during deployment. The authors want a principled, quantitative way to track and manage these time-dependent safety issues, inspired by a theoretical \"Second Law of Intelligence\" stating that ethical entropy naturally increases unless counteracted by alignment work.", "method": "They formalize ethical entropy S(t) as a behavioral state variable over model outputs using a five-category taxonomy of behaviors. They then train a classifier on model transcripts to estimate S(t) over time, and run stress tests on base and instruction-tuned versions of four frontier LLMs. Using the observed S(t) trajectories, they infer an effective alignment work rate parameter gamma_eff and integrate these quantities into a monitoring pipeline that issues alerts when S(t) drift passes a pre-defined stability threshold.", "result": "Experiments show that base LLMs exhibit sustained growth in ethical entropy under stress, indicating increasing misalignment or unsafe behavior. In contrast, instruction-tuned variants largely suppress this drift and reduce overall ethical entropy by about 80%. The fitted gamma_eff values quantify how much \"alignment work\" each model effectively performs to resist entropy increase, and these metrics can be used for automated run-time monitoring.", "conclusion": "The paper demonstrates that ethical entropy can be made into a measurable, operational construct for LLMs, revealing clear differences between base and aligned models in their dynamic safety properties. By estimating entropy S(t) and effective alignment work gamma_eff and embedding them in a real-time monitoring pipeline, practitioners can detect and respond to value drift and emerging misalignment during deployment, moving beyond static benchmarks toward dynamic oversight of model safety."}}
{"id": "2512.03079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03079", "abs": "https://arxiv.org/abs/2512.03079", "authors": ["Anudeex Shetty"], "title": "Watermarks for Embeddings-as-a-Service Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.\n  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.\n  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.", "AI": {"tldr": "The paper analyzes vulnerabilities of Embeddings-as-a-Service (EaaS) watermarking against imitation attacks and proposes a more robust watermarking method based on linear transformations of embeddings.", "motivation": "Embeddings-as-a-Service providers rely on proprietary LLM-based embedding models that can be cloned via black-box imitation attacks, threatening their intellectual property. Existing embedding watermarking techniques aim to establish model ownership but may be vulnerable to realistic attack strategies like paraphrasing. There is a need to understand these vulnerabilities and design more robust watermarking schemes that can withstand such attacks while maintaining usability for downstream NLP tasks.", "method": "The authors first empirically evaluate existing EaaS watermarking schemes under imitation attacks where attackers paraphrase input text before querying the service, using multiple paraphrasing approaches, models, and datasets. They measure watermark detectability to assess robustness. After identifying weaknesses, they introduce WET (Watermarking EaaS with Linear Transformation), which embeds a watermark via a linear transformation applied to the output embeddings. Ownership verification is done by applying the inverse transformation to suspected embeddings and checking their similarity to original embeddings. The method is extensively studied via ablations on its components and hyperparameters to understand what drives robustness.", "result": "Experiments show that paraphrasing-based imitation attacks can reliably remove or significantly weaken current state-of-the-art EaaS watermarks across different attack settings and datasets, revealing a substantial vulnerability. In contrast, the proposed WET technique maintains high watermark verifiability even under paraphrasing attacks, demonstrating strong robustness. Ablation results highlight which design choices and parameter settings in WET are crucial for its performance.", "conclusion": "Existing watermarking mechanisms for EaaS are not sufficiently robust, as paraphrasing of query texts allows attackers to bypass watermark detection during model imitation. The proposed WET method, based on linear transformations of embeddings and inverse-transformation-based verification, offers a more resilient alternative with near-perfect robustness to paraphrasing attacks. This work both exposes key weaknesses in current defenses and provides a practical, more secure watermarking strategy for protecting EaaS model ownership."}}
{"id": "2512.03082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03082", "abs": "https://arxiv.org/abs/2512.03082", "authors": ["Nan Zhuang", "Wenshuo Wang", "Lekai Qian", "Yuxiao Wang", "Boyu Cao", "Qi Liu"], "title": "Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation", "comment": null, "summary": "Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.", "AI": {"tldr": "The paper proposes a new framework, Reasoning Dependency Generation (RDG), to reduce choice-supportive bias in large language models by generating unbiased reasoning data for fine-tuning, significantly improving bias-related performance without harming standard benchmark accuracy.", "motivation": "Large Language Models can exhibit choice-supportive bias (CSB), where they favor options they have already chosen, which undermines objectivity in AI-assisted decision making. Existing debiasing methods mainly focus on demographic and social biases and do not address cognitive biases like CSB. There is a need for systematic methods to detect and mitigate these cognitive biases in LLMs to make decision support more reliable.", "method": "The authors introduce Reasoning Dependency Generation (RDG), a framework that automatically constructs balanced reasoning question\u2013answer pairs. RDG explicitly models and unmodels dependencies between choices, evidences, and justifications to reduce the model\u2019s tendency to support its prior choice. It generates large-scale QA datasets across domains, containing both Contextual Dependency Data (where evidence and choices are linked) and Dependency Decouple Data (where such links are removed or altered), and uses these data to fine-tune LLMs against CSB.", "result": "LLMs fine-tuned on RDG-generated data show substantial reductions in choice-supportive bias: an 81.5% improvement in memory-based experiments and a 94.3% improvement in evaluation-based experiments. At the same time, their performance on standard bias benchmark tests such as BBQ remains similar to that of the original models, indicating that RDG reduces CSB without degrading other capabilities.", "conclusion": "Reasoning Dependency Generation provides an effective way to mitigate choice-supportive cognitive bias in LLMs by generating structured, dependency-aware reasoning data for fine-tuning. The approach substantially reduces CSB while preserving general benchmark performance, demonstrating a promising path toward more objective and trustworthy AI-assisted decision-making systems and opening a new direction for addressing cognitive biases in LLMs beyond traditional demographic or social bias mitigation."}}
{"id": "2512.03195", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03195", "abs": "https://arxiv.org/abs/2512.03195", "authors": ["Stylianos Saroglou", "Konstantinos Diamantaras", "Francesco Preta", "Marina Delianidi", "Apostolos Benisis", "Christian Johannes Meyer"], "title": "Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies", "comment": "14 pages, 1 figure, Preprint", "summary": "This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier", "AI": {"tldr": "The paper explores how language models can better classify job vacancy texts by linking them to European labor taxonomies (ESCO and EQF), compares two main linking methods, provides tools and datasets, and tests generative LLMs for job entity extraction.", "motivation": "Labor market information, especially job vacancy texts, contains rich but unstructured data on occupations, skills, and qualifications. Existing approaches often focus on shallow skill extraction and lack robust links to standardized frameworks like ESCO and EQF, which are essential for policy, analytics, and labor market matching across countries. There is also a need for shared tools and datasets to systematically evaluate how modern language models can improve job classification and analysis of employment narratives.", "method": "The authors study two established methodologies for linking job texts to structured frameworks: Sentence Linking (mapping whole sentences or segments to ESCO/EQF concepts) and Entity Linking (identifying and mapping specific entities such as occupations or qualifications to taxonomy entries). They build and release an open-source tool implementing both approaches, create two annotated datasets that label how occupations and qualifications appear in job vacancy texts, and then experiment with different ways of applying generative large language models to perform these linking and extraction tasks.", "result": "The study shows that language-model-based approaches can advance the state of the art in job entity extraction when aligning vacancy texts with ESCO and EQF. The comparative evaluation of Sentence Linking vs. Entity Linking clarifies their strengths and weaknesses. The released tool and datasets function effectively as a benchmark and development environment. Overall, the experiments demonstrate that generative LLMs can be productively integrated into workflows for labor market text classification and entity linking.", "conclusion": "Language models, including generative LLMs, can substantially improve how job vacancy texts are linked to European labor market frameworks such as ESCO and EQF. By providing an open-source tool and two specialized annotated datasets, the paper offers both methodological insights and practical infrastructure for researchers and practitioners working on job classification, labor market analytics, and the study of skills and work narratives in digital economies."}}
{"id": "2512.03048", "categories": ["cs.AI", "cs.CY", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03048", "abs": "https://arxiv.org/abs/2512.03048", "authors": ["Austin Spizzirri"], "title": "Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation", "comment": "Approx. 3,000 words, 10 pages. Philosophical analysis of AI alignment (process-based / syntropy framework)", "summary": "I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.", "AI": {"tldr": "The paper argues for rethinking AI alignment as designing agents that develop moral and value capacities through processes and interactions, rather than trying to hard-code human values directly.", "motivation": "To address fundamental philosophical and technical problems in current AI alignment approaches that attempt to specify and encode human values explicitly, which may be unstable or incoherent given the is-ought gap, value pluralism, and the extended frame problem.", "method": "Philosophical analysis and conceptual engineering: (1) develop the 'specification trap' argument against content-based value specification; (2) introduce the concept of syntropy as an information-theoretic account of alignment as mutual uncertainty reduction between agents; (3) distinguish genuine vs. simulated moral capacity using compatibilist guidance control theories, and propose an embodied experimental paradigm and verification regime as operational tests.", "result": "A reconceptualization of AI alignment as building syntropic, reasons-responsive agents via process-based, multi-agent, developmental mechanisms, along with an information-theoretic framework (syntropy) and a philosophical account plus proposed experimental criteria for distinguishing real moral agency from mere simulation in AI systems.", "conclusion": "AI alignment should focus on architecting developmental, multi-agent systems that become morally and value competent through syntropic processes and reasons-responsiveness, rather than trying to encode fixed human value content; this framework yields testable predictions about value emergence and moral agency, but empirical validation is still forthcoming in a separate empirical project."}}
{"id": "2512.03197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03197", "abs": "https://arxiv.org/abs/2512.03197", "authors": ["Faezeh Faez", "Marzieh S. Tahaei", "Yaochen Hu", "Ali Pourranjbar", "Mahdi Biparva", "Mark Coates", "Yingxue Zhang"], "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.", "AI": {"tldr": "InvertiTune is a framework that inverts the Text2KG task: instead of directly extracting knowledge graphs from text with large models, it generates realistic text from subgraphs and uses this synthetic data to fine-tune smaller models for efficient, single-shot knowledge graph construction.", "motivation": "Existing Text2KG methods typically rely on iterative prompting of large language models, which is computationally costly and often fails to capture complex, dispersed relations in longer texts. Moreover, current benchmarks use short text and small graphs, not matching real-world scenarios. There is a need for a method that produces efficient, accurate KG construction from realistic longer texts without heavy iterative LLM usage.", "method": "InvertiTune introduces a controlled data generation pipeline coupled with supervised fine-tuning. First, it systematically extracts subgraphs from large existing knowledge bases. Then it applies noise filtering to clean these subgraphs. Next, large language models are used in the easier direction\u2014generating natural language descriptions for the subgraphs rather than extracting graphs from text. This produces paired (text, KG) data with longer documents and larger graphs. Lightweight models are then supervisedly fine-tuned on this dataset to perform single-shot KG construction from text.", "result": "Using the pipeline, the authors build CE12k, a dataset of long texts paired with substantial KGs. Models fine-tuned with InvertiTune on CE12k outperform larger, non-fine-tuned LLMs and prior state-of-the-art Text2KG systems. They also generalize better across datasets, as shown on CrossEval-1200, a test set assembled from three established benchmarks plus CE12k-derived data.", "conclusion": "Generating realistic, high-quality training data by inverting the Text2KG task\u2014text generation from KG subgraphs\u2014and then fine-tuning smaller models enables more efficient and accurate KG construction than relying on large, iteratively prompted LLMs. The work demonstrates that the quality and realism of training data are key drivers of performance and generalization in Text2KG systems."}}
{"id": "2512.03072", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.03072", "abs": "https://arxiv.org/abs/2512.03072", "authors": ["Hu Keyi"], "title": "Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI", "comment": null, "summary": "Current AI paradigms, as \"architects of experience,\" face fundamental challenges in explainability and value alignment. This paper introduces \"Weight-Calculatism,\" a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.", "AI": {"tldr": "Introduces Weight-Calculatism, a first-principles cognitive architecture that decomposes cognition into logical atoms and interpretable weight calculations to achieve explainable, value-aligned AGI.", "motivation": "Existing AI systems lack deep explainability and robust value alignment; current paradigms struggle with transparent, human-auditable reasoning and generality in novel scenarios. The paper aims to provide a principled architecture that makes every cognitive step and value traceable and interpretable while moving toward AGI.", "method": "Proposes a cognitive architecture that represents cognition as Logical Atoms operated on by two primitive operations, Pointing and Comparison. Decision-making is expressed via a transparent weight-calculation rule (Weight = Benefit * Probability) with all values ultimately grounded in a finite set of Initial Weights. The architecture is realized using a graph-algorithmic computational engine and a global workspace-like workflow, backed by preliminary code and scenario-based validation.", "result": "Preliminary implementation and scenario tests suggest the system can perform transparent, human-like reasoning, learn robustly in previously unseen situations, and maintain traceable value alignment via its Initial Weights and atomic operations.", "conclusion": "Weight-Calculatism offers both a theoretical and practical framework for highly explainable, value-aligned AGI. By reducing cognition to logical atoms and interpretable weight calculations, it addresses key shortcomings of current AI paradigms and provides a promising path toward trustworthy general intelligence."}}
{"id": "2512.03214", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03214", "abs": "https://arxiv.org/abs/2512.03214", "authors": ["Paulina Garcia-Corral"], "title": "Identifying attributions of causality in political text", "comment": null, "summary": "Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.", "AI": {"tldr": "The paper presents a computational framework to automatically detect and structure causal explanations in political text as cause\u2013effect pairs, enabling large-scale analysis of political explanations.", "motivation": "Explanations\u2014answers to why and responsibility questions\u2014are central to how citizens understand politics, attribute blame, and think about remedies. Despite this, political science lacks a unified, systematic way to analyze explanations in text, with current work being fragmented, case-specific, and often manually coded. The author aims to fill this gap by creating a scalable, generalizable method to identify and structure causal explanations in political discourse.", "method": "The author develops a framework that uses a lightweight causal language model to automatically identify and parse explanations in political texts. The system detects causal claims and outputs them as structured cause\u2013effect pairs, which can then be used in downstream quantitative or qualitative analyses. The approach emphasizes modest annotation requirements to train the model and tests its performance on multiple types of political text to assess generalizability and accuracy relative to human coders.", "result": "The framework successfully extracts causal explanations at scale from political texts, producing structured datasets of cause\u2013effect pairs. The model demonstrates reasonable accuracy compared with human coding, while requiring relatively limited annotated data. It also generalizes across different corpora and issue domains, indicating that the method is not narrowly tailored to a single topic or text type.", "conclusion": "The study shows that explanations in political text can be systematically detected, structured, and analyzed using computational methods. By offering a generalizable, relatively low-cost tool for extracting causal claims, the paper provides political scientists with new opportunities to study how explanations are used, who or what is blamed, and what remedies are proposed across large bodies of political discourse."}}
{"id": "2512.03272", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03272", "abs": "https://arxiv.org/abs/2512.03272", "authors": ["Zhiyuan He", "Dingmin Wang"], "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?", "comment": null, "summary": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.", "AI": {"tldr": "The paper compares traditional long Chain-of-Thought (CoT) reasoning in large reasoning models with an approach that integrates symbolic solvers, aiming to reduce token overhead and overthinking while improving accuracy on certain problem types.", "motivation": "Long Chains of Thought used by Large Reasoning Models can be inefficient and even harmful due to token overhead and overthinking, and it's unclear when integrating symbolic solvers via code generation can provide benefits.", "method": "The authors integrate symbolic solvers with LLMs by converting reasoning tasks into executable code using the models' code generation abilities, then systematically evaluate performance across different problem types varying in implicit reasoning depth and search space complexity.", "result": "They find that symbolic-solver integration is beneficial mainly for problems that have limited implicit reasoning but a large search space, such as constraint satisfaction problems with many backtracks, while modern LLMs like GPT-4o excel more on deductive problems with shallow reasoning depth using conventional CoT.", "conclusion": "Symbolic solvers complement LLM-based reasoning: for shallow but combinatorially complex tasks, solver integration can substantially improve performance (even enabling smaller models like CodeLlama-13B to surpass GPT-4o on hard Zebra puzzles), whereas for deeper deductive reasoning tasks, long-CoT alone from advanced LLMs often suffices or performs better."}}
{"id": "2512.03310", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03310", "abs": "https://arxiv.org/abs/2512.03310", "authors": ["Kunj Joshi", "David A. Smith"], "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs", "comment": "To be submitted for ICML 2026", "summary": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.", "AI": {"tldr": "The paper proposes Randomized Masked Fine-Tuning (RMFT), a privacy-preserving fine-tuning method that significantly reduces personally identifiable information (PII) memorization in LLMs while maintaining model utility, and introduces MaxTER, a Pareto-based framework to evaluate privacy\u2013utility tradeoffs.", "motivation": "Existing large language models tend to memorize personally identifying information from their training data, creating serious security and privacy concerns when such information can be extracted through prompting or attacks. Current mitigation strategies like deduplication are insufficient or degrade model performance noticeably. There is a need for a fine-tuning strategy that specifically targets PII memorization reduction without substantially harming perplexity or downstream performance, as well as a principled way to quantify the privacy\u2013utility tradeoff.", "method": "The authors propose Randomized Masked Fine-Tuning (RMFT), where during fine-tuning, parts of the text\u2014especially PII tokens\u2014are randomly masked so the model learns to rely less on exact surface forms and more on context, thereby discouraging direct memorization of sensitive strings. They apply RMFT to a language model trained on the Enron Email Dataset and compare it against standard fine-tuning and deduplication-based baselines. To evaluate, they measure Total Extraction Rate and Seen Extraction Rate, which quantify how often PIIs can be extracted from the model, and they also track perplexity to capture utility loss. Additionally, they introduce MaxTER, a Pareto-optimal evaluation framework, and use Area Under the Response Curve (AURC) to compare the privacy\u2013utility tradeoffs achieved by RMFT vs. deduplication.", "result": "On the Enron Email Dataset, RMFT leads to an 80.81% reduction in Total Extraction Rate and an 80.17% reduction in Seen Extraction Rate compared with baseline fine-tuning, indicating a substantial decrease in PII memorization and extractability. RMFT outperforms deduplication-based approaches in terms of privacy protection, while incurring only a modest 5.73% increase in perplexity, meaning that the text generation quality / utility is largely preserved. Under the MaxTER framework and the AURC metric, RMFT shows a better privacy\u2013utility Pareto profile than deduplication, suggesting it provides more favorable tradeoffs across different operating points.", "conclusion": "Randomized Masked Fine-Tuning is an effective privacy-preserving fine-tuning method that significantly reduces the memorization and extractability of personally identifying information in LLMs with minimal impact on model utility. By combining RMFT with the MaxTER evaluation framework and the AURC metric, the paper offers both a practical mitigation technique and a principled way to assess privacy\u2013utility tradeoffs. The findings suggest that targeted fine-tuning strategies like RMFT can be more effective than data-centric methods such as deduplication for mitigating PII leakage in language models."}}
{"id": "2512.03293", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.03293", "abs": "https://arxiv.org/abs/2512.03293", "authors": ["Filippo Torresan", "Ryota Kanai", "Manuel Baltieri"], "title": "Prior preferences in active inference agents: soft, hard, and goal shaping", "comment": "41 pages, 23 figures", "summary": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).", "AI": {"tldr": "The paper studies how different ways of defining preference (goal) distributions in active inference agents affect their balance between exploration and exploitation in a grid world task.", "motivation": "In active inference, the preference distribution encodes what states an agent aims for, but the literature has largely overlooked how to specify this distribution and how different specifications influence behavior and learning. The authors aim to fill this gap.", "method": "They formalize four types of preference distributions that vary along two dimensions: hard vs. soft goals and presence vs. absence of goal shaping (intermediate goals). They implement four corresponding active inference agents and compare their performance in a grid world navigation task, analyzing both task performance and learning of transition dynamics.", "result": "Agents with goal shaping show the best task performance (strong exploitation) but learn less about the environment's transition dynamics (weaker exploration). Agents without goal shaping explore more and learn the dynamics better but perform worse on the navigation task.", "conclusion": "How preferences are specified critically shapes the exploration\u2013exploitation trade-off in active inference agents. Introducing intermediate goals (goal shaping) boosts performance on goal-directed tasks but comes at the cost of reduced environmental learning, highlighting a key design choice when building active inference agents."}}
{"id": "2512.03334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03334", "abs": "https://arxiv.org/abs/2512.03334", "authors": ["Nemika Tyagi", "Nelvin Licona Guevara", "Olga Kellert"], "title": "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaran\u00ed", "comment": "10 pages, 4 figures", "summary": "This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaran\u00ed. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaran\u00ed dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaran\u00ed and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.", "AI": {"tldr": "The paper introduces an LLM-based pipeline to automatically annotate sociolinguistic and topical features in bilingual code-switched corpora (Spanish-English, Spanish-Guaran\u00ed) and shows it can reproduce known patterns and uncover new ones at corpus scale.", "motivation": "Manual sociolinguistic annotation of bilingual and code-switched discourse is labor-intensive, slow, and hard to scale, especially for cross-linguistic comparison and low-resource language pairs like Spanish-Guaran\u00ed. Existing work has identified important relationships between demographic factors, discourse-pragmatic functions, and language choice, but largely through small, manually analyzed datasets. The authors want a scalable, computational method that preserves the interpretability and nuance of traditional sociolinguistic analysis while enabling corpus-scale quantitative study of bilingual discourse.", "method": "They design an annotation pipeline that uses large language models to automatically label each code-switched sentence with topic, genre, and discourse-pragmatic function. The pipeline is applied to 3,691 sentences from two corpora: the Miami Bilingual Corpus (Spanish-English) and a Spanish-Guaran\u00ed dataset. For the Miami data, they integrate existing demographic metadata (e.g., gender, language dominance) with the LLM-generated discourse labels. For the Spanish-Guaran\u00ed texts, they add new topic annotations with the LLM. They then analyze the distributions of topics, genres, and functions across social and linguistic variables to test for systematic sociolinguistic patterns and to compare with earlier qualitative findings.", "result": "In the Miami corpus, the LLM-based annotations show systematic links between speaker gender, language dominance, and discourse-pragmatic functions in code-switched sentences, mirroring and extending prior interactional and sociolinguistic research. In the Paraguayan Spanish-Guaran\u00ed data, the results reveal a clear diglossic pattern: Guaran\u00ed is used primarily in more formal contexts, while Spanish is associated with informal discourse. The LLM labels are consistent enough to produce interpretable, corpus-level quantitative patterns that align with established sociolinguistic insights.", "conclusion": "The study concludes that large language models can be used as reliable annotation tools for sociolinguistic and topical analysis of bilingual code-switching, recovering patterns that were previously accessible mainly via painstaking manual coding. This approach both scales up traditional methods and extends them to cross-linguistic and low-resource bilingual settings, thereby advancing computational sociolinguistics and providing a practical pathway to large, richly annotated bilingual corpora."}}
{"id": "2512.03318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03318", "abs": "https://arxiv.org/abs/2512.03318", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema Parre\u00f1o Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili Garc\u00eda Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo Pe\u00f1a", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "comment": "Published at NeurIPS Datasets and Benchmarks 2025, 10 pages", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "AI": {"tldr": "The paper proposes a new evaluation method and benchmark for testing how well LLM agents can cooperate in novel, mixed-motive social environments, showing current systems still struggle to reliably generalize cooperation.", "motivation": "Current LLM agents are increasingly used in social and multi-agent settings, interacting with both humans and other AIs. However, existing evaluations do not adequately measure whether these agents can generalize cooperative behavior to new, complex social situations, especially where incentives are partly aligned and partly in conflict (mixed-motive). There is a need for a principled way to assess general cooperative intelligence in such zero-shot settings.", "method": "The authors introduce an evaluation framework built on Concordia, a natural-language multi-agent simulation environment. They design a suite of diverse, mixed-motive scenarios (e.g., negotiation tasks, collective action problems) and measure agents\u2019 ability to identify and realize opportunities for mutual gain without task-specific fine-tuning (zero-shot). Agents\u2019 performance is assessed across varying partners and contexts, focusing on cooperation outcomes and robustness of behavior across scenarios.", "result": "Using this framework, they evaluate agents submitted to the NeurIPS 2024 Concordia Contest. The empirical results show that while agents can sometimes achieve mutual benefits, there are large and systematic failures to generalize cooperative behavior, especially in tasks that require sophisticated skills like persuasion, negotiation, and enforcement of social norms. Overall, current LLM agents fall short of robust, general cooperative intelligence.", "conclusion": "The study concludes that present-day LLM-based agents are not yet reliably cooperative in zero-shot, mixed-motive environments. The introduced Concordia-based benchmark exposes important gaps in their ability to generalize cooperation, particularly in persuasion and norm-enforcement scenarios. The authors argue that future work should focus on improving and systematically evaluating these aspects to build trustworthy, socially capable AI agents."}}
{"id": "2512.03340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03340", "abs": "https://arxiv.org/abs/2512.03340", "authors": ["Rohan Charudatt Salvi", "Chirag Chawla", "Dhruv Jain", "Swapnil Panigrahi", "Md Shad Akhtar", "Shweta Yadav"], "title": "PERCS: Persona-Guided Controllable Biomedical Summarization Dataset", "comment": "9 pages, 4 figures, 6 tables", "summary": "Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.", "AI": {"tldr": "The paper introduces PERCS, a persona-guided biomedical summarization dataset with four audience types and benchmarks LLMs on it.", "motivation": "Existing medical text simplification resources assume a single generic audience and fail to address the diverse medical literacy levels and information needs of different user groups. This gap hinders effective communication of biomedical research to laypersons, students, researchers from other fields, and clinicians.", "method": "The authors construct PERCS, a dataset of biomedical abstracts with four aligned summaries per abstract, each tailored to a specific persona: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. Physician reviewers check each summary for factual accuracy and persona alignment using a detailed error taxonomy. They then perform technical validation analyzing readability, vocabulary, and content depth, and benchmark four large language models on the dataset using automatic metrics of comprehensiveness, readability, and faithfulness.", "result": "The PERCS dataset exhibits clear and systematic differences between personas in readability, terminology, and depth of content, confirming that the summaries are persona-differentiated. Benchmarking results for four LLMs provide baseline performance on comprehensiveness, readability, and faithfulness, demonstrating room for improvement in persona-specific controllable summarization.", "conclusion": "PERCS offers a high-quality, physician-validated resource for research on persona-specific biomedical summarization and controllable medical communication. By making the dataset, annotation guidelines, and evaluation materials publicly available, the authors enable future work on adapting medical content to diverse audiences and improving health literacy."}}
{"id": "2512.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03438", "abs": "https://arxiv.org/abs/2512.03438", "authors": ["Reuben Tan", "Baolin Peng", "Zhengyuan Yang", "Hao Cheng", "Oier Mees", "Theodore Zhao", "Andrea Tupini", "Isar Meijier", "Qianhui Wu", "Yuncong Yang", "Lars Liden", "Yu Gu", "Sheng Zhang", "Xiaodong Liu", "Lijuan Wang", "Marc Pollefeys", "Yong Jae Lee", "Jianfeng Gao"], "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents", "comment": null, "summary": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.", "AI": {"tldr": "Argos is a reward agent for multimodal RL that scores not only final answers but also grounding and reasoning, using a pool of teacher and rule-based rewards, leading to better, less-hacky agentic reasoning models.", "motivation": "Existing multimodal agentic reasoning models in RL are typically trained with sparse, outcome-only rewards based on the final answer. This limits learning efficiency and robustness, and leads to issues such as ungrounded reasoning, hallucinations, and reward hacking. Richer token-level or process-level rewards could help, but designing them is hard: different tasks need different scoring functions, teacher models are noisy, and multimodal aspects like spatial/temporal grounding are rarely evaluated. The authors aim to create a principled, flexible reward mechanism that can robustly evaluate both the result and the reasoning process, especially for spatial and embodied tasks.", "method": "They propose Argos, a reward agent that, for each training sample, selects from a pool of teacher-model\u2013derived and rule-based scoring functions. Argos jointly evaluates (1) final response accuracy, (2) spatiotemporal localization of mentioned entities/actions in the multimodal input, and (3) the quality and faithfulness of the step-by-step reasoning trace. Argos is used both to curate supervised fine-tuning (SFT) data and to provide online rewards during RL training. The framework is analyzed theoretically using Pareto-optimality to justify combining multiple reward criteria.", "result": "Using Argos within SFT curation and RL training yields state-of-the-art performance on several multimodal agentic tasks, including spatial reasoning, visual hallucination detection/avoidance, and robotics/embodied AI benchmarks. Models trained without Argos\u2014even with strong curated SFT data\u2014tend to collapse to ungrounded, brittle solutions during RL. Argos-based training also reduces reward hacking behaviors in multimodal RL settings.", "conclusion": "A dedicated agentic verifier, Argos, that adaptively combines multiple teacher and rule-based scoring functions can provide rich, grounded, and process-aware rewards for multimodal RL. This leads to more robust, better-grounded agentic reasoning models, mitigates reward hacking, and cannot be replaced by SFT-only post-training on curated reasoning traces. The framework has both empirical benefits and theoretical support via a Pareto-optimality perspective."}}
{"id": "2512.03343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03343", "abs": "https://arxiv.org/abs/2512.03343", "authors": ["Darshan Fofadiya"], "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning", "comment": "Code available at https://github.com/DarshanFofadiya/idea-gated-transformers/tree/main", "summary": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.", "AI": {"tldr": "They propose an Idea-Gated Transformer that adds a semantic planning head to standard LMs, using predicted future bag-of-words to gate token generation and reduce topic drift while keeping perplexity similar to GPT-2.", "motivation": "Autoregressive LLMs trained with next-token prediction tend to suffer from topic drift, straying from the initial prompt because they rely on short-range associations instead of a global semantic plan. Scaling model size helps but does not fix the inherent local myopia of the objective. The authors want a more controllable LM that retains domain/topic without sacrificing modeling quality or requiring huge parameter growth.", "method": "They modify the Transformer architecture by adding an auxiliary Idea Head that predicts a bag-of-words distribution over a future context window. This prediction is encoded into a latent Concept Vector, which is then used to gate the main vocabulary logits during generation via a differentiable gating mechanism. The gate downweights or suppresses tokens deemed semantically irrelevant to the anticipated future content, effectively pruning the search space. Training jointly optimizes standard NTP along with the auxiliary BoW prediction, and they evaluate on WikiText-103 against a GPT-2 baseline.", "result": "On WikiText-103, the Idea-Gated Transformer reaches similar validation perplexity to a standard GPT-2 model of comparable size, indicating it does not hurt core language modeling performance. However, it substantially improves Domain Retention metrics, with generated continuations staying in the same topic/domain cluster as the prompt more often and for longer spans. Both quantitative metrics and qualitative samples show that generations remain semantically consistent (e.g., sticking to Finance or Science topics) and exhibit reduced associative drift.", "conclusion": "Separating high-level semantic planning from low-level token generation via an auxiliary idea head and gating mechanism reduces topic drift in autoregressive LMs without sacrificing perplexity. The Idea-Gated Transformer keeps generations locked into semantically coherent domains and offers a parameter-efficient, differentiable way to exert semantic control over language models. This suggests that augmenting next-token prediction with explicit future semantic prediction and vocab gating is a promising direction for more controllable and reliable text generation."}}
{"id": "2512.03528", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03528", "abs": "https://arxiv.org/abs/2512.03528", "authors": ["Guang Yang", "Tianpei Yang", "Jingwen Qiao", "Yanqing Wu", "Jing Huo", "Xingguo Chen", "Yang Gao"], "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors", "comment": null, "summary": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.", "AI": {"tldr": "They propose a new MARL framework that handles lossy communication by modeling communication constraints and explicitly separating the roles of lossy and lossless messages in policy learning, improving performance on benchmarks.", "motivation": "In real-world multi-agent systems, communication links are often unreliable (lossy), but existing communication-based MARL methods assume relatively clean and reliable communication, which harms scalability and robustness in complex, dynamic environments. There is a need for a unified way to model communication constraints and to learn effective policies that remain robust despite message loss.", "method": "1) Propose a generalized communication-constrained model that can describe various real-world communication conditions in a unified manner. 2) Use this model as a learning prior to enable agents to distinguish between lossy and lossless messages in specific tasks. 3) Introduce a dual mutual information estimator to decouple and analyze the impact of lossy vs. lossless messages on agents' distributed decision-making. 4) Build a communication-constrained multi-agent reinforcement learning framework that integrates these ideas and incorporates the quantified impact of messages directly into the global reward signal.", "result": "Their framework can distinguish and separately handle lossy and lossless messages, quantify their contribution to team performance via a dual MI estimator, and learns more robust cooperative policies. Experiments on multiple communication-constrained benchmarks show better effectiveness (likely higher rewards, more stable learning, better robustness) than baselines.", "conclusion": "Explicitly modeling communication constraints and decomposing the effects of lossy vs. lossless messages, combined with information-theoretic quantification in the reward, leads to more scalable and robust MARL with communication under realistic, lossy conditions, as validated on standard benchmarks."}}
{"id": "2512.03360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03360", "abs": "https://arxiv.org/abs/2512.03360", "authors": ["Qingchuan Li", "Mingyue Cheng", "Zirui Liu", "Daoyu Wang", "Yuting Zeng", "Tongxuan Liu"], "title": "From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation", "comment": "Accepted by AAAI2026", "summary": "Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.", "AI": {"tldr": "The paper introduces HBLR, a hypothesis-driven backward logical reasoning framework that combines partial symbolic translation with reflective verification to improve the accuracy and efficiency of LLM-based reasoning.", "motivation": "Existing LLM reasoning mainly uses forward, chain-of-thought style inference from premises to conclusion, which often leads to redundant and unreliable reasoning due to hallucinated steps and semantic drift. There is a need for a more disciplined reasoning paradigm that is both accurate and efficient, better aligned with how humans reason deductively and able to leverage symbolic structure without over-committing to brittle formalization.", "method": "The authors propose Hypothesis-driven Backward Logical Reasoning (HBLR). First, in a translation phase, the model converts only high-confidence text spans into symbolic logic (e.g., FOL) while leaving uncertain parts in natural language. A translation reflection module checks these symbolic translations for semantic fidelity and reverts lossy or incorrect translations back to text. Then, in the reasoning phase, HBLR performs backward reasoning: it assumes the target conclusion is true and recursively verifies the premises required to support it. A reasoning reflection module inspects intermediate inference steps, detects flawed reasoning, and corrects them to maintain logical coherence.", "result": "On five logical reasoning benchmarks, HBLR achieves consistently better performance than strong LLM-based baselines, improving both reasoning accuracy and computational efficiency. This empirically validates that hypothesis-driven backward reasoning with confidence-aware symbolic translation and reflection can enhance LLM reasoning quality.", "conclusion": "By integrating partial, confidence-aware symbolic translation with hypothesis-driven backward reasoning and reflective verification, HBLR offers a more reliable and efficient reasoning paradigm for LLMs than conventional forward chain-of-thought approaches. The framework more closely emulates human deductive reasoning and demonstrates superior performance on multiple benchmarks, indicating its promise as a foundation for advanced logical reasoning in natural language understanding and AI systems."}}
{"id": "2512.03549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03549", "abs": "https://arxiv.org/abs/2512.03549", "authors": ["Yuki Orimo", "Iori Kurata", "Hodaka Mori", "Ryuhei Okuno", "Ryohto Sawada", "Daisuke Okanohara"], "title": "PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks", "comment": null, "summary": "We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.", "AI": {"tldr": "PARC is a hierarchical, self-evaluating coding agent that can autonomously execute long, complex scientific and data-science workflows, coordinating many long-running jobs while detecting and correcting its own strategic mistakes.", "motivation": "Current code agents and LLM-based tools struggle with long-horizon, complex computational tasks that require planning, orchestration of many jobs, robust error handling, and minimal human supervision. Scientific computing and serious data-science workflows often involve many long-running simulations or analyses with interdependencies, where high-level strategy errors are costly and hard to fix mid-stream. The authors aim to build an autonomous system that can manage such workflows end-to-end, maintaining progress and quality without constant human guidance.", "method": "The authors design PARC as a hierarchical multi-agent architecture. At a high level, it has agents for task planning and decomposition, agents that execute code and run jobs, and a distinct mechanism for self-assessment and self-feedback. This self-assessment component evaluates prior actions and outcomes from an independent context, detects strategic or high-level errors, and issues feedback or corrections to adjust future behavior. The system is tested on real-world computational science and data-science tasks that require coordinating many long, parallel jobs and complex analysis pipelines, starting from natural-language instructions.", "result": "In materials science case studies, PARC autonomously reproduces key results from published work on lithium-ion conduction and alloy segregation. It successfully coordinates dozens of parallel simulation tasks, each about 43 hours long, while handling orchestration, monitoring, and error correction without human intervention. In Kaggle-style data-science experiments, given only minimal natural-language task descriptions, it designs and runs data analysis and search strategies that achieve performance competitive with human-engineered baselines.", "conclusion": "A hierarchical multi-agent architecture enhanced with explicit self-assessment and self-feedback can enable coding agents to autonomously perform long-horizon, large-scale scientific and analytical tasks. PARC\u2019s ability to plan, execute, monitor, and correct its own work across many long-running jobs suggests a path toward AI systems that can independently carry out complex computational research and data-science workflows with minimal human oversight."}}
{"id": "2512.03377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03377", "abs": "https://arxiv.org/abs/2512.03377", "authors": ["Hanting Chen", "Chu Zhong", "Kai Han", "Yuchuan Tian", "Yuchen Liang", "Tianyu Guo", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "title": "Nexus: Higher-Order Attention Mechanisms in Transformers", "comment": null, "summary": "Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \\textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \\textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.", "AI": {"tldr": "They propose Higher-Order Attention Network (Hon), which recursively refines queries and keys via inner self-attention loops to overcome the low-rank limitation of standard attention, achieving better performance without adding many parameters.", "motivation": "Standard Transformer self-attention is effectively low-rank and struggles to capture complex, multi-hop, higher-order token relationships within a single layer. The authors want a more expressive attention mechanism that can model higher-order correlations and richer global context without significantly increasing parameter count.", "method": "Introduce Higher-Order Attention Network (Hon), where instead of using single linear projections to form queries and keys, these vectors are obtained through recursive, nested self-attention operations. Each recursion step lets tokens re-aggregate global contextual information, and weights are shared across recursion steps to keep parameters constant. The final attention uses these context-enriched, higher-order Q/K representations. They also provide theoretical analysis showing this breaks the linear low-rank bottleneck of standard attention.", "result": "Theoretically, Hon is shown to overcome the rank limitations of standard first-order attention. Empirically, when evaluated on several benchmarks, Hon achieves better performance than standard Transformer architectures while using only O(1) extra parameters due to weight sharing across recursive steps.", "conclusion": "Recursive, higher-order attention over Q/K representations can significantly enhance the expressiveness of Transformers without substantial parameter growth. Hon breaks the linear bottleneck in standard attention and yields consistent empirical gains across tasks, making it a promising alternative attention module for future architectures."}}
{"id": "2512.03560", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03560", "abs": "https://arxiv.org/abs/2512.03560", "authors": ["Gianni Molinari", "Fabio Ciravegna"], "title": "Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks", "comment": "11 pages, 1 figure, 2 tables, Workshop AAAI 2026 agentic AI Benchmarks and Applications for Enterprise Tasks", "summary": "Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.", "AI": {"tldr": "The paper introduces RP-ReAct, a multi-agent framework that separates high-level reasoning and planning from low-level tool execution, improving reliability and efficiency for complex enterprise tasks under context window constraints.", "motivation": "Autonomous agents in enterprise environments face challenges solving complex, multi-step tasks requiring coordination of many tools and heterogeneous data. Existing single-agent architectures use a monolithic plan-execute loop, which causes unstable trajectories, and enterprise privacy constraints often force the use of smaller open-weight models whose limited context windows are easily overwhelmed by large tool outputs. These limitations hinder robustness, reliability, and practicality of agentic systems for real-world deployment.", "method": "The authors propose RP-ReAct, a multi-agent architecture that decouples strategic planning from execution. A Reasoner Planner Agent (RPA), powered by a strong Large Reasoning Model, performs continuous high-level planning and analysis of intermediate results. One or more Proxy-Execution Agents (PEAs) realize these plans by interacting with tools using a ReAct-style approach. The PEAs integrate a context-saving strategy that stores large tool outputs externally and retrieves relevant parts on demand, mitigating context window overflow. The framework is evaluated on the multi-domain ToolQA benchmark using six different open-weight reasoning models.", "result": "On the ToolQA benchmark, RP-ReAct outperforms state-of-the-art baselines in solving diverse, complex tasks across multiple domains. It demonstrates better generalization, robustness, and trajectory stability across varying model scales, indicating that separating planning and execution plus explicit context management yields more reliable and effective agent behavior.", "conclusion": "RP-ReAct effectively addresses two key limitations of current autonomous agent systems: trajectory instability from monolithic single-agent loops and context constraints from smaller open-weight models. By splitting reasoning/planning from execution and adding an explicit context-saving mechanism, the framework achieves superior performance, robustness, and scalability on a challenging multi-domain benchmark, showing strong potential for practical enterprise deployments of tool-using agents."}}
{"id": "2512.03381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03381", "abs": "https://arxiv.org/abs/2512.03381", "authors": ["Nicholas Tomlin", "Naitian Zhou", "Eve Fleisig", "Liangyuan", "Chen", "T\u00e9a Wright", "Lauren Vinh", "Laura X. Ma", "Seun Eisape", "Ellie French", "Tingting Du", "Tianjiao Zhang", "Alexander Koller", "Alane Suhr"], "title": "Characterizing Language Use in a Collaborative Situated Game", "comment": null, "summary": "Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.", "AI": {"tldr": "They build and release a Portal 2 cooperative game dialogue corpus and show it exhibits rich collaborative and spatial language phenomena not common in existing datasets.", "motivation": "Existing dialogue corpora mostly cover chitchat or simple task-oriented settings and lack data where people must communicate and coordinate under uncertainty in rich, dynamic virtual environments. Such data is needed to study how language is used for complex, situated, collaborative problem-solving, including spatial reasoning and coordination.", "method": "Record and transcribe 11.5 hours of spoken dialogue from human pairs playing the cooperative mode of Portal 2, producing 24.5K utterances. Collect synchronized game-state logs and player video/audio. Annotate the corpus both manually and automatically for relevant linguistic phenomena and behaviors, and then analyze the data to identify distinctive language patterns such as complex spatial reference, clarification/repair, and ad-hoc convention formation.", "result": "The authors create the Portal Dialogue Corpus containing videos, audio, transcripts, game-state data, and multi-layer annotations. Their analysis shows that players frequently produce linguistic behaviors that are rare in standard corpora, such as intricate spatial descriptions, negotiation of shared conventions, and conversational repair strategies specific to collaborative gameplay and problem solving.", "conclusion": "The new Portal Dialogue Corpus captures rich, situated, collaborative dialogue phenomena not adequately represented in existing datasets. By releasing the data and annotations, the authors provide a valuable resource for future research on language use in complex cooperative tasks, multimodal grounding, and coordination under uncertainty."}}
{"id": "2512.03571", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03571", "abs": "https://arxiv.org/abs/2512.03571", "authors": ["Zhening Li", "Armando Solar-Lezama", "Yisong Yue", "Stephan Zheng"], "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "comment": "65 pages, 2 figures, published in NeurIPS 2025", "summary": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.", "AI": {"tldr": "The paper proposes PAN, a programming model and Python framework (EnCompass) that cleanly separates agent workflow logic from inference-time strategies for LLM-based agents, enabling easier experimentation and reliability improvements.", "motivation": "Existing LLM-based agent programming approaches intertwine the specification of an agent\u2019s core workflow with the choice of inference-time strategy (e.g., tree search, planning, refinement). This coupling makes it difficult to swap strategies, systematically explore alternatives, or improve reliability without substantial code changes. The authors want a way to describe the agent\u2019s logical workflow once and then flexibly plug in different inference-time decision processes.", "method": "The authors introduce probabilistic angelic nondeterminism (PAN), a programming model in which the agent workflow is written as a high-level program whose nondeterministic choices are left unspecified (\"angelic\"); at inference time, separate strategy modules resolve these choices using probabilistic search over a compiled search space. They realize PAN in a Python framework called EnCompass, which uses decorators to transform ordinary-looking workflow code into an explicit search space representation. Different inference strategies (e.g., various tree searches or selection heuristics) can then be applied without changing the core workflow code, only by adjusting configuration inputs.", "result": "Using three case studies, the authors show that EnCompass can be used to rapidly improve LLM agent reliability and to switch between different inference-time strategies with minimal code modifications. The empirical examples illustrate that (1) the same workflow can support multiple search/inference strategies, and (2) tuning or swapping these strategies can yield better task performance while preserving the original program structure.", "conclusion": "Separating workflow specification from inference-time strategy via the PAN model and the EnCompass framework yields a more modular and flexible way to program LLM-based agents. This abstraction enables quick experimentation with different search and decision procedures, facilitates reliability improvements, and reduces the engineering burden typically associated with revising or upgrading agent reasoning strategies."}}
{"id": "2512.03402", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03402", "abs": "https://arxiv.org/abs/2512.03402", "authors": ["Yixing Xu", "Chao Li", "Xuanwu Yin", "Spandan Tiwari", "Dong Li", "Ashish Sirasao", "Emad Barsoum"], "title": "Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates", "comment": null, "summary": "Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.", "AI": {"tldr": "They propose Dual LoRA, a modified LoRA method that introduces separate magnitude and direction controls using ReLU and sign functions to better mimic full fine-tuning, achieving consistently better performance than LoRA variants with the same parameter budget.", "motivation": "Standard LoRA assumes that the task-specific parameter updates lie in a fixed low-rank subspace, which can be too restrictive and lead to suboptimal performance. There is a need for a PEFT method that remains parameter-efficient while more faithfully approximating the dynamics of full gradient-based fine-tuning.", "method": "Dual LoRA decomposes each LoRA low-rank update into two groups of matrices: a magnitude group and a direction group. The magnitude group, passed through a ReLU nonlinearity, determines whether and how strongly a parameter should be updated. The direction group, transformed with a sign function, determines the update direction (positive or negative). This design is meant to emulate the behavior of gradient updates where both magnitude and direction matter, while keeping the overall number of trainable parameters equal to conventional LoRA. The method is applied on top of existing LLM architectures (GPT-2, RoBERTa, DeBERTa, LLaMA-1/2/3).", "result": "Across various NLP tasks, including NLG, NLU, and commonsense reasoning, and on multiple LLM backbones, Dual LoRA consistently outperforms vanilla LoRA and state-of-the-art LoRA variants under the same parameter budget. This indicates that the added inductive bias improves the expressivity and effectiveness of PEFT without sacrificing efficiency.", "conclusion": "Introducing an explicit separation between magnitude and direction in low-rank parameter updates, implemented via simple ReLU and sign functions, yields a more faithful approximation of full fine-tuning dynamics, leading to consistent performance gains over existing LoRA-based PEFT approaches at the same level of parameter efficiency."}}
{"id": "2512.03607", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03607", "abs": "https://arxiv.org/abs/2512.03607", "authors": ["Yusen Wu", "Xiaotie Deng"], "title": "DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization", "comment": null, "summary": "This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.\n  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.", "AI": {"tldr": "DeepRule is a tri-level framework that turns messy retail data and constraints into interpretable, profit-optimizing pricing and assortment rules.", "motivation": "Retail assortment and pricing decisions rely on theoretical models that often fail in practice because they ignore (a) rich but unstructured textual information such as negotiation notes and approval documents, (b) complex, time-varying and nonlinear relationships like dynamic price elasticity, and (c) real multi-tier business constraints across manufacturers, distributors, and retailers. There is a need for a unified system that can ingest unstructured knowledge, handle multi-agent incentives, and still output operational, auditable business rules rather than black-box recommendations.", "method": "The paper proposes DeepRule, a tri-level architecture: (1) a hybrid knowledge fusion engine that uses LLMs to semantically parse unstructured texts (contracts, negotiations, assessments) into structured, model-ready features while incorporating expert knowledge; (2) a game-theoretic constrained optimization layer that models bilateral utility functions for manufacturers and distributors, treating profit redistribution and multi-level constraints as endogenous objectives; and (3) an interpretable decision distillation interface that employs LLM-guided symbolic regression to derive human-readable pricing strategies and business rules, embedding economic priors such as non-negative elasticity as hard constraints during expression search.", "result": "In real retail deployments, DeepRule outperforms established B2C pricing baselines in terms of profit while maintaining adherence to operational and contractual constraints. The framework successfully converts unstructured business knowledge into usable signals, reconciles multi-agent incentives, and produces interpretable rules that can be audited by managers.", "conclusion": "DeepRule demonstrates that combining LLM-based text understanding, game-theoretic constrained optimization, and constrained symbolic regression can create a closed-loop pipeline for real-world retail pricing and assortment optimization. The framework bridges the gap between theory and practice by unifying unstructured knowledge integration, multi-agent economic modeling, and interpretable rule generation, advancing the state of practical economic intelligence systems."}}
{"id": "2512.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03442", "abs": "https://arxiv.org/abs/2512.03442", "authors": ["Xingrun Xing", "Zhiyuan Fan", "Jie Lou", "Guoqi Li", "Jiajun Zhang", "Debing Zhang"], "title": "PretrainZero: Reinforcement Active Pretraining", "comment": null, "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "AI": {"tldr": "The paper proposes PretrainZero, a reinforcement learning framework that moves RL-based reasoning from domain-specific post-training to general pretraining on large corpora without explicit supervision, improving general reasoning ability.", "motivation": "Current RL-based large reasoning models achieve strong performance in specific domains (like coding and math) but depend heavily on explicit, verifiable rewards and task-specific environments. This dependency limits scalability to broad, general reasoning, where labeled or verifiable feedback is scarce or unavailable. The authors aim to overcome this bottleneck and enable RL to improve general-purpose reasoning directly during pretraining, using only raw text corpora.", "method": "The authors introduce PretrainZero, an RL-based active learning framework applied during pretraining on large unlabeled corpora (e.g., Wikipedia). Instead of passively modeling text, the model learns a unified reasoning policy that actively selects informative spans in the corpus and attempts to predict or reconstruct them via RL. The setup is self-supervised: there are no external labels, reward models, or supervised fine-tuning. Rewards are derived from the model\u2019s success in reasoning over increasingly challenging masked spans, making the process akin to self-play in masked prediction. The approach is applied to base models ranging from 3B to 30B parameters and is designed to scale with verification difficulty (\"verification scaling\").", "result": "Using PretrainZero for reinforcement pretraining on Wikipedia, the authors significantly improve general reasoning benchmarks compared with the original base models. For example, Qwen3-4B-Base gains 8.43 points on MMLU-Pro, 5.96 on SuperGPQA, and 10.60 on an average of math benchmarks. The pretrained models also serve effectively as starting points (reasoning foundations) for downstream RL with verifiable rewards (RLVR) tasks, showing that the method transfers beyond the pretraining domain.", "conclusion": "PretrainZero demonstrates that reinforcement learning can be moved from narrow, reward-rich post-training setups to broad, self-supervised pretraining on general corpora. By actively selecting informative content and learning to reason over progressively harder masked spans without explicit labels or reward models, it breaks the reliance on verifiable task-specific rewards and substantially boosts general reasoning performance. The framework also yields strong initialization for subsequent task-specific RL, suggesting a promising direction toward more general, human-like learning and reasoning in large models."}}
{"id": "2512.03627", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03627", "abs": "https://arxiv.org/abs/2512.03627", "authors": ["Junming Liu", "Yifei Sun", "Weihua Cheng", "Haodong Lei", "Yirong Chen", "Licheng Wen", "Xuemeng Yang", "Daocheng Fu", "Pinlong Cai", "Nianchen Deng", "Yi Yu", "Shuyue Hu", "Botian Shi", "Ding Wang"], "title": "MemVerse: Multimodal Memory for Lifelong Learning Agents", "comment": "11 pages, 2 figures, 2 tables", "summary": "Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.", "AI": {"tldr": "MemVerse is a plug-and-play memory framework that gives AI agents scalable, hierarchical, and multimodal memory, improving long-horizon reasoning and continual learning.", "motivation": "Current large language and vision models lack reliable, persistent memory, causing them to forget past experiences, fail at long-horizon and interactive tasks, and struggle to maintain coherence over time. The paper aims to overcome this core limitation by designing an external memory system that is both scalable and tightly integrated with parametric models.", "method": "The authors propose MemVerse, a model-agnostic memory framework. It maintains a short-term memory for recent context and converts raw multimodal experiences into structured long-term memories represented as hierarchical knowledge graphs. MemVerse supports continual consolidation, adaptive forgetting, and bounded memory growth. It also introduces a periodic distillation process that compresses salient long-term knowledge back into the parametric model, enabling fast, differentiable recall while keeping the memory interpretable.", "result": "Experiments show that integrating MemVerse with AI agents yields substantial gains in multimodal reasoning performance and in the efficiency of continual learning across extended interactions. Agents become better at remembering relevant past information and using it for current tasks.", "conclusion": "MemVerse effectively augments AI agents with scalable, structured, and adaptive memory. By combining hierarchical retrieval-based memory with periodic distillation into the base model, it allows agents to remember and reason coherently over long time horizons and complex multimodal interactions, addressing a key limitation of current large models."}}
{"id": "2512.03494", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03494", "abs": "https://arxiv.org/abs/2512.03494", "authors": ["Di Xiu", "Hongyin Tang", "Bolin Rong", "Lizhi Yan", "Jingang Wang", "Yifan Lu", "Xunliang Cai"], "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention", "comment": null, "summary": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.", "AI": {"tldr": "The paper studies Top-k Attention for long-context LLMs, showing that keeping only the most relevant keys can preserve or improve performance while reducing computation, especially when training and inference both use Top-k and when approximate Top-k is sufficiently accurate.", "motivation": "LLMs with long-context capabilities face prohibitive inference costs because standard full attention scales quadratically with sequence length. This computational bottleneck limits practical deployment for agentic and multimodal long-context tasks. There is a need to understand whether sparse mechanisms like Top-k Attention can reduce cost without sacrificing, or even improving, downstream performance and to clarify the theoretical reasons behind their effectiveness.", "method": "The authors empirically evaluate exact Top-k Attention in decoding by retaining only keys with highest similarity to each query instead of full attention, testing on downstream benchmarks like HELMET and LongBench v2. They then introduce and study a native Top-k Attention training strategy that aligns training-time and inference-time attention sparsity patterns. They further investigate approximate Top-k algorithms (e.g., Lightning Indexer in the DeepSeek-V3.2-Exp model), measuring how approximation fidelity affects downstream performance. Finally, they analyze entropy changes in model outputs after Top-k Attention supervised fine-tuning (SFT) to build a theoretical explanation for why Top-k works, focusing on entropy reduction and its relation to Top-k decoding effectiveness.", "result": "Exact Top-k Decoding performs on par with or better than full attention on HELMET and LongBench v2, even though it only keeps a subset of the most similar keys as context. Training with native Top-k Attention, i.e., using Top-k during both training and inference, further boosts performance, indicating a strong benefit from train\u2013test consistency in sparsity. For approximate Top-k methods, they find that downstream performance improves with higher approximation fidelity and they quantify the precision of the Lightning Indexer as used in DeepSeek-V3.2-Exp. Empirically, models trained with Top-k Attention SFT show clear entropy reductions on downstream tasks.", "conclusion": "Top-k Attention is an effective way to reduce long-context inference cost for LLMs without sacrificing, and sometimes improving, task performance, especially when the same Top-k mechanism is used consistently during training and decoding. Approximate Top-k methods can be viable if their fidelity is sufficiently high, as evidenced by the Lightning Indexer analysis. The observed entropy reduction after Top-k Attention SFT supports a theoretical view that models in lower-entropy regimes are better suited for Top-k Decoding, offering a principled explanation for the empirical gains and guiding future design of efficient long-context attention mechanisms."}}
{"id": "2512.03762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03762", "abs": "https://arxiv.org/abs/2512.03762", "authors": ["Jiawei Xu", "Fengfeng Wei", "Weineng Chen"], "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design", "comment": null, "summary": "Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.", "AI": {"tldr": "The paper introduces RoCo, a multi-agent role-based LLM framework for Automatic Heuristic Design (AHD) that outperforms prior LLM-based and evolutionary approaches on multiple combinatorial optimization problems.", "motivation": "Existing LLM-based AHD methods typically use a single model/role, limiting diversity, robustness, and the ability to balance exploration and exploitation when generating heuristics for combinatorial optimization problems. There is a need for a more structured, collaborative mechanism that leverages different reasoning styles and feedback to systematically evolve stronger heuristics, and that works in both white-box and black-box settings.", "method": "RoCo defines four specialized LLM-guided agents\u2014explorer, exploiter, critic, and integrator\u2014organized into a structured multi-round interaction protocol. The explorer generates diverse, creative heuristic variants emphasizing long-term potential; the exploiter produces more conservative, efficiency-oriented refinements that exploit current good ideas; the critic evaluates each evolution step and provides explicit feedback and reflection; and the integrator synthesizes and mutates candidate heuristics based on both short-term feedback and accumulated long-term reflections. This collaborative loop iteratively evolves a population of heuristics for combinatorial optimization problems, applicable when problem internals are available (white-box) or hidden (black-box).", "result": "Across five combinatorial optimization problems and under both white-box and black-box evaluation, RoCo consistently discovers heuristics that are competitive with or superior to existing AHD baselines, including ReEvo and HSEvo. The method shows robust performance gains, indicating that the multi-role collaboration and reflection mechanism leads to higher-quality, more general heuristics than single-role LLM approaches or prior evolutionary systems.", "conclusion": "A role-based multi-agent LLM architecture can significantly improve Automatic Heuristic Design for combinatorial optimization. By explicitly separating and coordinating exploration, exploitation, critique, and integration, RoCo produces stronger heuristics than prior state-of-the-art methods in diverse settings. This suggests that structured role specialization and collaborative reflection are key ingredients for building robust, high-performing LLM-driven AHD systems and may generalize to other optimization and program synthesis domains."}}
{"id": "2512.03503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03503", "abs": "https://arxiv.org/abs/2512.03503", "authors": ["Haohan Yuan", "Siu Cheung Hui", "Haopeng Zhang"], "title": "Understanding LLM Reasoning for Abstractive Summarization", "comment": "26 pages,15 figures", "summary": "While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.", "AI": {"tldr": "The paper evaluates how different reasoning strategies in large language and reasoning models affect abstractive summarization quality and factual faithfulness, finding a trade-off between fluency and factual grounding.", "motivation": "Although LLMs are praised for reasoning in tasks like math and coding, it is mostly assumed, not proven, that these reasoning abilities help abstractive summarization; the paper aims to rigorously test and clarify this assumption.", "method": "The authors adapt general reasoning strategies specifically for summarization, then run a large-scale comparison of 8 reasoning strategies and 3 large reasoning models over 8 varied summarization datasets, evaluating both summary quality and factual faithfulness.", "result": "They find that no single reasoning strategy universally helps summarization: explicit reasoning tends to improve fluency but harms factual accuracy, implicit reasoning in LRMs tends to do the opposite, and simply increasing an LRM\u2019s internal reasoning budget can fail to help or even worsen factual consistency.", "conclusion": "Reasoning in LLMs is not a one-size-fits-all solution for summarization; effective summarization requires carefully balancing reasoning with faithful compression, avoiding excessive or creative over-thinking that may hurt factual grounding."}}
{"id": "2512.03783", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.03783", "abs": "https://arxiv.org/abs/2512.03783", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "AI": {"tldr": "Omni-AutoThink introduces an adaptive reasoning framework for multimodal Omni models that dynamically adjusts reasoning depth by combining adaptive supervised fine-tuning, reinforcement learning, and a new multimodal benchmark, achieving better performance than prior baselines.", "motivation": "Existing Omni multimodal models can perceive and generate across modalities but show rigid, non-adaptive reasoning: they may overthink trivial tasks and under-reason on hard ones. There is a need for a framework that can automatically calibrate how much reasoning a model performs based on task difficulty. Moreover, there is a lack of comprehensive benchmarks to evaluate adaptive reasoning across multiple modalities (text, audio, visual, and their combinations).", "method": "The authors propose Omni-AutoThink, an adaptive reasoning framework with two main stages. First, an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage trains an Omni model on large-scale, reasoning-augmented data to provide basic reasoning skills. Second, an Adaptive Reinforcement Learning stage (Adaptive GRPO) further optimizes the model\u2019s reasoning behaviors by conditioning on task complexity and using reward feedback to learn when to reason more or less. Additionally, they build a multimodal adaptive reasoning benchmark covering text-only, text-audio, text-visual, and text-audio-visual tasks, with separate splits for training and evaluation.", "result": "Experiments on the proposed benchmark show that Omni-AutoThink outperforms previous baselines in adaptive reasoning, meaning it can better modulate reasoning depth depending on task difficulty across various modalities. Quantitative evaluations demonstrate significant performance gains over non-adaptive or less adaptive methods.", "conclusion": "Omni-AutoThink effectively equips Omni models with dynamic, task-aware reasoning depth, alleviating the problems of overthinking simple tasks and under-reasoning complex ones. The combination of adaptive SFT, adaptive reinforcement learning, and a multimodal benchmark leads to superior adaptive reasoning performance. The authors commit to releasing their benchmark data and code to support further research in multimodal adaptive reasoning."}}
{"id": "2512.03582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03582", "abs": "https://arxiv.org/abs/2512.03582", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "title": "Fine-grained Narrative Classification in Biased News Articles", "comment": null, "summary": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "AI": {"tldr": "The paper introduces INDI-PROP, a fine-grained, ideologically grounded narrative dataset for Indian news propaganda, and proposes two GPT-4o-mini based multi-hop prompt reasoning frameworks (FANTA and TPTC) that significantly improve bias, narrative, and persuasive technique classification.", "motivation": "Existing propaganda detection work largely focuses on coarse bias labels or generic persuasive techniques and is mostly centered on Western contexts. There is a lack of ideologically grounded, fine-grained narrative analysis, especially for Indian media and highly polarizing socio-political events. The authors aim to fill this gap by modeling narratives as central structures of propaganda that connect techniques, ideology, and communicative intent, and by building resources and methods tailored to this setting.", "method": "The authors construct INDI-PROP, a dataset of 1,266 Indian news articles about two contentious events (CAA and Farmers' protest), annotated at three hierarchical levels: (1) ideological article bias (pro-government, pro-opposition, neutral); (2) event-specific fine-grained narrative frames tied to ideological polarity and communicative intent; and (3) persuasive techniques. They then design two GPT-4o-mini\u2013driven multi-hop prompt-based reasoning frameworks: FANTA, which performs hierarchical reasoning by first extracting information and contextual framing to exploit multi-layer communicative phenomena, and TPTC, which decomposes persuasive cues in a two-stage process for technique classification. Both frameworks are evaluated against baselines on the tasks of bias, narrative, and technique classification.", "result": "INDI-PROP provides a rich, multi-level resource for analyzing propaganda in Indian news related to CAA and the Farmers' protest. Experiments demonstrate that both proposed frameworks, FANTA and TPTC, achieve substantial performance gains over baseline models on the tasks of ideological bias detection, narrative frame classification, and persuasive technique identification, showing the effectiveness of multi-hop, prompt-based reasoning with GPT-4o-mini for these hierarchical, fine-grained tasks.", "conclusion": "Narratives function as key scaffolds for propaganda, and modeling them in a fine-grained, ideologically grounded, and event-specific manner substantially improves our ability to analyze biased news. The INDI-PROP dataset fills an important resource gap for Indian media propaganda research, while the FANTA and TPTC frameworks show that GPT-4o-mini\u2013guided multi-hop reasoning is a powerful approach for hierarchical tasks such as article bias, narrative frame, and persuasive technique classification. This work opens avenues for richer narrative-centered propaganda analysis beyond coarse bias labels and generic technique taxonomies."}}
{"id": "2512.03887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03887", "abs": "https://arxiv.org/abs/2512.03887", "authors": ["Saurav Prateek"], "title": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)", "comment": null, "summary": "The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.\n  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.\n  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/", "AI": {"tldr": "The paper proposes Static-DRA, a configurable, tree-structured deep research agent with tunable depth and breadth that trades off report quality vs. LLM cost, and shows that higher depth/breadth improve benchmark scores.", "motivation": "Static RAG pipelines struggle with complex, multi-turn research tasks, while many existing agentic systems are dynamic, opaque, and computationally expensive. The authors aim to design a more transparent, resource-aware deep research agent that gives users explicit control over the intensity and cost of the research process.", "method": "They design Static-DRA, a static, hierarchical tree-based workflow consisting of Supervisor, Independent, and Worker agents. Two key hyperparameters\u2014Depth (levels of reasoning and decomposition) and Breadth (number of branches/subtopics explored per level)\u2014control research intensity. The system performs multi-hop retrieval and parallel subtopic exploration. They evaluate it on the DeepResearch Bench using the RACE evaluation framework, using gemini-2.5-pro as the underlying LLM and varying Depth and Breadth to measure impact on performance and cost.", "result": "With Depth=2 and Breadth=5 using gemini-2.5-pro, Static-DRA obtains an overall DeepResearch Bench score of 34.72. Experiments show that increasing depth and breadth yields more detailed research behavior and higher RACE evaluation scores, at the cost of more LLM calls.", "conclusion": "Static-DRA delivers a practical, transparent, and resource-aware alternative to dynamic deep research agents. Its tunable depth and breadth parameters allow users to balance research quality and computational cost, and higher settings are empirically shown to improve benchmark performance. The system and artifacts are fully open-sourced for reproducibility and extension."}}
{"id": "2512.03634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03634", "abs": "https://arxiv.org/abs/2512.03634", "authors": ["Ahmad Aghaebrahimian"], "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment", "comment": null, "summary": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.", "AI": {"tldr": "They propose an interpretable, weighted, fact-level evaluation framework to measure and control factual consistency (hallucinations) of LLM outputs, including in clinical domains, and benchmark it on general and clinical datasets.", "motivation": "LLMs often hallucinate\u2014producing fluent but factually incorrect statements\u2014which is dangerous in high-stakes areas like medicine. Current factuality metrics are inadequate: they don\u2019t capture factual consistency well, lack interpretability, use rigid schemas, and provide only coarse, absolute scores, making it hard to understand and fix errors or train fact-aware models.", "method": "They decompose generated and reference texts into atomic facts and evaluate factual consistency at this fine-grained level. The framework is schema-free and flexible, so it can generalize across in-domain and open-domain scenarios. They introduce a weighted metric that assigns different importance to different facts rather than treating all equally, and they add a mechanism to tune or bound the assessment complexity for very complex domains, such as clinical text. They then implement this framework and apply it to common general and clinical benchmarks.", "result": "Their fact-decomposition framework yields interpretable factual consistency scores and allows differential weighting of facts, improving the sensitivity and usefulness of evaluation compared to absolute, unweighted metrics. On popular general and clinical datasets, the approach effectively captures hallucinations and supports richer diagnostics of model behavior; they also provide an open-source implementation.", "conclusion": "An interpretable, atomic-fact-based, weighted evaluation framework can better assess and analyze LLM hallucinations across general and clinical domains, overcoming limits of previous absolute metrics. By controlling evaluation complexity and releasing code, the work enables more precise, fact-aware training and diagnostic tools for reliable LLM deployment, especially in high-stakes settings."}}
{"id": "2512.03931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03931", "abs": "https://arxiv.org/abs/2512.03931", "authors": ["Vineel Tummala", "Daniela Inclezan"], "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties", "comment": "27 pages, 5 figures", "summary": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "The paper proposes a logic-programming framework that lets autonomous agents weigh policy compliance against penalties, enabling them to sometimes break rules rationally when high-stakes goals are involved.", "motivation": "Most existing work for policy-aware agents focuses on strict compliance, assuming that rules must always be followed. In real high-stakes scenarios, however, agents may need to consider whether violating some rules is justified when critical goals are at risk. Moreover, policymakers need tools that can realistically model how humans might sometimes choose non-compliance when penalties are acceptable. The paper aims to provide a formal, explainable way to reason about such trade-offs between goals, policies, and penalties.", "method": "The authors extend Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) by adding explicit representations of penalties and policy priorities. They then automatically translate this extended AOPL into Answer Set Programming (ASP), and adapt ASP-based planning algorithms so that they compute plans while explicitly tracking policy violations and their associated penalties. Their reasoning mechanism compares non-compliant plans based on the penalties incurred, preferring those that achieve goals with minimal negative consequences, and ensures policies are well-formed and explainable by surfacing what was violated and why.", "result": "In two experimental domains, the proposed framework generates plans that are of higher quality with respect to policy awareness: they avoid harmful actions and minimize penalties when rule violations are necessary. In some cases, the approach also improves computational efficiency compared to previous ASP-based methods that lack explicit penalty reasoning.", "conclusion": "Incorporating penalties and policy priorities into a logic-programming framework for autonomous agents allows for nuanced reasoning about when and how to deviate from policies. This improves the quality and explainability of autonomous decision-making in high-stakes contexts and provides a useful tool for policymakers to explore and refine policy designs. The work demonstrates that policy-aware planning with explicit penalty reasoning is both practical and beneficial, and is promising enough to be considered for publication in TPLP."}}
{"id": "2512.03671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03671", "abs": "https://arxiv.org/abs/2512.03671", "authors": ["Beatrice Savoldi", "Giuseppe Attanasio", "Olga Gorodetskaya", "Marta Marchiori Manerba", "Elisa Bassignana", "Silvia Casola", "Matteo Negri", "Tommaso Caselli", "Luisa Bentivogli", "Alan Ramponi", "Arianna Muti", "Nicoletta Balbo", "Debora Nozza"], "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context", "comment": null, "summary": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.", "AI": {"tldr": "The paper empirically maps how Italian adults adopt and use generative AI chatbots, showing widespread but low-literacy use, significant gender gaps, and the need for targeted education and further research on non-competence barriers.", "motivation": "Generative AI chatbots are rapidly transforming digital interactions and information seeking, but uneven adoption, low awareness of limitations, and possible digital divides (including gender gaps) may deepen existing inequalities. There is a lack of comprehensive, country-level empirical evidence on who uses GenAI, how, and with what level of literacy, especially in Italy.", "method": "The authors conducted a large-scale survey of 1,906 Italian-speaking adults in Italy, collecting data on GenAI adoption, frequency and domains of use (work, personal, sensitive contexts), replacement of other information sources, and measures of digital and AI literacy. They then performed statistical analyses to map adoption patterns, identify predictors of use, and quantify demographic divides such as gender differences across age groups.", "result": "The survey shows that GenAI chatbots are widely adopted in Italy for both professional and personal purposes, including sensitive uses like emotional and medical advice. GenAI is increasingly replacing other information sources as a primary channel, even though many users exhibit low digital/AI literacy and struggle to detect errors or misinformation. A marked gender divide is found: women, especially older women, are about half as likely as men to adopt and frequently use GenAI. Literacy is an important predictor of adoption, but it only partially accounts for the gender gap, indicating additional unidentified barriers.", "conclusion": "GenAI has already become a multipurpose, primary information tool for many Italian adults, but this shift is occurring under conditions of low user literacy and pronounced gender disparities. Addressing these issues requires targeted educational initiatives to improve GenAI literacy and critical evaluation skills, as well as deeper investigation into structural, cultural, or socio-psychological barriers that limit equitable participation beyond simple competence differences."}}
{"id": "2512.03955", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03955", "abs": "https://arxiv.org/abs/2512.03955", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "comment": "This work has been submitted to IFAC for possible publication", "summary": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "AI": {"tldr": "Benchmarking LLM-based agents for adaptive industrial automation using an executable Blocksworld simulation with standardized tool interfacing via MCP.", "motivation": "Industrial automation needs flexible, adaptive control that can handle varying tasks and environments. While LLM-based agents show promise for such adaptive planning and execution, there is currently no standardized way to benchmark and compare different agent designs. This gap makes it hard to measure progress or identify which architectures are most effective.", "method": "The authors design a benchmark built around an executable Blocksworld simulation environment with five clearly defined complexity categories. They integrate the Model Context Protocol (MCP) as a uniform tool interface, allowing different LLM-agent implementations to plug into the environment without custom integration work. They then implement a single-agent architecture as a reference to demonstrate how the benchmark is used and to generate baseline results.", "result": "The benchmark successfully provides a working environment where LLM-based agents can be systematically evaluated on planning and execution tasks of varying difficulty in Blocksworld. The MCP-based interface allows different agent architectures to be connected consistently. The reference single-agent implementation produces quantitative performance metrics, establishing initial baselines for future comparisons.", "conclusion": "The proposed benchmark and simulation environment enable standardized, implementation-agnostic evaluation of LLM-based agents for planning and execution in industrial-style settings. Using MCP as a tool interface supports easy integration of diverse agent architectures. The provided single-agent implementation validates the framework\u2019s practicality and offers baseline metrics, paving the way for more rigorous and comparable research on LLM-based industrial control agents."}}
{"id": "2512.03672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03672", "abs": "https://arxiv.org/abs/2512.03672", "authors": ["Shiruo Hu", "Wenbo Shan", "Yingjia Li", "Zhiqi Wan", "Xinpeng Yu", "Yunjia Qi", "Haotian Xia", "Yang Xiao", "Dingxiao Liu", "Jiaru Wang", "Chenxu Gong", "Ruixi Zhang", "Shuyue Wu", "Shibo Cui", "Chee Hui Lai", "Wei Luo", "Yubin He", "Bin Xu", "Jianshi Zhao"], "title": "Evaluating Hydro-Science and Engineering Knowledge of Large Language Models", "comment": "Hydro-SE Bench sets a new benchmark for the evaluation of LLMs in the Hydro-Science and Engineering domain, with its code and data available at \\url{https://github.com/sheishijun/Hydro-SE-Bench}", "summary": "Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.", "AI": {"tldr": "The paper introduces Hydro-SE Bench, a 4,000-question benchmark to systematically evaluate large language models\u2019 knowledge and application ability in Hydro-Science and Engineering, revealing good performance on general science but weaknesses on domain-specific engineering tasks.", "motivation": "Hydro-Science and Engineering is vital for water security, clean energy, and disaster mitigation, and requires complex, expert-driven decision-making. As large language models rapidly improve, their potential use in this domain is promising but poorly understood because there has been no systematic evaluation of their domain knowledge, engineering application skills, and reasoning abilities specific to Hydro-SE.", "method": "The authors construct Hydro-SE Bench, a benchmark of 4,000 multiple-choice questions spanning nine Hydro-SE subfields. The questions are designed to test three dimensions: basic conceptual knowledge, engineering application ability, and reasoning/calculation ability. They then evaluate a variety of models\u2014both commercial LLMs and smaller-parameter open models\u2014on this benchmark and compare accuracy across subfields and ability types.", "result": "Commercial LLMs achieve accuracy between 0.74 and 0.80 on Hydro-SE Bench, while smaller-parameter models score between 0.41 and 0.68. Models perform relatively well in subfields tied closely to natural and physical sciences but show clear weaknesses on highly specialized content, such as industry standards and hydraulic structures. Larger models especially show gains in reasoning and calculation tasks.", "conclusion": "Hydro-SE Bench exposes both the strengths and current limitations of LLMs in Hydro-Science and Engineering: they are already competent on general scientific aspects but insufficient for many practical, domain-specific engineering applications. This benchmark can guide model developers toward targeted training and offer Hydro-SE practitioners realistic expectations and usage guidance when deploying LLMs in their workflows."}}
{"id": "2512.03676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03676", "abs": "https://arxiv.org/abs/2512.03676", "authors": ["Daria Kryvosheieva", "Andrea de Varda", "Evelina Fedorenko", "Greta Tuckute"], "title": "Different types of syntactic agreement recruit the same units within large language models", "comment": null, "summary": "Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.", "AI": {"tldr": "The paper shows that large language models (LLMs) represent many kinds of syntactic agreement using overlapping internal units, suggesting agreement is a coherent functional category in their representations across multiple languages.", "motivation": "While LLMs are good at grammaticality judgments, it is unclear how grammatical or syntactic knowledge is internally represented. Specifically, we do not know whether different syntactic phenomena are handled by distinct subsystems or by overlapping/shared components. Understanding this can clarify whether LLMs discover linguistically meaningful categories like syntactic agreement and how comparable they are to human-like representations.", "method": "The authors use a functional localization approach, inspired by methods in cognitive neuroscience. For seven open-weight LLMs, they probe internal units (e.g., neurons or features) to find those most responsive to 67 English syntactic phenomena. They test consistency of recruitment of these units across sentences that instantiate each phenomenon and conduct causal interventions (e.g., ablating or amplifying units) to see if these units are necessary for syntactic performance. They then examine overlap of units recruited by different agreement types (subject\u2013verb, anaphor, determiner\u2013noun) and extend the analysis to Russian and Chinese, plus a broad cross-lingual study of 57 languages for subject\u2013verb agreement, relating unit sharing to structural similarity among languages.", "result": "They identify sets of units that are reliably and selectively activated by each of the 67 syntactic phenomena and find that these units causally contribute to syntactic judgments. Importantly, different agreement phenomena (subject\u2013verb, anaphor agreement, determiner\u2013noun agreement) are supported by overlapping sets of units instead of entirely disjoint ones. This overlap pattern is replicated in English, Russian, and Chinese. In a cross-lingual analysis over 57 languages, languages that are structurally more similar show greater sharing of subject\u2013verb agreement units in the LLMs.", "conclusion": "LLMs\u2019 internal representations cluster syntactic agreement phenomena into a coherent functional category: different kinds of agreement depend on overlapping neural substrates within the models. This suggests that LLMs naturally organize syntactic dependencies into meaningful representational categories, and that cross-linguistic structural similarity is reflected in shared model circuitry for agreement. Overall, syntactic agreement emerges as a core, organized component of LLM grammatical knowledge rather than a set of isolated tricks learned separately for each construction or language."}}
{"id": "2512.03688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03688", "abs": "https://arxiv.org/abs/2512.03688", "authors": ["Numaan Naeem", "Kaushal Kumar Maurya", "Kseniia Petukhova", "Ekaterina Kochmar"], "title": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors", "comment": null, "summary": "We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.", "AI": {"tldr": "AITutor-EvalKit is a toolkit for evaluating the pedagogical quality of AI tutors using language technology, including demo, evaluation, model inspection, and visualization tools.", "motivation": "As AI tutors become more common, there is a need for systematic ways to assess and inspect their pedagogical quality, both for education stakeholders and the computational linguistics community, and to support data collection and user feedback.", "method": "The paper presents an application that leverages language technologies to automatically evaluate AI tutors, bundles software components for demonstration and evaluation, and offers interfaces for model inspection and data visualization, also enabling collection of user feedback and annotations.", "result": "The resulting toolkit, AITutor-EvalKit, provides a practical platform where users can evaluate AI tutors, inspect model behaviour, visualize data, and gather feedback and annotations from real users.", "conclusion": "AITutor-EvalKit can serve as a shared resource for education stakeholders and the *ACL community to assess AI tutor quality, support learning, and facilitate research through feedback and annotation collection."}}
{"id": "2512.03704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03704", "abs": "https://arxiv.org/abs/2512.03704", "authors": ["Yijun Liao"], "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue", "comment": "22 pages, 2 figures, 13 tables. Code available at https://github.com/lyj20071013/DZ-TDPO", "summary": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO", "AI": {"tldr": "The paper tackles 'State Inertia' in long-context dialogue models and proposes DZ-TDPO, an alignment method that dynamically handles conflicts between new user intents and past context while preserving general capabilities.", "motivation": "Long-context dialogue systems often over-commit to earlier conversation history, causing 'State Inertia' where models fail to adapt to changed or conflicting user intents. Existing alignment or fine-tuning methods can be destructive, harming general abilities and causing instability, especially when resolving conflicts between old and new information. The authors want a way to dynamically resolve intent-context conflicts without degrading the model\u2019s broader performance.", "method": "They introduce DZ-TDPO, a non-destructive alignment framework combining two core ideas: (1) conflict-aware dynamic KL constraints that adaptively relax or tighten the divergence penalty depending on the degree of conflict between current supervision and historical state; and (2) a learnable temporal attention bias that explicitly regulates how much the model attends to different parts of the dialogue history over time. Together, these mechanisms encourage the model to appropriately downweight outdated or conflicting context without globally overwriting its parameters.", "result": "On the Multi-Session Chat dataset, DZ-TDPO achieves state-of-the-art win rates (e.g., 86.2% on Phi-3.5) while maintaining strong zero-shot generalization. Scaling experiments show that as model size increases, the method attains higher alignment quality with smaller perplexity increases. Specifically, the Qwen2.5-7B model reaches a 99.4% win rate with negligible perplexity overhead, indicating minimal cost to base language modeling performance.", "conclusion": "The study identifies a Capacity-Stability Trade-off: smaller models pay an 'alignment tax' in perplexity to overcome historical inertia, whereas larger models can achieve near-perfect alignment with little degradation. This suggests that temporal attention regulation\u2014rather than aggressive weight updates\u2014is sufficient to mitigate State Inertia while preserving general capabilities (e.g., MMLU) across scales. DZ-TDPO offers a scalable, non-destructive way to align long-context dialogue systems with evolving user intents."}}
{"id": "2512.03759", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03759", "abs": "https://arxiv.org/abs/2512.03759", "authors": ["Jingyang Ou", "Jiaqi Han", "Minkai Xu", "Shaoxuan Xu", "Jianwen Xie", "Stefano Ermon", "Yi Wu", "Chongxuan Li"], "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective", "comment": null, "summary": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.", "AI": {"tldr": "The paper introduces ESPO, a reinforcement learning framework tailored for diffusion large language models (dLLMs) that optimizes entire sequences as single actions using an ELBO-based sequence-level likelihood proxy, overcoming the lack of token-level probabilities and achieving strong gains on reasoning, coding, and planning tasks.", "motivation": "Existing RL methods for language models are designed for autoregressive architectures, which naturally expose token-level conditional probabilities required for token-level policy optimization methods such as GRPO. Diffusion large language models (dLLMs), however, generate text via iterative non-autoregressive denoising and do not yield a simple token-factorized likelihood, making it difficult to apply standard RL techniques. There is a need for a principled, stable RL framework that works with the generative process of dLLMs and can leverage preference or reward signals to improve performance on complex tasks.", "method": "The authors propose ELBO-based Sequence-level Policy Optimization (ESPO), which reframes RL for dLLMs as optimizing a sequence-level objective where the entire generated sequence is treated as a single action. They approximate sequence-level likelihoods using the evidence lower bound (ELBO) derived from the diffusion generative process and use this as a tractable proxy for the policy\u2019s probability. ESPO integrates per-token normalization of importance sampling ratios to stabilize gradients and employs robust estimation of the KL divergence between the updated and reference policies, enabling large-scale training. The method thus bypasses the need for token-level conditional probabilities while still permitting principled policy optimization with off-policy data or preference-based rewards.", "result": "On a range of benchmarks\u2014mathematical reasoning, code generation, and planning (including the Countdown task)\u2014ESPO consistently outperforms token-level RL baselines adapted to dLLMs. The paper reports especially large improvements of about 20\u201340 points on the Countdown planning task, demonstrating that sequence-level optimization aligns well with the diffusion generation process. Performance gains are also observed on math and coding benchmarks, indicating broad applicability and robustness across domains.", "conclusion": "The study concludes that sequence-level policy optimization using an ELBO-based likelihood proxy is a principled and effective way to perform RL on diffusion large language models. By avoiding the need for token-level factorization and stabilizing training via normalized importance ratios and robust KL estimation, ESPO enables dLLMs to benefit from RL-based fine-tuning and achieves substantial empirical gains. This establishes sequence-level optimization as a promising direction for future RL methods in non-autoregressive generative architectures. The authors release their implementation to encourage further research in this area."}}
{"id": "2512.03771", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03771", "abs": "https://arxiv.org/abs/2512.03771", "authors": ["Itay Yona", "Amir Sarid", "Michael Karasik", "Yossi Gandelsman"], "title": "In-Context Representation Hijacking", "comment": null, "summary": "We introduce \\textbf{Doublespeak}, a simple \\emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \\textit{bomb}) with a benign token (e.g., \\textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", "AI": {"tldr": "Doublespeak is a simple in-context attack that hijacks token representations so that harmless-looking words take on harmful meanings inside LLMs, bypassing safety mechanisms.", "motivation": "Current LLM safety alignment mainly constrains outputs and surface prompts, but attackers may exploit the model\u2019s internal representations (latent space) to bypass safeguards. The authors want to understand and demonstrate a new attack vector that operates by manipulating internal token semantics rather than explicit prompts or optimization-based jailbreaks.", "method": "They propose an in-context representation hijacking attack called Doublespeak. The attacker systematically replaces a harmful keyword (e.g., \u201cbomb\u201d) with a benign token (e.g., \u201ccarrot\u201d) in multiple in-context examples that appear before a harmful request. This repeated paired usage causes the model\u2019s internal embedding/representation of the benign token in that context to drift toward the harmful concept. They then use interpretability tools to trace how the representation of the benign token evolves across network layers, examining how semantics change layer by layer.", "result": "The benign token\u2019s representation, when used in this crafted context, converges toward the representation of the harmful token, effectively encoding the harmful meaning under a euphemism. This allows superficially safe prompts (like \u201cHow to build a carrot?\u201d) to be internally interpreted as harmful requests (like \u201cHow to build a bomb?\u201d), bypassing standard safety alignment. The attack is optimization-free, works across different model families (transferable), and attains high attack success rates, e.g., 74% on Llama-3.3-70B-Instruct using just a single-sentence override in the context.", "conclusion": "The work reveals an overlooked vulnerability: alignment that focuses only on surface text and outputs is not enough, because attackers can hijack internal representations via in-context examples to embed harmful semantics into harmless tokens. Future safety mechanisms need to operate directly at the representation level and account for how token meanings can be overwritten layer by layer in the model\u2019s latent space."}}
{"id": "2512.03803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03803", "abs": "https://arxiv.org/abs/2512.03803", "authors": ["Huey Sun", "Anabel Yong", "Lorenzo Gilly", "Felipe Jin"], "title": "Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5", "comment": null, "summary": "Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.", "AI": {"tldr": "This paper adapts the DoLa contrastive decoding method to encoder-decoder models like T5 and FLAN-T5 to study its effect on instruction following.", "motivation": "Existing contrastive decoding methods such as DoLa have been applied only to decoder-only LLMs and mainly evaluated for factuality, leaving a gap in understanding how such methods work with encoder-decoder architectures and affect instruction following.", "method": "Implement DoLa contrastive decoding in T5 and FLAN-T5 model families, then experimentally evaluate its effect on instruction following tasks and perform a layer-by-layer analysis of logit evolution in FLAN-T5 to quantify how DoLa changes token probability distributions.", "result": "DoLa improves the faithfulness of text generation on some categories of instruction-following tasks but degrades performance on others, demonstrating task-dependent effects. The layer-wise logit analysis reveals how DoLa alters token output probabilities across layers.", "conclusion": "Contrastive decoding via DoLa is feasible in encoder-decoder models and can enhance faithfulness for certain tasks, but it is not universally beneficial; understanding its layer-wise impact on token probabilities helps explain when and why it helps or harms instruction following."}}
{"id": "2512.03818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03818", "abs": "https://arxiv.org/abs/2512.03818", "authors": ["Kylie L. Anglin", "Stephanie Milan", "Brittney Hernandez", "Claudia Ventura"], "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology", "comment": "22 pages, 2 figures", "summary": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.", "AI": {"tldr": "The paper studies how to systematically design prompts so large language models can accurately classify psychological constructs in text, showing that careful, empirically tested prompt wording\u2014especially construct definitions, task framing, and selected examples\u2014matters more than popular techniques like personas or chain-of-thought.", "motivation": "Large language models can classify text well, but their outputs vary greatly with small changes in prompt wording, especially in specialized domains like psychology where constructs have precise, theory-based definitions that may be poorly represented in pre-training data. Existing prompt-engineering work rarely focuses on such classification tasks or on domains needing close alignment with expert definitions, leaving researchers without a clear, systematic way to design prompts that match expert judgments.", "method": "The authors propose an empirical framework for optimizing prompts used to classify psychological constructs in text. They experimentally compare five prompting strategies\u2014(1) codebook-guided empirical prompt selection, (2) automatic prompt engineering, (3) persona prompting, (4) chain-of-thought reasoning, and (5) explanatory prompting\u2014under both zero-shot and few-shot conditions. They test these methods across three psychological constructs and two LLMs, evaluating how well model classifications align with expert ratings and analyzing which prompt components (construct definitions, task framing, examples) most affect performance.", "result": "The study finds that persona prompting, chain-of-thought reasoning, and explanatory prompting do not adequately compensate for poorly worded prompts in classification tasks. Instead, the most critical factors for performance are the clarity and precision of the construct definition, the way the task is framed, and, to a lesser degree, the specific examples given. The best alignment with expert judgments is achieved using few-shot prompts that combine codebook-guided empirical prompt selection with automatic prompt engineering across different constructs and models.", "conclusion": "Prompt wording is a dominant driver of LLM performance in classification of psychological constructs, and popular techniques like personas or chain-of-thought alone are insufficient to fix poorly designed prompts. Researchers should generate many prompt variants\u2014both human-crafted and automatically generated\u2014then empirically evaluate them on a training dataset and validate the selected prompt and examples on a holdout set. This structured, theory-driven, and data-driven process offers a practical way to optimize LLM prompts in domains where close agreement with expert judgments is essential."}}
{"id": "2512.03903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03903", "abs": "https://arxiv.org/abs/2512.03903", "authors": ["Ekhi Azurmendi", "Joseba Fernandez de Landa", "Jaione Bengoetxea", "Maite Heredia", "Julen Etxaniz", "Mikel Zubillaga", "Ander Soraluze", "Aitor Soroa"], "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity", "comment": "Submitted to LREC 2026", "summary": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.", "AI": {"tldr": "The paper shows that training language models on a mix of standard and non-standard Basque (social media, historical, etc.) yields better performance and robustness than using only standardized text.", "motivation": "Conventional pre-training corpora are heavily filtered for quality, which tends to remove non-standard, dialectal, historical, and informal language. This exclusion can reduce robustness and reinforce representational biases, particularly for low-resource languages, and ignores the natural diversity of language use. The authors want to understand whether explicitly including such diverse data leads to more inclusive and generalizable language models.", "method": "They focus on Basque, a morphologically rich, low-resource language, and build new pre-training corpora that combine standard, social media, and historical text sources. They pre-train an encoder-only model family called BERnaT in three variants corresponding to different pre-training data: (1) standard-only corpus, (2) diverse corpus with non-standard varieties, and (3) a combined corpus including both. They also design an evaluation framework that partitions NLU tasks into standard vs. diverse subsets to test linguistic generalization across language varieties.", "result": "Across evaluations, the models trained on the combined standard+diverse data systematically outperform the ones trained only on standard corpora. The gains appear across both standard NLU benchmarks and the more diverse task subsets, and performance on standard benchmarks is not degraded by the inclusion of diverse data.", "conclusion": "Incorporating linguistically diverse data\u2014dialectal, informal, historical, and social-media varieties\u2014into pre-training improves model robustness and inclusiveness without sacrificing performance on standardized tasks. For low-resource languages like Basque, using a mixture of standard and non-standard sources is an effective strategy for building more generalizable language models and should be preferred over strictly standardized corpora."}}
{"id": "2512.03838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03838", "abs": "https://arxiv.org/abs/2512.03838", "authors": ["Michael Staniek", "Artem Sokolov", "Stefan Riezler"], "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs", "comment": null, "summary": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.", "AI": {"tldr": "The paper proposes a way to train LLMs to make early medical predictions while explicitly following step\u2011by\u2011step consensus clinical guidelines, and evaluates both their reasoning process and prediction accuracy, using Sepsis\u20113 as a case study.", "motivation": "Existing machine learning models in medicine prioritize prediction accuracy but often ignore faithful, interpretable reasoning that matches clinical guidelines, which is crucial for clinician trust and adoption. There is a need for models that not only predict correctly but can also justify their predictions using standard medical consensus rules and handle exceptions. Additionally, it is unclear whether performance limits come from guideline reasoning itself or from the difficulty of forecasting future clinical states from sparse, irregular data.", "method": "The authors verbalize formal medical consensus guidelines as explicit inference rules and instantiate them on electronic health record data to create training pairs for LLMs. They fine\u2011tune relatively small LLMs to follow these rule chains step\u2011by\u2011step, learning both standard rules and exception patterns. They define two automatic evaluation dimensions: derivation correctness (whether the model\u2019s reasoning correctly derives a conclusion from given premises under the rules) and value correctness (whether predicted clinical values match observed measurements). They use the Sepsis\u20113 consensus definition as a complex testbed and further build a multimodal architecture that combines an LLM with a dedicated time\u2011series forecasting module whose outputs are fed into the LLM.", "result": "Fine\u2011tuned small LLMs that are trained on verbalized rule instantiations achieve near\u2011perfect derivation correctness for Sepsis\u20113 rules and their exceptions on unseen patients, and they outperform much larger LLMs used in a one\u2011shot prompted manner as well as models trained only on general medical texts containing consensus definitions. The study finds that once guideline reasoning is learned, the main limitation for early prediction performance is not learning or generalizing the rules themselves, but forecasting future clinical trajectories from sparse, irregular time\u2011series data. Integrating a time\u2011series forecasting model with the LLM in a multimodal setup improves forecasting\u2011related performance metrics compared with using the LLM alone.", "conclusion": "Teaching LLMs via verbalized instantiations of medical consensus rules effectively yields models that can both follow and explain guideline\u2011based reasoning with high fidelity, even when using relatively small models. This shifts the main challenge in early medical prediction from guideline reasoning to accurate future forecasting under sparse, irregular measurements. Combining specialized time\u2011series forecasting with guideline\u2011aware LLM reasoning in a multimodal architecture improves prediction performance and offers a promising path toward trustworthy, interpretable clinical decision support systems grounded in established medical consensus."}}
{"id": "2512.04032", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04032", "abs": "https://arxiv.org/abs/2512.04032", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Florian H\u00f6nicke", "Sedigheh Eslami", "Guillaume Roncari", "Scott Martens", "Han Xiao"], "title": "Jina-VLM: Small Multilingual Vision Language Model", "comment": "18 pages, 1-7 main content", "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.", "AI": {"tldr": "Introduces Jina-VLM, a 2.4B parameter vision-language model that attains SOTA multilingual VQA among open 2B-scale VLMs.", "motivation": "To build a relatively small, open 2B-scale vision-language model that still reaches state-of-the-art performance in multilingual visual question answering, while remaining efficient and strong on pure text tasks.", "method": "Combine a SigLIP2 vision encoder and a Qwen3 language model backbone using an attention-pooling connector that allows token-efficient handling of images of arbitrary resolution. Evaluate the model on standard VQA and multilingual benchmarks, and compare with other open VLMs of similar size.", "result": "Jina-VLM surpasses comparable 2B-scale open VLMs on standard VQA and multilingual benchmarks, and maintains competitive performance on text-only tasks.", "conclusion": "A carefully designed 2.4B parameter architecture that integrates SigLIP2 and Qwen3 via an attention-pooling connector can deliver state-of-the-art multilingual VQA performance for its scale without sacrificing text-only capabilities, offering an efficient and competitive open VLM option."}}
{"id": "2512.03870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03870", "abs": "https://arxiv.org/abs/2512.03870", "authors": ["Hongzhan Lin", "Zhiqi Bai", "Xinmiao Zhang", "Sen Yang", "Xiang Li", "Siran Yang", "Yunlong Xu", "Jiaheng Liu", "Yongchi Zhao", "Jiamang Wang", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers", "comment": "under review", "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.", "AI": {"tldr": "The paper proposes FusedKV and FusedKV-Lite, cross-layer KV cache fusion methods that cut KV cache memory by ~50% while improving or closely matching perplexity versus standard Transformer decoders.", "motivation": "Transformer decoders need to cache keys and values (KV cache) for fast autoregressive inference, but this cache scales linearly with sequence length and depth, becoming a major memory bottleneck for long contexts. Existing cross-layer KV-sharing approaches reduce memory but underperform within-layer schemes like grouped-query attention (GQA). The authors want to understand why and design a cross-layer method that retains or improves model quality while substantially lowering KV memory.", "method": "They empirically analyze information flow in keys and values of upper Transformer layers, finding that values mostly originate from bottom layers whereas keys draw from both bottom and middle layers. Based on this, they introduce FusedKV, which learns a fused KV cache for top layers by combining post-RoPE keys and values from selected bottom and middle layers, avoiding recomputation of rotary embeddings. They also design FusedKV-Lite, a more efficient variant where top-layer KVs are directly reused from bottom-layer values and middle-layer keys without extra fusion parameters, trading a small perplexity increase for lower I/O overhead. Both are evaluated on LLMs from 332M to 4B parameters.", "result": "Experiments show that FusedKV and FusedKV-Lite can halve KV cache memory usage (about 50% reduction) compared with standard Transformer decoders, while FusedKV achieves lower validation perplexity and FusedKV-Lite maintains competitive perplexity with less overhead. They also outperform prior cross-layer KV sharing techniques like YOCO and CLA and rival or surpass within-layer approaches like GQA in quality-memory tradeoffs.", "conclusion": "By analyzing how information is distributed across layers in keys and values, the authors design FusedKV and FusedKV-Lite, which share and fuse KV caches across layers in a way that preserves essential representations. This yields a memory-efficient Transformer architecture that substantially reduces KV cache size while maintaining or improving language modeling performance, offering a practical solution for long-context LLMs."}}
{"id": "2512.04072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04072", "abs": "https://arxiv.org/abs/2512.04072", "authors": ["Zayne Sprague", "Jack Lu", "Manya Wadhwa", "Sedrick Keh", "Mengye Ren", "Greg Durrett"], "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors", "comment": null, "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.", "AI": {"tldr": "SkillFactory is a supervised fine-tuning method that constructs \u201csilver\u201d cognitive-skill traces from a model\u2019s own outputs to prime it for better reasoning and robustness when later trained with reinforcement learning.", "motivation": "Reinforcement-learning\u2013trained reasoning models can learn to exploit cognitive skills such as verification, backtracking, and retrying, but only if those skills are already present in the base model. This limits the kinds of reasoning behaviors RL can amplify, and makes RL training less robust and less able to generalize to harder tasks. The authors want a way to induce and scaffold such cognitive skills before RL, without relying on more capable teacher models, so that RL can more effectively learn robust, generalizable reasoning behavior.", "method": "The authors propose SkillFactory, a supervised fine-tuning (SFT) pipeline that constructs training data for various cognitive skills directly from the base model\u2019s own samples. They generate model outputs, then rearrange and reformat them into traces that resemble use of desired skills\u2014such as verification steps, retries, or alternative-solution attempts\u2014forming \u201csilver\u201d (imperfect) examples. The model is then SFT-trained on these synthetic skill-formatted traces, with no distillation from a stronger teacher. After this priming stage, the model is further trained with RL to refine and exploit these induced cognitive skills during reasoning.", "result": "Empirically, models initialized with SkillFactory SFT show several benefits once trained with RL: (1) despite weaker pre-RL performance on tasks, they achieve better post-RL generalization to harder variants of the tasks; (2) analysis shows that the models actually deploy the targeted cognitive skills (e.g., verification, backtracking) during inference; and (3) compared to RL directly on the base models, RL on SkillFactory-primed models yields systems that are more robust to performance regressions on out-of-domain tasks.", "conclusion": "Inducing cognitive-skill-like structure in model behavior via synthetic, self-generated SFT traces before RL creates useful inductive biases for later learning. Even though the silver training traces are imperfect, they successfully prime the model to acquire and robustly deploy verification, backtracking, and related reasoning skills under RL. This approach reduces dependence on stronger teacher models and leads to better generalization and robustness in reasoning-focused language models."}}
{"id": "2512.03943", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03943", "abs": "https://arxiv.org/abs/2512.03943", "authors": ["Kazi Abrab Hossain", "Jannatul Somiya Mahmud", "Maria Hossain Tuli", "Anik Mitra", "S. M. Taiabul Haque", "Farig Y. Sadeque"], "title": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions", "comment": "18 pages, 7 figures", "summary": "While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.", "AI": {"tldr": "The paper presents BRAND, a bilingual dataset for evaluating and revealing religious bias in multilingual language models, especially between English and Bengali.", "motivation": "Existing large language models, despite improvements in bias detection, still struggle with sensitive topics like religion, particularly in multilingual settings where misrepresentation can cause serious harm. There is a lack of focused resources to systematically study and evaluate such religious biases, especially across languages like English and Bengali for South Asian religions.", "method": "The authors construct BRAND, the Bilingual Religious Accountable Norm Dataset, centered on Buddhism, Christianity, Hinduism, and Islam, with over 2,400 entries. They design three different prompt types in both English and Bengali, then evaluate multilingual models on this dataset to compare performance and bias patterns between languages and across religions.", "result": "Models perform better overall in English than in Bengali, and they consistently exhibit bias toward Islam, including in religion-neutral questions. This reveals systematic disparities in how models treat religions depending on language and context.", "conclusion": "Multilingual language models exhibit persistent, language-dependent religious bias, performing worse in Bengali than in English and displaying a particular bias toward Islam. The BRAND dataset provides a new benchmark for studying and mitigating such biases and links these technical findings to broader HCI concerns about responsibly handling religion and spirituality in AI systems."}}
{"id": "2512.03976", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03976", "abs": "https://arxiv.org/abs/2512.03976", "authors": ["Lifeng Chen", "Ryan Lai", "Tianming Liu"], "title": "Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study", "comment": null, "summary": "Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\\rightarrow$ 1.54) and substantial improvements in Chinese$\\rightarrow$Tibetan translation quality (BLEU: 0.046 $\\rightarrow$ 0.261; chrF: 2.2 $\\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.", "AI": {"tldr": "Two-stage adaptation of Qwen LLM to Tibetan using continual pretraining then supervised fine-tuning, achieving strong perplexity and translation gains and analyzing where in the network the adaptation happens.", "motivation": "Low-resource languages like Tibetan lack sufficient data for effective LLM adaptation, causing poor performance and instability due to cross-lingual drift. The paper aims to systematically adapt a strong base LLM to Tibetan while understanding how the model\u2019s internal representations change, and to provide a reproducible recipe for extending LLMs to other low-resource languages.", "method": "Start from Qwen2.5-3B and adapt it to Tibetan in two stages. First, apply continual pretraining (CPT) on Tibetan data to build robust linguistic grounding and a Tibetan semantic space. Second, perform supervised fine-tuning (SFT) on Tibetan tasks, especially Chinese\u2192Tibetan translation, to specialize the model\u2019s capabilities. Evaluate with perplexity and translation metrics (BLEU, chrF). Additionally, perform layer-wise analysis (over 435 layers of a related Qwen3-4B variant) to inspect how embeddings, output heads, and MLP layers change during adaptation.", "result": "Perplexity on Tibetan decreases from 2.98 to 1.54 after adaptation, indicating much better language modeling. Chinese\u2192Tibetan translation sees large gains: BLEU improves from 0.046 to 0.261 and chrF from 2.2 to 6.6. Layer-wise analysis finds that most adaptation occurs in the embedding and output head layers, with mid\u2013late MLP projections capturing domain-specific transformations, while the rest of the network remains relatively stable.", "conclusion": "The two-stage CPT+SFT pipeline successfully adapts Qwen to Tibetan, constructing a Tibetan semantic manifold during continual pretraining and then refining task alignment with minimal disruption via SFT. The study is the first to quantitatively analyze Tibetan LLM adaptation dynamics and offers an open, reproducible framework that can guide multilingual foundation model extension to other low-resource languages."}}
{"id": "2512.03989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03989", "abs": "https://arxiv.org/abs/2512.03989", "authors": ["Taido Purason", "Pavel Chizhov", "Ivan P. Yamshchikov", "Mark Fishel"], "title": "Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models", "comment": null, "summary": "Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.", "AI": {"tldr": "The paper proposes improved methods for adapting tokenizers to new domains or languages by continuing BPE training for vocabulary extension and using leaf-based pruning to remove redundant tokens, improving tokenization efficiency without hurting model quality.", "motivation": "Existing tokenizer adaptation methods for new domains or languages typically train a new tokenizer on domain-specific text and then append non-overlapping tokens to the original vocabulary. This naive extension often leads to many added tokens that are never or rarely used, causing inefficiencies and bloated vocabularies. There is also a lack of principled methods to remove redundant tokens from an existing vocabulary while maintaining model performance. The paper aims to provide more efficient and controlled ways to modify vocabularies when transferring pre-trained language models.", "method": "The authors propose two complementary techniques. First, for vocabulary extension, they introduce continued BPE training, where the original tokenizer\u2019s BPE merge operations are not restarted from scratch but are continued on new in-domain data, letting the tokenizer evolve in a way that respects the existing vocabulary while adapting to new statistics. Second, for vocabulary pruning, they propose leaf-based vocabulary pruning, which identifies and removes redundant tokens\u2014typically those that correspond to leaves in the tokenization graph or hierarchy\u2014while ensuring that the remaining tokens can still compose the same text and maintain model quality. These methods are implemented as practical tools and released in an open-source package.", "result": "Across multiple languages and model families, continued BPE training yields more efficient tokenization compared to naive vocabulary extension, as measured by metrics such as average tokens per sequence and coverage of the added tokens. The added vocabulary is better utilized, with fewer unreachable or unused tokens. Leaf-based vocabulary pruning successfully removes redundant tokens from the vocabulary without significant degradation in model quality on downstream tasks or language modeling benchmarks, demonstrating that vocabularies can be slimmed down in a controlled manner.", "conclusion": "The paper concludes that tokenizer adaptation can be made substantially more efficient and controlled by (1) extending vocabularies via continued BPE training rather than naive token appending and (2) pruning vocabularies using a leaf-based strategy to remove redundancies. Together, these techniques improve tokenization efficiency, ensure better use of added tokens, and allow safe vocabulary reduction, and they are made available as practical, open-source tools for the community."}}
{"id": "2512.04013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04013", "abs": "https://arxiv.org/abs/2512.04013", "authors": ["Ying Wang", "Zhen Jin", "Jiexiong Xu", "Wenhai Lin", "Yiquan Chen", "Wenzhi Chen"], "title": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving", "comment": null, "summary": "As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.\n  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.", "AI": {"tldr": "AugServe is an inference framework that boosts effective throughput and cuts latency for tool-augmented LLM web services via adaptive two-stage scheduling and dynamic batching.", "motivation": "Augmented LLMs that call external tools are increasingly used in web apps, but current inference serving systems waste capacity and violate latency SLOs due to FCFS scheduling and static batching, causing head-of-line blocking and poor user experience. There is a need for a serving framework that simultaneously respects latency constraints and maximizes the number of requests completed within those constraints.", "method": "The paper proposes AugServe, which uses a two-stage adaptive request scheduling strategy that first orders requests based on their augmented LLM inference characteristics, then refines those decisions online using runtime feedback about request behavior and system state. It also introduces a dynamic token batching mechanism that adjusts batch sizes based on real-time hardware status and workload intensity to better utilize GPU resources without violating latency SLOs.", "result": "In experiments, AugServe delivers 4.7\u201333.1\u00d7 higher effective throughput compared to vLLM and 3.3\u201313.2\u00d7 higher than InferCept. It also reduces time-to-first-token (TTFT) by up to 96.3% versus vLLM and 95.0% versus InferCept, showing substantially better latency and capacity under varying loads.", "conclusion": "Adaptive scheduling and dynamic batching tailored to augmented LLM workloads can dramatically increase effective throughput and reduce latency violations versus state-of-the-art systems, making AugServe a more efficient and SLO-friendly serving solution for tool-augmented LLM applications."}}
